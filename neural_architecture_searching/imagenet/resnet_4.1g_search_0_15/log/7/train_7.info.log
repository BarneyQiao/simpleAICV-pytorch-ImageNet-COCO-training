2022-08-10 08:31:06 - net_idx: 7
2022-08-10 08:31:06 - net_config: {'stem_width': 64, 'depth': 13, 'w_0': 32, 'w_a': 26.454471101326174, 'w_m': 2.0693894510494877}
2022-08-10 08:31:06 - num_classes: 1000
2022-08-10 08:31:06 - input_image_size: 224
2022-08-10 08:31:06 - scale: 1.1428571428571428
2022-08-10 08:31:06 - seed: 0
2022-08-10 08:31:06 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-10 08:31:06 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-10 08:31:06 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce2b0>
2022-08-10 08:31:06 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce6d0>
2022-08-10 08:31:06 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce700>
2022-08-10 08:31:06 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce760>
2022-08-10 08:31:06 - batch_size: 256
2022-08-10 08:31:06 - num_workers: 16
2022-08-10 08:31:06 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-10 08:31:06 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-10 08:31:06 - epochs: 25
2022-08-10 08:31:06 - print_interval: 100
2022-08-10 08:31:06 - accumulation_steps: 1
2022-08-10 08:31:06 - sync_bn: False
2022-08-10 08:31:06 - apex: True
2022-08-10 08:31:06 - use_ema_model: False
2022-08-10 08:31:06 - ema_model_decay: 0.9999
2022-08-10 08:31:06 - log_dir: ./log
2022-08-10 08:31:06 - checkpoint_dir: ./checkpoints
2022-08-10 08:31:06 - gpus_type: NVIDIA RTX A5000
2022-08-10 08:31:06 - gpus_num: 2
2022-08-10 08:31:06 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5dddaeb570>
2022-08-10 08:31:06 - ema_model: None
2022-08-10 08:31:06 - --------------------parameters--------------------
2022-08-10 08:31:06 - name: conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-10 08:31:06 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-10 08:31:06 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-10 08:31:06 - name: fc.weight, grad: True
2022-08-10 08:31:06 - name: fc.bias, grad: True
2022-08-10 08:31:06 - --------------------buffers--------------------
2022-08-10 08:31:06 - name: conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-10 08:31:06 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-10 08:31:06 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 08:31:06 - -----------no weight decay layers--------------
2022-08-10 08:31:06 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 08:31:06 - -------------weight decay layers---------------
2022-08-10 08:31:06 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 08:31:06 - epoch 001 lr: 0.100000
2022-08-10 08:31:46 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9115
2022-08-10 08:32:19 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.8941
2022-08-10 08:32:51 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.9063
2022-08-10 08:33:24 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.8795
2022-08-10 08:33:57 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.8818
2022-08-10 08:34:30 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.7592
2022-08-10 08:35:04 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.7582
2022-08-10 08:35:37 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.6077
2022-08-10 08:36:11 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.5298
2022-08-10 08:36:44 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.4923
2022-08-10 08:37:17 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.4697
2022-08-10 08:37:51 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.3859
2022-08-10 08:38:24 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.4088
2022-08-10 08:38:57 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.3298
2022-08-10 08:39:31 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 6.2453
2022-08-10 08:40:04 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.2374
2022-08-10 08:40:38 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.9643
2022-08-10 08:41:11 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.8602
2022-08-10 08:41:45 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.8657
2022-08-10 08:42:18 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.8821
2022-08-10 08:42:52 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.7397
2022-08-10 08:43:25 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.6783
2022-08-10 08:43:58 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.4644
2022-08-10 08:44:32 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.5134
2022-08-10 08:45:05 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.4160
2022-08-10 08:45:39 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.5466
2022-08-10 08:46:13 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.3460
2022-08-10 08:46:46 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.2959
2022-08-10 08:47:20 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.2044
2022-08-10 08:47:53 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.2958
2022-08-10 08:48:27 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.2842
2022-08-10 08:49:01 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.3672
2022-08-10 08:49:35 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 5.0794
2022-08-10 08:50:08 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 5.0954
2022-08-10 08:50:42 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 5.0079
2022-08-10 08:51:15 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.8709
2022-08-10 08:51:49 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.9565
2022-08-10 08:52:23 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.7363
2022-08-10 08:52:56 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.7621
2022-08-10 08:53:30 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.8472
2022-08-10 08:54:03 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.8195
2022-08-10 08:54:37 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.6580
2022-08-10 08:55:11 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.6834
2022-08-10 08:55:44 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.4545
2022-08-10 08:56:18 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.5628
2022-08-10 08:56:52 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.8875
2022-08-10 08:57:25 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.5808
2022-08-10 08:57:59 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.7412
2022-08-10 08:58:32 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.5069
2022-08-10 08:59:05 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.3731
2022-08-10 08:59:06 - train: epoch 001, train_loss: 5.6128
2022-08-10 09:00:21 - eval: epoch: 001, acc1: 16.214%, acc5: 37.026%, test_loss: 4.4226, per_image_load_time: 2.111ms, per_image_inference_time: 0.541ms
2022-08-10 09:00:21 - until epoch: 001, best_acc1: 16.214%
2022-08-10 09:00:21 - epoch 002 lr: 0.099606
2022-08-10 09:01:01 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.3198
2022-08-10 09:01:34 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.2899
2022-08-10 09:02:07 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.4304
2022-08-10 09:02:40 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.4041
2022-08-10 09:03:13 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.0807
2022-08-10 09:03:47 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.0807
2022-08-10 09:04:20 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.4355
2022-08-10 09:04:53 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 4.1561
2022-08-10 09:05:27 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.9901
2022-08-10 09:06:01 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.1922
2022-08-10 09:06:35 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.2526
2022-08-10 09:07:09 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 4.0250
2022-08-10 09:07:42 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 4.1688
2022-08-10 09:08:15 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.2559
2022-08-10 09:08:48 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 4.1019
2022-08-10 09:09:22 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.8545
2022-08-10 09:09:55 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 4.0195
2022-08-10 09:10:28 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.9744
2022-08-10 09:11:01 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.8675
2022-08-10 09:11:35 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.5599
2022-08-10 09:12:08 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.9100
2022-08-10 09:12:41 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.8558
2022-08-10 09:13:15 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.9272
2022-08-10 09:13:49 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.6823
2022-08-10 09:14:22 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.5491
2022-08-10 09:14:55 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.5744
2022-08-10 09:15:29 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.8194
2022-08-10 09:16:02 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.7807
2022-08-10 09:16:36 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.8684
2022-08-10 09:17:10 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.5450
2022-08-10 09:17:44 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.5783
2022-08-10 09:18:17 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.7272
2022-08-10 09:18:50 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.7380
2022-08-10 09:19:24 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.5527
2022-08-10 09:19:57 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.4688
2022-08-10 09:20:31 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.5314
2022-08-10 09:21:04 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.6547
2022-08-10 09:21:38 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.2820
2022-08-10 09:22:12 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.6446
2022-08-10 09:22:45 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.3734
2022-08-10 09:23:19 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.5153
2022-08-10 09:23:52 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.5662
2022-08-10 09:24:26 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.4753
2022-08-10 09:25:00 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.4600
2022-08-10 09:25:33 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.5099
2022-08-10 09:26:07 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.2695
2022-08-10 09:26:41 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.3800
2022-08-10 09:27:15 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.6276
2022-08-10 09:27:48 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.3425
2022-08-10 09:28:20 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.5305
2022-08-10 09:28:22 - train: epoch 002, train_loss: 3.8285
2022-08-10 09:29:36 - eval: epoch: 002, acc1: 28.360%, acc5: 53.920%, test_loss: 3.7388, per_image_load_time: 2.306ms, per_image_inference_time: 0.543ms
2022-08-10 09:29:36 - until epoch: 002, best_acc1: 28.360%
2022-08-10 09:29:36 - epoch 003 lr: 0.098429
2022-08-10 09:30:16 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.4289
2022-08-10 09:30:49 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.5030
2022-08-10 09:31:22 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.4277
2022-08-10 09:31:55 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.2465
2022-08-10 09:32:28 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.5714
2022-08-10 09:33:01 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.0497
2022-08-10 09:33:34 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.2869
2022-08-10 09:34:07 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.4897
2022-08-10 09:34:41 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.1829
2022-08-10 09:35:14 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.3158
2022-08-10 09:35:47 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.0704
2022-08-10 09:36:21 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.3347
2022-08-10 09:36:55 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.0383
2022-08-10 09:37:28 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.0350
2022-08-10 09:38:02 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.3121
2022-08-10 09:38:36 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.1931
2022-08-10 09:39:10 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.1408
2022-08-10 09:39:43 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.1823
2022-08-10 09:40:17 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.1506
2022-08-10 09:40:51 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.2168
2022-08-10 09:41:25 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.1976
2022-08-10 09:41:58 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.5283
2022-08-10 09:42:31 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 3.0748
2022-08-10 09:43:05 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.0995
2022-08-10 09:43:39 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.2190
2022-08-10 09:44:12 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.1438
2022-08-10 09:44:46 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.5241
2022-08-10 09:45:19 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 2.9388
2022-08-10 09:45:53 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.9798
2022-08-10 09:46:27 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.2262
2022-08-10 09:47:00 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.5494
2022-08-10 09:47:34 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.0604
2022-08-10 09:48:08 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0847
2022-08-10 09:48:41 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.2048
2022-08-10 09:49:15 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.9529
2022-08-10 09:49:49 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.9700
2022-08-10 09:50:23 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.1808
2022-08-10 09:50:57 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.2002
2022-08-10 09:51:31 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.2694
2022-08-10 09:52:04 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7442
2022-08-10 09:52:38 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.9263
2022-08-10 09:53:11 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.0293
2022-08-10 09:53:46 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.8564
2022-08-10 09:54:20 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.9739
2022-08-10 09:54:54 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.2192
2022-08-10 09:55:27 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.8650
2022-08-10 09:56:01 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 3.0566
2022-08-10 09:56:35 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.1112
2022-08-10 09:57:09 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.1455
2022-08-10 09:57:42 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.9829
2022-08-10 09:57:43 - train: epoch 003, train_loss: 3.1626
2022-08-10 09:58:58 - eval: epoch: 003, acc1: 36.662%, acc5: 62.726%, test_loss: 2.9667, per_image_load_time: 2.255ms, per_image_inference_time: 0.536ms
2022-08-10 09:58:58 - until epoch: 003, best_acc1: 36.662%
2022-08-10 09:58:58 - epoch 004 lr: 0.096488
2022-08-10 09:59:38 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 3.0230
2022-08-10 10:00:10 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.7898
2022-08-10 10:00:43 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 3.0205
2022-08-10 10:01:16 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 3.0499
2022-08-10 10:01:49 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.8279
2022-08-10 10:02:22 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.1435
2022-08-10 10:02:55 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 3.0199
2022-08-10 10:03:28 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.8545
2022-08-10 10:04:02 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.6380
2022-08-10 10:04:35 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.8609
2022-08-10 10:05:09 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.0844
2022-08-10 10:05:42 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.7155
2022-08-10 10:06:15 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.7222
2022-08-10 10:06:49 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.9275
2022-08-10 10:07:22 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 3.1261
2022-08-10 10:07:55 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 3.1286
2022-08-10 10:08:29 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.8184
2022-08-10 10:09:02 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 3.0372
2022-08-10 10:09:36 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.8926
2022-08-10 10:10:09 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.7688
2022-08-10 10:10:43 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.7943
2022-08-10 10:11:16 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.8558
2022-08-10 10:11:50 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.6279
2022-08-10 10:12:23 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.8319
2022-08-10 10:12:56 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.6538
2022-08-10 10:13:30 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.7483
2022-08-10 10:14:03 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.6074
2022-08-10 10:14:37 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8925
2022-08-10 10:15:10 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.7881
2022-08-10 10:15:44 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.8387
2022-08-10 10:16:17 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.8162
2022-08-10 10:16:50 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8277
2022-08-10 10:17:24 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8270
2022-08-10 10:17:57 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.9075
2022-08-10 10:18:31 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.6492
2022-08-10 10:19:04 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.6810
2022-08-10 10:19:37 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.6690
2022-08-10 10:20:10 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.4596
2022-08-10 10:20:43 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.6689
2022-08-10 10:21:16 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.7051
2022-08-10 10:21:50 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.5626
2022-08-10 10:22:23 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6728
2022-08-10 10:22:57 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.7002
2022-08-10 10:23:30 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.6746
2022-08-10 10:24:04 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.1688
2022-08-10 10:24:38 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.6523
2022-08-10 10:25:11 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.7227
2022-08-10 10:25:44 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.5290
2022-08-10 10:26:18 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.8382
2022-08-10 10:26:51 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.8081
2022-08-10 10:26:52 - train: epoch 004, train_loss: 2.8366
2022-08-10 10:28:06 - eval: epoch: 004, acc1: 45.288%, acc5: 71.700%, test_loss: 2.4243, per_image_load_time: 1.303ms, per_image_inference_time: 0.557ms
2022-08-10 10:28:06 - until epoch: 004, best_acc1: 45.288%
2022-08-10 10:28:06 - epoch 005 lr: 0.093815
2022-08-10 10:28:45 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.7023
2022-08-10 10:29:18 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.9345
2022-08-10 10:29:52 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.8502
2022-08-10 10:30:25 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.7106
2022-08-10 10:30:58 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.4704
2022-08-10 10:31:31 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.8052
2022-08-10 10:32:05 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.7287
2022-08-10 10:32:38 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.9547
2022-08-10 10:33:12 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.7888
2022-08-10 10:33:45 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.5859
2022-08-10 10:34:19 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.6987
2022-08-10 10:34:53 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.5593
2022-08-10 10:35:26 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.4848
2022-08-10 10:36:00 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.6742
2022-08-10 10:36:33 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.5893
2022-08-10 10:37:06 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.4304
2022-08-10 10:37:40 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.6658
2022-08-10 10:38:14 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.6058
2022-08-10 10:38:47 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.4209
2022-08-10 10:39:21 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.5194
2022-08-10 10:39:54 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.4851
2022-08-10 10:40:28 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.4840
2022-08-10 10:41:02 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.4063
2022-08-10 10:41:35 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5760
2022-08-10 10:42:09 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.5742
2022-08-10 10:42:43 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.7958
2022-08-10 10:43:18 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.8988
2022-08-10 10:43:51 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.5420
2022-08-10 10:44:25 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.5004
2022-08-10 10:44:58 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.4295
2022-08-10 10:45:32 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.5515
2022-08-10 10:46:06 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.6193
2022-08-10 10:46:39 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.4389
2022-08-10 10:47:13 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.4963
2022-08-10 10:47:46 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.7656
2022-08-10 10:48:20 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6532
2022-08-10 10:48:53 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.5281
2022-08-10 10:49:26 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5654
2022-08-10 10:50:00 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.7106
2022-08-10 10:50:34 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.7134
2022-08-10 10:51:07 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.4940
2022-08-10 10:51:41 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.7393
2022-08-10 10:52:14 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.6714
2022-08-10 10:52:48 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.5652
2022-08-10 10:53:22 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.9248
2022-08-10 10:53:55 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.4655
2022-08-10 10:54:29 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.4199
2022-08-10 10:55:03 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.2911
2022-08-10 10:55:36 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.6911
2022-08-10 10:56:09 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.5074
2022-08-10 10:56:10 - train: epoch 005, train_loss: 2.6271
2022-08-10 10:57:25 - eval: epoch: 005, acc1: 46.566%, acc5: 73.172%, test_loss: 2.3543, per_image_load_time: 2.210ms, per_image_inference_time: 0.564ms
2022-08-10 10:57:25 - until epoch: 005, best_acc1: 46.566%
2022-08-10 10:57:25 - epoch 006 lr: 0.090450
2022-08-10 10:58:05 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.5008
2022-08-10 10:58:38 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.4691
2022-08-10 10:59:11 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.5302
2022-08-10 10:59:44 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.6338
2022-08-10 11:00:17 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.3925
2022-08-10 11:00:51 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.5029
2022-08-10 11:01:24 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.5313
2022-08-10 11:01:58 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.5543
2022-08-10 11:02:31 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.2667
2022-08-10 11:03:05 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.3828
2022-08-10 11:03:38 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.7216
2022-08-10 11:04:12 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.6739
2022-08-10 11:04:45 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.4278
2022-08-10 11:05:19 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.5518
2022-08-10 11:05:53 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.7970
2022-08-10 11:06:26 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.4473
2022-08-10 11:06:59 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.5166
2022-08-10 11:07:33 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.4993
2022-08-10 11:08:07 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.3743
2022-08-10 11:08:40 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.7302
2022-08-10 11:09:13 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.3877
2022-08-10 11:09:47 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.1297
2022-08-10 11:10:21 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.3120
2022-08-10 11:10:54 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.2948
2022-08-10 11:11:28 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.5180
2022-08-10 11:12:01 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.4603
2022-08-10 11:12:35 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.4273
2022-08-10 11:13:09 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.4034
2022-08-10 11:13:42 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.6932
2022-08-10 11:14:16 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.3690
2022-08-10 11:14:49 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.2468
2022-08-10 11:15:22 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.3448
2022-08-10 11:15:56 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.3460
2022-08-10 11:16:30 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.5966
2022-08-10 11:17:03 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.6162
2022-08-10 11:17:37 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.5154
2022-08-10 11:18:10 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.3229
2022-08-10 11:18:43 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.2556
2022-08-10 11:19:17 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.4176
2022-08-10 11:19:50 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.4083
2022-08-10 11:20:24 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.3386
2022-08-10 11:20:58 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.2967
2022-08-10 11:21:32 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.5270
2022-08-10 11:22:05 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.3381
2022-08-10 11:22:39 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.5444
2022-08-10 11:23:12 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.5116
2022-08-10 11:23:46 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4543
2022-08-10 11:24:19 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.5103
2022-08-10 11:24:53 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.4018
2022-08-10 11:25:26 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.3175
2022-08-10 11:25:28 - train: epoch 006, train_loss: 2.4881
2022-08-10 11:26:43 - eval: epoch: 006, acc1: 47.062%, acc5: 73.318%, test_loss: 2.3274, per_image_load_time: 2.221ms, per_image_inference_time: 0.563ms
2022-08-10 11:26:43 - until epoch: 006, best_acc1: 47.062%
2022-08-10 11:26:43 - epoch 007 lr: 0.086448
2022-08-10 11:27:23 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.2719
2022-08-10 11:27:57 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.7213
2022-08-10 11:28:30 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.7287
2022-08-10 11:29:03 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.5149
2022-08-10 11:29:36 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.2210
2022-08-10 11:30:09 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.4087
2022-08-10 11:30:43 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.4837
2022-08-10 11:31:17 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.7043
2022-08-10 11:31:50 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.5264
2022-08-10 11:32:24 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.4986
2022-08-10 11:32:58 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.3002
2022-08-10 11:33:32 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.4271
2022-08-10 11:34:05 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.0922
2022-08-10 11:34:39 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.3851
2022-08-10 11:35:13 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.4438
2022-08-10 11:35:46 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.4004
2022-08-10 11:36:19 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.5190
2022-08-10 11:36:53 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.4935
2022-08-10 11:37:27 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.5288
2022-08-10 11:38:00 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.1481
2022-08-10 11:38:34 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.5093
2022-08-10 11:39:08 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.4605
2022-08-10 11:39:41 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.4853
2022-08-10 11:40:15 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.5439
2022-08-10 11:40:49 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.3591
2022-08-10 11:41:23 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.3688
2022-08-10 11:41:56 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.2474
2022-08-10 11:42:30 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.2881
2022-08-10 11:43:04 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.2326
2022-08-10 11:43:38 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.4575
2022-08-10 11:44:11 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.4532
2022-08-10 11:44:45 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.4835
2022-08-10 11:45:19 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.3946
2022-08-10 11:45:52 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.2934
2022-08-10 11:46:25 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.4837
2022-08-10 11:46:59 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.1834
2022-08-10 11:47:32 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.3667
2022-08-10 11:48:05 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.4871
2022-08-10 11:48:39 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.3372
2022-08-10 11:49:12 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.5969
2022-08-10 11:49:45 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.1903
2022-08-10 11:50:19 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2195
2022-08-10 11:50:53 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.3537
2022-08-10 11:51:26 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.2712
2022-08-10 11:52:00 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.2959
2022-08-10 11:52:34 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.3877
2022-08-10 11:53:07 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.2820
2022-08-10 11:53:41 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.6711
2022-08-10 11:54:14 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.2120
2022-08-10 11:54:47 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.2616
2022-08-10 11:54:49 - train: epoch 007, train_loss: 2.3835
2022-08-10 11:56:03 - eval: epoch: 007, acc1: 51.560%, acc5: 77.026%, test_loss: 2.0894, per_image_load_time: 2.281ms, per_image_inference_time: 0.565ms
2022-08-10 11:56:03 - until epoch: 007, best_acc1: 51.560%
2022-08-10 11:56:03 - epoch 008 lr: 0.081870
2022-08-10 11:56:43 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.1593
2022-08-10 11:57:16 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.4165
2022-08-10 11:57:48 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.2191
2022-08-10 11:58:22 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.2561
2022-08-10 11:58:55 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.1403
2022-08-10 11:59:28 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.3174
2022-08-10 12:00:01 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.4252
2022-08-10 12:00:34 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.2511
2022-08-10 12:01:07 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.1666
2022-08-10 12:01:40 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.3957
2022-08-10 12:02:14 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.2034
2022-08-10 12:02:47 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.1421
2022-08-10 12:03:20 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.4390
2022-08-10 12:03:54 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.2308
2022-08-10 12:04:27 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.3447
2022-08-10 12:05:01 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.4448
2022-08-10 12:05:35 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.3238
2022-08-10 12:06:08 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.3826
2022-08-10 12:06:42 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.1388
2022-08-10 12:07:16 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.4155
2022-08-10 12:07:49 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.2020
2022-08-10 12:08:23 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.3115
2022-08-10 12:08:56 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.4033
2022-08-10 12:09:30 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.1957
2022-08-10 12:10:03 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.2578
2022-08-10 12:10:37 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.2289
2022-08-10 12:11:10 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.5001
2022-08-10 12:11:44 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.5840
2022-08-10 12:12:17 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.3313
2022-08-10 12:12:51 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.3566
2022-08-10 12:13:24 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.4163
2022-08-10 12:13:58 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.5832
2022-08-10 12:14:32 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.3788
2022-08-10 12:15:05 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.3489
2022-08-10 12:15:38 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.3604
2022-08-10 12:16:12 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.5501
2022-08-10 12:16:46 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.3339
2022-08-10 12:17:19 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.1354
2022-08-10 12:17:53 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.3455
2022-08-10 12:18:26 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.5568
2022-08-10 12:19:00 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.0857
2022-08-10 12:19:33 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.2362
2022-08-10 12:20:07 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.0974
2022-08-10 12:20:40 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.2106
2022-08-10 12:21:13 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.4677
2022-08-10 12:21:47 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.3511
2022-08-10 12:22:21 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.3305
2022-08-10 12:22:53 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.3279
2022-08-10 12:23:27 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.3635
2022-08-10 12:24:00 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.2564
2022-08-10 12:24:01 - train: epoch 008, train_loss: 2.2992
2022-08-10 12:25:16 - eval: epoch: 008, acc1: 53.554%, acc5: 78.904%, test_loss: 1.9990, per_image_load_time: 2.350ms, per_image_inference_time: 0.556ms
2022-08-10 12:25:17 - until epoch: 008, best_acc1: 53.554%
2022-08-10 12:25:17 - epoch 009 lr: 0.076790
2022-08-10 12:25:56 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 1.9186
2022-08-10 12:26:29 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.1564
2022-08-10 12:27:03 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.1233
2022-08-10 12:27:36 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.6673
2022-08-10 12:28:10 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.1940
2022-08-10 12:28:44 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.3192
2022-08-10 12:29:18 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 1.9783
2022-08-10 12:29:51 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.1983
2022-08-10 12:30:24 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.0742
2022-08-10 12:30:58 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1152
2022-08-10 12:31:32 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.4442
2022-08-10 12:32:06 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.2829
2022-08-10 12:32:40 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.4346
2022-08-10 12:33:13 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9012
2022-08-10 12:33:46 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.1176
2022-08-10 12:34:20 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2239
2022-08-10 12:34:53 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.2496
2022-08-10 12:35:27 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.3510
2022-08-10 12:36:00 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.0441
2022-08-10 12:36:34 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 1.9795
2022-08-10 12:37:07 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.1688
2022-08-10 12:37:40 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.3883
2022-08-10 12:38:14 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.0960
2022-08-10 12:38:48 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.2038
2022-08-10 12:39:22 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.0436
2022-08-10 12:39:55 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.2774
2022-08-10 12:40:29 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.2284
2022-08-10 12:41:03 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2636
2022-08-10 12:41:36 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.1820
2022-08-10 12:42:10 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1659
2022-08-10 12:42:43 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.2846
2022-08-10 12:43:16 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.1465
2022-08-10 12:43:50 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.1767
2022-08-10 12:44:23 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.3058
2022-08-10 12:44:56 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.2882
2022-08-10 12:45:30 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.1439
2022-08-10 12:46:03 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.3454
2022-08-10 12:46:37 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.3145
2022-08-10 12:47:10 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.1603
2022-08-10 12:47:44 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.3880
2022-08-10 12:48:17 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.2592
2022-08-10 12:48:51 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 1.9706
2022-08-10 12:49:25 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.1961
2022-08-10 12:49:59 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.3887
2022-08-10 12:50:32 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.4070
2022-08-10 12:51:06 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.2671
2022-08-10 12:51:39 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.3948
2022-08-10 12:52:13 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.2416
2022-08-10 12:52:47 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.4160
2022-08-10 12:53:19 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.0810
2022-08-10 12:53:21 - train: epoch 009, train_loss: 2.2289
2022-08-10 12:54:35 - eval: epoch: 009, acc1: 54.184%, acc5: 79.052%, test_loss: 1.9780, per_image_load_time: 1.984ms, per_image_inference_time: 0.558ms
2022-08-10 12:54:35 - until epoch: 009, best_acc1: 54.184%
2022-08-10 12:54:35 - epoch 010 lr: 0.071288
2022-08-10 12:55:16 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.1451
2022-08-10 12:55:48 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.1885
2022-08-10 12:56:22 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.2037
2022-08-10 12:56:55 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.2685
2022-08-10 12:57:28 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.1371
2022-08-10 12:58:02 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.4358
2022-08-10 12:58:35 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.2496
2022-08-10 12:59:08 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.3022
2022-08-10 12:59:41 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.2036
2022-08-10 13:00:15 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.0997
2022-08-10 13:00:48 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.1830
2022-08-10 13:01:22 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.1877
2022-08-10 13:01:55 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.0032
2022-08-10 13:02:28 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.2538
2022-08-10 13:03:02 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 2.0891
2022-08-10 13:03:35 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.1273
2022-08-10 13:04:09 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.1737
2022-08-10 13:04:42 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 2.1255
2022-08-10 13:05:15 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.2536
2022-08-10 13:05:49 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.2069
2022-08-10 13:06:22 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 1.9749
2022-08-10 13:06:55 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.3312
2022-08-10 13:07:29 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.4008
2022-08-10 13:08:02 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.3409
2022-08-10 13:08:36 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.2082
2022-08-10 13:09:09 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.2309
2022-08-10 13:09:43 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.8979
2022-08-10 13:10:16 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.0977
2022-08-10 13:10:50 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.1766
2022-08-10 13:11:24 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.0782
2022-08-10 13:11:57 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.3007
2022-08-10 13:12:31 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.1424
2022-08-10 13:13:04 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.1986
2022-08-10 13:13:37 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.4875
2022-08-10 13:14:11 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.4260
2022-08-10 13:14:44 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.3187
2022-08-10 13:15:18 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.1292
2022-08-10 13:15:52 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1818
2022-08-10 13:16:25 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 1.9390
2022-08-10 13:16:59 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.0324
2022-08-10 13:17:32 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 1.9902
2022-08-10 13:18:06 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.1562
2022-08-10 13:18:40 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.0340
2022-08-10 13:19:13 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.1166
2022-08-10 13:19:47 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.2173
2022-08-10 13:20:21 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.1091
2022-08-10 13:20:54 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.2004
2022-08-10 13:21:28 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.1283
2022-08-10 13:22:01 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.3057
2022-08-10 13:22:34 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 1.9669
2022-08-10 13:22:35 - train: epoch 010, train_loss: 2.1602
2022-08-10 13:23:50 - eval: epoch: 010, acc1: 55.116%, acc5: 80.078%, test_loss: 1.9173, per_image_load_time: 2.302ms, per_image_inference_time: 0.561ms
2022-08-10 13:23:50 - until epoch: 010, best_acc1: 55.116%
2022-08-10 13:23:50 - epoch 011 lr: 0.065450
2022-08-10 13:24:30 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.2077
2022-08-10 13:25:03 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.2538
2022-08-10 13:25:36 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.1268
2022-08-10 13:26:09 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.2021
2022-08-10 13:26:43 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 1.9661
2022-08-10 13:27:15 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.0830
2022-08-10 13:27:49 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.1588
2022-08-10 13:28:22 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.2172
2022-08-10 13:28:55 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.1640
2022-08-10 13:29:28 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 1.9785
2022-08-10 13:30:01 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2636
2022-08-10 13:30:35 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.1763
2022-08-10 13:31:09 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.2741
2022-08-10 13:31:42 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.2512
2022-08-10 13:32:15 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 1.9704
2022-08-10 13:32:49 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.3979
2022-08-10 13:33:22 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.0281
2022-08-10 13:33:55 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 1.9358
2022-08-10 13:34:28 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 1.8730
2022-08-10 13:35:02 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1699
2022-08-10 13:35:35 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.0820
2022-08-10 13:36:08 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 1.9379
2022-08-10 13:36:42 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.1187
2022-08-10 13:37:16 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.8668
2022-08-10 13:37:49 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.1001
2022-08-10 13:38:23 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.1228
2022-08-10 13:38:56 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 1.9129
2022-08-10 13:39:30 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.9986
2022-08-10 13:40:04 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.0554
2022-08-10 13:40:37 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.4383
2022-08-10 13:41:10 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.0298
2022-08-10 13:41:44 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.0282
2022-08-10 13:42:18 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.1884
2022-08-10 13:42:51 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.2037
2022-08-10 13:43:24 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.1784
2022-08-10 13:43:58 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.0329
2022-08-10 13:44:31 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.1616
2022-08-10 13:45:04 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.9203
2022-08-10 13:45:38 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.1473
2022-08-10 13:46:12 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.0549
2022-08-10 13:46:45 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.9513
2022-08-10 13:47:18 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 2.1755
2022-08-10 13:47:52 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 1.9177
2022-08-10 13:48:26 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.1667
2022-08-10 13:49:00 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 1.9859
2022-08-10 13:49:34 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 1.9031
2022-08-10 13:50:07 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.8551
2022-08-10 13:50:41 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.6444
2022-08-10 13:51:14 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 1.9841
2022-08-10 13:51:47 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.0381
2022-08-10 13:51:48 - train: epoch 011, train_loss: 2.1005
2022-08-10 13:53:04 - eval: epoch: 011, acc1: 57.132%, acc5: 81.520%, test_loss: 1.8086, per_image_load_time: 2.360ms, per_image_inference_time: 0.538ms
2022-08-10 13:53:05 - until epoch: 011, best_acc1: 57.132%
2022-08-10 13:53:05 - epoch 012 lr: 0.059368
2022-08-10 13:53:45 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.7953
2022-08-10 13:54:18 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.7707
2022-08-10 13:54:51 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 2.0211
2022-08-10 13:55:24 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 1.9755
2022-08-10 13:55:58 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.2935
2022-08-10 13:56:31 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.8181
2022-08-10 13:57:04 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.8065
2022-08-10 13:57:38 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.2159
2022-08-10 13:58:11 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.2226
2022-08-10 13:58:45 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9814
2022-08-10 13:59:18 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.3178
2022-08-10 13:59:52 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.8038
2022-08-10 14:00:25 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.9513
2022-08-10 14:00:59 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.2898
2022-08-10 14:01:32 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 1.8692
2022-08-10 14:02:06 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 2.1029
2022-08-10 14:02:40 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.9337
2022-08-10 14:03:13 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.1401
2022-08-10 14:03:47 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.2253
2022-08-10 14:04:21 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.1496
2022-08-10 14:04:54 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.0553
2022-08-10 14:05:28 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 1.9674
2022-08-10 14:06:01 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 2.0543
2022-08-10 14:06:35 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.0432
2022-08-10 14:07:08 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.6981
2022-08-10 14:07:42 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.7970
2022-08-10 14:08:15 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.1936
2022-08-10 14:08:49 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.8896
2022-08-10 14:09:23 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 1.8974
2022-08-10 14:09:56 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.9901
2022-08-10 14:10:29 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.0974
2022-08-10 14:11:02 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.8003
2022-08-10 14:11:36 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.0112
2022-08-10 14:12:09 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.1971
2022-08-10 14:12:43 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 1.9482
2022-08-10 14:13:16 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.8262
2022-08-10 14:13:50 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.0794
2022-08-10 14:14:23 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.0592
2022-08-10 14:14:57 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8896
2022-08-10 14:15:30 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1981
2022-08-10 14:16:04 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.0524
2022-08-10 14:16:37 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 2.1864
2022-08-10 14:17:11 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.1079
2022-08-10 14:17:44 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.8177
2022-08-10 14:18:17 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.7151
2022-08-10 14:18:51 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.0993
2022-08-10 14:19:25 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.8774
2022-08-10 14:19:58 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.1392
2022-08-10 14:20:32 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.7895
2022-08-10 14:21:04 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.9444
2022-08-10 14:21:05 - train: epoch 012, train_loss: 2.0399
2022-08-10 14:22:20 - eval: epoch: 012, acc1: 58.372%, acc5: 82.382%, test_loss: 1.7500, per_image_load_time: 2.353ms, per_image_inference_time: 0.549ms
2022-08-10 14:22:21 - until epoch: 012, best_acc1: 58.372%
2022-08-10 14:22:21 - epoch 013 lr: 0.053138
2022-08-10 14:23:01 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.8682
2022-08-10 14:23:34 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.9110
2022-08-10 14:24:07 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8850
2022-08-10 14:24:40 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.8483
2022-08-10 14:25:13 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 2.0236
2022-08-10 14:25:46 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 1.9567
2022-08-10 14:26:19 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.8108
2022-08-10 14:26:53 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.0509
2022-08-10 14:27:26 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 2.0039
2022-08-10 14:27:59 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.9961
2022-08-10 14:28:32 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.0490
2022-08-10 14:29:05 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 1.9085
2022-08-10 14:29:39 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.7669
2022-08-10 14:30:12 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.0333
2022-08-10 14:30:45 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.2860
2022-08-10 14:31:18 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.7836
2022-08-10 14:31:52 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.0394
2022-08-10 14:32:25 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9332
2022-08-10 14:32:58 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 1.9771
2022-08-10 14:33:31 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.1503
2022-08-10 14:34:05 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.2342
2022-08-10 14:34:38 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.7983
2022-08-10 14:35:12 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 1.9787
2022-08-10 14:35:45 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 1.9341
2022-08-10 14:36:18 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.9842
2022-08-10 14:36:52 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.6979
2022-08-10 14:37:25 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.8658
2022-08-10 14:37:59 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.2101
2022-08-10 14:38:32 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.0384
2022-08-10 14:39:06 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.8249
2022-08-10 14:39:39 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.7254
2022-08-10 14:40:13 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 1.9931
2022-08-10 14:40:46 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.8366
2022-08-10 14:41:20 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.9436
2022-08-10 14:41:53 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.8512
2022-08-10 14:42:27 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.1974
2022-08-10 14:43:00 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.7996
2022-08-10 14:43:34 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1826
2022-08-10 14:44:08 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.2382
2022-08-10 14:44:41 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 2.0449
2022-08-10 14:45:15 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.8580
2022-08-10 14:45:49 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 1.9831
2022-08-10 14:46:22 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.8260
2022-08-10 14:46:56 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.8946
2022-08-10 14:47:30 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.7977
2022-08-10 14:48:03 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 1.9414
2022-08-10 14:48:37 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 2.1988
2022-08-10 14:49:10 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.0051
2022-08-10 14:49:44 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0451
2022-08-10 14:50:17 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.1312
2022-08-10 14:50:18 - train: epoch 013, train_loss: 1.9789
2022-08-10 14:51:33 - eval: epoch: 013, acc1: 59.652%, acc5: 83.394%, test_loss: 1.7049, per_image_load_time: 2.318ms, per_image_inference_time: 0.562ms
2022-08-10 14:51:33 - until epoch: 013, best_acc1: 59.652%
2022-08-10 14:51:33 - epoch 014 lr: 0.046859
2022-08-10 14:52:13 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.8782
2022-08-10 14:52:46 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.8268
2022-08-10 14:53:20 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.8102
2022-08-10 14:53:53 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 2.0522
2022-08-10 14:54:26 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.8654
2022-08-10 14:55:00 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 2.0692
2022-08-10 14:55:33 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.7768
2022-08-10 14:56:06 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.8305
2022-08-10 14:56:39 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 2.0025
2022-08-10 14:57:13 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.0089
2022-08-10 14:57:46 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.8650
2022-08-10 14:58:19 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 1.9444
2022-08-10 14:58:53 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 1.9768
2022-08-10 14:59:26 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 2.0352
2022-08-10 14:59:59 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.0661
2022-08-10 15:00:33 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.7159
2022-08-10 15:01:06 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 2.0893
2022-08-10 15:01:39 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.0019
2022-08-10 15:02:12 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.8038
2022-08-10 15:02:46 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.7823
2022-08-10 15:03:20 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.1008
2022-08-10 15:03:53 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.9799
2022-08-10 15:04:27 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.7532
2022-08-10 15:05:00 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.8966
2022-08-10 15:05:34 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.7672
2022-08-10 15:06:08 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8413
2022-08-10 15:06:41 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.8071
2022-08-10 15:07:15 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 2.0383
2022-08-10 15:07:49 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9320
2022-08-10 15:08:22 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 2.0072
2022-08-10 15:08:56 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 2.0171
2022-08-10 15:09:29 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.8266
2022-08-10 15:10:03 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.9283
2022-08-10 15:10:37 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 2.0062
2022-08-10 15:11:10 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.8461
2022-08-10 15:11:43 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.7568
2022-08-10 15:12:17 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.8596
2022-08-10 15:12:51 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 2.1155
2022-08-10 15:13:24 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.8477
2022-08-10 15:13:58 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.8400
2022-08-10 15:14:31 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.7643
2022-08-10 15:15:05 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.8329
2022-08-10 15:15:39 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.8146
2022-08-10 15:16:13 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.8483
2022-08-10 15:16:46 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.8596
2022-08-10 15:17:20 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.8698
2022-08-10 15:17:53 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.8481
2022-08-10 15:18:27 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.8954
2022-08-10 15:19:00 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.6965
2022-08-10 15:19:32 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.9117
2022-08-10 15:19:34 - train: epoch 014, train_loss: 1.9181
2022-08-10 15:20:48 - eval: epoch: 014, acc1: 60.228%, acc5: 83.590%, test_loss: 1.6723, per_image_load_time: 2.153ms, per_image_inference_time: 0.555ms
2022-08-10 15:20:49 - until epoch: 014, best_acc1: 60.228%
2022-08-10 15:20:49 - epoch 015 lr: 0.040630
2022-08-10 15:21:28 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.8919
2022-08-10 15:22:01 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 2.0204
2022-08-10 15:22:35 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.1085
2022-08-10 15:23:08 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.8846
2022-08-10 15:23:41 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.9262
2022-08-10 15:24:14 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 1.9082
2022-08-10 15:24:47 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.8696
2022-08-10 15:25:20 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.8862
2022-08-10 15:25:54 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 2.0162
2022-08-10 15:26:27 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.8111
2022-08-10 15:27:00 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7398
2022-08-10 15:27:33 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.9405
2022-08-10 15:28:07 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 2.0054
2022-08-10 15:28:40 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.8591
2022-08-10 15:29:14 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.6241
2022-08-10 15:29:47 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.7756
2022-08-10 15:30:20 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.7278
2022-08-10 15:30:53 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.8730
2022-08-10 15:31:26 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.7041
2022-08-10 15:32:00 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.6196
2022-08-10 15:32:33 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.5004
2022-08-10 15:33:06 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9225
2022-08-10 15:33:39 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.8499
2022-08-10 15:34:13 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.7794
2022-08-10 15:34:47 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.9858
2022-08-10 15:35:20 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.5956
2022-08-10 15:35:54 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.9036
2022-08-10 15:36:28 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.8567
2022-08-10 15:37:02 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.9821
2022-08-10 15:37:35 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.6576
2022-08-10 15:38:08 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.7038
2022-08-10 15:38:42 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.8057
2022-08-10 15:39:15 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.6161
2022-08-10 15:39:49 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.7243
2022-08-10 15:40:22 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.1293
2022-08-10 15:40:56 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.6014
2022-08-10 15:41:29 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.8295
2022-08-10 15:42:03 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.9004
2022-08-10 15:42:37 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.9706
2022-08-10 15:43:10 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.8752
2022-08-10 15:43:44 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.8340
2022-08-10 15:44:17 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.7989
2022-08-10 15:44:51 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.6790
2022-08-10 15:45:24 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.9841
2022-08-10 15:45:57 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.6150
2022-08-10 15:46:31 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.7720
2022-08-10 15:47:05 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.8650
2022-08-10 15:47:38 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.7603
2022-08-10 15:48:12 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.7750
2022-08-10 15:48:44 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.8608
2022-08-10 15:48:45 - train: epoch 015, train_loss: 1.8530
2022-08-10 15:50:00 - eval: epoch: 015, acc1: 62.956%, acc5: 85.366%, test_loss: 1.5392, per_image_load_time: 2.265ms, per_image_inference_time: 0.553ms
2022-08-10 15:50:00 - until epoch: 015, best_acc1: 62.956%
2022-08-10 15:50:00 - epoch 016 lr: 0.034548
2022-08-10 15:50:41 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.7776
2022-08-10 15:51:14 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.5610
2022-08-10 15:51:48 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.7793
2022-08-10 15:52:21 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 1.9899
2022-08-10 15:52:53 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.6306
2022-08-10 15:53:26 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.7058
2022-08-10 15:54:00 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.8370
2022-08-10 15:54:33 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.8365
2022-08-10 15:55:06 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.8470
2022-08-10 15:55:39 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.7590
2022-08-10 15:56:13 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.6444
2022-08-10 15:56:46 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.7858
2022-08-10 15:57:19 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.9164
2022-08-10 15:57:53 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.7172
2022-08-10 15:58:27 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 2.0188
2022-08-10 15:59:00 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.9808
2022-08-10 15:59:34 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.8709
2022-08-10 16:00:07 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.9174
2022-08-10 16:00:40 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.8428
2022-08-10 16:01:14 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.5610
2022-08-10 16:01:47 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8440
2022-08-10 16:02:21 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.9311
2022-08-10 16:02:54 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.7981
2022-08-10 16:03:28 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.8860
2022-08-10 16:04:01 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.7974
2022-08-10 16:04:35 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.9636
2022-08-10 16:05:08 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.5914
2022-08-10 16:05:42 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.6211
2022-08-10 16:06:16 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.7660
2022-08-10 16:06:49 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.7506
2022-08-10 16:07:22 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.8352
2022-08-10 16:07:56 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.7382
2022-08-10 16:08:29 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 2.0610
2022-08-10 16:09:03 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.6251
2022-08-10 16:09:36 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.9099
2022-08-10 16:10:09 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.6507
2022-08-10 16:10:43 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.8998
2022-08-10 16:11:16 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.1670
2022-08-10 16:11:49 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.7759
2022-08-10 16:12:23 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.8711
2022-08-10 16:12:56 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.8646
2022-08-10 16:13:30 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.6446
2022-08-10 16:14:03 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.8014
2022-08-10 16:14:37 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.6684
2022-08-10 16:15:11 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.9103
2022-08-10 16:15:45 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.6880
2022-08-10 16:16:18 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.9211
2022-08-10 16:16:52 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.6835
2022-08-10 16:17:25 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.9079
2022-08-10 16:17:58 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.8829
2022-08-10 16:17:59 - train: epoch 016, train_loss: 1.7888
2022-08-10 16:19:14 - eval: epoch: 016, acc1: 63.038%, acc5: 85.416%, test_loss: 1.5303, per_image_load_time: 2.308ms, per_image_inference_time: 0.552ms
2022-08-10 16:19:14 - until epoch: 016, best_acc1: 63.038%
2022-08-10 16:19:14 - epoch 017 lr: 0.028710
2022-08-10 16:19:53 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.6361
2022-08-10 16:20:26 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.6432
2022-08-10 16:20:59 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.8593
2022-08-10 16:21:33 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.3939
2022-08-10 16:22:07 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.8075
2022-08-10 16:22:40 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 2.0610
2022-08-10 16:23:13 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.5874
2022-08-10 16:23:45 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.6984
2022-08-10 16:24:18 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.7019
2022-08-10 16:24:51 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.6276
2022-08-10 16:25:25 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.8726
2022-08-10 16:25:58 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.7998
2022-08-10 16:26:31 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.8402
2022-08-10 16:27:04 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.7370
2022-08-10 16:27:37 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.5360
2022-08-10 16:28:11 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.5012
2022-08-10 16:28:44 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.6744
2022-08-10 16:29:18 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.7539
2022-08-10 16:29:51 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.6838
2022-08-10 16:30:25 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.7458
2022-08-10 16:30:58 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.5946
2022-08-10 16:31:31 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.6262
2022-08-10 16:32:05 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.6514
2022-08-10 16:32:38 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.6106
2022-08-10 16:33:12 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.6719
2022-08-10 16:33:45 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.7001
2022-08-10 16:34:19 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6731
2022-08-10 16:34:52 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.7600
2022-08-10 16:35:26 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.9889
2022-08-10 16:35:59 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.6105
2022-08-10 16:36:32 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8709
2022-08-10 16:37:06 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.5813
2022-08-10 16:37:39 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8514
2022-08-10 16:38:13 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.6416
2022-08-10 16:38:47 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.8986
2022-08-10 16:39:20 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.7783
2022-08-10 16:39:54 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.7323
2022-08-10 16:40:28 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.6209
2022-08-10 16:41:01 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.6165
2022-08-10 16:41:35 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.6837
2022-08-10 16:42:09 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.7633
2022-08-10 16:42:42 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.7145
2022-08-10 16:43:16 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.6985
2022-08-10 16:43:50 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8348
2022-08-10 16:44:23 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.8847
2022-08-10 16:44:57 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.6061
2022-08-10 16:45:30 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.9878
2022-08-10 16:46:03 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.7271
2022-08-10 16:46:37 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.6772
2022-08-10 16:47:09 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.5206
2022-08-10 16:47:10 - train: epoch 017, train_loss: 1.7209
2022-08-10 16:48:26 - eval: epoch: 017, acc1: 65.244%, acc5: 86.922%, test_loss: 1.4373, per_image_load_time: 2.338ms, per_image_inference_time: 0.552ms
2022-08-10 16:48:26 - until epoch: 017, best_acc1: 65.244%
2022-08-10 16:48:26 - epoch 018 lr: 0.023208
2022-08-10 16:49:05 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.6726
2022-08-10 16:49:38 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.8835
2022-08-10 16:50:11 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.7405
2022-08-10 16:50:44 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.8670
2022-08-10 16:51:18 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.6792
2022-08-10 16:51:51 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.8708
2022-08-10 16:52:24 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.6257
2022-08-10 16:52:57 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.8373
2022-08-10 16:53:30 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.7693
2022-08-10 16:54:04 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.4644
2022-08-10 16:54:37 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.7307
2022-08-10 16:55:11 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.7061
2022-08-10 16:55:44 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.8260
2022-08-10 16:56:17 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.6810
2022-08-10 16:56:51 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.6910
2022-08-10 16:57:25 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.5976
2022-08-10 16:57:59 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7559
2022-08-10 16:58:33 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.4167
2022-08-10 16:59:07 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.5421
2022-08-10 16:59:40 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.9140
2022-08-10 17:00:14 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.8936
2022-08-10 17:00:48 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.6390
2022-08-10 17:01:21 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6510
2022-08-10 17:01:55 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.6615
2022-08-10 17:02:29 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.3553
2022-08-10 17:03:02 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.6648
2022-08-10 17:03:35 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.7290
2022-08-10 17:04:08 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.6721
2022-08-10 17:04:42 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.6141
2022-08-10 17:05:15 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.7679
2022-08-10 17:05:49 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.9866
2022-08-10 17:06:22 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.5807
2022-08-10 17:06:56 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.6362
2022-08-10 17:07:29 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.7165
2022-08-10 17:08:03 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.6993
2022-08-10 17:08:37 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.6451
2022-08-10 17:09:10 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.7099
2022-08-10 17:09:44 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.7348
2022-08-10 17:10:18 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.7801
2022-08-10 17:10:51 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.8087
2022-08-10 17:11:25 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.8422
2022-08-10 17:11:59 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.6477
2022-08-10 17:12:33 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.5342
2022-08-10 17:13:06 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.5384
2022-08-10 17:13:40 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.6028
2022-08-10 17:14:14 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.5422
2022-08-10 17:14:48 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.8168
2022-08-10 17:15:22 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.7185
2022-08-10 17:15:55 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.5569
2022-08-10 17:16:28 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.6704
2022-08-10 17:16:29 - train: epoch 018, train_loss: 1.6484
2022-08-10 17:17:44 - eval: epoch: 018, acc1: 66.166%, acc5: 87.430%, test_loss: 1.3929, per_image_load_time: 2.262ms, per_image_inference_time: 0.579ms
2022-08-10 17:17:44 - until epoch: 018, best_acc1: 66.166%
2022-08-10 17:17:44 - epoch 019 lr: 0.018128
2022-08-10 17:18:25 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.3980
2022-08-10 17:18:58 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.6007
2022-08-10 17:19:32 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.5544
2022-08-10 17:20:05 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.4980
2022-08-10 17:20:37 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.6427
2022-08-10 17:21:10 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.6470
2022-08-10 17:21:44 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.6051
2022-08-10 17:22:17 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.9033
2022-08-10 17:22:50 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.6413
2022-08-10 17:23:24 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.6161
2022-08-10 17:23:57 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5489
2022-08-10 17:24:30 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.6163
2022-08-10 17:25:03 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.5606
2022-08-10 17:25:37 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.5442
2022-08-10 17:26:10 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.7080
2022-08-10 17:26:44 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.4941
2022-08-10 17:27:18 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.7099
2022-08-10 17:27:51 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.5635
2022-08-10 17:28:25 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.8107
2022-08-10 17:28:58 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.4598
2022-08-10 17:29:32 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.3838
2022-08-10 17:30:05 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.5660
2022-08-10 17:30:39 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.5773
2022-08-10 17:31:13 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.6697
2022-08-10 17:31:46 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.5986
2022-08-10 17:32:20 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.6200
2022-08-10 17:32:53 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.4384
2022-08-10 17:33:27 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.5958
2022-08-10 17:34:00 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.6066
2022-08-10 17:34:34 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.7958
2022-08-10 17:35:07 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.7128
2022-08-10 17:35:41 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.2313
2022-08-10 17:36:14 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.6951
2022-08-10 17:36:48 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.5841
2022-08-10 17:37:22 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.9386
2022-08-10 17:37:56 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.3717
2022-08-10 17:38:30 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.6925
2022-08-10 17:39:04 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.7354
2022-08-10 17:39:37 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.5687
2022-08-10 17:40:11 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.4864
2022-08-10 17:40:44 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.5700
2022-08-10 17:41:17 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.5989
2022-08-10 17:41:51 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.3953
2022-08-10 17:42:25 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.8877
2022-08-10 17:42:58 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.9199
2022-08-10 17:43:32 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.4690
2022-08-10 17:44:05 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.5811
2022-08-10 17:44:39 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.5028
2022-08-10 17:45:12 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.5666
2022-08-10 17:45:45 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.5874
2022-08-10 17:45:46 - train: epoch 019, train_loss: 1.5734
2022-08-10 17:47:01 - eval: epoch: 019, acc1: 67.814%, acc5: 88.402%, test_loss: 1.3243, per_image_load_time: 2.121ms, per_image_inference_time: 0.555ms
2022-08-10 17:47:01 - until epoch: 019, best_acc1: 67.814%
2022-08-10 17:47:01 - epoch 020 lr: 0.013551
2022-08-10 17:47:41 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.4453
2022-08-10 17:48:14 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.4005
2022-08-10 17:48:47 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.3609
2022-08-10 17:49:21 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.3917
2022-08-10 17:49:54 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.3652
2022-08-10 17:50:27 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.5271
2022-08-10 17:51:01 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.3462
2022-08-10 17:51:34 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.5831
2022-08-10 17:52:08 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.5182
2022-08-10 17:52:41 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.4600
2022-08-10 17:53:14 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.5414
2022-08-10 17:53:47 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.4911
2022-08-10 17:54:21 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.3955
2022-08-10 17:54:54 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.5226
2022-08-10 17:55:27 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.5911
2022-08-10 17:56:01 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.5790
2022-08-10 17:56:34 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.4217
2022-08-10 17:57:07 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.6763
2022-08-10 17:57:41 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.5271
2022-08-10 17:58:14 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.3496
2022-08-10 17:58:48 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.6879
2022-08-10 17:59:21 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.3769
2022-08-10 17:59:55 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.4026
2022-08-10 18:00:29 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.6975
2022-08-10 18:01:02 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.4251
2022-08-10 18:01:36 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.3998
2022-08-10 18:02:09 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.6740
2022-08-10 18:02:42 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.4846
2022-08-10 18:03:15 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.5950
2022-08-10 18:03:48 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.6541
2022-08-10 18:04:22 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.6463
2022-08-10 18:04:55 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.6617
2022-08-10 18:05:28 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3338
2022-08-10 18:06:02 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.5684
2022-08-10 18:06:35 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.4539
2022-08-10 18:07:08 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.4453
2022-08-10 18:07:42 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.5679
2022-08-10 18:08:15 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.7125
2022-08-10 18:08:48 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.6643
2022-08-10 18:09:22 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.5307
2022-08-10 18:09:55 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.5958
2022-08-10 18:10:29 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.3408
2022-08-10 18:11:02 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.5682
2022-08-10 18:11:35 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.2701
2022-08-10 18:12:09 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.5033
2022-08-10 18:12:42 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.6017
2022-08-10 18:13:15 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.3887
2022-08-10 18:13:49 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4921
2022-08-10 18:14:23 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.4212
2022-08-10 18:14:56 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.3304
2022-08-10 18:14:57 - train: epoch 020, train_loss: 1.4963
2022-08-10 18:16:12 - eval: epoch: 020, acc1: 68.830%, acc5: 88.992%, test_loss: 1.2750, per_image_load_time: 2.298ms, per_image_inference_time: 0.559ms
2022-08-10 18:16:12 - until epoch: 020, best_acc1: 68.830%
2022-08-10 18:16:12 - epoch 021 lr: 0.009548
2022-08-10 18:16:51 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.4262
2022-08-10 18:17:25 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.6024
2022-08-10 18:17:58 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.3397
2022-08-10 18:18:31 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.5080
2022-08-10 18:19:04 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.2054
2022-08-10 18:19:37 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.3140
2022-08-10 18:20:11 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3194
2022-08-10 18:20:44 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.7107
2022-08-10 18:21:18 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.4352
2022-08-10 18:21:51 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3427
2022-08-10 18:22:25 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.4061
2022-08-10 18:22:59 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.3244
2022-08-10 18:23:32 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.3616
2022-08-10 18:24:06 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.3113
2022-08-10 18:24:40 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.3466
2022-08-10 18:25:14 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3656
2022-08-10 18:25:47 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5176
2022-08-10 18:26:21 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.2842
2022-08-10 18:26:54 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.4720
2022-08-10 18:27:28 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.4615
2022-08-10 18:28:01 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3019
2022-08-10 18:28:35 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.3722
2022-08-10 18:29:08 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.3853
2022-08-10 18:29:42 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.2245
2022-08-10 18:30:15 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.3418
2022-08-10 18:30:49 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.6604
2022-08-10 18:31:23 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.4804
2022-08-10 18:31:56 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.4048
2022-08-10 18:32:30 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.2612
2022-08-10 18:33:03 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.5134
2022-08-10 18:33:36 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.3078
2022-08-10 18:34:10 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.2268
2022-08-10 18:34:44 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.4857
2022-08-10 18:35:17 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.5534
2022-08-10 18:35:51 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.6424
2022-08-10 18:36:25 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.2432
2022-08-10 18:36:59 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.3685
2022-08-10 18:37:33 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.4934
2022-08-10 18:38:07 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.4147
2022-08-10 18:38:41 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.6391
2022-08-10 18:39:15 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.4127
2022-08-10 18:39:49 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.3432
2022-08-10 18:40:23 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3580
2022-08-10 18:40:56 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.5245
2022-08-10 18:41:30 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.4614
2022-08-10 18:42:04 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.3868
2022-08-10 18:42:38 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.6922
2022-08-10 18:43:12 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.5085
2022-08-10 18:43:46 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.1906
2022-08-10 18:44:18 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.4451
2022-08-10 18:44:20 - train: epoch 021, train_loss: 1.4200
2022-08-10 18:45:35 - eval: epoch: 021, acc1: 70.050%, acc5: 89.670%, test_loss: 1.2163, per_image_load_time: 2.358ms, per_image_inference_time: 0.538ms
2022-08-10 18:45:35 - until epoch: 021, best_acc1: 70.050%
2022-08-10 18:45:35 - epoch 022 lr: 0.006184
2022-08-10 18:46:15 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.0416
2022-08-10 18:46:48 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.5448
2022-08-10 18:47:22 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.3065
2022-08-10 18:47:55 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.5815
2022-08-10 18:48:28 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3907
2022-08-10 18:49:01 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.4298
2022-08-10 18:49:34 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.3866
2022-08-10 18:50:07 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.5134
2022-08-10 18:50:41 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.4249
2022-08-10 18:51:14 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.4743
2022-08-10 18:51:48 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3728
2022-08-10 18:52:21 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.2893
2022-08-10 18:52:55 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.4855
2022-08-10 18:53:28 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.1186
2022-08-10 18:54:02 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.5496
2022-08-10 18:54:36 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.1887
2022-08-10 18:55:09 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.3686
2022-08-10 18:55:43 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.7166
2022-08-10 18:56:16 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.3915
2022-08-10 18:56:50 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4485
2022-08-10 18:57:24 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.3749
2022-08-10 18:57:57 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.2883
2022-08-10 18:58:31 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.2947
2022-08-10 18:59:04 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.4019
2022-08-10 18:59:38 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.4940
2022-08-10 19:00:11 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.2688
2022-08-10 19:00:45 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.1787
2022-08-10 19:01:18 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.3161
2022-08-10 19:01:52 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.4068
2022-08-10 19:02:25 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.3204
2022-08-10 19:02:59 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.4941
2022-08-10 19:03:33 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.2408
2022-08-10 19:04:07 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.3270
2022-08-10 19:04:41 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.2042
2022-08-10 19:05:15 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.5667
2022-08-10 19:05:49 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.3528
2022-08-10 19:06:22 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.3303
2022-08-10 19:06:56 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.5264
2022-08-10 19:07:30 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.2298
2022-08-10 19:08:04 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.4216
2022-08-10 19:08:37 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.3185
2022-08-10 19:09:11 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.2405
2022-08-10 19:09:44 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4450
2022-08-10 19:10:18 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.4820
2022-08-10 19:10:51 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.4309
2022-08-10 19:11:24 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.4522
2022-08-10 19:11:58 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.3692
2022-08-10 19:12:31 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.2434
2022-08-10 19:13:04 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.3587
2022-08-10 19:13:36 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.2137
2022-08-10 19:13:38 - train: epoch 022, train_loss: 1.3533
2022-08-10 19:14:52 - eval: epoch: 022, acc1: 70.984%, acc5: 90.108%, test_loss: 1.1743, per_image_load_time: 2.344ms, per_image_inference_time: 0.554ms
2022-08-10 19:14:53 - until epoch: 022, best_acc1: 70.984%
2022-08-10 19:14:53 - epoch 023 lr: 0.003511
2022-08-10 19:15:33 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.3352
2022-08-10 19:16:06 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.1271
2022-08-10 19:16:39 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.3981
2022-08-10 19:17:12 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.3571
2022-08-10 19:17:45 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.4605
2022-08-10 19:18:18 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.1500
2022-08-10 19:18:51 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.0729
2022-08-10 19:19:24 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3168
2022-08-10 19:19:57 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4745
2022-08-10 19:20:30 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.3573
2022-08-10 19:21:03 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.3730
2022-08-10 19:21:37 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.3856
2022-08-10 19:22:11 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.3403
2022-08-10 19:22:44 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.4232
2022-08-10 19:23:17 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.2640
2022-08-10 19:23:51 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.2970
2022-08-10 19:24:24 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.3279
2022-08-10 19:24:58 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.3070
2022-08-10 19:25:31 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.3313
2022-08-10 19:26:05 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.1725
2022-08-10 19:26:39 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.3470
2022-08-10 19:27:13 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.1403
2022-08-10 19:27:46 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.1339
2022-08-10 19:28:20 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.2370
2022-08-10 19:28:53 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.3355
2022-08-10 19:29:27 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.4571
2022-08-10 19:30:01 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.2441
2022-08-10 19:30:35 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.3191
2022-08-10 19:31:09 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.3675
2022-08-10 19:31:43 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.3334
2022-08-10 19:32:16 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.2260
2022-08-10 19:32:50 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.4009
2022-08-10 19:33:23 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.3298
2022-08-10 19:33:57 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.4424
2022-08-10 19:34:30 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.2170
2022-08-10 19:35:03 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.3131
2022-08-10 19:35:37 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.1498
2022-08-10 19:36:10 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.2631
2022-08-10 19:36:44 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.4404
2022-08-10 19:37:18 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.0454
2022-08-10 19:37:51 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.2631
2022-08-10 19:38:25 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.0630
2022-08-10 19:38:59 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.2892
2022-08-10 19:39:33 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.3553
2022-08-10 19:40:07 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.1960
2022-08-10 19:40:41 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.2858
2022-08-10 19:41:15 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.1536
2022-08-10 19:41:49 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.2362
2022-08-10 19:42:23 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.1925
2022-08-10 19:42:56 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.4673
2022-08-10 19:42:57 - train: epoch 023, train_loss: 1.2936
2022-08-10 19:44:13 - eval: epoch: 023, acc1: 71.952%, acc5: 90.578%, test_loss: 1.1383, per_image_load_time: 1.731ms, per_image_inference_time: 0.574ms
2022-08-10 19:44:13 - until epoch: 023, best_acc1: 71.952%
2022-08-10 19:44:13 - epoch 024 lr: 0.001571
2022-08-10 19:44:53 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.3655
2022-08-10 19:45:27 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.3759
2022-08-10 19:46:00 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.0233
2022-08-10 19:46:33 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.2116
2022-08-10 19:47:06 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.2135
2022-08-10 19:47:39 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.2796
2022-08-10 19:48:13 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.1888
2022-08-10 19:48:46 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.0817
2022-08-10 19:49:19 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.2987
2022-08-10 19:49:53 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.1937
2022-08-10 19:50:27 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 0.9932
2022-08-10 19:51:00 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.2698
2022-08-10 19:51:34 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.5299
2022-08-10 19:52:08 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.2887
2022-08-10 19:52:42 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.3430
2022-08-10 19:53:16 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.1915
2022-08-10 19:53:49 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.2588
2022-08-10 19:54:23 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.4287
2022-08-10 19:54:56 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.2284
2022-08-10 19:55:30 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.2783
2022-08-10 19:56:04 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.2813
2022-08-10 19:56:38 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.2131
2022-08-10 19:57:12 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.2611
2022-08-10 19:57:46 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.1411
2022-08-10 19:58:19 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.3016
2022-08-10 19:58:53 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.3263
2022-08-10 19:59:27 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.3044
2022-08-10 20:00:00 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.3017
2022-08-10 20:00:34 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.2369
2022-08-10 20:01:07 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.2410
2022-08-10 20:01:41 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.1994
2022-08-10 20:02:14 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.2731
2022-08-10 20:02:48 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.1158
2022-08-10 20:03:22 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.1953
2022-08-10 20:03:55 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.1297
2022-08-10 20:04:29 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.3821
2022-08-10 20:05:04 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.2951
2022-08-10 20:05:37 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.3653
2022-08-10 20:06:11 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.1918
2022-08-10 20:06:44 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.1890
2022-08-10 20:07:18 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.2278
2022-08-10 20:07:51 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.1835
2022-08-10 20:08:24 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.2489
2022-08-10 20:08:57 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.1514
2022-08-10 20:09:31 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.2921
2022-08-10 20:10:04 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.3574
2022-08-10 20:10:38 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.1939
2022-08-10 20:11:12 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.0077
2022-08-10 20:11:45 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.3260
2022-08-10 20:12:18 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.1754
2022-08-10 20:12:19 - train: epoch 024, train_loss: 1.2556
2022-08-10 20:13:35 - eval: epoch: 024, acc1: 72.356%, acc5: 90.798%, test_loss: 1.1227, per_image_load_time: 2.304ms, per_image_inference_time: 0.537ms
2022-08-10 20:13:35 - until epoch: 024, best_acc1: 72.356%
2022-08-10 20:13:35 - epoch 025 lr: 0.000394
2022-08-10 20:14:15 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.1212
2022-08-10 20:14:48 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1329
2022-08-10 20:15:21 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.2185
2022-08-10 20:15:54 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.3512
2022-08-10 20:16:27 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.0485
2022-08-10 20:17:00 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3277
2022-08-10 20:17:33 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.1426
2022-08-10 20:18:07 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.3568
2022-08-10 20:18:40 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0087
2022-08-10 20:19:14 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3654
2022-08-10 20:19:47 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.1615
2022-08-10 20:20:21 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.1012
2022-08-10 20:20:54 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3771
2022-08-10 20:21:28 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.2164
2022-08-10 20:22:01 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.2225
2022-08-10 20:22:35 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.0300
2022-08-10 20:23:08 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.2505
2022-08-10 20:23:43 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.1077
2022-08-10 20:24:16 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.2077
2022-08-10 20:24:50 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.2518
2022-08-10 20:25:24 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.1305
2022-08-10 20:25:58 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.0746
2022-08-10 20:26:32 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.1012
2022-08-10 20:27:05 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 0.9644
2022-08-10 20:27:39 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.1128
2022-08-10 20:28:13 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.3193
2022-08-10 20:28:46 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.2759
2022-08-10 20:29:20 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.3749
2022-08-10 20:29:53 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.2756
2022-08-10 20:30:27 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.2197
2022-08-10 20:31:01 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.1893
2022-08-10 20:31:35 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.2098
2022-08-10 20:32:09 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.1548
2022-08-10 20:32:43 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.1446
2022-08-10 20:33:17 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.0299
2022-08-10 20:33:51 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.3649
2022-08-10 20:34:24 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.2451
2022-08-10 20:34:58 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3318
2022-08-10 20:35:32 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.3355
2022-08-10 20:36:05 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.2033
2022-08-10 20:36:39 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.4314
2022-08-10 20:37:13 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.2890
2022-08-10 20:37:47 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.1566
2022-08-10 20:38:21 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.2469
2022-08-10 20:38:55 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.1808
2022-08-10 20:39:28 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.1107
2022-08-10 20:40:02 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.1786
2022-08-10 20:40:36 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.2211
2022-08-10 20:41:09 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.2809
2022-08-10 20:41:42 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.2531
2022-08-10 20:41:43 - train: epoch 025, train_loss: 1.2359
2022-08-10 20:42:58 - eval: epoch: 025, acc1: 72.398%, acc5: 90.846%, test_loss: 1.1203, per_image_load_time: 2.326ms, per_image_inference_time: 0.559ms
2022-08-10 20:42:58 - until epoch: 025, best_acc1: 72.398%
2022-08-10 20:42:58 - train done. train time: 12.196 hours, best_acc1: 72.398%
