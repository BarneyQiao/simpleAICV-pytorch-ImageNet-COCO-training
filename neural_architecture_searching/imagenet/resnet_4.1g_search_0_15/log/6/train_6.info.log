2022-08-06 20:08:51 - net_idx: 6
2022-08-06 20:08:51 - net_config: {'stem_width': 64, 'depth': 14, 'w_0': 40, 'w_a': 16.33160987858594, 'w_m': 1.809915948599379}
2022-08-06 20:08:51 - num_classes: 1000
2022-08-06 20:08:51 - input_image_size: 224
2022-08-06 20:08:51 - scale: 1.1428571428571428
2022-08-06 20:08:51 - seed: 0
2022-08-06 20:08:51 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-06 20:08:51 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-06 20:08:51 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fadba72ea00>
2022-08-06 20:08:51 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fadb9da1af0>
2022-08-06 20:08:51 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fadb9da1b20>
2022-08-06 20:08:51 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fadb9da1b80>
2022-08-06 20:08:51 - batch_size: 256
2022-08-06 20:08:51 - num_workers: 16
2022-08-06 20:08:51 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-06 20:08:51 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-06 20:08:51 - epochs: 25
2022-08-06 20:08:51 - print_interval: 100
2022-08-06 20:08:51 - accumulation_steps: 1
2022-08-06 20:08:51 - sync_bn: False
2022-08-06 20:08:51 - apex: True
2022-08-06 20:08:51 - use_ema_model: False
2022-08-06 20:08:51 - ema_model_decay: 0.9999
2022-08-06 20:08:51 - log_dir: ./log
2022-08-06 20:08:51 - checkpoint_dir: ./checkpoints
2022-08-06 20:08:51 - gpus_type: NVIDIA RTX A5000
2022-08-06 20:08:51 - gpus_num: 2
2022-08-06 20:08:51 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fad965d05f0>
2022-08-06 20:08:51 - ema_model: None
2022-08-06 20:08:51 - --------------------parameters--------------------
2022-08-06 20:08:51 - name: conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-06 20:08:51 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-06 20:08:51 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-06 20:08:51 - name: fc.weight, grad: True
2022-08-06 20:08:51 - name: fc.bias, grad: True
2022-08-06 20:08:51 - --------------------buffers--------------------
2022-08-06 20:08:51 - name: conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-06 20:08:51 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-06 20:08:51 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 20:08:51 - -----------no weight decay layers--------------
2022-08-06 20:08:51 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 20:08:51 - -------------weight decay layers---------------
2022-08-06 20:08:51 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 20:08:51 - epoch 001 lr: 0.100000
2022-08-06 20:09:30 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9082
2022-08-06 20:10:02 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9022
2022-08-06 20:10:34 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8742
2022-08-06 20:11:06 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.7627
2022-08-06 20:11:39 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.7717
2022-08-06 20:12:12 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.5285
2022-08-06 20:12:44 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.6423
2022-08-06 20:13:17 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.4466
2022-08-06 20:13:50 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.3868
2022-08-06 20:14:22 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.3736
2022-08-06 20:14:55 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.2753
2022-08-06 20:15:27 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.1194
2022-08-06 20:16:00 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.1311
2022-08-06 20:16:33 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.0726
2022-08-06 20:17:06 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 5.9597
2022-08-06 20:17:38 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 5.9630
2022-08-06 20:18:11 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.6986
2022-08-06 20:18:44 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.7168
2022-08-06 20:19:16 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.6955
2022-08-06 20:19:49 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.5236
2022-08-06 20:20:21 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.4435
2022-08-06 20:20:54 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.4242
2022-08-06 20:21:26 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.2336
2022-08-06 20:21:59 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.2500
2022-08-06 20:22:31 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.2134
2022-08-06 20:23:04 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.3858
2022-08-06 20:23:37 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.1626
2022-08-06 20:24:10 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.0422
2022-08-06 20:24:42 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.0157
2022-08-06 20:25:15 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.1335
2022-08-06 20:25:48 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.1755
2022-08-06 20:26:21 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.0948
2022-08-06 20:26:53 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.8048
2022-08-06 20:27:26 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.8559
2022-08-06 20:27:59 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.8064
2022-08-06 20:28:31 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.6417
2022-08-06 20:29:04 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.8689
2022-08-06 20:29:37 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.5793
2022-08-06 20:30:10 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.6897
2022-08-06 20:30:42 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.6170
2022-08-06 20:31:15 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.6315
2022-08-06 20:31:48 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.5526
2022-08-06 20:32:21 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.4535
2022-08-06 20:32:53 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.3434
2022-08-06 20:33:26 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.3982
2022-08-06 20:33:59 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.6898
2022-08-06 20:34:32 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.5126
2022-08-06 20:35:05 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.4581
2022-08-06 20:35:37 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.3927
2022-08-06 20:36:09 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.2572
2022-08-06 20:36:11 - train: epoch 001, train_loss: 5.4083
2022-08-06 20:37:24 - eval: epoch: 001, acc1: 17.328%, acc5: 38.774%, test_loss: 4.3869, per_image_load_time: 2.231ms, per_image_inference_time: 0.610ms
2022-08-06 20:37:24 - until epoch: 001, best_acc1: 17.328%
2022-08-06 20:37:24 - epoch 002 lr: 0.099606
2022-08-06 20:38:03 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.1730
2022-08-06 20:38:34 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.0766
2022-08-06 20:39:06 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.2074
2022-08-06 20:39:38 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.1515
2022-08-06 20:40:10 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.0716
2022-08-06 20:40:43 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.0288
2022-08-06 20:41:16 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.3582
2022-08-06 20:41:48 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 4.0298
2022-08-06 20:42:21 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.9017
2022-08-06 20:42:53 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.2106
2022-08-06 20:43:25 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 3.9419
2022-08-06 20:43:58 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.8593
2022-08-06 20:44:31 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.9852
2022-08-06 20:45:03 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.1025
2022-08-06 20:45:36 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 4.0560
2022-08-06 20:46:08 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.7598
2022-08-06 20:46:41 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.9211
2022-08-06 20:47:13 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.7937
2022-08-06 20:47:46 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.6013
2022-08-06 20:48:19 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.5907
2022-08-06 20:48:52 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.8605
2022-08-06 20:49:24 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.6989
2022-08-06 20:49:57 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.8349
2022-08-06 20:50:30 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.5339
2022-08-06 20:51:03 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.6006
2022-08-06 20:51:35 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.6964
2022-08-06 20:52:07 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.8194
2022-08-06 20:52:40 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.8732
2022-08-06 20:53:12 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.7730
2022-08-06 20:53:45 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.5040
2022-08-06 20:54:17 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.7260
2022-08-06 20:54:50 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.5860
2022-08-06 20:55:22 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.5498
2022-08-06 20:55:54 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.5455
2022-08-06 20:56:27 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.4991
2022-08-06 20:57:00 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.6642
2022-08-06 20:57:33 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.6362
2022-08-06 20:58:05 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.3584
2022-08-06 20:58:38 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.5094
2022-08-06 20:59:11 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.3990
2022-08-06 20:59:43 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.6962
2022-08-06 21:00:16 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.3651
2022-08-06 21:00:49 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.3187
2022-08-06 21:01:21 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.3128
2022-08-06 21:01:54 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.2721
2022-08-06 21:02:27 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.3578
2022-08-06 21:02:59 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.3853
2022-08-06 21:03:32 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.3819
2022-08-06 21:04:05 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.3225
2022-08-06 21:04:37 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.4664
2022-08-06 21:04:39 - train: epoch 002, train_loss: 3.7272
2022-08-06 21:05:51 - eval: epoch: 002, acc1: 27.162%, acc5: 53.230%, test_loss: 3.5090, per_image_load_time: 1.824ms, per_image_inference_time: 0.616ms
2022-08-06 21:05:51 - until epoch: 002, best_acc1: 27.162%
2022-08-06 21:05:51 - epoch 003 lr: 0.098429
2022-08-06 21:06:30 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.1143
2022-08-06 21:07:02 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.2851
2022-08-06 21:07:34 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.2885
2022-08-06 21:08:06 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.1470
2022-08-06 21:08:38 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.4213
2022-08-06 21:09:10 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.0869
2022-08-06 21:09:43 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.4911
2022-08-06 21:10:15 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.4653
2022-08-06 21:10:47 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.2213
2022-08-06 21:11:20 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.2807
2022-08-06 21:11:52 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.0728
2022-08-06 21:12:25 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.1655
2022-08-06 21:12:58 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.0988
2022-08-06 21:13:30 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.0794
2022-08-06 21:14:03 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.2054
2022-08-06 21:14:36 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.0524
2022-08-06 21:15:08 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.1818
2022-08-06 21:15:41 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.0484
2022-08-06 21:16:14 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.1054
2022-08-06 21:16:47 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.0072
2022-08-06 21:17:20 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.2740
2022-08-06 21:17:52 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.5322
2022-08-06 21:18:25 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 3.0002
2022-08-06 21:18:57 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 2.9669
2022-08-06 21:19:30 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.1846
2022-08-06 21:20:02 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.1451
2022-08-06 21:20:35 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.2222
2022-08-06 21:21:08 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.0714
2022-08-06 21:21:41 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.9927
2022-08-06 21:22:14 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.0690
2022-08-06 21:22:46 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.2972
2022-08-06 21:23:19 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.2115
2022-08-06 21:23:52 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 2.9400
2022-08-06 21:24:24 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.1801
2022-08-06 21:24:57 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.9584
2022-08-06 21:25:30 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.9075
2022-08-06 21:26:03 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.0790
2022-08-06 21:26:36 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.2770
2022-08-06 21:27:09 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.3167
2022-08-06 21:27:42 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7459
2022-08-06 21:28:14 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.9311
2022-08-06 21:28:47 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.0571
2022-08-06 21:29:20 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.9159
2022-08-06 21:29:53 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.8562
2022-08-06 21:30:25 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 2.9319
2022-08-06 21:30:58 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.9023
2022-08-06 21:31:30 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 3.0271
2022-08-06 21:32:03 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.1902
2022-08-06 21:32:36 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.2294
2022-08-06 21:33:08 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 3.0894
2022-08-06 21:33:09 - train: epoch 003, train_loss: 3.1330
2022-08-06 21:34:22 - eval: epoch: 003, acc1: 37.258%, acc5: 63.652%, test_loss: 2.9447, per_image_load_time: 1.767ms, per_image_inference_time: 0.601ms
2022-08-06 21:34:22 - until epoch: 003, best_acc1: 37.258%
2022-08-06 21:34:22 - epoch 004 lr: 0.096488
2022-08-06 21:35:01 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.8205
2022-08-06 21:35:33 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.8753
2022-08-06 21:36:05 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.9661
2022-08-06 21:36:37 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.7910
2022-08-06 21:37:10 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.7319
2022-08-06 21:37:42 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.2936
2022-08-06 21:38:15 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.9106
2022-08-06 21:38:48 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.7885
2022-08-06 21:39:20 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.7821
2022-08-06 21:39:53 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.9317
2022-08-06 21:40:26 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 2.9477
2022-08-06 21:40:59 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.5129
2022-08-06 21:41:31 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.9014
2022-08-06 21:42:04 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.7425
2022-08-06 21:42:37 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.8486
2022-08-06 21:43:10 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.8862
2022-08-06 21:43:43 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.9509
2022-08-06 21:44:15 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 3.0741
2022-08-06 21:44:49 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.8029
2022-08-06 21:45:22 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.7827
2022-08-06 21:45:54 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.9811
2022-08-06 21:46:27 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.7066
2022-08-06 21:47:00 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.4787
2022-08-06 21:47:33 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.8255
2022-08-06 21:48:06 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.6580
2022-08-06 21:48:39 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.8202
2022-08-06 21:49:11 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.6331
2022-08-06 21:49:44 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8341
2022-08-06 21:50:16 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8980
2022-08-06 21:50:50 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.8668
2022-08-06 21:51:22 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.8097
2022-08-06 21:51:55 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.7448
2022-08-06 21:52:28 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8338
2022-08-06 21:53:00 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.8238
2022-08-06 21:53:33 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.8267
2022-08-06 21:54:06 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.7862
2022-08-06 21:54:39 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.7064
2022-08-06 21:55:11 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.6864
2022-08-06 21:55:44 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.6606
2022-08-06 21:56:17 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.7719
2022-08-06 21:56:50 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.7181
2022-08-06 21:57:23 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6938
2022-08-06 21:57:56 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.7573
2022-08-06 21:58:28 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.6866
2022-08-06 21:59:01 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.2704
2022-08-06 21:59:34 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.8012
2022-08-06 22:00:07 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.7727
2022-08-06 22:00:40 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.6491
2022-08-06 22:01:13 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.7216
2022-08-06 22:01:45 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.8596
2022-08-06 22:01:46 - train: epoch 004, train_loss: 2.8306
2022-08-06 22:02:58 - eval: epoch: 004, acc1: 44.820%, acc5: 71.248%, test_loss: 2.4515, per_image_load_time: 2.203ms, per_image_inference_time: 0.597ms
2022-08-06 22:02:58 - until epoch: 004, best_acc1: 44.820%
2022-08-06 22:02:58 - epoch 005 lr: 0.093815
2022-08-06 22:03:37 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.8278
2022-08-06 22:04:08 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.6690
2022-08-06 22:04:40 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.9272
2022-08-06 22:05:12 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.6810
2022-08-06 22:05:44 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.3981
2022-08-06 22:06:17 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.8348
2022-08-06 22:06:50 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.5667
2022-08-06 22:07:22 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.8022
2022-08-06 22:07:54 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.7339
2022-08-06 22:08:27 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.7302
2022-08-06 22:09:00 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.7436
2022-08-06 22:09:32 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.8415
2022-08-06 22:10:05 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.4559
2022-08-06 22:10:37 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.5911
2022-08-06 22:11:10 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.5736
2022-08-06 22:11:42 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.3857
2022-08-06 22:12:15 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.4854
2022-08-06 22:12:48 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.6394
2022-08-06 22:13:20 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.6564
2022-08-06 22:13:53 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.5723
2022-08-06 22:14:26 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.5904
2022-08-06 22:14:59 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.5398
2022-08-06 22:15:32 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.4362
2022-08-06 22:16:04 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.6364
2022-08-06 22:16:37 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.5418
2022-08-06 22:17:09 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.7945
2022-08-06 22:17:42 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.6039
2022-08-06 22:18:15 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.6830
2022-08-06 22:18:47 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.3616
2022-08-06 22:19:19 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.4948
2022-08-06 22:19:52 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.8755
2022-08-06 22:20:25 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.5301
2022-08-06 22:20:58 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.3860
2022-08-06 22:21:30 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.5416
2022-08-06 22:22:03 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.6166
2022-08-06 22:22:36 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6539
2022-08-06 22:23:08 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.4987
2022-08-06 22:23:41 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5086
2022-08-06 22:24:13 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.7583
2022-08-06 22:24:46 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.4558
2022-08-06 22:25:19 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.5975
2022-08-06 22:25:52 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.6439
2022-08-06 22:26:24 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.7838
2022-08-06 22:26:58 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.5319
2022-08-06 22:27:31 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.3909
2022-08-06 22:28:03 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.8795
2022-08-06 22:28:36 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.2977
2022-08-06 22:29:09 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.5409
2022-08-06 22:29:41 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.7006
2022-08-06 22:30:13 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.4780
2022-08-06 22:30:15 - train: epoch 005, train_loss: 2.6354
2022-08-06 22:31:28 - eval: epoch: 005, acc1: 47.588%, acc5: 73.840%, test_loss: 2.3020, per_image_load_time: 2.082ms, per_image_inference_time: 0.618ms
2022-08-06 22:31:28 - until epoch: 005, best_acc1: 47.588%
2022-08-09 22:49:21 - epoch 006 lr: 0.090450
2022-08-09 22:50:01 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.4668
2022-08-09 22:50:33 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.5158
2022-08-09 22:51:05 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.6888
2022-08-09 22:51:38 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.5569
2022-08-09 22:52:11 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.4734
2022-08-09 22:52:44 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.6427
2022-08-09 22:53:17 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.6414
2022-08-09 22:53:50 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.5264
2022-08-09 22:54:24 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.4959
2022-08-09 22:54:57 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.3949
2022-08-09 22:55:30 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.5153
2022-08-09 22:56:04 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.6320
2022-08-09 22:56:37 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.3562
2022-08-09 22:57:11 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.6484
2022-08-09 22:57:45 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.6968
2022-08-09 22:58:18 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3523
2022-08-09 22:58:52 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.5533
2022-08-09 22:59:25 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.5957
2022-08-09 22:59:58 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.3989
2022-08-09 23:00:32 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.6697
2022-08-09 23:01:05 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.5326
2022-08-09 23:01:38 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.3296
2022-08-09 23:02:12 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.5037
2022-08-09 23:02:45 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.4453
2022-08-09 23:03:19 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4254
2022-08-09 23:03:52 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.3421
2022-08-09 23:04:26 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.3649
2022-08-09 23:05:00 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.1638
2022-08-09 23:05:33 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.5871
2022-08-09 23:06:07 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.5065
2022-08-09 23:06:40 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.3236
2022-08-09 23:07:14 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.4982
2022-08-09 23:07:47 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.5905
2022-08-09 23:08:21 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.9468
2022-08-09 23:08:55 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.9409
2022-08-09 23:09:28 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.2794
2022-08-09 23:10:01 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.6684
2022-08-09 23:10:35 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.2535
2022-08-09 23:11:09 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.5074
2022-08-09 23:11:42 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.7564
2022-08-09 23:12:15 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.5715
2022-08-09 23:12:49 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.2817
2022-08-09 23:13:22 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.2215
2022-08-09 23:13:56 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.5257
2022-08-09 23:14:29 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.5196
2022-08-09 23:15:02 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.6432
2022-08-09 23:15:36 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4576
2022-08-09 23:16:09 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.5284
2022-08-09 23:16:43 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.3804
2022-08-09 23:17:16 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.3527
2022-08-09 23:17:17 - train: epoch 006, train_loss: 2.5065
2022-08-09 23:18:33 - eval: epoch: 006, acc1: 49.172%, acc5: 75.020%, test_loss: 2.2297, per_image_load_time: 2.191ms, per_image_inference_time: 0.584ms
2022-08-09 23:18:33 - until epoch: 006, best_acc1: 49.172%
2022-08-09 23:18:33 - epoch 007 lr: 0.086448
2022-08-09 23:19:13 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.4267
2022-08-09 23:19:46 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.7153
2022-08-09 23:20:19 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.6034
2022-08-09 23:20:52 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.4852
2022-08-09 23:21:25 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.2997
2022-08-09 23:21:58 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.5427
2022-08-09 23:22:32 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.3796
2022-08-09 23:23:05 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.5699
2022-08-09 23:23:38 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.4478
2022-08-09 23:24:12 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.4332
2022-08-09 23:24:45 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.4751
2022-08-09 23:25:18 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.2995
2022-08-09 23:25:51 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.1090
2022-08-09 23:26:25 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.5159
2022-08-09 23:26:58 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.5205
2022-08-09 23:27:32 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.4780
2022-08-09 23:28:05 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.3776
2022-08-09 23:28:39 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.5371
2022-08-09 23:29:12 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.6767
2022-08-09 23:29:46 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.2821
2022-08-09 23:30:20 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.5715
2022-08-09 23:30:53 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.4127
2022-08-09 23:31:27 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.2814
2022-08-09 23:32:00 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.4953
2022-08-09 23:32:34 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.3442
2022-08-09 23:33:07 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.3596
2022-08-09 23:33:41 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.3612
2022-08-09 23:34:14 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.5182
2022-08-09 23:34:48 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.2172
2022-08-09 23:35:21 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.5947
2022-08-09 23:35:55 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.3939
2022-08-09 23:36:29 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.3463
2022-08-09 23:37:02 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.5202
2022-08-09 23:37:35 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.4021
2022-08-09 23:38:09 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.4862
2022-08-09 23:38:42 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.1677
2022-08-09 23:39:15 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.2673
2022-08-09 23:39:49 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.2820
2022-08-09 23:40:22 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.4585
2022-08-09 23:40:55 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.5917
2022-08-09 23:41:28 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.4151
2022-08-09 23:42:02 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.3428
2022-08-09 23:42:35 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.4448
2022-08-09 23:43:09 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.3432
2022-08-09 23:43:42 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.5416
2022-08-09 23:44:16 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.5869
2022-08-09 23:44:49 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.1519
2022-08-09 23:45:23 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.3343
2022-08-09 23:45:56 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.3633
2022-08-09 23:46:29 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.3158
2022-08-09 23:46:30 - train: epoch 007, train_loss: 2.4058
2022-08-09 23:47:45 - eval: epoch: 007, acc1: 50.728%, acc5: 76.042%, test_loss: 2.1447, per_image_load_time: 1.622ms, per_image_inference_time: 0.588ms
2022-08-09 23:47:45 - until epoch: 007, best_acc1: 50.728%
2022-08-09 23:47:45 - epoch 008 lr: 0.081870
2022-08-09 23:48:25 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.5386
2022-08-09 23:48:57 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.2703
2022-08-09 23:49:30 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.3792
2022-08-09 23:50:03 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.2128
2022-08-09 23:50:36 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.2852
2022-08-09 23:51:09 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.2857
2022-08-09 23:51:42 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.4952
2022-08-09 23:52:14 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.1953
2022-08-09 23:52:47 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.2833
2022-08-09 23:53:20 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.2225
2022-08-09 23:53:53 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.4404
2022-08-09 23:54:26 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.2204
2022-08-09 23:55:00 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.4441
2022-08-09 23:55:33 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.4086
2022-08-09 23:56:06 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.3093
2022-08-09 23:56:40 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.4122
2022-08-09 23:57:13 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.2971
2022-08-09 23:57:46 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.4933
2022-08-09 23:58:19 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.2719
2022-08-09 23:58:52 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.4389
2022-08-09 23:59:26 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.4470
2022-08-09 23:59:59 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.1918
2022-08-10 00:00:33 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.3339
2022-08-10 00:01:06 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.2647
2022-08-10 00:01:39 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.4164
2022-08-10 00:02:13 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.4315
2022-08-10 00:02:46 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.3835
2022-08-10 00:03:19 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.4446
2022-08-10 00:03:53 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.3057
2022-08-10 00:04:26 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.3958
2022-08-10 00:04:59 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.2783
2022-08-10 00:05:33 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.4533
2022-08-10 00:06:06 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.4495
2022-08-10 00:06:40 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.2830
2022-08-10 00:07:13 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.3929
2022-08-10 00:07:47 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.3716
2022-08-10 00:08:20 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.0702
2022-08-10 00:08:53 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.2507
2022-08-10 00:09:27 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.2888
2022-08-10 00:10:00 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.5433
2022-08-10 00:10:34 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 1.9876
2022-08-10 00:11:07 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.2360
2022-08-10 00:11:40 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.0609
2022-08-10 00:12:13 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.1954
2022-08-10 00:12:47 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.3090
2022-08-10 00:13:20 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.2478
2022-08-10 00:13:53 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.5451
2022-08-10 00:14:26 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.3439
2022-08-10 00:14:59 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.4124
2022-08-10 00:15:32 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.2660
2022-08-10 00:15:33 - train: epoch 008, train_loss: 2.3229
2022-08-10 00:16:48 - eval: epoch: 008, acc1: 53.616%, acc5: 78.876%, test_loss: 1.9792, per_image_load_time: 2.286ms, per_image_inference_time: 0.604ms
2022-08-10 00:16:48 - until epoch: 008, best_acc1: 53.616%
2022-08-10 00:16:48 - epoch 009 lr: 0.076790
2022-08-10 00:17:28 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 2.0640
2022-08-10 00:18:00 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.2139
2022-08-10 00:18:32 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.1298
2022-08-10 00:19:06 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.5176
2022-08-10 00:19:38 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.2240
2022-08-10 00:20:11 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.0326
2022-08-10 00:20:45 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.0387
2022-08-10 00:21:18 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.2237
2022-08-10 00:21:50 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 1.9044
2022-08-10 00:22:23 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.2346
2022-08-10 00:22:56 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.4721
2022-08-10 00:23:29 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.3576
2022-08-10 00:24:02 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.5638
2022-08-10 00:24:36 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 2.1951
2022-08-10 00:25:09 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.0253
2022-08-10 00:25:42 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2970
2022-08-10 00:26:15 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.3646
2022-08-10 00:26:49 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.2198
2022-08-10 00:27:22 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.0392
2022-08-10 00:27:55 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 1.9718
2022-08-10 00:28:29 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.3438
2022-08-10 00:29:02 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.3292
2022-08-10 00:29:35 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.0910
2022-08-10 00:30:08 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.2750
2022-08-10 00:30:42 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.1321
2022-08-10 00:31:15 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.2578
2022-08-10 00:31:48 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.0846
2022-08-10 00:32:21 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.3930
2022-08-10 00:32:54 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.0054
2022-08-10 00:33:27 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.2559
2022-08-10 00:34:00 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.2942
2022-08-10 00:34:33 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.0672
2022-08-10 00:35:07 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.1583
2022-08-10 00:35:40 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.3756
2022-08-10 00:36:13 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.3744
2022-08-10 00:36:46 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.0189
2022-08-10 00:37:20 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.2481
2022-08-10 00:37:53 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.4651
2022-08-10 00:38:26 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.0744
2022-08-10 00:39:00 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.5967
2022-08-10 00:39:33 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.3137
2022-08-10 00:40:07 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.1067
2022-08-10 00:40:40 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.0060
2022-08-10 00:41:14 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.2774
2022-08-10 00:41:48 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.2446
2022-08-10 00:42:21 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.4871
2022-08-10 00:42:54 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.3438
2022-08-10 00:43:28 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.1298
2022-08-10 00:44:01 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.4963
2022-08-10 00:44:33 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.2035
2022-08-10 00:44:35 - train: epoch 009, train_loss: 2.2598
2022-08-10 00:45:49 - eval: epoch: 009, acc1: 53.562%, acc5: 79.126%, test_loss: 1.9815, per_image_load_time: 1.460ms, per_image_inference_time: 0.608ms
2022-08-10 00:45:49 - until epoch: 009, best_acc1: 53.616%
2022-08-10 00:45:49 - epoch 010 lr: 0.071288
2022-08-10 00:46:28 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 1.9921
2022-08-10 00:47:01 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.2944
2022-08-10 00:47:34 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.2477
2022-08-10 00:48:07 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.4032
2022-08-10 00:48:40 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.1719
2022-08-10 00:49:13 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.3215
2022-08-10 00:49:46 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.1678
2022-08-10 00:50:19 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.2211
2022-08-10 00:50:52 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.1777
2022-08-10 00:51:25 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.1847
2022-08-10 00:51:59 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.1073
2022-08-10 00:52:32 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.0083
2022-08-10 00:53:06 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 1.9489
2022-08-10 00:53:39 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.1193
2022-08-10 00:54:12 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 2.0128
2022-08-10 00:54:46 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.1751
2022-08-10 00:55:19 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.2830
2022-08-10 00:55:52 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 2.1345
2022-08-10 00:56:25 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.2078
2022-08-10 00:56:58 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.1214
2022-08-10 00:57:31 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 1.9758
2022-08-10 00:58:04 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.4289
2022-08-10 00:58:37 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.1988
2022-08-10 00:59:10 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.3743
2022-08-10 00:59:44 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.1746
2022-08-10 01:00:17 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.4274
2022-08-10 01:00:51 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 2.1070
2022-08-10 01:01:24 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.4013
2022-08-10 01:01:58 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.3560
2022-08-10 01:02:31 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.0603
2022-08-10 01:03:04 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.4555
2022-08-10 01:03:38 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.0525
2022-08-10 01:04:11 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.2538
2022-08-10 01:04:45 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.2637
2022-08-10 01:05:18 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.3889
2022-08-10 01:05:52 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.2677
2022-08-10 01:06:25 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.1456
2022-08-10 01:06:58 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1265
2022-08-10 01:07:32 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 2.0332
2022-08-10 01:08:05 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.0672
2022-08-10 01:08:39 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.1102
2022-08-10 01:09:12 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.3582
2022-08-10 01:09:45 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.1582
2022-08-10 01:10:18 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.0385
2022-08-10 01:10:52 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.3270
2022-08-10 01:11:25 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.1143
2022-08-10 01:11:59 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.2005
2022-08-10 01:12:32 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.0978
2022-08-10 01:13:06 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.1916
2022-08-10 01:13:38 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.0116
2022-08-10 01:13:39 - train: epoch 010, train_loss: 2.1928
2022-08-10 01:14:54 - eval: epoch: 010, acc1: 55.370%, acc5: 80.512%, test_loss: 1.8925, per_image_load_time: 2.324ms, per_image_inference_time: 0.604ms
2022-08-10 01:14:55 - until epoch: 010, best_acc1: 55.370%
2022-08-10 01:14:55 - epoch 011 lr: 0.065450
2022-08-10 01:15:34 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 1.9954
2022-08-10 01:16:07 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.0843
2022-08-10 01:16:40 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.2296
2022-08-10 01:17:12 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.2286
2022-08-10 01:17:45 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.1160
2022-08-10 01:18:18 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.3714
2022-08-10 01:18:51 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.3363
2022-08-10 01:19:24 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.1328
2022-08-10 01:19:57 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.3057
2022-08-10 01:20:30 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.1291
2022-08-10 01:21:03 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2975
2022-08-10 01:21:36 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2988
2022-08-10 01:22:09 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.2995
2022-08-10 01:22:42 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.0143
2022-08-10 01:23:16 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 1.9331
2022-08-10 01:23:49 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.3356
2022-08-10 01:24:22 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.2566
2022-08-10 01:24:56 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 1.9956
2022-08-10 01:25:29 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 1.9435
2022-08-10 01:26:02 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1039
2022-08-10 01:26:36 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.1687
2022-08-10 01:27:09 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 2.0637
2022-08-10 01:27:42 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.2896
2022-08-10 01:28:16 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.9775
2022-08-10 01:28:49 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.0349
2022-08-10 01:29:23 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.0615
2022-08-10 01:29:56 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.1654
2022-08-10 01:30:30 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.7900
2022-08-10 01:31:03 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.2446
2022-08-10 01:31:36 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.3770
2022-08-10 01:32:09 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.1822
2022-08-10 01:32:43 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.1609
2022-08-10 01:33:16 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.2604
2022-08-10 01:33:50 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 1.9559
2022-08-10 01:34:23 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 1.9564
2022-08-10 01:34:56 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 1.9550
2022-08-10 01:35:30 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.1919
2022-08-10 01:36:03 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.9721
2022-08-10 01:36:36 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.1537
2022-08-10 01:37:10 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.1628
2022-08-10 01:37:43 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.8830
2022-08-10 01:38:17 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 1.9905
2022-08-10 01:38:51 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.0916
2022-08-10 01:39:24 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.2665
2022-08-10 01:39:58 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 1.9180
2022-08-10 01:40:32 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 2.0035
2022-08-10 01:41:05 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.8771
2022-08-10 01:41:39 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.8398
2022-08-10 01:42:12 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.1774
2022-08-10 01:42:45 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 1.9039
2022-08-10 01:42:46 - train: epoch 011, train_loss: 2.1322
2022-08-10 01:44:01 - eval: epoch: 011, acc1: 55.878%, acc5: 80.530%, test_loss: 1.8722, per_image_load_time: 1.905ms, per_image_inference_time: 0.597ms
2022-08-10 01:44:01 - until epoch: 011, best_acc1: 55.878%
2022-08-10 01:44:01 - epoch 012 lr: 0.059368
2022-08-10 01:44:40 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.7968
2022-08-10 01:45:13 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.8661
2022-08-10 01:45:45 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 1.9545
2022-08-10 01:46:18 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.0561
2022-08-10 01:46:52 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.0988
2022-08-10 01:47:25 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.8990
2022-08-10 01:47:58 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.7274
2022-08-10 01:48:31 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.1501
2022-08-10 01:49:04 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.1270
2022-08-10 01:49:38 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9638
2022-08-10 01:50:11 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.4289
2022-08-10 01:50:44 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 2.0375
2022-08-10 01:51:17 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 2.1554
2022-08-10 01:51:51 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.1245
2022-08-10 01:52:24 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 1.9693
2022-08-10 01:52:57 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 2.0653
2022-08-10 01:53:30 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.9247
2022-08-10 01:54:03 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.1087
2022-08-10 01:54:36 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1144
2022-08-10 01:55:09 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.3330
2022-08-10 01:55:43 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.1611
2022-08-10 01:56:16 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.0870
2022-08-10 01:56:49 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 2.1225
2022-08-10 01:57:22 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.2848
2022-08-10 01:57:55 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.8414
2022-08-10 01:58:28 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.9764
2022-08-10 01:59:01 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.1611
2022-08-10 01:59:35 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.9877
2022-08-10 02:00:08 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.1513
2022-08-10 02:00:41 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 2.0897
2022-08-10 02:01:15 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 1.9608
2022-08-10 02:01:49 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.9416
2022-08-10 02:02:22 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.2033
2022-08-10 02:02:56 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.0814
2022-08-10 02:03:30 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 1.9661
2022-08-10 02:04:03 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.9317
2022-08-10 02:04:36 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 1.9903
2022-08-10 02:05:09 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.3214
2022-08-10 02:05:42 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.9083
2022-08-10 02:06:16 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1395
2022-08-10 02:06:49 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.0704
2022-08-10 02:07:22 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 2.0645
2022-08-10 02:07:56 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.0344
2022-08-10 02:08:29 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.8220
2022-08-10 02:09:03 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.9356
2022-08-10 02:09:36 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 1.9766
2022-08-10 02:10:10 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.9371
2022-08-10 02:10:43 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.0736
2022-08-10 02:11:16 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.8685
2022-08-10 02:11:49 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.9369
2022-08-10 02:11:50 - train: epoch 012, train_loss: 2.0726
2022-08-10 02:13:05 - eval: epoch: 012, acc1: 58.848%, acc5: 82.500%, test_loss: 1.7480, per_image_load_time: 1.984ms, per_image_inference_time: 0.602ms
2022-08-10 02:13:05 - until epoch: 012, best_acc1: 58.848%
2022-08-10 02:13:05 - epoch 013 lr: 0.053138
2022-08-10 02:13:44 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.9027
2022-08-10 02:14:17 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 2.1123
2022-08-10 02:14:51 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8766
2022-08-10 02:15:23 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.7522
2022-08-10 02:15:56 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 2.0180
2022-08-10 02:16:29 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 2.0174
2022-08-10 02:17:02 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.9890
2022-08-10 02:17:35 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.1865
2022-08-10 02:18:08 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 2.0867
2022-08-10 02:18:41 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.9926
2022-08-10 02:19:14 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.1395
2022-08-10 02:19:47 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.2061
2022-08-10 02:20:20 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.6866
2022-08-10 02:20:53 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.1344
2022-08-10 02:21:26 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 1.9476
2022-08-10 02:21:59 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.9243
2022-08-10 02:22:32 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.0053
2022-08-10 02:23:05 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9914
2022-08-10 02:23:38 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.0296
2022-08-10 02:24:11 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.1507
2022-08-10 02:24:45 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.3528
2022-08-10 02:25:18 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.8677
2022-08-10 02:25:51 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 2.0716
2022-08-10 02:26:24 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.0876
2022-08-10 02:26:57 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.9492
2022-08-10 02:27:31 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.8332
2022-08-10 02:28:04 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.9240
2022-08-10 02:28:37 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 1.9866
2022-08-10 02:29:10 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.2115
2022-08-10 02:29:42 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.7932
2022-08-10 02:30:16 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 2.0917
2022-08-10 02:30:49 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 2.1220
2022-08-10 02:31:22 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.8660
2022-08-10 02:31:56 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.9788
2022-08-10 02:32:28 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.8582
2022-08-10 02:33:01 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.1870
2022-08-10 02:33:34 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.6790
2022-08-10 02:34:08 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.2601
2022-08-10 02:34:40 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.1217
2022-08-10 02:35:14 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.9007
2022-08-10 02:35:47 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.8912
2022-08-10 02:36:20 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 2.0094
2022-08-10 02:36:54 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 2.0268
2022-08-10 02:37:27 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.9317
2022-08-10 02:38:00 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 2.1003
2022-08-10 02:38:34 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 1.9263
2022-08-10 02:39:07 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.8586
2022-08-10 02:39:40 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.1067
2022-08-10 02:40:14 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0769
2022-08-10 02:40:46 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.1814
2022-08-10 02:40:48 - train: epoch 013, train_loss: 2.0133
2022-08-10 02:42:02 - eval: epoch: 013, acc1: 59.848%, acc5: 83.306%, test_loss: 1.6844, per_image_load_time: 2.298ms, per_image_inference_time: 0.596ms
2022-08-10 02:42:02 - until epoch: 013, best_acc1: 59.848%
2022-08-10 02:42:02 - epoch 014 lr: 0.046859
2022-08-10 02:42:41 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.7489
2022-08-10 02:43:14 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.8709
2022-08-10 02:43:46 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.8248
2022-08-10 02:44:19 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.8987
2022-08-10 02:44:51 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.8800
2022-08-10 02:45:24 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.8761
2022-08-10 02:45:56 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.7004
2022-08-10 02:46:29 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.9126
2022-08-10 02:47:02 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.9159
2022-08-10 02:47:35 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 1.9954
2022-08-10 02:48:07 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.9706
2022-08-10 02:48:40 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 1.9379
2022-08-10 02:49:13 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 1.9548
2022-08-10 02:49:46 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.8705
2022-08-10 02:50:19 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.0015
2022-08-10 02:50:52 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.8880
2022-08-10 02:51:25 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9254
2022-08-10 02:51:59 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.0142
2022-08-10 02:52:32 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.7639
2022-08-10 02:53:05 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.9593
2022-08-10 02:53:38 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.0184
2022-08-10 02:54:11 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 2.1891
2022-08-10 02:54:44 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.9500
2022-08-10 02:55:17 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 2.1426
2022-08-10 02:55:50 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 2.1778
2022-08-10 02:56:23 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8096
2022-08-10 02:56:56 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.8114
2022-08-10 02:57:29 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 2.0655
2022-08-10 02:58:02 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9641
2022-08-10 02:58:35 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.8488
2022-08-10 02:59:08 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 2.0719
2022-08-10 02:59:41 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 2.0963
2022-08-10 03:00:15 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.8569
2022-08-10 03:00:48 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.9326
2022-08-10 03:01:21 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.9545
2022-08-10 03:01:54 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.8313
2022-08-10 03:02:27 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.8658
2022-08-10 03:03:00 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 1.9819
2022-08-10 03:03:34 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.9846
2022-08-10 03:04:07 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 2.1254
2022-08-10 03:04:41 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.8992
2022-08-10 03:05:14 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.8896
2022-08-10 03:05:48 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.7743
2022-08-10 03:06:21 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.8790
2022-08-10 03:06:54 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.9001
2022-08-10 03:07:27 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.8601
2022-08-10 03:08:01 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.8773
2022-08-10 03:08:34 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.9394
2022-08-10 03:09:07 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8442
2022-08-10 03:09:40 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 2.1871
2022-08-10 03:09:41 - train: epoch 014, train_loss: 1.9514
2022-08-10 03:10:56 - eval: epoch: 014, acc1: 60.660%, acc5: 83.902%, test_loss: 1.6520, per_image_load_time: 2.068ms, per_image_inference_time: 0.601ms
2022-08-10 03:10:56 - until epoch: 014, best_acc1: 60.660%
2022-08-10 03:10:56 - epoch 015 lr: 0.040630
2022-08-10 03:11:36 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.6108
2022-08-10 03:12:08 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 2.1003
2022-08-10 03:12:41 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.0984
2022-08-10 03:13:13 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.9174
2022-08-10 03:13:46 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.7452
2022-08-10 03:14:19 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 2.0074
2022-08-10 03:14:52 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 2.0692
2022-08-10 03:15:25 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.7082
2022-08-10 03:15:58 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.9261
2022-08-10 03:16:31 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.8575
2022-08-10 03:17:04 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7247
2022-08-10 03:17:37 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 2.0742
2022-08-10 03:18:11 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 2.0335
2022-08-10 03:18:44 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.8958
2022-08-10 03:19:18 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.8336
2022-08-10 03:19:51 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.8136
2022-08-10 03:20:24 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.7458
2022-08-10 03:20:57 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.6932
2022-08-10 03:21:30 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.8295
2022-08-10 03:22:03 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.7779
2022-08-10 03:22:37 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.7475
2022-08-10 03:23:10 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 2.0647
2022-08-10 03:23:43 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.9141
2022-08-10 03:24:16 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.9121
2022-08-10 03:24:49 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 2.1808
2022-08-10 03:25:23 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.7978
2022-08-10 03:25:56 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.9671
2022-08-10 03:26:29 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.8521
2022-08-10 03:27:02 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.8109
2022-08-10 03:27:35 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.8326
2022-08-10 03:28:08 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.9054
2022-08-10 03:28:42 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.9221
2022-08-10 03:29:15 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.7290
2022-08-10 03:29:48 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.7337
2022-08-10 03:30:22 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.1803
2022-08-10 03:30:55 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.8977
2022-08-10 03:31:28 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.7694
2022-08-10 03:32:02 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.8174
2022-08-10 03:32:35 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.9476
2022-08-10 03:33:09 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.8290
2022-08-10 03:33:42 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.8564
2022-08-10 03:34:15 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6653
2022-08-10 03:34:49 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.8652
2022-08-10 03:35:22 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.8895
2022-08-10 03:35:56 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.9494
2022-08-10 03:36:29 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.9218
2022-08-10 03:37:02 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.8312
2022-08-10 03:37:36 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.9376
2022-08-10 03:38:09 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.7370
2022-08-10 03:38:42 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 2.1198
2022-08-10 03:38:43 - train: epoch 015, train_loss: 1.8905
2022-08-10 03:39:57 - eval: epoch: 015, acc1: 62.472%, acc5: 85.152%, test_loss: 1.5578, per_image_load_time: 1.774ms, per_image_inference_time: 0.594ms
2022-08-10 03:39:57 - until epoch: 015, best_acc1: 62.472%
2022-08-10 03:39:57 - epoch 016 lr: 0.034548
2022-08-10 03:40:37 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.7736
2022-08-10 03:41:09 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.8328
2022-08-10 03:41:42 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.8190
2022-08-10 03:42:14 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 1.8970
2022-08-10 03:42:47 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.7673
2022-08-10 03:43:20 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.8456
2022-08-10 03:43:53 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.7686
2022-08-10 03:44:26 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.9007
2022-08-10 03:44:59 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 2.0047
2022-08-10 03:45:33 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.8134
2022-08-10 03:46:06 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.8451
2022-08-10 03:46:39 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.8108
2022-08-10 03:47:13 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.9608
2022-08-10 03:47:47 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.6861
2022-08-10 03:48:20 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 2.1883
2022-08-10 03:48:53 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.9583
2022-08-10 03:49:27 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.8664
2022-08-10 03:50:00 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.8274
2022-08-10 03:50:33 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.7664
2022-08-10 03:51:06 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.5133
2022-08-10 03:51:40 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8271
2022-08-10 03:52:13 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.8442
2022-08-10 03:52:46 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.9193
2022-08-10 03:53:19 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.8498
2022-08-10 03:53:52 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.8085
2022-08-10 03:54:25 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.8408
2022-08-10 03:54:58 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.7305
2022-08-10 03:55:32 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.7083
2022-08-10 03:56:05 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.7854
2022-08-10 03:56:38 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.7558
2022-08-10 03:57:11 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.9017
2022-08-10 03:57:45 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.8652
2022-08-10 03:58:18 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 2.0050
2022-08-10 03:58:51 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.5834
2022-08-10 03:59:25 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.9606
2022-08-10 03:59:58 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.7752
2022-08-10 04:00:32 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.8549
2022-08-10 04:01:05 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.2280
2022-08-10 04:01:38 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.9002
2022-08-10 04:02:11 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 2.0091
2022-08-10 04:02:44 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.9032
2022-08-10 04:03:18 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.7486
2022-08-10 04:03:51 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.7784
2022-08-10 04:04:25 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.5901
2022-08-10 04:04:59 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.8447
2022-08-10 04:05:32 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.6677
2022-08-10 04:06:06 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.9666
2022-08-10 04:06:40 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.7424
2022-08-10 04:07:13 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.7015
2022-08-10 04:07:46 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.6810
2022-08-10 04:07:47 - train: epoch 016, train_loss: 1.8231
2022-08-10 04:09:03 - eval: epoch: 016, acc1: 62.992%, acc5: 85.492%, test_loss: 1.5315, per_image_load_time: 1.715ms, per_image_inference_time: 0.611ms
2022-08-10 04:09:03 - until epoch: 016, best_acc1: 62.992%
2022-08-10 04:09:03 - epoch 017 lr: 0.028710
2022-08-10 04:09:43 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.6949
2022-08-10 04:10:15 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.7416
2022-08-10 04:10:48 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.9760
2022-08-10 04:11:21 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.5889
2022-08-10 04:11:55 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.7452
2022-08-10 04:12:29 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 1.9916
2022-08-10 04:13:02 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.6036
2022-08-10 04:13:36 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.8703
2022-08-10 04:14:09 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.7686
2022-08-10 04:14:43 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.9404
2022-08-10 04:15:17 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.9348
2022-08-10 04:15:50 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.8069
2022-08-10 04:16:24 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.9098
2022-08-10 04:16:57 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.8241
2022-08-10 04:17:31 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.4581
2022-08-10 04:18:04 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.7758
2022-08-10 04:18:38 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.7718
2022-08-10 04:19:11 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.6773
2022-08-10 04:19:45 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.5501
2022-08-10 04:20:19 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.7713
2022-08-10 04:20:52 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.7972
2022-08-10 04:21:26 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.5530
2022-08-10 04:22:00 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.8467
2022-08-10 04:22:33 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.8566
2022-08-10 04:23:06 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 2.0039
2022-08-10 04:23:40 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.7804
2022-08-10 04:24:13 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.7831
2022-08-10 04:24:47 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.8625
2022-08-10 04:25:21 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 2.0098
2022-08-10 04:25:54 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.7144
2022-08-10 04:26:28 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.9953
2022-08-10 04:27:02 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.6095
2022-08-10 04:27:35 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8299
2022-08-10 04:28:09 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.7813
2022-08-10 04:28:43 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.8202
2022-08-10 04:29:16 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.9312
2022-08-10 04:29:49 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.7146
2022-08-10 04:30:23 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.7003
2022-08-10 04:30:57 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.6527
2022-08-10 04:31:30 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.7923
2022-08-10 04:32:04 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.7857
2022-08-10 04:32:37 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.7042
2022-08-10 04:33:11 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.6404
2022-08-10 04:33:45 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.7418
2022-08-10 04:34:19 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.7706
2022-08-10 04:34:52 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.6766
2022-08-10 04:35:26 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.9594
2022-08-10 04:35:59 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.7895
2022-08-10 04:36:33 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.5800
2022-08-10 04:37:05 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.5696
2022-08-10 04:37:07 - train: epoch 017, train_loss: 1.7578
2022-08-10 04:38:22 - eval: epoch: 017, acc1: 64.300%, acc5: 86.256%, test_loss: 1.4809, per_image_load_time: 1.448ms, per_image_inference_time: 0.615ms
2022-08-10 04:38:22 - until epoch: 017, best_acc1: 64.300%
2022-08-10 04:38:22 - epoch 018 lr: 0.023208
2022-08-10 04:39:02 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.6038
2022-08-10 04:39:34 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.8363
2022-08-10 04:40:07 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.8708
2022-08-10 04:40:39 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.8154
2022-08-10 04:41:12 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.4882
2022-08-10 04:41:46 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.7893
2022-08-10 04:42:19 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.5671
2022-08-10 04:42:52 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.6295
2022-08-10 04:43:25 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.5750
2022-08-10 04:43:59 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.5090
2022-08-10 04:44:32 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.8267
2022-08-10 04:45:06 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.7392
2022-08-10 04:45:40 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.7688
2022-08-10 04:46:14 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.7823
2022-08-10 04:46:47 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.8699
2022-08-10 04:47:21 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.6352
2022-08-10 04:47:55 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7379
2022-08-10 04:48:28 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6563
2022-08-10 04:49:02 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.6192
2022-08-10 04:49:36 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.7720
2022-08-10 04:50:09 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.8111
2022-08-10 04:50:43 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.8803
2022-08-10 04:51:16 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6751
2022-08-10 04:51:50 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.7034
2022-08-10 04:52:23 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.4418
2022-08-10 04:52:57 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.7291
2022-08-10 04:53:30 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.7272
2022-08-10 04:54:04 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5954
2022-08-10 04:54:37 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.6633
2022-08-10 04:55:10 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.6345
2022-08-10 04:55:44 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.9319
2022-08-10 04:56:17 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.6257
2022-08-10 04:56:50 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.7267
2022-08-10 04:57:24 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.6164
2022-08-10 04:57:57 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.9470
2022-08-10 04:58:30 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.4884
2022-08-10 04:59:04 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.9473
2022-08-10 04:59:37 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.5384
2022-08-10 05:00:11 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.8360
2022-08-10 05:00:44 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.6466
2022-08-10 05:01:17 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.6881
2022-08-10 05:01:51 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.6643
2022-08-10 05:02:24 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.6632
2022-08-10 05:02:58 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.7322
2022-08-10 05:03:31 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.5399
2022-08-10 05:04:05 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.6354
2022-08-10 05:04:38 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.9152
2022-08-10 05:05:12 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.7776
2022-08-10 05:05:45 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.6978
2022-08-10 05:06:17 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.7592
2022-08-10 05:06:19 - train: epoch 018, train_loss: 1.6887
2022-08-10 05:07:33 - eval: epoch: 018, acc1: 66.038%, acc5: 87.282%, test_loss: 1.3971, per_image_load_time: 2.287ms, per_image_inference_time: 0.595ms
2022-08-10 05:07:33 - until epoch: 018, best_acc1: 66.038%
2022-08-10 05:07:33 - epoch 019 lr: 0.018128
2022-08-10 05:08:13 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4601
2022-08-10 05:08:45 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.5511
2022-08-10 05:09:17 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.6775
2022-08-10 05:09:49 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.6180
2022-08-10 05:10:22 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.5363
2022-08-10 05:10:55 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.8211
2022-08-10 05:11:28 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.5398
2022-08-10 05:12:01 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.7494
2022-08-10 05:12:34 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.5294
2022-08-10 05:13:07 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.4969
2022-08-10 05:13:40 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5561
2022-08-10 05:14:13 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.6050
2022-08-10 05:14:46 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.8571
2022-08-10 05:15:19 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.5296
2022-08-10 05:15:53 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.9525
2022-08-10 05:16:26 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.4591
2022-08-10 05:16:59 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.8154
2022-08-10 05:17:32 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.5459
2022-08-10 05:18:05 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.7171
2022-08-10 05:18:39 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.4023
2022-08-10 05:19:12 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.4120
2022-08-10 05:19:46 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.5634
2022-08-10 05:20:19 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.6757
2022-08-10 05:20:52 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.9400
2022-08-10 05:21:25 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.4247
2022-08-10 05:21:59 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.7585
2022-08-10 05:22:32 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.5117
2022-08-10 05:23:05 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.4698
2022-08-10 05:23:38 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.6421
2022-08-10 05:24:11 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.7178
2022-08-10 05:24:44 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.5678
2022-08-10 05:25:18 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.3527
2022-08-10 05:25:51 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.7106
2022-08-10 05:26:24 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.4862
2022-08-10 05:26:57 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.7681
2022-08-10 05:27:30 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.3532
2022-08-10 05:28:04 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.5832
2022-08-10 05:28:37 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.7188
2022-08-10 05:29:10 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.4389
2022-08-10 05:29:44 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.5511
2022-08-10 05:30:17 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.7368
2022-08-10 05:30:50 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.5974
2022-08-10 05:31:23 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.5701
2022-08-10 05:31:56 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.6023
2022-08-10 05:32:30 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.8372
2022-08-10 05:33:03 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.5386
2022-08-10 05:33:37 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.6112
2022-08-10 05:34:10 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.7081
2022-08-10 05:34:44 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.6067
2022-08-10 05:35:16 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.6024
2022-08-10 05:35:17 - train: epoch 019, train_loss: 1.6191
2022-08-10 05:36:33 - eval: epoch: 019, acc1: 67.348%, acc5: 88.122%, test_loss: 1.3414, per_image_load_time: 1.506ms, per_image_inference_time: 0.591ms
2022-08-10 05:36:33 - until epoch: 019, best_acc1: 67.348%
2022-08-10 05:36:33 - epoch 020 lr: 0.013551
2022-08-10 05:37:13 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.5373
2022-08-10 05:37:46 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.4244
2022-08-10 05:38:19 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.6206
2022-08-10 05:38:52 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.3806
2022-08-10 05:39:25 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.3036
2022-08-10 05:39:58 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.5203
2022-08-10 05:40:31 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.4061
2022-08-10 05:41:05 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.4695
2022-08-10 05:41:38 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.8309
2022-08-10 05:42:12 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.4982
2022-08-10 05:42:45 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.4806
2022-08-10 05:43:18 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.4355
2022-08-10 05:43:51 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.4556
2022-08-10 05:44:25 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.4828
2022-08-10 05:44:58 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.5888
2022-08-10 05:45:31 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.5906
2022-08-10 05:46:05 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.5734
2022-08-10 05:46:38 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.7324
2022-08-10 05:47:11 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.3973
2022-08-10 05:47:44 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.5127
2022-08-10 05:48:17 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.4685
2022-08-10 05:48:50 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.4134
2022-08-10 05:49:24 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.4423
2022-08-10 05:49:57 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.8028
2022-08-10 05:50:31 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.5090
2022-08-10 05:51:04 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.4689
2022-08-10 05:51:38 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.5654
2022-08-10 05:52:11 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.4999
2022-08-10 05:52:44 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.6888
2022-08-10 05:53:18 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.6520
2022-08-10 05:53:51 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.6062
2022-08-10 05:54:25 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.8500
2022-08-10 05:54:58 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3148
2022-08-10 05:55:31 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.5804
2022-08-10 05:56:05 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.5835
2022-08-10 05:56:38 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.5755
2022-08-10 05:57:12 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.3161
2022-08-10 05:57:45 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.6815
2022-08-10 05:58:18 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.8205
2022-08-10 05:58:51 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.4615
2022-08-10 05:59:25 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.7903
2022-08-10 05:59:58 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.4436
2022-08-10 06:00:32 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.7888
2022-08-10 06:01:05 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.3720
2022-08-10 06:01:39 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.4131
2022-08-10 06:02:13 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.6441
2022-08-10 06:02:46 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.4187
2022-08-10 06:03:19 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.6267
2022-08-10 06:03:52 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.5602
2022-08-10 06:04:25 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.3926
2022-08-10 06:04:26 - train: epoch 020, train_loss: 1.5441
2022-08-10 06:05:40 - eval: epoch: 020, acc1: 68.496%, acc5: 88.898%, test_loss: 1.2808, per_image_load_time: 2.309ms, per_image_inference_time: 0.576ms
2022-08-10 06:05:41 - until epoch: 020, best_acc1: 68.496%
2022-08-10 06:05:41 - epoch 021 lr: 0.009548
2022-08-10 06:06:21 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.6188
2022-08-10 06:06:53 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.5415
2022-08-10 06:07:25 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.2609
2022-08-10 06:07:58 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4639
2022-08-10 06:08:31 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.3961
2022-08-10 06:09:04 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.4537
2022-08-10 06:09:37 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.4704
2022-08-10 06:10:10 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.4802
2022-08-10 06:10:42 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.6686
2022-08-10 06:11:16 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.4258
2022-08-10 06:11:49 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.3569
2022-08-10 06:12:22 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.4679
2022-08-10 06:12:55 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.3285
2022-08-10 06:13:29 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.2999
2022-08-10 06:14:02 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.2844
2022-08-10 06:14:35 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.4450
2022-08-10 06:15:08 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.7263
2022-08-10 06:15:41 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.3334
2022-08-10 06:16:15 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.6383
2022-08-10 06:16:48 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.5064
2022-08-10 06:17:20 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.2762
2022-08-10 06:17:54 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.5302
2022-08-10 06:18:27 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.4018
2022-08-10 06:19:01 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.3841
2022-08-10 06:19:34 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.3967
2022-08-10 06:20:07 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.3675
2022-08-10 06:20:40 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.5551
2022-08-10 06:21:13 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.2992
2022-08-10 06:21:46 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.4112
2022-08-10 06:22:20 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.5900
2022-08-10 06:22:53 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.5126
2022-08-10 06:23:26 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.4151
2022-08-10 06:23:59 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.6909
2022-08-10 06:24:32 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.4975
2022-08-10 06:25:05 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.5147
2022-08-10 06:25:39 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.2968
2022-08-10 06:26:12 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.3912
2022-08-10 06:26:45 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.5081
2022-08-10 06:27:19 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.4539
2022-08-10 06:27:53 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.4591
2022-08-10 06:28:26 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.4812
2022-08-10 06:28:59 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.3967
2022-08-10 06:29:33 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.4081
2022-08-10 06:30:06 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.4520
2022-08-10 06:30:40 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.3408
2022-08-10 06:31:13 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.2638
2022-08-10 06:31:47 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.5254
2022-08-10 06:32:20 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.4867
2022-08-10 06:32:54 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.3063
2022-08-10 06:33:27 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.3513
2022-08-10 06:33:28 - train: epoch 021, train_loss: 1.4732
2022-08-10 06:34:43 - eval: epoch: 021, acc1: 69.498%, acc5: 89.352%, test_loss: 1.2369, per_image_load_time: 2.005ms, per_image_inference_time: 0.602ms
2022-08-10 06:34:43 - until epoch: 021, best_acc1: 69.498%
2022-08-10 06:34:43 - epoch 022 lr: 0.006184
2022-08-10 06:35:22 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.2368
2022-08-10 06:35:55 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.2976
2022-08-10 06:36:28 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.2791
2022-08-10 06:37:01 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.4095
2022-08-10 06:37:34 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.4023
2022-08-10 06:38:08 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.6034
2022-08-10 06:38:41 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4147
2022-08-10 06:39:14 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.4760
2022-08-10 06:39:47 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.5360
2022-08-10 06:40:21 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.3773
2022-08-10 06:40:54 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.4998
2022-08-10 06:41:27 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.1756
2022-08-10 06:42:00 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.4006
2022-08-10 06:42:34 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.3634
2022-08-10 06:43:07 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.4581
2022-08-10 06:43:40 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.2789
2022-08-10 06:44:14 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.3863
2022-08-10 06:44:47 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.6914
2022-08-10 06:45:20 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.4424
2022-08-10 06:45:54 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.3823
2022-08-10 06:46:27 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.2391
2022-08-10 06:47:01 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.0708
2022-08-10 06:47:34 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.4417
2022-08-10 06:48:07 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.4958
2022-08-10 06:48:41 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.3372
2022-08-10 06:49:15 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.2645
2022-08-10 06:49:48 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.0763
2022-08-10 06:50:21 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.4458
2022-08-10 06:50:55 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.3433
2022-08-10 06:51:28 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.5393
2022-08-10 06:52:01 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.3964
2022-08-10 06:52:34 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4186
2022-08-10 06:53:07 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.4233
2022-08-10 06:53:41 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.2656
2022-08-10 06:54:14 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.5150
2022-08-10 06:54:47 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.4119
2022-08-10 06:55:21 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.3864
2022-08-10 06:55:54 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.4977
2022-08-10 06:56:28 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.2836
2022-08-10 06:57:01 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.5086
2022-08-10 06:57:34 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.3487
2022-08-10 06:58:08 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.3495
2022-08-10 06:58:41 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4430
2022-08-10 06:59:15 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.4730
2022-08-10 06:59:48 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.2867
2022-08-10 07:00:21 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.5823
2022-08-10 07:00:55 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.5202
2022-08-10 07:01:28 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.2492
2022-08-10 07:02:02 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.3226
2022-08-10 07:02:34 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.2856
2022-08-10 07:02:35 - train: epoch 022, train_loss: 1.4080
2022-08-10 07:03:51 - eval: epoch: 022, acc1: 70.612%, acc5: 89.906%, test_loss: 1.1929, per_image_load_time: 2.320ms, per_image_inference_time: 0.597ms
2022-08-10 07:03:51 - until epoch: 022, best_acc1: 70.612%
2022-08-10 07:03:51 - epoch 023 lr: 0.003511
2022-08-10 07:04:31 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.3191
2022-08-10 07:05:03 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.3702
2022-08-10 07:05:36 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.4600
2022-08-10 07:06:10 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.3566
2022-08-10 07:06:43 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.4543
2022-08-10 07:07:16 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.3438
2022-08-10 07:07:49 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.1981
2022-08-10 07:08:23 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3545
2022-08-10 07:08:56 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4823
2022-08-10 07:09:30 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.2536
2022-08-10 07:10:03 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.3775
2022-08-10 07:10:36 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.3766
2022-08-10 07:11:09 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.5548
2022-08-10 07:11:42 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.4419
2022-08-10 07:12:15 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.2227
2022-08-10 07:12:48 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.4536
2022-08-10 07:13:21 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.2800
2022-08-10 07:13:55 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.2640
2022-08-10 07:14:28 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.3481
2022-08-10 07:15:01 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.3650
2022-08-10 07:15:35 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.4951
2022-08-10 07:16:09 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.2017
2022-08-10 07:16:42 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.3136
2022-08-10 07:17:16 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.2755
2022-08-10 07:17:49 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.3324
2022-08-10 07:18:23 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.4444
2022-08-10 07:18:56 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.4103
2022-08-10 07:19:29 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.3735
2022-08-10 07:20:02 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.3005
2022-08-10 07:20:36 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.3317
2022-08-10 07:21:09 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.3645
2022-08-10 07:21:42 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.3539
2022-08-10 07:22:15 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.4007
2022-08-10 07:22:48 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.4016
2022-08-10 07:23:22 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.4151
2022-08-10 07:23:55 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.3495
2022-08-10 07:24:28 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.2195
2022-08-10 07:25:02 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.3103
2022-08-10 07:25:35 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3352
2022-08-10 07:26:09 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2288
2022-08-10 07:26:42 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.2649
2022-08-10 07:27:15 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.2310
2022-08-10 07:27:48 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.2852
2022-08-10 07:28:22 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.2590
2022-08-10 07:28:55 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.3143
2022-08-10 07:29:29 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.2860
2022-08-10 07:30:02 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.2143
2022-08-10 07:30:35 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.2692
2022-08-10 07:31:09 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.1713
2022-08-10 07:31:42 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.3342
2022-08-10 07:31:43 - train: epoch 023, train_loss: 1.3491
2022-08-10 07:32:58 - eval: epoch: 023, acc1: 71.352%, acc5: 90.244%, test_loss: 1.1602, per_image_load_time: 1.369ms, per_image_inference_time: 0.600ms
2022-08-10 07:32:58 - until epoch: 023, best_acc1: 71.352%
2022-08-10 07:32:58 - epoch 024 lr: 0.001571
2022-08-10 07:33:38 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.4442
2022-08-10 07:34:10 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.3072
2022-08-10 07:34:43 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.3440
2022-08-10 07:35:17 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.4638
2022-08-10 07:35:50 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.3404
2022-08-10 07:36:23 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.2862
2022-08-10 07:36:57 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.2859
2022-08-10 07:37:30 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.1340
2022-08-10 07:38:03 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.3460
2022-08-10 07:38:35 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.3385
2022-08-10 07:39:08 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.0442
2022-08-10 07:39:41 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.1104
2022-08-10 07:40:15 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.5113
2022-08-10 07:40:48 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.3487
2022-08-10 07:41:21 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.3989
2022-08-10 07:41:54 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.1256
2022-08-10 07:42:28 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.2496
2022-08-10 07:43:01 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.5522
2022-08-10 07:43:34 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.1849
2022-08-10 07:44:08 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.3460
2022-08-10 07:44:41 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.4003
2022-08-10 07:45:15 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.2904
2022-08-10 07:45:48 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.3965
2022-08-10 07:46:21 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.4228
2022-08-10 07:46:54 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.2710
2022-08-10 07:47:27 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.5496
2022-08-10 07:48:01 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.3117
2022-08-10 07:48:34 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.2320
2022-08-10 07:49:07 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.3561
2022-08-10 07:49:40 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.1809
2022-08-10 07:50:14 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.0740
2022-08-10 07:50:47 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.3536
2022-08-10 07:51:21 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.1552
2022-08-10 07:51:54 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.3914
2022-08-10 07:52:28 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.2057
2022-08-10 07:53:01 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.3278
2022-08-10 07:53:35 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.3876
2022-08-10 07:54:09 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.5099
2022-08-10 07:54:42 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.3594
2022-08-10 07:55:16 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.2506
2022-08-10 07:55:49 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.2165
2022-08-10 07:56:22 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.2302
2022-08-10 07:56:56 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.2799
2022-08-10 07:57:29 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.1966
2022-08-10 07:58:03 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.2182
2022-08-10 07:58:36 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.2222
2022-08-10 07:59:09 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.3891
2022-08-10 07:59:43 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.1934
2022-08-10 08:00:16 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.5189
2022-08-10 08:00:49 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.2194
2022-08-10 08:00:50 - train: epoch 024, train_loss: 1.3137
2022-08-10 08:02:04 - eval: epoch: 024, acc1: 71.672%, acc5: 90.488%, test_loss: 1.1449, per_image_load_time: 1.765ms, per_image_inference_time: 0.603ms
2022-08-10 08:02:04 - until epoch: 024, best_acc1: 71.672%
2022-08-10 08:02:04 - epoch 025 lr: 0.000394
2022-08-10 08:02:44 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.2930
2022-08-10 08:03:16 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1725
2022-08-10 08:03:49 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.2652
2022-08-10 08:04:21 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.3220
2022-08-10 08:04:54 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.1349
2022-08-10 08:05:27 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.1937
2022-08-10 08:05:59 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.2500
2022-08-10 08:06:32 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.3114
2022-08-10 08:07:05 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0125
2022-08-10 08:07:39 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3311
2022-08-10 08:08:12 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.3700
2022-08-10 08:08:45 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.1703
2022-08-10 08:09:18 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.2929
2022-08-10 08:09:51 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.2453
2022-08-10 08:10:25 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.3058
2022-08-10 08:10:58 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1755
2022-08-10 08:11:31 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.3504
2022-08-10 08:12:04 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.1263
2022-08-10 08:12:37 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.1901
2022-08-10 08:13:11 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.5019
2022-08-10 08:13:44 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.3058
2022-08-10 08:14:17 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.1137
2022-08-10 08:14:51 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.2130
2022-08-10 08:15:25 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.1124
2022-08-10 08:15:58 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.2346
2022-08-10 08:16:31 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.4011
2022-08-10 08:17:05 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.2050
2022-08-10 08:17:38 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.0894
2022-08-10 08:18:11 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.4186
2022-08-10 08:18:44 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.3552
2022-08-10 08:19:18 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.3725
2022-08-10 08:19:51 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.3540
2022-08-10 08:20:24 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.1095
2022-08-10 08:20:57 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.4518
2022-08-10 08:21:30 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1388
2022-08-10 08:22:04 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.2057
2022-08-10 08:22:37 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.2401
2022-08-10 08:23:10 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.4086
2022-08-10 08:23:44 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.3273
2022-08-10 08:24:17 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.3274
2022-08-10 08:24:50 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.3291
2022-08-10 08:25:24 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.2735
2022-08-10 08:25:57 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.2946
2022-08-10 08:26:30 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.2212
2022-08-10 08:27:03 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.5728
2022-08-10 08:27:37 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.2069
2022-08-10 08:28:10 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.2118
2022-08-10 08:28:43 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.2183
2022-08-10 08:29:17 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.2678
2022-08-10 08:29:49 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.5754
2022-08-10 08:29:51 - train: epoch 025, train_loss: 1.2963
2022-08-10 08:31:05 - eval: epoch: 025, acc1: 71.740%, acc5: 90.486%, test_loss: 1.1410, per_image_load_time: 2.316ms, per_image_inference_time: 0.567ms
2022-08-10 08:31:06 - until epoch: 025, best_acc1: 71.740%
2022-08-10 08:31:06 - train done. train time: 12.072 hours, best_acc1: 71.740%
