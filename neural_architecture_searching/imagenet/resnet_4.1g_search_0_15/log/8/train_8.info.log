2022-08-10 20:42:58 - net_idx: 8
2022-08-10 20:42:58 - net_config: {'stem_width': 64, 'depth': 12, 'w_0': 32, 'w_a': 23.186472934432135, 'w_m': 2.1412442365785926}
2022-08-10 20:42:58 - num_classes: 1000
2022-08-10 20:42:58 - input_image_size: 224
2022-08-10 20:42:58 - scale: 1.1428571428571428
2022-08-10 20:42:58 - seed: 0
2022-08-10 20:42:58 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-10 20:42:58 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-10 20:42:58 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce2b0>
2022-08-10 20:42:58 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce6d0>
2022-08-10 20:42:58 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce700>
2022-08-10 20:42:58 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce760>
2022-08-10 20:42:58 - batch_size: 256
2022-08-10 20:42:58 - num_workers: 16
2022-08-10 20:42:58 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-10 20:42:58 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-10 20:42:58 - epochs: 25
2022-08-10 20:42:58 - print_interval: 100
2022-08-10 20:42:58 - accumulation_steps: 1
2022-08-10 20:42:58 - sync_bn: False
2022-08-10 20:42:58 - apex: True
2022-08-10 20:42:58 - use_ema_model: False
2022-08-10 20:42:58 - ema_model_decay: 0.9999
2022-08-10 20:42:58 - log_dir: ./log
2022-08-10 20:42:58 - checkpoint_dir: ./checkpoints
2022-08-10 20:42:58 - gpus_type: NVIDIA RTX A5000
2022-08-10 20:42:58 - gpus_num: 2
2022-08-10 20:42:58 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5dddaeb570>
2022-08-10 20:42:58 - ema_model: None
2022-08-10 20:42:58 - --------------------parameters--------------------
2022-08-10 20:42:58 - name: conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-10 20:42:58 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-10 20:42:58 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-10 20:42:58 - name: fc.weight, grad: True
2022-08-10 20:42:58 - name: fc.bias, grad: True
2022-08-10 20:42:58 - --------------------buffers--------------------
2022-08-10 20:42:58 - name: conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-10 20:42:58 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-10 20:42:58 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-10 20:42:58 - -----------no weight decay layers--------------
2022-08-10 20:42:58 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-10 20:42:58 - -------------weight decay layers---------------
2022-08-10 20:42:58 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-10 20:42:58 - epoch 001 lr: 0.100000
2022-08-10 20:43:38 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9160
2022-08-10 20:44:10 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.8833
2022-08-10 20:44:43 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8411
2022-08-10 20:45:16 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.7643
2022-08-10 20:45:49 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.7148
2022-08-10 20:46:23 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.4829
2022-08-10 20:46:56 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.5982
2022-08-10 20:47:30 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.4989
2022-08-10 20:48:03 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.3911
2022-08-10 20:48:37 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.4256
2022-08-10 20:49:11 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.3357
2022-08-10 20:49:44 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.2590
2022-08-10 20:50:17 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.2179
2022-08-10 20:50:51 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.0797
2022-08-10 20:51:24 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 5.9143
2022-08-10 20:51:58 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 5.9784
2022-08-10 20:52:31 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.7124
2022-08-10 20:53:05 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.7310
2022-08-10 20:53:38 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.7290
2022-08-10 20:54:11 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.6104
2022-08-10 20:54:44 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.5426
2022-08-10 20:55:17 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.4666
2022-08-10 20:55:51 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.3383
2022-08-10 20:56:24 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.3632
2022-08-10 20:56:58 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.3235
2022-08-10 20:57:31 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.3639
2022-08-10 20:58:04 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.1904
2022-08-10 20:58:37 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.2243
2022-08-10 20:59:11 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 4.9252
2022-08-10 20:59:44 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.2370
2022-08-10 21:00:17 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.0599
2022-08-10 21:00:51 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.0828
2022-08-10 21:01:24 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.8572
2022-08-10 21:01:57 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.9913
2022-08-10 21:02:30 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.7985
2022-08-10 21:03:04 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.7606
2022-08-10 21:03:37 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.8600
2022-08-10 21:04:10 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.6701
2022-08-10 21:04:44 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.6041
2022-08-10 21:05:17 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.6992
2022-08-10 21:05:50 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.7051
2022-08-10 21:06:24 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.6493
2022-08-10 21:06:57 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.4990
2022-08-10 21:07:30 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.3184
2022-08-10 21:08:04 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.4261
2022-08-10 21:08:37 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.6591
2022-08-10 21:09:11 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.4438
2022-08-10 21:09:44 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.5498
2022-08-10 21:10:17 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.3114
2022-08-10 21:10:50 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.3937
2022-08-10 21:10:51 - train: epoch 001, train_loss: 5.4384
2022-08-10 21:12:05 - eval: epoch: 001, acc1: 18.958%, acc5: 41.214%, test_loss: 4.3765, per_image_load_time: 2.271ms, per_image_inference_time: 0.599ms
2022-08-10 21:12:06 - until epoch: 001, best_acc1: 18.958%
2022-08-10 21:12:06 - epoch 002 lr: 0.099606
2022-08-10 21:12:46 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.1988
2022-08-10 21:13:19 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 3.9537
2022-08-10 21:13:51 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.3162
2022-08-10 21:14:25 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.1804
2022-08-10 21:14:58 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.0328
2022-08-10 21:15:31 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.0024
2022-08-10 21:16:04 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.3527
2022-08-10 21:16:37 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 3.9588
2022-08-10 21:17:11 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.9587
2022-08-10 21:17:44 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.1159
2022-08-10 21:18:17 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.0314
2022-08-10 21:18:50 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.9623
2022-08-10 21:19:24 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 4.0816
2022-08-10 21:19:57 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.1069
2022-08-10 21:20:31 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 3.8938
2022-08-10 21:21:04 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.8093
2022-08-10 21:21:37 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.9711
2022-08-10 21:22:11 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.8525
2022-08-10 21:22:44 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.7190
2022-08-10 21:23:18 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.5922
2022-08-10 21:23:51 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.8287
2022-08-10 21:24:24 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.8451
2022-08-10 21:24:58 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.7353
2022-08-10 21:25:31 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.5255
2022-08-10 21:26:04 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.5401
2022-08-10 21:26:38 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.5926
2022-08-10 21:27:11 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.8508
2022-08-10 21:27:44 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.6976
2022-08-10 21:28:18 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.6707
2022-08-10 21:28:51 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.6328
2022-08-10 21:29:24 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.6783
2022-08-10 21:29:57 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.5812
2022-08-10 21:30:31 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.6697
2022-08-10 21:31:04 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.5769
2022-08-10 21:31:38 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.4511
2022-08-10 21:32:11 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.6308
2022-08-10 21:32:45 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.7045
2022-08-10 21:33:18 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.2973
2022-08-10 21:33:51 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.3848
2022-08-10 21:34:25 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.3700
2022-08-10 21:34:58 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.5124
2022-08-10 21:35:31 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.4055
2022-08-10 21:36:04 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.3146
2022-08-10 21:36:38 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.3800
2022-08-10 21:37:11 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.1758
2022-08-10 21:37:45 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.2118
2022-08-10 21:38:19 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.3385
2022-08-10 21:38:53 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.3232
2022-08-10 21:39:27 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.4405
2022-08-10 21:40:00 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.4396
2022-08-10 21:40:01 - train: epoch 002, train_loss: 3.7383
2022-08-10 21:41:17 - eval: epoch: 002, acc1: 30.056%, acc5: 55.994%, test_loss: 3.4534, per_image_load_time: 2.289ms, per_image_inference_time: 0.615ms
2022-08-10 21:41:17 - until epoch: 002, best_acc1: 30.056%
2022-08-10 21:41:17 - epoch 003 lr: 0.098429
2022-08-10 21:41:58 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.4267
2022-08-10 21:42:31 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.3987
2022-08-10 21:43:04 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.3999
2022-08-10 21:43:38 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.3913
2022-08-10 21:44:11 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.3866
2022-08-10 21:44:44 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.1306
2022-08-10 21:45:18 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.3101
2022-08-10 21:45:51 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.5309
2022-08-10 21:46:25 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.1421
2022-08-10 21:46:58 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.2305
2022-08-10 21:47:32 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.2037
2022-08-10 21:48:05 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.2131
2022-08-10 21:48:39 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.0641
2022-08-10 21:49:12 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 2.9906
2022-08-10 21:49:46 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.2810
2022-08-10 21:50:20 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.1679
2022-08-10 21:50:54 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.0038
2022-08-10 21:51:28 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.1278
2022-08-10 21:52:01 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.2574
2022-08-10 21:52:35 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 2.9740
2022-08-10 21:53:09 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.2201
2022-08-10 21:53:42 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.5889
2022-08-10 21:54:16 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.9547
2022-08-10 21:54:49 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.0484
2022-08-10 21:55:22 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.1174
2022-08-10 21:55:56 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.1320
2022-08-10 21:56:29 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.3953
2022-08-10 21:57:03 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.0280
2022-08-10 21:57:37 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.9715
2022-08-10 21:58:10 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.1885
2022-08-10 21:58:43 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.3869
2022-08-10 21:59:16 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 2.9742
2022-08-10 21:59:50 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0347
2022-08-10 22:00:23 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.1404
2022-08-10 22:00:56 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 3.0213
2022-08-10 22:01:30 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.8530
2022-08-10 22:02:03 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.0958
2022-08-10 22:02:36 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.3415
2022-08-10 22:03:10 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.3453
2022-08-10 22:03:43 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.9933
2022-08-10 22:04:17 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.9923
2022-08-10 22:04:51 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 2.9588
2022-08-10 22:05:24 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.8830
2022-08-10 22:05:57 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 3.0463
2022-08-10 22:06:30 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.1284
2022-08-10 22:07:04 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 3.1888
2022-08-10 22:07:37 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 2.8621
2022-08-10 22:08:10 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.2160
2022-08-10 22:08:44 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.0239
2022-08-10 22:09:17 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.9666
2022-08-10 22:09:18 - train: epoch 003, train_loss: 3.1269
2022-08-10 22:10:33 - eval: epoch: 003, acc1: 37.962%, acc5: 64.418%, test_loss: 2.9983, per_image_load_time: 2.328ms, per_image_inference_time: 0.546ms
2022-08-10 22:10:34 - until epoch: 003, best_acc1: 37.962%
2022-08-10 22:10:34 - epoch 004 lr: 0.096488
2022-08-10 22:11:13 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.9401
2022-08-10 22:11:47 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.6538
2022-08-10 22:12:21 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.8234
2022-08-10 22:12:54 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.9824
2022-08-10 22:13:28 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.9114
2022-08-10 22:14:01 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.1248
2022-08-10 22:14:34 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.8735
2022-08-10 22:15:07 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.8438
2022-08-10 22:15:40 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.5388
2022-08-10 22:16:13 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.9387
2022-08-10 22:16:47 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 2.9512
2022-08-10 22:17:20 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.6450
2022-08-10 22:17:53 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.7355
2022-08-10 22:18:26 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.7593
2022-08-10 22:18:59 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.9013
2022-08-10 22:19:32 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.9122
2022-08-10 22:20:06 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.7485
2022-08-10 22:20:39 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.9028
2022-08-10 22:21:13 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.8604
2022-08-10 22:21:47 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.7384
2022-08-10 22:22:20 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.8591
2022-08-10 22:22:54 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.8011
2022-08-10 22:23:28 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.5895
2022-08-10 22:24:02 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.6106
2022-08-10 22:24:35 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.8027
2022-08-10 22:25:09 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.8423
2022-08-10 22:25:43 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.6083
2022-08-10 22:26:16 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8582
2022-08-10 22:26:50 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8865
2022-08-10 22:27:23 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.8361
2022-08-10 22:27:57 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.7254
2022-08-10 22:28:30 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.9597
2022-08-10 22:29:04 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.7927
2022-08-10 22:29:37 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.6527
2022-08-10 22:30:10 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.8109
2022-08-10 22:30:44 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.8415
2022-08-10 22:31:17 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.7356
2022-08-10 22:31:51 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.5923
2022-08-10 22:32:24 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.4721
2022-08-10 22:32:57 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.5443
2022-08-10 22:33:31 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.7336
2022-08-10 22:34:04 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.7950
2022-08-10 22:34:38 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.5905
2022-08-10 22:35:11 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.7313
2022-08-10 22:35:44 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.1988
2022-08-10 22:36:17 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.7703
2022-08-10 22:36:51 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.8370
2022-08-10 22:37:24 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.5473
2022-08-10 22:37:57 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.7395
2022-08-10 22:38:30 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.9827
2022-08-10 22:38:32 - train: epoch 004, train_loss: 2.8209
2022-08-10 22:39:47 - eval: epoch: 004, acc1: 42.310%, acc5: 68.680%, test_loss: 2.6883, per_image_load_time: 2.329ms, per_image_inference_time: 0.578ms
2022-08-10 22:39:47 - until epoch: 004, best_acc1: 42.310%
2022-08-10 22:39:47 - epoch 005 lr: 0.093815
2022-08-10 22:40:28 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.6466
2022-08-10 22:41:01 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.8132
2022-08-10 22:41:34 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.6598
2022-08-10 22:42:07 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.7045
2022-08-10 22:42:40 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.4830
2022-08-10 22:43:13 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.8711
2022-08-10 22:43:46 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.8262
2022-08-10 22:44:19 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.9260
2022-08-10 22:44:52 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.5967
2022-08-10 22:45:25 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.6509
2022-08-10 22:45:58 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.6487
2022-08-10 22:46:32 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.5817
2022-08-10 22:47:05 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.5039
2022-08-10 22:47:38 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.4747
2022-08-10 22:48:10 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.4651
2022-08-10 22:48:44 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.4321
2022-08-10 22:49:17 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.6937
2022-08-10 22:49:50 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.8396
2022-08-10 22:50:24 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.6512
2022-08-10 22:50:57 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.5637
2022-08-10 22:51:30 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.4025
2022-08-10 22:52:03 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.7523
2022-08-10 22:52:37 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.6093
2022-08-10 22:53:10 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5010
2022-08-10 22:53:43 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.6413
2022-08-10 22:54:17 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.6939
2022-08-10 22:54:50 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.6530
2022-08-10 22:55:24 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.5973
2022-08-10 22:55:57 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.4426
2022-08-10 22:56:31 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.6576
2022-08-10 22:57:04 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.8445
2022-08-10 22:57:38 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.5794
2022-08-10 22:58:12 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.6013
2022-08-10 22:58:46 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.5501
2022-08-10 22:59:20 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.6132
2022-08-10 22:59:53 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6464
2022-08-10 23:00:27 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.7150
2022-08-10 23:01:01 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5694
2022-08-10 23:01:34 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.7392
2022-08-10 23:02:08 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.5541
2022-08-10 23:02:43 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.5859
2022-08-10 23:03:16 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.7558
2022-08-10 23:03:50 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.6109
2022-08-10 23:04:24 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.6870
2022-08-10 23:04:59 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.6917
2022-08-10 23:05:33 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.7243
2022-08-10 23:06:06 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.3014
2022-08-10 23:06:40 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.4069
2022-08-10 23:07:14 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.9087
2022-08-10 23:07:47 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.5236
2022-08-10 23:07:48 - train: epoch 005, train_loss: 2.6345
2022-08-10 23:09:05 - eval: epoch: 005, acc1: 48.010%, acc5: 74.036%, test_loss: 2.3145, per_image_load_time: 2.386ms, per_image_inference_time: 0.566ms
2022-08-10 23:09:06 - until epoch: 005, best_acc1: 48.010%
2022-08-10 23:09:06 - epoch 006 lr: 0.090450
2022-08-10 23:09:46 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.4645
2022-08-10 23:10:20 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.6058
2022-08-10 23:10:54 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.4040
2022-08-10 23:11:28 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.5269
2022-08-10 23:12:02 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.3396
2022-08-10 23:12:36 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.6499
2022-08-10 23:13:10 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.4823
2022-08-10 23:13:44 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.3689
2022-08-10 23:14:18 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3741
2022-08-10 23:14:52 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.4606
2022-08-10 23:15:26 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.5264
2022-08-10 23:16:01 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.4272
2022-08-10 23:16:35 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.4692
2022-08-10 23:17:09 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.5598
2022-08-10 23:17:43 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.6419
2022-08-10 23:18:16 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3855
2022-08-10 23:18:49 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.6176
2022-08-10 23:19:23 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.4802
2022-08-10 23:19:57 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.5835
2022-08-10 23:20:30 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.8106
2022-08-10 23:21:04 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.3291
2022-08-10 23:21:37 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.1702
2022-08-10 23:22:11 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.5440
2022-08-10 23:22:44 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.5624
2022-08-10 23:23:18 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.5781
2022-08-10 23:23:52 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.3145
2022-08-10 23:24:25 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.6064
2022-08-10 23:25:00 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.1820
2022-08-10 23:25:33 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.7424
2022-08-10 23:26:07 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.5438
2022-08-10 23:26:41 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.3017
2022-08-10 23:27:14 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.3906
2022-08-10 23:27:48 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.2168
2022-08-10 23:28:21 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.8071
2022-08-10 23:28:55 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.6414
2022-08-10 23:29:28 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.4445
2022-08-10 23:30:02 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.4431
2022-08-10 23:30:35 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.3268
2022-08-10 23:31:08 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.3540
2022-08-10 23:31:41 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.7312
2022-08-10 23:32:15 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.4176
2022-08-10 23:32:48 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.1928
2022-08-10 23:33:22 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.5033
2022-08-10 23:33:56 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.3979
2022-08-10 23:34:29 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.5613
2022-08-10 23:35:03 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.6477
2022-08-10 23:35:36 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4634
2022-08-10 23:36:10 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.4594
2022-08-10 23:36:44 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.4599
2022-08-10 23:37:17 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.4177
2022-08-10 23:37:18 - train: epoch 006, train_loss: 2.5034
2022-08-10 23:38:35 - eval: epoch: 006, acc1: 49.614%, acc5: 75.502%, test_loss: 2.2037, per_image_load_time: 2.371ms, per_image_inference_time: 0.558ms
2022-08-10 23:38:35 - until epoch: 006, best_acc1: 49.614%
2022-08-10 23:38:35 - epoch 007 lr: 0.086448
2022-08-10 23:39:16 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.4049
2022-08-10 23:39:50 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.7315
2022-08-10 23:40:23 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.5452
2022-08-10 23:40:57 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.3712
2022-08-10 23:41:31 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.3229
2022-08-10 23:42:04 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.4809
2022-08-10 23:42:38 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.3063
2022-08-10 23:43:12 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.4533
2022-08-10 23:43:46 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.4718
2022-08-10 23:44:19 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.3457
2022-08-10 23:44:53 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.2716
2022-08-10 23:45:27 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.2384
2022-08-10 23:46:00 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.1518
2022-08-10 23:46:33 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.4599
2022-08-10 23:47:07 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.4243
2022-08-10 23:47:41 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.2389
2022-08-10 23:48:15 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.6050
2022-08-10 23:48:48 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.4486
2022-08-10 23:49:22 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.5808
2022-08-10 23:49:56 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.1942
2022-08-10 23:50:29 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.6480
2022-08-10 23:51:03 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.5775
2022-08-10 23:51:37 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.5665
2022-08-10 23:52:10 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.6292
2022-08-10 23:52:44 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.6578
2022-08-10 23:53:18 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.2397
2022-08-10 23:53:52 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.1707
2022-08-10 23:54:26 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.4194
2022-08-10 23:54:59 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.3421
2022-08-10 23:55:33 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.5691
2022-08-10 23:56:07 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.3078
2022-08-10 23:56:41 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.2697
2022-08-10 23:57:15 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.4582
2022-08-10 23:57:48 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.3997
2022-08-10 23:58:22 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.3916
2022-08-10 23:58:56 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.1836
2022-08-10 23:59:29 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.3619
2022-08-11 00:00:03 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.4643
2022-08-11 00:00:37 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.2092
2022-08-11 00:01:11 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.4029
2022-08-11 00:01:44 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.4757
2022-08-11 00:02:19 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2231
2022-08-11 00:02:52 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.4174
2022-08-11 00:03:26 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.3710
2022-08-11 00:04:00 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.4949
2022-08-11 00:04:34 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.3383
2022-08-11 00:05:07 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.4158
2022-08-11 00:05:41 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.6542
2022-08-11 00:06:15 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.3918
2022-08-11 00:06:48 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.2081
2022-08-11 00:06:49 - train: epoch 007, train_loss: 2.4026
2022-08-11 00:08:06 - eval: epoch: 007, acc1: 51.526%, acc5: 77.130%, test_loss: 2.0889, per_image_load_time: 2.357ms, per_image_inference_time: 0.576ms
2022-08-11 00:08:07 - until epoch: 007, best_acc1: 51.526%
2022-08-11 00:08:07 - epoch 008 lr: 0.081870
2022-08-11 00:08:47 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.4647
2022-08-11 00:09:20 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.5373
2022-08-11 00:09:54 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.2729
2022-08-11 00:10:27 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.3153
2022-08-11 00:11:00 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.4160
2022-08-11 00:11:33 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.3388
2022-08-11 00:12:06 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.2512
2022-08-11 00:12:40 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.1536
2022-08-11 00:13:13 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.2014
2022-08-11 00:13:46 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.3613
2022-08-11 00:14:20 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.1492
2022-08-11 00:14:53 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.1784
2022-08-11 00:15:27 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.4571
2022-08-11 00:16:01 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.2651
2022-08-11 00:16:35 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.3225
2022-08-11 00:17:09 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.3875
2022-08-11 00:17:43 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.3201
2022-08-11 00:18:16 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.2830
2022-08-11 00:18:50 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.3392
2022-08-11 00:19:24 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.4483
2022-08-11 00:19:58 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.4286
2022-08-11 00:20:32 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.2293
2022-08-11 00:21:06 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.4371
2022-08-11 00:21:40 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.0414
2022-08-11 00:22:14 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.2479
2022-08-11 00:22:48 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.3684
2022-08-11 00:23:22 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.6025
2022-08-11 00:23:56 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.3308
2022-08-11 00:24:30 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.3926
2022-08-11 00:25:04 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.4307
2022-08-11 00:25:38 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.2999
2022-08-11 00:26:12 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.4823
2022-08-11 00:26:46 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.1887
2022-08-11 00:27:19 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.3016
2022-08-11 00:27:53 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.3639
2022-08-11 00:28:27 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.4690
2022-08-11 00:29:02 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.2500
2022-08-11 00:29:36 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.1737
2022-08-11 00:30:10 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.2957
2022-08-11 00:30:44 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.4281
2022-08-11 00:31:18 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 1.9840
2022-08-11 00:31:52 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.0853
2022-08-11 00:32:26 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 1.9680
2022-08-11 00:33:00 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.2545
2022-08-11 00:33:35 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.2571
2022-08-11 00:34:09 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.5906
2022-08-11 00:34:44 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2405
2022-08-11 00:35:18 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.2795
2022-08-11 00:35:53 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.4388
2022-08-11 00:36:26 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.3074
2022-08-11 00:36:27 - train: epoch 008, train_loss: 2.3192
2022-08-11 00:37:44 - eval: epoch: 008, acc1: 53.284%, acc5: 78.600%, test_loss: 2.0098, per_image_load_time: 2.247ms, per_image_inference_time: 0.573ms
2022-08-11 00:37:44 - until epoch: 008, best_acc1: 53.284%
2022-08-11 00:37:44 - epoch 009 lr: 0.076790
2022-08-11 00:38:24 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 1.9517
2022-08-11 00:38:58 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.2541
2022-08-11 00:39:32 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.2241
2022-08-11 00:40:05 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.5606
2022-08-11 00:40:39 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.2398
2022-08-11 00:41:13 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.1298
2022-08-11 00:41:47 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.1717
2022-08-11 00:42:22 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.3086
2022-08-11 00:42:56 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.0126
2022-08-11 00:43:30 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1133
2022-08-11 00:44:04 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.2192
2022-08-11 00:44:38 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.1219
2022-08-11 00:45:13 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.3613
2022-08-11 00:45:47 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.8959
2022-08-11 00:46:21 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.1802
2022-08-11 00:46:56 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.3397
2022-08-11 00:47:30 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.2188
2022-08-11 00:48:05 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.2437
2022-08-11 00:48:39 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.1526
2022-08-11 00:49:14 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.1273
2022-08-11 00:49:48 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.2026
2022-08-11 00:50:23 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.3406
2022-08-11 00:50:57 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.2573
2022-08-11 00:51:32 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.2829
2022-08-11 00:52:06 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.3233
2022-08-11 00:52:41 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.4776
2022-08-11 00:53:15 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.2758
2022-08-11 00:53:50 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2900
2022-08-11 00:54:24 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 1.8867
2022-08-11 00:54:59 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1128
2022-08-11 00:55:33 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.4276
2022-08-11 00:56:08 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.2036
2022-08-11 00:56:43 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.3171
2022-08-11 00:57:17 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.4690
2022-08-11 00:57:52 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.1554
2022-08-11 00:58:26 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.0205
2022-08-11 00:59:01 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.2941
2022-08-11 00:59:35 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.3640
2022-08-11 01:00:09 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.1698
2022-08-11 01:00:44 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.4394
2022-08-11 01:01:19 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.2162
2022-08-11 01:01:53 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.2744
2022-08-11 01:02:28 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.0917
2022-08-11 01:03:03 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.2938
2022-08-11 01:03:37 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.3670
2022-08-11 01:04:11 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.3254
2022-08-11 01:04:46 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.3412
2022-08-11 01:05:20 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.4270
2022-08-11 01:05:55 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.5250
2022-08-11 01:06:29 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.0520
2022-08-11 01:06:30 - train: epoch 009, train_loss: 2.2492
2022-08-11 01:07:48 - eval: epoch: 009, acc1: 54.278%, acc5: 79.166%, test_loss: 1.9675, per_image_load_time: 2.445ms, per_image_inference_time: 0.532ms
2022-08-11 01:07:48 - until epoch: 009, best_acc1: 54.278%
2022-08-11 01:07:48 - epoch 010 lr: 0.071288
2022-08-11 01:08:29 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.3294
2022-08-11 01:09:03 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.3159
2022-08-11 01:09:38 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.0993
2022-08-11 01:10:12 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.3516
2022-08-11 01:10:46 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.1002
2022-08-11 01:11:21 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.3830
2022-08-11 01:11:55 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.1164
2022-08-11 01:12:30 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.1979
2022-08-11 01:13:04 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 1.9892
2022-08-11 01:13:38 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.0959
2022-08-11 01:14:12 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.0459
2022-08-11 01:14:47 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 1.8812
2022-08-11 01:15:22 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.1472
2022-08-11 01:15:56 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.3678
2022-08-11 01:16:30 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.9230
2022-08-11 01:17:03 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.2891
2022-08-11 01:17:37 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.3539
2022-08-11 01:18:11 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 1.9664
2022-08-11 01:18:45 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.2616
2022-08-11 01:19:18 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.1898
2022-08-11 01:19:52 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 2.0274
2022-08-11 01:20:26 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.3011
2022-08-11 01:21:00 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.3241
2022-08-11 01:21:33 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.3881
2022-08-11 01:22:07 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.2153
2022-08-11 01:22:41 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.2971
2022-08-11 01:23:14 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 2.0108
2022-08-11 01:23:48 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.3159
2022-08-11 01:24:22 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.3281
2022-08-11 01:24:55 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1385
2022-08-11 01:25:29 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.1714
2022-08-11 01:26:03 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.1839
2022-08-11 01:26:36 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.2313
2022-08-11 01:27:10 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.2316
2022-08-11 01:27:43 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.3007
2022-08-11 01:28:17 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.3863
2022-08-11 01:28:51 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.2252
2022-08-11 01:29:25 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.3191
2022-08-11 01:29:59 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 2.2550
2022-08-11 01:30:33 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.0289
2022-08-11 01:31:07 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.0554
2022-08-11 01:31:41 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.2494
2022-08-11 01:32:15 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.0828
2022-08-11 01:32:49 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 1.8860
2022-08-11 01:33:23 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.0406
2022-08-11 01:33:57 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.1240
2022-08-11 01:34:31 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1204
2022-08-11 01:35:05 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 1.9813
2022-08-11 01:35:39 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.2758
2022-08-11 01:36:12 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.0308
2022-08-11 01:36:13 - train: epoch 010, train_loss: 2.1859
2022-08-11 01:37:29 - eval: epoch: 010, acc1: 56.584%, acc5: 80.768%, test_loss: 1.8508, per_image_load_time: 2.004ms, per_image_inference_time: 0.603ms
2022-08-11 01:37:29 - until epoch: 010, best_acc1: 56.584%
2022-08-11 01:37:29 - epoch 011 lr: 0.065450
2022-08-11 01:38:10 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.0076
2022-08-11 01:38:43 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.2343
2022-08-11 01:39:16 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.1995
2022-08-11 01:39:50 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.2336
2022-08-11 01:40:23 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.0830
2022-08-11 01:40:57 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.2247
2022-08-11 01:41:30 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.2336
2022-08-11 01:42:04 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.1858
2022-08-11 01:42:38 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.0666
2022-08-11 01:43:11 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.0992
2022-08-11 01:43:45 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2387
2022-08-11 01:44:19 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2534
2022-08-11 01:44:53 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.1551
2022-08-11 01:45:27 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.2038
2022-08-11 01:46:01 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 1.9740
2022-08-11 01:46:34 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.2841
2022-08-11 01:47:08 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.1452
2022-08-11 01:47:42 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 1.7600
2022-08-11 01:48:16 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.0619
2022-08-11 01:48:50 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1608
2022-08-11 01:49:23 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.1474
2022-08-11 01:49:57 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 1.9961
2022-08-11 01:50:31 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.1324
2022-08-11 01:51:04 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 2.1483
2022-08-11 01:51:38 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.1861
2022-08-11 01:52:12 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.3529
2022-08-11 01:52:46 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.0637
2022-08-11 01:53:20 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 2.0836
2022-08-11 01:53:54 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.1680
2022-08-11 01:54:27 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.2960
2022-08-11 01:55:00 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.0405
2022-08-11 01:55:34 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.2790
2022-08-11 01:56:08 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.1767
2022-08-11 01:56:42 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.1907
2022-08-11 01:57:15 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.2339
2022-08-11 01:57:49 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.0571
2022-08-11 01:58:23 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.0989
2022-08-11 01:58:56 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.8700
2022-08-11 01:59:30 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.2220
2022-08-11 02:00:04 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.1467
2022-08-11 02:00:38 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.8117
2022-08-11 02:01:11 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 2.0652
2022-08-11 02:01:45 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.0971
2022-08-11 02:02:19 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.0781
2022-08-11 02:02:53 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.1300
2022-08-11 02:03:27 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 1.9233
2022-08-11 02:04:01 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.7641
2022-08-11 02:04:35 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.8764
2022-08-11 02:05:08 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.0721
2022-08-11 02:05:41 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 1.9677
2022-08-11 02:05:42 - train: epoch 011, train_loss: 2.1255
2022-08-11 02:06:58 - eval: epoch: 011, acc1: 55.974%, acc5: 80.654%, test_loss: 1.8674, per_image_load_time: 2.302ms, per_image_inference_time: 0.590ms
2022-08-11 02:06:59 - until epoch: 011, best_acc1: 56.584%
2022-08-11 02:06:59 - epoch 012 lr: 0.059368
2022-08-11 02:07:39 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.8175
2022-08-11 02:08:12 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.8438
2022-08-11 02:08:45 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 1.9938
2022-08-11 02:09:19 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.0873
2022-08-11 02:09:52 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.2694
2022-08-11 02:10:26 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.8859
2022-08-11 02:10:59 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.7655
2022-08-11 02:11:33 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.1973
2022-08-11 02:12:06 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 1.9983
2022-08-11 02:12:40 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9846
2022-08-11 02:13:13 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.4752
2022-08-11 02:13:47 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.8951
2022-08-11 02:14:21 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 2.0428
2022-08-11 02:14:54 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.2801
2022-08-11 02:15:28 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 2.0294
2022-08-11 02:16:02 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 1.9902
2022-08-11 02:16:36 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.9908
2022-08-11 02:17:10 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.1051
2022-08-11 02:17:44 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.0880
2022-08-11 02:18:18 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.1641
2022-08-11 02:18:51 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.0005
2022-08-11 02:19:25 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.3471
2022-08-11 02:19:59 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 2.0903
2022-08-11 02:20:32 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.2701
2022-08-11 02:21:06 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.9107
2022-08-11 02:21:40 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.9997
2022-08-11 02:22:14 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.1619
2022-08-11 02:22:48 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.9911
2022-08-11 02:23:22 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.0425
2022-08-11 02:23:55 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.9499
2022-08-11 02:24:29 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.2605
2022-08-11 02:25:03 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.7563
2022-08-11 02:25:37 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 1.9790
2022-08-11 02:26:11 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.0811
2022-08-11 02:26:45 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 1.9925
2022-08-11 02:27:19 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 2.0286
2022-08-11 02:27:53 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.0349
2022-08-11 02:28:27 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.0856
2022-08-11 02:29:00 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8257
2022-08-11 02:29:34 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1865
2022-08-11 02:30:08 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 1.9033
2022-08-11 02:30:43 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.9010
2022-08-11 02:31:16 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.1137
2022-08-11 02:31:50 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 2.0177
2022-08-11 02:32:24 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.9771
2022-08-11 02:32:58 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.0830
2022-08-11 02:33:32 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.9078
2022-08-11 02:34:06 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.1701
2022-08-11 02:34:40 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 2.1002
2022-08-11 02:35:13 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.8544
2022-08-11 02:35:14 - train: epoch 012, train_loss: 2.0663
2022-08-11 02:36:31 - eval: epoch: 012, acc1: 58.952%, acc5: 82.840%, test_loss: 1.7309, per_image_load_time: 2.347ms, per_image_inference_time: 0.589ms
2022-08-11 02:36:31 - until epoch: 012, best_acc1: 58.952%
2022-08-11 02:36:31 - epoch 013 lr: 0.053138
2022-08-11 02:37:11 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 2.0197
2022-08-11 02:37:45 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.7497
2022-08-11 02:38:18 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.9045
2022-08-11 02:38:51 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.9125
2022-08-11 02:39:25 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 2.0033
2022-08-11 02:39:58 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 2.2848
2022-08-11 02:40:31 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.8447
2022-08-11 02:41:05 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.0693
2022-08-11 02:41:39 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.9210
2022-08-11 02:42:13 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.9088
2022-08-11 02:42:47 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 1.9520
2022-08-11 02:43:21 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 1.9047
2022-08-11 02:43:54 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 2.0361
2022-08-11 02:44:28 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 1.9392
2022-08-11 02:45:02 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.1831
2022-08-11 02:45:36 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.9095
2022-08-11 02:46:10 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 1.9052
2022-08-11 02:46:44 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9367
2022-08-11 02:47:17 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.0554
2022-08-11 02:47:51 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.2635
2022-08-11 02:48:25 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.2181
2022-08-11 02:48:58 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.9816
2022-08-11 02:49:33 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 2.0320
2022-08-11 02:50:06 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 1.9584
2022-08-11 02:50:40 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.8832
2022-08-11 02:51:14 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.9556
2022-08-11 02:51:48 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.7927
2022-08-11 02:52:22 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.0793
2022-08-11 02:52:56 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 1.9905
2022-08-11 02:53:29 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 2.0095
2022-08-11 02:54:03 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.7626
2022-08-11 02:54:37 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 2.0651
2022-08-11 02:55:11 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.8594
2022-08-11 02:55:45 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.9612
2022-08-11 02:56:18 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.8200
2022-08-11 02:56:52 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.0947
2022-08-11 02:57:26 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.7292
2022-08-11 02:58:00 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1070
2022-08-11 02:58:34 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.1277
2022-08-11 02:59:07 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.8255
2022-08-11 02:59:41 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.9235
2022-08-11 03:00:15 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 1.9251
2022-08-11 03:00:49 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.9646
2022-08-11 03:01:23 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.9108
2022-08-11 03:01:57 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.9042
2022-08-11 03:02:31 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.0508
2022-08-11 03:03:04 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 2.0712
2022-08-11 03:03:38 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.0112
2022-08-11 03:04:12 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0816
2022-08-11 03:04:45 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.0170
2022-08-11 03:04:46 - train: epoch 013, train_loss: 2.0079
2022-08-11 03:06:03 - eval: epoch: 013, acc1: 60.200%, acc5: 83.796%, test_loss: 1.6719, per_image_load_time: 2.351ms, per_image_inference_time: 0.597ms
2022-08-11 03:06:03 - until epoch: 013, best_acc1: 60.200%
2022-08-11 03:06:03 - epoch 014 lr: 0.046859
2022-08-11 03:06:43 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.8908
2022-08-11 03:07:17 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 2.0684
2022-08-11 03:07:51 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.7614
2022-08-11 03:08:24 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.9853
2022-08-11 03:08:58 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.8353
2022-08-11 03:09:32 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.8268
2022-08-11 03:10:05 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.9780
2022-08-11 03:10:39 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.8003
2022-08-11 03:11:13 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 2.1153
2022-08-11 03:11:46 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.0209
2022-08-11 03:12:20 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.8100
2022-08-11 03:12:54 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 2.0307
2022-08-11 03:13:27 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.1527
2022-08-11 03:14:01 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.9048
2022-08-11 03:14:35 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 1.9768
2022-08-11 03:15:08 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 2.1723
2022-08-11 03:15:42 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 2.2317
2022-08-11 03:16:16 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.0620
2022-08-11 03:16:50 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.8797
2022-08-11 03:17:24 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.7812
2022-08-11 03:17:57 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.0405
2022-08-11 03:18:31 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.9403
2022-08-11 03:19:05 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.8495
2022-08-11 03:19:39 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.9915
2022-08-11 03:20:12 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.8941
2022-08-11 03:20:46 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8666
2022-08-11 03:21:20 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.8627
2022-08-11 03:21:53 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 1.9863
2022-08-11 03:22:27 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9501
2022-08-11 03:23:01 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.9273
2022-08-11 03:23:34 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 2.0165
2022-08-11 03:24:08 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 2.2102
2022-08-11 03:24:42 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.7966
2022-08-11 03:25:16 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.9094
2022-08-11 03:25:50 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 2.0802
2022-08-11 03:26:23 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.7156
2022-08-11 03:26:57 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.7906
2022-08-11 03:27:31 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 2.0111
2022-08-11 03:28:05 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.9376
2022-08-11 03:28:39 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 2.0714
2022-08-11 03:29:13 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.9301
2022-08-11 03:29:47 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.9103
2022-08-11 03:30:21 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.6356
2022-08-11 03:30:54 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 2.0387
2022-08-11 03:31:28 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.8139
2022-08-11 03:32:02 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 2.0067
2022-08-11 03:32:35 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.9649
2022-08-11 03:33:09 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.7575
2022-08-11 03:33:43 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8871
2022-08-11 03:34:16 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.9662
2022-08-11 03:34:17 - train: epoch 014, train_loss: 1.9475
2022-08-11 03:35:33 - eval: epoch: 014, acc1: 60.072%, acc5: 83.406%, test_loss: 1.7007, per_image_load_time: 2.236ms, per_image_inference_time: 0.593ms
2022-08-11 03:35:33 - until epoch: 014, best_acc1: 60.200%
2022-08-11 03:35:33 - epoch 015 lr: 0.040630
2022-08-11 03:36:14 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.7492
2022-08-11 03:36:47 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 2.0238
2022-08-11 03:37:20 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.3123
2022-08-11 03:37:54 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.9595
2022-08-11 03:38:27 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.8431
2022-08-11 03:39:01 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 1.9390
2022-08-11 03:39:35 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 2.0570
2022-08-11 03:40:08 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.8040
2022-08-11 03:40:42 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 2.1128
2022-08-11 03:41:16 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.7811
2022-08-11 03:41:50 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.6342
2022-08-11 03:42:23 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 2.0788
2022-08-11 03:42:57 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.9479
2022-08-11 03:43:31 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.8855
2022-08-11 03:44:05 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.8252
2022-08-11 03:44:39 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.8314
2022-08-11 03:45:13 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.8870
2022-08-11 03:45:46 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.8466
2022-08-11 03:46:20 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.8101
2022-08-11 03:46:54 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.7706
2022-08-11 03:47:28 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.7923
2022-08-11 03:48:01 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 2.1214
2022-08-11 03:48:35 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.8260
2022-08-11 03:49:09 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.8197
2022-08-11 03:49:43 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.9478
2022-08-11 03:50:17 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.7853
2022-08-11 03:50:50 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 2.0319
2022-08-11 03:51:24 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.9314
2022-08-11 03:51:58 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.9212
2022-08-11 03:52:32 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.8704
2022-08-11 03:53:06 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.7461
2022-08-11 03:53:40 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.8213
2022-08-11 03:54:13 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.6070
2022-08-11 03:54:47 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 2.0135
2022-08-11 03:55:21 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0320
2022-08-11 03:55:55 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.7971
2022-08-11 03:56:29 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.5467
2022-08-11 03:57:03 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.8225
2022-08-11 03:57:37 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 2.0492
2022-08-11 03:58:11 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 2.0056
2022-08-11 03:58:45 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.7382
2022-08-11 03:59:19 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6831
2022-08-11 03:59:53 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.7425
2022-08-11 04:00:27 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.7876
2022-08-11 04:01:00 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.9144
2022-08-11 04:01:34 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.9042
2022-08-11 04:02:08 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.9187
2022-08-11 04:02:42 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.7761
2022-08-11 04:03:16 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.8202
2022-08-11 04:03:49 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.8972
2022-08-11 04:03:50 - train: epoch 015, train_loss: 1.8859
2022-08-11 04:05:06 - eval: epoch: 015, acc1: 62.076%, acc5: 84.942%, test_loss: 1.5825, per_image_load_time: 2.333ms, per_image_inference_time: 0.581ms
2022-08-11 04:05:06 - until epoch: 015, best_acc1: 62.076%
2022-08-11 04:05:06 - epoch 016 lr: 0.034548
2022-08-11 04:05:47 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.7505
2022-08-11 04:06:20 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.7858
2022-08-11 04:06:54 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.8580
2022-08-11 04:07:28 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.1565
2022-08-11 04:08:01 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.7355
2022-08-11 04:08:35 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.8143
2022-08-11 04:09:08 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.6656
2022-08-11 04:09:42 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.8569
2022-08-11 04:10:16 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 2.0266
2022-08-11 04:10:50 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.6492
2022-08-11 04:11:23 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.7880
2022-08-11 04:11:57 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.8894
2022-08-11 04:12:31 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.9034
2022-08-11 04:13:04 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.8153
2022-08-11 04:13:38 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.8837
2022-08-11 04:14:12 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.7796
2022-08-11 04:14:46 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.8230
2022-08-11 04:15:20 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.9645
2022-08-11 04:15:53 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.8101
2022-08-11 04:16:27 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.5628
2022-08-11 04:17:01 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8021
2022-08-11 04:17:35 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.8700
2022-08-11 04:18:09 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.9849
2022-08-11 04:18:42 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.8216
2022-08-11 04:19:16 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.8336
2022-08-11 04:19:49 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.9838
2022-08-11 04:20:23 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.8309
2022-08-11 04:20:56 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.8376
2022-08-11 04:21:30 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.6493
2022-08-11 04:22:04 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 2.0889
2022-08-11 04:22:38 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.9759
2022-08-11 04:23:12 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.7342
2022-08-11 04:23:46 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9781
2022-08-11 04:24:20 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.8131
2022-08-11 04:24:54 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.9354
2022-08-11 04:25:28 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.6175
2022-08-11 04:26:01 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.7104
2022-08-11 04:26:35 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.0587
2022-08-11 04:27:09 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.8696
2022-08-11 04:27:42 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.9125
2022-08-11 04:28:16 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.8753
2022-08-11 04:28:49 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.7441
2022-08-11 04:29:23 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.8789
2022-08-11 04:29:57 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.5457
2022-08-11 04:30:31 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.9459
2022-08-11 04:31:05 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.7254
2022-08-11 04:31:39 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.9322
2022-08-11 04:32:13 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.7484
2022-08-11 04:32:46 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.8399
2022-08-11 04:33:19 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.8714
2022-08-11 04:33:20 - train: epoch 016, train_loss: 1.8211
2022-08-11 04:34:37 - eval: epoch: 016, acc1: 63.258%, acc5: 85.718%, test_loss: 1.5135, per_image_load_time: 2.323ms, per_image_inference_time: 0.593ms
2022-08-11 04:34:37 - until epoch: 016, best_acc1: 63.258%
2022-08-11 04:34:37 - epoch 017 lr: 0.028710
2022-08-11 04:35:17 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.8760
2022-08-11 04:35:51 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.9123
2022-08-11 04:36:24 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.9793
2022-08-11 04:36:58 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.5563
2022-08-11 04:37:32 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.8410
2022-08-11 04:38:06 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 1.9276
2022-08-11 04:38:39 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.7738
2022-08-11 04:39:12 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.7698
2022-08-11 04:39:46 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.6826
2022-08-11 04:40:20 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.8021
2022-08-11 04:40:54 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 2.2365
2022-08-11 04:41:27 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.6981
2022-08-11 04:42:01 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.7344
2022-08-11 04:42:35 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.7386
2022-08-11 04:43:09 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.6220
2022-08-11 04:43:42 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.4725
2022-08-11 04:44:16 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.6277
2022-08-11 04:44:50 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.6653
2022-08-11 04:45:24 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.6823
2022-08-11 04:45:58 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.6967
2022-08-11 04:46:32 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.5959
2022-08-11 04:47:06 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.5988
2022-08-11 04:47:40 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.8503
2022-08-11 04:48:13 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.7277
2022-08-11 04:48:47 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.9176
2022-08-11 04:49:21 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.8161
2022-08-11 04:49:55 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6238
2022-08-11 04:50:29 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.7057
2022-08-11 04:51:03 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.8728
2022-08-11 04:51:36 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.6457
2022-08-11 04:52:10 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8836
2022-08-11 04:52:44 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.6701
2022-08-11 04:53:18 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8985
2022-08-11 04:53:52 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.7183
2022-08-11 04:54:26 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.6729
2022-08-11 04:55:00 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.7425
2022-08-11 04:55:34 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.8075
2022-08-11 04:56:08 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.5762
2022-08-11 04:56:42 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.6797
2022-08-11 04:57:16 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.8621
2022-08-11 04:57:50 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.7342
2022-08-11 04:58:24 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.4917
2022-08-11 04:58:57 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.6934
2022-08-11 04:59:31 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8653
2022-08-11 05:00:06 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.8110
2022-08-11 05:00:40 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.6314
2022-08-11 05:01:14 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 2.0280
2022-08-11 05:01:48 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.8519
2022-08-11 05:02:22 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.6389
2022-08-11 05:02:55 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.5119
2022-08-11 05:02:56 - train: epoch 017, train_loss: 1.7565
2022-08-11 05:04:12 - eval: epoch: 017, acc1: 64.866%, acc5: 86.648%, test_loss: 1.4484, per_image_load_time: 2.320ms, per_image_inference_time: 0.602ms
2022-08-11 05:04:12 - until epoch: 017, best_acc1: 64.866%
2022-08-11 05:04:12 - epoch 018 lr: 0.023208
2022-08-11 05:04:52 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.6696
2022-08-11 05:05:26 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.7639
2022-08-11 05:05:59 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.8710
2022-08-11 05:06:33 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.7697
2022-08-11 05:07:06 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.7567
2022-08-11 05:07:40 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.7681
2022-08-11 05:08:13 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.5076
2022-08-11 05:08:46 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.6217
2022-08-11 05:09:19 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.6526
2022-08-11 05:09:53 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.6167
2022-08-11 05:10:27 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.8801
2022-08-11 05:11:01 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.6424
2022-08-11 05:11:34 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.8596
2022-08-11 05:12:08 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.8051
2022-08-11 05:12:42 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.8930
2022-08-11 05:13:16 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.5767
2022-08-11 05:13:50 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.6102
2022-08-11 05:14:24 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6946
2022-08-11 05:14:57 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.6612
2022-08-11 05:15:31 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.9377
2022-08-11 05:16:05 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.7025
2022-08-11 05:16:38 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.9259
2022-08-11 05:17:12 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6345
2022-08-11 05:17:46 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.6164
2022-08-11 05:18:19 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.4703
2022-08-11 05:18:53 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.6036
2022-08-11 05:19:27 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 2.0764
2022-08-11 05:20:00 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.6241
2022-08-11 05:20:34 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.5063
2022-08-11 05:21:08 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.6343
2022-08-11 05:21:42 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.9609
2022-08-11 05:22:16 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.6490
2022-08-11 05:22:50 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.4250
2022-08-11 05:23:24 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.7125
2022-08-11 05:23:58 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.6353
2022-08-11 05:24:32 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.6543
2022-08-11 05:25:06 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.9253
2022-08-11 05:25:39 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.7106
2022-08-11 05:26:13 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.9325
2022-08-11 05:26:47 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.7017
2022-08-11 05:27:21 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.8929
2022-08-11 05:27:55 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.7110
2022-08-11 05:28:29 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.5669
2022-08-11 05:29:02 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.7759
2022-08-11 05:29:36 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.5414
2022-08-11 05:30:11 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.7440
2022-08-11 05:30:45 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.8585
2022-08-11 05:31:19 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.8913
2022-08-11 05:31:52 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.5153
2022-08-11 05:32:25 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.9381
2022-08-11 05:32:26 - train: epoch 018, train_loss: 1.6817
2022-08-11 05:33:43 - eval: epoch: 018, acc1: 65.804%, acc5: 87.060%, test_loss: 1.4154, per_image_load_time: 1.546ms, per_image_inference_time: 0.592ms
2022-08-11 05:33:43 - until epoch: 018, best_acc1: 65.804%
2022-08-11 05:33:43 - epoch 019 lr: 0.018128
2022-08-11 05:34:23 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4335
2022-08-11 05:34:57 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.6087
2022-08-11 05:35:30 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.4994
2022-08-11 05:36:03 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.5517
2022-08-11 05:36:37 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.7672
2022-08-11 05:37:10 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.7370
2022-08-11 05:37:44 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.4306
2022-08-11 05:38:18 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.7085
2022-08-11 05:38:52 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.8063
2022-08-11 05:39:26 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.7518
2022-08-11 05:40:00 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5988
2022-08-11 05:40:34 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.5140
2022-08-11 05:41:07 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.4473
2022-08-11 05:41:41 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.5059
2022-08-11 05:42:15 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.8629
2022-08-11 05:42:48 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.6507
2022-08-11 05:43:22 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.8142
2022-08-11 05:43:56 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.5527
2022-08-11 05:44:30 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.6792
2022-08-11 05:45:04 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.3847
2022-08-11 05:45:38 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.5076
2022-08-11 05:46:12 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.6257
2022-08-11 05:46:45 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.6600
2022-08-11 05:47:19 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.6654
2022-08-11 05:47:53 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.6246
2022-08-11 05:48:26 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.6795
2022-08-11 05:49:00 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.5612
2022-08-11 05:49:34 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.5259
2022-08-11 05:50:08 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.8102
2022-08-11 05:50:42 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.8255
2022-08-11 05:51:15 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.6107
2022-08-11 05:51:49 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.3513
2022-08-11 05:52:23 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.5016
2022-08-11 05:52:57 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.6061
2022-08-11 05:53:31 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.8460
2022-08-11 05:54:04 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.3680
2022-08-11 05:54:38 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.6039
2022-08-11 05:55:12 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.7447
2022-08-11 05:55:46 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.4614
2022-08-11 05:56:20 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.5154
2022-08-11 05:56:54 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.6114
2022-08-11 05:57:27 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.6651
2022-08-11 05:58:01 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.5479
2022-08-11 05:58:35 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.6658
2022-08-11 05:59:09 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.7857
2022-08-11 05:59:43 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.4821
2022-08-11 06:00:17 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.6626
2022-08-11 06:00:51 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.6273
2022-08-11 06:01:25 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.4672
2022-08-11 06:01:58 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.5370
2022-08-11 06:01:59 - train: epoch 019, train_loss: 1.6105
2022-08-11 06:03:16 - eval: epoch: 019, acc1: 66.646%, acc5: 87.664%, test_loss: 1.3689, per_image_load_time: 2.334ms, per_image_inference_time: 0.584ms
2022-08-11 06:03:16 - until epoch: 019, best_acc1: 66.646%
2022-08-11 06:03:16 - epoch 020 lr: 0.013551
2022-08-11 06:03:57 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.5412
2022-08-11 06:04:30 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.4136
2022-08-11 06:05:04 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.4835
2022-08-11 06:05:37 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.4860
2022-08-11 06:06:11 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.4105
2022-08-11 06:06:44 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.6296
2022-08-11 06:07:18 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.3516
2022-08-11 06:07:52 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.6585
2022-08-11 06:08:26 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.6748
2022-08-11 06:09:01 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.6722
2022-08-11 06:09:34 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.5026
2022-08-11 06:10:08 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.2970
2022-08-11 06:10:42 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.5286
2022-08-11 06:11:16 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.3885
2022-08-11 06:11:50 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.6520
2022-08-11 06:12:24 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.7314
2022-08-11 06:12:58 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.3607
2022-08-11 06:13:32 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.5672
2022-08-11 06:14:06 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.4305
2022-08-11 06:14:40 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.4969
2022-08-11 06:15:14 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.6130
2022-08-11 06:15:48 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.3986
2022-08-11 06:16:22 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.5158
2022-08-11 06:16:56 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.8421
2022-08-11 06:17:30 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.4578
2022-08-11 06:18:03 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.4059
2022-08-11 06:18:37 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.4976
2022-08-11 06:19:11 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.6825
2022-08-11 06:19:45 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.6579
2022-08-11 06:20:19 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.6866
2022-08-11 06:20:53 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.6386
2022-08-11 06:21:27 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.7756
2022-08-11 06:22:01 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.4098
2022-08-11 06:22:35 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.5860
2022-08-11 06:23:09 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.4959
2022-08-11 06:23:43 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.5364
2022-08-11 06:24:17 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.3155
2022-08-11 06:24:51 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.6108
2022-08-11 06:25:25 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.8619
2022-08-11 06:25:59 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.4211
2022-08-11 06:26:33 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.5725
2022-08-11 06:27:07 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.6156
2022-08-11 06:27:41 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.6722
2022-08-11 06:28:15 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.4370
2022-08-11 06:28:48 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.5197
2022-08-11 06:29:22 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.4311
2022-08-11 06:29:56 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.5382
2022-08-11 06:30:30 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4749
2022-08-11 06:31:04 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.3379
2022-08-11 06:31:37 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.3358
2022-08-11 06:31:39 - train: epoch 020, train_loss: 1.5385
2022-08-11 06:32:55 - eval: epoch: 020, acc1: 68.608%, acc5: 88.740%, test_loss: 1.2839, per_image_load_time: 2.386ms, per_image_inference_time: 0.582ms
2022-08-11 06:32:56 - until epoch: 020, best_acc1: 68.608%
2022-08-11 06:32:56 - epoch 021 lr: 0.009548
2022-08-11 06:33:35 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.5036
2022-08-11 06:34:09 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.4158
2022-08-11 06:34:42 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.3502
2022-08-11 06:35:15 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.3896
2022-08-11 06:35:49 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.5002
2022-08-11 06:36:22 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.3925
2022-08-11 06:36:55 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3365
2022-08-11 06:37:29 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.6244
2022-08-11 06:38:02 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.3281
2022-08-11 06:38:36 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3696
2022-08-11 06:39:10 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.5027
2022-08-11 06:39:43 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.3808
2022-08-11 06:40:17 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.4738
2022-08-11 06:40:50 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.1306
2022-08-11 06:41:24 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.4612
2022-08-11 06:41:57 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.5491
2022-08-11 06:42:31 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5682
2022-08-11 06:43:04 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.5831
2022-08-11 06:43:38 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.5280
2022-08-11 06:44:12 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.4664
2022-08-11 06:44:46 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.2786
2022-08-11 06:45:20 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.4610
2022-08-11 06:45:54 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.3416
2022-08-11 06:46:28 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.4262
2022-08-11 06:47:02 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.4466
2022-08-11 06:47:36 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.4228
2022-08-11 06:48:10 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.3779
2022-08-11 06:48:43 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.3813
2022-08-11 06:49:17 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.3474
2022-08-11 06:49:51 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.5226
2022-08-11 06:50:25 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.2823
2022-08-11 06:50:59 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.3084
2022-08-11 06:51:33 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.6761
2022-08-11 06:52:06 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.5990
2022-08-11 06:52:40 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4165
2022-08-11 06:53:14 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.3510
2022-08-11 06:53:48 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.4741
2022-08-11 06:54:22 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.4044
2022-08-11 06:54:56 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.5214
2022-08-11 06:55:30 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.7059
2022-08-11 06:56:04 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.4627
2022-08-11 06:56:37 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.4761
2022-08-11 06:57:11 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.4588
2022-08-11 06:57:45 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.5510
2022-08-11 06:58:19 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.4235
2022-08-11 06:58:53 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.4703
2022-08-11 06:59:26 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.6181
2022-08-11 07:00:00 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.4932
2022-08-11 07:00:34 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.2853
2022-08-11 07:01:07 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.4338
2022-08-11 07:01:09 - train: epoch 021, train_loss: 1.4657
2022-08-11 07:02:25 - eval: epoch: 021, acc1: 69.212%, acc5: 89.182%, test_loss: 1.2462, per_image_load_time: 2.366ms, per_image_inference_time: 0.559ms
2022-08-11 07:02:25 - until epoch: 021, best_acc1: 69.212%
2022-08-11 07:02:25 - epoch 022 lr: 0.006184
2022-08-11 07:03:05 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.2920
2022-08-11 07:03:38 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.3877
2022-08-11 07:04:12 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.3684
2022-08-11 07:04:45 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.4231
2022-08-11 07:05:19 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.2816
2022-08-11 07:05:53 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.5250
2022-08-11 07:06:27 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.6049
2022-08-11 07:07:01 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.6978
2022-08-11 07:07:35 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.5087
2022-08-11 07:08:08 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.5401
2022-08-11 07:08:42 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3578
2022-08-11 07:09:16 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.1624
2022-08-11 07:09:49 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.3622
2022-08-11 07:10:23 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.1295
2022-08-11 07:10:57 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.4722
2022-08-11 07:11:31 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.3197
2022-08-11 07:12:04 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.4942
2022-08-11 07:12:38 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.5755
2022-08-11 07:13:12 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.3424
2022-08-11 07:13:45 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4830
2022-08-11 07:14:19 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.3994
2022-08-11 07:14:53 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.2887
2022-08-11 07:15:26 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.3927
2022-08-11 07:16:00 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.4956
2022-08-11 07:16:34 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.3047
2022-08-11 07:17:07 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.2577
2022-08-11 07:17:41 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.2990
2022-08-11 07:18:15 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.3086
2022-08-11 07:18:49 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.3506
2022-08-11 07:19:23 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.3661
2022-08-11 07:19:57 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.5007
2022-08-11 07:20:31 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4450
2022-08-11 07:21:05 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.3035
2022-08-11 07:21:39 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.3718
2022-08-11 07:22:12 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.4190
2022-08-11 07:22:46 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.6358
2022-08-11 07:23:19 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.3642
2022-08-11 07:23:53 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.6808
2022-08-11 07:24:27 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.3062
2022-08-11 07:25:01 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.4903
2022-08-11 07:25:35 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.2764
2022-08-11 07:26:09 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.2966
2022-08-11 07:26:43 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4590
2022-08-11 07:27:16 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3486
2022-08-11 07:27:50 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.3427
2022-08-11 07:28:24 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.5224
2022-08-11 07:28:58 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.3594
2022-08-11 07:29:32 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.2850
2022-08-11 07:30:06 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.4236
2022-08-11 07:30:39 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.3107
2022-08-11 07:30:40 - train: epoch 022, train_loss: 1.3994
2022-08-11 07:31:57 - eval: epoch: 022, acc1: 70.476%, acc5: 89.864%, test_loss: 1.1969, per_image_load_time: 1.494ms, per_image_inference_time: 0.605ms
2022-08-11 07:31:57 - until epoch: 022, best_acc1: 70.476%
2022-08-11 07:31:57 - epoch 023 lr: 0.003511
2022-08-11 07:32:37 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.3654
2022-08-11 07:33:10 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.3550
2022-08-11 07:33:44 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.5923
2022-08-11 07:34:17 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.5152
2022-08-11 07:34:50 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.4366
2022-08-11 07:35:24 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.2718
2022-08-11 07:35:58 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 0.9934
2022-08-11 07:36:31 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.4227
2022-08-11 07:37:05 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.2992
2022-08-11 07:37:39 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.1244
2022-08-11 07:38:13 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.3107
2022-08-11 07:38:47 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.3553
2022-08-11 07:39:20 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.5386
2022-08-11 07:39:54 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.3528
2022-08-11 07:40:28 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.2887
2022-08-11 07:41:02 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.2474
2022-08-11 07:41:35 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.2463
2022-08-11 07:42:09 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.2964
2022-08-11 07:42:43 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.2069
2022-08-11 07:43:17 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.1496
2022-08-11 07:43:51 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.3438
2022-08-11 07:44:25 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.2523
2022-08-11 07:44:59 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.1785
2022-08-11 07:45:33 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.2107
2022-08-11 07:46:07 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.3394
2022-08-11 07:46:41 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.4823
2022-08-11 07:47:15 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.5275
2022-08-11 07:47:48 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.5340
2022-08-11 07:48:22 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.3814
2022-08-11 07:48:56 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.4334
2022-08-11 07:49:30 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.4431
2022-08-11 07:50:04 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.4251
2022-08-11 07:50:38 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.5126
2022-08-11 07:51:11 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.5330
2022-08-11 07:51:45 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.3570
2022-08-11 07:52:19 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.3993
2022-08-11 07:52:52 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.2732
2022-08-11 07:53:26 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.4097
2022-08-11 07:54:00 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3780
2022-08-11 07:54:33 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.3322
2022-08-11 07:55:07 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.3758
2022-08-11 07:55:41 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.1335
2022-08-11 07:56:15 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.2844
2022-08-11 07:56:48 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.3145
2022-08-11 07:57:22 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.1845
2022-08-11 07:57:55 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.4556
2022-08-11 07:58:29 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.1667
2022-08-11 07:59:03 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.1917
2022-08-11 07:59:37 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.2432
2022-08-11 08:00:09 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.5335
2022-08-11 08:00:11 - train: epoch 023, train_loss: 1.3459
2022-08-11 08:01:27 - eval: epoch: 023, acc1: 71.094%, acc5: 90.198%, test_loss: 1.1686, per_image_load_time: 1.788ms, per_image_inference_time: 0.607ms
2022-08-11 08:01:27 - until epoch: 023, best_acc1: 71.094%
2022-08-11 08:01:27 - epoch 024 lr: 0.001571
2022-08-11 08:02:07 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.4523
2022-08-11 08:02:40 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.2417
2022-08-11 08:03:14 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.2662
2022-08-11 08:03:47 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.3017
2022-08-11 08:04:20 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.2846
2022-08-11 08:04:53 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.2001
2022-08-11 08:05:27 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.3370
2022-08-11 08:06:00 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.3382
2022-08-11 08:06:34 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.1292
2022-08-11 08:07:08 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.3049
2022-08-11 08:07:42 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.1304
2022-08-11 08:08:15 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.2594
2022-08-11 08:08:49 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.5966
2022-08-11 08:09:23 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.3141
2022-08-11 08:09:57 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.4418
2022-08-11 08:10:30 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.2103
2022-08-11 08:11:04 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.1749
2022-08-11 08:11:38 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.3983
2022-08-11 08:12:12 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.1397
2022-08-11 08:12:46 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.5352
2022-08-11 08:13:20 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.1042
2022-08-11 08:13:54 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.2945
2022-08-11 08:14:28 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.3331
2022-08-11 08:15:02 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.2884
2022-08-11 08:15:36 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.4877
2022-08-11 08:16:10 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.3685
2022-08-11 08:16:44 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.4136
2022-08-11 08:17:18 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.3200
2022-08-11 08:17:52 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.2159
2022-08-11 08:18:26 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.3189
2022-08-11 08:19:00 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.3647
2022-08-11 08:19:33 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.2574
2022-08-11 08:20:07 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.2233
2022-08-11 08:20:41 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.3401
2022-08-11 08:21:15 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.2944
2022-08-11 08:21:49 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.2890
2022-08-11 08:22:23 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.3355
2022-08-11 08:22:57 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.4759
2022-08-11 08:23:31 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.3616
2022-08-11 08:24:06 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.3679
2022-08-11 08:24:40 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.1332
2022-08-11 08:25:14 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.1410
2022-08-11 08:25:47 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.4351
2022-08-11 08:26:21 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.3119
2022-08-11 08:26:55 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.2914
2022-08-11 08:27:29 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.4011
2022-08-11 08:28:03 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.1537
2022-08-11 08:28:37 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.1199
2022-08-11 08:29:11 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.4956
2022-08-11 08:29:44 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.2402
2022-08-11 08:29:45 - train: epoch 024, train_loss: 1.3043
2022-08-11 08:31:02 - eval: epoch: 024, acc1: 71.558%, acc5: 90.356%, test_loss: 1.1545, per_image_load_time: 2.365ms, per_image_inference_time: 0.583ms
2022-08-11 08:31:02 - until epoch: 024, best_acc1: 71.558%
2022-08-11 08:31:02 - epoch 025 lr: 0.000394
2022-08-11 08:31:43 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.0226
2022-08-11 08:32:16 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.0269
2022-08-11 08:32:50 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.2702
2022-08-11 08:33:23 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.1743
2022-08-11 08:33:57 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.2256
2022-08-11 08:34:31 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3005
2022-08-11 08:35:04 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.3250
2022-08-11 08:35:38 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.3800
2022-08-11 08:36:11 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.1502
2022-08-11 08:36:45 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3994
2022-08-11 08:37:19 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.3469
2022-08-11 08:37:53 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.1547
2022-08-11 08:38:26 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3298
2022-08-11 08:38:59 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.4986
2022-08-11 08:39:33 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.1928
2022-08-11 08:40:07 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1364
2022-08-11 08:40:41 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.2015
2022-08-11 08:41:14 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 0.9844
2022-08-11 08:41:48 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.2013
2022-08-11 08:42:22 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.4944
2022-08-11 08:42:56 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.1711
2022-08-11 08:43:30 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.1560
2022-08-11 08:44:04 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.1819
2022-08-11 08:44:38 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.2517
2022-08-11 08:45:12 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.2327
2022-08-11 08:45:46 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.3269
2022-08-11 08:46:20 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.3082
2022-08-11 08:46:53 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.3121
2022-08-11 08:47:27 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.5183
2022-08-11 08:48:01 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.3092
2022-08-11 08:48:35 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.2273
2022-08-11 08:49:09 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.2709
2022-08-11 08:49:42 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.3041
2022-08-11 08:50:16 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.4028
2022-08-11 08:50:50 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1234
2022-08-11 08:51:24 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.1698
2022-08-11 08:51:58 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.2058
2022-08-11 08:52:31 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3927
2022-08-11 08:53:04 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.5552
2022-08-11 08:53:38 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.3661
2022-08-11 08:54:12 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.4243
2022-08-11 08:54:46 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.3865
2022-08-11 08:55:20 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.1517
2022-08-11 08:55:53 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.0782
2022-08-11 08:56:27 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.2639
2022-08-11 08:57:01 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.1616
2022-08-11 08:57:35 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.1589
2022-08-11 08:58:09 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.0530
2022-08-11 08:58:43 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.3481
2022-08-11 08:59:16 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.4865
2022-08-11 08:59:17 - train: epoch 025, train_loss: 1.2878
2022-08-11 09:00:33 - eval: epoch: 025, acc1: 71.624%, acc5: 90.442%, test_loss: 1.1489, per_image_load_time: 2.319ms, per_image_inference_time: 0.592ms
2022-08-11 09:00:33 - until epoch: 025, best_acc1: 71.624%
2022-08-11 09:00:33 - train done. train time: 12.292 hours, best_acc1: 71.624%
