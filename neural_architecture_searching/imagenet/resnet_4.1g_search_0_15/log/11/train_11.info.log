2022-08-12 09:16:59 - net_idx: 11
2022-08-12 09:16:59 - net_config: {'stem_width': 64, 'depth': 13, 'w_0': 48, 'w_a': 19.256788602705857, 'w_m': 1.6998302426794085}
2022-08-12 09:16:59 - num_classes: 1000
2022-08-12 09:16:59 - input_image_size: 224
2022-08-12 09:16:59 - scale: 1.1428571428571428
2022-08-12 09:16:59 - seed: 0
2022-08-12 09:16:59 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-12 09:16:59 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-12 09:16:59 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce2b0>
2022-08-12 09:16:59 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce6d0>
2022-08-12 09:16:59 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce700>
2022-08-12 09:16:59 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce760>
2022-08-12 09:16:59 - batch_size: 256
2022-08-12 09:16:59 - num_workers: 16
2022-08-12 09:16:59 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-12 09:16:59 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-12 09:16:59 - epochs: 25
2022-08-12 09:16:59 - print_interval: 100
2022-08-12 09:16:59 - accumulation_steps: 1
2022-08-12 09:16:59 - sync_bn: False
2022-08-12 09:16:59 - apex: True
2022-08-12 09:16:59 - use_ema_model: False
2022-08-12 09:16:59 - ema_model_decay: 0.9999
2022-08-12 09:16:59 - log_dir: ./log
2022-08-12 09:16:59 - checkpoint_dir: ./checkpoints
2022-08-12 09:16:59 - gpus_type: NVIDIA RTX A5000
2022-08-12 09:16:59 - gpus_num: 2
2022-08-12 09:16:59 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5dddaeb570>
2022-08-12 09:16:59 - ema_model: None
2022-08-12 09:16:59 - --------------------parameters--------------------
2022-08-12 09:16:59 - name: conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-12 09:16:59 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-12 09:16:59 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-12 09:16:59 - name: fc.weight, grad: True
2022-08-12 09:16:59 - name: fc.bias, grad: True
2022-08-12 09:16:59 - --------------------buffers--------------------
2022-08-12 09:16:59 - name: conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-12 09:16:59 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-12 09:16:59 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 09:16:59 - -----------no weight decay layers--------------
2022-08-12 09:16:59 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 09:16:59 - -------------weight decay layers---------------
2022-08-12 09:16:59 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 09:16:59 - epoch 001 lr: 0.100000
2022-08-12 09:17:39 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9084
2022-08-12 09:18:12 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9074
2022-08-12 09:18:45 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8793
2022-08-12 09:19:18 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.8615
2022-08-12 09:19:52 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.7765
2022-08-12 09:20:26 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.6257
2022-08-12 09:20:59 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.6902
2022-08-12 09:21:32 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.5896
2022-08-12 09:22:05 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.4733
2022-08-12 09:22:38 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.4298
2022-08-12 09:23:11 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.3269
2022-08-12 09:23:44 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.1695
2022-08-12 09:24:18 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.1275
2022-08-12 09:24:51 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.0389
2022-08-12 09:25:25 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 6.0233
2022-08-12 09:25:58 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.1215
2022-08-12 09:26:32 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.7888
2022-08-12 09:27:05 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.8315
2022-08-12 09:27:39 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.7119
2022-08-12 09:28:12 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.6262
2022-08-12 09:28:46 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.5675
2022-08-12 09:29:20 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.5085
2022-08-12 09:29:53 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.3952
2022-08-12 09:30:26 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.3991
2022-08-12 09:31:00 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.4719
2022-08-12 09:31:33 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.4538
2022-08-12 09:32:07 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.3370
2022-08-12 09:32:40 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.2331
2022-08-12 09:33:14 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.1469
2022-08-12 09:33:47 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.2552
2022-08-12 09:34:21 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.1833
2022-08-12 09:34:55 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.1241
2022-08-12 09:35:28 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 5.0645
2022-08-12 09:36:02 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.7934
2022-08-12 09:36:35 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.7839
2022-08-12 09:37:08 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.8344
2022-08-12 09:37:42 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.8450
2022-08-12 09:38:15 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.7184
2022-08-12 09:38:49 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.8072
2022-08-12 09:39:22 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.6982
2022-08-12 09:39:55 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.7362
2022-08-12 09:40:29 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.6297
2022-08-12 09:41:02 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.5759
2022-08-12 09:41:36 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.3226
2022-08-12 09:42:10 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.5055
2022-08-12 09:42:43 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.6696
2022-08-12 09:43:16 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.4673
2022-08-12 09:43:50 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.5384
2022-08-12 09:44:24 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.5643
2022-08-12 09:44:57 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.2756
2022-08-12 09:44:58 - train: epoch 001, train_loss: 5.4952
2022-08-12 09:46:13 - eval: epoch: 001, acc1: 17.282%, acc5: 38.564%, test_loss: 4.3888, per_image_load_time: 2.004ms, per_image_inference_time: 0.635ms
2022-08-12 09:46:13 - until epoch: 001, best_acc1: 17.282%
2022-08-12 09:46:13 - epoch 002 lr: 0.099606
2022-08-12 09:46:54 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.1678
2022-08-12 09:47:26 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.2408
2022-08-12 09:47:59 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.2637
2022-08-12 09:48:32 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.3580
2022-08-12 09:49:04 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.2191
2022-08-12 09:49:38 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.0176
2022-08-12 09:50:11 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.3406
2022-08-12 09:50:44 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 4.0133
2022-08-12 09:51:18 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.9814
2022-08-12 09:51:51 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.1157
2022-08-12 09:52:24 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.0216
2022-08-12 09:52:57 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.9451
2022-08-12 09:53:31 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 4.0576
2022-08-12 09:54:04 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.0981
2022-08-12 09:54:38 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 4.0609
2022-08-12 09:55:11 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.8453
2022-08-12 09:55:44 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.9322
2022-08-12 09:56:17 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.9195
2022-08-12 09:56:50 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.7610
2022-08-12 09:57:24 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.5778
2022-08-12 09:57:57 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 4.0241
2022-08-12 09:58:31 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.7594
2022-08-12 09:59:04 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.6455
2022-08-12 09:59:38 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.6183
2022-08-12 10:00:11 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.5090
2022-08-12 10:00:44 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.8318
2022-08-12 10:01:17 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.8461
2022-08-12 10:01:51 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.8398
2022-08-12 10:02:24 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.6916
2022-08-12 10:02:57 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.6229
2022-08-12 10:03:31 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.6415
2022-08-12 10:04:04 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.5870
2022-08-12 10:04:38 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.6763
2022-08-12 10:05:11 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.6365
2022-08-12 10:05:45 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.5852
2022-08-12 10:06:18 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.7182
2022-08-12 10:06:52 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.6069
2022-08-12 10:07:25 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.4611
2022-08-12 10:07:59 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.6313
2022-08-12 10:08:32 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.2488
2022-08-12 10:09:06 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.5616
2022-08-12 10:09:39 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.6071
2022-08-12 10:10:12 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.2881
2022-08-12 10:10:45 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.1827
2022-08-12 10:11:19 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.2904
2022-08-12 10:11:52 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.1584
2022-08-12 10:12:25 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.4495
2022-08-12 10:12:58 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.3989
2022-08-12 10:13:32 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.3812
2022-08-12 10:14:04 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.4698
2022-08-12 10:14:06 - train: epoch 002, train_loss: 3.7510
2022-08-12 10:15:22 - eval: epoch: 002, acc1: 31.412%, acc5: 57.944%, test_loss: 3.3694, per_image_load_time: 2.332ms, per_image_inference_time: 0.610ms
2022-08-12 10:15:22 - until epoch: 002, best_acc1: 31.412%
2022-08-12 10:15:22 - epoch 003 lr: 0.098429
2022-08-12 10:16:02 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.3882
2022-08-12 10:16:35 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.2594
2022-08-12 10:17:08 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.4335
2022-08-12 10:17:42 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.3812
2022-08-12 10:18:15 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.3044
2022-08-12 10:18:48 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 2.9674
2022-08-12 10:19:21 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.3254
2022-08-12 10:19:55 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.3838
2022-08-12 10:20:28 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.2428
2022-08-12 10:21:02 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.1770
2022-08-12 10:21:35 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.2102
2022-08-12 10:22:08 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.1008
2022-08-12 10:22:41 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.1171
2022-08-12 10:23:15 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.1804
2022-08-12 10:23:49 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.2999
2022-08-12 10:24:22 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.0644
2022-08-12 10:24:56 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.0479
2022-08-12 10:25:29 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.0297
2022-08-12 10:26:03 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.2246
2022-08-12 10:26:36 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.0874
2022-08-12 10:27:09 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.1678
2022-08-12 10:27:43 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.4783
2022-08-12 10:28:16 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.9867
2022-08-12 10:28:50 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.0398
2022-08-12 10:29:23 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 2.9525
2022-08-12 10:29:57 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.2944
2022-08-12 10:30:30 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.6080
2022-08-12 10:31:04 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 2.9455
2022-08-12 10:31:38 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.9256
2022-08-12 10:32:11 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.1644
2022-08-12 10:32:45 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.2278
2022-08-12 10:33:18 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 2.9926
2022-08-12 10:33:52 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 2.8950
2022-08-12 10:34:25 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.0761
2022-08-12 10:34:59 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.9383
2022-08-12 10:35:33 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.9113
2022-08-12 10:36:06 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.0327
2022-08-12 10:36:40 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.1336
2022-08-12 10:37:14 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.3187
2022-08-12 10:37:48 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7288
2022-08-12 10:38:21 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 3.0400
2022-08-12 10:38:55 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.1199
2022-08-12 10:39:28 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.7729
2022-08-12 10:40:02 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.8585
2022-08-12 10:40:35 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.0115
2022-08-12 10:41:09 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.8598
2022-08-12 10:41:43 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 3.0265
2022-08-12 10:42:17 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.0840
2022-08-12 10:42:50 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.2724
2022-08-12 10:43:22 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 3.0319
2022-08-12 10:43:24 - train: epoch 003, train_loss: 3.1083
2022-08-12 10:44:39 - eval: epoch: 003, acc1: 38.144%, acc5: 65.210%, test_loss: 2.8009, per_image_load_time: 2.022ms, per_image_inference_time: 0.646ms
2022-08-12 10:44:39 - until epoch: 003, best_acc1: 38.144%
2022-08-12 10:44:39 - epoch 004 lr: 0.096488
2022-08-12 10:45:19 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 3.0334
2022-08-12 10:45:52 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.8543
2022-08-12 10:46:25 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.8488
2022-08-12 10:46:59 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.8121
2022-08-12 10:47:32 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.6133
2022-08-12 10:48:05 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.1454
2022-08-12 10:48:39 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.9286
2022-08-12 10:49:12 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.9189
2022-08-12 10:49:45 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.5277
2022-08-12 10:50:19 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.7304
2022-08-12 10:50:53 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.0661
2022-08-12 10:51:27 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.7562
2022-08-12 10:52:00 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.6464
2022-08-12 10:52:33 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.9032
2022-08-12 10:53:07 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 3.0267
2022-08-12 10:53:41 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.7031
2022-08-12 10:54:14 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.7590
2022-08-12 10:54:48 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.9380
2022-08-12 10:55:22 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.8402
2022-08-12 10:55:56 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.8868
2022-08-12 10:56:29 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.8315
2022-08-12 10:57:03 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.8108
2022-08-12 10:57:36 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.5218
2022-08-12 10:58:09 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.6849
2022-08-12 10:58:43 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.6790
2022-08-12 10:59:16 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.7453
2022-08-12 10:59:49 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.6211
2022-08-12 11:00:22 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.7617
2022-08-12 11:00:56 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.7325
2022-08-12 11:01:29 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.9560
2022-08-12 11:02:02 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.8154
2022-08-12 11:02:36 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8987
2022-08-12 11:03:09 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.9217
2022-08-12 11:03:43 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.8365
2022-08-12 11:04:16 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.8352
2022-08-12 11:04:50 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.7567
2022-08-12 11:05:23 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.7064
2022-08-12 11:05:57 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.4776
2022-08-12 11:06:31 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.6250
2022-08-12 11:07:04 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.5477
2022-08-12 11:07:38 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.5136
2022-08-12 11:08:12 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.5841
2022-08-12 11:08:45 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.5697
2022-08-12 11:09:19 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.6982
2022-08-12 11:09:53 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.1931
2022-08-12 11:10:26 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.6144
2022-08-12 11:11:00 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.8100
2022-08-12 11:11:33 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.3922
2022-08-12 11:12:07 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.7569
2022-08-12 11:12:40 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.7694
2022-08-12 11:12:42 - train: epoch 004, train_loss: 2.7890
2022-08-12 11:13:57 - eval: epoch: 004, acc1: 45.210%, acc5: 71.696%, test_loss: 2.4272, per_image_load_time: 1.345ms, per_image_inference_time: 0.637ms
2022-08-12 11:13:57 - until epoch: 004, best_acc1: 45.210%
2022-08-12 11:13:57 - epoch 005 lr: 0.093815
2022-08-12 11:14:37 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.6530
2022-08-12 11:15:09 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.7157
2022-08-12 11:15:42 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.9193
2022-08-12 11:16:15 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.6223
2022-08-12 11:16:47 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.5542
2022-08-12 11:17:20 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.8356
2022-08-12 11:17:53 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.8104
2022-08-12 11:18:26 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.9243
2022-08-12 11:18:59 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.6342
2022-08-12 11:19:33 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.6149
2022-08-12 11:20:06 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.6679
2022-08-12 11:20:39 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.6489
2022-08-12 11:21:13 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.5370
2022-08-12 11:21:46 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.5534
2022-08-12 11:22:19 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.3685
2022-08-12 11:22:53 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.2996
2022-08-12 11:23:26 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.7157
2022-08-12 11:23:59 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.5905
2022-08-12 11:24:33 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.5822
2022-08-12 11:25:06 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.4999
2022-08-12 11:25:39 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.7757
2022-08-12 11:26:13 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.5154
2022-08-12 11:26:46 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.3664
2022-08-12 11:27:20 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.6623
2022-08-12 11:27:53 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.5547
2022-08-12 11:28:27 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.7619
2022-08-12 11:29:00 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.6735
2022-08-12 11:29:34 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.6367
2022-08-12 11:30:07 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.4707
2022-08-12 11:30:40 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.6259
2022-08-12 11:31:13 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.6112
2022-08-12 11:31:46 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.6544
2022-08-12 11:32:20 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.5584
2022-08-12 11:32:54 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.4433
2022-08-12 11:33:27 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.5900
2022-08-12 11:34:01 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6569
2022-08-12 11:34:34 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.4890
2022-08-12 11:35:07 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5326
2022-08-12 11:35:41 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.7899
2022-08-12 11:36:14 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.6511
2022-08-12 11:36:48 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.4668
2022-08-12 11:37:21 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.6170
2022-08-12 11:37:55 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.4127
2022-08-12 11:38:28 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.6835
2022-08-12 11:39:02 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.6707
2022-08-12 11:39:35 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.6010
2022-08-12 11:40:09 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.5348
2022-08-12 11:40:42 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.4852
2022-08-12 11:41:16 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.7731
2022-08-12 11:41:48 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.3832
2022-08-12 11:41:50 - train: epoch 005, train_loss: 2.6047
2022-08-12 11:43:05 - eval: epoch: 005, acc1: 48.252%, acc5: 74.778%, test_loss: 2.2539, per_image_load_time: 2.271ms, per_image_inference_time: 0.654ms
2022-08-12 11:43:06 - until epoch: 005, best_acc1: 48.252%
2022-08-12 11:43:06 - epoch 006 lr: 0.090450
2022-08-12 11:43:46 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.3696
2022-08-12 11:44:18 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.7708
2022-08-12 11:44:51 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.4295
2022-08-12 11:45:24 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.5220
2022-08-12 11:45:57 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.3457
2022-08-12 11:46:30 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.4113
2022-08-12 11:47:03 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.5913
2022-08-12 11:47:35 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.4032
2022-08-12 11:48:09 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3543
2022-08-12 11:48:42 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.4911
2022-08-12 11:49:15 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.5246
2022-08-12 11:49:48 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.5656
2022-08-12 11:50:21 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5392
2022-08-12 11:50:54 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.4087
2022-08-12 11:51:28 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.7634
2022-08-12 11:52:02 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3463
2022-08-12 11:52:35 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.5006
2022-08-12 11:53:08 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.4308
2022-08-12 11:53:42 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.4169
2022-08-12 11:54:15 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.7398
2022-08-12 11:54:48 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.6047
2022-08-12 11:55:22 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.4117
2022-08-12 11:55:55 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.3217
2022-08-12 11:56:29 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.4718
2022-08-12 11:57:02 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4332
2022-08-12 11:57:36 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.3046
2022-08-12 11:58:10 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.4237
2022-08-12 11:58:43 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.3021
2022-08-12 11:59:17 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.5224
2022-08-12 11:59:51 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.4332
2022-08-12 12:00:24 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.3256
2022-08-12 12:00:57 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.5965
2022-08-12 12:01:31 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.3479
2022-08-12 12:02:05 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.7626
2022-08-12 12:02:38 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.5991
2022-08-12 12:03:11 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.4927
2022-08-12 12:03:44 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.5835
2022-08-12 12:04:18 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.2154
2022-08-12 12:04:51 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.5653
2022-08-12 12:05:25 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.5905
2022-08-12 12:05:59 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.4217
2022-08-12 12:06:32 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.3648
2022-08-12 12:07:06 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.3765
2022-08-12 12:07:39 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.4414
2022-08-12 12:08:13 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.4340
2022-08-12 12:08:47 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.4675
2022-08-12 12:09:21 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4435
2022-08-12 12:09:54 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.4524
2022-08-12 12:10:28 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.4837
2022-08-12 12:11:01 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.4179
2022-08-12 12:11:02 - train: epoch 006, train_loss: 2.4783
2022-08-12 12:12:18 - eval: epoch: 006, acc1: 49.640%, acc5: 75.544%, test_loss: 2.1847, per_image_load_time: 2.072ms, per_image_inference_time: 0.637ms
2022-08-12 12:12:18 - until epoch: 006, best_acc1: 49.640%
2022-08-12 12:12:18 - epoch 007 lr: 0.086448
2022-08-12 12:12:58 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.1873
2022-08-12 12:13:30 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.5311
2022-08-12 12:14:03 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.4526
2022-08-12 12:14:36 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.5068
2022-08-12 12:15:09 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.4732
2022-08-12 12:15:42 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.3313
2022-08-12 12:16:15 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.3717
2022-08-12 12:16:48 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.5381
2022-08-12 12:17:21 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.4455
2022-08-12 12:17:55 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.3609
2022-08-12 12:18:28 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.4097
2022-08-12 12:19:01 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.2771
2022-08-12 12:19:34 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.1476
2022-08-12 12:20:08 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.3765
2022-08-12 12:20:41 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.4065
2022-08-12 12:21:14 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.3232
2022-08-12 12:21:48 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.3735
2022-08-12 12:22:22 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.3217
2022-08-12 12:22:55 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.5210
2022-08-12 12:23:29 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.3156
2022-08-12 12:24:02 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.5033
2022-08-12 12:24:36 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.3818
2022-08-12 12:25:09 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.5605
2022-08-12 12:25:43 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.5191
2022-08-12 12:26:16 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.4076
2022-08-12 12:26:50 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.4365
2022-08-12 12:27:23 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.3356
2022-08-12 12:27:57 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.1990
2022-08-12 12:28:30 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.1809
2022-08-12 12:29:03 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.5002
2022-08-12 12:29:37 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.4771
2022-08-12 12:30:11 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.2337
2022-08-12 12:30:44 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.4884
2022-08-12 12:31:17 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.3918
2022-08-12 12:31:51 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.5833
2022-08-12 12:32:24 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.0812
2022-08-12 12:32:57 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.4478
2022-08-12 12:33:31 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.3223
2022-08-12 12:34:04 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.3336
2022-08-12 12:34:38 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.6178
2022-08-12 12:35:11 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.3286
2022-08-12 12:35:45 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.3168
2022-08-12 12:36:18 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.4156
2022-08-12 12:36:52 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.2604
2022-08-12 12:37:25 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.4234
2022-08-12 12:37:59 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.3127
2022-08-12 12:38:32 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.2137
2022-08-12 12:39:06 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.5623
2022-08-12 12:39:40 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.3937
2022-08-12 12:40:13 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.1316
2022-08-12 12:40:15 - train: epoch 007, train_loss: 2.3802
2022-08-12 12:41:29 - eval: epoch: 007, acc1: 50.362%, acc5: 76.436%, test_loss: 2.1377, per_image_load_time: 1.632ms, per_image_inference_time: 0.655ms
2022-08-12 12:41:30 - until epoch: 007, best_acc1: 50.362%
2022-08-12 12:41:30 - epoch 008 lr: 0.081870
2022-08-12 12:42:09 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.3072
2022-08-12 12:42:42 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.2281
2022-08-12 12:43:15 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.4269
2022-08-12 12:43:47 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.1757
2022-08-12 12:44:20 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.1955
2022-08-12 12:44:52 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.4511
2022-08-12 12:45:25 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.2354
2022-08-12 12:45:58 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.1891
2022-08-12 12:46:32 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.3726
2022-08-12 12:47:04 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.3427
2022-08-12 12:47:38 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.0691
2022-08-12 12:48:11 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.1890
2022-08-12 12:48:44 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.3298
2022-08-12 12:49:17 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.2232
2022-08-12 12:49:51 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.2356
2022-08-12 12:50:24 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.3902
2022-08-12 12:50:57 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.2705
2022-08-12 12:51:30 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.4422
2022-08-12 12:52:04 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.3215
2022-08-12 12:52:37 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.1468
2022-08-12 12:53:11 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.1437
2022-08-12 12:53:44 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.1739
2022-08-12 12:54:18 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.2820
2022-08-12 12:54:51 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.2370
2022-08-12 12:55:24 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 1.9988
2022-08-12 12:55:58 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.2781
2022-08-12 12:56:31 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.6677
2022-08-12 12:57:04 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.4002
2022-08-12 12:57:38 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.4467
2022-08-12 12:58:11 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.3145
2022-08-12 12:58:44 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.2406
2022-08-12 12:59:18 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.5075
2022-08-12 12:59:52 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.3478
2022-08-12 13:00:25 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.3221
2022-08-12 13:00:59 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.2601
2022-08-12 13:01:32 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.4761
2022-08-12 13:02:06 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.2122
2022-08-12 13:02:39 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.2006
2022-08-12 13:03:13 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.5680
2022-08-12 13:03:47 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.5999
2022-08-12 13:04:20 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.0123
2022-08-12 13:04:54 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.1674
2022-08-12 13:05:27 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 1.9610
2022-08-12 13:06:01 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.2652
2022-08-12 13:06:34 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.2942
2022-08-12 13:07:07 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.2754
2022-08-12 13:07:40 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.3680
2022-08-12 13:08:13 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.1888
2022-08-12 13:08:47 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.3583
2022-08-12 13:09:20 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.0966
2022-08-12 13:09:21 - train: epoch 008, train_loss: 2.3032
2022-08-12 13:10:37 - eval: epoch: 008, acc1: 53.274%, acc5: 78.814%, test_loss: 1.9913, per_image_load_time: 2.312ms, per_image_inference_time: 0.601ms
2022-08-12 13:10:37 - until epoch: 008, best_acc1: 53.274%
2022-08-12 13:10:37 - epoch 009 lr: 0.076790
2022-08-12 13:11:17 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 2.1688
2022-08-12 13:11:50 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.2037
2022-08-12 13:12:22 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.2015
2022-08-12 13:12:55 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.4861
2022-08-12 13:13:27 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.1699
2022-08-12 13:14:00 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.1659
2022-08-12 13:14:34 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.0495
2022-08-12 13:15:07 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.3589
2022-08-12 13:15:40 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.1932
2022-08-12 13:16:14 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1269
2022-08-12 13:16:47 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.4164
2022-08-12 13:17:21 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.3388
2022-08-12 13:17:54 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.4642
2022-08-12 13:18:27 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 2.0015
2022-08-12 13:19:01 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.1011
2022-08-12 13:19:34 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.4278
2022-08-12 13:20:08 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.2415
2022-08-12 13:20:41 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.1454
2022-08-12 13:21:15 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.0749
2022-08-12 13:21:48 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.1731
2022-08-12 13:22:22 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.2504
2022-08-12 13:22:55 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.2860
2022-08-12 13:23:29 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.0707
2022-08-12 13:24:02 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.3578
2022-08-12 13:24:35 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.2303
2022-08-12 13:25:08 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.2318
2022-08-12 13:25:42 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 1.9813
2022-08-12 13:26:15 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.3019
2022-08-12 13:26:49 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.1143
2022-08-12 13:27:22 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1765
2022-08-12 13:27:56 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.2112
2022-08-12 13:28:29 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.3389
2022-08-12 13:29:03 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.2566
2022-08-12 13:29:36 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.3099
2022-08-12 13:30:10 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.2334
2022-08-12 13:30:43 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.1671
2022-08-12 13:31:16 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.3396
2022-08-12 13:31:50 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.3552
2022-08-12 13:32:23 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.0136
2022-08-12 13:32:56 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.4052
2022-08-12 13:33:30 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.2855
2022-08-12 13:34:03 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.0083
2022-08-12 13:34:37 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.0654
2022-08-12 13:35:10 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.4259
2022-08-12 13:35:43 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.4080
2022-08-12 13:36:17 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.3220
2022-08-12 13:36:50 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.4872
2022-08-12 13:37:24 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.2925
2022-08-12 13:37:57 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.2745
2022-08-12 13:38:30 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.0444
2022-08-12 13:38:31 - train: epoch 009, train_loss: 2.2325
2022-08-12 13:39:46 - eval: epoch: 009, acc1: 54.410%, acc5: 79.412%, test_loss: 1.9527, per_image_load_time: 2.211ms, per_image_inference_time: 0.596ms
2022-08-12 13:39:46 - until epoch: 009, best_acc1: 54.410%
2022-08-12 13:39:46 - epoch 010 lr: 0.071288
2022-08-12 13:40:27 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.1966
2022-08-12 13:41:00 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.2075
2022-08-12 13:41:33 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.2550
2022-08-12 13:42:06 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.3750
2022-08-12 13:42:39 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.0577
2022-08-12 13:43:12 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.1336
2022-08-12 13:43:46 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.3372
2022-08-12 13:44:19 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.2727
2022-08-12 13:44:53 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.0159
2022-08-12 13:45:26 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 1.9658
2022-08-12 13:45:59 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.2735
2022-08-12 13:46:32 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.1165
2022-08-12 13:47:05 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.0191
2022-08-12 13:47:39 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.5034
2022-08-12 13:48:12 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.9443
2022-08-12 13:48:45 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.1583
2022-08-12 13:49:18 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.1432
2022-08-12 13:49:51 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 1.9116
2022-08-12 13:50:24 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.0621
2022-08-12 13:50:57 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 1.9708
2022-08-12 13:51:30 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 1.9321
2022-08-12 13:52:03 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.2631
2022-08-12 13:52:36 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.3137
2022-08-12 13:53:09 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.2572
2022-08-12 13:53:42 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.3203
2022-08-12 13:54:15 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.1410
2022-08-12 13:54:48 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.9882
2022-08-12 13:55:21 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.2522
2022-08-12 13:55:54 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.3172
2022-08-12 13:56:26 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.2138
2022-08-12 13:57:00 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.5515
2022-08-12 13:57:32 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.1593
2022-08-12 13:58:05 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.4277
2022-08-12 13:58:38 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.2211
2022-08-12 13:59:11 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.2116
2022-08-12 13:59:44 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.4400
2022-08-12 14:00:17 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.1284
2022-08-12 14:00:50 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.2412
2022-08-12 14:01:23 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 2.0895
2022-08-12 14:01:56 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.0686
2022-08-12 14:02:30 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.0889
2022-08-12 14:03:03 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.1081
2022-08-12 14:03:36 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.3002
2022-08-12 14:04:09 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.0401
2022-08-12 14:04:42 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.2070
2022-08-12 14:05:15 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.0891
2022-08-12 14:05:48 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1075
2022-08-12 14:06:21 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.0251
2022-08-12 14:06:55 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.1019
2022-08-12 14:07:27 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.1157
2022-08-12 14:07:29 - train: epoch 010, train_loss: 2.1698
2022-08-12 14:08:43 - eval: epoch: 010, acc1: 56.236%, acc5: 80.782%, test_loss: 1.8659, per_image_load_time: 2.288ms, per_image_inference_time: 0.589ms
2022-08-12 14:08:43 - until epoch: 010, best_acc1: 56.236%
2022-08-12 14:08:43 - epoch 011 lr: 0.065450
2022-08-12 14:09:24 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.0550
2022-08-12 14:09:56 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.1451
2022-08-12 14:10:28 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.2011
2022-08-12 14:11:00 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.2656
2022-08-12 14:11:33 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.0968
2022-08-12 14:12:05 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.1203
2022-08-12 14:12:38 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.2512
2022-08-12 14:13:10 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.3112
2022-08-12 14:13:43 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.4115
2022-08-12 14:14:16 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.2106
2022-08-12 14:14:49 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.1962
2022-08-12 14:15:22 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2492
2022-08-12 14:15:55 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.2952
2022-08-12 14:16:28 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.1700
2022-08-12 14:17:01 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.0248
2022-08-12 14:17:34 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.2320
2022-08-12 14:18:07 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.2686
2022-08-12 14:18:41 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.1351
2022-08-12 14:19:14 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.0605
2022-08-12 14:19:47 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.3448
2022-08-12 14:20:19 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.0771
2022-08-12 14:20:52 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 2.1407
2022-08-12 14:21:25 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.3574
2022-08-12 14:21:58 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.9079
2022-08-12 14:22:31 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.0355
2022-08-12 14:23:04 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.2194
2022-08-12 14:23:37 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.1495
2022-08-12 14:24:10 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.8828
2022-08-12 14:24:43 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.2100
2022-08-12 14:25:16 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.2998
2022-08-12 14:25:49 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.1892
2022-08-12 14:26:22 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.0619
2022-08-12 14:26:55 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.2636
2022-08-12 14:27:28 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.1755
2022-08-12 14:28:01 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.0739
2022-08-12 14:28:34 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.0607
2022-08-12 14:29:07 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.4390
2022-08-12 14:29:40 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.7817
2022-08-12 14:30:12 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.0440
2022-08-12 14:30:46 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 1.9932
2022-08-12 14:31:18 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.9274
2022-08-12 14:31:52 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 2.0742
2022-08-12 14:32:25 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 1.8738
2022-08-12 14:32:58 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.0915
2022-08-12 14:33:30 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.0908
2022-08-12 14:34:03 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 2.1607
2022-08-12 14:34:36 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.7068
2022-08-12 14:35:09 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.7109
2022-08-12 14:35:42 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.0028
2022-08-12 14:36:14 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 1.9478
2022-08-12 14:36:16 - train: epoch 011, train_loss: 2.1103
2022-08-12 14:37:31 - eval: epoch: 011, acc1: 57.312%, acc5: 81.510%, test_loss: 1.8039, per_image_load_time: 1.689ms, per_image_inference_time: 0.632ms
2022-08-12 14:37:32 - until epoch: 011, best_acc1: 57.312%
2022-08-12 14:37:32 - epoch 012 lr: 0.059368
2022-08-12 14:38:12 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 2.0130
2022-08-12 14:38:44 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.9028
2022-08-12 14:39:17 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 1.9795
2022-08-12 14:39:50 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.1484
2022-08-12 14:40:23 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.0598
2022-08-12 14:40:56 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.8043
2022-08-12 14:41:29 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.8402
2022-08-12 14:42:02 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.2915
2022-08-12 14:42:35 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.0896
2022-08-12 14:43:07 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 2.0737
2022-08-12 14:43:40 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.2359
2022-08-12 14:44:13 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.7913
2022-08-12 14:44:45 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.8124
2022-08-12 14:45:18 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.3681
2022-08-12 14:45:51 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 1.9384
2022-08-12 14:46:24 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 1.9950
2022-08-12 14:46:57 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.7748
2022-08-12 14:47:30 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.1438
2022-08-12 14:48:03 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.0665
2022-08-12 14:48:36 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.1921
2022-08-12 14:49:09 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.1355
2022-08-12 14:49:42 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.0374
2022-08-12 14:50:15 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 2.0725
2022-08-12 14:50:48 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.1354
2022-08-12 14:51:21 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.9677
2022-08-12 14:51:54 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.8243
2022-08-12 14:52:27 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 1.9365
2022-08-12 14:53:00 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 2.0981
2022-08-12 14:53:33 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.0433
2022-08-12 14:54:06 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.9711
2022-08-12 14:54:40 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.1099
2022-08-12 14:55:13 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 2.0346
2022-08-12 14:55:46 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.1184
2022-08-12 14:56:19 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.0717
2022-08-12 14:56:51 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.1161
2022-08-12 14:57:24 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.9190
2022-08-12 14:57:57 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.0235
2022-08-12 14:58:30 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.0794
2022-08-12 14:59:02 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8110
2022-08-12 14:59:35 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.2458
2022-08-12 15:00:09 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.0136
2022-08-12 15:00:42 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 2.1367
2022-08-12 15:01:15 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.1561
2022-08-12 15:01:48 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 2.0681
2022-08-12 15:02:22 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.8745
2022-08-12 15:02:55 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.1136
2022-08-12 15:03:28 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.9147
2022-08-12 15:04:01 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.1885
2022-08-12 15:04:34 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.9403
2022-08-12 15:05:07 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.8710
2022-08-12 15:05:09 - train: epoch 012, train_loss: 2.0503
2022-08-12 15:06:25 - eval: epoch: 012, acc1: 58.606%, acc5: 82.696%, test_loss: 1.7275, per_image_load_time: 2.236ms, per_image_inference_time: 0.641ms
2022-08-12 15:06:25 - until epoch: 012, best_acc1: 58.606%
2022-08-12 15:06:25 - epoch 013 lr: 0.053138
2022-08-12 15:07:05 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.8256
2022-08-12 15:07:39 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.9128
2022-08-12 15:08:13 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.9415
2022-08-12 15:08:46 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.8337
2022-08-12 15:09:19 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 1.9989
2022-08-12 15:09:52 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 1.9830
2022-08-12 15:10:25 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 2.1199
2022-08-12 15:10:58 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.2168
2022-08-12 15:11:32 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.8769
2022-08-12 15:12:06 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.8890
2022-08-12 15:12:39 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.0589
2022-08-12 15:13:13 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.0226
2022-08-12 15:13:47 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.9033
2022-08-12 15:14:20 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 1.8249
2022-08-12 15:14:54 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.0255
2022-08-12 15:15:28 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.8044
2022-08-12 15:16:02 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.0821
2022-08-12 15:16:36 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9446
2022-08-12 15:17:10 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.1568
2022-08-12 15:17:44 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.1360
2022-08-12 15:18:18 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.0496
2022-08-12 15:18:52 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.9039
2022-08-12 15:19:27 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 2.0259
2022-08-12 15:20:01 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.0937
2022-08-12 15:20:35 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.8147
2022-08-12 15:21:09 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.8537
2022-08-12 15:21:43 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.8757
2022-08-12 15:22:17 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.0376
2022-08-12 15:22:50 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.0246
2022-08-12 15:23:23 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.9633
2022-08-12 15:23:56 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.9450
2022-08-12 15:24:29 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 2.1037
2022-08-12 15:25:03 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.9577
2022-08-12 15:25:36 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.9844
2022-08-12 15:26:09 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.9194
2022-08-12 15:26:42 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.0591
2022-08-12 15:27:16 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.7101
2022-08-12 15:27:49 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1044
2022-08-12 15:28:23 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.1030
2022-08-12 15:28:56 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.9102
2022-08-12 15:29:30 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.8834
2022-08-12 15:30:03 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 2.0565
2022-08-12 15:30:37 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.8306
2022-08-12 15:31:11 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.7858
2022-08-12 15:31:44 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.7501
2022-08-12 15:32:18 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.1230
2022-08-12 15:32:51 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 2.0402
2022-08-12 15:33:25 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.0068
2022-08-12 15:33:58 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0481
2022-08-12 15:34:31 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.2190
2022-08-12 15:34:33 - train: epoch 013, train_loss: 1.9933
2022-08-12 15:35:48 - eval: epoch: 013, acc1: 59.600%, acc5: 83.410%, test_loss: 1.6798, per_image_load_time: 1.425ms, per_image_inference_time: 0.614ms
2022-08-12 15:35:48 - until epoch: 013, best_acc1: 59.600%
2022-08-12 15:35:48 - epoch 014 lr: 0.046859
2022-08-12 15:36:29 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 2.0172
2022-08-12 15:37:02 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.9803
2022-08-12 15:37:35 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.8110
2022-08-12 15:38:08 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.7538
2022-08-12 15:38:41 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.7933
2022-08-12 15:39:13 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.6651
2022-08-12 15:39:46 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.7568
2022-08-12 15:40:19 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.8419
2022-08-12 15:40:52 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 2.0158
2022-08-12 15:41:25 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 1.9058
2022-08-12 15:41:59 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.7994
2022-08-12 15:42:32 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 2.0420
2022-08-12 15:43:05 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.1864
2022-08-12 15:43:38 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.7847
2022-08-12 15:44:11 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.1050
2022-08-12 15:44:45 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.7625
2022-08-12 15:45:18 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 2.0173
2022-08-12 15:45:52 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.1254
2022-08-12 15:46:25 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.7685
2022-08-12 15:46:58 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.9760
2022-08-12 15:47:32 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.0164
2022-08-12 15:48:05 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.9748
2022-08-12 15:48:39 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.6989
2022-08-12 15:49:12 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.8366
2022-08-12 15:49:46 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.7561
2022-08-12 15:50:19 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8903
2022-08-12 15:50:53 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.7836
2022-08-12 15:51:26 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 2.0959
2022-08-12 15:52:00 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9921
2022-08-12 15:52:33 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 2.1719
2022-08-12 15:53:07 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 2.0043
2022-08-12 15:53:41 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.9596
2022-08-12 15:54:15 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.9125
2022-08-12 15:54:48 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.8978
2022-08-12 15:55:22 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.9023
2022-08-12 15:55:55 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.6748
2022-08-12 15:56:29 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.8240
2022-08-12 15:57:02 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 2.1364
2022-08-12 15:57:36 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.9362
2022-08-12 15:58:09 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.8581
2022-08-12 15:58:43 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 2.0003
2022-08-12 15:59:17 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.9286
2022-08-12 15:59:50 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.8493
2022-08-12 16:00:24 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.8122
2022-08-12 16:00:57 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.8473
2022-08-12 16:01:31 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.8261
2022-08-12 16:02:05 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 2.0986
2022-08-12 16:02:38 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.8886
2022-08-12 16:03:12 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8241
2022-08-12 16:03:45 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.8143
2022-08-12 16:03:46 - train: epoch 014, train_loss: 1.9297
2022-08-12 16:05:02 - eval: epoch: 014, acc1: 60.576%, acc5: 84.220%, test_loss: 1.6351, per_image_load_time: 1.981ms, per_image_inference_time: 0.638ms
2022-08-12 16:05:03 - until epoch: 014, best_acc1: 60.576%
2022-08-12 16:05:03 - epoch 015 lr: 0.040630
2022-08-12 16:05:43 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.5820
2022-08-12 16:06:16 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.9677
2022-08-12 16:06:49 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.1471
2022-08-12 16:07:22 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.9329
2022-08-12 16:07:56 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.8349
2022-08-12 16:08:29 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 2.0903
2022-08-12 16:09:03 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.9461
2022-08-12 16:09:36 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.7984
2022-08-12 16:10:10 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.8780
2022-08-12 16:10:43 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.7801
2022-08-12 16:11:16 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7521
2022-08-12 16:11:50 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 2.0027
2022-08-12 16:12:23 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.8546
2022-08-12 16:12:57 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.8705
2022-08-12 16:13:30 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.8235
2022-08-12 16:14:04 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 2.0190
2022-08-12 16:14:38 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.7883
2022-08-12 16:15:11 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.7385
2022-08-12 16:15:45 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.7782
2022-08-12 16:16:19 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.7367
2022-08-12 16:16:52 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.7703
2022-08-12 16:17:26 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9899
2022-08-12 16:17:59 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.7362
2022-08-12 16:18:33 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.7902
2022-08-12 16:19:07 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.9303
2022-08-12 16:19:41 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.6824
2022-08-12 16:20:14 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 2.0922
2022-08-12 16:20:48 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.7914
2022-08-12 16:21:21 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.8811
2022-08-12 16:21:55 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.7321
2022-08-12 16:22:29 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.7410
2022-08-12 16:23:03 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.7820
2022-08-12 16:23:36 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.6389
2022-08-12 16:24:10 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.7685
2022-08-12 16:24:44 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 1.9757
2022-08-12 16:25:18 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.8215
2022-08-12 16:25:51 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.7181
2022-08-12 16:26:26 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.9405
2022-08-12 16:26:59 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.9684
2022-08-12 16:27:33 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.8376
2022-08-12 16:28:07 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.7249
2022-08-12 16:28:41 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.5505
2022-08-12 16:29:14 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.8620
2022-08-12 16:29:48 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.8288
2022-08-12 16:30:22 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.7415
2022-08-12 16:30:56 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.9396
2022-08-12 16:31:29 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.6955
2022-08-12 16:32:03 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.8917
2022-08-12 16:32:36 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.8348
2022-08-12 16:33:09 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.8673
2022-08-12 16:33:11 - train: epoch 015, train_loss: 1.8689
2022-08-12 16:34:27 - eval: epoch: 015, acc1: 62.764%, acc5: 85.168%, test_loss: 1.5521, per_image_load_time: 1.496ms, per_image_inference_time: 0.631ms
2022-08-12 16:34:27 - until epoch: 015, best_acc1: 62.764%
2022-08-12 16:34:27 - epoch 016 lr: 0.034548
2022-08-12 16:35:07 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.7371
2022-08-12 16:35:40 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.7295
2022-08-12 16:36:14 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.7057
2022-08-12 16:36:47 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.0706
2022-08-12 16:37:20 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.6566
2022-08-12 16:37:53 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.6523
2022-08-12 16:38:26 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.7863
2022-08-12 16:39:00 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.9123
2022-08-12 16:39:33 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.8852
2022-08-12 16:40:07 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.7854
2022-08-12 16:40:40 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.5914
2022-08-12 16:41:14 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.6501
2022-08-12 16:41:48 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.8705
2022-08-12 16:42:21 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.7019
2022-08-12 16:42:55 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 2.0663
2022-08-12 16:43:28 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 2.0173
2022-08-12 16:44:01 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.7679
2022-08-12 16:44:35 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 2.1256
2022-08-12 16:45:08 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.8949
2022-08-12 16:45:42 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.5613
2022-08-12 16:46:15 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.6530
2022-08-12 16:46:49 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.9732
2022-08-12 16:47:23 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.9742
2022-08-12 16:47:56 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.7675
2022-08-12 16:48:30 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.8078
2022-08-12 16:49:03 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.7960
2022-08-12 16:49:37 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.5744
2022-08-12 16:50:10 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.7353
2022-08-12 16:50:44 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.6682
2022-08-12 16:51:17 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.9210
2022-08-12 16:51:51 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.9119
2022-08-12 16:52:24 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.9876
2022-08-12 16:52:58 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9217
2022-08-12 16:53:32 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.7360
2022-08-12 16:54:05 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.8695
2022-08-12 16:54:39 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.6822
2022-08-12 16:55:12 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.8272
2022-08-12 16:55:46 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.1552
2022-08-12 16:56:19 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.9841
2022-08-12 16:56:52 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.9261
2022-08-12 16:57:26 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.8720
2022-08-12 16:58:00 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.6974
2022-08-12 16:58:34 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.7256
2022-08-12 16:59:07 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.6402
2022-08-12 16:59:41 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.8492
2022-08-12 17:00:15 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.6257
2022-08-12 17:00:48 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.7858
2022-08-12 17:01:22 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.8550
2022-08-12 17:01:55 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.9045
2022-08-12 17:02:28 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.8605
2022-08-12 17:02:30 - train: epoch 016, train_loss: 1.8061
2022-08-12 17:03:46 - eval: epoch: 016, acc1: 63.586%, acc5: 85.922%, test_loss: 1.4948, per_image_load_time: 0.662ms, per_image_inference_time: 0.617ms
2022-08-12 17:03:46 - until epoch: 016, best_acc1: 63.586%
2022-08-12 17:03:46 - epoch 017 lr: 0.028710
2022-08-12 17:04:26 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.6383
2022-08-12 17:05:00 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.9116
2022-08-12 17:05:33 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.8616
2022-08-12 17:06:06 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.4219
2022-08-12 17:06:40 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.8448
2022-08-12 17:07:13 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 2.1572
2022-08-12 17:07:47 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.8462
2022-08-12 17:08:21 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.7245
2022-08-12 17:08:55 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.6950
2022-08-12 17:09:28 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.9110
2022-08-12 17:10:01 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.9000
2022-08-12 17:10:35 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.5558
2022-08-12 17:11:09 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.8885
2022-08-12 17:11:42 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.8393
2022-08-12 17:12:16 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.6186
2022-08-12 17:12:50 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.4626
2022-08-12 17:13:23 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.8473
2022-08-12 17:13:57 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.7314
2022-08-12 17:14:30 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.8006
2022-08-12 17:15:04 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.7225
2022-08-12 17:15:38 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.5997
2022-08-12 17:16:12 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.5666
2022-08-12 17:16:46 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.8286
2022-08-12 17:17:19 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.6408
2022-08-12 17:17:53 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.8023
2022-08-12 17:18:27 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.6730
2022-08-12 17:19:01 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6381
2022-08-12 17:19:34 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.7116
2022-08-12 17:20:08 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.8663
2022-08-12 17:20:42 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.5130
2022-08-12 17:21:16 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8855
2022-08-12 17:21:49 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.6928
2022-08-12 17:22:23 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8787
2022-08-12 17:22:57 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.6522
2022-08-12 17:23:30 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.5222
2022-08-12 17:24:04 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.9879
2022-08-12 17:24:37 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.7369
2022-08-12 17:25:11 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.8153
2022-08-12 17:25:45 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.5734
2022-08-12 17:26:19 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.7794
2022-08-12 17:26:53 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.9117
2022-08-12 17:27:27 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.8070
2022-08-12 17:28:01 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.6928
2022-08-12 17:28:34 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8935
2022-08-12 17:29:08 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.9910
2022-08-12 17:29:42 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.6605
2022-08-12 17:30:16 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.9562
2022-08-12 17:30:49 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.9139
2022-08-12 17:31:23 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.5933
2022-08-12 17:31:56 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.4811
2022-08-12 17:31:57 - train: epoch 017, train_loss: 1.7374
2022-08-12 17:33:14 - eval: epoch: 017, acc1: 65.166%, acc5: 86.700%, test_loss: 1.4384, per_image_load_time: 0.659ms, per_image_inference_time: 0.616ms
2022-08-12 17:33:14 - until epoch: 017, best_acc1: 65.166%
2022-08-12 17:33:14 - epoch 018 lr: 0.023208
2022-08-12 17:33:54 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.6484
2022-08-12 17:34:28 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.8768
2022-08-12 17:35:01 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.8979
2022-08-12 17:35:34 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.7141
2022-08-12 17:36:08 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.6060
2022-08-12 17:36:41 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.8244
2022-08-12 17:37:14 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.5233
2022-08-12 17:37:48 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.7284
2022-08-12 17:38:21 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.7452
2022-08-12 17:38:54 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.5809
2022-08-12 17:39:27 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.6703
2022-08-12 17:40:00 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.7557
2022-08-12 17:40:34 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.9506
2022-08-12 17:41:08 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.6632
2022-08-12 17:41:41 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.9012
2022-08-12 17:42:15 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.5341
2022-08-12 17:42:49 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7965
2022-08-12 17:43:22 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6513
2022-08-12 17:43:56 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.7608
2022-08-12 17:44:29 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.8845
2022-08-12 17:45:03 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.8202
2022-08-12 17:45:37 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 2.0243
2022-08-12 17:46:10 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6784
2022-08-12 17:46:44 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.4288
2022-08-12 17:47:17 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.4965
2022-08-12 17:47:51 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.6940
2022-08-12 17:48:25 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.8468
2022-08-12 17:48:59 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.6371
2022-08-12 17:49:32 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.5498
2022-08-12 17:50:06 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.7102
2022-08-12 17:50:39 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.9469
2022-08-12 17:51:13 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.8251
2022-08-12 17:51:47 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.7681
2022-08-12 17:52:21 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.6754
2022-08-12 17:52:55 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.7991
2022-08-12 17:53:28 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.7346
2022-08-12 17:54:02 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.9003
2022-08-12 17:54:35 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.6095
2022-08-12 17:55:09 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.8064
2022-08-12 17:55:42 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 2.0031
2022-08-12 17:56:15 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7841
2022-08-12 17:56:49 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.4461
2022-08-12 17:57:23 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.6038
2022-08-12 17:57:56 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.5618
2022-08-12 17:58:30 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.6590
2022-08-12 17:59:04 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.7060
2022-08-12 17:59:38 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 2.0280
2022-08-12 18:00:12 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.5679
2022-08-12 18:00:45 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.5858
2022-08-12 18:01:18 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.6949
2022-08-12 18:01:20 - train: epoch 018, train_loss: 1.6681
2022-08-12 18:02:35 - eval: epoch: 018, acc1: 66.194%, acc5: 87.618%, test_loss: 1.3815, per_image_load_time: 0.693ms, per_image_inference_time: 0.600ms
2022-08-12 18:02:36 - until epoch: 018, best_acc1: 66.194%
2022-08-12 18:02:36 - epoch 019 lr: 0.018128
2022-08-12 18:03:16 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4794
2022-08-12 18:03:50 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.5224
2022-08-12 18:04:23 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.7540
2022-08-12 18:04:56 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.6142
2022-08-12 18:05:29 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.4927
2022-08-12 18:06:02 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.6712
2022-08-12 18:06:36 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.5539
2022-08-12 18:07:09 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.7193
2022-08-12 18:07:43 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.5458
2022-08-12 18:08:16 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.6006
2022-08-12 18:08:50 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.7011
2022-08-12 18:09:23 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.6909
2022-08-12 18:09:57 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.5895
2022-08-12 18:10:30 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.3437
2022-08-12 18:11:04 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.7911
2022-08-12 18:11:38 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.5959
2022-08-12 18:12:11 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.8286
2022-08-12 18:12:45 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.5415
2022-08-12 18:13:18 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.9180
2022-08-12 18:13:51 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.4677
2022-08-12 18:14:24 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.5060
2022-08-12 18:14:58 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.6872
2022-08-12 18:15:32 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.6147
2022-08-12 18:16:05 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.9068
2022-08-12 18:16:39 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.5575
2022-08-12 18:17:12 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.5771
2022-08-12 18:17:46 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.3514
2022-08-12 18:18:19 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.5443
2022-08-12 18:18:53 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.7610
2022-08-12 18:19:27 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.5677
2022-08-12 18:20:01 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.6902
2022-08-12 18:20:34 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.3066
2022-08-12 18:21:08 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.6089
2022-08-12 18:21:42 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.6616
2022-08-12 18:22:15 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.6748
2022-08-12 18:22:49 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.3850
2022-08-12 18:23:23 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.5636
2022-08-12 18:23:56 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.7671
2022-08-12 18:24:30 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.5352
2022-08-12 18:25:04 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.6983
2022-08-12 18:25:37 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.7136
2022-08-12 18:26:11 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.6175
2022-08-12 18:26:45 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.4631
2022-08-12 18:27:19 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.4504
2022-08-12 18:27:52 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.8708
2022-08-12 18:28:26 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.7006
2022-08-12 18:29:00 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.6952
2022-08-12 18:29:34 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.6209
2022-08-12 18:30:07 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.6532
2022-08-12 18:30:41 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.6834
2022-08-12 18:30:42 - train: epoch 019, train_loss: 1.6007
2022-08-12 18:31:59 - eval: epoch: 019, acc1: 67.732%, acc5: 88.260%, test_loss: 1.3220, per_image_load_time: 0.643ms, per_image_inference_time: 0.568ms
2022-08-12 18:31:59 - until epoch: 019, best_acc1: 67.732%
2022-08-12 18:31:59 - epoch 020 lr: 0.013551
2022-08-12 18:32:39 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.5074
2022-08-12 18:33:13 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.3557
2022-08-12 18:33:46 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.4845
2022-08-12 18:34:19 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.4009
2022-08-12 18:34:53 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.4111
2022-08-12 18:35:26 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.6109
2022-08-12 18:36:00 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.2404
2022-08-12 18:36:33 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.7324
2022-08-12 18:37:07 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.5230
2022-08-12 18:37:41 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.5991
2022-08-12 18:38:15 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.3367
2022-08-12 18:38:48 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.3959
2022-08-12 18:39:22 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.5010
2022-08-12 18:39:56 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.3898
2022-08-12 18:40:30 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.5383
2022-08-12 18:41:03 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.5677
2022-08-12 18:41:37 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.5403
2022-08-12 18:42:10 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.7838
2022-08-12 18:42:44 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.4912
2022-08-12 18:43:17 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.4362
2022-08-12 18:43:50 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.5214
2022-08-12 18:44:23 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.4883
2022-08-12 18:44:57 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.3123
2022-08-12 18:45:30 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.5599
2022-08-12 18:46:04 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.6376
2022-08-12 18:46:38 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.5672
2022-08-12 18:47:11 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.5019
2022-08-12 18:47:45 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.6429
2022-08-12 18:48:18 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.6865
2022-08-12 18:48:52 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.5707
2022-08-12 18:49:26 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.6143
2022-08-12 18:49:59 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.5778
2022-08-12 18:50:33 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.2355
2022-08-12 18:51:07 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.5101
2022-08-12 18:51:41 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.4063
2022-08-12 18:52:14 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.4186
2022-08-12 18:52:48 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.3611
2022-08-12 18:53:22 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.5342
2022-08-12 18:53:55 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.7774
2022-08-12 18:54:29 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.3978
2022-08-12 18:55:03 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.5676
2022-08-12 18:55:36 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.5302
2022-08-12 18:56:09 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.7259
2022-08-12 18:56:43 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.2883
2022-08-12 18:57:16 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.5384
2022-08-12 18:57:49 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.6036
2022-08-12 18:58:23 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.4222
2022-08-12 18:58:57 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.5801
2022-08-12 18:59:30 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.5229
2022-08-12 19:00:04 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.5409
2022-08-12 19:00:05 - train: epoch 020, train_loss: 1.5267
2022-08-12 19:01:21 - eval: epoch: 020, acc1: 68.660%, acc5: 88.992%, test_loss: 1.2696, per_image_load_time: 0.560ms, per_image_inference_time: 0.592ms
2022-08-12 19:01:21 - until epoch: 020, best_acc1: 68.660%
2022-08-12 19:01:21 - epoch 021 lr: 0.009548
2022-08-12 19:02:02 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.4343
2022-08-12 19:02:35 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.3473
2022-08-12 19:03:08 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.4561
2022-08-12 19:03:41 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.5580
2022-08-12 19:04:15 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.2774
2022-08-12 19:04:48 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.2320
2022-08-12 19:05:22 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.4095
2022-08-12 19:05:56 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.4410
2022-08-12 19:06:29 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.3798
2022-08-12 19:07:02 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.4343
2022-08-12 19:07:36 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.3516
2022-08-12 19:08:09 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.4821
2022-08-12 19:08:43 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.2430
2022-08-12 19:09:16 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.2173
2022-08-12 19:09:50 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.4480
2022-08-12 19:10:23 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.4144
2022-08-12 19:10:57 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5532
2022-08-12 19:11:30 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.5273
2022-08-12 19:12:04 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.5694
2022-08-12 19:12:37 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.5678
2022-08-12 19:13:11 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.5036
2022-08-12 19:13:44 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.3791
2022-08-12 19:14:18 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.3973
2022-08-12 19:14:51 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.3293
2022-08-12 19:15:25 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.4425
2022-08-12 19:15:58 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.4096
2022-08-12 19:16:32 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.2509
2022-08-12 19:17:06 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.5282
2022-08-12 19:17:40 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.3234
2022-08-12 19:18:13 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.4643
2022-08-12 19:18:47 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.4373
2022-08-12 19:19:21 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.2971
2022-08-12 19:19:54 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.6256
2022-08-12 19:20:27 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.5847
2022-08-12 19:21:01 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4188
2022-08-12 19:21:34 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.3889
2022-08-12 19:22:08 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.2739
2022-08-12 19:22:42 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.5876
2022-08-12 19:23:16 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.5762
2022-08-12 19:23:50 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.6624
2022-08-12 19:24:24 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.3713
2022-08-12 19:24:57 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.4233
2022-08-12 19:25:30 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.5129
2022-08-12 19:26:04 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.6684
2022-08-12 19:26:37 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.3680
2022-08-12 19:27:11 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.3457
2022-08-12 19:27:44 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.5539
2022-08-12 19:28:18 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.4041
2022-08-12 19:28:51 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.3262
2022-08-12 19:29:24 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.3299
2022-08-12 19:29:26 - train: epoch 021, train_loss: 1.4557
2022-08-12 19:30:41 - eval: epoch: 021, acc1: 69.646%, acc5: 89.530%, test_loss: 1.2236, per_image_load_time: 1.241ms, per_image_inference_time: 0.605ms
2022-08-12 19:30:42 - until epoch: 021, best_acc1: 69.646%
2022-08-12 19:30:42 - epoch 022 lr: 0.006184
2022-08-12 19:31:22 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.0204
2022-08-12 19:31:55 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.3892
2022-08-12 19:32:29 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.4234
2022-08-12 19:33:02 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.5245
2022-08-12 19:33:35 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3900
2022-08-12 19:34:09 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.6128
2022-08-12 19:34:42 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4644
2022-08-12 19:35:16 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.4443
2022-08-12 19:35:50 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.5665
2022-08-12 19:36:23 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.4769
2022-08-12 19:36:56 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3823
2022-08-12 19:37:29 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.1941
2022-08-12 19:38:03 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.2604
2022-08-12 19:38:36 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.2017
2022-08-12 19:39:10 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.5466
2022-08-12 19:39:44 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.3894
2022-08-12 19:40:17 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.6245
2022-08-12 19:40:51 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.7910
2022-08-12 19:41:24 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.4787
2022-08-12 19:41:57 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4337
2022-08-12 19:42:31 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.3607
2022-08-12 19:43:04 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.0603
2022-08-12 19:43:38 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.3783
2022-08-12 19:44:11 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.4401
2022-08-12 19:44:44 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.4344
2022-08-12 19:45:18 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.1642
2022-08-12 19:45:52 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.2266
2022-08-12 19:46:26 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.5650
2022-08-12 19:46:59 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.2656
2022-08-12 19:47:33 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.3837
2022-08-12 19:48:07 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.4173
2022-08-12 19:48:40 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.5242
2022-08-12 19:49:13 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.3511
2022-08-12 19:49:47 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.2489
2022-08-12 19:50:21 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.4179
2022-08-12 19:50:55 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.5123
2022-08-12 19:51:28 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.5830
2022-08-12 19:52:02 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.5423
2022-08-12 19:52:36 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.2038
2022-08-12 19:53:09 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.4334
2022-08-12 19:53:43 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.2375
2022-08-12 19:54:17 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.3841
2022-08-12 19:54:51 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.5003
2022-08-12 19:55:24 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.4367
2022-08-12 19:55:57 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.2189
2022-08-12 19:56:30 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.6164
2022-08-12 19:57:04 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.3163
2022-08-12 19:57:37 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.2980
2022-08-12 19:58:11 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.3191
2022-08-12 19:58:44 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.1573
2022-08-12 19:58:46 - train: epoch 022, train_loss: 1.3874
2022-08-12 20:00:02 - eval: epoch: 022, acc1: 70.848%, acc5: 90.066%, test_loss: 1.1757, per_image_load_time: 0.706ms, per_image_inference_time: 0.598ms
2022-08-12 20:00:02 - until epoch: 022, best_acc1: 70.848%
2022-08-12 20:00:02 - epoch 023 lr: 0.003511
2022-08-12 20:00:42 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.1878
2022-08-12 20:01:16 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.2511
2022-08-12 20:01:49 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.3440
2022-08-12 20:02:22 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.2504
2022-08-12 20:02:55 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.5059
2022-08-12 20:03:29 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.3058
2022-08-12 20:04:02 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.1552
2022-08-12 20:04:35 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.5620
2022-08-12 20:05:08 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.5011
2022-08-12 20:05:42 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.1699
2022-08-12 20:06:15 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.3224
2022-08-12 20:06:49 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.2996
2022-08-12 20:07:22 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.3650
2022-08-12 20:07:55 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.4015
2022-08-12 20:08:29 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.2355
2022-08-12 20:09:02 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.2596
2022-08-12 20:09:35 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.3027
2022-08-12 20:10:09 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.3182
2022-08-12 20:10:42 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.3755
2022-08-12 20:11:16 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.0420
2022-08-12 20:11:49 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.3057
2022-08-12 20:12:22 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.1656
2022-08-12 20:12:55 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.2227
2022-08-12 20:13:28 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.3690
2022-08-12 20:14:01 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.2075
2022-08-12 20:14:35 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.4744
2022-08-12 20:15:08 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.4160
2022-08-12 20:15:42 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.4026
2022-08-12 20:16:15 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.3376
2022-08-12 20:16:49 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.4721
2022-08-12 20:17:23 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.4480
2022-08-12 20:17:57 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.4550
2022-08-12 20:18:30 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.3845
2022-08-12 20:19:04 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.3240
2022-08-12 20:19:38 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.1931
2022-08-12 20:20:11 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.2680
2022-08-12 20:20:45 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.3091
2022-08-12 20:21:19 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.3352
2022-08-12 20:21:52 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3054
2022-08-12 20:22:26 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2293
2022-08-12 20:23:00 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.2326
2022-08-12 20:23:34 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.1298
2022-08-12 20:24:08 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.2198
2022-08-12 20:24:41 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.1328
2022-08-12 20:25:15 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.1328
2022-08-12 20:25:49 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.3850
2022-08-12 20:26:22 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.2022
2022-08-12 20:26:56 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.1279
2022-08-12 20:27:30 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.2087
2022-08-12 20:28:03 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.5200
2022-08-12 20:28:05 - train: epoch 023, train_loss: 1.3351
2022-08-12 20:29:21 - eval: epoch: 023, acc1: 71.704%, acc5: 90.502%, test_loss: 1.1474, per_image_load_time: 1.865ms, per_image_inference_time: 0.618ms
2022-08-12 20:29:21 - until epoch: 023, best_acc1: 71.704%
2022-08-12 20:29:21 - epoch 024 lr: 0.001571
2022-08-12 20:30:01 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.5146
2022-08-12 20:30:34 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.3165
2022-08-12 20:31:07 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.2467
2022-08-12 20:31:41 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.2322
2022-08-12 20:32:14 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.4166
2022-08-12 20:32:47 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.3495
2022-08-12 20:33:20 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.2271
2022-08-12 20:33:53 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.2004
2022-08-12 20:34:26 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.2214
2022-08-12 20:34:59 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.3896
2022-08-12 20:35:32 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.0459
2022-08-12 20:36:05 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.2674
2022-08-12 20:36:39 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4523
2022-08-12 20:37:11 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.2230
2022-08-12 20:37:44 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.3591
2022-08-12 20:38:17 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.3071
2022-08-12 20:38:50 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.0637
2022-08-12 20:39:23 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.4880
2022-08-12 20:39:56 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.1188
2022-08-12 20:40:30 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.3104
2022-08-12 20:41:03 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.2529
2022-08-12 20:41:37 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.2478
2022-08-12 20:42:10 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.2992
2022-08-12 20:42:44 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.3651
2022-08-12 20:43:17 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.3437
2022-08-12 20:43:51 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.4738
2022-08-12 20:44:24 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.2324
2022-08-12 20:44:57 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.3820
2022-08-12 20:45:31 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.3987
2022-08-12 20:46:04 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.2598
2022-08-12 20:46:38 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.2698
2022-08-12 20:47:11 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.2824
2022-08-12 20:47:45 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.2279
2022-08-12 20:48:18 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.3458
2022-08-12 20:48:51 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.3294
2022-08-12 20:49:25 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.2554
2022-08-12 20:49:58 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.2784
2022-08-12 20:50:32 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.3435
2022-08-12 20:51:05 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.4806
2022-08-12 20:51:39 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.1882
2022-08-12 20:52:12 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.1574
2022-08-12 20:52:45 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.2088
2022-08-12 20:53:19 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.3865
2022-08-12 20:53:52 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.2585
2022-08-12 20:54:25 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.1798
2022-08-12 20:54:58 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.1649
2022-08-12 20:55:31 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.3428
2022-08-12 20:56:05 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.1667
2022-08-12 20:56:38 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.3548
2022-08-12 20:57:11 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.2295
2022-08-12 20:57:12 - train: epoch 024, train_loss: 1.2943
2022-08-12 20:58:28 - eval: epoch: 024, acc1: 71.876%, acc5: 90.738%, test_loss: 1.1298, per_image_load_time: 0.627ms, per_image_inference_time: 0.607ms
2022-08-12 20:58:28 - until epoch: 024, best_acc1: 71.876%
2022-08-12 20:58:28 - epoch 025 lr: 0.000394
2022-08-12 20:59:08 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.2374
2022-08-12 20:59:41 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.2043
2022-08-12 21:00:14 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.3010
2022-08-12 21:00:47 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.2695
2022-08-12 21:01:20 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.1429
2022-08-12 21:01:54 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3061
2022-08-12 21:02:27 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.1690
2022-08-12 21:02:59 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.3227
2022-08-12 21:03:32 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0849
2022-08-12 21:04:06 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.4746
2022-08-12 21:04:38 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.2135
2022-08-12 21:05:11 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.2857
2022-08-12 21:05:43 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3715
2022-08-12 21:06:16 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.4974
2022-08-12 21:06:49 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.1552
2022-08-12 21:07:21 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.0849
2022-08-12 21:07:53 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.1791
2022-08-12 21:08:26 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.0729
2022-08-12 21:08:58 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.1306
2022-08-12 21:09:31 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.2880
2022-08-12 21:10:04 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 0.9619
2022-08-12 21:10:36 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.1343
2022-08-12 21:11:09 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.3441
2022-08-12 21:11:42 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.0641
2022-08-12 21:12:14 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.1672
2022-08-12 21:12:47 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.2074
2022-08-12 21:13:20 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.2625
2022-08-12 21:13:53 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.2147
2022-08-12 21:14:26 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.3294
2022-08-12 21:14:59 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.4421
2022-08-12 21:15:32 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.2359
2022-08-12 21:16:05 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.2810
2022-08-12 21:16:38 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.1616
2022-08-12 21:17:11 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.2661
2022-08-12 21:17:43 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.0262
2022-08-12 21:18:16 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.2423
2022-08-12 21:18:49 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.1624
2022-08-12 21:19:23 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3066
2022-08-12 21:19:55 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.4104
2022-08-12 21:20:28 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.2576
2022-08-12 21:21:02 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.3849
2022-08-12 21:21:35 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.2626
2022-08-12 21:22:08 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.1577
2022-08-12 21:22:41 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.1924
2022-08-12 21:23:14 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.2181
2022-08-12 21:23:47 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.1327
2022-08-12 21:24:20 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.2208
2022-08-12 21:24:53 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.1447
2022-08-12 21:25:27 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.4211
2022-08-12 21:25:59 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.3409
2022-08-12 21:26:01 - train: epoch 025, train_loss: 1.2798
2022-08-12 21:27:16 - eval: epoch: 025, acc1: 71.876%, acc5: 90.730%, test_loss: 1.1274, per_image_load_time: 2.057ms, per_image_inference_time: 0.626ms
2022-08-12 21:27:16 - until epoch: 025, best_acc1: 71.876%
2022-08-12 21:27:16 - train done. train time: 12.170 hours, best_acc1: 71.876%
