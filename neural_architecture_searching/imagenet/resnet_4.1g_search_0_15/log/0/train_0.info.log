2022-08-03 20:08:23 - net_idx: 0
2022-08-03 20:08:23 - net_config: {'stem_width': 64, 'depth': 14, 'w_0': 32, 'w_a': 18.639926481306944, 'w_m': 2.065381050297783}
2022-08-03 20:08:23 - num_classes: 1000
2022-08-03 20:08:23 - input_image_size: 224
2022-08-03 20:08:23 - scale: 1.1428571428571428
2022-08-03 20:08:23 - seed: 0
2022-08-03 20:08:23 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-03 20:08:23 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-03 20:08:23 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fadba72ea00>
2022-08-03 20:08:23 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fadb9da1af0>
2022-08-03 20:08:23 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fadb9da1b20>
2022-08-03 20:08:23 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fadb9da1b80>
2022-08-03 20:08:23 - batch_size: 256
2022-08-03 20:08:23 - num_workers: 16
2022-08-03 20:08:23 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-03 20:08:23 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-03 20:08:23 - epochs: 25
2022-08-03 20:08:23 - print_interval: 100
2022-08-03 20:08:23 - accumulation_steps: 1
2022-08-03 20:08:23 - sync_bn: False
2022-08-03 20:08:23 - apex: True
2022-08-03 20:08:23 - use_ema_model: False
2022-08-03 20:08:23 - ema_model_decay: 0.9999
2022-08-03 20:08:23 - log_dir: ./log
2022-08-03 20:08:23 - checkpoint_dir: ./checkpoints
2022-08-03 20:08:23 - gpus_type: NVIDIA RTX A5000
2022-08-03 20:08:23 - gpus_num: 2
2022-08-03 20:08:23 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fad965d05f0>
2022-08-03 20:08:23 - --------------------parameters--------------------
2022-08-03 20:08:23 - name: conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-03 20:08:23 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-03 20:08:23 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-03 20:08:23 - name: fc.weight, grad: True
2022-08-03 20:08:23 - name: fc.bias, grad: True
2022-08-03 20:08:23 - --------------------buffers--------------------
2022-08-03 20:08:23 - name: conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-03 20:08:23 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-03 20:08:23 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-03 20:08:23 - -----------no weight decay layers--------------
2022-08-03 20:08:23 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-03 20:08:23 - -------------weight decay layers---------------
2022-08-03 20:08:23 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-03 20:08:23 - epoch 001 lr: 0.100000
2022-08-03 20:09:02 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9207
2022-08-03 20:09:35 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.8985
2022-08-03 20:10:07 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8470
2022-08-03 20:10:40 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.8026
2022-08-03 20:11:13 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.7063
2022-08-03 20:11:45 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.5582
2022-08-03 20:12:18 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.6486
2022-08-03 20:12:50 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.5037
2022-08-03 20:13:23 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.3841
2022-08-03 20:13:55 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.4006
2022-08-03 20:14:27 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.3809
2022-08-03 20:15:00 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.2663
2022-08-03 20:15:33 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.2291
2022-08-03 20:16:06 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.1519
2022-08-03 20:16:38 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 6.0747
2022-08-03 20:17:11 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.1241
2022-08-03 20:17:44 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.8351
2022-08-03 20:18:17 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.7978
2022-08-03 20:18:49 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.8401
2022-08-03 20:19:22 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.6563
2022-08-03 20:19:55 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.5868
2022-08-03 20:20:28 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.4317
2022-08-03 20:21:00 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.4746
2022-08-03 20:21:34 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.3517
2022-08-03 20:22:06 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.3811
2022-08-03 20:22:39 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.4318
2022-08-03 20:23:12 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.2651
2022-08-03 20:23:45 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.3839
2022-08-03 20:24:18 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.0954
2022-08-03 20:24:51 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.2144
2022-08-03 20:25:24 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.1646
2022-08-03 20:25:57 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.2089
2022-08-03 20:26:30 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.9399
2022-08-03 20:27:03 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.9701
2022-08-03 20:27:36 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.8446
2022-08-03 20:28:09 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.7877
2022-08-03 20:28:42 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.8418
2022-08-03 20:29:15 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.7662
2022-08-03 20:29:48 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.5504
2022-08-03 20:30:21 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.7890
2022-08-03 20:30:54 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.7571
2022-08-03 20:31:26 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.6811
2022-08-03 20:31:59 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.5653
2022-08-03 20:32:32 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.3805
2022-08-03 20:33:05 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.5446
2022-08-03 20:33:38 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.6359
2022-08-03 20:34:11 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.3975
2022-08-03 20:34:44 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.6032
2022-08-03 20:35:17 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.5372
2022-08-03 20:35:49 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.3622
2022-08-03 20:35:51 - train: epoch 001, train_loss: 5.4921
2022-08-03 20:37:02 - eval: epoch: 001, acc1: 16.446%, acc5: 36.962%, test_loss: 4.3123, per_image_load_time: 1.647ms, per_image_inference_time: 0.627ms
2022-08-03 20:37:02 - until epoch: 001, best_acc1: 16.446%
2022-08-03 20:37:02 - epoch 002 lr: 0.099606
2022-08-03 20:37:41 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.1177
2022-08-03 20:38:13 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.1794
2022-08-03 20:38:45 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.2767
2022-08-03 20:39:18 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.2367
2022-08-03 20:39:50 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.0995
2022-08-03 20:40:23 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.0598
2022-08-03 20:40:55 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.3673
2022-08-03 20:41:28 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 4.1179
2022-08-03 20:42:01 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.9183
2022-08-03 20:42:33 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.1639
2022-08-03 20:43:06 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.0373
2022-08-03 20:43:40 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.8819
2022-08-03 20:44:12 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 4.1374
2022-08-03 20:44:46 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.0621
2022-08-03 20:45:19 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 3.9135
2022-08-03 20:45:52 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.9104
2022-08-03 20:46:25 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.8617
2022-08-03 20:46:58 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.9144
2022-08-03 20:47:31 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.7335
2022-08-03 20:48:04 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.5712
2022-08-03 20:48:38 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.9172
2022-08-03 20:49:12 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.6580
2022-08-03 20:49:45 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.9096
2022-08-03 20:50:18 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.7005
2022-08-03 20:50:51 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.5124
2022-08-03 20:51:24 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.7416
2022-08-03 20:51:58 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.8588
2022-08-03 20:52:31 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.7976
2022-08-03 20:53:04 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.8354
2022-08-03 20:53:37 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.4451
2022-08-03 20:54:11 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.7047
2022-08-03 20:54:44 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.5366
2022-08-03 20:55:17 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.6525
2022-08-03 20:55:50 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.5694
2022-08-03 20:56:23 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.5610
2022-08-03 20:56:56 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.8813
2022-08-03 20:57:30 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.6593
2022-08-03 20:58:03 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.3881
2022-08-03 20:58:36 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.4555
2022-08-03 20:59:09 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.3106
2022-08-03 20:59:43 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.5225
2022-08-03 21:00:16 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.5991
2022-08-03 21:00:48 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.3817
2022-08-03 21:01:22 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.3079
2022-08-03 21:01:55 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.2496
2022-08-03 21:02:28 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.2926
2022-08-03 21:03:02 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.4529
2022-08-03 21:03:35 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.6418
2022-08-03 21:04:08 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.2085
2022-08-03 21:04:40 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.5314
2022-08-03 21:04:41 - train: epoch 002, train_loss: 3.7659
2022-08-03 21:05:54 - eval: epoch: 002, acc1: 29.692%, acc5: 55.324%, test_loss: 3.6840, per_image_load_time: 2.252ms, per_image_inference_time: 0.583ms
2022-08-03 21:05:55 - until epoch: 002, best_acc1: 29.692%
2022-08-03 21:05:55 - epoch 003 lr: 0.098429
2022-08-03 21:06:34 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.2931
2022-08-03 21:07:07 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.3972
2022-08-03 21:07:39 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.3366
2022-08-03 21:08:11 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.3476
2022-08-03 21:08:44 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.5021
2022-08-03 21:09:16 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.1106
2022-08-03 21:09:49 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.3507
2022-08-03 21:10:22 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.3341
2022-08-03 21:10:55 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.2749
2022-08-03 21:11:28 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.4806
2022-08-03 21:12:01 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.1200
2022-08-03 21:12:34 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.2103
2022-08-03 21:13:07 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.1313
2022-08-03 21:13:40 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.0555
2022-08-03 21:14:13 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.3265
2022-08-03 21:14:46 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.0352
2022-08-03 21:15:19 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 2.9331
2022-08-03 21:15:52 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.0776
2022-08-03 21:16:26 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.2654
2022-08-03 21:16:59 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.1755
2022-08-03 21:17:31 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.3289
2022-08-03 21:18:05 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.5525
2022-08-03 21:18:37 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 3.1802
2022-08-03 21:19:11 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.1096
2022-08-03 21:19:43 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.0081
2022-08-03 21:20:17 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.0661
2022-08-03 21:20:50 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.3783
2022-08-03 21:21:24 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.1012
2022-08-03 21:21:56 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.9188
2022-08-03 21:22:30 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.0598
2022-08-03 21:23:04 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.3487
2022-08-03 21:23:36 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.2690
2022-08-03 21:24:10 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0263
2022-08-03 21:24:43 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.2474
2022-08-03 21:25:16 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.8792
2022-08-03 21:25:49 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.9818
2022-08-03 21:26:22 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.1615
2022-08-03 21:26:56 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.0781
2022-08-03 21:27:29 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.3192
2022-08-03 21:28:02 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7685
2022-08-03 21:28:35 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 3.0561
2022-08-03 21:29:08 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 2.9668
2022-08-03 21:29:41 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 3.0871
2022-08-03 21:30:14 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.9422
2022-08-03 21:30:48 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.0606
2022-08-03 21:31:21 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.9434
2022-08-03 21:31:54 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 3.0003
2022-08-03 21:32:27 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.1193
2022-08-03 21:33:01 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.1070
2022-08-03 21:33:33 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.9615
2022-08-03 21:33:34 - train: epoch 003, train_loss: 3.1301
2022-08-03 21:34:47 - eval: epoch: 003, acc1: 36.380%, acc5: 63.520%, test_loss: 2.9236, per_image_load_time: 2.228ms, per_image_inference_time: 0.589ms
2022-08-03 21:34:48 - until epoch: 003, best_acc1: 36.380%
2022-08-03 21:34:48 - epoch 004 lr: 0.096488
2022-08-03 21:35:26 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 3.0914
2022-08-03 21:35:58 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.7854
2022-08-03 21:36:31 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.8703
2022-08-03 21:37:03 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.7223
2022-08-03 21:37:36 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.8375
2022-08-03 21:38:08 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.2312
2022-08-03 21:38:40 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.8710
2022-08-03 21:39:13 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.8583
2022-08-03 21:39:45 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.7710
2022-08-03 21:40:18 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 3.0023
2022-08-03 21:40:51 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 2.9893
2022-08-03 21:41:24 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.8094
2022-08-03 21:41:56 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.7986
2022-08-03 21:42:29 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.7768
2022-08-03 21:43:02 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 3.1058
2022-08-03 21:43:34 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.9051
2022-08-03 21:44:07 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.8329
2022-08-03 21:44:40 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 3.0510
2022-08-03 21:45:13 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.8244
2022-08-03 21:45:46 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.7638
2022-08-03 21:46:20 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.9305
2022-08-03 21:46:53 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 3.0153
2022-08-03 21:47:25 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.6890
2022-08-03 21:47:58 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.6014
2022-08-03 21:48:31 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.7782
2022-08-03 21:49:04 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.6143
2022-08-03 21:49:37 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.6476
2022-08-03 21:50:10 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8628
2022-08-03 21:50:42 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8030
2022-08-03 21:51:16 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 3.0112
2022-08-03 21:51:49 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.7942
2022-08-03 21:52:22 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8847
2022-08-03 21:52:55 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8824
2022-08-03 21:53:28 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.9360
2022-08-03 21:54:01 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.7848
2022-08-03 21:54:33 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 3.0168
2022-08-03 21:55:07 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.9059
2022-08-03 21:55:39 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.4858
2022-08-03 21:56:12 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.5750
2022-08-03 21:56:45 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.7502
2022-08-03 21:57:17 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.7396
2022-08-03 21:57:50 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6509
2022-08-03 21:58:23 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.5399
2022-08-03 21:58:56 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.8263
2022-08-03 21:59:29 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.2435
2022-08-03 22:00:02 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.8290
2022-08-03 22:00:35 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.8124
2022-08-03 22:01:07 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.4675
2022-08-03 22:01:40 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.8631
2022-08-03 22:02:12 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.6969
2022-08-03 22:02:13 - train: epoch 004, train_loss: 2.8210
2022-08-03 22:03:26 - eval: epoch: 004, acc1: 45.238%, acc5: 71.654%, test_loss: 2.4251, per_image_load_time: 1.934ms, per_image_inference_time: 0.598ms
2022-08-03 22:03:26 - until epoch: 004, best_acc1: 45.238%
2022-08-03 22:03:26 - epoch 005 lr: 0.093815
2022-08-03 22:04:05 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.5265
2022-08-03 22:04:37 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.7468
2022-08-03 22:05:09 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.8317
2022-08-03 22:05:41 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.4913
2022-08-03 22:06:13 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.3180
2022-08-03 22:06:45 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.6712
2022-08-03 22:07:18 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.6053
2022-08-03 22:07:51 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.7554
2022-08-03 22:08:23 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.6471
2022-08-03 22:08:56 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.6857
2022-08-03 22:09:28 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.7621
2022-08-03 22:10:01 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.7609
2022-08-03 22:10:34 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.5387
2022-08-03 22:11:06 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.7288
2022-08-03 22:11:39 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.3683
2022-08-03 22:12:12 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.2981
2022-08-03 22:12:45 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.6412
2022-08-03 22:13:18 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.7456
2022-08-03 22:13:50 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.5316
2022-08-03 22:14:23 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.6852
2022-08-03 22:14:56 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.3400
2022-08-03 22:15:29 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.8115
2022-08-03 22:16:02 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.5429
2022-08-03 22:16:35 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.6501
2022-08-03 22:17:07 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.5326
2022-08-03 22:17:41 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.7320
2022-08-03 22:18:14 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.5963
2022-08-03 22:18:47 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.5781
2022-08-03 22:19:20 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.3993
2022-08-03 22:19:52 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.4944
2022-08-03 22:20:25 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.6729
2022-08-03 22:20:58 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.4671
2022-08-03 22:21:31 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.4085
2022-08-03 22:22:05 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.4889
2022-08-03 22:22:38 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.5897
2022-08-03 22:23:11 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6225
2022-08-03 22:23:44 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.6910
2022-08-03 22:24:16 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5386
2022-08-03 22:24:49 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.8129
2022-08-03 22:25:22 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.5409
2022-08-03 22:25:55 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.5672
2022-08-03 22:26:29 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.6353
2022-08-03 22:27:01 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.8519
2022-08-03 22:27:34 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.5481
2022-08-03 22:28:07 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.6533
2022-08-03 22:28:40 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.6555
2022-08-03 22:29:13 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.4349
2022-08-03 22:29:46 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.3044
2022-08-03 22:30:19 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.6830
2022-08-03 22:30:51 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.4691
2022-08-03 22:30:53 - train: epoch 005, train_loss: 2.6171
2022-08-03 22:32:05 - eval: epoch: 005, acc1: 45.910%, acc5: 72.256%, test_loss: 2.3928, per_image_load_time: 1.602ms, per_image_inference_time: 0.617ms
2022-08-03 22:32:05 - until epoch: 005, best_acc1: 45.910%
2022-08-03 22:32:05 - epoch 006 lr: 0.090450
2022-08-03 22:32:44 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.5059
2022-08-03 22:33:17 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.6813
2022-08-03 22:33:49 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.5757
2022-08-03 22:34:22 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.5822
2022-08-03 22:34:55 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.3300
2022-08-03 22:35:28 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.5327
2022-08-03 22:36:00 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.6077
2022-08-03 22:36:33 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.4290
2022-08-03 22:37:06 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3189
2022-08-03 22:37:39 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.4615
2022-08-03 22:38:13 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.6003
2022-08-03 22:38:45 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.5732
2022-08-03 22:39:19 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.4009
2022-08-03 22:39:51 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.3735
2022-08-03 22:40:25 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.8432
2022-08-03 22:40:58 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.4879
2022-08-03 22:41:31 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.6648
2022-08-03 22:42:05 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.6661
2022-08-03 22:42:38 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.2369
2022-08-03 22:43:11 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.6468
2022-08-03 22:43:44 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.5194
2022-08-03 22:44:18 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.3016
2022-08-03 22:44:50 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.3135
2022-08-03 22:45:23 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.4735
2022-08-03 22:45:57 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.5041
2022-08-03 22:46:30 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.3396
2022-08-03 22:47:03 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.5336
2022-08-03 22:47:36 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.3727
2022-08-03 22:48:10 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.6624
2022-08-03 22:48:43 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.6351
2022-08-03 22:49:16 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.2883
2022-08-03 22:49:49 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.3417
2022-08-03 22:50:22 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.4848
2022-08-03 22:50:56 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.5312
2022-08-03 22:51:30 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.5122
2022-08-03 22:52:03 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.5218
2022-08-03 22:52:36 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.5879
2022-08-03 22:53:09 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.4985
2022-08-03 22:53:42 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.3737
2022-08-03 22:54:16 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.5052
2022-08-03 22:54:49 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.4113
2022-08-03 22:55:22 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.2252
2022-08-03 22:55:55 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.4609
2022-08-03 22:56:28 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.4680
2022-08-03 22:57:01 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.4778
2022-08-03 22:57:35 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.4227
2022-08-03 22:58:07 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4227
2022-08-03 22:58:41 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.6103
2022-08-03 22:59:14 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.5051
2022-08-03 22:59:47 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.4247
2022-08-03 22:59:48 - train: epoch 006, train_loss: 2.4802
2022-08-03 23:01:01 - eval: epoch: 006, acc1: 48.348%, acc5: 73.788%, test_loss: 2.4046, per_image_load_time: 2.075ms, per_image_inference_time: 0.598ms
2022-08-03 23:01:01 - until epoch: 006, best_acc1: 48.348%
2022-08-03 23:01:01 - epoch 007 lr: 0.086448
2022-08-03 23:01:39 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.2849
2022-08-03 23:02:12 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.5369
2022-08-03 23:02:45 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.5490
2022-08-03 23:03:18 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.4469
2022-08-03 23:03:50 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.3255
2022-08-03 23:04:23 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.4921
2022-08-03 23:04:56 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.2869
2022-08-03 23:05:29 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.3708
2022-08-03 23:06:02 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.6059
2022-08-03 23:06:34 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.4151
2022-08-03 23:07:07 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.4070
2022-08-03 23:07:41 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.3162
2022-08-03 23:08:13 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.2743
2022-08-03 23:08:46 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.4024
2022-08-03 23:09:19 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.5097
2022-08-03 23:09:52 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.3853
2022-08-03 23:10:25 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.3604
2022-08-03 23:10:58 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.2215
2022-08-03 23:11:32 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.3755
2022-08-03 23:12:05 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.3037
2022-08-03 23:12:38 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.5794
2022-08-03 23:13:10 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.3298
2022-08-03 23:13:43 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.3125
2022-08-03 23:14:16 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.4217
2022-08-03 23:14:49 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.3066
2022-08-03 23:15:23 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.5701
2022-08-03 23:15:56 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.3903
2022-08-03 23:16:30 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.3848
2022-08-03 23:17:03 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.3335
2022-08-03 23:17:35 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.4868
2022-08-03 23:18:09 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.4451
2022-08-03 23:18:42 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.5188
2022-08-03 23:19:15 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.5836
2022-08-03 23:19:48 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.5641
2022-08-03 23:20:21 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.4316
2022-08-03 23:20:55 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.1910
2022-08-03 23:21:27 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.2242
2022-08-03 23:22:00 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.5605
2022-08-03 23:22:33 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.2657
2022-08-03 23:23:06 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.4634
2022-08-03 23:23:40 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.2518
2022-08-03 23:24:13 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2527
2022-08-03 23:24:46 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.5459
2022-08-03 23:25:19 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.2977
2022-08-03 23:25:52 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.3486
2022-08-03 23:26:25 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.4272
2022-08-03 23:26:58 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.2959
2022-08-03 23:27:31 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.6711
2022-08-03 23:28:05 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.4437
2022-08-03 23:28:37 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.3886
2022-08-03 23:28:39 - train: epoch 007, train_loss: 2.3791
2022-08-03 23:29:51 - eval: epoch: 007, acc1: 51.752%, acc5: 77.394%, test_loss: 2.0832, per_image_load_time: 1.201ms, per_image_inference_time: 0.607ms
2022-08-03 23:29:52 - until epoch: 007, best_acc1: 51.752%
2022-08-03 23:29:52 - epoch 008 lr: 0.081870
2022-08-03 23:30:30 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.4515
2022-08-03 23:31:02 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.4209
2022-08-03 23:31:34 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.3147
2022-08-03 23:32:07 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.4219
2022-08-03 23:32:39 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.2402
2022-08-03 23:33:11 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.2256
2022-08-03 23:33:44 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.4067
2022-08-03 23:34:17 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.0421
2022-08-03 23:34:49 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.3212
2022-08-03 23:35:22 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.3251
2022-08-03 23:35:54 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.3489
2022-08-03 23:36:27 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.1070
2022-08-03 23:37:00 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.4468
2022-08-03 23:37:32 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.1906
2022-08-03 23:38:05 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.3161
2022-08-03 23:38:38 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.3183
2022-08-03 23:39:11 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.2715
2022-08-03 23:39:43 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.3673
2022-08-03 23:40:17 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.1382
2022-08-03 23:40:50 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.3897
2022-08-03 23:41:22 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.2097
2022-08-03 23:41:56 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.2359
2022-08-03 23:42:29 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.3712
2022-08-03 23:43:02 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.2927
2022-08-03 23:43:35 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.1827
2022-08-03 23:44:08 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.3462
2022-08-03 23:44:41 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.4476
2022-08-03 23:45:14 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.3033
2022-08-03 23:45:47 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.3619
2022-08-03 23:46:20 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.5036
2022-08-03 23:46:53 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.2496
2022-08-03 23:47:26 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.4327
2022-08-03 23:47:58 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.3465
2022-08-03 23:48:31 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.3885
2022-08-03 23:49:04 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.1205
2022-08-03 23:49:37 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.3935
2022-08-03 23:50:10 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.1212
2022-08-03 23:50:43 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.0531
2022-08-03 23:51:16 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.2601
2022-08-03 23:51:50 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.5892
2022-08-03 23:52:23 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.1384
2022-08-03 23:52:56 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.2389
2022-08-03 23:53:30 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.1598
2022-08-03 23:54:03 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.3616
2022-08-03 23:54:35 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.1901
2022-08-03 23:55:08 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.2643
2022-08-03 23:55:41 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.3475
2022-08-03 23:56:14 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.1049
2022-08-03 23:56:47 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.3874
2022-08-03 23:57:19 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.1683
2022-08-03 23:57:20 - train: epoch 008, train_loss: 2.3016
2022-08-03 23:58:33 - eval: epoch: 008, acc1: 53.844%, acc5: 79.088%, test_loss: 1.9678, per_image_load_time: 1.896ms, per_image_inference_time: 0.578ms
2022-08-03 23:58:33 - until epoch: 008, best_acc1: 53.844%
2022-08-03 23:58:33 - epoch 009 lr: 0.076790
2022-08-03 23:59:12 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 2.1031
2022-08-03 23:59:44 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.1460
2022-08-04 00:00:16 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 1.8769
2022-08-04 00:00:48 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.4601
2022-08-04 00:01:21 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.1262
2022-08-04 00:01:54 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.2090
2022-08-04 00:02:27 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.1944
2022-08-04 00:03:00 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.2320
2022-08-04 00:03:32 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 1.9830
2022-08-04 00:04:05 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1018
2022-08-04 00:04:38 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.5064
2022-08-04 00:05:10 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.3413
2022-08-04 00:05:43 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.3563
2022-08-04 00:06:15 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9625
2022-08-04 00:06:48 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.2296
2022-08-04 00:07:21 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.3687
2022-08-04 00:07:54 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.2409
2022-08-04 00:08:27 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.2050
2022-08-04 00:08:59 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.0494
2022-08-04 00:09:32 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.0665
2022-08-04 00:10:05 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.4019
2022-08-04 00:10:39 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.4204
2022-08-04 00:11:12 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.2025
2022-08-04 00:11:45 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.1740
2022-08-04 00:12:18 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.1246
2022-08-04 00:12:51 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.2079
2022-08-04 00:13:24 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.0552
2022-08-04 00:13:57 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.3205
2022-08-04 00:14:30 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 1.9731
2022-08-04 00:15:03 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1576
2022-08-04 00:15:36 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.4632
2022-08-04 00:16:09 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.2180
2022-08-04 00:16:42 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.3135
2022-08-04 00:17:14 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.4493
2022-08-04 00:17:48 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.1673
2022-08-04 00:18:21 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.0724
2022-08-04 00:18:54 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.2524
2022-08-04 00:19:27 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.3892
2022-08-04 00:20:00 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 1.9607
2022-08-04 00:20:33 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.5467
2022-08-04 00:21:06 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.2341
2022-08-04 00:21:40 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.0116
2022-08-04 00:22:13 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.0718
2022-08-04 00:22:46 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.4678
2022-08-04 00:23:20 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.3267
2022-08-04 00:23:52 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.4615
2022-08-04 00:24:26 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.4099
2022-08-04 00:25:00 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.2939
2022-08-04 00:25:33 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.2421
2022-08-04 00:26:05 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.3300
2022-08-04 00:26:06 - train: epoch 009, train_loss: 2.2346
2022-08-04 00:27:19 - eval: epoch: 009, acc1: 54.718%, acc5: 79.820%, test_loss: 1.9430, per_image_load_time: 2.038ms, per_image_inference_time: 0.591ms
2022-08-04 00:27:19 - until epoch: 009, best_acc1: 54.718%
2022-08-04 00:27:19 - epoch 010 lr: 0.071288
2022-08-04 00:27:58 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.1325
2022-08-04 00:28:31 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.2510
2022-08-04 00:29:04 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.2128
2022-08-04 00:29:36 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.2288
2022-08-04 00:30:09 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.0617
2022-08-04 00:30:42 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.3429
2022-08-04 00:31:14 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.2338
2022-08-04 00:31:47 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.2222
2022-08-04 00:32:19 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.3007
2022-08-04 00:32:52 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.1027
2022-08-04 00:33:25 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.0423
2022-08-04 00:33:57 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.1144
2022-08-04 00:34:31 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 1.9642
2022-08-04 00:35:03 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.5076
2022-08-04 00:35:36 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.9782
2022-08-04 00:36:09 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.3017
2022-08-04 00:36:42 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.1357
2022-08-04 00:37:15 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 1.7831
2022-08-04 00:37:49 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.2218
2022-08-04 00:38:21 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.2026
2022-08-04 00:38:55 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 2.0320
2022-08-04 00:39:27 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.3035
2022-08-04 00:40:02 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.3100
2022-08-04 00:40:34 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.3381
2022-08-04 00:41:08 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.1865
2022-08-04 00:41:41 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.2168
2022-08-04 00:42:14 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.8049
2022-08-04 00:42:47 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.2533
2022-08-04 00:43:20 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.2173
2022-08-04 00:43:53 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1137
2022-08-04 00:44:27 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.4897
2022-08-04 00:45:00 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.2114
2022-08-04 00:45:33 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.3872
2022-08-04 00:46:06 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.3364
2022-08-04 00:46:39 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.2636
2022-08-04 00:47:13 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.5517
2022-08-04 00:47:45 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.0865
2022-08-04 00:48:18 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1675
2022-08-04 00:48:51 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 1.9642
2022-08-04 00:49:24 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.2278
2022-08-04 00:49:57 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.1066
2022-08-04 00:50:30 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.3189
2022-08-04 00:51:03 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.0847
2022-08-04 00:51:37 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 1.9869
2022-08-04 00:52:10 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.1539
2022-08-04 00:52:42 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.2385
2022-08-04 00:53:16 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.2635
2022-08-04 00:53:49 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.0825
2022-08-04 00:54:23 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.2481
2022-08-04 00:54:55 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.0280
2022-08-04 00:54:56 - train: epoch 010, train_loss: 2.1713
2022-08-04 00:56:09 - eval: epoch: 010, acc1: 55.928%, acc5: 80.618%, test_loss: 1.8710, per_image_load_time: 2.192ms, per_image_inference_time: 0.623ms
2022-08-04 00:56:09 - until epoch: 010, best_acc1: 55.928%
2022-08-04 00:56:09 - epoch 011 lr: 0.065450
2022-08-04 00:56:47 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.1808
2022-08-04 00:57:20 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.1139
2022-08-04 00:57:52 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.2480
2022-08-04 00:58:25 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.3737
2022-08-04 00:58:58 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 1.9401
2022-08-04 00:59:30 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.0689
2022-08-04 01:00:03 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.2036
2022-08-04 01:00:35 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.2216
2022-08-04 01:01:08 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.2240
2022-08-04 01:01:41 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.0053
2022-08-04 01:02:14 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.1072
2022-08-04 01:02:46 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.1217
2022-08-04 01:03:20 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.2272
2022-08-04 01:03:52 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.0426
2022-08-04 01:04:25 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.0590
2022-08-04 01:04:58 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.2021
2022-08-04 01:05:32 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.2834
2022-08-04 01:06:05 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.0336
2022-08-04 01:06:38 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.1490
2022-08-04 01:07:11 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1687
2022-08-04 01:07:44 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.1934
2022-08-04 01:08:17 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 1.8830
2022-08-04 01:08:50 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.2642
2022-08-04 01:09:24 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.9809
2022-08-04 01:09:57 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.0873
2022-08-04 01:10:30 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 1.9742
2022-08-04 01:11:04 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 1.9536
2022-08-04 01:11:37 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.9128
2022-08-04 01:12:10 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.2084
2022-08-04 01:12:44 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.5503
2022-08-04 01:13:17 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.1072
2022-08-04 01:13:49 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 1.9857
2022-08-04 01:14:23 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.3568
2022-08-04 01:14:56 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.0663
2022-08-04 01:15:29 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.1018
2022-08-04 01:16:02 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.0888
2022-08-04 01:16:36 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.2303
2022-08-04 01:17:08 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 2.0851
2022-08-04 01:17:41 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.1061
2022-08-04 01:18:14 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 1.9342
2022-08-04 01:18:48 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.9516
2022-08-04 01:19:21 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 1.9934
2022-08-04 01:19:53 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 1.9371
2022-08-04 01:20:27 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.0886
2022-08-04 01:21:00 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.0324
2022-08-04 01:21:34 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 2.0255
2022-08-04 01:22:07 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.8327
2022-08-04 01:22:40 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.9353
2022-08-04 01:23:14 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.0646
2022-08-04 01:23:46 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 1.9951
2022-08-04 01:23:47 - train: epoch 011, train_loss: 2.1061
2022-08-04 01:24:59 - eval: epoch: 011, acc1: 56.566%, acc5: 81.072%, test_loss: 1.8421, per_image_load_time: 1.153ms, per_image_inference_time: 0.609ms
2022-08-04 01:24:59 - until epoch: 011, best_acc1: 56.566%
2022-08-04 01:24:59 - epoch 012 lr: 0.059368
2022-08-04 01:25:38 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.9705
2022-08-04 01:26:10 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.7937
2022-08-04 01:26:42 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 2.0077
2022-08-04 01:27:15 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.1813
2022-08-04 01:27:47 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.0247
2022-08-04 01:28:20 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 2.0016
2022-08-04 01:28:52 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.9539
2022-08-04 01:29:25 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.4121
2022-08-04 01:29:57 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.0124
2022-08-04 01:30:30 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9396
2022-08-04 01:31:03 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.3820
2022-08-04 01:31:35 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.7917
2022-08-04 01:32:08 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.9828
2022-08-04 01:32:41 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.2683
2022-08-04 01:33:14 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 1.8960
2022-08-04 01:33:47 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 1.9350
2022-08-04 01:34:19 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.9440
2022-08-04 01:34:52 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.1363
2022-08-04 01:35:25 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1912
2022-08-04 01:35:58 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.2454
2022-08-04 01:36:31 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 1.9977
2022-08-04 01:37:03 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.0499
2022-08-04 01:37:36 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 2.1646
2022-08-04 01:38:09 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.3499
2022-08-04 01:38:42 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.9537
2022-08-04 01:39:15 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.9142
2022-08-04 01:39:47 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.1182
2022-08-04 01:40:20 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 2.0744
2022-08-04 01:40:53 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.0828
2022-08-04 01:41:26 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.8909
2022-08-04 01:41:59 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.0889
2022-08-04 01:42:32 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.8703
2022-08-04 01:43:05 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.1305
2022-08-04 01:43:38 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 1.9876
2022-08-04 01:44:11 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.0361
2022-08-04 01:44:44 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.9466
2022-08-04 01:45:17 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 1.9155
2022-08-04 01:45:50 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.0549
2022-08-04 01:46:23 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8649
2022-08-04 01:46:56 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.2096
2022-08-04 01:47:29 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 1.9172
2022-08-04 01:48:02 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.8632
2022-08-04 01:48:35 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.2128
2022-08-04 01:49:08 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.9178
2022-08-04 01:49:41 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.9965
2022-08-04 01:50:14 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.0747
2022-08-04 01:50:47 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.8854
2022-08-04 01:51:21 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.1614
2022-08-04 01:51:54 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 2.0555
2022-08-04 01:52:26 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.9322
2022-08-04 01:52:27 - train: epoch 012, train_loss: 2.0477
2022-08-04 01:53:40 - eval: epoch: 012, acc1: 58.780%, acc5: 82.690%, test_loss: 1.7376, per_image_load_time: 2.229ms, per_image_inference_time: 0.558ms
2022-08-04 01:53:40 - until epoch: 012, best_acc1: 58.780%
2022-08-04 01:53:40 - epoch 013 lr: 0.053138
2022-08-04 01:54:20 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.6558
2022-08-04 01:54:52 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.9088
2022-08-04 01:55:24 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.9256
2022-08-04 01:55:56 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.9389
2022-08-04 01:56:28 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 1.9947
2022-08-04 01:57:00 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 2.0010
2022-08-04 01:57:33 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.9946
2022-08-04 01:58:05 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.0312
2022-08-04 01:58:38 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.9161
2022-08-04 01:59:11 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 2.0096
2022-08-04 01:59:44 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.0710
2022-08-04 02:00:17 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.1294
2022-08-04 02:00:50 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.8827
2022-08-04 02:01:24 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.2301
2022-08-04 02:01:57 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.1424
2022-08-04 02:02:29 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.8290
2022-08-04 02:03:03 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.0856
2022-08-04 02:03:36 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9120
2022-08-04 02:04:09 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.1009
2022-08-04 02:04:42 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.2486
2022-08-04 02:05:16 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.2540
2022-08-04 02:05:48 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 2.0032
2022-08-04 02:06:21 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 1.8240
2022-08-04 02:06:54 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.0729
2022-08-04 02:07:28 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.7126
2022-08-04 02:08:00 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.8415
2022-08-04 02:08:34 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 2.0258
2022-08-04 02:09:07 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.0187
2022-08-04 02:09:40 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.1333
2022-08-04 02:10:13 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.9226
2022-08-04 02:10:46 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 2.0660
2022-08-04 02:11:19 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 2.1581
2022-08-04 02:11:52 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.9071
2022-08-04 02:12:26 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 2.1189
2022-08-04 02:12:58 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 2.0797
2022-08-04 02:13:31 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.1094
2022-08-04 02:14:04 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.9687
2022-08-04 02:14:37 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1713
2022-08-04 02:15:10 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.0914
2022-08-04 02:15:43 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 2.0149
2022-08-04 02:16:17 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.9071
2022-08-04 02:16:49 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 1.8944
2022-08-04 02:17:23 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 2.0598
2022-08-04 02:17:56 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.8906
2022-08-04 02:18:30 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.8529
2022-08-04 02:19:03 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.1010
2022-08-04 02:19:35 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.8064
2022-08-04 02:20:09 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 1.9536
2022-08-04 02:20:42 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.2198
2022-08-04 02:21:15 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.1337
2022-08-04 02:21:16 - train: epoch 013, train_loss: 1.9873
2022-08-04 02:22:29 - eval: epoch: 013, acc1: 59.746%, acc5: 83.358%, test_loss: 1.7006, per_image_load_time: 2.113ms, per_image_inference_time: 0.574ms
2022-08-04 02:22:29 - until epoch: 013, best_acc1: 59.746%
2022-08-04 02:22:29 - epoch 014 lr: 0.046859
2022-08-04 02:23:07 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.8668
2022-08-04 02:23:40 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 2.2241
2022-08-04 02:24:14 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.9064
2022-08-04 02:24:46 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 2.0591
2022-08-04 02:25:19 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.9362
2022-08-04 02:25:52 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 2.0462
2022-08-04 02:26:25 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.7109
2022-08-04 02:26:58 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.9314
2022-08-04 02:27:31 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.8362
2022-08-04 02:28:04 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.0004
2022-08-04 02:28:37 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.7726
2022-08-04 02:29:10 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 2.0168
2022-08-04 02:29:43 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 1.9862
2022-08-04 02:30:16 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.9051
2022-08-04 02:30:49 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 1.9968
2022-08-04 02:31:22 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.7706
2022-08-04 02:31:55 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9432
2022-08-04 02:32:28 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 1.9657
2022-08-04 02:33:01 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.8426
2022-08-04 02:33:34 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.8358
2022-08-04 02:34:07 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.0028
2022-08-04 02:34:40 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.7868
2022-08-04 02:35:13 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.8733
2022-08-04 02:35:46 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.8583
2022-08-04 02:36:19 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.9792
2022-08-04 02:36:52 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8555
2022-08-04 02:37:25 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.7803
2022-08-04 02:37:58 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 2.0330
2022-08-04 02:38:31 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9500
2022-08-04 02:39:03 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 2.0478
2022-08-04 02:39:36 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.7452
2022-08-04 02:40:09 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.8597
2022-08-04 02:40:43 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.9543
2022-08-04 02:41:16 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.8145
2022-08-04 02:41:49 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.8828
2022-08-04 02:42:22 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.8884
2022-08-04 02:42:56 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.9891
2022-08-04 02:43:28 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 1.9848
2022-08-04 02:44:02 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.8712
2022-08-04 02:44:35 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.9372
2022-08-04 02:45:08 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.9224
2022-08-04 02:45:42 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.8468
2022-08-04 02:46:14 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.6842
2022-08-04 02:46:48 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.8443
2022-08-04 02:47:21 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.8181
2022-08-04 02:47:54 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.9346
2022-08-04 02:48:28 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 2.0415
2022-08-04 02:49:00 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 2.0156
2022-08-04 02:49:34 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.9510
2022-08-04 02:50:06 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.8695
2022-08-04 02:50:07 - train: epoch 014, train_loss: 1.9264
2022-08-04 02:51:20 - eval: epoch: 014, acc1: 60.016%, acc5: 83.642%, test_loss: 1.6678, per_image_load_time: 2.088ms, per_image_inference_time: 0.601ms
2022-08-04 02:51:20 - until epoch: 014, best_acc1: 60.016%
2022-08-04 02:51:20 - epoch 015 lr: 0.040630
2022-08-04 02:51:59 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.6943
2022-08-04 02:52:31 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 2.0558
2022-08-04 02:53:04 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.0618
2022-08-04 02:53:36 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.8086
2022-08-04 02:54:09 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.7882
2022-08-04 02:54:41 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 1.9538
2022-08-04 02:55:14 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.9197
2022-08-04 02:55:47 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.6925
2022-08-04 02:56:19 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.8880
2022-08-04 02:56:52 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.8525
2022-08-04 02:57:25 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.5264
2022-08-04 02:57:57 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.9936
2022-08-04 02:58:29 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 2.0183
2022-08-04 02:59:02 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.7058
2022-08-04 02:59:36 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.8957
2022-08-04 03:00:08 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.8004
2022-08-04 03:00:41 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.8246
2022-08-04 03:01:14 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.7498
2022-08-04 03:01:47 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.8608
2022-08-04 03:02:20 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.8735
2022-08-04 03:02:53 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.8087
2022-08-04 03:03:27 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9840
2022-08-04 03:04:00 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.8798
2022-08-04 03:04:32 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.9526
2022-08-04 03:05:06 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.9817
2022-08-04 03:05:39 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.7221
2022-08-04 03:06:11 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.8287
2022-08-04 03:06:45 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.8911
2022-08-04 03:07:17 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.9616
2022-08-04 03:07:50 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.7833
2022-08-04 03:08:24 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.7349
2022-08-04 03:08:57 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.8713
2022-08-04 03:09:30 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.8544
2022-08-04 03:10:03 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.7835
2022-08-04 03:10:36 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0650
2022-08-04 03:11:09 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.6743
2022-08-04 03:11:42 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6932
2022-08-04 03:12:15 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.8389
2022-08-04 03:12:48 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8835
2022-08-04 03:13:21 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 2.0252
2022-08-04 03:13:54 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.8346
2022-08-04 03:14:27 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6311
2022-08-04 03:15:00 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.8401
2022-08-04 03:15:33 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.8193
2022-08-04 03:16:07 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.9963
2022-08-04 03:16:39 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.9930
2022-08-04 03:17:12 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.7098
2022-08-04 03:17:46 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.8452
2022-08-04 03:18:18 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.7105
2022-08-04 03:18:51 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 2.0695
2022-08-04 03:18:52 - train: epoch 015, train_loss: 1.8659
2022-08-04 03:20:06 - eval: epoch: 015, acc1: 62.354%, acc5: 85.038%, test_loss: 1.5704, per_image_load_time: 2.313ms, per_image_inference_time: 0.546ms
2022-08-04 03:20:06 - until epoch: 015, best_acc1: 62.354%
2022-08-04 03:20:06 - epoch 016 lr: 0.034548
2022-08-04 03:20:44 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.7218
2022-08-04 03:21:17 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.8005
2022-08-04 03:21:50 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.8866
2022-08-04 03:22:22 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 1.9780
2022-08-04 03:22:55 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.7806
2022-08-04 03:23:28 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.7387
2022-08-04 03:24:01 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.6712
2022-08-04 03:24:34 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.7438
2022-08-04 03:25:06 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.7700
2022-08-04 03:25:39 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.5446
2022-08-04 03:26:11 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.8390
2022-08-04 03:26:44 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.5525
2022-08-04 03:27:18 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.8514
2022-08-04 03:27:50 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.6580
2022-08-04 03:28:24 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 2.1260
2022-08-04 03:28:57 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.7959
2022-08-04 03:29:30 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.9111
2022-08-04 03:30:03 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 2.0558
2022-08-04 03:30:36 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.8585
2022-08-04 03:31:09 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.6209
2022-08-04 03:31:42 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8722
2022-08-04 03:32:15 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.8203
2022-08-04 03:32:49 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.7614
2022-08-04 03:33:22 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.8973
2022-08-04 03:33:54 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.9935
2022-08-04 03:34:28 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.9321
2022-08-04 03:35:01 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.6241
2022-08-04 03:35:34 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.7503
2022-08-04 03:36:06 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.7156
2022-08-04 03:36:40 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.9717
2022-08-04 03:37:13 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.8086
2022-08-04 03:37:46 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.8962
2022-08-04 03:38:19 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.8454
2022-08-04 03:38:52 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.5567
2022-08-04 03:39:26 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.7938
2022-08-04 03:39:58 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.6830
2022-08-04 03:40:32 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.9362
2022-08-04 03:41:05 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.1467
2022-08-04 03:41:38 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.7059
2022-08-04 03:42:11 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.9348
2022-08-04 03:42:44 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.9748
2022-08-04 03:43:18 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.7547
2022-08-04 03:43:51 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.7590
2022-08-04 03:44:24 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.5079
2022-08-04 03:44:58 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.7872
2022-08-04 03:45:31 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.5403
2022-08-04 03:46:04 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 2.0086
2022-08-04 03:46:37 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.8423
2022-08-04 03:47:11 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.7316
2022-08-04 03:47:43 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.8655
2022-08-04 03:47:44 - train: epoch 016, train_loss: 1.7998
2022-08-04 03:48:57 - eval: epoch: 016, acc1: 63.860%, acc5: 85.896%, test_loss: 1.5016, per_image_load_time: 1.496ms, per_image_inference_time: 0.599ms
2022-08-04 03:48:57 - until epoch: 016, best_acc1: 63.860%
2022-08-04 03:48:57 - epoch 017 lr: 0.028710
2022-08-04 03:49:35 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.7287
2022-08-04 03:50:08 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.9196
2022-08-04 03:50:41 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 2.0016
2022-08-04 03:51:14 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.4645
2022-08-04 03:51:47 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.7609
2022-08-04 03:52:20 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 1.9688
2022-08-04 03:52:52 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.8315
2022-08-04 03:53:25 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.6414
2022-08-04 03:53:58 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.7318
2022-08-04 03:54:31 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.6487
2022-08-04 03:55:04 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.8324
2022-08-04 03:55:37 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.6456
2022-08-04 03:56:10 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.7379
2022-08-04 03:56:43 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.6851
2022-08-04 03:57:16 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.6996
2022-08-04 03:57:49 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.6232
2022-08-04 03:58:22 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.5174
2022-08-04 03:58:55 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.6943
2022-08-04 03:59:29 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.6613
2022-08-04 04:00:02 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.6659
2022-08-04 04:00:35 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.8604
2022-08-04 04:01:08 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.4696
2022-08-04 04:01:40 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.7670
2022-08-04 04:02:14 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.6950
2022-08-04 04:02:47 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.9632
2022-08-04 04:03:20 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.7256
2022-08-04 04:03:53 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.5229
2022-08-04 04:04:26 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.6488
2022-08-04 04:04:59 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.8883
2022-08-04 04:05:32 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.4922
2022-08-04 04:06:05 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8441
2022-08-04 04:06:38 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.6168
2022-08-04 04:07:11 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8306
2022-08-04 04:07:44 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.6228
2022-08-04 04:08:17 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.6562
2022-08-04 04:08:51 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.8081
2022-08-04 04:09:24 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.6929
2022-08-04 04:09:57 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.8192
2022-08-04 04:10:30 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.6280
2022-08-04 04:11:03 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.7513
2022-08-04 04:11:36 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.7774
2022-08-04 04:12:09 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.6715
2022-08-04 04:12:42 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.7594
2022-08-04 04:13:15 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8466
2022-08-04 04:13:48 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.8856
2022-08-04 04:14:21 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.6687
2022-08-04 04:14:54 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 2.0430
2022-08-04 04:15:27 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.7146
2022-08-04 04:16:00 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.5665
2022-08-04 04:16:32 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.3916
2022-08-04 04:16:33 - train: epoch 017, train_loss: 1.7313
2022-08-04 04:17:47 - eval: epoch: 017, acc1: 64.686%, acc5: 86.424%, test_loss: 1.4569, per_image_load_time: 2.292ms, per_image_inference_time: 0.548ms
2022-08-04 04:17:47 - until epoch: 017, best_acc1: 64.686%
2022-08-04 04:17:47 - epoch 018 lr: 0.023208
2022-08-04 04:18:26 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.4461
2022-08-04 04:18:58 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.8045
2022-08-04 04:19:30 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.7584
2022-08-04 04:20:03 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.8202
2022-08-04 04:20:35 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.6641
2022-08-04 04:21:08 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.7391
2022-08-04 04:21:40 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.4569
2022-08-04 04:22:12 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.6576
2022-08-04 04:22:45 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.7945
2022-08-04 04:23:18 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.5377
2022-08-04 04:23:51 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.8693
2022-08-04 04:24:23 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.8706
2022-08-04 04:24:56 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.7164
2022-08-04 04:25:29 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.7306
2022-08-04 04:26:02 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.7302
2022-08-04 04:26:35 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.4932
2022-08-04 04:27:08 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7834
2022-08-04 04:27:41 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.7219
2022-08-04 04:28:13 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.6851
2022-08-04 04:28:46 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.9204
2022-08-04 04:29:20 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.8846
2022-08-04 04:29:52 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.7451
2022-08-04 04:30:25 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6243
2022-08-04 04:30:57 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.5382
2022-08-04 04:31:30 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.3581
2022-08-04 04:32:03 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.6443
2022-08-04 04:32:35 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.8049
2022-08-04 04:33:08 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5721
2022-08-04 04:33:41 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.7963
2022-08-04 04:34:14 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.7380
2022-08-04 04:34:47 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.8888
2022-08-04 04:35:20 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.7509
2022-08-04 04:35:53 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.5005
2022-08-04 04:36:26 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.5372
2022-08-04 04:36:59 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.8224
2022-08-04 04:37:32 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.6474
2022-08-04 04:38:05 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.8732
2022-08-04 04:38:38 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.9129
2022-08-04 04:39:11 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.7805
2022-08-04 04:39:45 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.8944
2022-08-04 04:40:18 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7379
2022-08-04 04:40:50 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.7099
2022-08-04 04:41:24 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.4092
2022-08-04 04:41:58 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.7517
2022-08-04 04:42:31 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.5993
2022-08-04 04:43:04 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.4968
2022-08-04 04:43:37 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.8919
2022-08-04 04:44:10 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6604
2022-08-04 04:44:43 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.3366
2022-08-04 04:45:16 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.9764
2022-08-04 04:45:17 - train: epoch 018, train_loss: 1.6590
2022-08-04 04:46:30 - eval: epoch: 018, acc1: 66.186%, acc5: 87.440%, test_loss: 1.3937, per_image_load_time: 1.290ms, per_image_inference_time: 0.586ms
2022-08-04 04:46:30 - until epoch: 018, best_acc1: 66.186%
2022-08-04 04:46:30 - epoch 019 lr: 0.018128
2022-08-04 04:47:09 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4967
2022-08-04 04:47:41 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.5496
2022-08-04 04:48:13 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.6785
2022-08-04 04:48:45 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.2331
2022-08-04 04:49:18 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.6162
2022-08-04 04:49:50 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.6356
2022-08-04 04:50:23 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.4571
2022-08-04 04:50:55 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.5593
2022-08-04 04:51:27 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.6807
2022-08-04 04:52:00 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.6769
2022-08-04 04:52:33 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5937
2022-08-04 04:53:07 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.6305
2022-08-04 04:53:39 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.5512
2022-08-04 04:54:12 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.4981
2022-08-04 04:54:44 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.8095
2022-08-04 04:55:18 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.6706
2022-08-04 04:55:51 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.6337
2022-08-04 04:56:24 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.5027
2022-08-04 04:56:58 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.5243
2022-08-04 04:57:31 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.6061
2022-08-04 04:58:04 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.2309
2022-08-04 04:58:37 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.6382
2022-08-04 04:59:10 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.6200
2022-08-04 04:59:43 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.6759
2022-08-04 05:00:15 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.5317
2022-08-04 05:00:48 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.6747
2022-08-04 05:01:21 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.5722
2022-08-04 05:01:54 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.5821
2022-08-04 05:02:27 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.6469
2022-08-04 05:03:01 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.6264
2022-08-04 05:03:34 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.5573
2022-08-04 05:04:07 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.3383
2022-08-04 05:04:40 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.6779
2022-08-04 05:05:12 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.5120
2022-08-04 05:05:46 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.5120
2022-08-04 05:06:18 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.2848
2022-08-04 05:06:51 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.6709
2022-08-04 05:07:25 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.7338
2022-08-04 05:07:57 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.5239
2022-08-04 05:08:30 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.5402
2022-08-04 05:09:04 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.6825
2022-08-04 05:09:38 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.5615
2022-08-04 05:10:11 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.4830
2022-08-04 05:10:43 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.6675
2022-08-04 05:11:16 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.9403
2022-08-04 05:11:49 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.4953
2022-08-04 05:12:22 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.4884
2022-08-04 05:12:55 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.6334
2022-08-04 05:13:28 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.5649
2022-08-04 05:14:00 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.7609
2022-08-04 05:14:02 - train: epoch 019, train_loss: 1.5873
2022-08-04 05:15:14 - eval: epoch: 019, acc1: 67.140%, acc5: 88.072%, test_loss: 1.3478, per_image_load_time: 0.801ms, per_image_inference_time: 0.586ms
2022-08-04 05:15:14 - until epoch: 019, best_acc1: 67.140%
2022-08-04 05:15:14 - epoch 020 lr: 0.013551
2022-08-04 05:15:53 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.5518
2022-08-04 05:16:26 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.4846
2022-08-04 05:16:58 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.4431
2022-08-04 05:17:31 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.3525
2022-08-04 05:18:04 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.3135
2022-08-04 05:18:37 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.6359
2022-08-04 05:19:10 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.3433
2022-08-04 05:19:43 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.5124
2022-08-04 05:20:16 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.7039
2022-08-04 05:20:49 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.5645
2022-08-04 05:21:22 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.6306
2022-08-04 05:21:55 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.2439
2022-08-04 05:22:29 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.5630
2022-08-04 05:23:02 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.5412
2022-08-04 05:23:35 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.6462
2022-08-04 05:24:08 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.6384
2022-08-04 05:24:41 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.6118
2022-08-04 05:25:14 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.4245
2022-08-04 05:25:46 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.5792
2022-08-04 05:26:19 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.4230
2022-08-04 05:26:52 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.6129
2022-08-04 05:27:25 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.4125
2022-08-04 05:27:58 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.5326
2022-08-04 05:28:31 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.7808
2022-08-04 05:29:04 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.5405
2022-08-04 05:29:37 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.4865
2022-08-04 05:30:10 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.4050
2022-08-04 05:30:43 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.5738
2022-08-04 05:31:16 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.7377
2022-08-04 05:31:49 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.5095
2022-08-04 05:32:22 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.5752
2022-08-04 05:32:55 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.6630
2022-08-04 05:33:29 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.4189
2022-08-04 05:34:02 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.4751
2022-08-04 05:34:36 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.3211
2022-08-04 05:35:09 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.6799
2022-08-04 05:35:42 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.4580
2022-08-04 05:36:15 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.5023
2022-08-04 05:36:48 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.5985
2022-08-04 05:37:21 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.3770
2022-08-04 05:37:53 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.5226
2022-08-04 05:38:27 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.5326
2022-08-04 05:39:00 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.5325
2022-08-04 05:39:33 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.3334
2022-08-04 05:40:07 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.6312
2022-08-04 05:40:40 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.3554
2022-08-04 05:41:13 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.4825
2022-08-04 05:41:46 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4754
2022-08-04 05:42:19 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.5261
2022-08-04 05:42:52 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.5604
2022-08-04 05:42:53 - train: epoch 020, train_loss: 1.5144
2022-08-04 05:44:06 - eval: epoch: 020, acc1: 68.906%, acc5: 89.026%, test_loss: 1.2696, per_image_load_time: 2.216ms, per_image_inference_time: 0.602ms
2022-08-04 05:44:06 - until epoch: 020, best_acc1: 68.906%
2022-08-04 05:44:06 - epoch 021 lr: 0.009548
2022-08-04 05:44:44 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.5238
2022-08-04 05:45:17 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.5427
2022-08-04 05:45:50 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.3383
2022-08-04 05:46:23 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.5138
2022-08-04 05:46:55 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.3418
2022-08-04 05:47:28 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.4299
2022-08-04 05:48:01 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3396
2022-08-04 05:48:34 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.3542
2022-08-04 05:49:07 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.4182
2022-08-04 05:49:39 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3355
2022-08-04 05:50:13 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.3336
2022-08-04 05:50:45 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.2522
2022-08-04 05:51:18 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.3433
2022-08-04 05:51:51 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.3524
2022-08-04 05:52:24 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.4413
2022-08-04 05:52:56 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3677
2022-08-04 05:53:29 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.4377
2022-08-04 05:54:02 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.3365
2022-08-04 05:54:36 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.6536
2022-08-04 05:55:09 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.5110
2022-08-04 05:55:42 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3170
2022-08-04 05:56:16 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.3870
2022-08-04 05:56:49 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.3135
2022-08-04 05:57:22 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.2724
2022-08-04 05:57:55 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.4507
2022-08-04 05:58:28 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.5939
2022-08-04 05:59:02 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.3802
2022-08-04 05:59:35 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.4369
2022-08-04 06:00:07 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.4310
2022-08-04 06:00:40 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.4302
2022-08-04 06:01:14 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.3992
2022-08-04 06:01:47 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.4427
2022-08-04 06:02:20 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.6692
2022-08-04 06:02:54 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.5219
2022-08-04 06:03:26 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4586
2022-08-04 06:04:00 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.2331
2022-08-04 06:04:33 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.2271
2022-08-04 06:05:06 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.5641
2022-08-04 06:05:39 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.4072
2022-08-04 06:06:12 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.7167
2022-08-04 06:06:45 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.2961
2022-08-04 06:07:19 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.3084
2022-08-04 06:07:53 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3830
2022-08-04 06:08:25 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.3962
2022-08-04 06:08:58 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.5083
2022-08-04 06:09:31 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.3339
2022-08-04 06:10:05 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.5933
2022-08-04 06:10:37 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.6438
2022-08-04 06:11:11 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.2045
2022-08-04 06:11:44 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.4102
2022-08-04 06:11:45 - train: epoch 021, train_loss: 1.4420
2022-08-04 06:12:58 - eval: epoch: 021, acc1: 69.606%, acc5: 89.444%, test_loss: 1.2368, per_image_load_time: 1.994ms, per_image_inference_time: 0.608ms
2022-08-04 06:12:58 - until epoch: 021, best_acc1: 69.606%
2022-08-04 06:12:58 - epoch 022 lr: 0.006184
2022-08-04 06:13:36 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.1894
2022-08-04 06:14:09 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.4084
2022-08-04 06:14:42 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.3135
2022-08-04 06:15:15 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.4011
2022-08-04 06:15:48 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3525
2022-08-04 06:16:20 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.6379
2022-08-04 06:16:53 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4164
2022-08-04 06:17:26 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.3877
2022-08-04 06:17:59 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.5156
2022-08-04 06:18:32 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.4134
2022-08-04 06:19:05 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.1085
2022-08-04 06:19:37 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.1599
2022-08-04 06:20:11 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.3196
2022-08-04 06:20:44 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.1833
2022-08-04 06:21:17 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.4501
2022-08-04 06:21:50 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.2055
2022-08-04 06:22:22 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.4842
2022-08-04 06:22:56 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.5886
2022-08-04 06:23:29 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.4534
2022-08-04 06:24:02 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.2732
2022-08-04 06:24:35 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.4128
2022-08-04 06:25:08 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.1594
2022-08-04 06:25:41 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.5243
2022-08-04 06:26:14 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.5202
2022-08-04 06:26:47 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.4022
2022-08-04 06:27:20 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.1797
2022-08-04 06:27:54 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.2534
2022-08-04 06:28:27 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.2556
2022-08-04 06:29:00 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.3093
2022-08-04 06:29:32 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.3527
2022-08-04 06:30:06 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.5237
2022-08-04 06:30:39 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4398
2022-08-04 06:31:12 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.3060
2022-08-04 06:31:45 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.2191
2022-08-04 06:32:18 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.4610
2022-08-04 06:32:51 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.4338
2022-08-04 06:33:25 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.4261
2022-08-04 06:33:58 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.5788
2022-08-04 06:34:32 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.3024
2022-08-04 06:35:04 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.4480
2022-08-04 06:35:38 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.4118
2022-08-04 06:36:11 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.3491
2022-08-04 06:36:44 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4778
2022-08-04 06:37:17 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3299
2022-08-04 06:37:51 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.2513
2022-08-04 06:38:23 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.5350
2022-08-04 06:38:57 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.2352
2022-08-04 06:39:30 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.2867
2022-08-04 06:40:03 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.3001
2022-08-04 06:40:35 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.4557
2022-08-04 06:40:36 - train: epoch 022, train_loss: 1.3754
2022-08-04 06:41:49 - eval: epoch: 022, acc1: 70.798%, acc5: 90.000%, test_loss: 1.1844, per_image_load_time: 1.868ms, per_image_inference_time: 0.603ms
2022-08-04 06:41:49 - until epoch: 022, best_acc1: 70.798%
2022-08-04 06:41:49 - epoch 023 lr: 0.003511
2022-08-04 06:42:28 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.2544
2022-08-04 06:43:00 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.1960
2022-08-04 06:43:33 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.2335
2022-08-04 06:44:05 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.4280
2022-08-04 06:44:37 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.4045
2022-08-04 06:45:10 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.3199
2022-08-04 06:45:42 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.1651
2022-08-04 06:46:14 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3134
2022-08-04 06:46:47 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.3295
2022-08-04 06:47:20 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.2222
2022-08-04 06:47:53 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.1056
2022-08-04 06:48:27 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.1600
2022-08-04 06:49:00 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.1935
2022-08-04 06:49:33 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.3719
2022-08-04 06:50:06 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.1926
2022-08-04 06:50:39 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.3792
2022-08-04 06:51:12 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.2878
2022-08-04 06:51:46 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.3552
2022-08-04 06:52:19 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.4385
2022-08-04 06:52:51 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.2016
2022-08-04 06:53:25 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.4682
2022-08-04 06:53:58 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.1530
2022-08-04 06:54:31 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.3977
2022-08-04 06:55:04 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.3103
2022-08-04 06:55:37 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.2400
2022-08-04 06:56:10 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.4286
2022-08-04 06:56:43 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.2831
2022-08-04 06:57:16 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.2302
2022-08-04 06:57:49 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.3146
2022-08-04 06:58:22 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.4081
2022-08-04 06:58:55 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.3419
2022-08-04 06:59:28 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.6424
2022-08-04 07:00:01 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.2892
2022-08-04 07:00:34 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.3738
2022-08-04 07:01:07 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.2789
2022-08-04 07:01:40 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.2491
2022-08-04 07:02:13 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.3977
2022-08-04 07:02:46 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.3151
2022-08-04 07:03:19 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3344
2022-08-04 07:03:52 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2383
2022-08-04 07:04:25 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.2410
2022-08-04 07:04:58 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.1932
2022-08-04 07:05:31 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.1170
2022-08-04 07:06:05 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.5588
2022-08-04 07:06:37 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.2859
2022-08-04 07:07:10 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.3269
2022-08-04 07:07:43 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.1944
2022-08-04 07:08:16 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.0607
2022-08-04 07:08:49 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.2132
2022-08-04 07:09:22 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.3241
2022-08-04 07:09:23 - train: epoch 023, train_loss: 1.3177
2022-08-04 07:10:36 - eval: epoch: 023, acc1: 71.550%, acc5: 90.478%, test_loss: 1.1557, per_image_load_time: 2.236ms, per_image_inference_time: 0.584ms
2022-08-04 07:10:36 - until epoch: 023, best_acc1: 71.550%
2022-08-04 07:10:36 - epoch 024 lr: 0.001571
2022-08-04 07:11:15 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.3906
2022-08-04 07:11:48 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.2931
2022-08-04 07:12:20 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.2454
2022-08-04 07:12:53 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.1268
2022-08-04 07:13:25 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.0595
2022-08-04 07:13:58 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.3535
2022-08-04 07:14:31 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.3439
2022-08-04 07:15:04 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.2885
2022-08-04 07:15:36 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.3014
2022-08-04 07:16:09 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.2209
2022-08-04 07:16:42 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.0504
2022-08-04 07:17:15 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.2871
2022-08-04 07:17:47 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4323
2022-08-04 07:18:21 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.1980
2022-08-04 07:18:54 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.2523
2022-08-04 07:19:27 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.1564
2022-08-04 07:20:01 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.1484
2022-08-04 07:20:34 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.4161
2022-08-04 07:21:07 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.2868
2022-08-04 07:21:40 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.3421
2022-08-04 07:22:13 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.2267
2022-08-04 07:22:47 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.3720
2022-08-04 07:23:20 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.2391
2022-08-04 07:23:53 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.4878
2022-08-04 07:24:26 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.2819
2022-08-04 07:24:59 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.3847
2022-08-04 07:25:32 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.3485
2022-08-04 07:26:05 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.2969
2022-08-04 07:26:38 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.2073
2022-08-04 07:27:11 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.2314
2022-08-04 07:27:44 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.1995
2022-08-04 07:28:18 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.4428
2022-08-04 07:28:51 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.1584
2022-08-04 07:29:24 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.1101
2022-08-04 07:29:57 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.3637
2022-08-04 07:30:30 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.3022
2022-08-04 07:31:03 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.2776
2022-08-04 07:31:36 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.3161
2022-08-04 07:32:09 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.2862
2022-08-04 07:32:43 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.3585
2022-08-04 07:33:16 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.1730
2022-08-04 07:33:49 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.2691
2022-08-04 07:34:22 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.3444
2022-08-04 07:34:54 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.1880
2022-08-04 07:35:28 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.0767
2022-08-04 07:36:01 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.2864
2022-08-04 07:36:34 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.1724
2022-08-04 07:37:08 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.1066
2022-08-04 07:37:42 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.2535
2022-08-04 07:38:13 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.2101
2022-08-04 07:38:15 - train: epoch 024, train_loss: 1.2808
2022-08-04 07:39:28 - eval: epoch: 024, acc1: 71.888%, acc5: 90.646%, test_loss: 1.1388, per_image_load_time: 1.695ms, per_image_inference_time: 0.600ms
2022-08-04 07:39:28 - until epoch: 024, best_acc1: 71.888%
2022-08-04 07:39:28 - epoch 025 lr: 0.000394
2022-08-04 07:40:07 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.3344
2022-08-04 07:40:39 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1382
2022-08-04 07:41:11 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.1350
2022-08-04 07:41:44 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.4311
2022-08-04 07:42:17 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 0.9839
2022-08-04 07:42:49 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3269
2022-08-04 07:43:21 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.3460
2022-08-04 07:43:55 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.1822
2022-08-04 07:44:27 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.1971
2022-08-04 07:45:00 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.2904
2022-08-04 07:45:33 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.3016
2022-08-04 07:46:06 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.2325
2022-08-04 07:46:39 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.2384
2022-08-04 07:47:13 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.2548
2022-08-04 07:47:45 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.3045
2022-08-04 07:48:18 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1517
2022-08-04 07:48:51 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.2458
2022-08-04 07:49:24 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.1084
2022-08-04 07:49:57 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.3385
2022-08-04 07:50:30 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.2486
2022-08-04 07:51:03 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.1524
2022-08-04 07:51:36 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.0816
2022-08-04 07:52:10 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.2369
2022-08-04 07:52:42 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.0715
2022-08-04 07:53:15 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.1365
2022-08-04 07:53:49 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.3255
2022-08-04 07:54:22 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.2702
2022-08-04 07:54:55 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.3221
2022-08-04 07:55:28 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.3214
2022-08-04 07:56:01 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.3237
2022-08-04 07:56:34 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.2449
2022-08-04 07:57:07 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.3211
2022-08-04 07:57:40 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.2210
2022-08-04 07:58:13 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.2802
2022-08-04 07:58:46 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1158
2022-08-04 07:59:19 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.2968
2022-08-04 07:59:53 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.2317
2022-08-04 08:00:26 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3396
2022-08-04 08:00:59 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.2018
2022-08-04 08:01:32 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.2052
2022-08-04 08:02:05 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.4475
2022-08-04 08:02:38 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.2399
2022-08-04 08:03:11 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.1086
2022-08-04 08:03:44 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.2390
2022-08-04 08:04:18 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.3346
2022-08-04 08:04:51 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.0246
2022-08-04 08:05:24 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.2004
2022-08-04 08:05:57 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.0435
2022-08-04 08:06:29 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.2943
2022-08-04 08:07:01 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.1882
2022-08-04 08:07:03 - train: epoch 025, train_loss: 1.2618
2022-08-04 08:08:16 - eval: epoch: 025, acc1: 71.996%, acc5: 90.620%, test_loss: 1.1358, per_image_load_time: 0.863ms, per_image_inference_time: 0.605ms
2022-08-04 08:08:16 - until epoch: 025, best_acc1: 71.996%
2022-08-04 08:08:16 - train done. train time: 11.997 hours, best_acc1: 71.996%
2022-08-09 22:49:19 - net_idx: 0
2022-08-09 22:49:19 - net_config: {'stem_width': 64, 'depth': 14, 'w_0': 32, 'w_a': 18.639926481306944, 'w_m': 2.065381050297783}
2022-08-09 22:49:19 - num_classes: 1000
2022-08-09 22:49:19 - input_image_size: 224
2022-08-09 22:49:19 - scale: 1.1428571428571428
2022-08-09 22:49:19 - seed: 0
2022-08-09 22:49:19 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-09 22:49:19 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-09 22:49:19 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce2b0>
2022-08-09 22:49:19 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce6d0>
2022-08-09 22:49:19 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce700>
2022-08-09 22:49:19 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce760>
2022-08-09 22:49:19 - batch_size: 256
2022-08-09 22:49:19 - num_workers: 16
2022-08-09 22:49:19 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-09 22:49:19 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-09 22:49:19 - epochs: 25
2022-08-09 22:49:19 - print_interval: 100
2022-08-09 22:49:19 - accumulation_steps: 1
2022-08-09 22:49:19 - sync_bn: False
2022-08-09 22:49:19 - apex: True
2022-08-09 22:49:19 - use_ema_model: False
2022-08-09 22:49:19 - ema_model_decay: 0.9999
2022-08-09 22:49:19 - log_dir: ./log
2022-08-09 22:49:19 - checkpoint_dir: ./checkpoints
2022-08-09 22:49:19 - gpus_type: NVIDIA RTX A5000
2022-08-09 22:49:19 - gpus_num: 2
2022-08-09 22:49:19 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5dddaeb570>
2022-08-09 22:49:19 - --------------------parameters--------------------
2022-08-09 22:49:19 - name: conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-09 22:49:19 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-09 22:49:19 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-09 22:49:19 - name: fc.weight, grad: True
2022-08-09 22:49:19 - name: fc.bias, grad: True
2022-08-09 22:49:19 - --------------------buffers--------------------
2022-08-09 22:49:19 - name: conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:19 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:19 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:19 - -----------no weight decay layers--------------
2022-08-09 22:49:19 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:19 - -------------weight decay layers---------------
2022-08-09 22:49:19 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:19 - resuming model from ./checkpoints/0/latest.pth. resume_epoch: 025, used_time: 11.997 hours, best_acc1: 71.996%, test_loss: 1.1358, lr: 0.000000
2022-08-09 22:49:19 - train done. train time: 11.997 hours, best_acc1: 71.996%
