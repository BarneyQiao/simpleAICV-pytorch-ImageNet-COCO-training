2022-08-12 21:27:16 - net_idx: 12
2022-08-12 21:27:16 - net_config: {'stem_width': 64, 'depth': 18, 'w_0': 32, 'w_a': 17.77597968937125, 'w_m': 1.9609275565633586}
2022-08-12 21:27:16 - num_classes: 1000
2022-08-12 21:27:16 - input_image_size: 224
2022-08-12 21:27:16 - scale: 1.1428571428571428
2022-08-12 21:27:16 - seed: 0
2022-08-12 21:27:16 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-12 21:27:16 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-12 21:27:16 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce2b0>
2022-08-12 21:27:16 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce6d0>
2022-08-12 21:27:16 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce700>
2022-08-12 21:27:16 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce760>
2022-08-12 21:27:16 - batch_size: 256
2022-08-12 21:27:16 - num_workers: 16
2022-08-12 21:27:16 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-12 21:27:16 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-12 21:27:16 - epochs: 25
2022-08-12 21:27:16 - print_interval: 100
2022-08-12 21:27:16 - accumulation_steps: 1
2022-08-12 21:27:16 - sync_bn: False
2022-08-12 21:27:16 - apex: True
2022-08-12 21:27:16 - use_ema_model: False
2022-08-12 21:27:16 - ema_model_decay: 0.9999
2022-08-12 21:27:16 - log_dir: ./log
2022-08-12 21:27:16 - checkpoint_dir: ./checkpoints
2022-08-12 21:27:16 - gpus_type: NVIDIA RTX A5000
2022-08-12 21:27:16 - gpus_num: 2
2022-08-12 21:27:16 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5dddaeb570>
2022-08-12 21:27:16 - ema_model: None
2022-08-12 21:27:17 - --------------------parameters--------------------
2022-08-12 21:27:17 - name: conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.6.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.6.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.6.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.6.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.6.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.6.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.6.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.6.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.6.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.7.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.7.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.7.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.7.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.7.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.7.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.7.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.7.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.7.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.8.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.8.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.8.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.8.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.8.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.8.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.8.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.8.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.8.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.9.conv1.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.9.conv1.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.9.conv1.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.9.conv2.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.9.conv2.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.9.conv2.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: layer4.9.conv3.layer.0.weight, grad: True
2022-08-12 21:27:17 - name: layer4.9.conv3.layer.1.weight, grad: True
2022-08-12 21:27:17 - name: layer4.9.conv3.layer.1.bias, grad: True
2022-08-12 21:27:17 - name: fc.weight, grad: True
2022-08-12 21:27:17 - name: fc.bias, grad: True
2022-08-12 21:27:17 - --------------------buffers--------------------
2022-08-12 21:27:17 - name: conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.6.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.6.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.6.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.6.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.6.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.6.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.6.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.6.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.6.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.7.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.7.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.7.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.7.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.7.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.7.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.7.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.7.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.7.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.8.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.8.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.8.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.8.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.8.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.8.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.8.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.8.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.8.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.9.conv1.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.9.conv1.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.9.conv1.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.9.conv2.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.9.conv2.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.9.conv2.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - name: layer4.9.conv3.layer.1.running_mean, grad: False
2022-08-12 21:27:17 - name: layer4.9.conv3.layer.1.running_var, grad: False
2022-08-12 21:27:17 - name: layer4.9.conv3.layer.1.num_batches_tracked, grad: False
2022-08-12 21:27:17 - -----------no weight decay layers--------------
2022-08-12 21:27:17 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.7.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.7.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.7.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.7.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.7.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.7.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.8.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.8.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.8.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.8.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.8.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.8.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.9.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.9.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.9.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.9.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.9.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.9.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-12 21:27:17 - -------------weight decay layers---------------
2022-08-12 21:27:17 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.6.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.6.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.6.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.7.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.7.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.7.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.8.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.8.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.8.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.9.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.9.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: layer4.9.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-12 21:27:17 - epoch 001 lr: 0.100000
2022-08-12 21:27:56 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9137
2022-08-12 21:28:29 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9069
2022-08-12 21:29:02 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.9017
2022-08-12 21:29:35 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.9066
2022-08-12 21:30:07 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.8590
2022-08-12 21:30:41 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.7480
2022-08-12 21:31:13 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.8366
2022-08-12 21:31:46 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.7519
2022-08-12 21:32:19 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.7058
2022-08-12 21:32:52 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.6663
2022-08-12 21:33:25 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.6489
2022-08-12 21:33:58 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.5046
2022-08-12 21:34:30 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.5644
2022-08-12 21:35:03 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.4372
2022-08-12 21:35:36 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 6.4147
2022-08-12 21:36:09 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.4143
2022-08-12 21:36:42 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 6.1498
2022-08-12 21:37:15 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 6.1931
2022-08-12 21:37:48 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 6.1468
2022-08-12 21:38:21 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 6.0254
2022-08-12 21:38:54 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 6.0496
2022-08-12 21:39:27 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.9310
2022-08-12 21:40:00 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.9957
2022-08-12 21:40:33 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.8679
2022-08-12 21:41:06 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.8086
2022-08-12 21:41:39 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.7700
2022-08-12 21:42:12 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.6851
2022-08-12 21:42:45 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.6287
2022-08-12 21:43:18 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.5369
2022-08-12 21:43:51 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.6381
2022-08-12 21:44:24 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.5030
2022-08-12 21:44:56 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.5795
2022-08-12 21:45:29 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 5.3584
2022-08-12 21:46:02 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 5.3414
2022-08-12 21:46:35 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 5.2583
2022-08-12 21:47:08 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 5.2773
2022-08-12 21:47:41 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 5.3427
2022-08-12 21:48:14 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 5.0928
2022-08-12 21:48:47 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 5.0591
2022-08-12 21:49:20 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 5.1048
2022-08-12 21:49:53 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 5.1595
2022-08-12 21:50:26 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 5.1005
2022-08-12 21:51:00 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.9261
2022-08-12 21:51:33 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.8411
2022-08-12 21:52:06 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.9114
2022-08-12 21:52:39 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 5.1235
2022-08-12 21:53:12 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.7000
2022-08-12 21:53:45 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.9076
2022-08-12 21:54:18 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.9283
2022-08-12 21:54:51 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.6742
2022-08-12 21:54:52 - train: epoch 001, train_loss: 5.8450
2022-08-12 21:56:08 - eval: epoch: 001, acc1: 11.756%, acc5: 28.862%, test_loss: 4.7257, per_image_load_time: 0.744ms, per_image_inference_time: 0.600ms
2022-08-12 21:56:09 - until epoch: 001, best_acc1: 11.756%
2022-08-12 21:56:09 - epoch 002 lr: 0.099606
2022-08-12 21:56:49 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.7078
2022-08-12 21:57:22 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.4665
2022-08-12 21:57:55 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.8485
2022-08-12 21:58:28 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.6060
2022-08-12 21:59:01 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.5177
2022-08-12 21:59:34 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.3710
2022-08-12 22:00:07 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.6890
2022-08-12 22:00:40 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 4.5664
2022-08-12 22:01:13 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 4.3324
2022-08-12 22:01:46 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.4438
2022-08-12 22:02:20 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.3960
2022-08-12 22:02:53 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 4.3570
2022-08-12 22:03:27 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 4.5147
2022-08-12 22:04:00 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.5488
2022-08-12 22:04:33 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 4.2528
2022-08-12 22:05:07 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 4.1032
2022-08-12 22:05:40 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 4.3722
2022-08-12 22:06:13 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 4.2208
2022-08-12 22:06:47 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.9707
2022-08-12 22:07:21 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.7789
2022-08-12 22:07:54 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 4.0243
2022-08-12 22:08:28 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.9102
2022-08-12 22:09:01 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 4.1224
2022-08-12 22:09:35 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 4.0286
2022-08-12 22:10:08 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.8744
2022-08-12 22:10:41 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.8370
2022-08-12 22:11:15 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 4.1152
2022-08-12 22:11:48 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 4.0342
2022-08-12 22:12:22 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.7713
2022-08-12 22:12:55 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.8260
2022-08-12 22:13:28 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.9272
2022-08-12 22:14:02 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.8373
2022-08-12 22:14:35 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.9174
2022-08-12 22:15:09 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.8873
2022-08-12 22:15:42 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.7009
2022-08-12 22:16:16 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.8198
2022-08-12 22:16:49 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.8279
2022-08-12 22:17:23 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.5023
2022-08-12 22:17:57 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 4.0294
2022-08-12 22:18:30 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.5513
2022-08-12 22:19:04 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.8609
2022-08-12 22:19:37 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.6696
2022-08-12 22:20:10 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.5229
2022-08-12 22:20:44 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.4900
2022-08-12 22:21:17 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.4148
2022-08-12 22:21:51 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.3751
2022-08-12 22:22:24 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.3399
2022-08-12 22:22:58 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.6136
2022-08-12 22:23:31 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.3625
2022-08-12 22:24:04 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.3434
2022-08-12 22:24:06 - train: epoch 002, train_loss: 4.0308
2022-08-12 22:25:21 - eval: epoch: 002, acc1: 26.924%, acc5: 51.924%, test_loss: 3.6705, per_image_load_time: 0.707ms, per_image_inference_time: 0.607ms
2022-08-12 22:25:22 - until epoch: 002, best_acc1: 26.924%
2022-08-12 22:25:22 - epoch 003 lr: 0.098429
2022-08-12 22:26:02 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.5701
2022-08-12 22:26:35 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.3585
2022-08-12 22:27:08 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.5081
2022-08-12 22:27:41 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.3787
2022-08-12 22:28:15 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.4716
2022-08-12 22:28:48 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.2276
2022-08-12 22:29:21 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.4485
2022-08-12 22:29:54 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.6606
2022-08-12 22:30:27 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.4365
2022-08-12 22:31:00 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.2942
2022-08-12 22:31:33 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.1811
2022-08-12 22:32:07 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.3815
2022-08-12 22:32:40 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.2335
2022-08-12 22:33:14 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.3613
2022-08-12 22:33:47 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.5757
2022-08-12 22:34:20 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.1825
2022-08-12 22:34:54 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.2178
2022-08-12 22:35:27 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.0814
2022-08-12 22:36:00 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.2147
2022-08-12 22:36:34 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.1479
2022-08-12 22:37:07 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.3160
2022-08-12 22:37:40 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.4952
2022-08-12 22:38:14 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 3.1865
2022-08-12 22:38:47 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.2520
2022-08-12 22:39:21 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.3137
2022-08-12 22:39:54 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.2230
2022-08-12 22:40:28 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.3868
2022-08-12 22:41:01 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.2986
2022-08-12 22:41:34 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 3.2662
2022-08-12 22:42:08 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.0958
2022-08-12 22:42:41 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.7071
2022-08-12 22:43:14 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.0926
2022-08-12 22:43:48 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.1567
2022-08-12 22:44:21 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.3556
2022-08-12 22:44:55 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 3.0936
2022-08-12 22:45:28 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 3.1048
2022-08-12 22:46:02 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.2281
2022-08-12 22:46:36 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.0678
2022-08-12 22:47:09 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.4271
2022-08-12 22:47:42 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.6763
2022-08-12 22:48:16 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 3.0196
2022-08-12 22:48:49 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.1823
2022-08-12 22:49:22 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 3.2388
2022-08-12 22:49:56 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.8690
2022-08-12 22:50:29 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 2.9919
2022-08-12 22:51:03 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.8674
2022-08-12 22:51:36 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 3.0121
2022-08-12 22:52:10 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.2763
2022-08-12 22:52:43 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.1499
2022-08-12 22:53:16 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 3.0910
2022-08-12 22:53:18 - train: epoch 003, train_loss: 3.2185
2022-08-12 22:54:34 - eval: epoch: 003, acc1: 38.404%, acc5: 64.932%, test_loss: 2.8240, per_image_load_time: 0.696ms, per_image_inference_time: 0.585ms
2022-08-12 22:54:34 - until epoch: 003, best_acc1: 38.404%
2022-08-12 22:54:34 - epoch 004 lr: 0.096488
2022-08-12 22:55:14 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 3.0844
2022-08-12 22:55:48 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.8680
2022-08-12 22:56:21 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 3.0363
2022-08-12 22:56:54 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 3.0735
2022-08-12 22:57:27 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.8340
2022-08-12 22:58:00 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.1200
2022-08-12 22:58:34 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.7843
2022-08-12 22:59:07 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.8714
2022-08-12 22:59:40 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.8090
2022-08-12 23:00:13 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.8690
2022-08-12 23:00:46 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 2.9721
2022-08-12 23:01:19 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.8510
2022-08-12 23:01:53 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.9074
2022-08-12 23:02:26 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.8506
2022-08-12 23:02:59 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 3.0821
2022-08-12 23:03:32 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.9056
2022-08-12 23:04:05 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.9053
2022-08-12 23:04:38 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 3.1416
2022-08-12 23:05:12 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.9515
2022-08-12 23:05:45 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.7990
2022-08-12 23:06:19 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.9666
2022-08-12 23:06:52 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.9455
2022-08-12 23:07:25 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.5420
2022-08-12 23:07:59 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.5901
2022-08-12 23:08:32 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.7659
2022-08-12 23:09:06 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.8526
2022-08-12 23:09:39 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.6735
2022-08-12 23:10:13 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8306
2022-08-12 23:10:46 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8564
2022-08-12 23:11:20 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.9142
2022-08-12 23:11:54 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.7804
2022-08-12 23:12:27 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.9674
2022-08-12 23:13:00 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.9190
2022-08-12 23:13:34 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.6724
2022-08-12 23:14:07 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.6905
2022-08-12 23:14:41 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.8688
2022-08-12 23:15:14 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.5774
2022-08-12 23:15:47 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.5971
2022-08-12 23:16:21 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.8004
2022-08-12 23:16:54 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.5879
2022-08-12 23:17:27 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.6902
2022-08-12 23:18:01 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6291
2022-08-12 23:18:34 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.7453
2022-08-12 23:19:07 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.5163
2022-08-12 23:19:40 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.3618
2022-08-12 23:20:13 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.6023
2022-08-12 23:20:47 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.7275
2022-08-12 23:21:20 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.4975
2022-08-12 23:21:53 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.7316
2022-08-12 23:22:26 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.7282
2022-08-12 23:22:28 - train: epoch 004, train_loss: 2.8403
2022-08-12 23:23:44 - eval: epoch: 004, acc1: 43.876%, acc5: 70.356%, test_loss: 2.5399, per_image_load_time: 0.715ms, per_image_inference_time: 0.609ms
2022-08-12 23:23:45 - until epoch: 004, best_acc1: 43.876%
2022-08-12 23:23:45 - epoch 005 lr: 0.093815
2022-08-12 23:24:24 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.7785
2022-08-12 23:24:57 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.5991
2022-08-12 23:25:30 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.9480
2022-08-12 23:26:04 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.7405
2022-08-12 23:26:37 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.5550
2022-08-12 23:27:11 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.6607
2022-08-12 23:27:44 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.6681
2022-08-12 23:28:17 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.9214
2022-08-12 23:28:50 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.6897
2022-08-12 23:29:23 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.5105
2022-08-12 23:29:57 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.7038
2022-08-12 23:30:30 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.5761
2022-08-12 23:31:03 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.4321
2022-08-12 23:31:36 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.8507
2022-08-12 23:32:09 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.5486
2022-08-12 23:32:42 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.4653
2022-08-12 23:33:15 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.6189
2022-08-12 23:33:48 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.3973
2022-08-12 23:34:22 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.5719
2022-08-12 23:34:55 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.5736
2022-08-12 23:35:29 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.3027
2022-08-12 23:36:02 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.5837
2022-08-12 23:36:36 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.2820
2022-08-12 23:37:09 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.4985
2022-08-12 23:37:42 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.6627
2022-08-12 23:38:15 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.6693
2022-08-12 23:38:49 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.6455
2022-08-12 23:39:22 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.5722
2022-08-12 23:39:55 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.4952
2022-08-12 23:40:29 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.7445
2022-08-12 23:41:02 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.8506
2022-08-12 23:41:35 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.4791
2022-08-12 23:42:09 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.6348
2022-08-12 23:42:42 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.3884
2022-08-12 23:43:16 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.5304
2022-08-12 23:43:49 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.4532
2022-08-12 23:44:22 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.6583
2022-08-12 23:44:55 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5166
2022-08-12 23:45:29 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.7427
2022-08-12 23:46:02 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.6221
2022-08-12 23:46:36 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.5585
2022-08-12 23:47:09 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.7108
2022-08-12 23:47:43 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.5067
2022-08-12 23:48:17 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.8539
2022-08-12 23:48:50 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.7134
2022-08-12 23:49:24 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.5544
2022-08-12 23:49:58 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.5209
2022-08-12 23:50:31 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.5135
2022-08-12 23:51:05 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.6657
2022-08-12 23:51:37 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.2841
2022-08-12 23:51:39 - train: epoch 005, train_loss: 2.6189
2022-08-12 23:52:55 - eval: epoch: 005, acc1: 48.108%, acc5: 74.064%, test_loss: 2.2777, per_image_load_time: 0.619ms, per_image_inference_time: 0.614ms
2022-08-12 23:52:55 - until epoch: 005, best_acc1: 48.108%
2022-08-12 23:52:55 - epoch 006 lr: 0.090450
2022-08-12 23:53:36 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.4731
2022-08-12 23:54:08 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.5983
2022-08-12 23:54:42 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.5009
2022-08-12 23:55:15 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.3445
2022-08-12 23:55:48 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.3331
2022-08-12 23:56:22 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.5739
2022-08-12 23:56:55 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.5807
2022-08-12 23:57:28 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.3810
2022-08-12 23:58:02 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3484
2022-08-12 23:58:35 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.3154
2022-08-12 23:59:09 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.5441
2022-08-12 23:59:43 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.5286
2022-08-13 00:00:16 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5107
2022-08-13 00:00:50 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.4774
2022-08-13 00:01:23 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.6754
2022-08-13 00:01:57 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.4512
2022-08-13 00:02:31 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.6163
2022-08-13 00:03:04 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.4361
2022-08-13 00:03:38 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.3733
2022-08-13 00:04:12 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.5860
2022-08-13 00:04:46 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.3757
2022-08-13 00:05:19 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.0981
2022-08-13 00:05:53 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.3389
2022-08-13 00:06:27 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.3432
2022-08-13 00:07:00 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4024
2022-08-13 00:07:34 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.2952
2022-08-13 00:08:07 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.4314
2022-08-13 00:08:41 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.1984
2022-08-13 00:09:14 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.4684
2022-08-13 00:09:48 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.4986
2022-08-13 00:10:22 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.4343
2022-08-13 00:10:55 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.7351
2022-08-13 00:11:29 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.2758
2022-08-13 00:12:03 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.6842
2022-08-13 00:12:37 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.6975
2022-08-13 00:13:10 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.3629
2022-08-13 00:13:44 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.4973
2022-08-13 00:14:18 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.2618
2022-08-13 00:14:51 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.4067
2022-08-13 00:15:24 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.6284
2022-08-13 00:15:58 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.5803
2022-08-13 00:16:32 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.3058
2022-08-13 00:17:05 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.3979
2022-08-13 00:17:39 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.4846
2022-08-13 00:18:12 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.4729
2022-08-13 00:18:46 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.5674
2022-08-13 00:19:19 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4955
2022-08-13 00:19:53 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.4222
2022-08-13 00:20:26 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.7051
2022-08-13 00:20:59 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.2838
2022-08-13 00:21:01 - train: epoch 006, train_loss: 2.4673
2022-08-13 00:22:17 - eval: epoch: 006, acc1: 50.514%, acc5: 75.922%, test_loss: 2.1609, per_image_load_time: 1.011ms, per_image_inference_time: 0.615ms
2022-08-13 00:22:17 - until epoch: 006, best_acc1: 50.514%
2022-08-13 00:22:17 - epoch 007 lr: 0.086448
2022-08-13 00:22:57 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.1905
2022-08-13 00:23:30 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.5123
2022-08-13 00:24:04 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.4124
2022-08-13 00:24:37 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.2662
2022-08-13 00:25:10 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.2449
2022-08-13 00:25:43 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.5293
2022-08-13 00:26:16 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.4466
2022-08-13 00:26:49 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.5108
2022-08-13 00:27:23 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.4172
2022-08-13 00:27:56 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.5268
2022-08-13 00:28:30 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.3531
2022-08-13 00:29:03 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.2293
2022-08-13 00:29:36 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.0572
2022-08-13 00:30:10 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.3233
2022-08-13 00:30:43 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.3238
2022-08-13 00:31:16 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.4634
2022-08-13 00:31:50 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.4107
2022-08-13 00:32:23 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.3149
2022-08-13 00:32:56 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.5503
2022-08-13 00:33:30 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.1409
2022-08-13 00:34:03 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.4341
2022-08-13 00:34:36 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.2805
2022-08-13 00:35:09 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.5305
2022-08-13 00:35:42 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.6900
2022-08-13 00:36:15 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.3456
2022-08-13 00:36:48 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.3085
2022-08-13 00:37:22 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.2518
2022-08-13 00:37:55 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.1832
2022-08-13 00:38:29 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.1370
2022-08-13 00:39:02 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.3360
2022-08-13 00:39:36 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.2590
2022-08-13 00:40:09 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.4172
2022-08-13 00:40:43 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.5633
2022-08-13 00:41:17 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.3815
2022-08-13 00:41:50 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.3051
2022-08-13 00:42:23 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.2225
2022-08-13 00:42:56 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.3379
2022-08-13 00:43:29 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.4237
2022-08-13 00:44:03 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.2884
2022-08-13 00:44:36 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.4933
2022-08-13 00:45:09 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.1842
2022-08-13 00:45:43 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.1257
2022-08-13 00:46:16 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.4771
2022-08-13 00:46:50 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.0639
2022-08-13 00:47:23 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.4390
2022-08-13 00:47:57 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.4858
2022-08-13 00:48:30 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.2274
2022-08-13 00:49:04 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.4113
2022-08-13 00:49:38 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.3431
2022-08-13 00:50:11 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.3036
2022-08-13 00:50:12 - train: epoch 007, train_loss: 2.3580
2022-08-13 00:51:29 - eval: epoch: 007, acc1: 52.942%, acc5: 78.470%, test_loss: 2.0129, per_image_load_time: 0.732ms, per_image_inference_time: 0.611ms
2022-08-13 00:51:29 - until epoch: 007, best_acc1: 52.942%
2022-08-13 00:51:29 - epoch 008 lr: 0.081870
2022-08-13 00:52:09 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.2613
2022-08-13 00:52:41 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.3358
2022-08-13 00:53:14 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.2473
2022-08-13 00:53:47 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.2492
2022-08-13 00:54:20 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.0518
2022-08-13 00:54:53 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.4182
2022-08-13 00:55:27 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.3482
2022-08-13 00:55:59 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.2534
2022-08-13 00:56:32 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.2728
2022-08-13 00:57:05 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.2152
2022-08-13 00:57:39 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.1941
2022-08-13 00:58:12 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.0719
2022-08-13 00:58:45 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.4201
2022-08-13 00:59:19 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.0617
2022-08-13 00:59:52 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.2288
2022-08-13 01:00:25 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.3518
2022-08-13 01:00:58 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.1140
2022-08-13 01:01:31 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.1856
2022-08-13 01:02:05 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.3227
2022-08-13 01:02:38 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.3771
2022-08-13 01:03:12 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.2508
2022-08-13 01:03:45 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.2231
2022-08-13 01:04:18 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.3125
2022-08-13 01:04:52 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.1042
2022-08-13 01:05:25 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.4736
2022-08-13 01:05:59 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.1928
2022-08-13 01:06:31 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.3819
2022-08-13 01:07:05 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.2842
2022-08-13 01:07:38 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.2557
2022-08-13 01:08:12 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.5453
2022-08-13 01:08:45 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.2898
2022-08-13 01:09:19 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.4529
2022-08-13 01:09:53 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.3041
2022-08-13 01:10:26 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.2235
2022-08-13 01:11:00 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.1927
2022-08-13 01:11:33 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.2567
2022-08-13 01:12:07 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.0248
2022-08-13 01:12:41 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.1811
2022-08-13 01:13:14 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.2918
2022-08-13 01:13:48 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.4491
2022-08-13 01:14:21 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.1009
2022-08-13 01:14:55 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.0831
2022-08-13 01:15:28 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.2808
2022-08-13 01:16:02 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.3264
2022-08-13 01:16:35 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.1904
2022-08-13 01:17:09 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.1694
2022-08-13 01:17:42 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.1440
2022-08-13 01:18:16 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.2804
2022-08-13 01:18:49 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.3854
2022-08-13 01:19:23 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.0672
2022-08-13 01:19:24 - train: epoch 008, train_loss: 2.2677
2022-08-13 01:20:40 - eval: epoch: 008, acc1: 53.294%, acc5: 78.766%, test_loss: 1.9978, per_image_load_time: 0.778ms, per_image_inference_time: 0.602ms
2022-08-13 01:20:40 - until epoch: 008, best_acc1: 53.294%
2022-08-13 01:20:40 - epoch 009 lr: 0.076790
2022-08-13 01:21:21 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 1.9415
2022-08-13 01:21:54 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.1903
2022-08-13 01:22:27 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.0268
2022-08-13 01:23:00 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.3724
2022-08-13 01:23:33 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.0810
2022-08-13 01:24:07 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.0917
2022-08-13 01:24:40 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.1530
2022-08-13 01:25:14 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.1739
2022-08-13 01:25:47 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 1.9961
2022-08-13 01:26:21 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1788
2022-08-13 01:26:54 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.4342
2022-08-13 01:27:27 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.1958
2022-08-13 01:28:01 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.1634
2022-08-13 01:28:34 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9234
2022-08-13 01:29:08 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.0501
2022-08-13 01:29:42 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2626
2022-08-13 01:30:15 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.2430
2022-08-13 01:30:49 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.2259
2022-08-13 01:31:22 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.0260
2022-08-13 01:31:55 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 1.9163
2022-08-13 01:32:28 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.0625
2022-08-13 01:33:02 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.3007
2022-08-13 01:33:35 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.0412
2022-08-13 01:34:09 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.2404
2022-08-13 01:34:42 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.1443
2022-08-13 01:35:16 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.1993
2022-08-13 01:35:49 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.2189
2022-08-13 01:36:23 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2292
2022-08-13 01:36:56 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 1.9607
2022-08-13 01:37:30 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.0347
2022-08-13 01:38:03 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.1949
2022-08-13 01:38:37 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.2178
2022-08-13 01:39:10 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.1224
2022-08-13 01:39:43 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.3223
2022-08-13 01:40:17 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.2687
2022-08-13 01:40:50 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.2703
2022-08-13 01:41:24 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.1230
2022-08-13 01:41:57 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.0445
2022-08-13 01:42:31 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.1660
2022-08-13 01:43:04 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.3994
2022-08-13 01:43:38 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.1348
2022-08-13 01:44:12 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 1.8865
2022-08-13 01:44:45 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 1.8760
2022-08-13 01:45:19 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.3019
2022-08-13 01:45:52 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.2536
2022-08-13 01:46:26 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.2756
2022-08-13 01:46:59 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.2998
2022-08-13 01:47:33 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.2528
2022-08-13 01:48:07 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.3658
2022-08-13 01:48:40 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.0634
2022-08-13 01:48:41 - train: epoch 009, train_loss: 2.1934
2022-08-13 01:49:56 - eval: epoch: 009, acc1: 55.692%, acc5: 80.424%, test_loss: 1.8841, per_image_load_time: 0.787ms, per_image_inference_time: 0.605ms
2022-08-13 01:49:57 - until epoch: 009, best_acc1: 55.692%
2022-08-13 01:49:57 - epoch 010 lr: 0.071288
2022-08-13 01:50:37 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 1.9188
2022-08-13 01:51:10 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.2682
2022-08-13 01:51:43 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.0965
2022-08-13 01:52:16 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.2425
2022-08-13 01:52:50 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.2087
2022-08-13 01:53:23 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.3111
2022-08-13 01:53:56 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.0411
2022-08-13 01:54:29 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.1894
2022-08-13 01:55:02 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 1.9475
2022-08-13 01:55:35 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 1.9942
2022-08-13 01:56:08 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.1320
2022-08-13 01:56:41 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.0134
2022-08-13 01:57:14 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.0299
2022-08-13 01:57:47 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.2585
2022-08-13 01:58:21 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.8914
2022-08-13 01:58:54 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.1949
2022-08-13 01:59:27 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.1334
2022-08-13 02:00:01 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 1.8435
2022-08-13 02:00:34 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.3407
2022-08-13 02:01:07 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.1879
2022-08-13 02:01:40 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 2.0279
2022-08-13 02:02:13 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.1244
2022-08-13 02:02:47 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.2623
2022-08-13 02:03:20 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.2723
2022-08-13 02:03:53 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.0662
2022-08-13 02:04:26 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.2668
2022-08-13 02:05:00 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.8441
2022-08-13 02:05:33 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.0310
2022-08-13 02:06:06 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.3437
2022-08-13 02:06:39 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.0379
2022-08-13 02:07:12 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.4089
2022-08-13 02:07:45 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.1132
2022-08-13 02:08:19 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.1875
2022-08-13 02:08:52 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 1.8990
2022-08-13 02:09:25 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.3307
2022-08-13 02:09:58 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.1867
2022-08-13 02:10:31 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.1682
2022-08-13 02:11:04 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1936
2022-08-13 02:11:38 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 1.9694
2022-08-13 02:12:11 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 1.9112
2022-08-13 02:12:44 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 1.9730
2022-08-13 02:13:18 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.1591
2022-08-13 02:13:51 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.0035
2022-08-13 02:14:24 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 1.9508
2022-08-13 02:14:58 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.0475
2022-08-13 02:15:31 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 1.8679
2022-08-13 02:16:05 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.2133
2022-08-13 02:16:38 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 1.8342
2022-08-13 02:17:12 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.2054
2022-08-13 02:17:44 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.0777
2022-08-13 02:17:46 - train: epoch 010, train_loss: 2.1211
2022-08-13 02:19:02 - eval: epoch: 010, acc1: 57.482%, acc5: 81.622%, test_loss: 1.8045, per_image_load_time: 0.605ms, per_image_inference_time: 0.582ms
2022-08-13 02:19:03 - until epoch: 010, best_acc1: 57.482%
2022-08-13 02:19:03 - epoch 011 lr: 0.065450
2022-08-13 02:19:43 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.1300
2022-08-13 02:20:15 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.0481
2022-08-13 02:20:48 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.1785
2022-08-13 02:21:21 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.2614
2022-08-13 02:21:54 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.0623
2022-08-13 02:22:27 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.0740
2022-08-13 02:23:00 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.1870
2022-08-13 02:23:34 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.0863
2022-08-13 02:24:06 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.1698
2022-08-13 02:24:39 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 1.7662
2022-08-13 02:25:12 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.1630
2022-08-13 02:25:46 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.1824
2022-08-13 02:26:19 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.1242
2022-08-13 02:26:52 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.1648
2022-08-13 02:27:26 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.2442
2022-08-13 02:27:59 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.1560
2022-08-13 02:28:33 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.2176
2022-08-13 02:29:06 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 1.8354
2022-08-13 02:29:39 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 1.9589
2022-08-13 02:30:13 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.0225
2022-08-13 02:30:46 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.0044
2022-08-13 02:31:20 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 2.0451
2022-08-13 02:31:53 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.0945
2022-08-13 02:32:26 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.9367
2022-08-13 02:32:59 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.0485
2022-08-13 02:33:33 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.1137
2022-08-13 02:34:06 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.0376
2022-08-13 02:34:40 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.7123
2022-08-13 02:35:13 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.1019
2022-08-13 02:35:46 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.4330
2022-08-13 02:36:19 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 1.8926
2022-08-13 02:36:52 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.1109
2022-08-13 02:37:26 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.2291
2022-08-13 02:38:00 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.1033
2022-08-13 02:38:33 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 1.9340
2022-08-13 02:39:06 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 1.8596
2022-08-13 02:39:39 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.1437
2022-08-13 02:40:13 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.9841
2022-08-13 02:40:46 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.0660
2022-08-13 02:41:19 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 1.7743
2022-08-13 02:41:53 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.7913
2022-08-13 02:42:26 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 1.9818
2022-08-13 02:42:59 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 1.9737
2022-08-13 02:43:32 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 1.9595
2022-08-13 02:44:05 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 1.8944
2022-08-13 02:44:38 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 1.9929
2022-08-13 02:45:12 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.7508
2022-08-13 02:45:45 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.6859
2022-08-13 02:46:19 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.1101
2022-08-13 02:46:52 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.0047
2022-08-13 02:46:53 - train: epoch 011, train_loss: 2.0530
2022-08-13 02:48:09 - eval: epoch: 011, acc1: 58.332%, acc5: 82.350%, test_loss: 1.7559, per_image_load_time: 0.668ms, per_image_inference_time: 0.582ms
2022-08-13 02:48:09 - until epoch: 011, best_acc1: 58.332%
2022-08-13 02:48:09 - epoch 012 lr: 0.059368
2022-08-13 02:48:49 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.8228
2022-08-13 02:49:22 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.7601
2022-08-13 02:49:55 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 1.8865
2022-08-13 02:50:28 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 1.8937
2022-08-13 02:51:01 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.2030
2022-08-13 02:51:34 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.8406
2022-08-13 02:52:08 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.9745
2022-08-13 02:52:41 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.0530
2022-08-13 02:53:14 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 1.9630
2022-08-13 02:53:48 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 2.0016
2022-08-13 02:54:21 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.2306
2022-08-13 02:54:54 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.8703
2022-08-13 02:55:28 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.8372
2022-08-13 02:56:01 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.1541
2022-08-13 02:56:34 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 1.8305
2022-08-13 02:57:08 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 1.9718
2022-08-13 02:57:41 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.8915
2022-08-13 02:58:14 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.0070
2022-08-13 02:58:47 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.0966
2022-08-13 02:59:21 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.0713
2022-08-13 02:59:54 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 1.9570
2022-08-13 03:00:27 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 1.9763
2022-08-13 03:01:01 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 2.0290
2022-08-13 03:01:34 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.0444
2022-08-13 03:02:07 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.8043
2022-08-13 03:02:40 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.7180
2022-08-13 03:03:14 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 1.9030
2022-08-13 03:03:47 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.8297
2022-08-13 03:04:20 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.1041
2022-08-13 03:04:54 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.9103
2022-08-13 03:05:27 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 1.9046
2022-08-13 03:06:00 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 2.0125
2022-08-13 03:06:33 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.0486
2022-08-13 03:07:07 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 1.9076
2022-08-13 03:07:40 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 1.9816
2022-08-13 03:08:14 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.7697
2022-08-13 03:08:47 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 1.9163
2022-08-13 03:09:21 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.0359
2022-08-13 03:09:54 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.7053
2022-08-13 03:10:28 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1599
2022-08-13 03:11:01 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 1.9244
2022-08-13 03:11:35 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.8853
2022-08-13 03:12:09 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.2094
2022-08-13 03:12:42 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.7936
2022-08-13 03:13:16 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.9994
2022-08-13 03:13:50 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.0415
2022-08-13 03:14:24 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.9482
2022-08-13 03:14:57 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 1.9724
2022-08-13 03:15:30 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.8604
2022-08-13 03:16:03 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.6646
2022-08-13 03:16:05 - train: epoch 012, train_loss: 1.9914
2022-08-13 03:17:20 - eval: epoch: 012, acc1: 59.506%, acc5: 83.290%, test_loss: 1.6972, per_image_load_time: 0.796ms, per_image_inference_time: 0.598ms
2022-08-13 03:17:21 - until epoch: 012, best_acc1: 59.506%
2022-08-13 03:17:21 - epoch 013 lr: 0.053138
2022-08-13 03:18:01 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.8804
2022-08-13 03:18:34 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.9594
2022-08-13 03:19:08 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.7079
2022-08-13 03:19:40 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.9725
2022-08-13 03:20:13 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 1.8914
2022-08-13 03:20:45 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 1.8222
2022-08-13 03:21:19 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.8770
2022-08-13 03:21:52 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.0312
2022-08-13 03:22:25 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.8419
2022-08-13 03:22:58 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.9065
2022-08-13 03:23:32 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 1.8529
2022-08-13 03:24:05 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 1.9510
2022-08-13 03:24:39 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.8435
2022-08-13 03:25:12 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.0246
2022-08-13 03:25:46 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.0063
2022-08-13 03:26:19 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.8962
2022-08-13 03:26:52 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 1.7968
2022-08-13 03:27:26 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9785
2022-08-13 03:27:59 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.2141
2022-08-13 03:28:32 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 1.9630
2022-08-13 03:29:06 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.0659
2022-08-13 03:29:39 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.7332
2022-08-13 03:30:13 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 1.8842
2022-08-13 03:30:46 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.0184
2022-08-13 03:31:19 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 2.0554
2022-08-13 03:31:53 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.8521
2022-08-13 03:32:26 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.8672
2022-08-13 03:33:00 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 1.8783
2022-08-13 03:33:33 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 1.9542
2022-08-13 03:34:06 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.6796
2022-08-13 03:34:39 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.7834
2022-08-13 03:35:13 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 2.0079
2022-08-13 03:35:46 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.8189
2022-08-13 03:36:20 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.8850
2022-08-13 03:36:53 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.7199
2022-08-13 03:37:26 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.1107
2022-08-13 03:38:00 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.6084
2022-08-13 03:38:34 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1828
2022-08-13 03:39:07 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 1.9525
2022-08-13 03:39:41 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.8492
2022-08-13 03:40:14 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.8105
2022-08-13 03:40:48 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 1.9539
2022-08-13 03:41:22 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.8945
2022-08-13 03:41:55 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.7729
2022-08-13 03:42:29 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.6700
2022-08-13 03:43:03 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 1.7865
2022-08-13 03:43:36 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.7473
2022-08-13 03:44:10 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.1678
2022-08-13 03:44:44 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 1.9124
2022-08-13 03:45:17 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 1.9517
2022-08-13 03:45:19 - train: epoch 013, train_loss: 1.9287
2022-08-13 03:46:35 - eval: epoch: 013, acc1: 60.836%, acc5: 84.316%, test_loss: 1.6341, per_image_load_time: 0.622ms, per_image_inference_time: 0.583ms
2022-08-13 03:46:35 - until epoch: 013, best_acc1: 60.836%
2022-08-13 03:46:35 - epoch 014 lr: 0.046859
2022-08-13 03:47:16 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.9230
2022-08-13 03:47:49 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.8648
2022-08-13 03:48:23 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.7207
2022-08-13 03:48:56 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.7651
2022-08-13 03:49:29 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.6787
2022-08-13 03:50:02 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.9343
2022-08-13 03:50:35 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.7447
2022-08-13 03:51:08 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.8290
2022-08-13 03:51:42 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.8943
2022-08-13 03:52:15 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 1.9000
2022-08-13 03:52:48 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.8567
2022-08-13 03:53:22 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 1.9486
2022-08-13 03:53:55 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 1.9580
2022-08-13 03:54:29 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.8779
2022-08-13 03:55:02 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.0223
2022-08-13 03:55:36 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.7259
2022-08-13 03:56:09 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9755
2022-08-13 03:56:43 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 1.8448
2022-08-13 03:57:16 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.8284
2022-08-13 03:57:50 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.7531
2022-08-13 03:58:23 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 1.7427
2022-08-13 03:58:56 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.8594
2022-08-13 03:59:30 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.8949
2022-08-13 04:00:04 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.8601
2022-08-13 04:00:37 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.8265
2022-08-13 04:01:11 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.7718
2022-08-13 04:01:44 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.8312
2022-08-13 04:02:17 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 2.0323
2022-08-13 04:02:50 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9029
2022-08-13 04:03:23 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.8291
2022-08-13 04:03:56 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.8301
2022-08-13 04:04:30 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.8605
2022-08-13 04:05:03 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.8881
2022-08-13 04:05:37 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.6630
2022-08-13 04:06:10 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.8100
2022-08-13 04:06:43 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.7213
2022-08-13 04:07:17 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.7340
2022-08-13 04:07:50 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 1.9050
2022-08-13 04:08:24 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.9895
2022-08-13 04:08:57 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 2.0516
2022-08-13 04:09:31 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.7717
2022-08-13 04:10:04 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.8254
2022-08-13 04:10:38 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.6397
2022-08-13 04:11:12 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.8375
2022-08-13 04:11:45 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.9040
2022-08-13 04:12:18 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.7992
2022-08-13 04:12:52 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.9633
2022-08-13 04:13:25 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.6916
2022-08-13 04:13:59 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8442
2022-08-13 04:14:31 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.7767
2022-08-13 04:14:33 - train: epoch 014, train_loss: 1.8655
2022-08-13 04:15:48 - eval: epoch: 014, acc1: 62.020%, acc5: 84.816%, test_loss: 1.5820, per_image_load_time: 1.415ms, per_image_inference_time: 0.637ms
2022-08-13 04:15:48 - until epoch: 014, best_acc1: 62.020%
2022-08-13 04:15:48 - epoch 015 lr: 0.040630
2022-08-13 04:16:29 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.6009
2022-08-13 04:17:02 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.7170
2022-08-13 04:17:35 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.0314
2022-08-13 04:18:08 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 2.0380
2022-08-13 04:18:41 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.6611
2022-08-13 04:19:14 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 1.8659
2022-08-13 04:19:47 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.8959
2022-08-13 04:20:21 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.7088
2022-08-13 04:20:54 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.8791
2022-08-13 04:21:27 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.7913
2022-08-13 04:22:00 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7584
2022-08-13 04:22:33 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.9772
2022-08-13 04:23:07 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.8433
2022-08-13 04:23:40 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.8083
2022-08-13 04:24:14 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.7320
2022-08-13 04:24:48 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.7554
2022-08-13 04:25:21 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.7144
2022-08-13 04:25:54 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.8008
2022-08-13 04:26:28 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.6888
2022-08-13 04:27:01 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.7799
2022-08-13 04:27:35 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.6866
2022-08-13 04:28:08 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.8417
2022-08-13 04:28:41 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.5651
2022-08-13 04:29:14 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.8276
2022-08-13 04:29:48 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.8568
2022-08-13 04:30:22 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.6722
2022-08-13 04:30:55 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.7814
2022-08-13 04:31:29 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.7935
2022-08-13 04:32:02 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.8889
2022-08-13 04:32:36 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.7132
2022-08-13 04:33:09 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 2.0129
2022-08-13 04:33:42 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.6173
2022-08-13 04:34:16 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.4877
2022-08-13 04:34:49 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.5577
2022-08-13 04:35:22 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 1.9829
2022-08-13 04:35:56 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.9008
2022-08-13 04:36:29 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6693
2022-08-13 04:37:02 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.7397
2022-08-13 04:37:35 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.7883
2022-08-13 04:38:08 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.9070
2022-08-13 04:38:41 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.6167
2022-08-13 04:39:14 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.7032
2022-08-13 04:39:48 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.9117
2022-08-13 04:40:21 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.7098
2022-08-13 04:40:54 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.6405
2022-08-13 04:41:27 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.7663
2022-08-13 04:42:00 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.7186
2022-08-13 04:42:33 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.9362
2022-08-13 04:43:06 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.7078
2022-08-13 04:43:38 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 2.1076
2022-08-13 04:43:40 - train: epoch 015, train_loss: 1.8050
2022-08-13 04:44:56 - eval: epoch: 015, acc1: 63.176%, acc5: 85.532%, test_loss: 1.5244, per_image_load_time: 1.208ms, per_image_inference_time: 0.656ms
2022-08-13 04:44:56 - until epoch: 015, best_acc1: 63.176%
2022-08-13 04:44:56 - epoch 016 lr: 0.034548
2022-08-13 04:45:36 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.8528
2022-08-13 04:46:09 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.5600
2022-08-13 04:46:42 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.7254
2022-08-13 04:47:15 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 1.8560
2022-08-13 04:47:48 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.6080
2022-08-13 04:48:21 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.7884
2022-08-13 04:48:54 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.4829
2022-08-13 04:49:27 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.8896
2022-08-13 04:50:00 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.7867
2022-08-13 04:50:33 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.5668
2022-08-13 04:51:06 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.6665
2022-08-13 04:51:39 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.8117
2022-08-13 04:52:12 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.8294
2022-08-13 04:52:45 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.6852
2022-08-13 04:53:18 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.8620
2022-08-13 04:53:51 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.6501
2022-08-13 04:54:23 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.9294
2022-08-13 04:54:56 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.7540
2022-08-13 04:55:29 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.7713
2022-08-13 04:56:02 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.4763
2022-08-13 04:56:36 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8577
2022-08-13 04:57:09 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.9031
2022-08-13 04:57:42 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.7906
2022-08-13 04:58:15 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.7494
2022-08-13 04:58:48 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.5581
2022-08-13 04:59:21 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.7394
2022-08-13 04:59:53 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.6704
2022-08-13 05:00:27 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.7344
2022-08-13 05:01:00 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.6584
2022-08-13 05:01:33 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.7200
2022-08-13 05:02:06 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.7771
2022-08-13 05:02:39 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.8299
2022-08-13 05:03:12 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.8157
2022-08-13 05:03:46 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.6226
2022-08-13 05:04:19 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.7481
2022-08-13 05:04:52 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.5335
2022-08-13 05:05:25 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.5428
2022-08-13 05:05:59 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 1.9703
2022-08-13 05:06:32 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.8132
2022-08-13 05:07:05 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.8146
2022-08-13 05:07:38 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.8717
2022-08-13 05:08:11 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.7884
2022-08-13 05:08:45 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.6894
2022-08-13 05:09:18 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.5627
2022-08-13 05:09:51 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.8126
2022-08-13 05:10:25 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.7540
2022-08-13 05:10:58 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.7915
2022-08-13 05:11:31 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.7122
2022-08-13 05:12:05 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.8242
2022-08-13 05:12:37 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.6960
2022-08-13 05:12:38 - train: epoch 016, train_loss: 1.7366
2022-08-13 05:13:55 - eval: epoch: 016, acc1: 64.212%, acc5: 86.272%, test_loss: 1.4698, per_image_load_time: 1.667ms, per_image_inference_time: 0.636ms
2022-08-13 05:13:56 - until epoch: 016, best_acc1: 64.212%
2022-08-13 05:13:56 - epoch 017 lr: 0.028710
2022-08-13 05:14:35 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.8443
2022-08-13 05:15:08 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.7447
2022-08-13 05:15:41 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.7883
2022-08-13 05:16:14 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.3953
2022-08-13 05:16:47 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.7209
2022-08-13 05:17:19 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 1.9705
2022-08-13 05:17:52 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.5873
2022-08-13 05:18:25 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.7288
2022-08-13 05:18:57 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.5151
2022-08-13 05:19:30 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.6617
2022-08-13 05:20:03 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.9163
2022-08-13 05:20:36 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.6554
2022-08-13 05:21:09 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.7977
2022-08-13 05:21:42 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.6972
2022-08-13 05:22:15 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.3703
2022-08-13 05:22:48 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.4973
2022-08-13 05:23:21 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.5887
2022-08-13 05:23:54 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.6128
2022-08-13 05:24:27 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.6231
2022-08-13 05:25:00 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.6879
2022-08-13 05:25:33 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.5743
2022-08-13 05:26:06 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.6919
2022-08-13 05:26:39 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.6180
2022-08-13 05:27:12 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.7421
2022-08-13 05:27:46 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.6848
2022-08-13 05:28:19 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.5820
2022-08-13 05:28:52 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6203
2022-08-13 05:29:25 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.7352
2022-08-13 05:29:58 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.7220
2022-08-13 05:30:31 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.5820
2022-08-13 05:31:04 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.7870
2022-08-13 05:31:37 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.5748
2022-08-13 05:32:10 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.6290
2022-08-13 05:32:43 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.4956
2022-08-13 05:33:16 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.5003
2022-08-13 05:33:49 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.6627
2022-08-13 05:34:22 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.4308
2022-08-13 05:34:55 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.8368
2022-08-13 05:35:28 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.5467
2022-08-13 05:36:01 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.5990
2022-08-13 05:36:34 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.5692
2022-08-13 05:37:07 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.6968
2022-08-13 05:37:41 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.6953
2022-08-13 05:38:14 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.9631
2022-08-13 05:38:47 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.7534
2022-08-13 05:39:20 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.4335
2022-08-13 05:39:53 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.8688
2022-08-13 05:40:26 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.9043
2022-08-13 05:40:59 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.6057
2022-08-13 05:41:32 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.6576
2022-08-13 05:41:33 - train: epoch 017, train_loss: 1.6671
2022-08-13 05:42:49 - eval: epoch: 017, acc1: 66.008%, acc5: 87.264%, test_loss: 1.3948, per_image_load_time: 2.191ms, per_image_inference_time: 0.615ms
2022-08-13 05:42:50 - until epoch: 017, best_acc1: 66.008%
2022-08-13 05:42:50 - epoch 018 lr: 0.023208
2022-08-13 05:43:30 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.6091
2022-08-13 05:44:02 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.7477
2022-08-13 05:44:36 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.7581
2022-08-13 05:45:09 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.7227
2022-08-13 05:45:42 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.6377
2022-08-13 05:46:15 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.5776
2022-08-13 05:46:48 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.4778
2022-08-13 05:47:20 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.5622
2022-08-13 05:47:53 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.5883
2022-08-13 05:48:26 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.4603
2022-08-13 05:48:59 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.6453
2022-08-13 05:49:32 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.6112
2022-08-13 05:50:05 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.6964
2022-08-13 05:50:37 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.6966
2022-08-13 05:51:10 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.7453
2022-08-13 05:51:43 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.5247
2022-08-13 05:52:16 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.6735
2022-08-13 05:52:50 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.4665
2022-08-13 05:53:23 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.5139
2022-08-13 05:53:56 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 2.0135
2022-08-13 05:54:29 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.8760
2022-08-13 05:55:02 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.7498
2022-08-13 05:55:35 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.5504
2022-08-13 05:56:08 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.5206
2022-08-13 05:56:41 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.3383
2022-08-13 05:57:14 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.4467
2022-08-13 05:57:47 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.6724
2022-08-13 05:58:20 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5715
2022-08-13 05:58:53 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.4111
2022-08-13 05:59:27 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.6649
2022-08-13 06:00:00 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.8599
2022-08-13 06:00:33 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.4733
2022-08-13 06:01:06 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.4841
2022-08-13 06:01:39 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.6296
2022-08-13 06:02:12 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.4970
2022-08-13 06:02:45 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.5550
2022-08-13 06:03:18 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.9580
2022-08-13 06:03:52 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.5935
2022-08-13 06:04:25 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.6138
2022-08-13 06:04:58 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.6532
2022-08-13 06:05:31 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.8726
2022-08-13 06:06:04 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.5929
2022-08-13 06:06:37 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.3908
2022-08-13 06:07:10 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.4763
2022-08-13 06:07:43 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.5490
2022-08-13 06:08:17 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.5726
2022-08-13 06:08:50 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.8828
2022-08-13 06:09:24 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6006
2022-08-13 06:09:57 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.5136
2022-08-13 06:10:30 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.6696
2022-08-13 06:10:31 - train: epoch 018, train_loss: 1.5927
2022-08-13 06:11:47 - eval: epoch: 018, acc1: 67.050%, acc5: 88.038%, test_loss: 1.3497, per_image_load_time: 1.099ms, per_image_inference_time: 0.632ms
2022-08-13 06:11:47 - until epoch: 018, best_acc1: 67.050%
2022-08-13 06:11:47 - epoch 019 lr: 0.018128
2022-08-13 06:12:27 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.3659
2022-08-13 06:13:00 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.5453
2022-08-13 06:13:33 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.5349
2022-08-13 06:14:06 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.4430
2022-08-13 06:14:39 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.4042
2022-08-13 06:15:12 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.7250
2022-08-13 06:15:46 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.4645
2022-08-13 06:16:18 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.6772
2022-08-13 06:16:51 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.5099
2022-08-13 06:17:24 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.4670
2022-08-13 06:17:57 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.4853
2022-08-13 06:18:30 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.4589
2022-08-13 06:19:03 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.6722
2022-08-13 06:19:36 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.4085
2022-08-13 06:20:09 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.7196
2022-08-13 06:20:42 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.3592
2022-08-13 06:21:15 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.5667
2022-08-13 06:21:49 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.5262
2022-08-13 06:22:21 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.6563
2022-08-13 06:22:54 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.4833
2022-08-13 06:23:27 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.3153
2022-08-13 06:24:01 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.5153
2022-08-13 06:24:34 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.6443
2022-08-13 06:25:07 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.7079
2022-08-13 06:25:40 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.4466
2022-08-13 06:26:13 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.5088
2022-08-13 06:26:46 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.3286
2022-08-13 06:27:19 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.5461
2022-08-13 06:27:52 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.6560
2022-08-13 06:28:25 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.6930
2022-08-13 06:28:58 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.6732
2022-08-13 06:29:31 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.2301
2022-08-13 06:30:04 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.6289
2022-08-13 06:30:38 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.4643
2022-08-13 06:31:11 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.8684
2022-08-13 06:31:44 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.2835
2022-08-13 06:32:17 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.4908
2022-08-13 06:32:51 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.7584
2022-08-13 06:33:24 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.4520
2022-08-13 06:33:57 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.4437
2022-08-13 06:34:31 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.6446
2022-08-13 06:35:03 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.3910
2022-08-13 06:35:37 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.5553
2022-08-13 06:36:09 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.4856
2022-08-13 06:36:42 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.7888
2022-08-13 06:37:15 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.4423
2022-08-13 06:37:48 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.4332
2022-08-13 06:38:21 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.5836
2022-08-13 06:38:54 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.4833
2022-08-13 06:39:27 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.5641
2022-08-13 06:39:28 - train: epoch 019, train_loss: 1.5207
2022-08-13 06:40:44 - eval: epoch: 019, acc1: 68.700%, acc5: 88.972%, test_loss: 1.2822, per_image_load_time: 1.001ms, per_image_inference_time: 0.634ms
2022-08-13 06:40:45 - until epoch: 019, best_acc1: 68.700%
2022-08-13 06:40:45 - epoch 020 lr: 0.013551
2022-08-13 06:41:24 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.4108
2022-08-13 06:41:57 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.4886
2022-08-13 06:42:31 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.4845
2022-08-13 06:43:04 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.4726
2022-08-13 06:43:37 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.3910
2022-08-13 06:44:10 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.3533
2022-08-13 06:44:44 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.1154
2022-08-13 06:45:17 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.4716
2022-08-13 06:45:50 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.5094
2022-08-13 06:46:24 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.4335
2022-08-13 06:46:57 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.4953
2022-08-13 06:47:30 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.2336
2022-08-13 06:48:03 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.4055
2022-08-13 06:48:37 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.4520
2022-08-13 06:49:10 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.4532
2022-08-13 06:49:44 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.5263
2022-08-13 06:50:17 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.3516
2022-08-13 06:50:50 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.5046
2022-08-13 06:51:23 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.4912
2022-08-13 06:51:56 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.4485
2022-08-13 06:52:30 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.4843
2022-08-13 06:53:03 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.4065
2022-08-13 06:53:36 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.3801
2022-08-13 06:54:10 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.6909
2022-08-13 06:54:43 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.5011
2022-08-13 06:55:16 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.3192
2022-08-13 06:55:49 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.2626
2022-08-13 06:56:22 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.3961
2022-08-13 06:56:56 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.5825
2022-08-13 06:57:29 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.5233
2022-08-13 06:58:02 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.4686
2022-08-13 06:58:35 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.5402
2022-08-13 06:59:08 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.1797
2022-08-13 06:59:42 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.4279
2022-08-13 07:00:15 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.3466
2022-08-13 07:00:49 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.5072
2022-08-13 07:01:22 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.4983
2022-08-13 07:01:55 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.5028
2022-08-13 07:02:29 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.5478
2022-08-13 07:03:01 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.1650
2022-08-13 07:03:34 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.4198
2022-08-13 07:04:08 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.4855
2022-08-13 07:04:41 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.2713
2022-08-13 07:05:14 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.2949
2022-08-13 07:05:48 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.4828
2022-08-13 07:06:21 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.3642
2022-08-13 07:06:54 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.2929
2022-08-13 07:07:27 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4218
2022-08-13 07:08:01 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.3527
2022-08-13 07:08:33 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.3884
2022-08-13 07:08:35 - train: epoch 020, train_loss: 1.4431
2022-08-13 07:09:51 - eval: epoch: 020, acc1: 69.780%, acc5: 89.596%, test_loss: 1.2313, per_image_load_time: 0.855ms, per_image_inference_time: 0.612ms
2022-08-13 07:09:51 - until epoch: 020, best_acc1: 69.780%
2022-08-13 07:09:51 - epoch 021 lr: 0.009548
2022-08-13 07:10:31 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.3841
2022-08-13 07:11:04 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.4224
2022-08-13 07:11:37 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.1731
2022-08-13 07:12:09 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4486
2022-08-13 07:12:42 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.1781
2022-08-13 07:13:15 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.2991
2022-08-13 07:13:47 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3416
2022-08-13 07:14:20 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.5227
2022-08-13 07:14:53 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.3042
2022-08-13 07:15:26 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.2334
2022-08-13 07:15:59 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.1873
2022-08-13 07:16:32 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.4003
2022-08-13 07:17:05 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.4085
2022-08-13 07:17:38 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.2273
2022-08-13 07:18:11 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.2666
2022-08-13 07:18:44 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3970
2022-08-13 07:19:17 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.3484
2022-08-13 07:19:50 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.3057
2022-08-13 07:20:23 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.5141
2022-08-13 07:20:56 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.4118
2022-08-13 07:21:28 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3953
2022-08-13 07:22:01 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.2335
2022-08-13 07:22:34 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.3124
2022-08-13 07:23:07 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.2850
2022-08-13 07:23:40 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.3034
2022-08-13 07:24:13 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.6486
2022-08-13 07:24:46 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.5784
2022-08-13 07:25:19 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.4147
2022-08-13 07:25:52 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.2010
2022-08-13 07:26:24 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.2616
2022-08-13 07:26:57 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.3691
2022-08-13 07:27:30 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.3236
2022-08-13 07:28:02 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.4907
2022-08-13 07:28:35 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.3589
2022-08-13 07:29:08 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.3355
2022-08-13 07:29:41 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.3687
2022-08-13 07:30:14 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.3092
2022-08-13 07:30:47 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.4666
2022-08-13 07:31:20 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.4435
2022-08-13 07:31:53 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.6050
2022-08-13 07:32:26 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.2684
2022-08-13 07:32:58 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.3291
2022-08-13 07:33:31 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3921
2022-08-13 07:34:04 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.4759
2022-08-13 07:34:37 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.4328
2022-08-13 07:35:10 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.1735
2022-08-13 07:35:42 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.4604
2022-08-13 07:36:15 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.3386
2022-08-13 07:36:49 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.1844
2022-08-13 07:37:21 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.2967
2022-08-13 07:37:23 - train: epoch 021, train_loss: 1.3652
2022-08-13 07:38:39 - eval: epoch: 021, acc1: 70.774%, acc5: 90.112%, test_loss: 1.1794, per_image_load_time: 0.599ms, per_image_inference_time: 0.569ms
2022-08-13 07:38:39 - until epoch: 021, best_acc1: 70.774%
2022-08-13 07:38:39 - epoch 022 lr: 0.006184
2022-08-13 07:39:19 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.0812
2022-08-13 07:39:52 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.4156
2022-08-13 07:40:25 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.1640
2022-08-13 07:40:57 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.4697
2022-08-13 07:41:30 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3585
2022-08-13 07:42:02 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.5301
2022-08-13 07:42:35 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.5230
2022-08-13 07:43:08 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.4728
2022-08-13 07:43:41 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.3266
2022-08-13 07:44:14 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.3057
2022-08-13 07:44:47 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3781
2022-08-13 07:45:20 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.1500
2022-08-13 07:45:52 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.1673
2022-08-13 07:46:25 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.0594
2022-08-13 07:46:58 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.2617
2022-08-13 07:47:30 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.2867
2022-08-13 07:48:03 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.3170
2022-08-13 07:48:36 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.4309
2022-08-13 07:49:09 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.2849
2022-08-13 07:49:42 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4321
2022-08-13 07:50:15 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.1749
2022-08-13 07:50:48 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.1334
2022-08-13 07:51:22 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.4378
2022-08-13 07:51:55 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.3544
2022-08-13 07:52:28 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.2219
2022-08-13 07:53:01 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.3492
2022-08-13 07:53:34 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.2495
2022-08-13 07:54:07 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.3587
2022-08-13 07:54:40 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.2852
2022-08-13 07:55:13 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.2671
2022-08-13 07:55:47 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.4809
2022-08-13 07:56:20 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4780
2022-08-13 07:56:53 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.2915
2022-08-13 07:57:26 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.1649
2022-08-13 07:57:59 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.4593
2022-08-13 07:58:33 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.1452
2022-08-13 07:59:06 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.4402
2022-08-13 07:59:38 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.5333
2022-08-13 08:00:12 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.3344
2022-08-13 08:00:44 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.3519
2022-08-13 08:01:17 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.1810
2022-08-13 08:01:50 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.3131
2022-08-13 08:02:23 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.3393
2022-08-13 08:02:56 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.1749
2022-08-13 08:03:29 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.2878
2022-08-13 08:04:01 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.5485
2022-08-13 08:04:34 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.3830
2022-08-13 08:05:07 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.0919
2022-08-13 08:05:40 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.3170
2022-08-13 08:06:13 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.3082
2022-08-13 08:06:15 - train: epoch 022, train_loss: 1.2956
2022-08-13 08:07:30 - eval: epoch: 022, acc1: 71.890%, acc5: 90.608%, test_loss: 1.1357, per_image_load_time: 1.902ms, per_image_inference_time: 0.632ms
2022-08-13 08:07:31 - until epoch: 022, best_acc1: 71.890%
2022-08-13 08:07:31 - epoch 023 lr: 0.003511
2022-08-13 08:08:10 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.1477
2022-08-13 08:08:43 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.2013
2022-08-13 08:09:16 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.2721
2022-08-13 08:09:48 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.4203
2022-08-13 08:10:21 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.3469
2022-08-13 08:10:54 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.2160
2022-08-13 08:11:27 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.2006
2022-08-13 08:12:00 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.2613
2022-08-13 08:12:32 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4230
2022-08-13 08:13:05 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.2332
2022-08-13 08:13:38 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.1866
2022-08-13 08:14:11 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.1780
2022-08-13 08:14:44 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.3098
2022-08-13 08:15:17 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.3292
2022-08-13 08:15:50 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.0872
2022-08-13 08:16:22 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.1878
2022-08-13 08:16:55 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.1668
2022-08-13 08:17:28 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 0.9848
2022-08-13 08:18:01 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.1846
2022-08-13 08:18:34 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.0627
2022-08-13 08:19:07 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.3662
2022-08-13 08:19:40 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.0805
2022-08-13 08:20:13 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.1034
2022-08-13 08:20:46 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.1277
2022-08-13 08:21:19 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.1532
2022-08-13 08:21:52 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.2967
2022-08-13 08:22:25 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.2662
2022-08-13 08:22:58 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.2419
2022-08-13 08:23:31 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.0930
2022-08-13 08:24:04 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.0919
2022-08-13 08:24:37 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.4319
2022-08-13 08:25:10 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.3482
2022-08-13 08:25:43 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.2495
2022-08-13 08:26:16 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.3060
2022-08-13 08:26:49 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.1587
2022-08-13 08:27:22 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.1189
2022-08-13 08:27:55 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.1737
2022-08-13 08:28:28 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.3017
2022-08-13 08:29:01 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.1812
2022-08-13 08:29:33 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.1467
2022-08-13 08:30:06 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.0607
2022-08-13 08:30:39 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.2117
2022-08-13 08:31:11 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.2648
2022-08-13 08:31:44 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.2339
2022-08-13 08:32:17 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.1472
2022-08-13 08:32:50 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.3545
2022-08-13 08:33:23 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.2480
2022-08-13 08:33:56 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.0715
2022-08-13 08:34:29 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.2053
2022-08-13 08:35:01 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.3451
2022-08-13 08:35:02 - train: epoch 023, train_loss: 1.2394
2022-08-13 08:36:19 - eval: epoch: 023, acc1: 72.582%, acc5: 91.002%, test_loss: 1.1027, per_image_load_time: 1.560ms, per_image_inference_time: 0.630ms
2022-08-13 08:36:19 - until epoch: 023, best_acc1: 72.582%
2022-08-13 08:36:19 - epoch 024 lr: 0.001571
2022-08-13 08:36:59 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.3446
2022-08-13 08:37:32 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.2292
2022-08-13 08:38:05 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.0804
2022-08-13 08:38:38 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.3477
2022-08-13 08:39:10 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.1478
2022-08-13 08:39:43 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.2377
2022-08-13 08:40:16 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.2027
2022-08-13 08:40:49 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.0773
2022-08-13 08:41:21 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.1668
2022-08-13 08:41:54 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.0866
2022-08-13 08:42:27 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.0603
2022-08-13 08:42:59 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.2961
2022-08-13 08:43:32 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4677
2022-08-13 08:44:05 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.1386
2022-08-13 08:44:37 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.1593
2022-08-13 08:45:10 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.0306
2022-08-13 08:45:42 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.0246
2022-08-13 08:46:15 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.4231
2022-08-13 08:46:47 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.0480
2022-08-13 08:47:20 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.1646
2022-08-13 08:47:52 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.2380
2022-08-13 08:48:24 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.2164
2022-08-13 08:48:57 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.1325
2022-08-13 08:49:29 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.2422
2022-08-13 08:50:02 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.1799
2022-08-13 08:50:34 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.2399
2022-08-13 08:51:06 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.3498
2022-08-13 08:51:39 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.2532
2022-08-13 08:52:12 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.3089
2022-08-13 08:52:44 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.1797
2022-08-13 08:53:17 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.1810
2022-08-13 08:53:49 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.2220
2022-08-13 08:54:22 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.0774
2022-08-13 08:54:55 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.2574
2022-08-13 08:55:28 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.1722
2022-08-13 08:56:01 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.2940
2022-08-13 08:56:33 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.2327
2022-08-13 08:57:06 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.2724
2022-08-13 08:57:39 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.1226
2022-08-13 08:58:12 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.2899
2022-08-13 08:58:44 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.1639
2022-08-13 08:59:17 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.0818
2022-08-13 08:59:50 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.2214
2022-08-13 09:00:22 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.1720
2022-08-13 09:00:55 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.1823
2022-08-13 09:01:27 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.0916
2022-08-13 09:02:00 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.0846
2022-08-13 09:02:32 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.0138
2022-08-13 09:03:05 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.1025
2022-08-13 09:03:37 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.1514
2022-08-13 09:03:38 - train: epoch 024, train_loss: 1.2002
2022-08-13 09:04:55 - eval: epoch: 024, acc1: 72.960%, acc5: 91.198%, test_loss: 1.0862, per_image_load_time: 1.408ms, per_image_inference_time: 0.602ms
2022-08-13 09:04:55 - until epoch: 024, best_acc1: 72.960%
2022-08-13 09:04:55 - epoch 025 lr: 0.000394
2022-08-13 09:05:35 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.1612
2022-08-13 09:06:08 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1274
2022-08-13 09:06:41 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.0552
2022-08-13 09:07:14 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.2697
2022-08-13 09:07:47 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.0579
2022-08-13 09:08:20 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3431
2022-08-13 09:08:53 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.2149
2022-08-13 09:09:25 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.1300
2022-08-13 09:09:58 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0859
2022-08-13 09:10:31 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.2939
2022-08-13 09:11:04 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.2733
2022-08-13 09:11:36 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.2155
2022-08-13 09:12:09 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.2368
2022-08-13 09:12:42 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.1857
2022-08-13 09:13:14 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.1462
2022-08-13 09:13:47 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.0989
2022-08-13 09:14:19 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.0905
2022-08-13 09:14:51 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.0563
2022-08-13 09:15:23 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.1994
2022-08-13 09:15:56 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.2234
2022-08-13 09:16:28 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.1139
2022-08-13 09:17:00 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.0049
2022-08-13 09:17:32 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.0805
2022-08-13 09:18:05 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.0900
2022-08-13 09:18:37 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.0205
2022-08-13 09:19:09 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.2698
2022-08-13 09:19:41 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.1938
2022-08-13 09:20:14 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.2195
2022-08-13 09:20:46 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.2547
2022-08-13 09:21:18 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.2971
2022-08-13 09:21:50 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.0891
2022-08-13 09:22:23 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.1689
2022-08-13 09:22:54 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.0542
2022-08-13 09:23:27 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.1511
2022-08-13 09:23:59 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1620
2022-08-13 09:24:31 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.2444
2022-08-13 09:25:04 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.1281
2022-08-13 09:25:36 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3029
2022-08-13 09:26:08 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.2978
2022-08-13 09:26:40 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.1944
2022-08-13 09:27:12 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.5001
2022-08-13 09:27:45 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.2267
2022-08-13 09:28:17 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.0566
2022-08-13 09:28:49 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 0.8724
2022-08-13 09:29:21 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.0969
2022-08-13 09:29:54 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.1096
2022-08-13 09:30:26 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.0639
2022-08-13 09:30:58 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.3036
2022-08-13 09:31:31 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.1423
2022-08-13 09:32:03 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.1542
2022-08-13 09:32:04 - train: epoch 025, train_loss: 1.1806
2022-08-13 09:33:20 - eval: epoch: 025, acc1: 72.978%, acc5: 91.154%, test_loss: 1.0834, per_image_load_time: 1.930ms, per_image_inference_time: 0.588ms
2022-08-13 09:33:20 - until epoch: 025, best_acc1: 72.978%
2022-08-13 09:33:20 - train done. train time: 12.099 hours, best_acc1: 72.978%
