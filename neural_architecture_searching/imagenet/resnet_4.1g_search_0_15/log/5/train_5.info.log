2022-08-06 08:12:53 - net_idx: 5
2022-08-06 08:12:53 - net_config: {'stem_width': 64, 'depth': 16, 'w_0': 32, 'w_a': 18.29005053248016, 'w_m': 2.0003720871829462}
2022-08-06 08:12:53 - num_classes: 1000
2022-08-06 08:12:53 - input_image_size: 224
2022-08-06 08:12:53 - scale: 1.1428571428571428
2022-08-06 08:12:53 - seed: 0
2022-08-06 08:12:53 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-06 08:12:53 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-06 08:12:53 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fadba72ea00>
2022-08-06 08:12:53 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fadb9da1af0>
2022-08-06 08:12:53 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fadb9da1b20>
2022-08-06 08:12:53 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fadb9da1b80>
2022-08-06 08:12:53 - batch_size: 256
2022-08-06 08:12:53 - num_workers: 16
2022-08-06 08:12:53 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-06 08:12:53 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-06 08:12:53 - epochs: 25
2022-08-06 08:12:53 - print_interval: 100
2022-08-06 08:12:53 - accumulation_steps: 1
2022-08-06 08:12:53 - sync_bn: False
2022-08-06 08:12:53 - apex: True
2022-08-06 08:12:53 - use_ema_model: False
2022-08-06 08:12:53 - ema_model_decay: 0.9999
2022-08-06 08:12:53 - log_dir: ./log
2022-08-06 08:12:53 - checkpoint_dir: ./checkpoints
2022-08-06 08:12:53 - gpus_type: NVIDIA RTX A5000
2022-08-06 08:12:53 - gpus_num: 2
2022-08-06 08:12:53 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fad965d05f0>
2022-08-06 08:12:53 - ema_model: None
2022-08-06 08:12:53 - --------------------parameters--------------------
2022-08-06 08:12:53 - name: conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.6.conv1.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.6.conv1.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.6.conv1.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.6.conv2.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.6.conv2.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.6.conv2.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: layer4.6.conv3.layer.0.weight, grad: True
2022-08-06 08:12:53 - name: layer4.6.conv3.layer.1.weight, grad: True
2022-08-06 08:12:53 - name: layer4.6.conv3.layer.1.bias, grad: True
2022-08-06 08:12:53 - name: fc.weight, grad: True
2022-08-06 08:12:53 - name: fc.bias, grad: True
2022-08-06 08:12:53 - --------------------buffers--------------------
2022-08-06 08:12:53 - name: conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.6.conv1.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.6.conv1.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.6.conv1.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.6.conv2.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.6.conv2.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.6.conv2.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - name: layer4.6.conv3.layer.1.running_mean, grad: False
2022-08-06 08:12:53 - name: layer4.6.conv3.layer.1.running_var, grad: False
2022-08-06 08:12:53 - name: layer4.6.conv3.layer.1.num_batches_tracked, grad: False
2022-08-06 08:12:53 - -----------no weight decay layers--------------
2022-08-06 08:12:53 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-06 08:12:53 - -------------weight decay layers---------------
2022-08-06 08:12:53 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.6.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.6.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: layer4.6.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-06 08:12:53 - epoch 001 lr: 0.100000
2022-08-06 08:13:31 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9190
2022-08-06 08:14:04 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9039
2022-08-06 08:14:36 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8799
2022-08-06 08:15:09 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.8743
2022-08-06 08:15:41 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.8324
2022-08-06 08:16:15 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.6961
2022-08-06 08:16:47 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.7698
2022-08-06 08:17:20 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.6844
2022-08-06 08:17:53 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.5232
2022-08-06 08:18:26 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.5134
2022-08-06 08:18:59 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.5884
2022-08-06 08:19:31 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.3689
2022-08-06 08:20:04 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.3159
2022-08-06 08:20:37 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.3533
2022-08-06 08:21:10 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 6.1704
2022-08-06 08:21:42 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.2616
2022-08-06 08:22:16 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.9654
2022-08-06 08:22:48 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.9434
2022-08-06 08:23:21 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.8845
2022-08-06 08:23:54 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.7275
2022-08-06 08:24:27 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.6993
2022-08-06 08:24:59 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.5253
2022-08-06 08:25:32 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.5840
2022-08-06 08:26:05 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.6546
2022-08-06 08:26:37 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.5278
2022-08-06 08:27:10 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.4513
2022-08-06 08:27:43 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.5658
2022-08-06 08:28:16 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.4248
2022-08-06 08:28:49 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.2833
2022-08-06 08:29:21 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.3717
2022-08-06 08:29:54 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.3635
2022-08-06 08:30:27 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.3215
2022-08-06 08:31:00 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 5.0175
2022-08-06 08:31:32 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.9851
2022-08-06 08:32:05 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 5.0168
2022-08-06 08:32:38 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.8514
2022-08-06 08:33:11 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.9408
2022-08-06 08:33:44 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.9097
2022-08-06 08:34:17 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.8093
2022-08-06 08:34:50 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.7356
2022-08-06 08:35:23 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.8651
2022-08-06 08:35:56 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.7185
2022-08-06 08:36:29 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.5810
2022-08-06 08:37:01 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.5941
2022-08-06 08:37:35 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.5532
2022-08-06 08:38:08 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.7666
2022-08-06 08:38:40 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.4182
2022-08-06 08:39:13 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.7109
2022-08-06 08:39:46 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.6865
2022-08-06 08:40:18 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.4142
2022-08-06 08:40:20 - train: epoch 001, train_loss: 5.6049
2022-08-06 08:41:32 - eval: epoch: 001, acc1: 16.774%, acc5: 38.022%, test_loss: 4.2259, per_image_load_time: 2.233ms, per_image_inference_time: 0.575ms
2022-08-06 08:41:32 - until epoch: 001, best_acc1: 16.774%
2022-08-06 08:41:32 - epoch 002 lr: 0.099606
2022-08-06 08:42:11 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.3019
2022-08-06 08:42:43 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.1239
2022-08-06 08:43:16 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.3797
2022-08-06 08:43:48 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.3521
2022-08-06 08:44:20 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.2352
2022-08-06 08:44:52 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.1082
2022-08-06 08:45:25 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.4032
2022-08-06 08:45:57 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 4.0132
2022-08-06 08:46:30 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 4.0190
2022-08-06 08:47:02 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.1074
2022-08-06 08:47:35 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.1560
2022-08-06 08:48:08 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 4.1622
2022-08-06 08:48:41 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.8817
2022-08-06 08:49:13 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.2401
2022-08-06 08:49:46 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 4.0084
2022-08-06 08:50:19 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.7998
2022-08-06 08:50:52 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.9405
2022-08-06 08:51:24 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 4.0054
2022-08-06 08:51:57 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.8538
2022-08-06 08:52:29 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.6999
2022-08-06 08:53:02 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.7966
2022-08-06 08:53:35 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.6596
2022-08-06 08:54:08 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 4.0941
2022-08-06 08:54:41 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.7452
2022-08-06 08:55:13 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.5805
2022-08-06 08:55:46 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.8223
2022-08-06 08:56:19 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.8898
2022-08-06 08:56:51 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.6774
2022-08-06 08:57:24 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.7534
2022-08-06 08:57:57 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.6569
2022-08-06 08:58:30 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.7185
2022-08-06 08:59:03 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.6658
2022-08-06 08:59:36 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.6344
2022-08-06 09:00:08 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.4096
2022-08-06 09:00:41 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.4172
2022-08-06 09:01:14 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.6628
2022-08-06 09:01:47 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.6032
2022-08-06 09:02:20 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.2726
2022-08-06 09:02:53 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.4655
2022-08-06 09:03:26 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.2700
2022-08-06 09:03:59 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.5575
2022-08-06 09:04:32 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.5502
2022-08-06 09:05:04 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.4095
2022-08-06 09:05:37 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.4508
2022-08-06 09:06:10 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.2666
2022-08-06 09:06:43 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.3117
2022-08-06 09:07:16 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.4285
2022-08-06 09:07:49 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.3601
2022-08-06 09:08:22 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.4171
2022-08-06 09:08:54 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.3926
2022-08-06 09:08:55 - train: epoch 002, train_loss: 3.7955
2022-08-06 09:10:07 - eval: epoch: 002, acc1: 29.872%, acc5: 55.510%, test_loss: 3.3640, per_image_load_time: 1.589ms, per_image_inference_time: 0.578ms
2022-08-06 09:10:08 - until epoch: 002, best_acc1: 29.872%
2022-08-06 09:10:08 - epoch 003 lr: 0.098429
2022-08-06 09:10:46 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.1582
2022-08-06 09:11:18 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.5358
2022-08-06 09:11:50 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.3555
2022-08-06 09:12:22 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.1171
2022-08-06 09:12:54 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.3701
2022-08-06 09:13:26 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.1748
2022-08-06 09:13:59 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.2644
2022-08-06 09:14:32 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.4190
2022-08-06 09:15:04 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.1463
2022-08-06 09:15:37 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.1657
2022-08-06 09:16:10 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.1372
2022-08-06 09:16:43 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.1579
2022-08-06 09:17:15 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.2786
2022-08-06 09:17:48 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.1634
2022-08-06 09:18:21 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.3410
2022-08-06 09:18:54 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 2.9885
2022-08-06 09:19:26 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.1316
2022-08-06 09:19:59 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 2.9598
2022-08-06 09:20:32 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.1894
2022-08-06 09:21:05 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.0687
2022-08-06 09:21:37 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.2495
2022-08-06 09:22:10 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.5910
2022-08-06 09:22:43 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.7577
2022-08-06 09:23:15 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.0766
2022-08-06 09:23:48 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.0987
2022-08-06 09:24:21 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.1293
2022-08-06 09:24:53 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.3875
2022-08-06 09:25:26 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.1111
2022-08-06 09:25:59 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.9944
2022-08-06 09:26:32 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.2160
2022-08-06 09:27:05 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.2789
2022-08-06 09:27:38 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.0528
2022-08-06 09:28:12 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0892
2022-08-06 09:28:44 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.1039
2022-08-06 09:29:17 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.8918
2022-08-06 09:29:50 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.9491
2022-08-06 09:30:23 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.2167
2022-08-06 09:30:56 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.1942
2022-08-06 09:31:29 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.1975
2022-08-06 09:32:02 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7879
2022-08-06 09:32:35 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.9334
2022-08-06 09:33:08 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.1007
2022-08-06 09:33:41 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.7644
2022-08-06 09:34:14 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 3.0946
2022-08-06 09:34:46 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.0567
2022-08-06 09:35:19 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.8065
2022-08-06 09:35:52 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 2.9788
2022-08-06 09:36:25 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.0347
2022-08-06 09:36:58 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.1093
2022-08-06 09:37:30 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 3.0096
2022-08-06 09:37:31 - train: epoch 003, train_loss: 3.1235
2022-08-06 09:38:43 - eval: epoch: 003, acc1: 38.966%, acc5: 65.626%, test_loss: 2.7757, per_image_load_time: 1.521ms, per_image_inference_time: 0.585ms
2022-08-06 09:38:43 - until epoch: 003, best_acc1: 38.966%
2022-08-06 09:38:43 - epoch 004 lr: 0.096488
2022-08-06 09:39:22 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.8564
2022-08-06 09:39:54 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.8607
2022-08-06 09:40:26 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.8918
2022-08-06 09:40:58 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.6719
2022-08-06 09:41:30 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.7088
2022-08-06 09:42:02 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.2708
2022-08-06 09:42:34 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.9339
2022-08-06 09:43:07 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.7538
2022-08-06 09:43:40 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.6227
2022-08-06 09:44:12 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.9401
2022-08-06 09:44:45 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.0762
2022-08-06 09:45:18 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.6149
2022-08-06 09:45:50 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.5754
2022-08-06 09:46:22 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.7475
2022-08-06 09:46:55 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.8176
2022-08-06 09:47:28 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.7704
2022-08-06 09:48:00 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.8653
2022-08-06 09:48:33 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.9811
2022-08-06 09:49:06 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.8781
2022-08-06 09:49:39 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.6229
2022-08-06 09:50:12 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.9045
2022-08-06 09:50:45 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.6143
2022-08-06 09:51:17 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.5142
2022-08-06 09:51:50 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.6730
2022-08-06 09:52:23 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.6571
2022-08-06 09:52:56 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.8674
2022-08-06 09:53:29 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.5411
2022-08-06 09:54:02 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8263
2022-08-06 09:54:34 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8557
2022-08-06 09:55:07 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.8728
2022-08-06 09:55:40 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.9311
2022-08-06 09:56:13 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8404
2022-08-06 09:56:46 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8217
2022-08-06 09:57:18 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.8430
2022-08-06 09:57:51 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.7130
2022-08-06 09:58:24 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.6537
2022-08-06 09:58:57 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.6285
2022-08-06 09:59:30 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.4374
2022-08-06 10:00:03 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.7625
2022-08-06 10:00:36 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.5171
2022-08-06 10:01:08 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.5682
2022-08-06 10:01:42 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.7209
2022-08-06 10:02:14 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.6266
2022-08-06 10:02:47 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.5136
2022-08-06 10:03:20 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.3469
2022-08-06 10:03:53 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.6628
2022-08-06 10:04:26 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.6527
2022-08-06 10:04:59 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.5137
2022-08-06 10:05:32 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.7159
2022-08-06 10:06:04 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.6794
2022-08-06 10:06:05 - train: epoch 004, train_loss: 2.7882
2022-08-06 10:07:18 - eval: epoch: 004, acc1: 44.394%, acc5: 70.752%, test_loss: 2.4925, per_image_load_time: 2.228ms, per_image_inference_time: 0.578ms
2022-08-06 10:07:18 - until epoch: 004, best_acc1: 44.394%
2022-08-06 10:07:18 - epoch 005 lr: 0.093815
2022-08-06 10:07:57 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.5565
2022-08-06 10:08:29 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.7887
2022-08-06 10:09:01 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.9097
2022-08-06 10:09:33 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.5319
2022-08-06 10:10:06 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.6061
2022-08-06 10:10:38 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.7112
2022-08-06 10:11:11 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.4638
2022-08-06 10:11:43 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.7817
2022-08-06 10:12:16 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.6881
2022-08-06 10:12:48 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.5811
2022-08-06 10:13:21 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.5726
2022-08-06 10:13:54 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.6289
2022-08-06 10:14:26 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.3626
2022-08-06 10:14:59 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.5974
2022-08-06 10:15:32 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.5458
2022-08-06 10:16:04 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.4184
2022-08-06 10:16:37 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.6139
2022-08-06 10:17:10 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.7050
2022-08-06 10:17:43 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.6186
2022-08-06 10:18:15 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.6796
2022-08-06 10:18:49 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.4004
2022-08-06 10:19:21 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.4650
2022-08-06 10:19:55 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.3654
2022-08-06 10:20:28 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5887
2022-08-06 10:21:01 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.7543
2022-08-06 10:21:33 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.6065
2022-08-06 10:22:06 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.5926
2022-08-06 10:22:39 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.4015
2022-08-06 10:23:12 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.5990
2022-08-06 10:23:45 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.5600
2022-08-06 10:24:18 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.5551
2022-08-06 10:24:51 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.5142
2022-08-06 10:25:24 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.3830
2022-08-06 10:25:57 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.4455
2022-08-06 10:26:30 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.5688
2022-08-06 10:27:03 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.5465
2022-08-06 10:27:35 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.7263
2022-08-06 10:28:08 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.3497
2022-08-06 10:28:41 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.8955
2022-08-06 10:29:14 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.4946
2022-08-06 10:29:47 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.5748
2022-08-06 10:30:20 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.8858
2022-08-06 10:30:53 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.6064
2022-08-06 10:31:26 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.4582
2022-08-06 10:31:59 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.6005
2022-08-06 10:32:33 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.6213
2022-08-06 10:33:05 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.4853
2022-08-06 10:33:38 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.3835
2022-08-06 10:34:11 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.8119
2022-08-06 10:34:43 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.5608
2022-08-06 10:34:44 - train: epoch 005, train_loss: 2.5904
2022-08-06 10:35:57 - eval: epoch: 005, acc1: 47.796%, acc5: 74.340%, test_loss: 2.2672, per_image_load_time: 2.299ms, per_image_inference_time: 0.567ms
2022-08-06 10:35:58 - until epoch: 005, best_acc1: 47.796%
2022-08-06 10:35:58 - epoch 006 lr: 0.090450
2022-08-06 10:36:37 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.4680
2022-08-06 10:37:09 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.5117
2022-08-06 10:37:41 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.4621
2022-08-06 10:38:14 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.5289
2022-08-06 10:38:46 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.4648
2022-08-06 10:39:19 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.4623
2022-08-06 10:39:52 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.5243
2022-08-06 10:40:24 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.5718
2022-08-06 10:40:57 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3614
2022-08-06 10:41:30 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.3901
2022-08-06 10:42:03 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.4714
2022-08-06 10:42:36 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.5217
2022-08-06 10:43:09 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5992
2022-08-06 10:43:41 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.5538
2022-08-06 10:44:14 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.8318
2022-08-06 10:44:47 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3381
2022-08-06 10:45:20 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.5130
2022-08-06 10:45:53 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.5656
2022-08-06 10:46:26 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.4275
2022-08-06 10:46:58 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.7072
2022-08-06 10:47:31 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.4577
2022-08-06 10:48:04 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.2985
2022-08-06 10:48:37 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.5723
2022-08-06 10:49:10 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.3111
2022-08-06 10:49:43 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4315
2022-08-06 10:50:15 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.4441
2022-08-06 10:50:48 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.6583
2022-08-06 10:51:21 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.1155
2022-08-06 10:51:54 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.4412
2022-08-06 10:52:27 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.3794
2022-08-06 10:53:00 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.2335
2022-08-06 10:53:33 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.6399
2022-08-06 10:54:06 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.0607
2022-08-06 10:54:39 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.6126
2022-08-06 10:55:12 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.5037
2022-08-06 10:55:45 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.4638
2022-08-06 10:56:18 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.4513
2022-08-06 10:56:51 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.4040
2022-08-06 10:57:24 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.7218
2022-08-06 10:57:57 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.6109
2022-08-06 10:58:30 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.4179
2022-08-06 10:59:02 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.3609
2022-08-06 10:59:35 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.4693
2022-08-06 11:00:08 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.3045
2022-08-06 11:00:41 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.5489
2022-08-06 11:01:14 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.4971
2022-08-06 11:01:47 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.6937
2022-08-06 11:02:19 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.3460
2022-08-06 11:02:52 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.3781
2022-08-06 11:03:24 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.3141
2022-08-06 11:03:26 - train: epoch 006, train_loss: 2.4550
2022-08-06 11:04:38 - eval: epoch: 006, acc1: 48.660%, acc5: 74.686%, test_loss: 2.2471, per_image_load_time: 1.229ms, per_image_inference_time: 0.586ms
2022-08-06 11:04:39 - until epoch: 006, best_acc1: 48.660%
2022-08-06 11:04:39 - epoch 007 lr: 0.086448
2022-08-06 11:05:17 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.2861
2022-08-06 11:05:49 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.6259
2022-08-06 11:06:21 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.6123
2022-08-06 11:06:53 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.3168
2022-08-06 11:07:25 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.0672
2022-08-06 11:07:58 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.3313
2022-08-06 11:08:30 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.4486
2022-08-06 11:09:03 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.3363
2022-08-06 11:09:36 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.3178
2022-08-06 11:10:08 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.3948
2022-08-06 11:10:41 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.2939
2022-08-06 11:11:14 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.2127
2022-08-06 11:11:46 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.1334
2022-08-06 11:12:19 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.4373
2022-08-06 11:12:52 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.4702
2022-08-06 11:13:24 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.3641
2022-08-06 11:13:57 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.4885
2022-08-06 11:14:30 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.3265
2022-08-06 11:15:03 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.3686
2022-08-06 11:15:36 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.1967
2022-08-06 11:16:09 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.5343
2022-08-06 11:16:42 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.2461
2022-08-06 11:17:15 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.3811
2022-08-06 11:17:47 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.7254
2022-08-06 11:18:20 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.2915
2022-08-06 11:18:53 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.2229
2022-08-06 11:19:25 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.2800
2022-08-06 11:19:58 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.4423
2022-08-06 11:20:31 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.1438
2022-08-06 11:21:03 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.1872
2022-08-06 11:21:36 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.2623
2022-08-06 11:22:09 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.2239
2022-08-06 11:22:42 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.4844
2022-08-06 11:23:15 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.3584
2022-08-06 11:23:48 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.2474
2022-08-06 11:24:21 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.1690
2022-08-06 11:24:54 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.1814
2022-08-06 11:25:27 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.4895
2022-08-06 11:25:59 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.2380
2022-08-06 11:26:32 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.4623
2022-08-06 11:27:04 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.2542
2022-08-06 11:27:38 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2581
2022-08-06 11:28:10 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.3621
2022-08-06 11:28:43 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.2451
2022-08-06 11:29:16 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.3837
2022-08-06 11:29:49 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.4921
2022-08-06 11:30:22 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.3965
2022-08-06 11:30:55 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.4248
2022-08-06 11:31:28 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.4960
2022-08-06 11:32:00 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.2142
2022-08-06 11:32:02 - train: epoch 007, train_loss: 2.3492
2022-08-06 11:33:14 - eval: epoch: 007, acc1: 52.972%, acc5: 78.410%, test_loss: 2.0107, per_image_load_time: 2.191ms, per_image_inference_time: 0.560ms
2022-08-06 11:33:14 - until epoch: 007, best_acc1: 52.972%
2022-08-06 11:33:14 - epoch 008 lr: 0.081870
2022-08-06 11:33:53 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.2549
2022-08-06 11:34:25 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.3095
2022-08-06 11:34:57 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.3794
2022-08-06 11:35:29 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.1132
2022-08-06 11:36:01 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.1098
2022-08-06 11:36:33 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.5643
2022-08-06 11:37:06 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.3967
2022-08-06 11:37:38 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.0096
2022-08-06 11:38:11 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.4563
2022-08-06 11:38:44 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.2261
2022-08-06 11:39:17 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.2740
2022-08-06 11:39:49 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.1233
2022-08-06 11:40:22 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.3201
2022-08-06 11:40:54 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.2659
2022-08-06 11:41:27 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.2345
2022-08-06 11:42:00 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.4454
2022-08-06 11:42:33 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.1888
2022-08-06 11:43:06 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.4307
2022-08-06 11:43:39 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.0555
2022-08-06 11:44:11 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.4077
2022-08-06 11:44:45 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.4124
2022-08-06 11:45:17 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.1853
2022-08-06 11:45:50 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.2389
2022-08-06 11:46:23 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.2118
2022-08-06 11:46:56 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.0981
2022-08-06 11:47:29 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.2578
2022-08-06 11:48:02 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.3998
2022-08-06 11:48:35 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.2453
2022-08-06 11:49:08 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.2793
2022-08-06 11:49:40 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.3552
2022-08-06 11:50:13 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.1951
2022-08-06 11:50:46 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.4100
2022-08-06 11:51:19 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.2876
2022-08-06 11:51:52 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.1873
2022-08-06 11:52:25 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.2920
2022-08-06 11:52:58 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.3135
2022-08-06 11:53:31 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.1986
2022-08-06 11:54:04 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.1383
2022-08-06 11:54:37 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.2583
2022-08-06 11:55:10 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.3840
2022-08-06 11:55:43 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.0183
2022-08-06 11:56:16 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.3005
2022-08-06 11:56:49 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.1615
2022-08-06 11:57:22 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.3663
2022-08-06 11:57:54 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.2247
2022-08-06 11:58:27 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.2554
2022-08-06 11:59:00 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.1199
2022-08-06 11:59:33 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.2434
2022-08-06 12:00:05 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.4394
2022-08-06 12:00:37 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.1806
2022-08-06 12:00:38 - train: epoch 008, train_loss: 2.2689
2022-08-06 12:01:51 - eval: epoch: 008, acc1: 53.122%, acc5: 78.546%, test_loss: 2.0004, per_image_load_time: 2.193ms, per_image_inference_time: 0.591ms
2022-08-06 12:01:51 - until epoch: 008, best_acc1: 53.122%
2022-08-06 12:01:51 - epoch 009 lr: 0.076790
2022-08-06 12:02:30 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 1.9759
2022-08-06 12:03:03 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.1452
2022-08-06 12:03:34 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.0756
2022-08-06 12:04:06 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.5415
2022-08-06 12:04:38 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.1375
2022-08-06 12:05:11 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.2886
2022-08-06 12:05:43 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.0810
2022-08-06 12:06:16 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 1.9123
2022-08-06 12:06:48 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.1898
2022-08-06 12:07:21 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.0188
2022-08-06 12:07:54 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.2646
2022-08-06 12:08:27 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.1663
2022-08-06 12:08:59 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.2925
2022-08-06 12:09:32 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 2.1391
2022-08-06 12:10:05 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.2219
2022-08-06 12:10:38 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2852
2022-08-06 12:11:10 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.3245
2022-08-06 12:11:43 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.1260
2022-08-06 12:12:16 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 1.9984
2022-08-06 12:12:49 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 1.9874
2022-08-06 12:13:22 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.0819
2022-08-06 12:13:54 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.2607
2022-08-06 12:14:27 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 1.9543
2022-08-06 12:15:00 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.2726
2022-08-06 12:15:32 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.2404
2022-08-06 12:16:05 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.2379
2022-08-06 12:16:38 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.2538
2022-08-06 12:17:11 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2659
2022-08-06 12:17:44 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 1.9497
2022-08-06 12:18:16 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.2159
2022-08-06 12:18:49 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.5532
2022-08-06 12:19:22 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.1939
2022-08-06 12:19:55 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.4346
2022-08-06 12:20:28 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.3265
2022-08-06 12:21:01 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.3323
2022-08-06 12:21:33 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.0818
2022-08-06 12:22:06 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.3421
2022-08-06 12:22:39 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.2104
2022-08-06 12:23:12 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 1.8675
2022-08-06 12:23:45 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.4284
2022-08-06 12:24:17 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.2935
2022-08-06 12:24:50 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.0667
2022-08-06 12:25:23 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.1077
2022-08-06 12:25:56 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.3224
2022-08-06 12:26:29 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.2667
2022-08-06 12:27:02 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.2368
2022-08-06 12:27:35 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.4579
2022-08-06 12:28:08 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.2638
2022-08-06 12:28:41 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.2723
2022-08-06 12:29:13 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 1.9664
2022-08-06 12:29:14 - train: epoch 009, train_loss: 2.1978
2022-08-06 12:30:27 - eval: epoch: 009, acc1: 53.564%, acc5: 78.470%, test_loss: 2.0109, per_image_load_time: 2.269ms, per_image_inference_time: 0.567ms
2022-08-06 12:30:27 - until epoch: 009, best_acc1: 53.564%
2022-08-06 12:30:27 - epoch 010 lr: 0.071288
2022-08-06 12:31:05 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.1089
2022-08-06 12:31:37 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.2548
2022-08-06 12:32:09 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.0289
2022-08-06 12:32:41 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.2191
2022-08-06 12:33:13 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.0606
2022-08-06 12:33:46 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.2941
2022-08-06 12:34:18 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.1357
2022-08-06 12:34:51 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.1140
2022-08-06 12:35:24 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.0217
2022-08-06 12:35:56 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.0847
2022-08-06 12:36:29 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.0363
2022-08-06 12:37:02 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.0284
2022-08-06 12:37:34 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.2143
2022-08-06 12:38:07 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.3667
2022-08-06 12:38:39 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.9112
2022-08-06 12:39:12 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.0185
2022-08-06 12:39:45 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.2446
2022-08-06 12:40:17 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 2.0771
2022-08-06 12:40:50 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 1.9783
2022-08-06 12:41:23 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.1763
2022-08-06 12:41:56 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 1.7713
2022-08-06 12:42:29 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.2650
2022-08-06 12:43:02 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.4147
2022-08-06 12:43:34 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.2126
2022-08-06 12:44:08 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.1732
2022-08-06 12:44:41 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.1442
2022-08-06 12:45:14 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.8727
2022-08-06 12:45:47 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.0647
2022-08-06 12:46:20 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.1794
2022-08-06 12:46:53 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1080
2022-08-06 12:47:26 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.2495
2022-08-06 12:47:59 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.1845
2022-08-06 12:48:32 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.3390
2022-08-06 12:49:05 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.1994
2022-08-06 12:49:37 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.3633
2022-08-06 12:50:11 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.2457
2022-08-06 12:50:44 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.1544
2022-08-06 12:51:17 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.2274
2022-08-06 12:51:50 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 1.9619
2022-08-06 12:52:23 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.1236
2022-08-06 12:52:56 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 1.9811
2022-08-06 12:53:28 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.2144
2022-08-06 12:54:01 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 1.9554
2022-08-06 12:54:34 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 1.8804
2022-08-06 12:55:07 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.1196
2022-08-06 12:55:41 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 1.9021
2022-08-06 12:56:13 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1939
2022-08-06 12:56:46 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.0264
2022-08-06 12:57:20 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.2063
2022-08-06 12:57:51 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.0602
2022-08-06 12:57:53 - train: epoch 010, train_loss: 2.1340
2022-08-06 12:59:05 - eval: epoch: 010, acc1: 54.978%, acc5: 79.940%, test_loss: 1.9229, per_image_load_time: 1.847ms, per_image_inference_time: 0.586ms
2022-08-06 12:59:05 - until epoch: 010, best_acc1: 54.978%
2022-08-06 12:59:05 - epoch 011 lr: 0.065450
2022-08-06 12:59:44 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 1.8700
2022-08-06 13:00:16 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.2298
2022-08-06 13:00:48 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 1.9791
2022-08-06 13:01:21 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.0874
2022-08-06 13:01:54 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 1.9576
2022-08-06 13:02:26 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.0683
2022-08-06 13:02:59 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.1426
2022-08-06 13:03:31 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.1299
2022-08-06 13:04:04 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.2440
2022-08-06 13:04:37 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.1140
2022-08-06 13:05:09 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2232
2022-08-06 13:05:42 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2503
2022-08-06 13:06:16 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.1717
2022-08-06 13:06:48 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.0268
2022-08-06 13:07:21 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.0744
2022-08-06 13:07:54 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.0576
2022-08-06 13:08:27 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.1151
2022-08-06 13:09:00 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 1.9956
2022-08-06 13:09:33 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.0001
2022-08-06 13:10:06 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.2032
2022-08-06 13:10:38 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.0102
2022-08-06 13:11:11 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 2.0529
2022-08-06 13:11:44 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.1493
2022-08-06 13:12:17 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.9065
2022-08-06 13:12:50 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.2223
2022-08-06 13:13:23 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.2889
2022-08-06 13:13:56 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 1.9259
2022-08-06 13:14:29 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.8114
2022-08-06 13:15:01 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.1159
2022-08-06 13:15:34 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.1778
2022-08-06 13:16:07 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 1.9884
2022-08-06 13:16:40 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.0455
2022-08-06 13:17:13 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.1657
2022-08-06 13:17:46 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 1.9471
2022-08-06 13:18:18 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.1080
2022-08-06 13:18:51 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.1242
2022-08-06 13:19:24 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.2882
2022-08-06 13:19:57 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 2.1352
2022-08-06 13:20:30 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.1667
2022-08-06 13:21:03 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.0098
2022-08-06 13:21:36 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.7364
2022-08-06 13:22:08 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 1.8969
2022-08-06 13:22:41 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.0062
2022-08-06 13:23:14 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.2147
2022-08-06 13:23:46 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.0445
2022-08-06 13:24:19 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 2.1418
2022-08-06 13:24:52 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.8444
2022-08-06 13:25:25 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.8645
2022-08-06 13:25:58 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.0531
2022-08-06 13:26:30 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.0606
2022-08-06 13:26:31 - train: epoch 011, train_loss: 2.0716
2022-08-06 13:27:44 - eval: epoch: 011, acc1: 56.232%, acc5: 80.842%, test_loss: 1.8516, per_image_load_time: 1.694ms, per_image_inference_time: 0.588ms
2022-08-06 13:27:44 - until epoch: 011, best_acc1: 56.232%
2022-08-06 13:27:44 - epoch 012 lr: 0.059368
2022-08-06 13:28:23 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.8606
2022-08-06 13:28:55 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.8629
2022-08-06 13:29:27 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 1.7928
2022-08-06 13:29:59 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 1.9875
2022-08-06 13:30:31 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.0829
2022-08-06 13:31:04 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.9035
2022-08-06 13:31:36 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.8597
2022-08-06 13:32:09 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.0635
2022-08-06 13:32:42 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.1678
2022-08-06 13:33:15 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9434
2022-08-06 13:33:47 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.1479
2022-08-06 13:34:20 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.7446
2022-08-06 13:34:52 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.8487
2022-08-06 13:35:25 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.2049
2022-08-06 13:35:58 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 2.1227
2022-08-06 13:36:31 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 1.9037
2022-08-06 13:37:04 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.9079
2022-08-06 13:37:37 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.0946
2022-08-06 13:38:09 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1465
2022-08-06 13:38:42 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.1187
2022-08-06 13:39:15 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.1936
2022-08-06 13:39:48 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.1041
2022-08-06 13:40:21 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 1.7727
2022-08-06 13:40:54 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 1.8114
2022-08-06 13:41:27 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.6696
2022-08-06 13:41:59 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 2.1537
2022-08-06 13:42:32 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.1603
2022-08-06 13:43:05 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.8705
2022-08-06 13:43:38 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 1.9717
2022-08-06 13:44:11 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.7680
2022-08-06 13:44:44 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 1.9809
2022-08-06 13:45:17 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 2.0073
2022-08-06 13:45:50 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.2946
2022-08-06 13:46:23 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.0798
2022-08-06 13:46:55 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 1.9762
2022-08-06 13:47:28 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.8631
2022-08-06 13:48:01 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 1.9708
2022-08-06 13:48:34 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.0419
2022-08-06 13:49:08 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.9031
2022-08-06 13:49:41 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1475
2022-08-06 13:50:14 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 1.9723
2022-08-06 13:50:47 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 2.0193
2022-08-06 13:51:20 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.4001
2022-08-06 13:51:53 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.8867
2022-08-06 13:52:26 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.9425
2022-08-06 13:52:59 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 1.9863
2022-08-06 13:53:32 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.9035
2022-08-06 13:54:05 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.1539
2022-08-06 13:54:38 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.7438
2022-08-06 13:55:10 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.8871
2022-08-06 13:55:11 - train: epoch 012, train_loss: 2.0113
2022-08-06 13:56:24 - eval: epoch: 012, acc1: 60.008%, acc5: 83.444%, test_loss: 1.6832, per_image_load_time: 1.969ms, per_image_inference_time: 0.586ms
2022-08-06 13:56:24 - until epoch: 012, best_acc1: 60.008%
2022-08-06 13:56:24 - epoch 013 lr: 0.053138
2022-08-06 13:57:03 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.7386
2022-08-06 13:57:35 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.7668
2022-08-06 13:58:08 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8172
2022-08-06 13:58:40 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.8869
2022-08-06 13:59:12 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 1.9523
2022-08-06 13:59:45 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 1.9096
2022-08-06 14:00:17 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.9082
2022-08-06 14:00:50 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 1.9400
2022-08-06 14:01:23 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 2.0192
2022-08-06 14:01:55 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.9757
2022-08-06 14:02:28 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.0870
2022-08-06 14:03:01 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 1.8970
2022-08-06 14:03:34 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.9391
2022-08-06 14:04:06 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 1.9865
2022-08-06 14:04:39 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.2270
2022-08-06 14:05:12 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.7631
2022-08-06 14:05:45 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 1.9458
2022-08-06 14:06:18 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9590
2022-08-06 14:06:51 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.0378
2022-08-06 14:07:24 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 1.9960
2022-08-06 14:07:57 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.1001
2022-08-06 14:08:29 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.7855
2022-08-06 14:09:02 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 1.9341
2022-08-06 14:09:35 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.1944
2022-08-06 14:10:08 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.9100
2022-08-06 14:10:41 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.8590
2022-08-06 14:11:13 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.7745
2022-08-06 14:11:46 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 1.8883
2022-08-06 14:12:19 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.1006
2022-08-06 14:12:52 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.7554
2022-08-06 14:13:25 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.8006
2022-08-06 14:13:58 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 1.9731
2022-08-06 14:14:31 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.7359
2022-08-06 14:15:04 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 2.0086
2022-08-06 14:15:36 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.8593
2022-08-06 14:16:09 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.0351
2022-08-06 14:16:42 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.6399
2022-08-06 14:17:15 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.0255
2022-08-06 14:17:48 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.1211
2022-08-06 14:18:21 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.8955
2022-08-06 14:18:53 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.8778
2022-08-06 14:19:27 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 1.9283
2022-08-06 14:19:59 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.7978
2022-08-06 14:20:32 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.7208
2022-08-06 14:21:05 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.8840
2022-08-06 14:21:38 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.0177
2022-08-06 14:22:11 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.9128
2022-08-06 14:22:44 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.1836
2022-08-06 14:23:17 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 1.9992
2022-08-06 14:23:49 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.0606
2022-08-06 14:23:51 - train: epoch 013, train_loss: 1.9488
2022-08-06 14:25:03 - eval: epoch: 013, acc1: 59.882%, acc5: 83.598%, test_loss: 1.6824, per_image_load_time: 1.513ms, per_image_inference_time: 0.585ms
2022-08-06 14:25:03 - until epoch: 013, best_acc1: 60.008%
2022-08-06 14:25:03 - epoch 014 lr: 0.046859
2022-08-06 14:25:42 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.9570
2022-08-06 14:26:14 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.9605
2022-08-06 14:26:46 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.8945
2022-08-06 14:27:19 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.9098
2022-08-06 14:27:51 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.9647
2022-08-06 14:28:24 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.7733
2022-08-06 14:28:57 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.8041
2022-08-06 14:29:29 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.7664
2022-08-06 14:30:02 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.7977
2022-08-06 14:30:35 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 1.9747
2022-08-06 14:31:08 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.9753
2022-08-06 14:31:41 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 1.7889
2022-08-06 14:32:13 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 1.8813
2022-08-06 14:32:46 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 2.0768
2022-08-06 14:33:19 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 1.8507
2022-08-06 14:33:52 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.8653
2022-08-06 14:34:25 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 2.0177
2022-08-06 14:34:58 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 1.9618
2022-08-06 14:35:31 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.7206
2022-08-06 14:36:04 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.8626
2022-08-06 14:36:37 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 1.8818
2022-08-06 14:37:10 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.9655
2022-08-06 14:37:43 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.7634
2022-08-06 14:38:15 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.9491
2022-08-06 14:38:48 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.8252
2022-08-06 14:39:21 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.7862
2022-08-06 14:39:54 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.9157
2022-08-06 14:40:27 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 1.8231
2022-08-06 14:41:00 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 2.0074
2022-08-06 14:41:33 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.8019
2022-08-06 14:42:06 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 2.0642
2022-08-06 14:42:39 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.8921
2022-08-06 14:43:12 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.8038
2022-08-06 14:43:45 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.8420
2022-08-06 14:44:17 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 2.0161
2022-08-06 14:44:50 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.7519
2022-08-06 14:45:23 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.7123
2022-08-06 14:45:55 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 1.9761
2022-08-06 14:46:28 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.8687
2022-08-06 14:47:01 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 2.0430
2022-08-06 14:47:34 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.8161
2022-08-06 14:48:07 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 2.0020
2022-08-06 14:48:40 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.6990
2022-08-06 14:49:13 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.8627
2022-08-06 14:49:46 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.9384
2022-08-06 14:50:19 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.9967
2022-08-06 14:50:52 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.7966
2022-08-06 14:51:25 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.8895
2022-08-06 14:51:58 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8414
2022-08-06 14:52:30 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.7969
2022-08-06 14:52:31 - train: epoch 014, train_loss: 1.8883
2022-08-06 14:53:44 - eval: epoch: 014, acc1: 60.824%, acc5: 84.182%, test_loss: 1.6278, per_image_load_time: 1.126ms, per_image_inference_time: 0.592ms
2022-08-06 14:53:44 - until epoch: 014, best_acc1: 60.824%
2022-08-06 14:53:44 - epoch 015 lr: 0.040630
2022-08-06 14:54:23 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.6443
2022-08-06 14:54:56 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 2.0072
2022-08-06 14:55:28 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 1.9956
2022-08-06 14:56:01 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.9070
2022-08-06 14:56:34 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.8562
2022-08-06 14:57:07 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 1.9470
2022-08-06 14:57:39 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.7867
2022-08-06 14:58:12 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.9473
2022-08-06 14:58:45 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.7283
2022-08-06 14:59:18 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.8049
2022-08-06 14:59:51 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7307
2022-08-06 15:00:24 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.9002
2022-08-06 15:00:57 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.8397
2022-08-06 15:01:30 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.8545
2022-08-06 15:02:02 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.7292
2022-08-06 15:02:35 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.8841
2022-08-06 15:03:08 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 2.0224
2022-08-06 15:03:41 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.6385
2022-08-06 15:04:14 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.7407
2022-08-06 15:04:47 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.5577
2022-08-06 15:05:20 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.6133
2022-08-06 15:05:53 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9882
2022-08-06 15:06:26 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.7452
2022-08-06 15:06:59 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.7503
2022-08-06 15:07:32 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.8083
2022-08-06 15:08:05 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.6806
2022-08-06 15:08:38 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.9211
2022-08-06 15:09:11 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.9003
2022-08-06 15:09:44 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.7760
2022-08-06 15:10:17 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.7637
2022-08-06 15:10:50 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.7703
2022-08-06 15:11:22 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.8226
2022-08-06 15:11:55 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.7455
2022-08-06 15:12:28 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.7841
2022-08-06 15:13:01 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0076
2022-08-06 15:13:34 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.8187
2022-08-06 15:14:06 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6193
2022-08-06 15:14:39 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.8510
2022-08-06 15:15:12 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 2.0891
2022-08-06 15:15:45 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.9431
2022-08-06 15:16:18 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.8008
2022-08-06 15:16:50 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6191
2022-08-06 15:17:23 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.7591
2022-08-06 15:17:56 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.7206
2022-08-06 15:18:28 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.8792
2022-08-06 15:19:01 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.9856
2022-08-06 15:19:34 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.5837
2022-08-06 15:20:07 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.8567
2022-08-06 15:20:39 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.7247
2022-08-06 15:21:11 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.9737
2022-08-06 15:21:13 - train: epoch 015, train_loss: 1.8230
2022-08-06 15:22:26 - eval: epoch: 015, acc1: 63.072%, acc5: 85.468%, test_loss: 1.5276, per_image_load_time: 2.260ms, per_image_inference_time: 0.598ms
2022-08-06 15:22:26 - until epoch: 015, best_acc1: 63.072%
2022-08-06 15:22:26 - epoch 016 lr: 0.034548
2022-08-06 15:23:05 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.6119
2022-08-06 15:23:37 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.5990
2022-08-06 15:24:09 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.8053
2022-08-06 15:24:42 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.1062
2022-08-06 15:25:14 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.5879
2022-08-06 15:25:47 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.6828
2022-08-06 15:26:20 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.6448
2022-08-06 15:26:53 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.8344
2022-08-06 15:27:26 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.8153
2022-08-06 15:27:58 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.5405
2022-08-06 15:28:31 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.7125
2022-08-06 15:29:04 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.8573
2022-08-06 15:29:37 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.6873
2022-08-06 15:30:10 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.6822
2022-08-06 15:30:43 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.9944
2022-08-06 15:31:16 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.9246
2022-08-06 15:31:49 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.8589
2022-08-06 15:32:22 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.8656
2022-08-06 15:32:55 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.7381
2022-08-06 15:33:28 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.6019
2022-08-06 15:34:00 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8202
2022-08-06 15:34:33 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.8445
2022-08-06 15:35:06 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.9882
2022-08-06 15:35:39 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.9873
2022-08-06 15:36:12 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.7102
2022-08-06 15:36:44 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.7884
2022-08-06 15:37:17 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.7088
2022-08-06 15:37:50 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.6245
2022-08-06 15:38:22 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.6642
2022-08-06 15:38:55 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.9466
2022-08-06 15:39:28 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.9045
2022-08-06 15:40:01 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 2.0492
2022-08-06 15:40:34 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9238
2022-08-06 15:41:07 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.6398
2022-08-06 15:41:39 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.7582
2022-08-06 15:42:12 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.6936
2022-08-06 15:42:45 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.7240
2022-08-06 15:43:18 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.2175
2022-08-06 15:43:51 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.8685
2022-08-06 15:44:24 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.7952
2022-08-06 15:44:57 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.6835
2022-08-06 15:45:30 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.6705
2022-08-06 15:46:03 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.6557
2022-08-06 15:46:36 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.6362
2022-08-06 15:47:09 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.7304
2022-08-06 15:47:42 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.5944
2022-08-06 15:48:15 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.8197
2022-08-06 15:48:48 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.5783
2022-08-06 15:49:21 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.8462
2022-08-06 15:49:53 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.6869
2022-08-06 15:49:55 - train: epoch 016, train_loss: 1.7566
2022-08-06 15:51:07 - eval: epoch: 016, acc1: 64.710%, acc5: 86.578%, test_loss: 1.4526, per_image_load_time: 1.820ms, per_image_inference_time: 0.595ms
2022-08-06 15:51:08 - until epoch: 016, best_acc1: 64.710%
2022-08-06 15:51:08 - epoch 017 lr: 0.028710
2022-08-06 15:51:45 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.7510
2022-08-06 15:52:17 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.6792
2022-08-06 15:52:50 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.8688
2022-08-06 15:53:22 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.3106
2022-08-06 15:53:55 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.9543
2022-08-06 15:54:27 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 2.0050
2022-08-06 15:55:00 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.5993
2022-08-06 15:55:32 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.6765
2022-08-06 15:56:05 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.6779
2022-08-06 15:56:37 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.6050
2022-08-06 15:57:10 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.9487
2022-08-06 15:57:43 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.7233
2022-08-06 15:58:15 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.7234
2022-08-06 15:58:48 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.6476
2022-08-06 15:59:21 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.3710
2022-08-06 15:59:54 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.6820
2022-08-06 16:00:27 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.5801
2022-08-06 16:00:59 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.7257
2022-08-06 16:01:32 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.6091
2022-08-06 16:02:05 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.6184
2022-08-06 16:02:38 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.4988
2022-08-06 16:03:11 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.4859
2022-08-06 16:03:44 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.8750
2022-08-06 16:04:16 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.6557
2022-08-06 16:04:49 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.7670
2022-08-06 16:05:22 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.6633
2022-08-06 16:05:54 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.5569
2022-08-06 16:06:27 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.9022
2022-08-06 16:07:00 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.9756
2022-08-06 16:07:32 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.4337
2022-08-06 16:08:05 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8594
2022-08-06 16:08:38 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.5087
2022-08-06 16:09:11 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.5493
2022-08-06 16:09:44 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.5425
2022-08-06 16:10:17 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.6540
2022-08-06 16:10:49 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.6525
2022-08-06 16:11:22 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.4948
2022-08-06 16:11:55 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.6707
2022-08-06 16:12:28 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.4055
2022-08-06 16:13:01 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.6616
2022-08-06 16:13:34 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.7971
2022-08-06 16:14:07 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.5556
2022-08-06 16:14:40 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.6152
2022-08-06 16:15:13 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.7486
2022-08-06 16:15:45 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.7459
2022-08-06 16:16:18 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.5363
2022-08-06 16:16:51 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.7833
2022-08-06 16:17:24 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.8102
2022-08-06 16:17:57 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.4968
2022-08-06 16:18:29 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.5585
2022-08-06 16:18:30 - train: epoch 017, train_loss: 1.6863
2022-08-06 16:19:43 - eval: epoch: 017, acc1: 65.724%, acc5: 87.070%, test_loss: 1.4200, per_image_load_time: 1.579ms, per_image_inference_time: 0.593ms
2022-08-06 16:19:43 - until epoch: 017, best_acc1: 65.724%
2022-08-06 16:19:43 - epoch 018 lr: 0.023208
2022-08-06 16:20:22 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.4147
2022-08-06 16:20:55 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.7444
2022-08-06 16:21:28 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.7561
2022-08-06 16:22:00 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.7048
2022-08-06 16:22:33 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.6974
2022-08-06 16:23:06 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.6428
2022-08-06 16:23:39 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.5927
2022-08-06 16:24:11 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.8412
2022-08-06 16:24:44 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.6988
2022-08-06 16:25:17 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.3975
2022-08-06 16:25:49 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.8053
2022-08-06 16:26:22 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.5089
2022-08-06 16:26:55 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.7865
2022-08-06 16:27:27 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.7147
2022-08-06 16:28:00 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.7589
2022-08-06 16:28:33 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.4698
2022-08-06 16:29:06 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7143
2022-08-06 16:29:39 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.5314
2022-08-06 16:30:12 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.5720
2022-08-06 16:30:44 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.7956
2022-08-06 16:31:18 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.9188
2022-08-06 16:31:51 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.6971
2022-08-06 16:32:24 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6772
2022-08-06 16:32:57 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.5084
2022-08-06 16:33:29 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.4152
2022-08-06 16:34:02 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.6284
2022-08-06 16:34:35 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.7151
2022-08-06 16:35:08 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.4467
2022-08-06 16:35:41 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.5989
2022-08-06 16:36:14 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.4440
2022-08-06 16:36:46 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.8233
2022-08-06 16:37:19 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.4611
2022-08-06 16:37:52 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.4381
2022-08-06 16:38:25 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.7065
2022-08-06 16:38:58 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.7642
2022-08-06 16:39:30 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.4770
2022-08-06 16:40:03 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.9145
2022-08-06 16:40:36 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.6801
2022-08-06 16:41:09 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.6527
2022-08-06 16:41:41 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.7411
2022-08-06 16:42:14 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7073
2022-08-06 16:42:47 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.5114
2022-08-06 16:43:20 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.3199
2022-08-06 16:43:53 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.5252
2022-08-06 16:44:26 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.6370
2022-08-06 16:44:59 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.7539
2022-08-06 16:45:31 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.7482
2022-08-06 16:46:05 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6116
2022-08-06 16:46:38 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.5887
2022-08-06 16:47:10 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.7719
2022-08-06 16:47:11 - train: epoch 018, train_loss: 1.6146
2022-08-06 16:48:24 - eval: epoch: 018, acc1: 66.736%, acc5: 88.080%, test_loss: 1.3533, per_image_load_time: 2.233ms, per_image_inference_time: 0.605ms
2022-08-06 16:48:24 - until epoch: 018, best_acc1: 66.736%
2022-08-06 16:48:24 - epoch 019 lr: 0.018128
2022-08-06 16:49:03 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.6433
2022-08-06 16:49:36 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.6064
2022-08-06 16:50:08 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.7244
2022-08-06 16:50:40 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.4355
2022-08-06 16:51:12 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.5646
2022-08-06 16:51:44 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.5810
2022-08-06 16:52:16 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.4526
2022-08-06 16:52:49 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.7004
2022-08-06 16:53:21 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.4759
2022-08-06 16:53:54 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.4261
2022-08-06 16:54:26 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.3667
2022-08-06 16:54:59 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.5554
2022-08-06 16:55:32 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.4462
2022-08-06 16:56:04 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.4813
2022-08-06 16:56:37 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.6933
2022-08-06 16:57:10 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.4318
2022-08-06 16:57:43 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.5839
2022-08-06 16:58:16 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.3669
2022-08-06 16:58:49 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.7759
2022-08-06 16:59:22 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.5231
2022-08-06 16:59:55 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.2551
2022-08-06 17:00:28 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.4723
2022-08-06 17:01:00 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.6224
2022-08-06 17:01:33 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.9203
2022-08-06 17:02:06 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.5018
2022-08-06 17:02:40 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.4191
2022-08-06 17:03:12 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.3881
2022-08-06 17:03:45 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.4660
2022-08-06 17:04:18 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.6180
2022-08-06 17:04:51 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.6120
2022-08-06 17:05:24 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.4232
2022-08-06 17:05:57 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.2484
2022-08-06 17:06:30 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.5447
2022-08-06 17:07:03 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.7263
2022-08-06 17:07:36 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.5396
2022-08-06 17:08:08 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.2405
2022-08-06 17:08:41 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.4167
2022-08-06 17:09:14 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.6832
2022-08-06 17:09:47 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.4472
2022-08-06 17:10:20 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.3759
2022-08-06 17:10:53 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.6054
2022-08-06 17:11:26 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.5402
2022-08-06 17:11:59 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.5205
2022-08-06 17:12:32 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.5794
2022-08-06 17:13:04 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.9205
2022-08-06 17:13:37 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.3695
2022-08-06 17:14:10 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.4637
2022-08-06 17:14:43 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.4340
2022-08-06 17:15:16 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.4608
2022-08-06 17:15:48 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.5810
2022-08-06 17:15:49 - train: epoch 019, train_loss: 1.5395
2022-08-06 17:17:02 - eval: epoch: 019, acc1: 68.174%, acc5: 88.812%, test_loss: 1.2919, per_image_load_time: 2.229ms, per_image_inference_time: 0.580ms
2022-08-06 17:17:02 - until epoch: 019, best_acc1: 68.174%
2022-08-06 17:17:02 - epoch 020 lr: 0.013551
2022-08-06 17:17:41 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.4792
2022-08-06 17:18:13 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.2875
2022-08-06 17:18:46 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.4915
2022-08-06 17:19:18 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.3519
2022-08-06 17:19:51 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.2939
2022-08-06 17:20:23 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.4087
2022-08-06 17:20:57 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.2515
2022-08-06 17:21:29 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.7848
2022-08-06 17:22:02 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.6061
2022-08-06 17:22:34 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.5184
2022-08-06 17:23:07 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.3847
2022-08-06 17:23:40 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.2231
2022-08-06 17:24:13 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.3721
2022-08-06 17:24:45 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.5551
2022-08-06 17:25:18 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.6060
2022-08-06 17:25:51 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.5210
2022-08-06 17:26:24 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.3599
2022-08-06 17:26:57 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.6146
2022-08-06 17:27:30 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.4361
2022-08-06 17:28:02 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.4520
2022-08-06 17:28:35 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.6104
2022-08-06 17:29:07 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.3575
2022-08-06 17:29:40 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.5200
2022-08-06 17:30:13 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.7645
2022-08-06 17:30:45 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.3656
2022-08-06 17:31:18 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.5614
2022-08-06 17:31:51 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.4905
2022-08-06 17:32:23 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.6314
2022-08-06 17:32:56 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.5205
2022-08-06 17:33:29 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.5267
2022-08-06 17:34:02 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.5790
2022-08-06 17:34:35 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.6032
2022-08-06 17:35:07 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3017
2022-08-06 17:35:41 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.3608
2022-08-06 17:36:13 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.3924
2022-08-06 17:36:46 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.5249
2022-08-06 17:37:19 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.4901
2022-08-06 17:37:52 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.4297
2022-08-06 17:38:25 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.7591
2022-08-06 17:38:58 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.3213
2022-08-06 17:39:31 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.4903
2022-08-06 17:40:04 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.4136
2022-08-06 17:40:36 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.5137
2022-08-06 17:41:09 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.5304
2022-08-06 17:41:42 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.4970
2022-08-06 17:42:15 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.5861
2022-08-06 17:42:47 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.3343
2022-08-06 17:43:20 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.3708
2022-08-06 17:43:53 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.3224
2022-08-06 17:44:25 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.3919
2022-08-06 17:44:27 - train: epoch 020, train_loss: 1.4672
2022-08-06 17:45:39 - eval: epoch: 020, acc1: 69.712%, acc5: 89.700%, test_loss: 1.2272, per_image_load_time: 1.431ms, per_image_inference_time: 0.593ms
2022-08-06 17:45:40 - until epoch: 020, best_acc1: 69.712%
2022-08-06 17:45:40 - epoch 021 lr: 0.009548
2022-08-06 17:46:18 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.5269
2022-08-06 17:46:50 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.4118
2022-08-06 17:47:22 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.3560
2022-08-06 17:47:55 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4770
2022-08-06 17:48:27 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.2526
2022-08-06 17:48:59 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.3221
2022-08-06 17:49:32 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.5148
2022-08-06 17:50:04 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.5476
2022-08-06 17:50:37 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.5258
2022-08-06 17:51:09 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3239
2022-08-06 17:51:42 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.2090
2022-08-06 17:52:14 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.2060
2022-08-06 17:52:47 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.1811
2022-08-06 17:53:20 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.2310
2022-08-06 17:53:52 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.1707
2022-08-06 17:54:25 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.4404
2022-08-06 17:54:58 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.4227
2022-08-06 17:55:30 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.3670
2022-08-06 17:56:03 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.5868
2022-08-06 17:56:36 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.5444
2022-08-06 17:57:08 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3103
2022-08-06 17:57:41 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.4773
2022-08-06 17:58:14 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.3981
2022-08-06 17:58:47 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.3832
2022-08-06 17:59:20 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.3737
2022-08-06 17:59:53 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.4154
2022-08-06 18:00:26 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.3818
2022-08-06 18:00:59 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.3908
2022-08-06 18:01:32 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.3143
2022-08-06 18:02:05 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.5450
2022-08-06 18:02:38 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.2385
2022-08-06 18:03:11 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.1673
2022-08-06 18:03:44 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.6662
2022-08-06 18:04:17 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.4965
2022-08-06 18:04:50 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4183
2022-08-06 18:05:23 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.2043
2022-08-06 18:05:56 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.3257
2022-08-06 18:06:29 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.3797
2022-08-06 18:07:02 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.4685
2022-08-06 18:07:35 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.6966
2022-08-06 18:08:08 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.3781
2022-08-06 18:08:41 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.4779
2022-08-06 18:09:14 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.2208
2022-08-06 18:09:46 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.5966
2022-08-06 18:10:19 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.4544
2022-08-06 18:10:52 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.2094
2022-08-06 18:11:25 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.3820
2022-08-06 18:11:58 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.4737
2022-08-06 18:12:31 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.3001
2022-08-06 18:13:03 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.4536
2022-08-06 18:13:04 - train: epoch 021, train_loss: 1.3901
2022-08-06 18:14:17 - eval: epoch: 021, acc1: 70.770%, acc5: 90.096%, test_loss: 1.1849, per_image_load_time: 2.242ms, per_image_inference_time: 0.578ms
2022-08-06 18:14:17 - until epoch: 021, best_acc1: 70.770%
2022-08-06 18:14:17 - epoch 022 lr: 0.006184
2022-08-06 18:14:55 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 0.9844
2022-08-06 18:15:27 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.3433
2022-08-06 18:15:59 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.2522
2022-08-06 18:16:31 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.2718
2022-08-06 18:17:03 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.4618
2022-08-06 18:17:36 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.4848
2022-08-06 18:18:08 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4333
2022-08-06 18:18:40 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.2541
2022-08-06 18:19:13 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.3186
2022-08-06 18:19:45 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.2900
2022-08-06 18:20:18 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.1908
2022-08-06 18:20:51 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.0789
2022-08-06 18:21:24 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.1919
2022-08-06 18:21:57 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.0801
2022-08-06 18:22:30 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.5509
2022-08-06 18:23:03 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.2724
2022-08-06 18:23:36 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.4079
2022-08-06 18:24:08 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.4521
2022-08-06 18:24:41 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.2707
2022-08-06 18:25:14 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.3960
2022-08-06 18:25:47 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.3661
2022-08-06 18:26:19 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.0243
2022-08-06 18:26:52 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.3713
2022-08-06 18:27:25 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.4374
2022-08-06 18:27:58 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.4468
2022-08-06 18:28:31 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.1973
2022-08-06 18:29:04 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.2180
2022-08-06 18:29:36 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.2474
2022-08-06 18:30:09 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.1182
2022-08-06 18:30:42 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.2169
2022-08-06 18:31:15 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.5334
2022-08-06 18:31:48 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.2023
2022-08-06 18:32:21 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.2998
2022-08-06 18:32:54 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.1641
2022-08-06 18:33:27 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.4460
2022-08-06 18:34:00 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.2541
2022-08-06 18:34:33 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.3682
2022-08-06 18:35:06 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.3394
2022-08-06 18:35:39 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.3008
2022-08-06 18:36:12 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.4299
2022-08-06 18:36:45 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.2685
2022-08-06 18:37:18 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.2715
2022-08-06 18:37:51 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4417
2022-08-06 18:38:24 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.2359
2022-08-06 18:38:56 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.3319
2022-08-06 18:39:29 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.4646
2022-08-06 18:40:02 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.3437
2022-08-06 18:40:35 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.2083
2022-08-06 18:41:07 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.2035
2022-08-06 18:41:39 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.1571
2022-08-06 18:41:41 - train: epoch 022, train_loss: 1.3205
2022-08-06 18:42:53 - eval: epoch: 022, acc1: 71.720%, acc5: 90.610%, test_loss: 1.1380, per_image_load_time: 2.226ms, per_image_inference_time: 0.588ms
2022-08-06 18:42:53 - until epoch: 022, best_acc1: 71.720%
2022-08-06 18:42:53 - epoch 023 lr: 0.003511
2022-08-06 18:43:32 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.1850
2022-08-06 18:44:03 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.2633
2022-08-06 18:44:35 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.1594
2022-08-06 18:45:07 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.4989
2022-08-06 18:45:40 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.4653
2022-08-06 18:46:12 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.1618
2022-08-06 18:46:45 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.1971
2022-08-06 18:47:17 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3718
2022-08-06 18:47:50 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.3906
2022-08-06 18:48:22 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.2067
2022-08-06 18:48:55 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.2763
2022-08-06 18:49:28 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.2600
2022-08-06 18:50:01 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.3042
2022-08-06 18:50:33 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.3793
2022-08-06 18:51:06 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.1702
2022-08-06 18:51:39 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.3238
2022-08-06 18:52:11 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.5598
2022-08-06 18:52:44 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.2185
2022-08-06 18:53:17 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.3906
2022-08-06 18:53:50 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.1600
2022-08-06 18:54:23 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.2669
2022-08-06 18:54:56 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.0909
2022-08-06 18:55:29 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.2655
2022-08-06 18:56:02 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.2990
2022-08-06 18:56:34 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.2254
2022-08-06 18:57:07 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.4290
2022-08-06 18:57:40 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.2496
2022-08-06 18:58:13 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.2110
2022-08-06 18:58:46 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.1854
2022-08-06 18:59:19 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.3603
2022-08-06 18:59:52 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.3168
2022-08-06 19:00:25 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.3777
2022-08-06 19:00:58 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.3241
2022-08-06 19:01:31 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.4110
2022-08-06 19:02:04 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.2419
2022-08-06 19:02:38 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.2067
2022-08-06 19:03:11 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.1599
2022-08-06 19:03:44 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.2430
2022-08-06 19:04:16 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.2372
2022-08-06 19:04:50 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.1296
2022-08-06 19:05:22 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.3284
2022-08-06 19:05:56 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.2601
2022-08-06 19:06:28 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.1994
2022-08-06 19:07:02 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.2591
2022-08-06 19:07:34 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.2214
2022-08-06 19:08:07 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.4546
2022-08-06 19:08:40 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.2107
2022-08-06 19:09:12 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.0710
2022-08-06 19:09:45 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.1622
2022-08-06 19:10:17 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.4041
2022-08-06 19:10:18 - train: epoch 023, train_loss: 1.2650
2022-08-06 19:11:31 - eval: epoch: 023, acc1: 72.584%, acc5: 90.946%, test_loss: 1.1045, per_image_load_time: 2.172ms, per_image_inference_time: 0.577ms
2022-08-06 19:11:32 - until epoch: 023, best_acc1: 72.584%
2022-08-06 19:11:32 - epoch 024 lr: 0.001571
2022-08-06 19:12:11 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.3531
2022-08-06 19:12:43 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.1777
2022-08-06 19:13:15 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.1126
2022-08-06 19:13:48 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.2588
2022-08-06 19:14:20 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.3971
2022-08-06 19:14:53 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.2607
2022-08-06 19:15:26 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.1553
2022-08-06 19:15:58 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.0168
2022-08-06 19:16:31 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.0966
2022-08-06 19:17:03 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.0691
2022-08-06 19:17:36 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.0995
2022-08-06 19:18:09 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.3164
2022-08-06 19:18:42 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4026
2022-08-06 19:19:15 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.2406
2022-08-06 19:19:48 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.2601
2022-08-06 19:20:21 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.1365
2022-08-06 19:20:54 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.2798
2022-08-06 19:21:26 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.3906
2022-08-06 19:21:59 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.0178
2022-08-06 19:22:32 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.1518
2022-08-06 19:23:04 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.0358
2022-08-06 19:23:37 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.1131
2022-08-06 19:24:10 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.1714
2022-08-06 19:24:42 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.2318
2022-08-06 19:25:15 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.2040
2022-08-06 19:25:47 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.4410
2022-08-06 19:26:20 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.3919
2022-08-06 19:26:52 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.3294
2022-08-06 19:27:25 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.2979
2022-08-06 19:27:58 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.1850
2022-08-06 19:28:31 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.1491
2022-08-06 19:29:04 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.2490
2022-08-06 19:29:37 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 0.9369
2022-08-06 19:30:10 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.1220
2022-08-06 19:30:43 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.1067
2022-08-06 19:31:16 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.2264
2022-08-06 19:31:49 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.1941
2022-08-06 19:32:22 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.2902
2022-08-06 19:32:55 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.2362
2022-08-06 19:33:28 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.3999
2022-08-06 19:34:01 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.1795
2022-08-06 19:34:34 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.1595
2022-08-06 19:35:07 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.2731
2022-08-06 19:35:40 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.0660
2022-08-06 19:36:13 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.1048
2022-08-06 19:36:45 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.3023
2022-08-06 19:37:18 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.2855
2022-08-06 19:37:51 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 0.9204
2022-08-06 19:38:24 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.2239
2022-08-06 19:38:56 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.0237
2022-08-06 19:38:57 - train: epoch 024, train_loss: 1.2242
2022-08-06 19:40:10 - eval: epoch: 024, acc1: 72.740%, acc5: 91.162%, test_loss: 1.0909, per_image_load_time: 2.242ms, per_image_inference_time: 0.594ms
2022-08-06 19:40:11 - until epoch: 024, best_acc1: 72.740%
2022-08-06 19:40:11 - epoch 025 lr: 0.000394
2022-08-06 19:40:49 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.2756
2022-08-06 19:41:21 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 0.9531
2022-08-06 19:41:53 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.1666
2022-08-06 19:42:26 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.2226
2022-08-06 19:42:58 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.0923
2022-08-06 19:43:30 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.2271
2022-08-06 19:44:03 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.1432
2022-08-06 19:44:36 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.3392
2022-08-06 19:45:09 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 0.9847
2022-08-06 19:45:41 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.2482
2022-08-06 19:46:15 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.2991
2022-08-06 19:46:47 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.0745
2022-08-06 19:47:20 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3004
2022-08-06 19:47:53 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.1468
2022-08-06 19:48:25 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.2278
2022-08-06 19:48:58 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1332
2022-08-06 19:49:31 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.0524
2022-08-06 19:50:04 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.1357
2022-08-06 19:50:37 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.1844
2022-08-06 19:51:10 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.2272
2022-08-06 19:51:43 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.1292
2022-08-06 19:52:16 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.0671
2022-08-06 19:52:49 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.0461
2022-08-06 19:53:22 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.0015
2022-08-06 19:53:55 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.2162
2022-08-06 19:54:28 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.1439
2022-08-06 19:55:01 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.1984
2022-08-06 19:55:34 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.2520
2022-08-06 19:56:07 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.2749
2022-08-06 19:56:39 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.1195
2022-08-06 19:57:12 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.2146
2022-08-06 19:57:45 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.3419
2022-08-06 19:58:18 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.0382
2022-08-06 19:58:51 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.3311
2022-08-06 19:59:24 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 0.9919
2022-08-06 19:59:57 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.1324
2022-08-06 20:00:30 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.1081
2022-08-06 20:01:02 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3760
2022-08-06 20:01:35 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.3087
2022-08-06 20:02:08 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.1250
2022-08-06 20:02:42 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.3158
2022-08-06 20:03:14 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.1430
2022-08-06 20:03:47 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.0719
2022-08-06 20:04:20 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.1316
2022-08-06 20:04:53 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.2030
2022-08-06 20:05:26 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 0.9794
2022-08-06 20:05:59 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.2176
2022-08-06 20:06:32 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.0717
2022-08-06 20:07:05 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.2522
2022-08-06 20:07:37 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.1520
2022-08-06 20:07:38 - train: epoch 025, train_loss: 1.2058
2022-08-06 20:08:51 - eval: epoch: 025, acc1: 72.800%, acc5: 91.150%, test_loss: 1.0878, per_image_load_time: 2.219ms, per_image_inference_time: 0.578ms
2022-08-06 20:08:51 - until epoch: 025, best_acc1: 72.800%
2022-08-06 20:08:51 - train done. train time: 11.931 hours, best_acc1: 72.800%
2022-08-09 22:49:20 - net_idx: 5
2022-08-09 22:49:20 - net_config: {'stem_width': 64, 'depth': 16, 'w_0': 32, 'w_a': 18.29005053248016, 'w_m': 2.0003720871829462}
2022-08-09 22:49:20 - num_classes: 1000
2022-08-09 22:49:20 - input_image_size: 224
2022-08-09 22:49:20 - scale: 1.1428571428571428
2022-08-09 22:49:20 - seed: 0
2022-08-09 22:49:20 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-09 22:49:20 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-09 22:49:20 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce2b0>
2022-08-09 22:49:20 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce6d0>
2022-08-09 22:49:20 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce700>
2022-08-09 22:49:20 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce760>
2022-08-09 22:49:20 - batch_size: 256
2022-08-09 22:49:20 - num_workers: 16
2022-08-09 22:49:20 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-09 22:49:20 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-09 22:49:20 - epochs: 25
2022-08-09 22:49:20 - print_interval: 100
2022-08-09 22:49:20 - accumulation_steps: 1
2022-08-09 22:49:20 - sync_bn: False
2022-08-09 22:49:20 - apex: True
2022-08-09 22:49:20 - use_ema_model: False
2022-08-09 22:49:20 - ema_model_decay: 0.9999
2022-08-09 22:49:20 - log_dir: ./log
2022-08-09 22:49:20 - checkpoint_dir: ./checkpoints
2022-08-09 22:49:20 - gpus_type: NVIDIA RTX A5000
2022-08-09 22:49:20 - gpus_num: 2
2022-08-09 22:49:20 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5dddaeb570>
2022-08-09 22:49:20 - ema_model: None
2022-08-09 22:49:20 - --------------------parameters--------------------
2022-08-09 22:49:20 - name: conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: fc.weight, grad: True
2022-08-09 22:49:20 - name: fc.bias, grad: True
2022-08-09 22:49:20 - --------------------buffers--------------------
2022-08-09 22:49:20 - name: conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - -----------no weight decay layers--------------
2022-08-09 22:49:20 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - -------------weight decay layers---------------
2022-08-09 22:49:20 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:21 - resuming model from ./checkpoints/5/latest.pth. resume_epoch: 025, used_time: 11.931 hours, best_acc1: 72.800%, test_loss: 1.0878, lr: 0.000000
2022-08-09 22:49:21 - train done. train time: 11.931 hours, best_acc1: 72.800%
