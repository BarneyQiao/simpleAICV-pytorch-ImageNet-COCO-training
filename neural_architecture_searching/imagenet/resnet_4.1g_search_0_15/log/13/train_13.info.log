2022-08-13 09:33:20 - net_idx: 13
2022-08-13 09:33:20 - net_config: {'stem_width': 64, 'depth': 16, 'w_0': 32, 'w_a': 16.522017597145847, 'w_m': 2.0086175352187157}
2022-08-13 09:33:20 - num_classes: 1000
2022-08-13 09:33:20 - input_image_size: 224
2022-08-13 09:33:20 - scale: 1.1428571428571428
2022-08-13 09:33:20 - seed: 0
2022-08-13 09:33:20 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-13 09:33:20 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-13 09:33:20 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce2b0>
2022-08-13 09:33:20 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce6d0>
2022-08-13 09:33:20 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce700>
2022-08-13 09:33:20 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce760>
2022-08-13 09:33:20 - batch_size: 256
2022-08-13 09:33:20 - num_workers: 16
2022-08-13 09:33:20 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-13 09:33:20 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-13 09:33:20 - epochs: 25
2022-08-13 09:33:20 - print_interval: 100
2022-08-13 09:33:20 - accumulation_steps: 1
2022-08-13 09:33:20 - sync_bn: False
2022-08-13 09:33:20 - apex: True
2022-08-13 09:33:20 - use_ema_model: False
2022-08-13 09:33:20 - ema_model_decay: 0.9999
2022-08-13 09:33:20 - log_dir: ./log
2022-08-13 09:33:20 - checkpoint_dir: ./checkpoints
2022-08-13 09:33:20 - gpus_type: NVIDIA RTX A5000
2022-08-13 09:33:20 - gpus_num: 2
2022-08-13 09:33:20 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5dddaeb570>
2022-08-13 09:33:20 - ema_model: None
2022-08-13 09:33:20 - --------------------parameters--------------------
2022-08-13 09:33:20 - name: conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-13 09:33:20 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-13 09:33:20 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-13 09:33:20 - name: fc.weight, grad: True
2022-08-13 09:33:20 - name: fc.bias, grad: True
2022-08-13 09:33:20 - --------------------buffers--------------------
2022-08-13 09:33:20 - name: conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-13 09:33:20 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-13 09:33:20 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-13 09:33:20 - -----------no weight decay layers--------------
2022-08-13 09:33:20 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-13 09:33:20 - -------------weight decay layers---------------
2022-08-13 09:33:20 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer3.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-13 09:33:20 - epoch 001 lr: 0.100000
2022-08-13 09:34:00 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9381
2022-08-13 09:34:33 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.8938
2022-08-13 09:35:06 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8792
2022-08-13 09:35:39 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.8436
2022-08-13 09:36:12 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.8272
2022-08-13 09:36:45 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.7174
2022-08-13 09:37:18 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.7077
2022-08-13 09:37:51 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.6414
2022-08-13 09:38:23 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.4956
2022-08-13 09:38:56 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.5271
2022-08-13 09:39:28 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.4823
2022-08-13 09:40:01 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.3594
2022-08-13 09:40:34 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.3710
2022-08-13 09:41:07 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.3045
2022-08-13 09:41:40 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 6.2699
2022-08-13 09:42:12 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.3970
2022-08-13 09:42:45 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.9805
2022-08-13 09:43:18 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 6.0129
2022-08-13 09:43:50 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.8857
2022-08-13 09:44:23 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.8544
2022-08-13 09:44:56 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.8605
2022-08-13 09:45:29 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.6392
2022-08-13 09:46:02 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.6904
2022-08-13 09:46:35 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.7062
2022-08-13 09:47:08 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.5821
2022-08-13 09:47:41 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.7265
2022-08-13 09:48:14 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.4151
2022-08-13 09:48:46 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.4314
2022-08-13 09:49:19 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.3536
2022-08-13 09:49:52 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.2983
2022-08-13 09:50:24 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.3884
2022-08-13 09:50:57 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.2833
2022-08-13 09:51:30 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 5.1559
2022-08-13 09:52:03 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 5.1426
2022-08-13 09:52:35 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.9402
2022-08-13 09:53:08 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 5.0682
2022-08-13 09:53:40 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 5.0948
2022-08-13 09:54:13 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.8625
2022-08-13 09:54:46 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.8010
2022-08-13 09:55:18 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.8497
2022-08-13 09:55:51 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.9487
2022-08-13 09:56:24 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.6730
2022-08-13 09:56:56 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.7988
2022-08-13 09:57:29 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.5775
2022-08-13 09:58:02 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.7643
2022-08-13 09:58:34 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.7310
2022-08-13 09:59:07 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.4663
2022-08-13 09:59:40 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.7919
2022-08-13 10:00:12 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.5661
2022-08-13 10:00:45 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.4348
2022-08-13 10:00:46 - train: epoch 001, train_loss: 5.6441
2022-08-13 10:02:02 - eval: epoch: 001, acc1: 15.414%, acc5: 35.464%, test_loss: 4.3829, per_image_load_time: 1.795ms, per_image_inference_time: 0.601ms
2022-08-13 10:02:02 - until epoch: 001, best_acc1: 15.414%
2022-08-13 10:02:02 - epoch 002 lr: 0.099606
2022-08-13 10:02:42 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.2763
2022-08-13 10:03:14 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.2267
2022-08-13 10:03:47 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.3880
2022-08-13 10:04:20 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.3657
2022-08-13 10:04:52 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.1829
2022-08-13 10:05:25 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.1792
2022-08-13 10:05:58 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.4864
2022-08-13 10:06:31 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 4.0943
2022-08-13 10:07:04 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.8784
2022-08-13 10:07:36 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.4254
2022-08-13 10:08:09 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.3417
2022-08-13 10:08:42 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 4.0008
2022-08-13 10:09:15 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 4.1222
2022-08-13 10:09:48 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.2618
2022-08-13 10:10:21 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 3.9767
2022-08-13 10:10:53 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.8271
2022-08-13 10:11:26 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.9902
2022-08-13 10:11:58 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.9578
2022-08-13 10:12:31 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.7625
2022-08-13 10:13:04 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.5158
2022-08-13 10:13:37 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.7925
2022-08-13 10:14:09 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.6901
2022-08-13 10:14:42 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 4.0708
2022-08-13 10:15:15 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.7808
2022-08-13 10:15:47 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.6490
2022-08-13 10:16:19 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.7712
2022-08-13 10:16:52 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 4.0145
2022-08-13 10:17:25 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.8632
2022-08-13 10:17:57 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.6843
2022-08-13 10:18:30 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.5589
2022-08-13 10:19:03 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.6566
2022-08-13 10:19:36 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.7797
2022-08-13 10:20:09 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.6510
2022-08-13 10:20:41 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.6153
2022-08-13 10:21:14 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.6326
2022-08-13 10:21:47 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.6503
2022-08-13 10:22:20 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.6820
2022-08-13 10:22:53 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.5019
2022-08-13 10:23:26 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.5098
2022-08-13 10:23:58 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.3089
2022-08-13 10:24:31 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.6605
2022-08-13 10:25:04 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.4615
2022-08-13 10:25:36 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.4424
2022-08-13 10:26:09 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.2932
2022-08-13 10:26:41 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.2620
2022-08-13 10:27:14 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.3963
2022-08-13 10:27:47 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.2486
2022-08-13 10:28:19 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.3746
2022-08-13 10:28:52 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.2686
2022-08-13 10:29:24 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.4879
2022-08-13 10:29:25 - train: epoch 002, train_loss: 3.8056
2022-08-13 10:30:41 - eval: epoch: 002, acc1: 28.254%, acc5: 54.072%, test_loss: 3.4227, per_image_load_time: 0.660ms, per_image_inference_time: 0.583ms
2022-08-13 10:30:41 - until epoch: 002, best_acc1: 28.254%
2022-08-13 10:30:41 - epoch 003 lr: 0.098429
2022-08-13 10:31:21 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.4110
2022-08-13 10:31:53 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.3583
2022-08-13 10:32:26 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.3445
2022-08-13 10:32:59 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.3593
2022-08-13 10:33:31 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.4236
2022-08-13 10:34:04 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.1481
2022-08-13 10:34:36 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.2944
2022-08-13 10:35:09 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.3630
2022-08-13 10:35:41 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.2634
2022-08-13 10:36:14 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.4091
2022-08-13 10:36:47 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.1593
2022-08-13 10:37:20 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.2275
2022-08-13 10:37:53 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.0689
2022-08-13 10:38:25 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.1087
2022-08-13 10:38:58 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.1626
2022-08-13 10:39:31 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.0710
2022-08-13 10:40:04 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.1636
2022-08-13 10:40:37 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.0954
2022-08-13 10:41:09 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.2359
2022-08-13 10:41:42 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.3113
2022-08-13 10:42:15 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.0524
2022-08-13 10:42:48 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.4285
2022-08-13 10:43:20 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 3.0737
2022-08-13 10:43:53 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 2.9126
2022-08-13 10:44:26 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.1834
2022-08-13 10:44:59 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.1858
2022-08-13 10:45:32 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.2889
2022-08-13 10:46:05 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.1804
2022-08-13 10:46:38 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.8920
2022-08-13 10:47:11 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.1399
2022-08-13 10:47:44 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.2428
2022-08-13 10:48:17 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.0930
2022-08-13 10:48:50 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0241
2022-08-13 10:49:23 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.2115
2022-08-13 10:49:56 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.9535
2022-08-13 10:50:29 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.9082
2022-08-13 10:51:02 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.0849
2022-08-13 10:51:35 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.1195
2022-08-13 10:52:08 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.2566
2022-08-13 10:52:41 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7138
2022-08-13 10:53:14 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.8575
2022-08-13 10:53:48 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 2.9315
2022-08-13 10:54:21 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.9795
2022-08-13 10:54:54 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.9300
2022-08-13 10:55:28 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 2.7933
2022-08-13 10:56:01 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.9151
2022-08-13 10:56:34 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 3.0192
2022-08-13 10:57:08 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.3237
2022-08-13 10:57:41 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.0339
2022-08-13 10:58:13 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.9940
2022-08-13 10:58:15 - train: epoch 003, train_loss: 3.1170
2022-08-13 10:59:30 - eval: epoch: 003, acc1: 39.044%, acc5: 65.528%, test_loss: 2.7767, per_image_load_time: 1.628ms, per_image_inference_time: 0.594ms
2022-08-13 10:59:30 - until epoch: 003, best_acc1: 39.044%
2022-08-13 10:59:30 - epoch 004 lr: 0.096488
2022-08-13 11:00:09 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.8920
2022-08-13 11:00:42 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.5791
2022-08-13 11:01:15 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.7938
2022-08-13 11:01:48 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.9760
2022-08-13 11:02:21 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.8226
2022-08-13 11:02:54 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.0384
2022-08-13 11:03:27 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 3.0054
2022-08-13 11:04:00 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.8412
2022-08-13 11:04:33 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.6198
2022-08-13 11:05:06 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.8296
2022-08-13 11:05:39 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.0591
2022-08-13 11:06:12 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.7083
2022-08-13 11:06:45 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.6382
2022-08-13 11:07:18 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.7925
2022-08-13 11:07:51 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.7924
2022-08-13 11:08:24 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.8630
2022-08-13 11:08:57 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 3.0375
2022-08-13 11:09:30 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.9872
2022-08-13 11:10:03 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.9806
2022-08-13 11:10:36 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.7784
2022-08-13 11:11:09 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.9099
2022-08-13 11:11:42 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.6835
2022-08-13 11:12:15 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.4944
2022-08-13 11:12:48 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.7860
2022-08-13 11:13:21 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.6408
2022-08-13 11:13:54 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.7210
2022-08-13 11:14:27 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.7519
2022-08-13 11:15:00 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.9793
2022-08-13 11:15:33 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8657
2022-08-13 11:16:06 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.6988
2022-08-13 11:16:39 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.6050
2022-08-13 11:17:12 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8210
2022-08-13 11:17:45 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8229
2022-08-13 11:18:18 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.7140
2022-08-13 11:18:51 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.7449
2022-08-13 11:19:24 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.7652
2022-08-13 11:19:57 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.6485
2022-08-13 11:20:30 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.4803
2022-08-13 11:21:03 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.5790
2022-08-13 11:21:36 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.5559
2022-08-13 11:22:09 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.7421
2022-08-13 11:22:42 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.5042
2022-08-13 11:23:15 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.6730
2022-08-13 11:23:48 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.6315
2022-08-13 11:24:22 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.2102
2022-08-13 11:24:54 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.6517
2022-08-13 11:25:27 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.7276
2022-08-13 11:26:00 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.6512
2022-08-13 11:26:33 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.7412
2022-08-13 11:27:05 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.7238
2022-08-13 11:27:07 - train: epoch 004, train_loss: 2.7794
2022-08-13 11:28:21 - eval: epoch: 004, acc1: 44.150%, acc5: 70.490%, test_loss: 2.6521, per_image_load_time: 1.736ms, per_image_inference_time: 0.601ms
2022-08-13 11:28:21 - until epoch: 004, best_acc1: 44.150%
2022-08-13 11:28:21 - epoch 005 lr: 0.093815
2022-08-13 11:29:01 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.7156
2022-08-13 11:29:34 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.6467
2022-08-13 11:30:07 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 3.0889
2022-08-13 11:30:40 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.7119
2022-08-13 11:31:13 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.4583
2022-08-13 11:31:46 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.8596
2022-08-13 11:32:19 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.7615
2022-08-13 11:32:52 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.8091
2022-08-13 11:33:25 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.5642
2022-08-13 11:33:57 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.6201
2022-08-13 11:34:29 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.6773
2022-08-13 11:35:02 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.6073
2022-08-13 11:35:35 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.5251
2022-08-13 11:36:07 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.6583
2022-08-13 11:36:39 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.6106
2022-08-13 11:37:12 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.5151
2022-08-13 11:37:44 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.5489
2022-08-13 11:38:17 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.5065
2022-08-13 11:38:50 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.6114
2022-08-13 11:39:23 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.4558
2022-08-13 11:39:55 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.3585
2022-08-13 11:40:28 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.4995
2022-08-13 11:41:01 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.4124
2022-08-13 11:41:34 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5630
2022-08-13 11:42:06 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.5754
2022-08-13 11:42:39 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.8209
2022-08-13 11:43:12 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.6613
2022-08-13 11:43:44 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.5533
2022-08-13 11:44:17 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.4745
2022-08-13 11:44:50 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.5562
2022-08-13 11:45:23 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.7403
2022-08-13 11:45:55 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.4511
2022-08-13 11:46:28 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.3646
2022-08-13 11:47:01 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.5174
2022-08-13 11:47:34 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.4766
2022-08-13 11:48:06 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6219
2022-08-13 11:48:39 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.3208
2022-08-13 11:49:12 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.4331
2022-08-13 11:49:45 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.6954
2022-08-13 11:50:18 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.7082
2022-08-13 11:50:51 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.4776
2022-08-13 11:51:23 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.6045
2022-08-13 11:51:56 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.6389
2022-08-13 11:52:29 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.4815
2022-08-13 11:53:02 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.6333
2022-08-13 11:53:35 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.5120
2022-08-13 11:54:08 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.3322
2022-08-13 11:54:41 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.3015
2022-08-13 11:55:14 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.6027
2022-08-13 11:55:46 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.7465
2022-08-13 11:55:48 - train: epoch 005, train_loss: 2.5790
2022-08-13 11:57:03 - eval: epoch: 005, acc1: 47.588%, acc5: 74.146%, test_loss: 2.2810, per_image_load_time: 1.786ms, per_image_inference_time: 0.591ms
2022-08-13 11:57:04 - until epoch: 005, best_acc1: 47.588%
2022-08-13 11:57:04 - epoch 006 lr: 0.090450
2022-08-13 11:57:44 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.2916
2022-08-13 11:58:17 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.7635
2022-08-13 11:58:50 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.3099
2022-08-13 11:59:23 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.5159
2022-08-13 11:59:56 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.4522
2022-08-13 12:00:29 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.4860
2022-08-13 12:01:01 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.3777
2022-08-13 12:01:34 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.5609
2022-08-13 12:02:07 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3767
2022-08-13 12:02:40 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.1946
2022-08-13 12:03:13 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.4793
2022-08-13 12:03:46 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.5074
2022-08-13 12:04:19 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.3939
2022-08-13 12:04:52 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.3277
2022-08-13 12:05:26 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.6466
2022-08-13 12:05:59 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3050
2022-08-13 12:06:32 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.5193
2022-08-13 12:07:05 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.3880
2022-08-13 12:07:39 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.2874
2022-08-13 12:08:12 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.6582
2022-08-13 12:08:45 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.3571
2022-08-13 12:09:18 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.2503
2022-08-13 12:09:51 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.3363
2022-08-13 12:10:24 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.3342
2022-08-13 12:10:56 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4089
2022-08-13 12:11:29 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.2332
2022-08-13 12:12:02 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.4850
2022-08-13 12:12:36 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 1.9809
2022-08-13 12:13:09 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.4243
2022-08-13 12:13:42 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.4584
2022-08-13 12:14:15 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.3052
2022-08-13 12:14:48 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.4561
2022-08-13 12:15:21 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.3752
2022-08-13 12:15:54 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.4489
2022-08-13 12:16:27 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.6145
2022-08-13 12:17:00 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.5102
2022-08-13 12:17:33 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.5671
2022-08-13 12:18:06 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.3024
2022-08-13 12:18:39 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.4086
2022-08-13 12:19:12 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.5582
2022-08-13 12:19:46 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.3770
2022-08-13 12:20:19 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.2036
2022-08-13 12:20:52 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.5124
2022-08-13 12:21:25 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.3917
2022-08-13 12:21:58 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.5714
2022-08-13 12:22:31 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.2930
2022-08-13 12:23:04 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4854
2022-08-13 12:23:37 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.3069
2022-08-13 12:24:10 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.3472
2022-08-13 12:24:43 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.5031
2022-08-13 12:24:44 - train: epoch 006, train_loss: 2.4463
2022-08-13 12:25:59 - eval: epoch: 006, acc1: 50.838%, acc5: 75.790%, test_loss: 2.1662, per_image_load_time: 1.325ms, per_image_inference_time: 0.599ms
2022-08-13 12:25:59 - until epoch: 006, best_acc1: 50.838%
2022-08-13 12:25:59 - epoch 007 lr: 0.086448
2022-08-13 12:26:40 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.3737
2022-08-13 12:27:12 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.7329
2022-08-13 12:27:45 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.3963
2022-08-13 12:28:17 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.4483
2022-08-13 12:28:50 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.2491
2022-08-13 12:29:23 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.3543
2022-08-13 12:29:56 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.4239
2022-08-13 12:30:29 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.3726
2022-08-13 12:31:02 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.5017
2022-08-13 12:31:35 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.3122
2022-08-13 12:32:08 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.3771
2022-08-13 12:32:41 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.2140
2022-08-13 12:33:14 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.0864
2022-08-13 12:33:47 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.3305
2022-08-13 12:34:20 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.4631
2022-08-13 12:34:54 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.2904
2022-08-13 12:35:27 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.4311
2022-08-13 12:36:00 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.3271
2022-08-13 12:36:33 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.2078
2022-08-13 12:37:06 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 1.9788
2022-08-13 12:37:39 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.4142
2022-08-13 12:38:12 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.3192
2022-08-13 12:38:45 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.3098
2022-08-13 12:39:18 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.7959
2022-08-13 12:39:52 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.4409
2022-08-13 12:40:25 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.2046
2022-08-13 12:40:58 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.0542
2022-08-13 12:41:32 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.2754
2022-08-13 12:42:05 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.0898
2022-08-13 12:42:38 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.3709
2022-08-13 12:43:11 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.5674
2022-08-13 12:43:45 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.3812
2022-08-13 12:44:17 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.5682
2022-08-13 12:44:50 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.4041
2022-08-13 12:45:23 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.3859
2022-08-13 12:45:56 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.1281
2022-08-13 12:46:29 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.2112
2022-08-13 12:47:02 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.2979
2022-08-13 12:47:35 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.4709
2022-08-13 12:48:08 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.4733
2022-08-13 12:48:41 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.2136
2022-08-13 12:49:14 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.4251
2022-08-13 12:49:48 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.4192
2022-08-13 12:50:21 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.3081
2022-08-13 12:50:54 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.2960
2022-08-13 12:51:27 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.5206
2022-08-13 12:52:00 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.2276
2022-08-13 12:52:34 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.5130
2022-08-13 12:53:07 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.2759
2022-08-13 12:53:40 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.3606
2022-08-13 12:53:41 - train: epoch 007, train_loss: 2.3466
2022-08-13 12:54:57 - eval: epoch: 007, acc1: 53.122%, acc5: 78.618%, test_loss: 2.0091, per_image_load_time: 1.729ms, per_image_inference_time: 0.584ms
2022-08-13 12:54:57 - until epoch: 007, best_acc1: 53.122%
2022-08-13 12:54:57 - epoch 008 lr: 0.081870
2022-08-13 12:55:36 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.1760
2022-08-13 12:56:09 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.3311
2022-08-13 12:56:42 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.2316
2022-08-13 12:57:15 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.2327
2022-08-13 12:57:47 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.0553
2022-08-13 12:58:20 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.3098
2022-08-13 12:58:53 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.1906
2022-08-13 12:59:26 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.1147
2022-08-13 12:59:59 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.3191
2022-08-13 13:00:32 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.3515
2022-08-13 13:01:05 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.1694
2022-08-13 13:01:38 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.1508
2022-08-13 13:02:11 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.3018
2022-08-13 13:02:44 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.1971
2022-08-13 13:03:17 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.1781
2022-08-13 13:03:50 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.3019
2022-08-13 13:04:23 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.1523
2022-08-13 13:04:56 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.3425
2022-08-13 13:05:29 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.2742
2022-08-13 13:06:02 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.2756
2022-08-13 13:06:35 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.2146
2022-08-13 13:07:09 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.1800
2022-08-13 13:07:42 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.0368
2022-08-13 13:08:15 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.0459
2022-08-13 13:08:48 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.3611
2022-08-13 13:09:21 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.2620
2022-08-13 13:09:54 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.5139
2022-08-13 13:10:27 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.5484
2022-08-13 13:11:00 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.4279
2022-08-13 13:11:33 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.3380
2022-08-13 13:12:06 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.1892
2022-08-13 13:12:39 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.7312
2022-08-13 13:13:12 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.2735
2022-08-13 13:13:45 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.1745
2022-08-13 13:14:19 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.3506
2022-08-13 13:14:52 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.5033
2022-08-13 13:15:26 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.3671
2022-08-13 13:15:59 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.0777
2022-08-13 13:16:33 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.5237
2022-08-13 13:17:07 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.5287
2022-08-13 13:17:40 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.0691
2022-08-13 13:18:13 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.2289
2022-08-13 13:18:46 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.0549
2022-08-13 13:19:19 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.1325
2022-08-13 13:19:52 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.3086
2022-08-13 13:20:25 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.2324
2022-08-13 13:20:58 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2316
2022-08-13 13:21:31 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.4849
2022-08-13 13:22:05 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.5082
2022-08-13 13:22:38 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.0898
2022-08-13 13:22:39 - train: epoch 008, train_loss: 2.2656
2022-08-13 13:23:54 - eval: epoch: 008, acc1: 52.378%, acc5: 77.734%, test_loss: 2.0644, per_image_load_time: 2.154ms, per_image_inference_time: 0.590ms
2022-08-13 13:23:54 - until epoch: 008, best_acc1: 53.122%
2022-08-13 13:23:54 - epoch 009 lr: 0.076790
2022-08-13 13:24:34 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 2.0315
2022-08-13 13:25:07 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.1925
2022-08-13 13:25:39 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.0567
2022-08-13 13:26:12 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.3238
2022-08-13 13:26:45 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.1994
2022-08-13 13:27:18 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.1348
2022-08-13 13:27:51 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.0474
2022-08-13 13:28:24 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.1996
2022-08-13 13:28:57 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.1397
2022-08-13 13:29:30 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1726
2022-08-13 13:30:03 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.4222
2022-08-13 13:30:36 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.2438
2022-08-13 13:31:10 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.2136
2022-08-13 13:31:43 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9442
2022-08-13 13:32:16 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.0246
2022-08-13 13:32:49 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.1271
2022-08-13 13:33:22 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.2902
2022-08-13 13:33:55 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.2290
2022-08-13 13:34:28 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.0145
2022-08-13 13:35:01 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.1070
2022-08-13 13:35:34 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.1331
2022-08-13 13:36:08 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.1511
2022-08-13 13:36:41 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.2777
2022-08-13 13:37:14 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.2105
2022-08-13 13:37:47 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.1390
2022-08-13 13:38:20 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.2404
2022-08-13 13:38:53 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.1398
2022-08-13 13:39:27 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.1683
2022-08-13 13:39:59 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 1.8316
2022-08-13 13:40:33 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1430
2022-08-13 13:41:06 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.4152
2022-08-13 13:41:39 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.1948
2022-08-13 13:42:12 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.1678
2022-08-13 13:42:45 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.2308
2022-08-13 13:43:18 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.1325
2022-08-13 13:43:52 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.1605
2022-08-13 13:44:25 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.2899
2022-08-13 13:44:58 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.2278
2022-08-13 13:45:31 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.0414
2022-08-13 13:46:04 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.5203
2022-08-13 13:46:37 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.1717
2022-08-13 13:47:10 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 1.9309
2022-08-13 13:47:44 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.0033
2022-08-13 13:48:17 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.4116
2022-08-13 13:48:50 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.1670
2022-08-13 13:49:24 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.3952
2022-08-13 13:49:57 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.3860
2022-08-13 13:50:31 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.2511
2022-08-13 13:51:04 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.2556
2022-08-13 13:51:37 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 1.9656
2022-08-13 13:51:38 - train: epoch 009, train_loss: 2.1980
2022-08-13 13:52:54 - eval: epoch: 009, acc1: 54.970%, acc5: 79.774%, test_loss: 1.9210, per_image_load_time: 2.365ms, per_image_inference_time: 0.563ms
2022-08-13 13:52:54 - until epoch: 009, best_acc1: 54.970%
2022-08-13 13:52:54 - epoch 010 lr: 0.071288
2022-08-13 13:53:34 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.0317
2022-08-13 13:54:07 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.1753
2022-08-13 13:54:40 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 1.9309
2022-08-13 13:55:13 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.1554
2022-08-13 13:55:46 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.1099
2022-08-13 13:56:19 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.0193
2022-08-13 13:56:52 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.2036
2022-08-13 13:57:25 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.0444
2022-08-13 13:57:58 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.0816
2022-08-13 13:58:30 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.2440
2022-08-13 13:59:03 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 1.9722
2022-08-13 13:59:36 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.1140
2022-08-13 14:00:09 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.0352
2022-08-13 14:00:42 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.2564
2022-08-13 14:01:15 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 2.1161
2022-08-13 14:01:48 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.1506
2022-08-13 14:02:21 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.2253
2022-08-13 14:02:54 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 2.0353
2022-08-13 14:03:26 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.1284
2022-08-13 14:03:59 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.1197
2022-08-13 14:04:32 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 1.9010
2022-08-13 14:05:05 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.3477
2022-08-13 14:05:38 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.1291
2022-08-13 14:06:11 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.5647
2022-08-13 14:06:44 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.0562
2022-08-13 14:07:17 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.0710
2022-08-13 14:07:50 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 2.0180
2022-08-13 14:08:23 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.2116
2022-08-13 14:08:56 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.2740
2022-08-13 14:09:29 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.0581
2022-08-13 14:10:02 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.2977
2022-08-13 14:10:35 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.3811
2022-08-13 14:11:08 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.0968
2022-08-13 14:11:41 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.2831
2022-08-13 14:12:14 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.2022
2022-08-13 14:12:47 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.1678
2022-08-13 14:13:20 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.3406
2022-08-13 14:13:53 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1733
2022-08-13 14:14:26 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 2.0525
2022-08-13 14:15:00 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.1820
2022-08-13 14:15:33 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 1.9476
2022-08-13 14:16:06 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.0635
2022-08-13 14:16:39 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 1.9908
2022-08-13 14:17:12 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 1.9149
2022-08-13 14:17:45 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.2865
2022-08-13 14:18:19 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 1.9511
2022-08-13 14:18:52 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1712
2022-08-13 14:19:25 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.1973
2022-08-13 14:19:58 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.1250
2022-08-13 14:20:30 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.1556
2022-08-13 14:20:32 - train: epoch 010, train_loss: 2.1322
2022-08-13 14:21:47 - eval: epoch: 010, acc1: 55.800%, acc5: 80.794%, test_loss: 1.8635, per_image_load_time: 2.210ms, per_image_inference_time: 0.605ms
2022-08-13 14:21:47 - until epoch: 010, best_acc1: 55.800%
2022-08-13 14:21:47 - epoch 011 lr: 0.065450
2022-08-13 14:22:27 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 1.9452
2022-08-13 14:23:00 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.2361
2022-08-13 14:23:32 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.0264
2022-08-13 14:24:06 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.1022
2022-08-13 14:24:39 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 1.9734
2022-08-13 14:25:12 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 1.9632
2022-08-13 14:25:45 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.0554
2022-08-13 14:26:18 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.2988
2022-08-13 14:26:51 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.1698
2022-08-13 14:27:24 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.1835
2022-08-13 14:27:57 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.3061
2022-08-13 14:28:30 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.3412
2022-08-13 14:29:03 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.5028
2022-08-13 14:29:36 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.1265
2022-08-13 14:30:08 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 1.9705
2022-08-13 14:30:41 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.2760
2022-08-13 14:31:13 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.2791
2022-08-13 14:31:46 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 1.8884
2022-08-13 14:32:19 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 1.9026
2022-08-13 14:32:52 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1278
2022-08-13 14:33:25 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 1.9735
2022-08-13 14:33:58 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 1.8731
2022-08-13 14:34:31 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.2199
2022-08-13 14:35:04 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.9983
2022-08-13 14:35:37 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.0106
2022-08-13 14:36:10 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 1.9941
2022-08-13 14:36:43 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.0138
2022-08-13 14:37:16 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.9952
2022-08-13 14:37:50 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.1198
2022-08-13 14:38:23 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.4394
2022-08-13 14:38:56 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 1.9714
2022-08-13 14:39:29 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.0678
2022-08-13 14:40:02 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.1379
2022-08-13 14:40:35 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.1475
2022-08-13 14:41:08 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.1525
2022-08-13 14:41:41 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.0252
2022-08-13 14:42:14 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.1393
2022-08-13 14:42:46 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 2.0250
2022-08-13 14:43:20 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.0423
2022-08-13 14:43:53 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.0952
2022-08-13 14:44:26 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.7999
2022-08-13 14:44:59 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 2.1083
2022-08-13 14:45:32 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 1.9446
2022-08-13 14:46:05 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.1615
2022-08-13 14:46:38 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 1.8800
2022-08-13 14:47:12 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 1.9606
2022-08-13 14:47:45 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.8804
2022-08-13 14:48:18 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.8203
2022-08-13 14:48:51 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.1225
2022-08-13 14:49:23 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 1.8880
2022-08-13 14:49:25 - train: epoch 011, train_loss: 2.0706
2022-08-13 14:50:40 - eval: epoch: 011, acc1: 57.998%, acc5: 82.226%, test_loss: 1.7614, per_image_load_time: 2.161ms, per_image_inference_time: 0.594ms
2022-08-13 14:50:40 - until epoch: 011, best_acc1: 57.998%
2022-08-13 14:50:40 - epoch 012 lr: 0.059368
2022-08-13 14:51:21 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.8913
2022-08-13 14:51:54 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.9261
2022-08-13 14:52:26 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 2.0487
2022-08-13 14:52:59 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 1.8995
2022-08-13 14:53:33 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.0663
2022-08-13 14:54:06 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.8431
2022-08-13 14:54:39 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.9458
2022-08-13 14:55:11 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.2665
2022-08-13 14:55:45 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 1.9069
2022-08-13 14:56:18 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.7765
2022-08-13 14:56:51 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.2367
2022-08-13 14:57:23 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.6613
2022-08-13 14:57:57 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.9519
2022-08-13 14:58:30 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.1994
2022-08-13 14:59:04 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 2.0223
2022-08-13 14:59:37 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 1.9040
2022-08-13 15:00:11 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 2.1162
2022-08-13 15:00:44 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.0756
2022-08-13 15:01:17 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1940
2022-08-13 15:01:50 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.0800
2022-08-13 15:02:24 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.0232
2022-08-13 15:02:57 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 1.9981
2022-08-13 15:03:30 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 1.9539
2022-08-13 15:04:02 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.3038
2022-08-13 15:04:35 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.7686
2022-08-13 15:05:08 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.9708
2022-08-13 15:05:41 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.0952
2022-08-13 15:06:15 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 2.1135
2022-08-13 15:06:48 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 1.8891
2022-08-13 15:07:21 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.8053
2022-08-13 15:07:54 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 1.9581
2022-08-13 15:08:27 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.9115
2022-08-13 15:09:00 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.0145
2022-08-13 15:09:34 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 1.9936
2022-08-13 15:10:07 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 1.9034
2022-08-13 15:10:40 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.7774
2022-08-13 15:11:13 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 1.9192
2022-08-13 15:11:47 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.2221
2022-08-13 15:12:20 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.9569
2022-08-13 15:12:53 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1175
2022-08-13 15:13:26 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.1049
2022-08-13 15:14:00 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.9283
2022-08-13 15:14:33 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.1274
2022-08-13 15:15:07 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.8640
2022-08-13 15:15:40 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.9758
2022-08-13 15:16:14 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.0293
2022-08-13 15:16:47 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.8950
2022-08-13 15:17:21 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.1734
2022-08-13 15:17:54 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.7939
2022-08-13 15:18:27 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.7746
2022-08-13 15:18:28 - train: epoch 012, train_loss: 2.0089
2022-08-13 15:19:45 - eval: epoch: 012, acc1: 59.638%, acc5: 83.092%, test_loss: 1.6952, per_image_load_time: 1.467ms, per_image_inference_time: 0.585ms
2022-08-13 15:19:45 - until epoch: 012, best_acc1: 59.638%
2022-08-13 15:19:45 - epoch 013 lr: 0.053138
2022-08-13 15:20:25 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.7712
2022-08-13 15:20:59 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.8418
2022-08-13 15:21:33 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.9527
2022-08-13 15:22:06 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.8044
2022-08-13 15:22:40 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 1.6761
2022-08-13 15:23:13 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 1.9707
2022-08-13 15:23:46 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.7004
2022-08-13 15:24:20 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.1192
2022-08-13 15:24:54 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.8789
2022-08-13 15:25:27 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.7877
2022-08-13 15:26:01 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 1.8920
2022-08-13 15:26:35 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 1.8225
2022-08-13 15:27:09 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.9212
2022-08-13 15:27:43 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 1.8149
2022-08-13 15:28:16 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.0440
2022-08-13 15:28:50 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.8930
2022-08-13 15:29:24 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.0356
2022-08-13 15:29:58 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9767
2022-08-13 15:30:32 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.0071
2022-08-13 15:31:06 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 1.9938
2022-08-13 15:31:39 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.0904
2022-08-13 15:32:13 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.7575
2022-08-13 15:32:47 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 1.9922
2022-08-13 15:33:21 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.0523
2022-08-13 15:33:55 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.8544
2022-08-13 15:34:29 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.8742
2022-08-13 15:35:03 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.9671
2022-08-13 15:35:37 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.1108
2022-08-13 15:36:11 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 1.9163
2022-08-13 15:36:44 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.7134
2022-08-13 15:37:18 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.8711
2022-08-13 15:37:52 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 1.9503
2022-08-13 15:38:26 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 2.0566
2022-08-13 15:38:59 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.9347
2022-08-13 15:39:33 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.7102
2022-08-13 15:40:07 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.0361
2022-08-13 15:40:41 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.6752
2022-08-13 15:41:14 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1410
2022-08-13 15:41:48 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.4269
2022-08-13 15:42:22 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.9447
2022-08-13 15:42:56 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.9363
2022-08-13 15:43:30 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 2.1262
2022-08-13 15:44:04 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.8591
2022-08-13 15:44:38 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.9869
2022-08-13 15:45:12 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.8611
2022-08-13 15:45:46 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 1.9678
2022-08-13 15:46:20 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.9693
2022-08-13 15:46:54 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.0285
2022-08-13 15:47:28 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0600
2022-08-13 15:48:01 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.0693
2022-08-13 15:48:03 - train: epoch 013, train_loss: 1.9520
2022-08-13 15:49:20 - eval: epoch: 013, acc1: 60.834%, acc5: 84.304%, test_loss: 1.6266, per_image_load_time: 1.998ms, per_image_inference_time: 0.609ms
2022-08-13 15:49:20 - until epoch: 013, best_acc1: 60.834%
2022-08-13 15:49:20 - epoch 014 lr: 0.046859
2022-08-13 15:50:01 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.8651
2022-08-13 15:50:34 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.8473
2022-08-13 15:51:07 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.8564
2022-08-13 15:51:40 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.9999
2022-08-13 15:52:14 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.9638
2022-08-13 15:52:49 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.8205
2022-08-13 15:53:22 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.9568
2022-08-13 15:53:56 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.9616
2022-08-13 15:54:30 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.7905
2022-08-13 15:55:04 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 1.9567
2022-08-13 15:55:38 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.9190
2022-08-13 15:56:11 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 2.0529
2022-08-13 15:56:45 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.0226
2022-08-13 15:57:19 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.6945
2022-08-13 15:57:53 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.2205
2022-08-13 15:58:27 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.7861
2022-08-13 15:59:01 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9041
2022-08-13 15:59:36 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 1.9976
2022-08-13 16:00:10 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.7330
2022-08-13 16:00:44 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.8368
2022-08-13 16:01:18 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 1.9470
2022-08-13 16:01:52 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.8799
2022-08-13 16:02:25 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.9705
2022-08-13 16:02:59 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 2.0390
2022-08-13 16:03:33 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.8152
2022-08-13 16:04:07 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8194
2022-08-13 16:04:40 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.9006
2022-08-13 16:05:14 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 1.9579
2022-08-13 16:05:47 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 2.0105
2022-08-13 16:06:21 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.9482
2022-08-13 16:06:54 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 2.0917
2022-08-13 16:07:28 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.8213
2022-08-13 16:08:02 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.6563
2022-08-13 16:08:37 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.7515
2022-08-13 16:09:11 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.9416
2022-08-13 16:09:44 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.7305
2022-08-13 16:10:18 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.7803
2022-08-13 16:10:52 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 1.9721
2022-08-13 16:11:26 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.8347
2022-08-13 16:12:00 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.7452
2022-08-13 16:12:34 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.7038
2022-08-13 16:13:08 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.9587
2022-08-13 16:13:42 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.7653
2022-08-13 16:14:16 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.7072
2022-08-13 16:14:50 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.8465
2022-08-13 16:15:24 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.8029
2022-08-13 16:15:57 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 2.0621
2022-08-13 16:16:32 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 2.0236
2022-08-13 16:17:05 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8442
2022-08-13 16:17:39 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.9070
2022-08-13 16:17:40 - train: epoch 014, train_loss: 1.8892
2022-08-13 16:18:56 - eval: epoch: 014, acc1: 61.386%, acc5: 84.680%, test_loss: 1.6154, per_image_load_time: 1.814ms, per_image_inference_time: 0.600ms
2022-08-13 16:18:56 - until epoch: 014, best_acc1: 61.386%
2022-08-13 16:18:56 - epoch 015 lr: 0.040630
2022-08-13 16:19:37 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.6790
2022-08-13 16:20:10 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.8216
2022-08-13 16:20:43 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 1.9790
2022-08-13 16:21:17 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.7069
2022-08-13 16:21:50 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.8890
2022-08-13 16:22:23 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 2.0676
2022-08-13 16:22:57 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.8120
2022-08-13 16:23:30 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.6673
2022-08-13 16:24:04 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.8573
2022-08-13 16:24:37 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.9200
2022-08-13 16:25:11 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.6384
2022-08-13 16:25:44 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.8297
2022-08-13 16:26:17 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.9749
2022-08-13 16:26:51 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.8599
2022-08-13 16:27:25 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.7686
2022-08-13 16:27:58 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.7060
2022-08-13 16:28:32 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.9368
2022-08-13 16:29:06 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.9284
2022-08-13 16:29:40 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.8479
2022-08-13 16:30:13 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.7210
2022-08-13 16:30:48 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.7479
2022-08-13 16:31:21 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.8322
2022-08-13 16:31:55 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.7687
2022-08-13 16:32:29 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.7724
2022-08-13 16:33:03 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.9919
2022-08-13 16:33:36 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.6616
2022-08-13 16:34:10 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.7965
2022-08-13 16:34:44 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.7265
2022-08-13 16:35:18 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.8845
2022-08-13 16:35:52 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.7522
2022-08-13 16:36:25 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.7767
2022-08-13 16:36:59 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.9484
2022-08-13 16:37:33 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.7697
2022-08-13 16:38:06 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.7171
2022-08-13 16:38:40 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 1.9920
2022-08-13 16:39:14 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.8008
2022-08-13 16:39:48 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.5635
2022-08-13 16:40:22 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.6869
2022-08-13 16:40:55 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8153
2022-08-13 16:41:29 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.7285
2022-08-13 16:42:03 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.7994
2022-08-13 16:42:36 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.4973
2022-08-13 16:43:10 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.6952
2022-08-13 16:43:45 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.9338
2022-08-13 16:44:18 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.7274
2022-08-13 16:44:52 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.9822
2022-08-13 16:45:26 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.6247
2022-08-13 16:46:00 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.7809
2022-08-13 16:46:34 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.7468
2022-08-13 16:47:07 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.9612
2022-08-13 16:47:08 - train: epoch 015, train_loss: 1.8253
2022-08-13 16:48:24 - eval: epoch: 015, acc1: 63.274%, acc5: 85.616%, test_loss: 1.5221, per_image_load_time: 1.763ms, per_image_inference_time: 0.589ms
2022-08-13 16:48:24 - until epoch: 015, best_acc1: 63.274%
2022-08-13 16:48:24 - epoch 016 lr: 0.034548
2022-08-13 16:49:05 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.7581
2022-08-13 16:49:38 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.6654
2022-08-13 16:50:12 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.8232
2022-08-13 16:50:45 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 1.8737
2022-08-13 16:51:19 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.5890
2022-08-13 16:51:53 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.6527
2022-08-13 16:52:26 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.8110
2022-08-13 16:53:00 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.7140
2022-08-13 16:53:33 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.9677
2022-08-13 16:54:07 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.7184
2022-08-13 16:54:40 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.8200
2022-08-13 16:55:14 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.8822
2022-08-13 16:55:48 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.7393
2022-08-13 16:56:22 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.6688
2022-08-13 16:56:56 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.9411
2022-08-13 16:57:29 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.6925
2022-08-13 16:58:03 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.8333
2022-08-13 16:58:36 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.8346
2022-08-13 16:59:10 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.7863
2022-08-13 16:59:44 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.5477
2022-08-13 17:00:18 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8500
2022-08-13 17:00:52 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.6994
2022-08-13 17:01:25 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.8703
2022-08-13 17:01:59 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.9245
2022-08-13 17:02:32 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.7799
2022-08-13 17:03:06 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.7137
2022-08-13 17:03:40 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.6837
2022-08-13 17:04:14 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.6314
2022-08-13 17:04:48 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.7530
2022-08-13 17:05:22 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.9313
2022-08-13 17:05:56 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.8541
2022-08-13 17:06:30 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.7596
2022-08-13 17:07:03 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.7968
2022-08-13 17:07:37 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.7046
2022-08-13 17:08:11 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.6876
2022-08-13 17:08:45 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.7058
2022-08-13 17:09:19 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.7945
2022-08-13 17:09:53 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.0640
2022-08-13 17:10:27 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.6125
2022-08-13 17:11:01 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.8652
2022-08-13 17:11:34 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.7773
2022-08-13 17:12:09 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.7492
2022-08-13 17:12:43 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.6950
2022-08-13 17:13:17 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.5714
2022-08-13 17:13:50 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.7393
2022-08-13 17:14:25 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.6432
2022-08-13 17:14:59 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.9363
2022-08-13 17:15:33 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.6120
2022-08-13 17:16:07 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.6698
2022-08-13 17:16:40 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.7467
2022-08-13 17:16:41 - train: epoch 016, train_loss: 1.7608
2022-08-13 17:17:58 - eval: epoch: 016, acc1: 64.110%, acc5: 86.102%, test_loss: 1.4776, per_image_load_time: 2.165ms, per_image_inference_time: 0.582ms
2022-08-13 17:17:58 - until epoch: 016, best_acc1: 64.110%
2022-08-13 17:17:58 - epoch 017 lr: 0.028710
2022-08-13 17:18:39 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.5764
2022-08-13 17:19:12 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.6043
2022-08-13 17:19:46 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.9319
2022-08-13 17:20:20 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.5289
2022-08-13 17:20:53 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.8789
2022-08-13 17:21:27 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 1.9854
2022-08-13 17:22:00 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.6360
2022-08-13 17:22:34 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.7301
2022-08-13 17:23:07 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.5907
2022-08-13 17:23:41 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.7081
2022-08-13 17:24:14 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 2.0743
2022-08-13 17:24:48 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.4847
2022-08-13 17:25:22 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.7425
2022-08-13 17:25:56 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.6616
2022-08-13 17:26:29 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.4500
2022-08-13 17:27:03 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.5142
2022-08-13 17:27:38 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.6369
2022-08-13 17:28:11 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.6385
2022-08-13 17:28:45 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.6510
2022-08-13 17:29:19 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.6475
2022-08-13 17:29:53 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.6276
2022-08-13 17:30:26 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.5579
2022-08-13 17:31:00 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.5662
2022-08-13 17:31:34 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.9041
2022-08-13 17:32:08 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.9493
2022-08-13 17:32:42 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.8586
2022-08-13 17:33:16 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.7265
2022-08-13 17:33:50 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.7615
2022-08-13 17:34:24 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.9428
2022-08-13 17:34:57 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.4602
2022-08-13 17:35:31 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.7849
2022-08-13 17:36:05 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.7289
2022-08-13 17:36:39 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8803
2022-08-13 17:37:13 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.5164
2022-08-13 17:37:47 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.6273
2022-08-13 17:38:20 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.9364
2022-08-13 17:38:54 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.8090
2022-08-13 17:39:27 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.8138
2022-08-13 17:40:01 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.7045
2022-08-13 17:40:35 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.8093
2022-08-13 17:41:08 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.7993
2022-08-13 17:41:42 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.5067
2022-08-13 17:42:16 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.8014
2022-08-13 17:42:50 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.6627
2022-08-13 17:43:23 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.7873
2022-08-13 17:43:57 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.5333
2022-08-13 17:44:31 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.7432
2022-08-13 17:45:05 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.6304
2022-08-13 17:45:39 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.7883
2022-08-13 17:46:12 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.4404
2022-08-13 17:46:14 - train: epoch 017, train_loss: 1.6945
2022-08-13 17:47:29 - eval: epoch: 017, acc1: 65.170%, acc5: 86.732%, test_loss: 1.4333, per_image_load_time: 1.207ms, per_image_inference_time: 0.607ms
2022-08-13 17:47:30 - until epoch: 017, best_acc1: 65.170%
2022-08-13 17:47:30 - epoch 018 lr: 0.023208
2022-08-13 17:48:10 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.5432
2022-08-13 17:48:44 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.7803
2022-08-13 17:49:17 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.8571
2022-08-13 17:49:51 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.7390
2022-08-13 17:50:24 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.5944
2022-08-13 17:50:58 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.8044
2022-08-13 17:51:32 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.6783
2022-08-13 17:52:05 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.6015
2022-08-13 17:52:39 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.6494
2022-08-13 17:53:12 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.5874
2022-08-13 17:53:47 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.7263
2022-08-13 17:54:20 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.5536
2022-08-13 17:54:54 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.8184
2022-08-13 17:55:28 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.6785
2022-08-13 17:56:02 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.9104
2022-08-13 17:56:35 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.4938
2022-08-13 17:57:08 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7450
2022-08-13 17:57:42 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.5159
2022-08-13 17:58:15 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.4844
2022-08-13 17:58:48 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.7457
2022-08-13 17:59:22 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.9070
2022-08-13 17:59:56 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.7854
2022-08-13 18:00:29 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6752
2022-08-13 18:01:03 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.5638
2022-08-13 18:01:37 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.3760
2022-08-13 18:02:10 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.5395
2022-08-13 18:02:44 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.7127
2022-08-13 18:03:18 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5803
2022-08-13 18:03:52 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.6440
2022-08-13 18:04:26 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.6901
2022-08-13 18:05:00 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.8797
2022-08-13 18:05:34 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.5747
2022-08-13 18:06:08 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.5125
2022-08-13 18:06:42 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.6300
2022-08-13 18:07:16 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.7736
2022-08-13 18:07:50 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.7120
2022-08-13 18:08:24 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.7589
2022-08-13 18:08:58 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.7046
2022-08-13 18:09:32 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.8089
2022-08-13 18:10:05 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.4446
2022-08-13 18:10:39 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7212
2022-08-13 18:11:13 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.5903
2022-08-13 18:11:47 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.5490
2022-08-13 18:12:21 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.7399
2022-08-13 18:12:54 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.7496
2022-08-13 18:13:28 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.6649
2022-08-13 18:14:02 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.7905
2022-08-13 18:14:36 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6372
2022-08-13 18:15:10 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.6636
2022-08-13 18:15:43 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.7105
2022-08-13 18:15:44 - train: epoch 018, train_loss: 1.6209
2022-08-13 18:17:00 - eval: epoch: 018, acc1: 67.052%, acc5: 87.850%, test_loss: 1.3478, per_image_load_time: 1.242ms, per_image_inference_time: 0.599ms
2022-08-13 18:17:00 - until epoch: 018, best_acc1: 67.052%
2022-08-13 18:17:00 - epoch 019 lr: 0.018128
2022-08-13 18:17:42 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.3405
2022-08-13 18:18:15 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.8313
2022-08-13 18:18:48 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.7896
2022-08-13 18:19:21 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.3987
2022-08-13 18:19:54 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.4590
2022-08-13 18:20:28 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.6433
2022-08-13 18:21:01 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.5030
2022-08-13 18:21:34 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.6518
2022-08-13 18:22:09 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.7239
2022-08-13 18:22:42 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.4078
2022-08-13 18:23:16 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5583
2022-08-13 18:23:49 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.4648
2022-08-13 18:24:23 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.6351
2022-08-13 18:24:56 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.4402
2022-08-13 18:25:30 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.9284
2022-08-13 18:26:03 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.4467
2022-08-13 18:26:37 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.6348
2022-08-13 18:27:11 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.5538
2022-08-13 18:27:45 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.7357
2022-08-13 18:28:19 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.4461
2022-08-13 18:28:53 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.1718
2022-08-13 18:29:27 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.6391
2022-08-13 18:30:01 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.6859
2022-08-13 18:30:35 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.7547
2022-08-13 18:31:09 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.5477
2022-08-13 18:31:43 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.4875
2022-08-13 18:32:16 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.4184
2022-08-13 18:32:50 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.4489
2022-08-13 18:33:24 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.6550
2022-08-13 18:33:58 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.8209
2022-08-13 18:34:32 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.6258
2022-08-13 18:35:06 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.0853
2022-08-13 18:35:40 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.4746
2022-08-13 18:36:14 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.4991
2022-08-13 18:36:48 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.6664
2022-08-13 18:37:22 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.2976
2022-08-13 18:37:56 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.6914
2022-08-13 18:38:30 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.5880
2022-08-13 18:39:04 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.5804
2022-08-13 18:39:38 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.5513
2022-08-13 18:40:12 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.6347
2022-08-13 18:40:46 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.4963
2022-08-13 18:41:20 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.3887
2022-08-13 18:41:53 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.5996
2022-08-13 18:42:27 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.7627
2022-08-13 18:43:01 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.4326
2022-08-13 18:43:35 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.6464
2022-08-13 18:44:09 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.6449
2022-08-13 18:44:43 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.5065
2022-08-13 18:45:16 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.5376
2022-08-13 18:45:17 - train: epoch 019, train_loss: 1.5485
2022-08-13 18:46:34 - eval: epoch: 019, acc1: 68.350%, acc5: 88.600%, test_loss: 1.2950, per_image_load_time: 0.995ms, per_image_inference_time: 0.578ms
2022-08-13 18:46:34 - until epoch: 019, best_acc1: 68.350%
2022-08-13 18:46:34 - epoch 020 lr: 0.013551
2022-08-13 18:47:14 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.4608
2022-08-13 18:47:47 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.3961
2022-08-13 18:48:21 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.4052
2022-08-13 18:48:55 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.3799
2022-08-13 18:49:28 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.4129
2022-08-13 18:50:02 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.5002
2022-08-13 18:50:35 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.1825
2022-08-13 18:51:09 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.8390
2022-08-13 18:51:43 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.5931
2022-08-13 18:52:16 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.4017
2022-08-13 18:52:50 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.3614
2022-08-13 18:53:24 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.2310
2022-08-13 18:53:57 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.3664
2022-08-13 18:54:31 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.6133
2022-08-13 18:55:05 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.4700
2022-08-13 18:55:38 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.5183
2022-08-13 18:56:11 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.5068
2022-08-13 18:56:45 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.5892
2022-08-13 18:57:18 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.2587
2022-08-13 18:57:52 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.3905
2022-08-13 18:58:26 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.6651
2022-08-13 18:58:59 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.2999
2022-08-13 18:59:33 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.3476
2022-08-13 19:00:07 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.7509
2022-08-13 19:00:40 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.5017
2022-08-13 19:01:14 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.3761
2022-08-13 19:01:48 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.6917
2022-08-13 19:02:22 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.4592
2022-08-13 19:02:56 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.5162
2022-08-13 19:03:30 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.7824
2022-08-13 19:04:03 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.5766
2022-08-13 19:04:37 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.4766
2022-08-13 19:05:11 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.2054
2022-08-13 19:05:44 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.4102
2022-08-13 19:06:18 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.4187
2022-08-13 19:06:52 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.4311
2022-08-13 19:07:26 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.3415
2022-08-13 19:07:59 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.4564
2022-08-13 19:08:33 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.7601
2022-08-13 19:09:06 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.4231
2022-08-13 19:09:40 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.4563
2022-08-13 19:10:14 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.5469
2022-08-13 19:10:48 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.4964
2022-08-13 19:11:21 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.3994
2022-08-13 19:11:55 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.5721
2022-08-13 19:12:29 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.4792
2022-08-13 19:13:02 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.5253
2022-08-13 19:13:36 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.2559
2022-08-13 19:14:10 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.3297
2022-08-13 19:14:43 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.4922
2022-08-13 19:14:45 - train: epoch 020, train_loss: 1.4753
2022-08-13 19:16:01 - eval: epoch: 020, acc1: 69.970%, acc5: 89.456%, test_loss: 1.2295, per_image_load_time: 1.125ms, per_image_inference_time: 0.603ms
2022-08-13 19:16:01 - until epoch: 020, best_acc1: 69.970%
2022-08-13 19:16:01 - epoch 021 lr: 0.009548
2022-08-13 19:16:41 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.3444
2022-08-13 19:17:14 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.5388
2022-08-13 19:17:48 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.3426
2022-08-13 19:18:21 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.5520
2022-08-13 19:18:54 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.2122
2022-08-13 19:19:28 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.2666
2022-08-13 19:20:01 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3117
2022-08-13 19:20:35 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.5140
2022-08-13 19:21:09 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.2913
2022-08-13 19:21:42 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3563
2022-08-13 19:22:16 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.2862
2022-08-13 19:22:50 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.3761
2022-08-13 19:23:24 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.2684
2022-08-13 19:23:58 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.3434
2022-08-13 19:24:31 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.3349
2022-08-13 19:25:05 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3061
2022-08-13 19:25:39 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5081
2022-08-13 19:26:13 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.4086
2022-08-13 19:26:47 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.5176
2022-08-13 19:27:21 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.4641
2022-08-13 19:27:54 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3157
2022-08-13 19:28:28 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.4974
2022-08-13 19:29:02 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.4597
2022-08-13 19:29:36 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.3187
2022-08-13 19:30:10 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.3432
2022-08-13 19:30:44 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.5059
2022-08-13 19:31:18 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.3697
2022-08-13 19:31:52 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.3886
2022-08-13 19:32:26 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.3123
2022-08-13 19:33:00 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.4858
2022-08-13 19:33:33 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.3042
2022-08-13 19:34:07 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.3094
2022-08-13 19:34:41 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.7007
2022-08-13 19:35:14 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.5678
2022-08-13 19:35:48 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.3704
2022-08-13 19:36:22 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.4115
2022-08-13 19:36:56 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.2989
2022-08-13 19:37:31 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.5190
2022-08-13 19:38:05 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.4121
2022-08-13 19:38:39 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.4348
2022-08-13 19:39:13 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.3041
2022-08-13 19:39:47 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.5622
2022-08-13 19:40:21 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3842
2022-08-13 19:40:54 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.4716
2022-08-13 19:41:28 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.4127
2022-08-13 19:42:02 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.2979
2022-08-13 19:42:35 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.6352
2022-08-13 19:43:09 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.4318
2022-08-13 19:43:43 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.3016
2022-08-13 19:44:15 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.2230
2022-08-13 19:44:17 - train: epoch 021, train_loss: 1.4005
2022-08-13 19:45:33 - eval: epoch: 021, acc1: 70.838%, acc5: 89.992%, test_loss: 1.1854, per_image_load_time: 1.774ms, per_image_inference_time: 0.545ms
2022-08-13 19:45:33 - until epoch: 021, best_acc1: 70.838%
2022-08-13 19:45:33 - epoch 022 lr: 0.006184
2022-08-13 19:46:13 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.0587
2022-08-13 19:46:47 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.2942
2022-08-13 19:47:20 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.2301
2022-08-13 19:47:53 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.3862
2022-08-13 19:48:26 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3277
2022-08-13 19:48:59 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.3811
2022-08-13 19:49:31 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4153
2022-08-13 19:50:04 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.4607
2022-08-13 19:50:38 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.3846
2022-08-13 19:51:11 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.3838
2022-08-13 19:51:44 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.2611
2022-08-13 19:52:17 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.0775
2022-08-13 19:52:50 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.2637
2022-08-13 19:53:23 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.0795
2022-08-13 19:53:56 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.4645
2022-08-13 19:54:29 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.1750
2022-08-13 19:55:02 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.3502
2022-08-13 19:55:35 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.4800
2022-08-13 19:56:08 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.3631
2022-08-13 19:56:41 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.3661
2022-08-13 19:57:14 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.5025
2022-08-13 19:57:47 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.1524
2022-08-13 19:58:20 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.3759
2022-08-13 19:58:53 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.5325
2022-08-13 19:59:26 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.3394
2022-08-13 20:00:00 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.3427
2022-08-13 20:00:33 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.2062
2022-08-13 20:01:06 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.3799
2022-08-13 20:01:39 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.2223
2022-08-13 20:02:12 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.3266
2022-08-13 20:02:46 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.4645
2022-08-13 20:03:19 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4786
2022-08-13 20:03:53 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.4603
2022-08-13 20:04:26 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.2258
2022-08-13 20:05:00 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.3119
2022-08-13 20:05:33 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.4604
2022-08-13 20:06:07 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.3279
2022-08-13 20:06:40 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.6019
2022-08-13 20:07:14 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.1613
2022-08-13 20:07:48 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.4626
2022-08-13 20:08:21 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.2026
2022-08-13 20:08:55 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.2679
2022-08-13 20:09:28 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.6557
2022-08-13 20:10:01 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3027
2022-08-13 20:10:35 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.1753
2022-08-13 20:11:08 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.5396
2022-08-13 20:11:42 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.3120
2022-08-13 20:12:15 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.3049
2022-08-13 20:12:48 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.2631
2022-08-13 20:13:21 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.2851
2022-08-13 20:13:23 - train: epoch 022, train_loss: 1.3325
2022-08-13 20:14:38 - eval: epoch: 022, acc1: 71.756%, acc5: 90.480%, test_loss: 1.1439, per_image_load_time: 2.315ms, per_image_inference_time: 0.614ms
2022-08-13 20:14:38 - until epoch: 022, best_acc1: 71.756%
2022-08-13 20:14:38 - epoch 023 lr: 0.003511
2022-08-13 20:15:19 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.2416
2022-08-13 20:15:52 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.2906
2022-08-13 20:16:25 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.3298
2022-08-13 20:16:58 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.3801
2022-08-13 20:17:31 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.2614
2022-08-13 20:18:04 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.1167
2022-08-13 20:18:37 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.1391
2022-08-13 20:19:11 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3890
2022-08-13 20:19:44 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4174
2022-08-13 20:20:18 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.2074
2022-08-13 20:20:52 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.1968
2022-08-13 20:21:26 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.3621
2022-08-13 20:22:00 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.2978
2022-08-13 20:22:33 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.4912
2022-08-13 20:23:07 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.1767
2022-08-13 20:23:40 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.3025
2022-08-13 20:24:14 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.3001
2022-08-13 20:24:47 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.1543
2022-08-13 20:25:21 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.2024
2022-08-13 20:25:55 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.0791
2022-08-13 20:26:29 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.3722
2022-08-13 20:27:02 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.1096
2022-08-13 20:27:36 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.0992
2022-08-13 20:28:10 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.3731
2022-08-13 20:28:44 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.2791
2022-08-13 20:29:18 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.3295
2022-08-13 20:29:51 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.2823
2022-08-13 20:30:25 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.4020
2022-08-13 20:30:59 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.1627
2022-08-13 20:31:32 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.1376
2022-08-13 20:32:06 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.4473
2022-08-13 20:32:40 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.3499
2022-08-13 20:33:13 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.3813
2022-08-13 20:33:47 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.3933
2022-08-13 20:34:21 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.3586
2022-08-13 20:34:54 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.0945
2022-08-13 20:35:28 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.2986
2022-08-13 20:36:02 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.3238
2022-08-13 20:36:36 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.2614
2022-08-13 20:37:09 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.0665
2022-08-13 20:37:43 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.2753
2022-08-13 20:38:17 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.0922
2022-08-13 20:38:50 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.1099
2022-08-13 20:39:24 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.3158
2022-08-13 20:39:58 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.1823
2022-08-13 20:40:31 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.3169
2022-08-13 20:41:05 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.0735
2022-08-13 20:41:38 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.2758
2022-08-13 20:42:12 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.2031
2022-08-13 20:42:46 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.4273
2022-08-13 20:42:47 - train: epoch 023, train_loss: 1.2761
2022-08-13 20:44:03 - eval: epoch: 023, acc1: 72.392%, acc5: 90.782%, test_loss: 1.1137, per_image_load_time: 1.203ms, per_image_inference_time: 0.594ms
2022-08-13 20:44:03 - until epoch: 023, best_acc1: 72.392%
2022-08-13 20:44:03 - epoch 024 lr: 0.001571
2022-08-13 20:44:43 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.2280
2022-08-13 20:45:17 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.1297
2022-08-13 20:45:50 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.1372
2022-08-13 20:46:24 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.2540
2022-08-13 20:46:57 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.2712
2022-08-13 20:47:31 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.2157
2022-08-13 20:48:05 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.2053
2022-08-13 20:48:39 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.1262
2022-08-13 20:49:12 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.2456
2022-08-13 20:49:46 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.1342
2022-08-13 20:50:19 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.0775
2022-08-13 20:50:53 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.2913
2022-08-13 20:51:26 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.3571
2022-08-13 20:52:00 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.1943
2022-08-13 20:52:34 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.1915
2022-08-13 20:53:08 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.1886
2022-08-13 20:53:41 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.0705
2022-08-13 20:54:15 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.5483
2022-08-13 20:54:49 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.0196
2022-08-13 20:55:23 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.2166
2022-08-13 20:55:56 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.2514
2022-08-13 20:56:30 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.2099
2022-08-13 20:57:04 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.1230
2022-08-13 20:57:37 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.3437
2022-08-13 20:58:11 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.3282
2022-08-13 20:58:45 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.3541
2022-08-13 20:59:18 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.3427
2022-08-13 20:59:52 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.3590
2022-08-13 21:00:25 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.2337
2022-08-13 21:00:59 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.1157
2022-08-13 21:01:33 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.2097
2022-08-13 21:02:06 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.1796
2022-08-13 21:02:40 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 0.9542
2022-08-13 21:03:14 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.2730
2022-08-13 21:03:48 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.0988
2022-08-13 21:04:22 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.3188
2022-08-13 21:04:56 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.1795
2022-08-13 21:05:30 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.5035
2022-08-13 21:06:04 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.1899
2022-08-13 21:06:38 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.2828
2022-08-13 21:07:11 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.1971
2022-08-13 21:07:45 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.1266
2022-08-13 21:08:19 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.2598
2022-08-13 21:08:53 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.0886
2022-08-13 21:09:27 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.1079
2022-08-13 21:10:01 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.2492
2022-08-13 21:10:35 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.2406
2022-08-13 21:11:09 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 0.9232
2022-08-13 21:11:43 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.3602
2022-08-13 21:12:16 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.2654
2022-08-13 21:12:17 - train: epoch 024, train_loss: 1.2381
2022-08-13 21:13:34 - eval: epoch: 024, acc1: 72.652%, acc5: 90.988%, test_loss: 1.0990, per_image_load_time: 1.519ms, per_image_inference_time: 0.622ms
2022-08-13 21:13:34 - until epoch: 024, best_acc1: 72.652%
2022-08-13 21:13:34 - epoch 025 lr: 0.000394
2022-08-13 21:14:15 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.1201
2022-08-13 21:14:48 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.2592
2022-08-13 21:15:22 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.3456
2022-08-13 21:15:55 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.2206
2022-08-13 21:16:28 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.0286
2022-08-13 21:17:02 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.1611
2022-08-13 21:17:35 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.1368
2022-08-13 21:18:09 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.1848
2022-08-13 21:18:42 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0664
2022-08-13 21:19:16 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3205
2022-08-13 21:19:50 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.1194
2022-08-13 21:20:23 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.2008
2022-08-13 21:20:57 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3412
2022-08-13 21:21:31 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.3114
2022-08-13 21:22:04 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.2394
2022-08-13 21:22:38 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.0988
2022-08-13 21:23:11 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.2062
2022-08-13 21:23:45 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 0.9697
2022-08-13 21:24:18 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.1096
2022-08-13 21:24:52 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.3272
2022-08-13 21:25:26 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.2175
2022-08-13 21:26:00 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 0.9338
2022-08-13 21:26:34 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.2625
2022-08-13 21:27:08 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.1984
2022-08-13 21:27:41 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.1232
2022-08-13 21:28:15 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.2808
2022-08-13 21:28:49 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.1439
2022-08-13 21:29:22 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.2800
2022-08-13 21:29:56 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.2955
2022-08-13 21:30:30 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.3243
2022-08-13 21:31:04 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.2050
2022-08-13 21:31:37 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.2486
2022-08-13 21:32:11 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.0345
2022-08-13 21:32:45 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.3939
2022-08-13 21:33:19 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.0614
2022-08-13 21:33:52 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.3205
2022-08-13 21:34:26 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.1535
2022-08-13 21:35:00 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3939
2022-08-13 21:35:34 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.4002
2022-08-13 21:36:07 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.2473
2022-08-13 21:36:42 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.4146
2022-08-13 21:37:15 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.1866
2022-08-13 21:37:49 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.0083
2022-08-13 21:38:23 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.0404
2022-08-13 21:38:57 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.1520
2022-08-13 21:39:31 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.2265
2022-08-13 21:40:04 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.0885
2022-08-13 21:40:38 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.2307
2022-08-13 21:41:12 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.1905
2022-08-13 21:41:46 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.2056
2022-08-13 21:41:47 - train: epoch 025, train_loss: 1.2204
2022-08-13 21:43:03 - eval: epoch: 025, acc1: 72.818%, acc5: 91.030%, test_loss: 1.0965, per_image_load_time: 0.864ms, per_image_inference_time: 0.588ms
2022-08-13 21:43:04 - until epoch: 025, best_acc1: 72.818%
2022-08-13 21:43:04 - train done. train time: 12.161 hours, best_acc1: 72.818%
