2022-08-05 08:13:47 - net_idx: 3
2022-08-05 08:13:47 - net_config: {'stem_width': 64, 'depth': 12, 'w_0': 32, 'w_a': 27.264071534824073, 'w_m': 2.097389031593275}
2022-08-05 08:13:47 - num_classes: 1000
2022-08-05 08:13:47 - input_image_size: 224
2022-08-05 08:13:47 - scale: 1.1428571428571428
2022-08-05 08:13:47 - seed: 0
2022-08-05 08:13:47 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-05 08:13:47 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-05 08:13:47 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fadba72ea00>
2022-08-05 08:13:47 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fadb9da1af0>
2022-08-05 08:13:47 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fadb9da1b20>
2022-08-05 08:13:47 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fadb9da1b80>
2022-08-05 08:13:47 - batch_size: 256
2022-08-05 08:13:47 - num_workers: 16
2022-08-05 08:13:47 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-05 08:13:47 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-05 08:13:47 - epochs: 25
2022-08-05 08:13:47 - print_interval: 100
2022-08-05 08:13:47 - accumulation_steps: 1
2022-08-05 08:13:47 - sync_bn: False
2022-08-05 08:13:47 - apex: True
2022-08-05 08:13:47 - use_ema_model: False
2022-08-05 08:13:47 - ema_model_decay: 0.9999
2022-08-05 08:13:47 - log_dir: ./log
2022-08-05 08:13:47 - checkpoint_dir: ./checkpoints
2022-08-05 08:13:47 - gpus_type: NVIDIA RTX A5000
2022-08-05 08:13:47 - gpus_num: 2
2022-08-05 08:13:47 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fad965d05f0>
2022-08-05 08:13:47 - ema_model: None
2022-08-05 08:13:47 - --------------------parameters--------------------
2022-08-05 08:13:47 - name: conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-05 08:13:47 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-05 08:13:47 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-05 08:13:47 - name: fc.weight, grad: True
2022-08-05 08:13:47 - name: fc.bias, grad: True
2022-08-05 08:13:47 - --------------------buffers--------------------
2022-08-05 08:13:47 - name: conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-05 08:13:47 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-05 08:13:47 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 08:13:47 - -----------no weight decay layers--------------
2022-08-05 08:13:47 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 08:13:47 - -------------weight decay layers---------------
2022-08-05 08:13:47 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 08:13:47 - epoch 001 lr: 0.100000
2022-08-05 08:14:25 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9051
2022-08-05 08:14:58 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.8948
2022-08-05 08:15:31 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8310
2022-08-05 08:16:04 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.8258
2022-08-05 08:16:37 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.7844
2022-08-05 08:17:10 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.5348
2022-08-05 08:17:43 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.6428
2022-08-05 08:18:17 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.5136
2022-08-05 08:18:50 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.4791
2022-08-05 08:19:22 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.3959
2022-08-05 08:19:55 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.3521
2022-08-05 08:20:28 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.2313
2022-08-05 08:21:02 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.1361
2022-08-05 08:21:35 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.0427
2022-08-05 08:22:08 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 5.9350
2022-08-05 08:22:41 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 5.9406
2022-08-05 08:23:13 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.6683
2022-08-05 08:23:47 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.6835
2022-08-05 08:24:20 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.6740
2022-08-05 08:24:53 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.4944
2022-08-05 08:25:25 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.4718
2022-08-05 08:25:59 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.4945
2022-08-05 08:26:31 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.2386
2022-08-05 08:27:04 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.2297
2022-08-05 08:27:38 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.3132
2022-08-05 08:28:10 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.3523
2022-08-05 08:28:44 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.2622
2022-08-05 08:29:17 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.0832
2022-08-05 08:29:51 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.0282
2022-08-05 08:30:24 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.1626
2022-08-05 08:30:57 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.0130
2022-08-05 08:31:30 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.0125
2022-08-05 08:32:04 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.8496
2022-08-05 08:32:37 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.8501
2022-08-05 08:33:09 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.7688
2022-08-05 08:33:43 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.7651
2022-08-05 08:34:16 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.7948
2022-08-05 08:34:49 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.6124
2022-08-05 08:35:22 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.5521
2022-08-05 08:35:56 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.6220
2022-08-05 08:36:28 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.8936
2022-08-05 08:37:01 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.5474
2022-08-05 08:37:35 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.4749
2022-08-05 08:38:08 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.2346
2022-08-05 08:38:41 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.4876
2022-08-05 08:39:15 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.8047
2022-08-05 08:39:48 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.2646
2022-08-05 08:40:22 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.3302
2022-08-05 08:40:55 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.5347
2022-08-05 08:41:27 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.3113
2022-08-05 08:41:28 - train: epoch 001, train_loss: 5.4256
2022-08-05 08:42:41 - eval: epoch: 001, acc1: 17.652%, acc5: 39.234%, test_loss: 4.1855, per_image_load_time: 2.052ms, per_image_inference_time: 0.553ms
2022-08-05 08:42:41 - until epoch: 001, best_acc1: 17.652%
2022-08-05 08:42:41 - epoch 002 lr: 0.099606
2022-08-05 08:43:20 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.1366
2022-08-05 08:43:52 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.0614
2022-08-05 08:44:25 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.2572
2022-08-05 08:44:57 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.2634
2022-08-05 08:45:30 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.0313
2022-08-05 08:46:02 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.0232
2022-08-05 08:46:34 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.5085
2022-08-05 08:47:08 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 3.8343
2022-08-05 08:47:42 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.9735
2022-08-05 08:48:14 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.0893
2022-08-05 08:48:47 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.0193
2022-08-05 08:49:21 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.8554
2022-08-05 08:49:54 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.9464
2022-08-05 08:50:27 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.1648
2022-08-05 08:51:00 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 3.9774
2022-08-05 08:51:34 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 4.0637
2022-08-05 08:52:06 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.9680
2022-08-05 08:52:39 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.7524
2022-08-05 08:53:12 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.6614
2022-08-05 08:53:46 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.3905
2022-08-05 08:54:19 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.8370
2022-08-05 08:54:52 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.5611
2022-08-05 08:55:24 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.6849
2022-08-05 08:55:58 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.7153
2022-08-05 08:56:32 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.6596
2022-08-05 08:57:04 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.5665
2022-08-05 08:57:37 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.6044
2022-08-05 08:58:10 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.6713
2022-08-05 08:58:44 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.6547
2022-08-05 08:59:17 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.5116
2022-08-05 08:59:50 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.5000
2022-08-05 09:00:24 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.5500
2022-08-05 09:00:57 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.6649
2022-08-05 09:01:29 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.4530
2022-08-05 09:02:03 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.4196
2022-08-05 09:02:36 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.5595
2022-08-05 09:03:10 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.6066
2022-08-05 09:03:43 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.3241
2022-08-05 09:04:15 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.4395
2022-08-05 09:04:50 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.3098
2022-08-05 09:05:23 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.5818
2022-08-05 09:05:56 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.4692
2022-08-05 09:06:28 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.5647
2022-08-05 09:07:02 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.3350
2022-08-05 09:07:35 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.1741
2022-08-05 09:08:09 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.2863
2022-08-05 09:08:42 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.2599
2022-08-05 09:09:14 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.4718
2022-08-05 09:09:48 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.3670
2022-08-05 09:10:20 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.1824
2022-08-05 09:10:21 - train: epoch 002, train_loss: 3.7004
2022-08-05 09:11:33 - eval: epoch: 002, acc1: 30.592%, acc5: 56.644%, test_loss: 3.4628, per_image_load_time: 2.224ms, per_image_inference_time: 0.542ms
2022-08-05 09:11:34 - until epoch: 002, best_acc1: 30.592%
2022-08-05 09:11:34 - epoch 003 lr: 0.098429
2022-08-05 09:12:12 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.2100
2022-08-05 09:12:44 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.2420
2022-08-05 09:13:17 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.3664
2022-08-05 09:13:50 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.2989
2022-08-05 09:14:23 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.3455
2022-08-05 09:14:55 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.1273
2022-08-05 09:15:28 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.3248
2022-08-05 09:16:01 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.2857
2022-08-05 09:16:34 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.2113
2022-08-05 09:17:07 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.2517
2022-08-05 09:17:40 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.0936
2022-08-05 09:18:13 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.1048
2022-08-05 09:18:46 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.1006
2022-08-05 09:19:19 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 2.9934
2022-08-05 09:19:52 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.3773
2022-08-05 09:20:25 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.1471
2022-08-05 09:20:59 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.0940
2022-08-05 09:21:31 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.0480
2022-08-05 09:22:05 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.1836
2022-08-05 09:22:39 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.1087
2022-08-05 09:23:12 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.2157
2022-08-05 09:23:45 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.4693
2022-08-05 09:24:18 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.9713
2022-08-05 09:24:51 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.0936
2022-08-05 09:25:24 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.1821
2022-08-05 09:25:57 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.0560
2022-08-05 09:26:31 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.3657
2022-08-05 09:27:04 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.2251
2022-08-05 09:27:38 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 3.0218
2022-08-05 09:28:10 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.0428
2022-08-05 09:28:44 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.3094
2022-08-05 09:29:17 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.0407
2022-08-05 09:29:51 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0089
2022-08-05 09:30:23 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.2273
2022-08-05 09:30:57 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.8881
2022-08-05 09:31:30 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 3.0340
2022-08-05 09:32:04 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.0879
2022-08-05 09:32:37 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.1781
2022-08-05 09:33:10 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.3312
2022-08-05 09:33:43 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7627
2022-08-05 09:34:16 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.8650
2022-08-05 09:34:49 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 2.9605
2022-08-05 09:35:23 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.8169
2022-08-05 09:35:56 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.9643
2022-08-05 09:36:29 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.0442
2022-08-05 09:37:02 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 3.0410
2022-08-05 09:37:36 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 2.9315
2022-08-05 09:38:09 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.0113
2022-08-05 09:38:43 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 2.9374
2022-08-05 09:39:15 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.9478
2022-08-05 09:39:16 - train: epoch 003, train_loss: 3.1033
2022-08-05 09:40:30 - eval: epoch: 003, acc1: 38.412%, acc5: 65.010%, test_loss: 2.8439, per_image_load_time: 1.771ms, per_image_inference_time: 0.558ms
2022-08-05 09:40:30 - until epoch: 003, best_acc1: 38.412%
2022-08-05 09:40:30 - epoch 004 lr: 0.096488
2022-08-05 09:41:09 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.8656
2022-08-05 09:41:41 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.8308
2022-08-05 09:42:14 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.9641
2022-08-05 09:42:47 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.8868
2022-08-05 09:43:20 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.7913
2022-08-05 09:43:53 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.2919
2022-08-05 09:44:26 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.7910
2022-08-05 09:44:59 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.6889
2022-08-05 09:45:32 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.7566
2022-08-05 09:46:05 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.9173
2022-08-05 09:46:38 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.2960
2022-08-05 09:47:12 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.7589
2022-08-05 09:47:45 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.8154
2022-08-05 09:48:18 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.7800
2022-08-05 09:48:51 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.9765
2022-08-05 09:49:24 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.7502
2022-08-05 09:49:57 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.9187
2022-08-05 09:50:31 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 3.0761
2022-08-05 09:51:03 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.8926
2022-08-05 09:51:37 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.8026
2022-08-05 09:52:10 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.7462
2022-08-05 09:52:43 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.8801
2022-08-05 09:53:16 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.6109
2022-08-05 09:53:50 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.5255
2022-08-05 09:54:23 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.7144
2022-08-05 09:54:57 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.8794
2022-08-05 09:55:29 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.5945
2022-08-05 09:56:03 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8255
2022-08-05 09:56:36 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.7882
2022-08-05 09:57:10 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.8595
2022-08-05 09:57:43 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.8790
2022-08-05 09:58:16 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.7396
2022-08-05 09:58:49 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8275
2022-08-05 09:59:22 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.8215
2022-08-05 09:59:55 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.7347
2022-08-05 10:00:28 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.7701
2022-08-05 10:01:02 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.7768
2022-08-05 10:01:36 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.4223
2022-08-05 10:02:09 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.5073
2022-08-05 10:02:43 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.4463
2022-08-05 10:03:16 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.6237
2022-08-05 10:03:49 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6777
2022-08-05 10:04:22 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.6570
2022-08-05 10:04:56 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.4884
2022-08-05 10:05:29 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.3218
2022-08-05 10:06:02 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.7121
2022-08-05 10:06:36 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.7477
2022-08-05 10:07:09 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.5771
2022-08-05 10:07:43 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.6810
2022-08-05 10:08:15 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.7574
2022-08-05 10:08:16 - train: epoch 004, train_loss: 2.8010
2022-08-05 10:09:28 - eval: epoch: 004, acc1: 45.244%, acc5: 71.434%, test_loss: 2.4514, per_image_load_time: 1.523ms, per_image_inference_time: 0.574ms
2022-08-05 10:09:29 - until epoch: 004, best_acc1: 45.244%
2022-08-05 10:09:29 - epoch 005 lr: 0.093815
2022-08-05 10:10:07 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.5797
2022-08-05 10:10:40 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.8238
2022-08-05 10:11:12 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.8940
2022-08-05 10:11:45 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.5456
2022-08-05 10:12:17 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.5087
2022-08-05 10:12:50 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.6689
2022-08-05 10:13:24 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.5746
2022-08-05 10:13:58 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.7171
2022-08-05 10:14:30 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.5642
2022-08-05 10:15:03 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.5386
2022-08-05 10:15:36 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.6493
2022-08-05 10:16:10 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.8592
2022-08-05 10:16:43 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.4639
2022-08-05 10:17:16 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.6610
2022-08-05 10:17:49 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.3967
2022-08-05 10:18:22 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.4468
2022-08-05 10:18:55 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.2965
2022-08-05 10:19:29 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.5954
2022-08-05 10:20:02 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.4223
2022-08-05 10:20:35 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.4956
2022-08-05 10:21:08 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.3703
2022-08-05 10:21:42 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.6554
2022-08-05 10:22:14 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.5938
2022-08-05 10:22:48 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.6763
2022-08-05 10:23:21 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.4727
2022-08-05 10:23:55 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.7552
2022-08-05 10:24:28 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.6454
2022-08-05 10:25:01 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.5127
2022-08-05 10:25:35 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.4147
2022-08-05 10:26:07 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.8008
2022-08-05 10:26:41 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.7528
2022-08-05 10:27:14 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.4681
2022-08-05 10:27:47 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.3916
2022-08-05 10:28:20 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.4794
2022-08-05 10:28:54 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.5239
2022-08-05 10:29:26 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.4989
2022-08-05 10:30:00 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.4847
2022-08-05 10:30:33 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.4759
2022-08-05 10:31:06 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.8175
2022-08-05 10:31:39 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.6195
2022-08-05 10:32:12 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.5609
2022-08-05 10:32:46 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.5612
2022-08-05 10:33:19 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.5436
2022-08-05 10:33:53 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.6207
2022-08-05 10:34:26 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.6779
2022-08-05 10:35:00 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.6658
2022-08-05 10:35:32 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.3873
2022-08-05 10:36:06 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.3826
2022-08-05 10:36:40 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.8252
2022-08-05 10:37:12 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.4354
2022-08-05 10:37:13 - train: epoch 005, train_loss: 2.6037
2022-08-05 10:38:26 - eval: epoch: 005, acc1: 47.616%, acc5: 74.208%, test_loss: 2.2804, per_image_load_time: 1.991ms, per_image_inference_time: 0.596ms
2022-08-05 10:38:26 - until epoch: 005, best_acc1: 47.616%
2022-08-05 10:38:26 - epoch 006 lr: 0.090450
2022-08-05 10:39:05 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.4804
2022-08-05 10:39:38 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.4703
2022-08-05 10:40:10 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.5533
2022-08-05 10:40:43 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.4330
2022-08-05 10:41:15 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.4533
2022-08-05 10:41:48 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.4955
2022-08-05 10:42:21 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.6761
2022-08-05 10:42:54 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.5306
2022-08-05 10:43:26 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3753
2022-08-05 10:44:00 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.3645
2022-08-05 10:44:32 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.5466
2022-08-05 10:45:06 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.5750
2022-08-05 10:45:39 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5264
2022-08-05 10:46:13 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.5350
2022-08-05 10:46:45 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.7263
2022-08-05 10:47:19 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.2881
2022-08-05 10:47:52 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.6351
2022-08-05 10:48:24 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.5356
2022-08-05 10:48:58 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.3840
2022-08-05 10:49:31 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.6297
2022-08-05 10:50:05 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.3369
2022-08-05 10:50:38 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.1528
2022-08-05 10:51:11 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.2873
2022-08-05 10:51:44 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.5108
2022-08-05 10:52:18 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4764
2022-08-05 10:52:51 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.3179
2022-08-05 10:53:24 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.4235
2022-08-05 10:53:57 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.3373
2022-08-05 10:54:32 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.6665
2022-08-05 10:55:05 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.5492
2022-08-05 10:55:37 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.2570
2022-08-05 10:56:11 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.4322
2022-08-05 10:56:43 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.3912
2022-08-05 10:57:17 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.6839
2022-08-05 10:57:50 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.5420
2022-08-05 10:58:24 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.4840
2022-08-05 10:58:57 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.3068
2022-08-05 10:59:30 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.3083
2022-08-05 11:00:03 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.2882
2022-08-05 11:00:37 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.4623
2022-08-05 11:01:10 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.4211
2022-08-05 11:01:44 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.2073
2022-08-05 11:02:16 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.3587
2022-08-05 11:02:50 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.3074
2022-08-05 11:03:23 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.4672
2022-08-05 11:03:56 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.5557
2022-08-05 11:04:30 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4564
2022-08-05 11:05:03 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.5085
2022-08-05 11:05:36 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.3749
2022-08-05 11:06:09 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.2608
2022-08-05 11:06:10 - train: epoch 006, train_loss: 2.4685
2022-08-05 11:07:23 - eval: epoch: 006, acc1: 49.516%, acc5: 75.324%, test_loss: 2.2065, per_image_load_time: 1.988ms, per_image_inference_time: 0.574ms
2022-08-05 11:07:24 - until epoch: 006, best_acc1: 49.516%
2022-08-05 11:07:24 - epoch 007 lr: 0.086448
2022-08-05 11:08:02 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.4584
2022-08-05 11:08:35 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.5460
2022-08-05 11:09:08 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.6995
2022-08-05 11:09:41 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.4441
2022-08-05 11:10:14 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.2103
2022-08-05 11:10:47 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.6282
2022-08-05 11:11:20 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.2457
2022-08-05 11:11:53 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.3967
2022-08-05 11:12:27 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.4594
2022-08-05 11:13:00 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.3880
2022-08-05 11:13:33 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.4161
2022-08-05 11:14:06 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.4351
2022-08-05 11:14:39 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.2128
2022-08-05 11:15:12 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.3743
2022-08-05 11:15:45 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.5174
2022-08-05 11:16:19 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.3518
2022-08-05 11:16:52 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.4818
2022-08-05 11:17:26 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.2243
2022-08-05 11:17:59 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.4542
2022-08-05 11:18:32 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.1311
2022-08-05 11:19:06 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.4988
2022-08-05 11:19:38 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.1659
2022-08-05 11:20:11 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.4199
2022-08-05 11:20:45 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.5913
2022-08-05 11:21:19 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.4298
2022-08-05 11:21:52 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.1921
2022-08-05 11:22:25 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.0982
2022-08-05 11:22:58 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.3121
2022-08-05 11:23:33 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.2198
2022-08-05 11:24:05 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.2896
2022-08-05 11:24:38 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.2795
2022-08-05 11:25:11 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.3295
2022-08-05 11:25:45 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.2163
2022-08-05 11:26:18 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.2899
2022-08-05 11:26:51 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.4293
2022-08-05 11:27:24 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.1921
2022-08-05 11:27:57 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.2933
2022-08-05 11:28:30 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.6156
2022-08-05 11:29:04 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.3880
2022-08-05 11:29:37 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.4804
2022-08-05 11:30:10 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.3336
2022-08-05 11:30:44 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.4445
2022-08-05 11:31:17 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.3819
2022-08-05 11:31:50 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.0264
2022-08-05 11:32:24 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.4175
2022-08-05 11:32:57 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.3968
2022-08-05 11:33:30 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.2501
2022-08-05 11:34:04 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.4670
2022-08-05 11:34:37 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.3752
2022-08-05 11:35:09 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.2815
2022-08-05 11:35:10 - train: epoch 007, train_loss: 2.3681
2022-08-05 11:36:23 - eval: epoch: 007, acc1: 51.956%, acc5: 77.456%, test_loss: 2.0787, per_image_load_time: 1.170ms, per_image_inference_time: 0.561ms
2022-08-05 11:36:24 - until epoch: 007, best_acc1: 51.956%
2022-08-05 11:36:24 - epoch 008 lr: 0.081870
2022-08-05 11:37:02 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.4847
2022-08-05 11:37:35 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.3055
2022-08-05 11:38:08 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.3220
2022-08-05 11:38:40 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.1621
2022-08-05 11:39:13 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.0886
2022-08-05 11:39:46 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.1960
2022-08-05 11:40:20 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.3270
2022-08-05 11:40:53 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.0654
2022-08-05 11:41:26 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.1827
2022-08-05 11:41:58 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.3625
2022-08-05 11:42:31 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.3325
2022-08-05 11:43:04 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.2565
2022-08-05 11:43:37 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.3656
2022-08-05 11:44:11 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.1106
2022-08-05 11:44:44 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.3342
2022-08-05 11:45:17 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.3456
2022-08-05 11:45:50 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.2751
2022-08-05 11:46:23 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.3220
2022-08-05 11:46:55 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.1958
2022-08-05 11:47:29 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.4182
2022-08-05 11:48:02 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.4513
2022-08-05 11:48:36 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.1941
2022-08-05 11:49:08 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.2594
2022-08-05 11:49:41 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.1857
2022-08-05 11:50:14 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.1012
2022-08-05 11:50:48 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.3141
2022-08-05 11:51:21 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.5769
2022-08-05 11:51:54 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.4127
2022-08-05 11:52:27 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.2628
2022-08-05 11:53:00 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.4432
2022-08-05 11:53:33 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.2062
2022-08-05 11:54:07 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.3939
2022-08-05 11:54:40 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.4891
2022-08-05 11:55:13 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.2395
2022-08-05 11:55:47 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.3458
2022-08-05 11:56:20 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.2967
2022-08-05 11:56:53 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.1537
2022-08-05 11:57:26 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.3370
2022-08-05 11:57:59 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.4911
2022-08-05 11:58:32 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.6021
2022-08-05 11:59:05 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.1788
2022-08-05 11:59:38 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.3218
2022-08-05 12:00:12 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.1156
2022-08-05 12:00:44 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.2556
2022-08-05 12:01:17 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.1192
2022-08-05 12:01:50 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.4004
2022-08-05 12:02:23 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2720
2022-08-05 12:02:56 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.2828
2022-08-05 12:03:29 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.2943
2022-08-05 12:04:01 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.2184
2022-08-05 12:04:02 - train: epoch 008, train_loss: 2.2896
2022-08-05 12:05:15 - eval: epoch: 008, acc1: 53.656%, acc5: 79.030%, test_loss: 1.9822, per_image_load_time: 1.960ms, per_image_inference_time: 0.553ms
2022-08-05 12:05:15 - until epoch: 008, best_acc1: 53.656%
2022-08-05 12:05:15 - epoch 009 lr: 0.076790
2022-08-05 12:05:54 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 1.9966
2022-08-05 12:06:27 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.1853
2022-08-05 12:06:59 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.1620
2022-08-05 12:07:31 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.4387
2022-08-05 12:08:04 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.0614
2022-08-05 12:08:37 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.0731
2022-08-05 12:09:10 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.0446
2022-08-05 12:09:42 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.3495
2022-08-05 12:10:16 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 1.9078
2022-08-05 12:10:49 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.2667
2022-08-05 12:11:22 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.4198
2022-08-05 12:11:55 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.2793
2022-08-05 12:12:28 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.3544
2022-08-05 12:13:02 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9109
2022-08-05 12:13:35 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.1987
2022-08-05 12:14:08 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2451
2022-08-05 12:14:42 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.3717
2022-08-05 12:15:15 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.3449
2022-08-05 12:15:49 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.0516
2022-08-05 12:16:22 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.0790
2022-08-05 12:16:55 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.0959
2022-08-05 12:17:29 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.1902
2022-08-05 12:18:02 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.2113
2022-08-05 12:18:35 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.1475
2022-08-05 12:19:09 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.0996
2022-08-05 12:19:42 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.3584
2022-08-05 12:20:15 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.0977
2022-08-05 12:20:49 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.4667
2022-08-05 12:21:22 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.1001
2022-08-05 12:21:55 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1434
2022-08-05 12:22:29 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.4031
2022-08-05 12:23:01 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.0947
2022-08-05 12:23:34 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.1880
2022-08-05 12:24:07 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.2687
2022-08-05 12:24:41 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.1865
2022-08-05 12:25:14 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.1247
2022-08-05 12:25:47 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.1499
2022-08-05 12:26:20 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.3460
2022-08-05 12:26:53 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 1.9533
2022-08-05 12:27:26 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.3037
2022-08-05 12:28:00 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.0970
2022-08-05 12:28:34 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 1.9906
2022-08-05 12:29:07 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.1684
2022-08-05 12:29:40 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.2429
2022-08-05 12:30:13 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.2354
2022-08-05 12:30:46 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.4844
2022-08-05 12:31:19 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.2914
2022-08-05 12:31:53 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.1814
2022-08-05 12:32:26 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.1502
2022-08-05 12:32:59 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.0689
2022-08-05 12:33:00 - train: epoch 009, train_loss: 2.2221
2022-08-05 12:34:13 - eval: epoch: 009, acc1: 55.360%, acc5: 80.230%, test_loss: 1.8982, per_image_load_time: 2.292ms, per_image_inference_time: 0.533ms
2022-08-05 12:34:13 - until epoch: 009, best_acc1: 55.360%
2022-08-05 12:34:13 - epoch 010 lr: 0.071288
2022-08-05 12:34:52 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.1953
2022-08-05 12:35:24 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.2577
2022-08-05 12:35:58 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.1111
2022-08-05 12:36:30 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.3335
2022-08-05 12:37:03 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.1170
2022-08-05 12:37:37 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.2263
2022-08-05 12:38:10 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.1093
2022-08-05 12:38:43 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.0540
2022-08-05 12:39:16 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.1112
2022-08-05 12:39:49 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.1757
2022-08-05 12:40:22 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.1848
2022-08-05 12:40:55 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.2030
2022-08-05 12:41:29 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 1.9463
2022-08-05 12:42:02 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.2979
2022-08-05 12:42:36 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 2.0689
2022-08-05 12:43:08 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.1821
2022-08-05 12:43:42 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.2377
2022-08-05 12:44:15 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 2.0197
2022-08-05 12:44:49 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.2173
2022-08-05 12:45:21 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.2777
2022-08-05 12:45:55 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 1.8679
2022-08-05 12:46:28 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.3751
2022-08-05 12:47:02 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.3181
2022-08-05 12:47:35 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.1425
2022-08-05 12:48:08 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.2580
2022-08-05 12:48:41 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.3751
2022-08-05 12:49:15 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.9646
2022-08-05 12:49:48 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.1520
2022-08-05 12:50:21 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.3980
2022-08-05 12:50:55 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1358
2022-08-05 12:51:29 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.4157
2022-08-05 12:52:01 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.1133
2022-08-05 12:52:35 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.1588
2022-08-05 12:53:08 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.2031
2022-08-05 12:53:41 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.2354
2022-08-05 12:54:15 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.3396
2022-08-05 12:54:48 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 1.9386
2022-08-05 12:55:21 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.2541
2022-08-05 12:55:54 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 2.0143
2022-08-05 12:56:28 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.0150
2022-08-05 12:57:01 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.0226
2022-08-05 12:57:34 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.0715
2022-08-05 12:58:08 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.1947
2022-08-05 12:58:41 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.0212
2022-08-05 12:59:14 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.2416
2022-08-05 12:59:48 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.1003
2022-08-05 13:00:21 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1232
2022-08-05 13:00:54 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.0943
2022-08-05 13:01:28 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.2685
2022-08-05 13:02:00 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 1.9501
2022-08-05 13:02:01 - train: epoch 010, train_loss: 2.1594
2022-08-05 13:03:14 - eval: epoch: 010, acc1: 56.666%, acc5: 81.068%, test_loss: 1.8474, per_image_load_time: 2.254ms, per_image_inference_time: 0.569ms
2022-08-05 13:03:14 - until epoch: 010, best_acc1: 56.666%
2022-08-05 13:03:14 - epoch 011 lr: 0.065450
2022-08-05 13:03:52 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.0027
2022-08-05 13:04:26 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.2509
2022-08-05 13:04:59 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.1530
2022-08-05 13:05:32 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.1434
2022-08-05 13:06:04 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.1182
2022-08-05 13:06:37 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.0891
2022-08-05 13:07:10 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.1139
2022-08-05 13:07:43 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.2239
2022-08-05 13:08:16 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.2159
2022-08-05 13:08:50 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 1.9513
2022-08-05 13:09:23 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.1183
2022-08-05 13:09:56 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.1618
2022-08-05 13:10:28 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.3761
2022-08-05 13:11:02 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 1.9360
2022-08-05 13:11:36 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.1554
2022-08-05 13:12:09 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.1399
2022-08-05 13:12:42 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.1583
2022-08-05 13:13:16 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.0017
2022-08-05 13:13:49 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.0662
2022-08-05 13:14:22 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1108
2022-08-05 13:14:55 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.0542
2022-08-05 13:15:29 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 2.0428
2022-08-05 13:16:02 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.0401
2022-08-05 13:16:35 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.9794
2022-08-05 13:17:08 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.0926
2022-08-05 13:17:42 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.1045
2022-08-05 13:18:15 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 1.8715
2022-08-05 13:18:47 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.7390
2022-08-05 13:19:21 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.0395
2022-08-05 13:19:54 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.4697
2022-08-05 13:20:27 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.0244
2022-08-05 13:21:01 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.1235
2022-08-05 13:21:33 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.3656
2022-08-05 13:22:06 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.0718
2022-08-05 13:22:40 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 1.9296
2022-08-05 13:23:13 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.2113
2022-08-05 13:23:47 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.1624
2022-08-05 13:24:19 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 2.0507
2022-08-05 13:24:54 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 1.9672
2022-08-05 13:25:26 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.0835
2022-08-05 13:26:00 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.8951
2022-08-05 13:26:33 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 2.0292
2022-08-05 13:27:06 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.1589
2022-08-05 13:27:40 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.1063
2022-08-05 13:28:13 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.0882
2022-08-05 13:28:46 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 1.9998
2022-08-05 13:29:19 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.8692
2022-08-05 13:29:52 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.8336
2022-08-05 13:30:26 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 1.9920
2022-08-05 13:30:58 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.0803
2022-08-05 13:30:59 - train: epoch 011, train_loss: 2.0980
2022-08-05 13:32:13 - eval: epoch: 011, acc1: 57.790%, acc5: 81.844%, test_loss: 1.7903, per_image_load_time: 2.255ms, per_image_inference_time: 0.517ms
2022-08-05 13:32:13 - until epoch: 011, best_acc1: 57.790%
2022-08-05 13:32:13 - epoch 012 lr: 0.059368
2022-08-05 13:32:51 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.8612
2022-08-05 13:33:24 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 2.0563
2022-08-05 13:33:58 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 2.0100
2022-08-05 13:34:31 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.0174
2022-08-05 13:35:04 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.0151
2022-08-05 13:35:36 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.9874
2022-08-05 13:36:10 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.9542
2022-08-05 13:36:43 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.2887
2022-08-05 13:37:16 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.1142
2022-08-05 13:37:49 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.8983
2022-08-05 13:38:22 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.3172
2022-08-05 13:38:54 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.8883
2022-08-05 13:39:28 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 2.0327
2022-08-05 13:40:01 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.2785
2022-08-05 13:40:35 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 2.1391
2022-08-05 13:41:08 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 1.9616
2022-08-05 13:41:41 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.8618
2022-08-05 13:42:14 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.0541
2022-08-05 13:42:47 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.2250
2022-08-05 13:43:20 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.1440
2022-08-05 13:43:53 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.1120
2022-08-05 13:44:26 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.0474
2022-08-05 13:44:59 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 2.1766
2022-08-05 13:45:32 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.1166
2022-08-05 13:46:06 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.8141
2022-08-05 13:46:39 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.7215
2022-08-05 13:47:12 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.1448
2022-08-05 13:47:46 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 2.0602
2022-08-05 13:48:19 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.0741
2022-08-05 13:48:52 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.8452
2022-08-05 13:49:25 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.0440
2022-08-05 13:49:58 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.9302
2022-08-05 13:50:32 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 1.9058
2022-08-05 13:51:05 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.1031
2022-08-05 13:51:37 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.1031
2022-08-05 13:52:11 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.9605
2022-08-05 13:52:44 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.1927
2022-08-05 13:53:18 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.1827
2022-08-05 13:53:51 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8890
2022-08-05 13:54:24 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.2982
2022-08-05 13:54:58 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.0089
2022-08-05 13:55:30 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 2.0634
2022-08-05 13:56:04 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.1725
2022-08-05 13:56:37 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.9926
2022-08-05 13:57:11 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 2.0650
2022-08-05 13:57:44 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.2611
2022-08-05 13:58:18 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.9425
2022-08-05 13:58:51 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.0987
2022-08-05 13:59:25 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.9739
2022-08-05 13:59:57 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.7466
2022-08-05 13:59:58 - train: epoch 012, train_loss: 2.0399
2022-08-05 14:01:11 - eval: epoch: 012, acc1: 59.244%, acc5: 83.014%, test_loss: 1.7128, per_image_load_time: 1.955ms, per_image_inference_time: 0.535ms
2022-08-05 14:01:11 - until epoch: 012, best_acc1: 59.244%
2022-08-05 14:01:11 - epoch 013 lr: 0.053138
2022-08-05 14:01:51 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 2.0029
2022-08-05 14:02:23 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.9282
2022-08-05 14:02:56 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8149
2022-08-05 14:03:28 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.8353
2022-08-05 14:04:01 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 1.9168
2022-08-05 14:04:34 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 2.1006
2022-08-05 14:05:07 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.7531
2022-08-05 14:05:40 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.0130
2022-08-05 14:06:13 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.8536
2022-08-05 14:06:46 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.9698
2022-08-05 14:07:19 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 1.9740
2022-08-05 14:07:52 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.2802
2022-08-05 14:08:26 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.8724
2022-08-05 14:08:59 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.0242
2022-08-05 14:09:32 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.1250
2022-08-05 14:10:05 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.8897
2022-08-05 14:10:38 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 1.7379
2022-08-05 14:11:11 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.7825
2022-08-05 14:11:45 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 1.9984
2022-08-05 14:12:18 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.2640
2022-08-05 14:12:52 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.1049
2022-08-05 14:13:25 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 2.0587
2022-08-05 14:13:58 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 1.9777
2022-08-05 14:14:32 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 1.8872
2022-08-05 14:15:05 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.8010
2022-08-05 14:15:38 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.9209
2022-08-05 14:16:11 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.9761
2022-08-05 14:16:44 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 1.8486
2022-08-05 14:17:18 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 1.9917
2022-08-05 14:17:50 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.8835
2022-08-05 14:18:24 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.8802
2022-08-05 14:18:57 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 2.0379
2022-08-05 14:19:30 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.7544
2022-08-05 14:20:03 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.8244
2022-08-05 14:20:37 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.8447
2022-08-05 14:21:10 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.1813
2022-08-05 14:21:43 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.5990
2022-08-05 14:22:16 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.2543
2022-08-05 14:22:49 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.1143
2022-08-05 14:23:22 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 2.0604
2022-08-05 14:23:56 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.9603
2022-08-05 14:24:30 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 1.9958
2022-08-05 14:25:02 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.9644
2022-08-05 14:25:36 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.8469
2022-08-05 14:26:10 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.8334
2022-08-05 14:26:44 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.1749
2022-08-05 14:27:17 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.8703
2022-08-05 14:27:50 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 1.9762
2022-08-05 14:28:24 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0909
2022-08-05 14:28:57 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.0382
2022-08-05 14:28:58 - train: epoch 013, train_loss: 1.9812
2022-08-05 14:30:11 - eval: epoch: 013, acc1: 60.670%, acc5: 83.516%, test_loss: 1.6742, per_image_load_time: 2.199ms, per_image_inference_time: 0.559ms
2022-08-05 14:30:11 - until epoch: 013, best_acc1: 60.670%
2022-08-05 14:30:11 - epoch 014 lr: 0.046859
2022-08-05 14:30:50 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.9111
2022-08-05 14:31:23 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 2.0494
2022-08-05 14:31:56 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.9026
2022-08-05 14:32:29 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 2.0670
2022-08-05 14:33:02 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.8290
2022-08-05 14:33:35 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.8411
2022-08-05 14:34:08 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.5516
2022-08-05 14:34:41 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.8417
2022-08-05 14:35:14 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.9478
2022-08-05 14:35:47 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.1705
2022-08-05 14:36:21 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.7845
2022-08-05 14:36:54 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 1.8417
2022-08-05 14:37:27 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.0485
2022-08-05 14:38:00 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 2.0698
2022-08-05 14:38:34 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 1.9369
2022-08-05 14:39:07 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.7620
2022-08-05 14:39:40 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9996
2022-08-05 14:40:14 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 1.9013
2022-08-05 14:40:47 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.7583
2022-08-05 14:41:20 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.7422
2022-08-05 14:41:54 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 1.9277
2022-08-05 14:42:27 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 2.0246
2022-08-05 14:43:00 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.8017
2022-08-05 14:43:34 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.8599
2022-08-05 14:44:07 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.9182
2022-08-05 14:44:40 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8839
2022-08-05 14:45:14 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.7652
2022-08-05 14:45:47 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 1.9891
2022-08-05 14:46:21 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9411
2022-08-05 14:46:54 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.9791
2022-08-05 14:47:27 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.9065
2022-08-05 14:48:00 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.8355
2022-08-05 14:48:34 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.6328
2022-08-05 14:49:08 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.9791
2022-08-05 14:49:41 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.9848
2022-08-05 14:50:15 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.8613
2022-08-05 14:50:48 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.9703
2022-08-05 14:51:21 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 1.9886
2022-08-05 14:51:55 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.7167
2022-08-05 14:52:28 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.8865
2022-08-05 14:53:01 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.9966
2022-08-05 14:53:35 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.8721
2022-08-05 14:54:08 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.8387
2022-08-05 14:54:41 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.8275
2022-08-05 14:55:14 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.9091
2022-08-05 14:55:47 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.7582
2022-08-05 14:56:21 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.7678
2022-08-05 14:56:54 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.8905
2022-08-05 14:57:27 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8623
2022-08-05 14:57:59 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.9675
2022-08-05 14:58:01 - train: epoch 014, train_loss: 1.9176
2022-08-05 14:59:15 - eval: epoch: 014, acc1: 61.748%, acc5: 84.580%, test_loss: 1.6039, per_image_load_time: 1.641ms, per_image_inference_time: 0.558ms
2022-08-05 14:59:15 - until epoch: 014, best_acc1: 61.748%
2022-08-05 14:59:15 - epoch 015 lr: 0.040630
2022-08-05 14:59:54 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.7359
2022-08-05 15:00:27 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.9774
2022-08-05 15:01:00 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.0509
2022-08-05 15:01:32 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.7960
2022-08-05 15:02:05 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.8216
2022-08-05 15:02:38 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 2.0243
2022-08-05 15:03:11 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.8730
2022-08-05 15:03:44 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.9297
2022-08-05 15:04:17 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.8021
2022-08-05 15:04:51 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.8604
2022-08-05 15:05:23 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.6020
2022-08-05 15:05:57 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.8906
2022-08-05 15:06:29 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.8636
2022-08-05 15:07:03 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.8151
2022-08-05 15:07:36 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.6598
2022-08-05 15:08:09 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.8022
2022-08-05 15:08:42 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.8810
2022-08-05 15:09:15 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.8021
2022-08-05 15:09:49 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.8767
2022-08-05 15:10:21 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.6659
2022-08-05 15:10:56 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.6716
2022-08-05 15:11:29 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 2.0281
2022-08-05 15:12:02 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.9236
2022-08-05 15:12:36 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.8572
2022-08-05 15:13:08 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.9176
2022-08-05 15:13:42 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.8464
2022-08-05 15:14:15 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.8695
2022-08-05 15:14:49 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.8472
2022-08-05 15:15:22 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.8783
2022-08-05 15:15:55 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.7013
2022-08-05 15:16:28 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.7091
2022-08-05 15:17:02 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.8673
2022-08-05 15:17:36 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.5402
2022-08-05 15:18:09 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.8528
2022-08-05 15:18:41 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0135
2022-08-05 15:19:14 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.7805
2022-08-05 15:19:48 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6107
2022-08-05 15:20:21 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.8131
2022-08-05 15:20:55 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8658
2022-08-05 15:21:28 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.7785
2022-08-05 15:22:01 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.8027
2022-08-05 15:22:34 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6972
2022-08-05 15:23:08 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.6991
2022-08-05 15:23:41 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.7733
2022-08-05 15:24:15 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.7902
2022-08-05 15:24:47 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 2.0054
2022-08-05 15:25:21 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.7658
2022-08-05 15:25:54 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.7908
2022-08-05 15:26:28 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.8207
2022-08-05 15:27:00 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.9640
2022-08-05 15:27:01 - train: epoch 015, train_loss: 1.8535
2022-08-05 15:28:14 - eval: epoch: 015, acc1: 62.764%, acc5: 85.286%, test_loss: 1.5444, per_image_load_time: 2.068ms, per_image_inference_time: 0.534ms
2022-08-05 15:28:14 - until epoch: 015, best_acc1: 62.764%
2022-08-05 15:28:14 - epoch 016 lr: 0.034548
2022-08-05 15:28:53 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.6723
2022-08-05 15:29:25 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.6203
2022-08-05 15:29:59 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.8694
2022-08-05 15:30:32 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 1.9040
2022-08-05 15:31:05 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.6063
2022-08-05 15:31:38 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.6497
2022-08-05 15:32:11 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.6463
2022-08-05 15:32:44 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.8384
2022-08-05 15:33:16 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.8313
2022-08-05 15:33:49 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.6021
2022-08-05 15:34:23 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.8801
2022-08-05 15:34:56 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.6299
2022-08-05 15:35:29 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.7225
2022-08-05 15:36:02 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.7389
2022-08-05 15:36:35 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.8132
2022-08-05 15:37:08 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.8196
2022-08-05 15:37:41 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 2.0071
2022-08-05 15:38:15 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.7857
2022-08-05 15:38:47 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.8631
2022-08-05 15:39:21 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.5464
2022-08-05 15:39:54 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8270
2022-08-05 15:40:27 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.8851
2022-08-05 15:41:00 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 2.0270
2022-08-05 15:41:33 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.7817
2022-08-05 15:42:06 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.8520
2022-08-05 15:42:39 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.8362
2022-08-05 15:43:13 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.7856
2022-08-05 15:43:45 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.6410
2022-08-05 15:44:19 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.9502
2022-08-05 15:44:53 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.9174
2022-08-05 15:45:26 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.8535
2022-08-05 15:45:58 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.9310
2022-08-05 15:46:32 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 2.0508
2022-08-05 15:47:05 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.7681
2022-08-05 15:47:38 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.8426
2022-08-05 15:48:12 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.6974
2022-08-05 15:48:45 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.7100
2022-08-05 15:49:18 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 1.8969
2022-08-05 15:49:51 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.6896
2022-08-05 15:50:24 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.8535
2022-08-05 15:50:57 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.8785
2022-08-05 15:51:31 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.7476
2022-08-05 15:52:04 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.6784
2022-08-05 15:52:38 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.5457
2022-08-05 15:53:10 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.9220
2022-08-05 15:53:44 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.6315
2022-08-05 15:54:17 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.8982
2022-08-05 15:54:50 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.6314
2022-08-05 15:55:23 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.6555
2022-08-05 15:55:56 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.7564
2022-08-05 15:55:57 - train: epoch 016, train_loss: 1.7890
2022-08-05 15:57:12 - eval: epoch: 016, acc1: 63.804%, acc5: 86.164%, test_loss: 1.4926, per_image_load_time: 2.297ms, per_image_inference_time: 0.548ms
2022-08-05 15:57:12 - until epoch: 016, best_acc1: 63.804%
2022-08-05 15:57:12 - epoch 017 lr: 0.028710
2022-08-05 15:57:51 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.7175
2022-08-05 15:58:23 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.7517
2022-08-05 15:58:56 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.7959
2022-08-05 15:59:29 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.5131
2022-08-05 16:00:02 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.9976
2022-08-05 16:00:35 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 2.0297
2022-08-05 16:01:08 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.5271
2022-08-05 16:01:41 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.7158
2022-08-05 16:02:14 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.5192
2022-08-05 16:02:47 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.6294
2022-08-05 16:03:21 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.8992
2022-08-05 16:03:54 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.6249
2022-08-05 16:04:26 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.8014
2022-08-05 16:05:00 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.7387
2022-08-05 16:05:33 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.5010
2022-08-05 16:06:06 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.5248
2022-08-05 16:06:40 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.6700
2022-08-05 16:07:12 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.5940
2022-08-05 16:07:46 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.6435
2022-08-05 16:08:19 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.7513
2022-08-05 16:08:53 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.6233
2022-08-05 16:09:25 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.5382
2022-08-05 16:09:59 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.8559
2022-08-05 16:10:32 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.6814
2022-08-05 16:11:05 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.9191
2022-08-05 16:11:39 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.8073
2022-08-05 16:12:12 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6065
2022-08-05 16:12:45 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.7124
2022-08-05 16:13:18 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.8085
2022-08-05 16:13:52 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.5233
2022-08-05 16:14:25 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.7453
2022-08-05 16:14:58 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.5048
2022-08-05 16:15:32 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.9314
2022-08-05 16:16:05 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.6760
2022-08-05 16:16:38 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.6493
2022-08-05 16:17:11 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.8930
2022-08-05 16:17:45 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.6384
2022-08-05 16:18:18 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.5391
2022-08-05 16:18:51 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.4776
2022-08-05 16:19:24 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.6938
2022-08-05 16:19:58 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.7245
2022-08-05 16:20:31 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.5090
2022-08-05 16:21:05 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.5185
2022-08-05 16:21:39 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8445
2022-08-05 16:22:12 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.7703
2022-08-05 16:22:45 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.6799
2022-08-05 16:23:18 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.8276
2022-08-05 16:23:51 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.8620
2022-08-05 16:24:24 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.7435
2022-08-05 16:24:57 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.5300
2022-08-05 16:24:58 - train: epoch 017, train_loss: 1.7211
2022-08-05 16:26:11 - eval: epoch: 017, acc1: 65.284%, acc5: 86.798%, test_loss: 1.4356, per_image_load_time: 2.264ms, per_image_inference_time: 0.549ms
2022-08-05 16:26:11 - until epoch: 017, best_acc1: 65.284%
2022-08-05 16:26:11 - epoch 018 lr: 0.023208
2022-08-05 16:26:50 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.6355
2022-08-05 16:27:22 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.6375
2022-08-05 16:27:56 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.6394
2022-08-05 16:28:29 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.8077
2022-08-05 16:29:01 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.6351
2022-08-05 16:29:34 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.7680
2022-08-05 16:30:07 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.6866
2022-08-05 16:30:40 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.6853
2022-08-05 16:31:13 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.6489
2022-08-05 16:31:46 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.6410
2022-08-05 16:32:19 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.7112
2022-08-05 16:32:53 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.6052
2022-08-05 16:33:26 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.9107
2022-08-05 16:33:59 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.6344
2022-08-05 16:34:33 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.8831
2022-08-05 16:35:06 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.4413
2022-08-05 16:35:40 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.6843
2022-08-05 16:36:12 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6611
2022-08-05 16:36:46 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.6493
2022-08-05 16:37:19 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.7896
2022-08-05 16:37:53 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.7764
2022-08-05 16:38:26 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.7506
2022-08-05 16:38:59 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6960
2022-08-05 16:39:33 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.5743
2022-08-05 16:40:06 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.5186
2022-08-05 16:40:39 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.6781
2022-08-05 16:41:12 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.6843
2022-08-05 16:41:46 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5419
2022-08-05 16:42:19 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.4645
2022-08-05 16:42:53 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.6536
2022-08-05 16:43:26 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 2.1510
2022-08-05 16:43:59 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.4803
2022-08-05 16:44:32 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.6916
2022-08-05 16:45:05 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.5436
2022-08-05 16:45:39 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.7547
2022-08-05 16:46:12 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.6705
2022-08-05 16:46:45 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.8326
2022-08-05 16:47:18 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.8142
2022-08-05 16:47:52 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.7348
2022-08-05 16:48:25 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.7385
2022-08-05 16:48:58 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.8827
2022-08-05 16:49:31 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.7306
2022-08-05 16:50:05 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.6124
2022-08-05 16:50:38 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.5738
2022-08-05 16:51:11 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.6812
2022-08-05 16:51:45 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.6563
2022-08-05 16:52:18 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.8169
2022-08-05 16:52:52 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6754
2022-08-05 16:53:25 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.6491
2022-08-05 16:53:57 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.6428
2022-08-05 16:53:59 - train: epoch 018, train_loss: 1.6493
2022-08-05 16:55:12 - eval: epoch: 018, acc1: 66.216%, acc5: 87.572%, test_loss: 1.3877, per_image_load_time: 2.184ms, per_image_inference_time: 0.535ms
2022-08-05 16:55:13 - until epoch: 018, best_acc1: 66.216%
2022-08-05 16:55:13 - epoch 019 lr: 0.018128
2022-08-05 16:55:52 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.3735
2022-08-05 16:56:24 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.5077
2022-08-05 16:56:57 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.8166
2022-08-05 16:57:30 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.5458
2022-08-05 16:58:03 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.3926
2022-08-05 16:58:36 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.7426
2022-08-05 16:59:09 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.5617
2022-08-05 16:59:43 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.5202
2022-08-05 17:00:16 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.4895
2022-08-05 17:00:49 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.6040
2022-08-05 17:01:22 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5610
2022-08-05 17:01:55 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.5457
2022-08-05 17:02:28 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.5612
2022-08-05 17:03:01 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.4592
2022-08-05 17:03:34 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.7070
2022-08-05 17:04:08 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.5733
2022-08-05 17:04:41 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.8451
2022-08-05 17:05:14 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.4411
2022-08-05 17:05:47 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.5985
2022-08-05 17:06:21 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.3842
2022-08-05 17:06:54 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.4405
2022-08-05 17:07:27 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.7110
2022-08-05 17:08:00 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.5884
2022-08-05 17:08:34 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.7410
2022-08-05 17:09:08 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.3895
2022-08-05 17:09:41 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.8080
2022-08-05 17:10:14 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.4095
2022-08-05 17:10:47 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.5449
2022-08-05 17:11:21 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.8452
2022-08-05 17:11:54 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.7570
2022-08-05 17:12:27 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.5491
2022-08-05 17:13:00 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.2796
2022-08-05 17:13:34 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.4685
2022-08-05 17:14:07 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.6102
2022-08-05 17:14:40 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.6788
2022-08-05 17:15:13 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.2019
2022-08-05 17:15:46 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.6445
2022-08-05 17:16:19 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.8980
2022-08-05 17:16:53 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.4313
2022-08-05 17:17:26 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.6176
2022-08-05 17:17:59 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.8131
2022-08-05 17:18:33 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.6697
2022-08-05 17:19:06 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.4006
2022-08-05 17:19:39 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.6305
2022-08-05 17:20:12 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.9812
2022-08-05 17:20:45 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.6014
2022-08-05 17:21:19 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.5247
2022-08-05 17:21:53 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.6068
2022-08-05 17:22:25 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.5740
2022-08-05 17:22:58 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.6058
2022-08-05 17:22:59 - train: epoch 019, train_loss: 1.5753
2022-08-05 17:24:13 - eval: epoch: 019, acc1: 67.526%, acc5: 88.398%, test_loss: 1.3257, per_image_load_time: 2.310ms, per_image_inference_time: 0.525ms
2022-08-05 17:24:13 - until epoch: 019, best_acc1: 67.526%
2022-08-05 17:24:13 - epoch 020 lr: 0.013551
2022-08-05 17:24:52 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.5937
2022-08-05 17:25:24 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.5779
2022-08-05 17:25:57 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.4199
2022-08-05 17:26:30 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.3389
2022-08-05 17:27:03 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.5539
2022-08-05 17:27:36 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.4231
2022-08-05 17:28:09 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.2137
2022-08-05 17:28:43 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.5659
2022-08-05 17:29:15 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.6120
2022-08-05 17:29:49 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.4295
2022-08-05 17:30:21 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.5449
2022-08-05 17:30:55 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.3965
2022-08-05 17:31:28 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.4749
2022-08-05 17:32:01 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.5618
2022-08-05 17:32:34 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.5487
2022-08-05 17:33:07 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.4726
2022-08-05 17:33:40 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.4557
2022-08-05 17:34:14 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.6558
2022-08-05 17:34:47 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.4529
2022-08-05 17:35:21 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.5424
2022-08-05 17:35:53 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.5886
2022-08-05 17:36:26 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.3773
2022-08-05 17:37:00 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.3870
2022-08-05 17:37:33 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.7278
2022-08-05 17:38:06 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.5061
2022-08-05 17:38:40 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.5688
2022-08-05 17:39:13 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.5267
2022-08-05 17:39:46 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.6153
2022-08-05 17:40:20 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.5063
2022-08-05 17:40:53 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.5610
2022-08-05 17:41:26 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.6947
2022-08-05 17:42:00 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.6321
2022-08-05 17:42:33 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3516
2022-08-05 17:43:06 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.4573
2022-08-05 17:43:40 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.2950
2022-08-05 17:44:13 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.4998
2022-08-05 17:44:47 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.3227
2022-08-05 17:45:19 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.4043
2022-08-05 17:45:52 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.6005
2022-08-05 17:46:26 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.4296
2022-08-05 17:46:59 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.5221
2022-08-05 17:47:33 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.5139
2022-08-05 17:48:05 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.4269
2022-08-05 17:48:39 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.5109
2022-08-05 17:49:12 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.6738
2022-08-05 17:49:46 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.5635
2022-08-05 17:50:19 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.5160
2022-08-05 17:50:52 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4879
2022-08-05 17:51:26 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.4959
2022-08-05 17:51:59 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.4558
2022-08-05 17:52:00 - train: epoch 020, train_loss: 1.5014
2022-08-05 17:53:14 - eval: epoch: 020, acc1: 68.954%, acc5: 89.118%, test_loss: 1.2657, per_image_load_time: 2.275ms, per_image_inference_time: 0.549ms
2022-08-05 17:53:14 - until epoch: 020, best_acc1: 68.954%
2022-08-05 17:53:14 - epoch 021 lr: 0.009548
2022-08-05 17:53:52 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.4328
2022-08-05 17:54:25 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.5453
2022-08-05 17:54:58 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.2489
2022-08-05 17:55:32 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.5830
2022-08-05 17:56:05 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.3757
2022-08-05 17:56:38 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.4209
2022-08-05 17:57:10 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.2475
2022-08-05 17:57:43 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.4079
2022-08-05 17:58:17 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.4493
2022-08-05 17:58:49 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.1634
2022-08-05 17:59:23 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.3271
2022-08-05 17:59:56 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.5175
2022-08-05 18:00:29 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.1973
2022-08-05 18:01:02 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.2887
2022-08-05 18:01:35 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.4176
2022-08-05 18:02:08 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3684
2022-08-05 18:02:42 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5462
2022-08-05 18:03:15 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.3729
2022-08-05 18:03:48 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.4948
2022-08-05 18:04:21 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.4917
2022-08-05 18:04:54 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3979
2022-08-05 18:05:28 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.4595
2022-08-05 18:06:02 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.5082
2022-08-05 18:06:35 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.2901
2022-08-05 18:07:08 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.4874
2022-08-05 18:07:42 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.4574
2022-08-05 18:08:15 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.3930
2022-08-05 18:08:48 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.5387
2022-08-05 18:09:21 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.4703
2022-08-05 18:09:54 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.4115
2022-08-05 18:10:28 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.4860
2022-08-05 18:11:01 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.1759
2022-08-05 18:11:34 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.6043
2022-08-05 18:12:08 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.4405
2022-08-05 18:12:41 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.5206
2022-08-05 18:13:14 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.3340
2022-08-05 18:13:48 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.3704
2022-08-05 18:14:22 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.5419
2022-08-05 18:14:55 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.4271
2022-08-05 18:15:28 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.5760
2022-08-05 18:16:02 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.3481
2022-08-05 18:16:36 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.4324
2022-08-05 18:17:09 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3472
2022-08-05 18:17:42 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.3815
2022-08-05 18:18:15 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.3996
2022-08-05 18:18:49 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.2806
2022-08-05 18:19:22 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.6380
2022-08-05 18:19:55 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.5749
2022-08-05 18:20:29 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.3202
2022-08-05 18:21:01 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.3484
2022-08-05 18:21:02 - train: epoch 021, train_loss: 1.4281
2022-08-05 18:22:15 - eval: epoch: 021, acc1: 69.806%, acc5: 89.716%, test_loss: 1.2225, per_image_load_time: 2.255ms, per_image_inference_time: 0.546ms
2022-08-05 18:22:16 - until epoch: 021, best_acc1: 69.806%
2022-08-05 18:22:16 - epoch 022 lr: 0.006184
2022-08-05 18:22:54 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.0896
2022-08-05 18:23:27 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.4959
2022-08-05 18:24:00 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.2634
2022-08-05 18:24:33 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.3956
2022-08-05 18:25:05 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3087
2022-08-05 18:25:38 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.4316
2022-08-05 18:26:11 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4544
2022-08-05 18:26:44 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.3132
2022-08-05 18:27:17 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.3902
2022-08-05 18:27:50 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.4562
2022-08-05 18:28:23 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3143
2022-08-05 18:28:56 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.1182
2022-08-05 18:29:29 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.3294
2022-08-05 18:30:02 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.2111
2022-08-05 18:30:35 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.3268
2022-08-05 18:31:08 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.2985
2022-08-05 18:31:42 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.5632
2022-08-05 18:32:15 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.4658
2022-08-05 18:32:48 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.2709
2022-08-05 18:33:21 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4530
2022-08-05 18:33:54 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.4784
2022-08-05 18:34:27 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.0242
2022-08-05 18:35:00 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.3184
2022-08-05 18:35:34 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.5345
2022-08-05 18:36:07 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.2694
2022-08-05 18:36:40 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.2105
2022-08-05 18:37:14 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.4046
2022-08-05 18:37:47 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.4480
2022-08-05 18:38:20 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.1341
2022-08-05 18:38:53 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.4414
2022-08-05 18:39:27 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.4852
2022-08-05 18:40:00 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.3104
2022-08-05 18:40:33 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.3585
2022-08-05 18:41:06 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.3647
2022-08-05 18:41:39 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.2815
2022-08-05 18:42:12 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.4758
2022-08-05 18:42:45 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.4302
2022-08-05 18:43:19 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.6322
2022-08-05 18:43:51 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.4411
2022-08-05 18:44:25 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.5644
2022-08-05 18:44:58 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.2708
2022-08-05 18:45:32 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.3416
2022-08-05 18:46:06 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.5793
2022-08-05 18:46:39 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3304
2022-08-05 18:47:13 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.2471
2022-08-05 18:47:45 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.5178
2022-08-05 18:48:18 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.4297
2022-08-05 18:48:52 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.2358
2022-08-05 18:49:25 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.5023
2022-08-05 18:49:57 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.3191
2022-08-05 18:49:58 - train: epoch 022, train_loss: 1.3625
2022-08-05 18:51:11 - eval: epoch: 022, acc1: 71.092%, acc5: 90.168%, test_loss: 1.1743, per_image_load_time: 2.254ms, per_image_inference_time: 0.534ms
2022-08-05 18:51:11 - until epoch: 022, best_acc1: 71.092%
2022-08-05 18:51:11 - epoch 023 lr: 0.003511
2022-08-05 18:51:50 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.3095
2022-08-05 18:52:23 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.3404
2022-08-05 18:52:56 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.3178
2022-08-05 18:53:29 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.4949
2022-08-05 18:54:02 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.5571
2022-08-05 18:54:35 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.2945
2022-08-05 18:55:09 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.3652
2022-08-05 18:55:41 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3195
2022-08-05 18:56:15 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.3998
2022-08-05 18:56:47 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.2597
2022-08-05 18:57:21 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.3428
2022-08-05 18:57:54 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.3185
2022-08-05 18:58:28 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.3057
2022-08-05 18:59:01 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.4211
2022-08-05 18:59:34 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.1270
2022-08-05 19:00:07 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.4259
2022-08-05 19:00:40 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.4190
2022-08-05 19:01:13 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.2394
2022-08-05 19:01:46 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.2643
2022-08-05 19:02:20 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.2198
2022-08-05 19:02:53 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.3460
2022-08-05 19:03:26 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.1974
2022-08-05 19:03:59 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.2050
2022-08-05 19:04:32 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.2677
2022-08-05 19:05:05 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.2552
2022-08-05 19:05:39 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.4610
2022-08-05 19:06:11 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.2765
2022-08-05 19:06:45 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.3747
2022-08-05 19:07:17 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.3486
2022-08-05 19:07:51 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.3385
2022-08-05 19:08:24 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.3712
2022-08-05 19:08:57 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.3061
2022-08-05 19:09:30 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.2638
2022-08-05 19:10:02 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.4115
2022-08-05 19:10:37 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.3144
2022-08-05 19:11:10 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.3050
2022-08-05 19:11:43 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.2205
2022-08-05 19:12:15 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.3099
2022-08-05 19:12:49 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3645
2022-08-05 19:13:22 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2556
2022-08-05 19:13:55 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.1988
2022-08-05 19:14:29 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.2214
2022-08-05 19:15:03 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.0439
2022-08-05 19:15:36 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.4272
2022-08-05 19:16:09 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.2022
2022-08-05 19:16:43 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.2292
2022-08-05 19:17:15 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.2925
2022-08-05 19:17:49 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.1936
2022-08-05 19:18:22 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.2311
2022-08-05 19:18:55 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.3270
2022-08-05 19:18:56 - train: epoch 023, train_loss: 1.3070
2022-08-05 19:20:10 - eval: epoch: 023, acc1: 71.892%, acc5: 90.570%, test_loss: 1.1398, per_image_load_time: 2.288ms, per_image_inference_time: 0.530ms
2022-08-05 19:20:10 - until epoch: 023, best_acc1: 71.892%
2022-08-05 19:20:10 - epoch 024 lr: 0.001571
2022-08-05 19:20:48 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.3522
2022-08-05 19:21:22 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.2280
2022-08-05 19:21:55 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.1482
2022-08-05 19:22:28 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.3369
2022-08-05 19:23:01 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.4329
2022-08-05 19:23:34 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.2607
2022-08-05 19:24:07 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.1879
2022-08-05 19:24:40 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.2289
2022-08-05 19:25:13 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.3274
2022-08-05 19:25:45 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.1333
2022-08-05 19:26:18 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.2673
2022-08-05 19:26:51 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.3706
2022-08-05 19:27:24 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4661
2022-08-05 19:27:58 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.2557
2022-08-05 19:28:30 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.4703
2022-08-05 19:29:03 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.1999
2022-08-05 19:29:36 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.2682
2022-08-05 19:30:10 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.7118
2022-08-05 19:30:42 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.2010
2022-08-05 19:31:15 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.2922
2022-08-05 19:31:49 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.2718
2022-08-05 19:32:22 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.1686
2022-08-05 19:32:55 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.2406
2022-08-05 19:33:28 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.2493
2022-08-05 19:34:01 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.3133
2022-08-05 19:34:34 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.5060
2022-08-05 19:35:06 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.4130
2022-08-05 19:35:40 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.2197
2022-08-05 19:36:14 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.2867
2022-08-05 19:36:47 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.1612
2022-08-05 19:37:20 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.2122
2022-08-05 19:37:53 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.4179
2022-08-05 19:38:26 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.2285
2022-08-05 19:38:59 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.2403
2022-08-05 19:39:32 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.3345
2022-08-05 19:40:06 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.3043
2022-08-05 19:40:39 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.2463
2022-08-05 19:41:12 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.5709
2022-08-05 19:41:46 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.3982
2022-08-05 19:42:19 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.3017
2022-08-05 19:42:53 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.0825
2022-08-05 19:43:25 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.1616
2022-08-05 19:43:59 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.2750
2022-08-05 19:44:32 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.1940
2022-08-05 19:45:05 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.1684
2022-08-05 19:45:39 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.2552
2022-08-05 19:46:13 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.2257
2022-08-05 19:46:46 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.0892
2022-08-05 19:47:20 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.3444
2022-08-05 19:47:51 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.1828
2022-08-05 19:47:53 - train: epoch 024, train_loss: 1.2660
2022-08-05 19:49:06 - eval: epoch: 024, acc1: 72.370%, acc5: 90.718%, test_loss: 1.1249, per_image_load_time: 2.295ms, per_image_inference_time: 0.535ms
2022-08-05 19:49:06 - until epoch: 024, best_acc1: 72.370%
2022-08-05 19:49:06 - epoch 025 lr: 0.000394
2022-08-05 19:49:45 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.0459
2022-08-05 19:50:18 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.0458
2022-08-05 19:50:51 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.2476
2022-08-05 19:51:24 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.1485
2022-08-05 19:51:57 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.1093
2022-08-05 19:52:30 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3724
2022-08-05 19:53:04 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.2584
2022-08-05 19:53:37 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.2501
2022-08-05 19:54:10 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.1239
2022-08-05 19:54:43 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.2036
2022-08-05 19:55:16 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.2938
2022-08-05 19:55:50 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.2556
2022-08-05 19:56:23 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3058
2022-08-05 19:56:56 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.3658
2022-08-05 19:57:29 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.3460
2022-08-05 19:58:02 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.0491
2022-08-05 19:58:34 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.1915
2022-08-05 19:59:07 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.1112
2022-08-05 19:59:41 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.1679
2022-08-05 20:00:13 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.1878
2022-08-05 20:00:47 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.1189
2022-08-05 20:01:20 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.2893
2022-08-05 20:01:54 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.2001
2022-08-05 20:02:27 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.3184
2022-08-05 20:03:00 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.1031
2022-08-05 20:03:33 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.2356
2022-08-05 20:04:07 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.3056
2022-08-05 20:04:40 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.3529
2022-08-05 20:05:13 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.2305
2022-08-05 20:05:47 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.2893
2022-08-05 20:06:20 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.2145
2022-08-05 20:06:54 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.2355
2022-08-05 20:07:27 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.1680
2022-08-05 20:08:00 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.1710
2022-08-05 20:08:33 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1700
2022-08-05 20:09:06 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.2644
2022-08-05 20:09:40 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.2304
2022-08-05 20:10:13 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.5360
2022-08-05 20:10:46 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.3848
2022-08-05 20:11:19 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.2722
2022-08-05 20:11:52 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.5084
2022-08-05 20:12:26 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.3959
2022-08-05 20:12:59 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.0812
2022-08-05 20:13:32 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.0494
2022-08-05 20:14:05 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.1759
2022-08-05 20:14:39 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.1560
2022-08-05 20:15:11 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.1859
2022-08-05 20:15:46 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 0.9873
2022-08-05 20:16:18 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.4400
2022-08-05 20:16:51 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.3510
2022-08-05 20:16:52 - train: epoch 025, train_loss: 1.2474
2022-08-05 20:18:05 - eval: epoch: 025, acc1: 72.322%, acc5: 90.742%, test_loss: 1.1226, per_image_load_time: 2.242ms, per_image_inference_time: 0.555ms
2022-08-05 20:18:06 - until epoch: 025, best_acc1: 72.370%
2022-08-05 20:18:06 - train done. train time: 12.070 hours, best_acc1: 72.370%
2022-08-09 22:49:20 - net_idx: 3
2022-08-09 22:49:20 - net_config: {'stem_width': 64, 'depth': 12, 'w_0': 32, 'w_a': 27.264071534824073, 'w_m': 2.097389031593275}
2022-08-09 22:49:20 - num_classes: 1000
2022-08-09 22:49:20 - input_image_size: 224
2022-08-09 22:49:20 - scale: 1.1428571428571428
2022-08-09 22:49:20 - seed: 0
2022-08-09 22:49:20 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-09 22:49:20 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-09 22:49:20 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce2b0>
2022-08-09 22:49:20 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce6d0>
2022-08-09 22:49:20 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce700>
2022-08-09 22:49:20 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce760>
2022-08-09 22:49:20 - batch_size: 256
2022-08-09 22:49:20 - num_workers: 16
2022-08-09 22:49:20 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-09 22:49:20 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-09 22:49:20 - epochs: 25
2022-08-09 22:49:20 - print_interval: 100
2022-08-09 22:49:20 - accumulation_steps: 1
2022-08-09 22:49:20 - sync_bn: False
2022-08-09 22:49:20 - apex: True
2022-08-09 22:49:20 - use_ema_model: False
2022-08-09 22:49:20 - ema_model_decay: 0.9999
2022-08-09 22:49:20 - log_dir: ./log
2022-08-09 22:49:20 - checkpoint_dir: ./checkpoints
2022-08-09 22:49:20 - gpus_type: NVIDIA RTX A5000
2022-08-09 22:49:20 - gpus_num: 2
2022-08-09 22:49:20 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5dddaeb570>
2022-08-09 22:49:20 - ema_model: None
2022-08-09 22:49:20 - --------------------parameters--------------------
2022-08-09 22:49:20 - name: conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: fc.weight, grad: True
2022-08-09 22:49:20 - name: fc.bias, grad: True
2022-08-09 22:49:20 - --------------------buffers--------------------
2022-08-09 22:49:20 - name: conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - -----------no weight decay layers--------------
2022-08-09 22:49:20 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - -------------weight decay layers---------------
2022-08-09 22:49:20 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - resuming model from ./checkpoints/3/latest.pth. resume_epoch: 025, used_time: 12.070 hours, best_acc1: 72.370%, test_loss: 1.1226, lr: 0.000000
2022-08-09 22:49:20 - train done. train time: 12.070 hours, best_acc1: 72.370%
