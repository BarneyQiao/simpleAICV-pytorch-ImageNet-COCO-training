2022-08-04 20:11:21 - net_idx: 2
2022-08-04 20:11:21 - net_config: {'stem_width': 64, 'depth': 15, 'w_0': 32, 'w_a': 16.53264444950164, 'w_m': 1.9698967349417775}
2022-08-04 20:11:21 - num_classes: 1000
2022-08-04 20:11:21 - input_image_size: 224
2022-08-04 20:11:21 - scale: 1.1428571428571428
2022-08-04 20:11:21 - seed: 0
2022-08-04 20:11:21 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-04 20:11:21 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-04 20:11:21 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fadba72ea00>
2022-08-04 20:11:21 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fadb9da1af0>
2022-08-04 20:11:21 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fadb9da1b20>
2022-08-04 20:11:21 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fadb9da1b80>
2022-08-04 20:11:21 - batch_size: 256
2022-08-04 20:11:21 - num_workers: 16
2022-08-04 20:11:21 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-04 20:11:21 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-04 20:11:21 - epochs: 25
2022-08-04 20:11:21 - print_interval: 100
2022-08-04 20:11:21 - accumulation_steps: 1
2022-08-04 20:11:21 - sync_bn: False
2022-08-04 20:11:21 - apex: True
2022-08-04 20:11:21 - use_ema_model: False
2022-08-04 20:11:21 - ema_model_decay: 0.9999
2022-08-04 20:11:21 - log_dir: ./log
2022-08-04 20:11:21 - checkpoint_dir: ./checkpoints
2022-08-04 20:11:21 - gpus_type: NVIDIA RTX A5000
2022-08-04 20:11:21 - gpus_num: 2
2022-08-04 20:11:21 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fad965d05f0>
2022-08-04 20:11:21 - ema_model: None
2022-08-04 20:11:21 - --------------------parameters--------------------
2022-08-04 20:11:21 - name: conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-04 20:11:21 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-04 20:11:21 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-04 20:11:21 - name: fc.weight, grad: True
2022-08-04 20:11:21 - name: fc.bias, grad: True
2022-08-04 20:11:21 - --------------------buffers--------------------
2022-08-04 20:11:21 - name: conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-04 20:11:21 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-04 20:11:21 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 20:11:21 - -----------no weight decay layers--------------
2022-08-04 20:11:21 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 20:11:21 - -------------weight decay layers---------------
2022-08-04 20:11:21 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-04 20:11:21 - epoch 001 lr: 0.100000
2022-08-04 20:12:00 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8892
2022-08-04 20:12:32 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.8926
2022-08-04 20:13:05 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8660
2022-08-04 20:13:37 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.7588
2022-08-04 20:14:10 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.7149
2022-08-04 20:14:43 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.5343
2022-08-04 20:15:16 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.6288
2022-08-04 20:15:49 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.5358
2022-08-04 20:16:22 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.4278
2022-08-04 20:16:55 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.3978
2022-08-04 20:17:28 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.3356
2022-08-04 20:18:01 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.1369
2022-08-04 20:18:34 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.1646
2022-08-04 20:19:07 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.1339
2022-08-04 20:19:41 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 5.9665
2022-08-04 20:20:14 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.0698
2022-08-04 20:20:47 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.8831
2022-08-04 20:21:19 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.7429
2022-08-04 20:21:52 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.6630
2022-08-04 20:22:25 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.5999
2022-08-04 20:22:58 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.5930
2022-08-04 20:23:31 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.4399
2022-08-04 20:24:05 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.3691
2022-08-04 20:24:38 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.3993
2022-08-04 20:25:11 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.4900
2022-08-04 20:25:44 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.4916
2022-08-04 20:26:17 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.2781
2022-08-04 20:26:50 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.3172
2022-08-04 20:27:23 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 4.9498
2022-08-04 20:27:56 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.2773
2022-08-04 20:28:29 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.1295
2022-08-04 20:29:02 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.1059
2022-08-04 20:29:35 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.9361
2022-08-04 20:30:08 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.8658
2022-08-04 20:30:41 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.8466
2022-08-04 20:31:15 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.8602
2022-08-04 20:31:48 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.8655
2022-08-04 20:32:21 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.7546
2022-08-04 20:32:54 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.7254
2022-08-04 20:33:27 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.6953
2022-08-04 20:34:00 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.7449
2022-08-04 20:34:33 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.7099
2022-08-04 20:35:06 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.5664
2022-08-04 20:35:39 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.4015
2022-08-04 20:36:12 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.4790
2022-08-04 20:36:46 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.8010
2022-08-04 20:37:18 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.4072
2022-08-04 20:37:52 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.5312
2022-08-04 20:38:25 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.5005
2022-08-04 20:38:57 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.3329
2022-08-04 20:38:59 - train: epoch 001, train_loss: 5.4811
2022-08-04 20:40:12 - eval: epoch: 001, acc1: 15.828%, acc5: 36.276%, test_loss: 4.3494, per_image_load_time: 2.308ms, per_image_inference_time: 0.543ms
2022-08-04 20:40:12 - until epoch: 001, best_acc1: 15.828%
2022-08-04 20:40:12 - epoch 002 lr: 0.099606
2022-08-04 20:40:51 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.1630
2022-08-04 20:41:24 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.0807
2022-08-04 20:41:57 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.4016
2022-08-04 20:42:30 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.2994
2022-08-04 20:43:03 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.1410
2022-08-04 20:43:35 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.0483
2022-08-04 20:44:08 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.3054
2022-08-04 20:44:41 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 3.9588
2022-08-04 20:45:15 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.8887
2022-08-04 20:45:47 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.1174
2022-08-04 20:46:21 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 3.9730
2022-08-04 20:46:54 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.9530
2022-08-04 20:47:27 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.9376
2022-08-04 20:48:00 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.0482
2022-08-04 20:48:34 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 4.0026
2022-08-04 20:49:06 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.8949
2022-08-04 20:49:40 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.8875
2022-08-04 20:50:13 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.9802
2022-08-04 20:50:46 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.8603
2022-08-04 20:51:20 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.5190
2022-08-04 20:51:53 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.9450
2022-08-04 20:52:26 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.7588
2022-08-04 20:53:00 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.8433
2022-08-04 20:53:33 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.5879
2022-08-04 20:54:06 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.5736
2022-08-04 20:54:40 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.7012
2022-08-04 20:55:12 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.9175
2022-08-04 20:55:46 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.8043
2022-08-04 20:56:20 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.6765
2022-08-04 20:56:53 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.4931
2022-08-04 20:57:27 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.5967
2022-08-04 20:58:00 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.4868
2022-08-04 20:58:34 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.7445
2022-08-04 20:59:07 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.4844
2022-08-04 20:59:41 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.3618
2022-08-04 21:00:14 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.5447
2022-08-04 21:00:47 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.7022
2022-08-04 21:01:20 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.3525
2022-08-04 21:01:54 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.2692
2022-08-04 21:02:28 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.3769
2022-08-04 21:03:01 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.5364
2022-08-04 21:03:34 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.5040
2022-08-04 21:04:08 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.4237
2022-08-04 21:04:41 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.3552
2022-08-04 21:05:15 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.1478
2022-08-04 21:05:47 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.2860
2022-08-04 21:06:21 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.2912
2022-08-04 21:06:55 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.4145
2022-08-04 21:07:28 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.4799
2022-08-04 21:08:00 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.3174
2022-08-04 21:08:01 - train: epoch 002, train_loss: 3.7392
2022-08-04 21:09:15 - eval: epoch: 002, acc1: 30.990%, acc5: 57.146%, test_loss: 3.2976, per_image_load_time: 1.612ms, per_image_inference_time: 0.559ms
2022-08-04 21:09:16 - until epoch: 002, best_acc1: 30.990%
2022-08-04 21:09:16 - epoch 003 lr: 0.098429
2022-08-04 21:09:54 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.3542
2022-08-04 21:10:27 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.3992
2022-08-04 21:10:59 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.4153
2022-08-04 21:11:33 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.1860
2022-08-04 21:12:06 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.2890
2022-08-04 21:12:39 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.0073
2022-08-04 21:13:12 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.5667
2022-08-04 21:13:45 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.3195
2022-08-04 21:14:19 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.1503
2022-08-04 21:14:52 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.3841
2022-08-04 21:15:25 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.0810
2022-08-04 21:15:58 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.1763
2022-08-04 21:16:31 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 2.9853
2022-08-04 21:17:04 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 2.9962
2022-08-04 21:17:37 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.3545
2022-08-04 21:18:10 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.1206
2022-08-04 21:18:43 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 2.9701
2022-08-04 21:19:17 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.0141
2022-08-04 21:19:50 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.1557
2022-08-04 21:20:22 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.1555
2022-08-04 21:20:56 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.0338
2022-08-04 21:21:29 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.4626
2022-08-04 21:22:02 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 3.1048
2022-08-04 21:22:35 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 2.9598
2022-08-04 21:23:09 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.2664
2022-08-04 21:23:42 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.2462
2022-08-04 21:24:15 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.4332
2022-08-04 21:24:48 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 2.9737
2022-08-04 21:25:21 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.8546
2022-08-04 21:25:55 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.1681
2022-08-04 21:26:29 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.2374
2022-08-04 21:27:01 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.0417
2022-08-04 21:27:35 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0437
2022-08-04 21:28:08 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.2390
2022-08-04 21:28:41 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 3.0072
2022-08-04 21:29:14 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 3.1257
2022-08-04 21:29:48 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.1269
2022-08-04 21:30:20 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.1917
2022-08-04 21:30:54 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.2294
2022-08-04 21:31:27 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7342
2022-08-04 21:32:01 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 3.1603
2022-08-04 21:32:34 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 2.8582
2022-08-04 21:33:07 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.8204
2022-08-04 21:33:41 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.9513
2022-08-04 21:34:13 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.0119
2022-08-04 21:34:47 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.9354
2022-08-04 21:35:19 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 2.9624
2022-08-04 21:35:53 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.1927
2022-08-04 21:36:26 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 2.9346
2022-08-04 21:36:59 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.8733
2022-08-04 21:37:00 - train: epoch 003, train_loss: 3.1085
2022-08-04 21:38:12 - eval: epoch: 003, acc1: 37.288%, acc5: 64.048%, test_loss: 2.9385, per_image_load_time: 1.654ms, per_image_inference_time: 0.582ms
2022-08-04 21:38:12 - until epoch: 003, best_acc1: 37.288%
2022-08-04 21:38:12 - epoch 004 lr: 0.096488
2022-08-04 21:38:50 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.9558
2022-08-04 21:39:23 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.5567
2022-08-04 21:39:56 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.9431
2022-08-04 21:40:28 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.6923
2022-08-04 21:41:01 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.8183
2022-08-04 21:41:33 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.1342
2022-08-04 21:42:06 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.9309
2022-08-04 21:42:38 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.7989
2022-08-04 21:43:11 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.8220
2022-08-04 21:43:44 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 3.0131
2022-08-04 21:44:17 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 2.8994
2022-08-04 21:44:50 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.7732
2022-08-04 21:45:23 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.6625
2022-08-04 21:45:56 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.9252
2022-08-04 21:46:29 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.8753
2022-08-04 21:47:02 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.8699
2022-08-04 21:47:35 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.8209
2022-08-04 21:48:09 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 3.0263
2022-08-04 21:48:42 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 3.0426
2022-08-04 21:49:15 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.8998
2022-08-04 21:49:48 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.9539
2022-08-04 21:50:21 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.8468
2022-08-04 21:50:54 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.5507
2022-08-04 21:51:27 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.4695
2022-08-04 21:52:00 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.7513
2022-08-04 21:52:33 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.6897
2022-08-04 21:53:07 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.5391
2022-08-04 21:53:41 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8108
2022-08-04 21:54:13 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.7912
2022-08-04 21:54:47 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.6102
2022-08-04 21:55:20 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.6108
2022-08-04 21:55:53 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8369
2022-08-04 21:56:27 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.7163
2022-08-04 21:57:00 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.8812
2022-08-04 21:57:33 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.9205
2022-08-04 21:58:06 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.8894
2022-08-04 21:58:39 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.7573
2022-08-04 21:59:13 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.5258
2022-08-04 21:59:46 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.7013
2022-08-04 22:00:19 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.3760
2022-08-04 22:00:52 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.7832
2022-08-04 22:01:26 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.5814
2022-08-04 22:01:59 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.6060
2022-08-04 22:02:32 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.4461
2022-08-04 22:03:06 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.3442
2022-08-04 22:03:39 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.7061
2022-08-04 22:04:12 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.6318
2022-08-04 22:04:45 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.7652
2022-08-04 22:05:19 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.6280
2022-08-04 22:05:51 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.6641
2022-08-04 22:05:52 - train: epoch 004, train_loss: 2.7980
2022-08-04 22:07:05 - eval: epoch: 004, acc1: 45.304%, acc5: 71.354%, test_loss: 2.5316, per_image_load_time: 2.223ms, per_image_inference_time: 0.554ms
2022-08-04 22:07:05 - until epoch: 004, best_acc1: 45.304%
2022-08-04 22:07:05 - epoch 005 lr: 0.093815
2022-08-04 22:07:43 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.7313
2022-08-04 22:08:16 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.7492
2022-08-04 22:08:48 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.9093
2022-08-04 22:09:21 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.5791
2022-08-04 22:09:53 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.3536
2022-08-04 22:10:26 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.7070
2022-08-04 22:11:00 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.7051
2022-08-04 22:11:33 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.7811
2022-08-04 22:12:06 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.6109
2022-08-04 22:12:39 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.6197
2022-08-04 22:13:12 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.8789
2022-08-04 22:13:44 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.5409
2022-08-04 22:14:18 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.5226
2022-08-04 22:14:51 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.6891
2022-08-04 22:15:24 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.3587
2022-08-04 22:15:57 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.5737
2022-08-04 22:16:29 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.4514
2022-08-04 22:17:02 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.5726
2022-08-04 22:17:36 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.6386
2022-08-04 22:18:09 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.5691
2022-08-04 22:18:42 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.4433
2022-08-04 22:19:15 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.7018
2022-08-04 22:19:48 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.2752
2022-08-04 22:20:21 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.4326
2022-08-04 22:20:55 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.5168
2022-08-04 22:21:28 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.8704
2022-08-04 22:22:01 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.6601
2022-08-04 22:22:34 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.6483
2022-08-04 22:23:08 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.5164
2022-08-04 22:23:41 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.6136
2022-08-04 22:24:14 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.6280
2022-08-04 22:24:47 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.6711
2022-08-04 22:25:21 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.5300
2022-08-04 22:25:54 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.5932
2022-08-04 22:26:27 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.6507
2022-08-04 22:27:00 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6524
2022-08-04 22:27:33 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.6659
2022-08-04 22:28:07 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.4331
2022-08-04 22:28:40 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 3.0087
2022-08-04 22:29:13 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.5955
2022-08-04 22:29:47 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.5509
2022-08-04 22:30:19 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.7894
2022-08-04 22:30:53 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.5862
2022-08-04 22:31:26 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.6179
2022-08-04 22:32:00 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.5925
2022-08-04 22:32:33 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.5159
2022-08-04 22:33:06 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.2964
2022-08-04 22:33:39 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.4346
2022-08-04 22:34:12 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.8279
2022-08-04 22:34:44 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.4560
2022-08-04 22:34:45 - train: epoch 005, train_loss: 2.6096
2022-08-04 22:35:58 - eval: epoch: 005, acc1: 47.058%, acc5: 73.416%, test_loss: 2.3621, per_image_load_time: 2.032ms, per_image_inference_time: 0.577ms
2022-08-04 22:35:58 - until epoch: 005, best_acc1: 47.058%
2022-08-04 22:35:58 - epoch 006 lr: 0.090450
2022-08-04 22:36:37 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.5275
2022-08-04 22:37:09 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.6001
2022-08-04 22:37:41 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.5056
2022-08-04 22:38:13 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.5864
2022-08-04 22:38:46 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.4258
2022-08-04 22:39:19 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.4844
2022-08-04 22:39:51 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.6192
2022-08-04 22:40:25 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.5875
2022-08-04 22:40:58 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3404
2022-08-04 22:41:31 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.4216
2022-08-04 22:42:04 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.4036
2022-08-04 22:42:37 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.7028
2022-08-04 22:43:10 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.4114
2022-08-04 22:43:42 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.5223
2022-08-04 22:44:16 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.7414
2022-08-04 22:44:50 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.1699
2022-08-04 22:45:22 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.6629
2022-08-04 22:45:55 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.5361
2022-08-04 22:46:29 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.4085
2022-08-04 22:47:02 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.6496
2022-08-04 22:47:35 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.3675
2022-08-04 22:48:08 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.3092
2022-08-04 22:48:41 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.2914
2022-08-04 22:49:14 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.4801
2022-08-04 22:49:48 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.5219
2022-08-04 22:50:21 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.4866
2022-08-04 22:50:54 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.5282
2022-08-04 22:51:27 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.2506
2022-08-04 22:52:01 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.4587
2022-08-04 22:52:34 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.4509
2022-08-04 22:53:07 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.3168
2022-08-04 22:53:40 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.5590
2022-08-04 22:54:13 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.5934
2022-08-04 22:54:46 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.5650
2022-08-04 22:55:19 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.6803
2022-08-04 22:55:53 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.4924
2022-08-04 22:56:25 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.4651
2022-08-04 22:56:58 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.1958
2022-08-04 22:57:31 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.4581
2022-08-04 22:58:04 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.5150
2022-08-04 22:58:37 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.6241
2022-08-04 22:59:10 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.3596
2022-08-04 22:59:42 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.5155
2022-08-04 23:00:15 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.5326
2022-08-04 23:00:48 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.4681
2022-08-04 23:01:21 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.4320
2022-08-04 23:01:55 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4750
2022-08-04 23:02:27 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.5313
2022-08-04 23:03:00 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.4937
2022-08-04 23:03:33 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.3490
2022-08-04 23:03:34 - train: epoch 006, train_loss: 2.4816
2022-08-04 23:04:47 - eval: epoch: 006, acc1: 48.742%, acc5: 74.462%, test_loss: 2.2999, per_image_load_time: 2.303ms, per_image_inference_time: 0.553ms
2022-08-04 23:04:47 - until epoch: 006, best_acc1: 48.742%
2022-08-04 23:04:47 - epoch 007 lr: 0.086448
2022-08-04 23:05:26 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.3103
2022-08-04 23:05:59 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.6560
2022-08-04 23:06:32 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.7027
2022-08-04 23:07:04 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.3531
2022-08-04 23:07:37 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.2757
2022-08-04 23:08:10 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.5731
2022-08-04 23:08:43 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.4978
2022-08-04 23:09:16 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.3384
2022-08-04 23:09:49 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.4570
2022-08-04 23:10:22 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.4928
2022-08-04 23:10:55 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.3122
2022-08-04 23:11:29 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.1948
2022-08-04 23:12:02 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.1927
2022-08-04 23:12:35 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.4550
2022-08-04 23:13:08 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.5676
2022-08-04 23:13:40 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.2795
2022-08-04 23:14:14 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.3562
2022-08-04 23:14:47 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.2804
2022-08-04 23:15:21 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.4587
2022-08-04 23:15:54 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.2326
2022-08-04 23:16:26 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.4930
2022-08-04 23:16:59 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.2607
2022-08-04 23:17:33 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.3695
2022-08-04 23:18:06 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.5651
2022-08-04 23:18:39 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.3758
2022-08-04 23:19:12 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.3088
2022-08-04 23:19:45 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.2492
2022-08-04 23:20:18 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.2684
2022-08-04 23:20:51 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.2610
2022-08-04 23:21:24 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.4446
2022-08-04 23:21:58 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.4060
2022-08-04 23:22:30 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.4727
2022-08-04 23:23:03 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.6123
2022-08-04 23:23:36 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.3608
2022-08-04 23:24:09 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.3820
2022-08-04 23:24:43 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.3990
2022-08-04 23:25:16 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.4143
2022-08-04 23:25:48 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.6064
2022-08-04 23:26:21 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.4379
2022-08-04 23:26:54 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.3392
2022-08-04 23:27:27 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.3661
2022-08-04 23:28:00 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2365
2022-08-04 23:28:33 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.4724
2022-08-04 23:29:05 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.1460
2022-08-04 23:29:39 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.3748
2022-08-04 23:30:12 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.3842
2022-08-04 23:30:46 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.2360
2022-08-04 23:31:19 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.5655
2022-08-04 23:31:53 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.2814
2022-08-04 23:32:25 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.2743
2022-08-04 23:32:26 - train: epoch 007, train_loss: 2.3785
2022-08-04 23:33:39 - eval: epoch: 007, acc1: 52.288%, acc5: 77.754%, test_loss: 2.0558, per_image_load_time: 1.694ms, per_image_inference_time: 0.575ms
2022-08-04 23:33:39 - until epoch: 007, best_acc1: 52.288%
2022-08-04 23:33:39 - epoch 008 lr: 0.081870
2022-08-04 23:34:18 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.4405
2022-08-04 23:34:51 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.4181
2022-08-04 23:35:24 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.3806
2022-08-04 23:35:57 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.1614
2022-08-04 23:36:30 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.4746
2022-08-04 23:37:03 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.1779
2022-08-04 23:37:36 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.1769
2022-08-04 23:38:09 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.1018
2022-08-04 23:38:42 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.4200
2022-08-04 23:39:15 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.5010
2022-08-04 23:39:49 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.1013
2022-08-04 23:40:22 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.2954
2022-08-04 23:40:55 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.4400
2022-08-04 23:41:28 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.2222
2022-08-04 23:42:02 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.4000
2022-08-04 23:42:34 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.2940
2022-08-04 23:43:08 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.3769
2022-08-04 23:43:41 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.2138
2022-08-04 23:44:15 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.1296
2022-08-04 23:44:48 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.2394
2022-08-04 23:45:21 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.4077
2022-08-04 23:45:54 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.0895
2022-08-04 23:46:28 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.3608
2022-08-04 23:47:00 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.0667
2022-08-04 23:47:34 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.3388
2022-08-04 23:48:07 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.2586
2022-08-04 23:48:40 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.5783
2022-08-04 23:49:13 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.3693
2022-08-04 23:49:47 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.3292
2022-08-04 23:50:19 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.3557
2022-08-04 23:50:53 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.2857
2022-08-04 23:51:26 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.3593
2022-08-04 23:52:00 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.3378
2022-08-04 23:52:33 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.2820
2022-08-04 23:53:06 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.2523
2022-08-04 23:53:40 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.4261
2022-08-04 23:54:13 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.2258
2022-08-04 23:54:46 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.4178
2022-08-04 23:55:20 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.5041
2022-08-04 23:55:53 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.5181
2022-08-04 23:56:27 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.1839
2022-08-04 23:57:00 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.2621
2022-08-04 23:57:33 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.1792
2022-08-04 23:58:06 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.1849
2022-08-04 23:58:39 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.2118
2022-08-04 23:59:13 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.4127
2022-08-04 23:59:46 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.4152
2022-08-05 00:00:19 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.3721
2022-08-05 00:00:53 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.3874
2022-08-05 00:01:25 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.1282
2022-08-05 00:01:26 - train: epoch 008, train_loss: 2.2982
2022-08-05 00:02:39 - eval: epoch: 008, acc1: 53.948%, acc5: 79.522%, test_loss: 1.9472, per_image_load_time: 2.258ms, per_image_inference_time: 0.550ms
2022-08-05 00:02:39 - until epoch: 008, best_acc1: 53.948%
2022-08-05 00:02:39 - epoch 009 lr: 0.076790
2022-08-05 00:03:18 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 2.0315
2022-08-05 00:03:51 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.0638
2022-08-05 00:04:23 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.1413
2022-08-05 00:04:56 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.4029
2022-08-05 00:05:29 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.2007
2022-08-05 00:06:01 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.0114
2022-08-05 00:06:35 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.1492
2022-08-05 00:07:08 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.2138
2022-08-05 00:07:41 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.1189
2022-08-05 00:08:13 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1603
2022-08-05 00:08:46 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.6291
2022-08-05 00:09:20 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.2322
2022-08-05 00:09:53 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.2705
2022-08-05 00:10:26 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9950
2022-08-05 00:11:00 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.1473
2022-08-05 00:11:33 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2559
2022-08-05 00:12:06 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.4452
2022-08-05 00:12:40 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.0961
2022-08-05 00:13:13 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 1.9952
2022-08-05 00:13:46 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.0643
2022-08-05 00:14:19 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.2486
2022-08-05 00:14:53 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.2721
2022-08-05 00:15:26 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.2528
2022-08-05 00:16:00 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.3426
2022-08-05 00:16:33 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.2197
2022-08-05 00:17:06 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.2959
2022-08-05 00:17:39 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 1.9707
2022-08-05 00:18:13 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.1223
2022-08-05 00:18:47 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 1.8759
2022-08-05 00:19:20 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 1.9946
2022-08-05 00:19:52 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.5248
2022-08-05 00:20:26 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.0785
2022-08-05 00:21:00 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.4110
2022-08-05 00:21:33 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.3394
2022-08-05 00:22:06 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.2584
2022-08-05 00:22:39 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.1402
2022-08-05 00:23:12 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.3453
2022-08-05 00:23:45 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.2774
2022-08-05 00:24:18 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.1751
2022-08-05 00:24:50 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.4342
2022-08-05 00:25:24 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.2669
2022-08-05 00:25:57 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.0759
2022-08-05 00:26:30 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.0976
2022-08-05 00:27:02 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.3721
2022-08-05 00:27:36 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.3776
2022-08-05 00:28:08 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.3238
2022-08-05 00:28:41 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.3983
2022-08-05 00:29:15 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.2819
2022-08-05 00:29:48 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.3389
2022-08-05 00:30:20 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.1486
2022-08-05 00:30:22 - train: epoch 009, train_loss: 2.2264
2022-08-05 00:31:34 - eval: epoch: 009, acc1: 54.856%, acc5: 79.652%, test_loss: 1.9417, per_image_load_time: 1.954ms, per_image_inference_time: 0.582ms
2022-08-05 00:31:35 - until epoch: 009, best_acc1: 54.856%
2022-08-05 00:31:35 - epoch 010 lr: 0.071288
2022-08-05 00:32:13 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.2569
2022-08-05 00:32:46 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.3924
2022-08-05 00:33:19 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.3080
2022-08-05 00:33:51 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.3532
2022-08-05 00:34:24 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.2650
2022-08-05 00:34:57 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.2659
2022-08-05 00:35:30 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.3352
2022-08-05 00:36:03 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.0747
2022-08-05 00:36:36 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.1979
2022-08-05 00:37:09 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.0296
2022-08-05 00:37:42 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.2041
2022-08-05 00:38:15 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.0821
2022-08-05 00:38:48 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 1.9811
2022-08-05 00:39:21 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.3076
2022-08-05 00:39:55 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.8835
2022-08-05 00:40:28 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.2486
2022-08-05 00:41:01 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.2658
2022-08-05 00:41:34 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 2.0558
2022-08-05 00:42:07 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.2574
2022-08-05 00:42:40 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.3161
2022-08-05 00:43:13 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 1.9802
2022-08-05 00:43:46 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.4917
2022-08-05 00:44:18 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.2674
2022-08-05 00:44:51 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.3923
2022-08-05 00:45:24 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.2455
2022-08-05 00:45:57 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.1840
2022-08-05 00:46:30 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.9956
2022-08-05 00:47:03 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.2538
2022-08-05 00:47:36 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.2371
2022-08-05 00:48:09 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.2031
2022-08-05 00:48:42 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.3518
2022-08-05 00:49:15 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.0934
2022-08-05 00:49:48 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.2156
2022-08-05 00:50:21 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.0396
2022-08-05 00:50:54 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.2397
2022-08-05 00:51:28 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.2980
2022-08-05 00:52:00 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.0381
2022-08-05 00:52:34 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.0944
2022-08-05 00:53:07 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 1.9873
2022-08-05 00:53:40 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.1203
2022-08-05 00:54:13 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.1465
2022-08-05 00:54:46 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.2274
2022-08-05 00:55:19 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 1.9929
2022-08-05 00:55:53 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.0294
2022-08-05 00:56:27 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.2481
2022-08-05 00:57:00 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.1356
2022-08-05 00:57:34 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.3133
2022-08-05 00:58:06 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.1106
2022-08-05 00:58:39 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.2822
2022-08-05 00:59:11 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.1936
2022-08-05 00:59:12 - train: epoch 010, train_loss: 2.1650
2022-08-05 01:00:25 - eval: epoch: 010, acc1: 55.600%, acc5: 80.388%, test_loss: 1.8867, per_image_load_time: 2.121ms, per_image_inference_time: 0.543ms
2022-08-05 01:00:25 - until epoch: 010, best_acc1: 55.600%
2022-08-05 01:00:25 - epoch 011 lr: 0.065450
2022-08-05 01:01:03 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 1.9599
2022-08-05 01:01:36 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.1692
2022-08-05 01:02:09 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.1180
2022-08-05 01:02:42 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.1879
2022-08-05 01:03:14 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.0813
2022-08-05 01:03:47 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.1539
2022-08-05 01:04:20 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.0379
2022-08-05 01:04:53 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.2745
2022-08-05 01:05:25 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.2354
2022-08-05 01:05:58 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.1386
2022-08-05 01:06:31 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.3406
2022-08-05 01:07:04 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2723
2022-08-05 01:07:38 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.4917
2022-08-05 01:08:11 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.1016
2022-08-05 01:08:44 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.1562
2022-08-05 01:09:18 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.3026
2022-08-05 01:09:51 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.2353
2022-08-05 01:10:24 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.0377
2022-08-05 01:10:57 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.0639
2022-08-05 01:11:30 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1702
2022-08-05 01:12:03 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.1724
2022-08-05 01:12:36 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 1.9835
2022-08-05 01:13:09 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.1361
2022-08-05 01:13:42 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 2.1241
2022-08-05 01:14:14 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.0816
2022-08-05 01:14:48 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.0404
2022-08-05 01:15:21 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 1.9541
2022-08-05 01:15:54 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.8667
2022-08-05 01:16:26 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.0491
2022-08-05 01:17:00 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.2710
2022-08-05 01:17:33 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.0794
2022-08-05 01:18:06 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.2015
2022-08-05 01:18:39 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.2924
2022-08-05 01:19:13 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.0097
2022-08-05 01:19:46 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.2428
2022-08-05 01:20:18 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 1.8494
2022-08-05 01:20:52 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.2780
2022-08-05 01:21:25 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 2.0558
2022-08-05 01:21:57 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.1356
2022-08-05 01:22:31 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.1307
2022-08-05 01:23:04 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.8138
2022-08-05 01:23:38 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 2.0947
2022-08-05 01:24:11 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 1.8976
2022-08-05 01:24:43 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.1935
2022-08-05 01:25:17 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.1816
2022-08-05 01:25:50 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 1.8215
2022-08-05 01:26:23 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.8005
2022-08-05 01:26:56 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.9376
2022-08-05 01:27:30 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 1.8869
2022-08-05 01:28:02 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 1.8924
2022-08-05 01:28:03 - train: epoch 011, train_loss: 2.1031
2022-08-05 01:29:17 - eval: epoch: 011, acc1: 57.796%, acc5: 81.662%, test_loss: 1.7950, per_image_load_time: 2.271ms, per_image_inference_time: 0.551ms
2022-08-05 01:29:17 - until epoch: 011, best_acc1: 57.796%
2022-08-05 01:29:17 - epoch 012 lr: 0.059368
2022-08-05 01:29:55 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.9821
2022-08-05 01:30:28 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.8693
2022-08-05 01:31:01 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 1.8572
2022-08-05 01:31:33 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 1.9770
2022-08-05 01:32:06 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.1007
2022-08-05 01:32:39 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.7007
2022-08-05 01:33:13 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.8519
2022-08-05 01:33:45 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.2511
2022-08-05 01:34:18 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.1352
2022-08-05 01:34:52 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9917
2022-08-05 01:35:24 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.2529
2022-08-05 01:35:58 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.8306
2022-08-05 01:36:31 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.9889
2022-08-05 01:37:04 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.0721
2022-08-05 01:37:37 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 1.8009
2022-08-05 01:38:10 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 1.9874
2022-08-05 01:38:43 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.9144
2022-08-05 01:39:16 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.2472
2022-08-05 01:39:49 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1454
2022-08-05 01:40:22 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.3578
2022-08-05 01:40:55 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.0157
2022-08-05 01:41:28 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.0745
2022-08-05 01:42:02 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 1.9268
2022-08-05 01:42:35 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.1674
2022-08-05 01:43:08 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.7754
2022-08-05 01:43:41 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.9142
2022-08-05 01:44:14 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.1844
2022-08-05 01:44:48 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.8227
2022-08-05 01:45:21 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 1.9907
2022-08-05 01:45:54 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 2.0095
2022-08-05 01:46:27 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.0138
2022-08-05 01:47:00 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.9712
2022-08-05 01:47:33 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.0684
2022-08-05 01:48:06 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 1.9867
2022-08-05 01:48:39 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.0004
2022-08-05 01:49:13 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.7678
2022-08-05 01:49:45 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.1430
2022-08-05 01:50:19 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 1.9980
2022-08-05 01:50:52 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8315
2022-08-05 01:51:26 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.3458
2022-08-05 01:51:59 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 1.8928
2022-08-05 01:52:32 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.9708
2022-08-05 01:53:05 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.2052
2022-08-05 01:53:39 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.8611
2022-08-05 01:54:12 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.9481
2022-08-05 01:54:45 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 1.8647
2022-08-05 01:55:18 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.9646
2022-08-05 01:55:51 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.1578
2022-08-05 01:56:25 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.9889
2022-08-05 01:56:57 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.8963
2022-08-05 01:56:58 - train: epoch 012, train_loss: 2.0417
2022-08-05 01:58:10 - eval: epoch: 012, acc1: 58.846%, acc5: 82.886%, test_loss: 1.7208, per_image_load_time: 1.276ms, per_image_inference_time: 0.568ms
2022-08-05 01:58:10 - until epoch: 012, best_acc1: 58.846%
2022-08-05 01:58:10 - epoch 013 lr: 0.053138
2022-08-05 01:58:49 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.7891
2022-08-05 01:59:21 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.8376
2022-08-05 01:59:54 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8087
2022-08-05 02:00:26 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.9494
2022-08-05 02:00:59 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 1.9682
2022-08-05 02:01:31 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 1.9853
2022-08-05 02:02:04 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.8965
2022-08-05 02:02:37 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.2240
2022-08-05 02:03:10 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.9743
2022-08-05 02:03:43 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.9816
2022-08-05 02:04:16 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.1359
2022-08-05 02:04:49 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.0956
2022-08-05 02:05:23 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.8471
2022-08-05 02:05:56 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.0562
2022-08-05 02:06:29 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.2703
2022-08-05 02:07:02 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.8033
2022-08-05 02:07:35 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.0010
2022-08-05 02:08:08 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9701
2022-08-05 02:08:42 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.1244
2022-08-05 02:09:15 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.1150
2022-08-05 02:09:48 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.2459
2022-08-05 02:10:22 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.9823
2022-08-05 02:10:55 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 2.0508
2022-08-05 02:11:29 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.1836
2022-08-05 02:12:02 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.8575
2022-08-05 02:12:36 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 2.0573
2022-08-05 02:13:09 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.9881
2022-08-05 02:13:42 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.0591
2022-08-05 02:14:15 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.0749
2022-08-05 02:14:48 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.7713
2022-08-05 02:15:22 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.8994
2022-08-05 02:15:55 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 2.0055
2022-08-05 02:16:28 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.9688
2022-08-05 02:17:00 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 2.0797
2022-08-05 02:17:34 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.8428
2022-08-05 02:18:07 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 1.8641
2022-08-05 02:18:40 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.6337
2022-08-05 02:19:13 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.0790
2022-08-05 02:19:47 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.1709
2022-08-05 02:20:20 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 2.0502
2022-08-05 02:20:52 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.8657
2022-08-05 02:21:26 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 2.0183
2022-08-05 02:21:58 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 2.0436
2022-08-05 02:22:31 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 2.0067
2022-08-05 02:23:05 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.8172
2022-08-05 02:23:38 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 1.9045
2022-08-05 02:24:11 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.8648
2022-08-05 02:24:44 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.1060
2022-08-05 02:25:17 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0212
2022-08-05 02:25:50 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.2134
2022-08-05 02:25:51 - train: epoch 013, train_loss: 1.9843
2022-08-05 02:27:04 - eval: epoch: 013, acc1: 60.038%, acc5: 83.762%, test_loss: 1.6701, per_image_load_time: 1.194ms, per_image_inference_time: 0.572ms
2022-08-05 02:27:04 - until epoch: 013, best_acc1: 60.038%
2022-08-05 02:27:04 - epoch 014 lr: 0.046859
2022-08-05 02:27:43 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.8467
2022-08-05 02:28:15 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 2.1718
2022-08-05 02:28:48 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.9005
2022-08-05 02:29:21 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.9132
2022-08-05 02:29:54 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.8490
2022-08-05 02:30:27 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.6856
2022-08-05 02:31:00 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.8174
2022-08-05 02:31:33 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.8030
2022-08-05 02:32:06 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.8883
2022-08-05 02:32:38 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.0022
2022-08-05 02:33:11 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.6688
2022-08-05 02:33:44 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 1.9704
2022-08-05 02:34:17 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.0130
2022-08-05 02:34:50 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.9233
2022-08-05 02:35:23 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.0510
2022-08-05 02:35:56 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.8383
2022-08-05 02:36:29 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 2.0068
2022-08-05 02:37:03 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.0880
2022-08-05 02:37:36 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.8272
2022-08-05 02:38:09 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.9054
2022-08-05 02:38:42 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.1374
2022-08-05 02:39:15 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.8838
2022-08-05 02:39:48 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.8852
2022-08-05 02:40:21 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.9646
2022-08-05 02:40:54 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.9916
2022-08-05 02:41:28 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.7756
2022-08-05 02:42:01 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.7169
2022-08-05 02:42:34 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 1.9839
2022-08-05 02:43:07 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9168
2022-08-05 02:43:40 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.8943
2022-08-05 02:44:13 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.8041
2022-08-05 02:44:46 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 2.0017
2022-08-05 02:45:20 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.7654
2022-08-05 02:45:53 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 2.0714
2022-08-05 02:46:26 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.8236
2022-08-05 02:47:00 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.7579
2022-08-05 02:47:33 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.8777
2022-08-05 02:48:06 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 2.1140
2022-08-05 02:48:40 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.9371
2022-08-05 02:49:13 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.9264
2022-08-05 02:49:46 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.8349
2022-08-05 02:50:19 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.7921
2022-08-05 02:50:52 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.7947
2022-08-05 02:51:26 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 2.0458
2022-08-05 02:51:59 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.8519
2022-08-05 02:52:32 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.7649
2022-08-05 02:53:06 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.8499
2022-08-05 02:53:39 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.9711
2022-08-05 02:54:13 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8926
2022-08-05 02:54:45 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.8556
2022-08-05 02:54:46 - train: epoch 014, train_loss: 1.9210
2022-08-05 02:55:59 - eval: epoch: 014, acc1: 60.374%, acc5: 83.688%, test_loss: 1.6627, per_image_load_time: 2.101ms, per_image_inference_time: 0.569ms
2022-08-05 02:55:59 - until epoch: 014, best_acc1: 60.374%
2022-08-05 02:55:59 - epoch 015 lr: 0.040630
2022-08-05 02:56:38 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.7891
2022-08-05 02:57:10 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.8846
2022-08-05 02:57:43 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.1674
2022-08-05 02:58:15 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 2.0839
2022-08-05 02:58:48 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.8515
2022-08-05 02:59:21 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 1.8696
2022-08-05 02:59:54 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 2.0110
2022-08-05 03:00:27 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.7372
2022-08-05 03:01:01 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.8704
2022-08-05 03:01:33 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.6976
2022-08-05 03:02:06 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.5580
2022-08-05 03:02:39 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 2.1455
2022-08-05 03:03:12 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.9841
2022-08-05 03:03:44 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.6923
2022-08-05 03:04:17 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.8796
2022-08-05 03:04:51 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.9069
2022-08-05 03:05:24 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.9486
2022-08-05 03:05:57 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.6450
2022-08-05 03:06:30 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.7128
2022-08-05 03:07:03 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.9683
2022-08-05 03:07:37 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.7329
2022-08-05 03:08:10 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9238
2022-08-05 03:08:43 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.8805
2022-08-05 03:09:16 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.7488
2022-08-05 03:09:50 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.8691
2022-08-05 03:10:23 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.6455
2022-08-05 03:10:56 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 2.0155
2022-08-05 03:11:29 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.9634
2022-08-05 03:12:02 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.8983
2022-08-05 03:12:35 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.6559
2022-08-05 03:13:08 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.6361
2022-08-05 03:13:41 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.8329
2022-08-05 03:14:14 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.6708
2022-08-05 03:14:47 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.9703
2022-08-05 03:15:19 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0181
2022-08-05 03:15:53 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.7353
2022-08-05 03:16:27 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6145
2022-08-05 03:17:00 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 2.0313
2022-08-05 03:17:33 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8235
2022-08-05 03:18:06 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.8036
2022-08-05 03:18:39 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.9234
2022-08-05 03:19:12 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.5865
2022-08-05 03:19:45 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.7046
2022-08-05 03:20:18 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.6903
2022-08-05 03:20:52 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.6484
2022-08-05 03:21:25 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 2.0392
2022-08-05 03:21:58 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.8511
2022-08-05 03:22:32 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.9372
2022-08-05 03:23:05 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.7732
2022-08-05 03:23:36 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.9383
2022-08-05 03:23:37 - train: epoch 015, train_loss: 1.8592
2022-08-05 03:24:50 - eval: epoch: 015, acc1: 62.926%, acc5: 85.214%, test_loss: 1.5470, per_image_load_time: 2.275ms, per_image_inference_time: 0.559ms
2022-08-05 03:24:51 - until epoch: 015, best_acc1: 62.926%
2022-08-05 03:24:51 - epoch 016 lr: 0.034548
2022-08-05 03:25:29 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.6298
2022-08-05 03:26:01 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.6910
2022-08-05 03:26:34 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.8573
2022-08-05 03:27:06 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.0229
2022-08-05 03:27:39 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.6454
2022-08-05 03:28:12 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.6741
2022-08-05 03:28:44 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.6990
2022-08-05 03:29:17 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.9620
2022-08-05 03:29:50 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.8581
2022-08-05 03:30:23 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.7953
2022-08-05 03:30:56 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.7324
2022-08-05 03:31:29 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.6747
2022-08-05 03:32:01 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.7177
2022-08-05 03:32:35 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.8279
2022-08-05 03:33:08 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.9623
2022-08-05 03:33:42 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 2.0265
2022-08-05 03:34:14 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.8921
2022-08-05 03:34:48 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.9924
2022-08-05 03:35:21 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.8979
2022-08-05 03:35:54 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.4143
2022-08-05 03:36:27 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8450
2022-08-05 03:37:01 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.7847
2022-08-05 03:37:34 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.8802
2022-08-05 03:38:07 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.9045
2022-08-05 03:38:40 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.6793
2022-08-05 03:39:13 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.9007
2022-08-05 03:39:46 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.7355
2022-08-05 03:40:19 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.7459
2022-08-05 03:40:52 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.8272
2022-08-05 03:41:25 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 2.0562
2022-08-05 03:41:58 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.7031
2022-08-05 03:42:31 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.9303
2022-08-05 03:43:05 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9673
2022-08-05 03:43:38 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.7014
2022-08-05 03:44:11 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.7763
2022-08-05 03:44:44 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.7331
2022-08-05 03:45:18 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.8226
2022-08-05 03:45:51 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.1485
2022-08-05 03:46:24 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.8515
2022-08-05 03:46:57 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.9057
2022-08-05 03:47:30 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.6517
2022-08-05 03:48:03 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.6221
2022-08-05 03:48:36 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.8417
2022-08-05 03:49:10 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.5240
2022-08-05 03:49:43 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.6833
2022-08-05 03:50:16 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.5891
2022-08-05 03:50:49 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.8494
2022-08-05 03:51:23 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.6861
2022-08-05 03:51:56 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.7273
2022-08-05 03:52:28 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.9013
2022-08-05 03:52:29 - train: epoch 016, train_loss: 1.7929
2022-08-05 03:53:41 - eval: epoch: 016, acc1: 63.978%, acc5: 86.058%, test_loss: 1.4882, per_image_load_time: 1.735ms, per_image_inference_time: 0.555ms
2022-08-05 03:53:41 - until epoch: 016, best_acc1: 63.978%
2022-08-05 03:53:41 - epoch 017 lr: 0.028710
2022-08-05 03:54:20 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.7267
2022-08-05 03:54:52 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.7698
2022-08-05 03:55:25 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 2.0120
2022-08-05 03:55:57 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.4789
2022-08-05 03:56:30 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.8991
2022-08-05 03:57:03 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 2.0131
2022-08-05 03:57:36 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.6192
2022-08-05 03:58:09 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.7760
2022-08-05 03:58:42 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.7683
2022-08-05 03:59:16 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.7060
2022-08-05 03:59:48 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.8541
2022-08-05 04:00:22 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.6194
2022-08-05 04:00:55 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.6194
2022-08-05 04:01:28 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.7733
2022-08-05 04:02:01 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.5741
2022-08-05 04:02:34 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.7499
2022-08-05 04:03:08 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.6680
2022-08-05 04:03:41 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.6122
2022-08-05 04:04:14 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.8857
2022-08-05 04:04:47 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.6316
2022-08-05 04:05:20 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.6371
2022-08-05 04:05:53 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.5955
2022-08-05 04:06:27 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.6928
2022-08-05 04:07:00 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.8373
2022-08-05 04:07:34 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.9948
2022-08-05 04:08:07 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.6305
2022-08-05 04:08:40 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6130
2022-08-05 04:09:13 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.7916
2022-08-05 04:09:46 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 2.0386
2022-08-05 04:10:19 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.6202
2022-08-05 04:10:52 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.7132
2022-08-05 04:11:25 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.5729
2022-08-05 04:11:59 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8792
2022-08-05 04:12:32 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.6515
2022-08-05 04:13:05 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.6419
2022-08-05 04:13:38 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.8057
2022-08-05 04:14:11 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.6751
2022-08-05 04:14:45 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.7164
2022-08-05 04:15:18 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.6735
2022-08-05 04:15:51 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.8291
2022-08-05 04:16:25 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.8237
2022-08-05 04:16:57 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.5615
2022-08-05 04:17:31 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.7028
2022-08-05 04:18:04 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.9371
2022-08-05 04:18:37 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.8148
2022-08-05 04:19:10 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.6883
2022-08-05 04:19:44 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.9238
2022-08-05 04:20:17 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.8062
2022-08-05 04:20:51 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.6593
2022-08-05 04:21:23 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.4884
2022-08-05 04:21:24 - train: epoch 017, train_loss: 1.7260
2022-08-05 04:22:37 - eval: epoch: 017, acc1: 65.058%, acc5: 86.648%, test_loss: 1.4464, per_image_load_time: 2.275ms, per_image_inference_time: 0.550ms
2022-08-05 04:22:37 - until epoch: 017, best_acc1: 65.058%
2022-08-05 04:22:37 - epoch 018 lr: 0.023208
2022-08-05 04:23:15 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.5348
2022-08-05 04:23:48 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.9629
2022-08-05 04:24:20 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.8885
2022-08-05 04:24:53 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.7813
2022-08-05 04:25:26 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.7302
2022-08-05 04:25:59 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.7065
2022-08-05 04:26:32 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.7097
2022-08-05 04:27:04 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.6774
2022-08-05 04:27:37 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.7807
2022-08-05 04:28:10 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.6885
2022-08-05 04:28:42 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.8221
2022-08-05 04:29:15 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.5121
2022-08-05 04:29:48 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.9517
2022-08-05 04:30:22 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.8585
2022-08-05 04:30:55 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.9210
2022-08-05 04:31:27 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.6276
2022-08-05 04:32:01 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7697
2022-08-05 04:32:34 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6922
2022-08-05 04:33:07 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.6819
2022-08-05 04:33:41 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 2.0054
2022-08-05 04:34:14 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.7607
2022-08-05 04:34:46 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.6509
2022-08-05 04:35:19 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.7848
2022-08-05 04:35:53 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.6808
2022-08-05 04:36:26 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.4214
2022-08-05 04:36:59 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.6533
2022-08-05 04:37:32 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.6921
2022-08-05 04:38:05 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5747
2022-08-05 04:38:38 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.6362
2022-08-05 04:39:11 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.5556
2022-08-05 04:39:44 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 2.0340
2022-08-05 04:40:17 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.5046
2022-08-05 04:40:50 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.5686
2022-08-05 04:41:23 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.6368
2022-08-05 04:41:56 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.6192
2022-08-05 04:42:29 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.7657
2022-08-05 04:43:02 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 2.0004
2022-08-05 04:43:36 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.6998
2022-08-05 04:44:10 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.7363
2022-08-05 04:44:43 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.7311
2022-08-05 04:45:15 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7519
2022-08-05 04:45:49 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.8800
2022-08-05 04:46:22 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.6648
2022-08-05 04:46:55 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.6125
2022-08-05 04:47:28 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.5580
2022-08-05 04:48:02 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.7662
2022-08-05 04:48:35 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.7717
2022-08-05 04:49:08 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6895
2022-08-05 04:49:41 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.4787
2022-08-05 04:50:14 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.7815
2022-08-05 04:50:15 - train: epoch 018, train_loss: 1.6546
2022-08-05 04:51:28 - eval: epoch: 018, acc1: 66.280%, acc5: 87.458%, test_loss: 1.3870, per_image_load_time: 1.920ms, per_image_inference_time: 0.593ms
2022-08-05 04:51:28 - until epoch: 018, best_acc1: 66.280%
2022-08-05 04:51:28 - epoch 019 lr: 0.018128
2022-08-05 04:52:07 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4948
2022-08-05 04:52:39 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.5223
2022-08-05 04:53:11 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.7005
2022-08-05 04:53:44 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.6367
2022-08-05 04:54:17 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.4138
2022-08-05 04:54:50 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.5827
2022-08-05 04:55:23 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.5278
2022-08-05 04:55:55 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.8689
2022-08-05 04:56:29 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.5721
2022-08-05 04:57:01 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.6191
2022-08-05 04:57:35 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.4716
2022-08-05 04:58:08 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.5594
2022-08-05 04:58:41 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.5397
2022-08-05 04:59:14 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.3651
2022-08-05 04:59:48 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.6045
2022-08-05 05:00:21 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.3831
2022-08-05 05:00:53 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.8404
2022-08-05 05:01:26 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.4636
2022-08-05 05:02:00 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.6599
2022-08-05 05:02:33 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.4375
2022-08-05 05:03:07 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.4863
2022-08-05 05:03:40 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.4973
2022-08-05 05:04:14 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.5274
2022-08-05 05:04:47 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.5929
2022-08-05 05:05:20 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.5926
2022-08-05 05:05:54 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.5233
2022-08-05 05:06:27 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.4140
2022-08-05 05:07:00 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.5398
2022-08-05 05:07:34 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.7620
2022-08-05 05:08:06 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.7500
2022-08-05 05:08:39 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.6025
2022-08-05 05:09:13 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.3309
2022-08-05 05:09:47 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.6532
2022-08-05 05:10:20 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.5210
2022-08-05 05:10:53 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.5815
2022-08-05 05:11:26 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.2440
2022-08-05 05:12:00 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.5876
2022-08-05 05:12:33 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.6982
2022-08-05 05:13:06 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.4500
2022-08-05 05:13:40 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.6382
2022-08-05 05:14:13 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.6461
2022-08-05 05:14:46 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.5981
2022-08-05 05:15:20 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.2858
2022-08-05 05:15:53 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.6608
2022-08-05 05:16:26 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.7924
2022-08-05 05:17:00 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.4908
2022-08-05 05:17:33 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.4576
2022-08-05 05:18:06 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.4230
2022-08-05 05:18:39 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.5855
2022-08-05 05:19:11 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.5385
2022-08-05 05:19:13 - train: epoch 019, train_loss: 1.5804
2022-08-05 05:20:25 - eval: epoch: 019, acc1: 67.644%, acc5: 88.250%, test_loss: 1.3275, per_image_load_time: 2.224ms, per_image_inference_time: 0.566ms
2022-08-05 05:20:26 - until epoch: 019, best_acc1: 67.644%
2022-08-05 05:20:26 - epoch 020 lr: 0.013551
2022-08-05 05:21:04 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.4446
2022-08-05 05:21:37 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.5413
2022-08-05 05:22:09 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.5575
2022-08-05 05:22:41 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.2575
2022-08-05 05:23:14 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.3502
2022-08-05 05:23:46 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.7473
2022-08-05 05:24:19 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.2204
2022-08-05 05:24:52 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.6498
2022-08-05 05:25:25 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.7047
2022-08-05 05:25:58 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.4262
2022-08-05 05:26:31 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.5614
2022-08-05 05:27:04 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.3101
2022-08-05 05:27:37 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.5816
2022-08-05 05:28:10 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.7049
2022-08-05 05:28:42 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.4569
2022-08-05 05:29:15 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.6402
2022-08-05 05:29:48 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.6066
2022-08-05 05:30:21 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.5252
2022-08-05 05:30:54 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.3746
2022-08-05 05:31:26 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.5606
2022-08-05 05:32:00 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.5440
2022-08-05 05:32:33 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.2551
2022-08-05 05:33:05 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.3469
2022-08-05 05:33:38 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.7756
2022-08-05 05:34:11 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.5845
2022-08-05 05:34:45 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.4466
2022-08-05 05:35:18 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.3359
2022-08-05 05:35:51 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.5876
2022-08-05 05:36:24 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.6475
2022-08-05 05:36:57 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.7880
2022-08-05 05:37:30 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.5317
2022-08-05 05:38:03 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.5664
2022-08-05 05:38:36 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3533
2022-08-05 05:39:10 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.5929
2022-08-05 05:39:43 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.4335
2022-08-05 05:40:16 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.5464
2022-08-05 05:40:49 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.4958
2022-08-05 05:41:22 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.3774
2022-08-05 05:41:55 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.6094
2022-08-05 05:42:28 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.3867
2022-08-05 05:43:01 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.4239
2022-08-05 05:43:35 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.5575
2022-08-05 05:44:08 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.5383
2022-08-05 05:44:41 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.4498
2022-08-05 05:45:14 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.4761
2022-08-05 05:45:48 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.4015
2022-08-05 05:46:21 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.5209
2022-08-05 05:46:54 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.2913
2022-08-05 05:47:28 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.5350
2022-08-05 05:48:00 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.4302
2022-08-05 05:48:01 - train: epoch 020, train_loss: 1.5051
2022-08-05 05:49:14 - eval: epoch: 020, acc1: 69.002%, acc5: 89.144%, test_loss: 1.2601, per_image_load_time: 2.262ms, per_image_inference_time: 0.582ms
2022-08-05 05:49:15 - until epoch: 020, best_acc1: 69.002%
2022-08-05 05:49:15 - epoch 021 lr: 0.009548
2022-08-05 05:49:54 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.4574
2022-08-05 05:50:27 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.3789
2022-08-05 05:51:00 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.2945
2022-08-05 05:51:32 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4813
2022-08-05 05:52:05 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.2692
2022-08-05 05:52:38 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.3415
2022-08-05 05:53:11 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3510
2022-08-05 05:53:44 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.5387
2022-08-05 05:54:17 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.2717
2022-08-05 05:54:50 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3250
2022-08-05 05:55:23 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.4962
2022-08-05 05:55:56 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.4178
2022-08-05 05:56:29 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.2314
2022-08-05 05:57:02 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.2065
2022-08-05 05:57:35 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.3561
2022-08-05 05:58:09 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3147
2022-08-05 05:58:42 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.6605
2022-08-05 05:59:15 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.4619
2022-08-05 05:59:47 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.7389
2022-08-05 06:00:20 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.3005
2022-08-05 06:00:54 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3178
2022-08-05 06:01:27 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.3844
2022-08-05 06:02:00 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.3573
2022-08-05 06:02:33 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.2375
2022-08-05 06:03:06 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.6077
2022-08-05 06:03:40 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.4355
2022-08-05 06:04:14 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.4039
2022-08-05 06:04:47 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.3605
2022-08-05 06:05:21 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.4396
2022-08-05 06:05:53 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.3614
2022-08-05 06:06:27 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.4000
2022-08-05 06:07:01 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.2961
2022-08-05 06:07:34 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.4393
2022-08-05 06:08:06 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.5130
2022-08-05 06:08:40 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4640
2022-08-05 06:09:13 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.4403
2022-08-05 06:09:47 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.3634
2022-08-05 06:10:20 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.5683
2022-08-05 06:10:53 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.3091
2022-08-05 06:11:26 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.7805
2022-08-05 06:12:00 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.3339
2022-08-05 06:12:33 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.4997
2022-08-05 06:13:06 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3483
2022-08-05 06:13:39 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.5564
2022-08-05 06:14:12 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.3141
2022-08-05 06:14:46 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.4726
2022-08-05 06:15:20 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.4762
2022-08-05 06:15:52 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.3991
2022-08-05 06:16:25 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.1823
2022-08-05 06:16:58 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.3674
2022-08-05 06:16:59 - train: epoch 021, train_loss: 1.4318
2022-08-05 06:18:12 - eval: epoch: 021, acc1: 70.100%, acc5: 89.562%, test_loss: 1.2115, per_image_load_time: 0.939ms, per_image_inference_time: 0.563ms
2022-08-05 06:18:13 - until epoch: 021, best_acc1: 70.100%
2022-08-05 06:18:13 - epoch 022 lr: 0.006184
2022-08-05 06:18:51 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.0131
2022-08-05 06:19:24 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.3191
2022-08-05 06:19:56 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.3191
2022-08-05 06:20:29 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.3362
2022-08-05 06:21:01 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3805
2022-08-05 06:21:34 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.3874
2022-08-05 06:22:06 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.5092
2022-08-05 06:22:39 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.3896
2022-08-05 06:23:13 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.3103
2022-08-05 06:23:45 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.4194
2022-08-05 06:24:18 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3124
2022-08-05 06:24:51 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.0867
2022-08-05 06:25:24 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.2660
2022-08-05 06:25:57 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.1970
2022-08-05 06:26:30 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.3933
2022-08-05 06:27:03 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.1998
2022-08-05 06:27:36 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.3942
2022-08-05 06:28:09 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.5187
2022-08-05 06:28:42 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.3700
2022-08-05 06:29:15 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.3100
2022-08-05 06:29:48 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.5869
2022-08-05 06:30:21 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.1031
2022-08-05 06:30:54 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.3348
2022-08-05 06:31:27 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.2658
2022-08-05 06:32:00 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.3243
2022-08-05 06:32:34 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.2564
2022-08-05 06:33:07 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.1863
2022-08-05 06:33:40 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.4819
2022-08-05 06:34:13 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.1755
2022-08-05 06:34:46 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.3282
2022-08-05 06:35:19 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.6565
2022-08-05 06:35:52 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.3570
2022-08-05 06:36:26 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.4422
2022-08-05 06:36:58 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.2770
2022-08-05 06:37:31 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.5235
2022-08-05 06:38:04 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.2200
2022-08-05 06:38:38 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.5920
2022-08-05 06:39:11 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.4738
2022-08-05 06:39:44 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.1763
2022-08-05 06:40:16 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.4206
2022-08-05 06:40:49 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.2167
2022-08-05 06:41:23 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.3488
2022-08-05 06:41:56 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.6133
2022-08-05 06:42:28 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3188
2022-08-05 06:43:03 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.2927
2022-08-05 06:43:35 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.4903
2022-08-05 06:44:08 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.3646
2022-08-05 06:44:41 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.2463
2022-08-05 06:45:14 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.1573
2022-08-05 06:45:47 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.1823
2022-08-05 06:45:48 - train: epoch 022, train_loss: 1.3671
2022-08-05 06:47:01 - eval: epoch: 022, acc1: 70.964%, acc5: 90.094%, test_loss: 1.1678, per_image_load_time: 2.137ms, per_image_inference_time: 0.597ms
2022-08-05 06:47:01 - until epoch: 022, best_acc1: 70.964%
2022-08-05 06:47:01 - epoch 023 lr: 0.003511
2022-08-05 06:47:40 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.3471
2022-08-05 06:48:12 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.2226
2022-08-05 06:48:45 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.4598
2022-08-05 06:49:18 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.3076
2022-08-05 06:49:51 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.4164
2022-08-05 06:50:23 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.3591
2022-08-05 06:50:56 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.2141
2022-08-05 06:51:29 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3303
2022-08-05 06:52:02 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4663
2022-08-05 06:52:35 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.2244
2022-08-05 06:53:08 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.3466
2022-08-05 06:53:41 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.2941
2022-08-05 06:54:15 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.3460
2022-08-05 06:54:47 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.3210
2022-08-05 06:55:20 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.2138
2022-08-05 06:55:54 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.3084
2022-08-05 06:56:27 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.3526
2022-08-05 06:57:00 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.1595
2022-08-05 06:57:32 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.3114
2022-08-05 06:58:05 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.0864
2022-08-05 06:58:39 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.2947
2022-08-05 06:59:12 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.1185
2022-08-05 06:59:46 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.2262
2022-08-05 07:00:18 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.3963
2022-08-05 07:00:51 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.2080
2022-08-05 07:01:25 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.4622
2022-08-05 07:01:58 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.3566
2022-08-05 07:02:31 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.3534
2022-08-05 07:03:05 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.1818
2022-08-05 07:03:37 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.4091
2022-08-05 07:04:11 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.4818
2022-08-05 07:04:44 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.3545
2022-08-05 07:05:17 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.3419
2022-08-05 07:05:50 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.3390
2022-08-05 07:06:23 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.4237
2022-08-05 07:06:56 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.2219
2022-08-05 07:07:30 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.3370
2022-08-05 07:08:03 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.4456
2022-08-05 07:08:36 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3116
2022-08-05 07:09:10 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2693
2022-08-05 07:09:43 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.3335
2022-08-05 07:10:17 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.2070
2022-08-05 07:10:50 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.1337
2022-08-05 07:11:23 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.3865
2022-08-05 07:11:57 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.2337
2022-08-05 07:12:30 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.4235
2022-08-05 07:13:03 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.2036
2022-08-05 07:13:36 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.2319
2022-08-05 07:14:09 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.0727
2022-08-05 07:14:42 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.3927
2022-08-05 07:14:43 - train: epoch 023, train_loss: 1.3117
2022-08-05 07:15:56 - eval: epoch: 023, acc1: 71.700%, acc5: 90.562%, test_loss: 1.1382, per_image_load_time: 2.264ms, per_image_inference_time: 0.557ms
2022-08-05 07:15:56 - until epoch: 023, best_acc1: 71.700%
2022-08-05 07:15:56 - epoch 024 lr: 0.001571
2022-08-05 07:16:35 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.3531
2022-08-05 07:17:08 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.2223
2022-08-05 07:17:40 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.2686
2022-08-05 07:18:13 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.2797
2022-08-05 07:18:45 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.4783
2022-08-05 07:19:19 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.2980
2022-08-05 07:19:51 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.2693
2022-08-05 07:20:24 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.1698
2022-08-05 07:20:57 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.2120
2022-08-05 07:21:29 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.3256
2022-08-05 07:22:03 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.2180
2022-08-05 07:22:36 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.1704
2022-08-05 07:23:09 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4897
2022-08-05 07:23:42 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.1693
2022-08-05 07:24:15 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.2922
2022-08-05 07:24:48 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.2419
2022-08-05 07:25:22 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.1401
2022-08-05 07:25:54 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.4348
2022-08-05 07:26:27 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.0495
2022-08-05 07:27:01 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.3293
2022-08-05 07:27:34 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.4370
2022-08-05 07:28:08 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.1813
2022-08-05 07:28:41 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.2773
2022-08-05 07:29:14 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.3244
2022-08-05 07:29:47 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.2280
2022-08-05 07:30:20 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.3482
2022-08-05 07:30:53 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.4436
2022-08-05 07:31:27 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.3480
2022-08-05 07:32:00 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.2536
2022-08-05 07:32:33 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.1818
2022-08-05 07:33:06 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.1345
2022-08-05 07:33:39 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.3791
2022-08-05 07:34:13 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.2065
2022-08-05 07:34:46 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.1758
2022-08-05 07:35:19 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.2697
2022-08-05 07:35:52 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.1630
2022-08-05 07:36:25 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.3642
2022-08-05 07:36:59 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.3899
2022-08-05 07:37:31 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.4465
2022-08-05 07:38:05 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.1161
2022-08-05 07:38:39 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.3717
2022-08-05 07:39:12 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.2014
2022-08-05 07:39:46 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.3371
2022-08-05 07:40:19 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.1434
2022-08-05 07:40:53 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.0683
2022-08-05 07:41:26 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.3687
2022-08-05 07:42:00 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.2299
2022-08-05 07:42:32 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.0527
2022-08-05 07:43:06 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.3411
2022-08-05 07:43:39 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.2692
2022-08-05 07:43:40 - train: epoch 024, train_loss: 1.2702
2022-08-05 07:44:53 - eval: epoch: 024, acc1: 72.188%, acc5: 90.628%, test_loss: 1.1205, per_image_load_time: 2.245ms, per_image_inference_time: 0.554ms
2022-08-05 07:44:53 - until epoch: 024, best_acc1: 72.188%
2022-08-05 07:44:53 - epoch 025 lr: 0.000394
2022-08-05 07:45:32 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.1479
2022-08-05 07:46:05 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1331
2022-08-05 07:46:37 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.0436
2022-08-05 07:47:10 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.2098
2022-08-05 07:47:42 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.1775
2022-08-05 07:48:15 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3769
2022-08-05 07:48:47 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.1887
2022-08-05 07:49:20 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.2949
2022-08-05 07:49:53 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.1787
2022-08-05 07:50:26 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.2672
2022-08-05 07:50:58 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.2027
2022-08-05 07:51:32 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.2557
2022-08-05 07:52:05 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3988
2022-08-05 07:52:38 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.2283
2022-08-05 07:53:11 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.2502
2022-08-05 07:53:45 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1646
2022-08-05 07:54:17 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.2276
2022-08-05 07:54:50 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 0.9776
2022-08-05 07:55:23 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.3196
2022-08-05 07:55:57 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.3529
2022-08-05 07:56:30 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.0922
2022-08-05 07:57:04 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.1845
2022-08-05 07:57:37 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.1546
2022-08-05 07:58:11 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.0768
2022-08-05 07:58:44 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.1208
2022-08-05 07:59:16 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.1607
2022-08-05 07:59:49 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.2482
2022-08-05 08:00:23 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.3590
2022-08-05 08:00:57 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.4332
2022-08-05 08:01:30 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.2628
2022-08-05 08:02:03 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.3479
2022-08-05 08:02:35 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.3666
2022-08-05 08:03:09 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.1489
2022-08-05 08:03:42 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.2975
2022-08-05 08:04:15 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1803
2022-08-05 08:04:48 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.1317
2022-08-05 08:05:22 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.3197
2022-08-05 08:05:55 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.5507
2022-08-05 08:06:28 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.4099
2022-08-05 08:07:01 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.2301
2022-08-05 08:07:34 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.3831
2022-08-05 08:08:08 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.3970
2022-08-05 08:08:40 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.0642
2022-08-05 08:09:13 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.0839
2022-08-05 08:09:48 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.3956
2022-08-05 08:10:21 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.0868
2022-08-05 08:10:54 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.2237
2022-08-05 08:11:27 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 0.9754
2022-08-05 08:12:01 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.2744
2022-08-05 08:12:33 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.3450
2022-08-05 08:12:34 - train: epoch 025, train_loss: 1.2521
2022-08-05 08:13:47 - eval: epoch: 025, acc1: 72.170%, acc5: 90.690%, test_loss: 1.1173, per_image_load_time: 2.183ms, per_image_inference_time: 0.590ms
2022-08-05 08:13:47 - until epoch: 025, best_acc1: 72.188%
2022-08-05 08:13:47 - train done. train time: 12.039 hours, best_acc1: 72.188%
2022-08-09 22:49:20 - net_idx: 2
2022-08-09 22:49:20 - net_config: {'stem_width': 64, 'depth': 15, 'w_0': 32, 'w_a': 16.53264444950164, 'w_m': 1.9698967349417775}
2022-08-09 22:49:20 - num_classes: 1000
2022-08-09 22:49:20 - input_image_size: 224
2022-08-09 22:49:20 - scale: 1.1428571428571428
2022-08-09 22:49:20 - seed: 0
2022-08-09 22:49:20 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-09 22:49:20 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-09 22:49:20 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce2b0>
2022-08-09 22:49:20 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce6d0>
2022-08-09 22:49:20 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce700>
2022-08-09 22:49:20 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce760>
2022-08-09 22:49:20 - batch_size: 256
2022-08-09 22:49:20 - num_workers: 16
2022-08-09 22:49:20 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-09 22:49:20 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-09 22:49:20 - epochs: 25
2022-08-09 22:49:20 - print_interval: 100
2022-08-09 22:49:20 - accumulation_steps: 1
2022-08-09 22:49:20 - sync_bn: False
2022-08-09 22:49:20 - apex: True
2022-08-09 22:49:20 - use_ema_model: False
2022-08-09 22:49:20 - ema_model_decay: 0.9999
2022-08-09 22:49:20 - log_dir: ./log
2022-08-09 22:49:20 - checkpoint_dir: ./checkpoints
2022-08-09 22:49:20 - gpus_type: NVIDIA RTX A5000
2022-08-09 22:49:20 - gpus_num: 2
2022-08-09 22:49:20 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5dddaeb570>
2022-08-09 22:49:20 - ema_model: None
2022-08-09 22:49:20 - --------------------parameters--------------------
2022-08-09 22:49:20 - name: conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: fc.weight, grad: True
2022-08-09 22:49:20 - name: fc.bias, grad: True
2022-08-09 22:49:20 - --------------------buffers--------------------
2022-08-09 22:49:20 - name: conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - -----------no weight decay layers--------------
2022-08-09 22:49:20 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - -------------weight decay layers---------------
2022-08-09 22:49:20 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - resuming model from ./checkpoints/2/latest.pth. resume_epoch: 025, used_time: 12.039 hours, best_acc1: 72.188%, test_loss: 1.1173, lr: 0.000000
2022-08-09 22:49:20 - train done. train time: 12.039 hours, best_acc1: 72.188%
