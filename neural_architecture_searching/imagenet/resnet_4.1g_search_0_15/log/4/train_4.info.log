2022-08-05 20:18:06 - net_idx: 4
2022-08-05 20:18:06 - net_config: {'stem_width': 64, 'depth': 15, 'w_0': 40, 'w_a': 17.36690916049138, 'w_m': 1.8119620337729927}
2022-08-05 20:18:06 - num_classes: 1000
2022-08-05 20:18:06 - input_image_size: 224
2022-08-05 20:18:06 - scale: 1.1428571428571428
2022-08-05 20:18:06 - seed: 0
2022-08-05 20:18:06 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-05 20:18:06 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-05 20:18:06 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fadba72ea00>
2022-08-05 20:18:06 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fadb9da1af0>
2022-08-05 20:18:06 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fadb9da1b20>
2022-08-05 20:18:06 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fadb9da1b80>
2022-08-05 20:18:06 - batch_size: 256
2022-08-05 20:18:06 - num_workers: 16
2022-08-05 20:18:06 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-05 20:18:06 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-05 20:18:06 - epochs: 25
2022-08-05 20:18:06 - print_interval: 100
2022-08-05 20:18:06 - accumulation_steps: 1
2022-08-05 20:18:06 - sync_bn: False
2022-08-05 20:18:06 - apex: True
2022-08-05 20:18:06 - use_ema_model: False
2022-08-05 20:18:06 - ema_model_decay: 0.9999
2022-08-05 20:18:06 - log_dir: ./log
2022-08-05 20:18:06 - checkpoint_dir: ./checkpoints
2022-08-05 20:18:06 - gpus_type: NVIDIA RTX A5000
2022-08-05 20:18:06 - gpus_num: 2
2022-08-05 20:18:06 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fad965d05f0>
2022-08-05 20:18:06 - ema_model: None
2022-08-05 20:18:06 - --------------------parameters--------------------
2022-08-05 20:18:06 - name: conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.6.conv1.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.6.conv1.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.6.conv1.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.6.conv2.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.6.conv2.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.6.conv2.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: layer4.6.conv3.layer.0.weight, grad: True
2022-08-05 20:18:06 - name: layer4.6.conv3.layer.1.weight, grad: True
2022-08-05 20:18:06 - name: layer4.6.conv3.layer.1.bias, grad: True
2022-08-05 20:18:06 - name: fc.weight, grad: True
2022-08-05 20:18:06 - name: fc.bias, grad: True
2022-08-05 20:18:06 - --------------------buffers--------------------
2022-08-05 20:18:06 - name: conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.6.conv1.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.6.conv1.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.6.conv1.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.6.conv2.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.6.conv2.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.6.conv2.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - name: layer4.6.conv3.layer.1.running_mean, grad: False
2022-08-05 20:18:06 - name: layer4.6.conv3.layer.1.running_var, grad: False
2022-08-05 20:18:06 - name: layer4.6.conv3.layer.1.num_batches_tracked, grad: False
2022-08-05 20:18:06 - -----------no weight decay layers--------------
2022-08-05 20:18:06 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-05 20:18:06 - -------------weight decay layers---------------
2022-08-05 20:18:06 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.6.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.6.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: layer4.6.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-05 20:18:06 - epoch 001 lr: 0.100000
2022-08-05 20:18:45 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9087
2022-08-05 20:19:18 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.8873
2022-08-05 20:19:50 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8810
2022-08-05 20:20:22 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.8272
2022-08-05 20:20:55 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.7151
2022-08-05 20:21:28 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.6284
2022-08-05 20:22:01 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.6499
2022-08-05 20:22:34 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.4926
2022-08-05 20:23:07 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.3397
2022-08-05 20:23:39 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.4409
2022-08-05 20:24:12 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.3475
2022-08-05 20:24:45 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.2219
2022-08-05 20:25:17 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.1719
2022-08-05 20:25:50 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.0728
2022-08-05 20:26:24 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 6.0561
2022-08-05 20:26:56 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.0494
2022-08-05 20:27:29 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.8002
2022-08-05 20:28:02 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.8361
2022-08-05 20:28:34 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.7221
2022-08-05 20:29:07 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.5336
2022-08-05 20:29:40 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.5083
2022-08-05 20:30:13 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.5947
2022-08-05 20:30:45 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.3665
2022-08-05 20:31:18 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.4079
2022-08-05 20:31:51 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.2929
2022-08-05 20:32:23 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.2660
2022-08-05 20:32:56 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.3376
2022-08-05 20:33:28 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.2065
2022-08-05 20:34:01 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.0397
2022-08-05 20:34:34 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.1823
2022-08-05 20:35:07 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.2533
2022-08-05 20:35:39 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.0953
2022-08-05 20:36:12 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.8442
2022-08-05 20:36:45 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.9510
2022-08-05 20:37:17 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.7937
2022-08-05 20:37:50 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.8678
2022-08-05 20:38:22 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.9394
2022-08-05 20:38:55 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.6555
2022-08-05 20:39:28 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.6678
2022-08-05 20:40:01 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.8035
2022-08-05 20:40:33 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.7961
2022-08-05 20:41:06 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.6412
2022-08-05 20:41:39 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.5973
2022-08-05 20:42:11 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.2580
2022-08-05 20:42:44 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.4890
2022-08-05 20:43:17 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.7528
2022-08-05 20:43:50 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.3195
2022-08-05 20:44:23 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.4880
2022-08-05 20:44:56 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.3820
2022-08-05 20:45:28 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.3202
2022-08-05 20:45:29 - train: epoch 001, train_loss: 5.4772
2022-08-05 20:46:43 - eval: epoch: 001, acc1: 16.260%, acc5: 37.012%, test_loss: 4.3179, per_image_load_time: 2.232ms, per_image_inference_time: 0.598ms
2022-08-05 20:46:43 - until epoch: 001, best_acc1: 16.260%
2022-08-05 20:46:43 - epoch 002 lr: 0.099606
2022-08-05 20:47:22 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.1796
2022-08-05 20:47:55 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.1526
2022-08-05 20:48:27 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.2573
2022-08-05 20:48:59 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.3413
2022-08-05 20:49:32 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.3045
2022-08-05 20:50:04 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 3.9765
2022-08-05 20:50:37 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.4406
2022-08-05 20:51:10 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 3.9349
2022-08-05 20:51:43 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.8666
2022-08-05 20:52:16 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.2181
2022-08-05 20:52:49 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.1118
2022-08-05 20:53:22 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.9356
2022-08-05 20:53:55 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 4.0252
2022-08-05 20:54:28 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.0768
2022-08-05 20:55:01 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 4.0278
2022-08-05 20:55:34 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.7234
2022-08-05 20:56:07 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.8164
2022-08-05 20:56:40 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.9355
2022-08-05 20:57:13 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.7309
2022-08-05 20:57:46 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.5484
2022-08-05 20:58:19 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.8429
2022-08-05 20:58:52 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.7822
2022-08-05 20:59:25 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.8845
2022-08-05 20:59:58 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.5782
2022-08-05 21:00:30 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.5653
2022-08-05 21:01:03 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.7232
2022-08-05 21:01:36 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.7499
2022-08-05 21:02:08 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.8828
2022-08-05 21:02:41 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.7201
2022-08-05 21:03:14 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.6940
2022-08-05 21:03:46 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.5053
2022-08-05 21:04:19 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.7157
2022-08-05 21:04:52 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.5742
2022-08-05 21:05:24 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.5750
2022-08-05 21:05:57 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.5184
2022-08-05 21:06:30 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.5998
2022-08-05 21:07:03 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.7138
2022-08-05 21:07:36 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.3661
2022-08-05 21:08:10 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.6318
2022-08-05 21:08:43 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.2970
2022-08-05 21:09:16 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.6041
2022-08-05 21:09:48 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.4113
2022-08-05 21:10:21 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.4245
2022-08-05 21:10:54 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.1571
2022-08-05 21:11:27 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.2631
2022-08-05 21:11:59 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.0817
2022-08-05 21:12:32 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.5238
2022-08-05 21:13:05 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.4486
2022-08-05 21:13:38 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.3037
2022-08-05 21:14:10 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.4276
2022-08-05 21:14:12 - train: epoch 002, train_loss: 3.7398
2022-08-05 21:15:25 - eval: epoch: 002, acc1: 31.430%, acc5: 57.842%, test_loss: 3.2272, per_image_load_time: 2.002ms, per_image_inference_time: 0.578ms
2022-08-05 21:15:25 - until epoch: 002, best_acc1: 31.430%
2022-08-05 21:15:25 - epoch 003 lr: 0.098429
2022-08-05 21:16:04 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.3912
2022-08-05 21:16:36 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.4011
2022-08-05 21:17:08 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.2253
2022-08-05 21:17:41 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.1104
2022-08-05 21:18:13 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.3446
2022-08-05 21:18:46 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 2.9927
2022-08-05 21:19:18 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.3066
2022-08-05 21:19:51 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.5127
2022-08-05 21:20:23 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.3104
2022-08-05 21:20:56 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.2883
2022-08-05 21:21:28 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 2.9318
2022-08-05 21:22:01 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.1621
2022-08-05 21:22:33 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.1012
2022-08-05 21:23:06 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.0982
2022-08-05 21:23:38 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.3554
2022-08-05 21:24:11 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.1321
2022-08-05 21:24:43 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.1009
2022-08-05 21:25:16 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.1042
2022-08-05 21:25:49 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.2493
2022-08-05 21:26:21 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.0562
2022-08-05 21:26:54 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.1035
2022-08-05 21:27:26 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.3291
2022-08-05 21:27:59 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.9994
2022-08-05 21:28:32 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.0849
2022-08-05 21:29:04 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.1118
2022-08-05 21:29:37 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.1823
2022-08-05 21:30:10 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.2468
2022-08-05 21:30:43 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.0992
2022-08-05 21:31:16 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.9767
2022-08-05 21:31:49 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.1306
2022-08-05 21:32:21 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.3033
2022-08-05 21:32:54 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 2.8503
2022-08-05 21:33:27 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0482
2022-08-05 21:33:59 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.2110
2022-08-05 21:34:32 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.7474
2022-08-05 21:35:05 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.9879
2022-08-05 21:35:38 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.0749
2022-08-05 21:36:11 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.1553
2022-08-05 21:36:44 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.3190
2022-08-05 21:37:17 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7463
2022-08-05 21:37:49 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.7939
2022-08-05 21:38:22 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 2.9769
2022-08-05 21:38:55 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 3.0187
2022-08-05 21:39:27 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 3.0283
2022-08-05 21:40:00 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.1042
2022-08-05 21:40:33 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.8628
2022-08-05 21:41:05 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 2.9094
2022-08-05 21:41:38 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.0882
2022-08-05 21:42:11 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.1456
2022-08-05 21:42:43 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.8641
2022-08-05 21:42:44 - train: epoch 003, train_loss: 3.1046
2022-08-05 21:43:57 - eval: epoch: 003, acc1: 39.724%, acc5: 66.318%, test_loss: 2.7372, per_image_load_time: 2.201ms, per_image_inference_time: 0.572ms
2022-08-05 21:43:58 - until epoch: 003, best_acc1: 39.724%
2022-08-05 21:43:58 - epoch 004 lr: 0.096488
2022-08-05 21:44:36 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 3.0814
2022-08-05 21:45:08 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.6556
2022-08-05 21:45:40 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.9276
2022-08-05 21:46:12 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.9043
2022-08-05 21:46:45 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.7942
2022-08-05 21:47:17 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.2201
2022-08-05 21:47:50 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.9728
2022-08-05 21:48:22 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.6120
2022-08-05 21:48:55 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.6379
2022-08-05 21:49:28 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.7210
2022-08-05 21:50:00 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 2.8913
2022-08-05 21:50:33 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.6241
2022-08-05 21:51:06 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.7795
2022-08-05 21:51:38 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.7979
2022-08-05 21:52:11 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.8681
2022-08-05 21:52:44 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.7201
2022-08-05 21:53:17 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.8792
2022-08-05 21:53:50 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 3.0003
2022-08-05 21:54:23 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.7730
2022-08-05 21:54:57 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.9210
2022-08-05 21:55:29 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 3.0837
2022-08-05 21:56:03 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.8586
2022-08-05 21:56:36 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.6469
2022-08-05 21:57:08 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.6589
2022-08-05 21:57:41 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.6045
2022-08-05 21:58:14 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.7470
2022-08-05 21:58:47 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.7288
2022-08-05 21:59:20 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.9400
2022-08-05 21:59:53 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.7728
2022-08-05 22:00:26 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.8970
2022-08-05 22:00:59 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.6778
2022-08-05 22:01:32 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.7480
2022-08-05 22:02:04 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.9730
2022-08-05 22:02:37 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.8048
2022-08-05 22:03:10 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.8010
2022-08-05 22:03:42 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.7026
2022-08-05 22:04:15 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.7840
2022-08-05 22:04:48 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.6052
2022-08-05 22:05:21 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.7695
2022-08-05 22:05:54 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.5048
2022-08-05 22:06:27 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.7986
2022-08-05 22:07:00 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6317
2022-08-05 22:07:33 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.6257
2022-08-05 22:08:06 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.5366
2022-08-05 22:08:39 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.1837
2022-08-05 22:09:11 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.7955
2022-08-05 22:09:44 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.7754
2022-08-05 22:10:17 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.8536
2022-08-05 22:10:49 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.8032
2022-08-05 22:11:21 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.7887
2022-08-05 22:11:23 - train: epoch 004, train_loss: 2.7908
2022-08-05 22:12:37 - eval: epoch: 004, acc1: 46.008%, acc5: 72.502%, test_loss: 2.3845, per_image_load_time: 2.248ms, per_image_inference_time: 0.557ms
2022-08-05 22:12:37 - until epoch: 004, best_acc1: 46.008%
2022-08-05 22:12:37 - epoch 005 lr: 0.093815
2022-08-05 22:13:16 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.6298
2022-08-05 22:13:48 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.8398
2022-08-05 22:14:20 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.8890
2022-08-05 22:14:53 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.5405
2022-08-05 22:15:25 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.5825
2022-08-05 22:15:57 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.7385
2022-08-05 22:16:30 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.7072
2022-08-05 22:17:03 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.8523
2022-08-05 22:17:35 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.5738
2022-08-05 22:18:08 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.6396
2022-08-05 22:18:40 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.5392
2022-08-05 22:19:13 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.6722
2022-08-05 22:19:46 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.6436
2022-08-05 22:20:18 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.5496
2022-08-05 22:20:51 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.4136
2022-08-05 22:21:23 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.4688
2022-08-05 22:21:56 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.5749
2022-08-05 22:22:29 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.5753
2022-08-05 22:23:02 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.4798
2022-08-05 22:23:35 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.6157
2022-08-05 22:24:08 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.4369
2022-08-05 22:24:41 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.4418
2022-08-05 22:25:14 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.3605
2022-08-05 22:25:47 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5654
2022-08-05 22:26:20 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.4122
2022-08-05 22:26:53 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.6544
2022-08-05 22:27:26 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.7151
2022-08-05 22:27:59 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.5880
2022-08-05 22:28:32 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.5802
2022-08-05 22:29:05 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.4295
2022-08-05 22:29:38 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.6481
2022-08-05 22:30:11 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.4839
2022-08-05 22:30:44 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.5072
2022-08-05 22:31:17 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.4159
2022-08-05 22:31:49 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.5602
2022-08-05 22:32:22 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6611
2022-08-05 22:32:55 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.4494
2022-08-05 22:33:28 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5202
2022-08-05 22:34:02 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.8612
2022-08-05 22:34:34 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.4539
2022-08-05 22:35:07 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.4100
2022-08-05 22:35:40 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.7794
2022-08-05 22:36:13 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.8218
2022-08-05 22:36:46 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.5719
2022-08-05 22:37:19 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.5397
2022-08-05 22:37:52 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.6535
2022-08-05 22:38:25 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.2348
2022-08-05 22:38:58 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.3754
2022-08-05 22:39:30 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.7262
2022-08-05 22:40:02 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.7364
2022-08-05 22:40:04 - train: epoch 005, train_loss: 2.5991
2022-08-05 22:41:17 - eval: epoch: 005, acc1: 47.936%, acc5: 74.182%, test_loss: 2.2832, per_image_load_time: 2.220ms, per_image_inference_time: 0.587ms
2022-08-05 22:41:17 - until epoch: 005, best_acc1: 47.936%
2022-08-05 22:41:17 - epoch 006 lr: 0.090450
2022-08-05 22:41:57 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.4378
2022-08-05 22:42:29 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.5955
2022-08-05 22:43:01 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.6236
2022-08-05 22:43:34 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.4986
2022-08-05 22:44:06 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.4478
2022-08-05 22:44:39 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.4773
2022-08-05 22:45:11 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.5027
2022-08-05 22:45:43 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.6169
2022-08-05 22:46:16 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3836
2022-08-05 22:46:48 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.3791
2022-08-05 22:47:21 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.5165
2022-08-05 22:47:54 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.6374
2022-08-05 22:48:26 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5254
2022-08-05 22:48:59 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.4525
2022-08-05 22:49:32 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.6722
2022-08-05 22:50:04 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.2319
2022-08-05 22:50:37 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.4534
2022-08-05 22:51:10 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.6295
2022-08-05 22:51:43 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.5089
2022-08-05 22:52:16 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.5983
2022-08-05 22:52:49 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.4764
2022-08-05 22:53:22 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.3008
2022-08-05 22:53:55 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.4760
2022-08-05 22:54:28 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.5385
2022-08-05 22:55:01 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.3347
2022-08-05 22:55:34 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.4053
2022-08-05 22:56:06 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.6376
2022-08-05 22:56:40 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.1922
2022-08-05 22:57:12 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.5892
2022-08-05 22:57:45 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.5830
2022-08-05 22:58:18 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.2632
2022-08-05 22:58:51 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.6086
2022-08-05 22:59:24 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.4841
2022-08-05 22:59:56 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.4755
2022-08-05 23:00:29 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.6150
2022-08-05 23:01:02 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.5895
2022-08-05 23:01:34 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.3176
2022-08-05 23:02:07 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.2164
2022-08-05 23:02:40 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.4520
2022-08-05 23:03:13 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.6011
2022-08-05 23:03:46 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.4895
2022-08-05 23:04:18 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.3760
2022-08-05 23:04:51 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.5483
2022-08-05 23:05:24 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.3741
2022-08-05 23:05:56 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.7018
2022-08-05 23:06:29 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.4740
2022-08-05 23:07:02 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4231
2022-08-05 23:07:35 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.4894
2022-08-05 23:08:07 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.5603
2022-08-05 23:08:39 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.3509
2022-08-05 23:08:41 - train: epoch 006, train_loss: 2.4673
2022-08-05 23:09:54 - eval: epoch: 006, acc1: 49.438%, acc5: 75.224%, test_loss: 2.2247, per_image_load_time: 2.162ms, per_image_inference_time: 0.594ms
2022-08-05 23:09:55 - until epoch: 006, best_acc1: 49.438%
2022-08-05 23:09:55 - epoch 007 lr: 0.086448
2022-08-05 23:10:34 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.3315
2022-08-05 23:11:05 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.6202
2022-08-05 23:11:37 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.6605
2022-08-05 23:12:09 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.5175
2022-08-05 23:12:42 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.3221
2022-08-05 23:13:14 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.4114
2022-08-05 23:13:46 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.1063
2022-08-05 23:14:19 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.4326
2022-08-05 23:14:51 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.3312
2022-08-05 23:15:24 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.3932
2022-08-05 23:15:56 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.4249
2022-08-05 23:16:29 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.3197
2022-08-05 23:17:01 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.1303
2022-08-05 23:17:34 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.5703
2022-08-05 23:18:07 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.3742
2022-08-05 23:18:40 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.4113
2022-08-05 23:19:12 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.3419
2022-08-05 23:19:45 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.5805
2022-08-05 23:20:18 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.3987
2022-08-05 23:20:50 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.0585
2022-08-05 23:21:23 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.5358
2022-08-05 23:21:56 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.3436
2022-08-05 23:22:29 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.3960
2022-08-05 23:23:01 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.4323
2022-08-05 23:23:34 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.3547
2022-08-05 23:24:07 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.3734
2022-08-05 23:24:40 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.2090
2022-08-05 23:25:13 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.2800
2022-08-05 23:25:45 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.2181
2022-08-05 23:26:18 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.3819
2022-08-05 23:26:51 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.2717
2022-08-05 23:27:24 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.3117
2022-08-05 23:27:57 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.4767
2022-08-05 23:28:30 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.2110
2022-08-05 23:29:03 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.4036
2022-08-05 23:29:36 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.1605
2022-08-05 23:30:09 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.3379
2022-08-05 23:30:41 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.3044
2022-08-05 23:31:14 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.2421
2022-08-05 23:31:47 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.5384
2022-08-05 23:32:20 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.0891
2022-08-05 23:32:53 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.3119
2022-08-05 23:33:25 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.5244
2022-08-05 23:33:58 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.2468
2022-08-05 23:34:31 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.3222
2022-08-05 23:35:04 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.4638
2022-08-05 23:35:37 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.3298
2022-08-05 23:36:10 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.5056
2022-08-05 23:36:43 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.4331
2022-08-05 23:37:15 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.2470
2022-08-05 23:37:16 - train: epoch 007, train_loss: 2.3662
2022-08-05 23:38:30 - eval: epoch: 007, acc1: 50.056%, acc5: 75.906%, test_loss: 2.1731, per_image_load_time: 2.228ms, per_image_inference_time: 0.608ms
2022-08-05 23:38:30 - until epoch: 007, best_acc1: 50.056%
2022-08-05 23:38:30 - epoch 008 lr: 0.081870
2022-08-05 23:39:09 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.2078
2022-08-05 23:39:40 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.3349
2022-08-05 23:40:12 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.3926
2022-08-05 23:40:44 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.2756
2022-08-05 23:41:16 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.2590
2022-08-05 23:41:48 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.1389
2022-08-05 23:42:20 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.2407
2022-08-05 23:42:53 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.0442
2022-08-05 23:43:26 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.2352
2022-08-05 23:43:58 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.3754
2022-08-05 23:44:31 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.4201
2022-08-05 23:45:03 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.1837
2022-08-05 23:45:36 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.4914
2022-08-05 23:46:09 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.1968
2022-08-05 23:46:42 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.3840
2022-08-05 23:47:14 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.4006
2022-08-05 23:47:47 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.2799
2022-08-05 23:48:19 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.3485
2022-08-05 23:48:52 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.1472
2022-08-05 23:49:25 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.3359
2022-08-05 23:49:58 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.2879
2022-08-05 23:50:30 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.2954
2022-08-05 23:51:03 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.1359
2022-08-05 23:51:36 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.1211
2022-08-05 23:52:08 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.2275
2022-08-05 23:52:41 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.1683
2022-08-05 23:53:14 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.4182
2022-08-05 23:53:46 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.3347
2022-08-05 23:54:19 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.2401
2022-08-05 23:54:52 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.4059
2022-08-05 23:55:25 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.3211
2022-08-05 23:55:58 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.4790
2022-08-05 23:56:30 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.4507
2022-08-05 23:57:03 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.2862
2022-08-05 23:57:36 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.3638
2022-08-05 23:58:08 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.2073
2022-08-05 23:58:41 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.2577
2022-08-05 23:59:14 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.3041
2022-08-05 23:59:47 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.2683
2022-08-06 00:00:20 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.4749
2022-08-06 00:00:53 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.1067
2022-08-06 00:01:26 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.1992
2022-08-06 00:01:59 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.0016
2022-08-06 00:02:32 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.2330
2022-08-06 00:03:04 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.2017
2022-08-06 00:03:37 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.1907
2022-08-06 00:04:10 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2936
2022-08-06 00:04:42 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.2226
2022-08-06 00:05:15 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.3572
2022-08-06 00:05:47 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.2417
2022-08-06 00:05:49 - train: epoch 008, train_loss: 2.2835
2022-08-06 00:07:03 - eval: epoch: 008, acc1: 53.258%, acc5: 78.370%, test_loss: 2.0173, per_image_load_time: 2.235ms, per_image_inference_time: 0.565ms
2022-08-06 00:07:03 - until epoch: 008, best_acc1: 53.258%
2022-08-06 00:07:03 - epoch 009 lr: 0.076790
2022-08-06 00:07:41 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 2.0099
2022-08-06 00:08:13 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.2198
2022-08-06 00:08:45 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.1001
2022-08-06 00:09:17 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.5353
2022-08-06 00:09:49 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.1079
2022-08-06 00:10:22 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.1813
2022-08-06 00:10:55 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.1888
2022-08-06 00:11:27 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.1797
2022-08-06 00:12:00 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.2204
2022-08-06 00:12:33 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.2566
2022-08-06 00:13:06 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.3709
2022-08-06 00:13:38 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.2713
2022-08-06 00:14:11 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.3670
2022-08-06 00:14:44 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 2.0769
2022-08-06 00:15:17 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.1569
2022-08-06 00:15:49 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.1672
2022-08-06 00:16:21 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.1854
2022-08-06 00:16:54 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.1516
2022-08-06 00:17:26 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.1158
2022-08-06 00:17:59 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.0537
2022-08-06 00:18:31 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.2840
2022-08-06 00:19:04 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.2872
2022-08-06 00:19:37 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.1732
2022-08-06 00:20:10 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.2403
2022-08-06 00:20:43 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.2339
2022-08-06 00:21:15 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.5101
2022-08-06 00:21:48 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.0926
2022-08-06 00:22:21 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2717
2022-08-06 00:22:54 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.0549
2022-08-06 00:23:26 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1145
2022-08-06 00:23:59 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.1862
2022-08-06 00:24:32 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.2530
2022-08-06 00:25:05 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.1249
2022-08-06 00:25:38 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.4523
2022-08-06 00:26:11 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.5103
2022-08-06 00:26:44 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.1008
2022-08-06 00:27:16 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.4189
2022-08-06 00:27:49 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.3167
2022-08-06 00:28:22 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 1.9560
2022-08-06 00:28:55 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.2654
2022-08-06 00:29:28 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.1817
2022-08-06 00:30:01 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.0620
2022-08-06 00:30:35 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.1055
2022-08-06 00:31:08 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.2290
2022-08-06 00:31:41 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.3457
2022-08-06 00:32:14 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.2622
2022-08-06 00:32:47 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.1672
2022-08-06 00:33:21 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.1478
2022-08-06 00:33:54 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.1575
2022-08-06 00:34:26 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.1046
2022-08-06 00:34:27 - train: epoch 009, train_loss: 2.2131
2022-08-06 00:35:41 - eval: epoch: 009, acc1: 53.322%, acc5: 78.482%, test_loss: 2.0258, per_image_load_time: 2.148ms, per_image_inference_time: 0.578ms
2022-08-06 00:35:41 - until epoch: 009, best_acc1: 53.322%
2022-08-06 00:35:41 - epoch 010 lr: 0.071288
2022-08-06 00:36:20 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 1.9878
2022-08-06 00:36:52 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.2775
2022-08-06 00:37:24 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.2683
2022-08-06 00:37:56 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.4313
2022-08-06 00:38:29 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.1610
2022-08-06 00:39:01 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.3431
2022-08-06 00:39:33 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.2626
2022-08-06 00:40:06 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.0851
2022-08-06 00:40:38 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.1614
2022-08-06 00:41:10 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.0633
2022-08-06 00:41:43 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.0640
2022-08-06 00:42:15 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.0821
2022-08-06 00:42:48 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.0556
2022-08-06 00:43:20 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.2403
2022-08-06 00:43:53 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 2.1343
2022-08-06 00:44:25 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.1340
2022-08-06 00:44:58 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.1743
2022-08-06 00:45:31 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 2.0268
2022-08-06 00:46:03 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.1199
2022-08-06 00:46:36 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.3290
2022-08-06 00:47:08 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 1.7310
2022-08-06 00:47:41 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.3200
2022-08-06 00:48:14 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.3686
2022-08-06 00:48:46 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.2356
2022-08-06 00:49:19 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.3445
2022-08-06 00:49:52 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.1941
2022-08-06 00:50:24 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.8987
2022-08-06 00:50:57 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.1877
2022-08-06 00:51:30 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.1833
2022-08-06 00:52:02 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1075
2022-08-06 00:52:35 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.2884
2022-08-06 00:53:08 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.1785
2022-08-06 00:53:40 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.2739
2022-08-06 00:54:12 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.2326
2022-08-06 00:54:45 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.4651
2022-08-06 00:55:18 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.2773
2022-08-06 00:55:50 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.0061
2022-08-06 00:56:23 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.0251
2022-08-06 00:56:56 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 2.0585
2022-08-06 00:57:28 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.1373
2022-08-06 00:58:00 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.0396
2022-08-06 00:58:33 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.1533
2022-08-06 00:59:05 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 1.9797
2022-08-06 00:59:38 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 1.9489
2022-08-06 01:00:10 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 1.9756
2022-08-06 01:00:43 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.0919
2022-08-06 01:01:15 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1818
2022-08-06 01:01:48 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 1.9282
2022-08-06 01:02:21 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.1401
2022-08-06 01:02:53 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.0832
2022-08-06 01:02:54 - train: epoch 010, train_loss: 2.1517
2022-08-06 01:04:07 - eval: epoch: 010, acc1: 56.896%, acc5: 81.538%, test_loss: 1.8258, per_image_load_time: 1.842ms, per_image_inference_time: 0.590ms
2022-08-06 01:04:07 - until epoch: 010, best_acc1: 56.896%
2022-08-06 01:04:07 - epoch 011 lr: 0.065450
2022-08-06 01:04:46 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.0481
2022-08-06 01:05:18 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.2345
2022-08-06 01:05:50 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.2815
2022-08-06 01:06:23 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.1859
2022-08-06 01:06:55 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 1.9658
2022-08-06 01:07:28 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.1038
2022-08-06 01:08:00 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.3765
2022-08-06 01:08:33 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 1.9910
2022-08-06 01:09:05 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.3560
2022-08-06 01:09:38 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.1062
2022-08-06 01:10:10 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.3859
2022-08-06 01:10:43 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.1470
2022-08-06 01:11:15 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.2916
2022-08-06 01:11:48 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.2585
2022-08-06 01:12:21 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 1.9252
2022-08-06 01:12:54 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.2571
2022-08-06 01:13:27 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.1939
2022-08-06 01:14:00 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.0287
2022-08-06 01:14:32 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.0928
2022-08-06 01:15:05 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1041
2022-08-06 01:15:38 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 1.9144
2022-08-06 01:16:10 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 2.0928
2022-08-06 01:16:43 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.1511
2022-08-06 01:17:16 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.8435
2022-08-06 01:17:48 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 1.9276
2022-08-06 01:18:21 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.0871
2022-08-06 01:18:54 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 1.9390
2022-08-06 01:19:27 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 2.0000
2022-08-06 01:20:00 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.1859
2022-08-06 01:20:32 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.3255
2022-08-06 01:21:05 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.0786
2022-08-06 01:21:38 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.1503
2022-08-06 01:22:11 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 1.9972
2022-08-06 01:22:44 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 1.9327
2022-08-06 01:23:16 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.1271
2022-08-06 01:23:49 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 1.9566
2022-08-06 01:24:22 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.3988
2022-08-06 01:24:54 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.8815
2022-08-06 01:25:27 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 1.9982
2022-08-06 01:26:00 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 1.9706
2022-08-06 01:26:33 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.9522
2022-08-06 01:27:05 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 1.9626
2022-08-06 01:27:38 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.0853
2022-08-06 01:28:11 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.0350
2022-08-06 01:28:44 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 1.9121
2022-08-06 01:29:16 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 2.0269
2022-08-06 01:29:49 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.8460
2022-08-06 01:30:22 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.8527
2022-08-06 01:30:55 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 1.8718
2022-08-06 01:31:28 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 1.9842
2022-08-06 01:31:29 - train: epoch 011, train_loss: 2.0922
2022-08-06 01:32:42 - eval: epoch: 011, acc1: 57.976%, acc5: 82.356%, test_loss: 1.7565, per_image_load_time: 2.187ms, per_image_inference_time: 0.587ms
2022-08-06 01:32:43 - until epoch: 011, best_acc1: 57.976%
2022-08-06 01:32:43 - epoch 012 lr: 0.059368
2022-08-06 01:33:21 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.7820
2022-08-06 01:33:53 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.8758
2022-08-06 01:34:26 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 1.8829
2022-08-06 01:34:58 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 1.8688
2022-08-06 01:35:30 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.1423
2022-08-06 01:36:03 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.7052
2022-08-06 01:36:36 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.7653
2022-08-06 01:37:09 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.2585
2022-08-06 01:37:42 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.1042
2022-08-06 01:38:14 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9438
2022-08-06 01:38:47 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.2230
2022-08-06 01:39:20 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.8457
2022-08-06 01:39:52 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 2.0560
2022-08-06 01:40:25 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 1.9814
2022-08-06 01:40:58 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 2.0764
2022-08-06 01:41:31 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 2.1339
2022-08-06 01:42:03 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.6212
2022-08-06 01:42:36 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.0777
2022-08-06 01:43:09 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1565
2022-08-06 01:43:42 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.2393
2022-08-06 01:44:15 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.1650
2022-08-06 01:44:48 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.0025
2022-08-06 01:45:21 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 1.9569
2022-08-06 01:45:53 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.1560
2022-08-06 01:46:26 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 2.0907
2022-08-06 01:46:59 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 2.0802
2022-08-06 01:47:32 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.0932
2022-08-06 01:48:05 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 2.0293
2022-08-06 01:48:38 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.1251
2022-08-06 01:49:11 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.8424
2022-08-06 01:49:44 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.1368
2022-08-06 01:50:16 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 2.0255
2022-08-06 01:50:49 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.1092
2022-08-06 01:51:22 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 1.9931
2022-08-06 01:51:55 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.1024
2022-08-06 01:52:28 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.7652
2022-08-06 01:53:00 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.0306
2022-08-06 01:53:33 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.0645
2022-08-06 01:54:06 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8043
2022-08-06 01:54:40 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 1.9614
2022-08-06 01:55:13 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 1.9334
2022-08-06 01:55:46 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.8929
2022-08-06 01:56:18 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.1762
2022-08-06 01:56:51 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.9009
2022-08-06 01:57:24 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.8617
2022-08-06 01:57:57 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.0536
2022-08-06 01:58:30 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.9463
2022-08-06 01:59:02 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.0816
2022-08-06 01:59:36 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.8413
2022-08-06 02:00:07 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.8465
2022-08-06 02:00:09 - train: epoch 012, train_loss: 2.0290
2022-08-06 02:01:22 - eval: epoch: 012, acc1: 58.332%, acc5: 82.236%, test_loss: 1.7608, per_image_load_time: 2.211ms, per_image_inference_time: 0.608ms
2022-08-06 02:01:22 - until epoch: 012, best_acc1: 58.332%
2022-08-06 02:01:22 - epoch 013 lr: 0.053138
2022-08-06 02:02:01 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.7103
2022-08-06 02:02:33 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.9065
2022-08-06 02:03:05 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8925
2022-08-06 02:03:37 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.9459
2022-08-06 02:04:10 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 1.9745
2022-08-06 02:04:42 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 2.1026
2022-08-06 02:05:15 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.9312
2022-08-06 02:05:47 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.0900
2022-08-06 02:06:20 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.9880
2022-08-06 02:06:53 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 2.0185
2022-08-06 02:07:26 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 1.8394
2022-08-06 02:07:58 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.1260
2022-08-06 02:08:31 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.8657
2022-08-06 02:09:04 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.1386
2022-08-06 02:09:37 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.2483
2022-08-06 02:10:09 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.8430
2022-08-06 02:10:42 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.1553
2022-08-06 02:11:15 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9362
2022-08-06 02:11:47 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.0775
2022-08-06 02:12:20 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 1.9890
2022-08-06 02:12:53 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.1812
2022-08-06 02:13:25 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.7370
2022-08-06 02:13:59 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 2.1612
2022-08-06 02:14:31 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 1.9905
2022-08-06 02:15:04 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.9308
2022-08-06 02:15:37 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.7530
2022-08-06 02:16:10 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.9100
2022-08-06 02:16:43 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.1609
2022-08-06 02:17:16 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 1.9704
2022-08-06 02:17:48 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.9404
2022-08-06 02:18:21 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.7975
2022-08-06 02:18:54 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 2.0417
2022-08-06 02:19:26 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.7988
2022-08-06 02:19:59 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.9877
2022-08-06 02:20:31 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.9206
2022-08-06 02:21:04 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.1347
2022-08-06 02:21:37 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.7795
2022-08-06 02:22:09 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1415
2022-08-06 02:22:42 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.1917
2022-08-06 02:23:14 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 2.0172
2022-08-06 02:23:47 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.8419
2022-08-06 02:24:20 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 1.8983
2022-08-06 02:24:53 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.8774
2022-08-06 02:25:26 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 2.0630
2022-08-06 02:25:58 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.8130
2022-08-06 02:26:31 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.0314
2022-08-06 02:27:04 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.8294
2022-08-06 02:27:37 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.0672
2022-08-06 02:28:10 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0900
2022-08-06 02:28:42 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.1796
2022-08-06 02:28:43 - train: epoch 013, train_loss: 1.9686
2022-08-06 02:29:58 - eval: epoch: 013, acc1: 60.904%, acc5: 84.146%, test_loss: 1.6431, per_image_load_time: 2.251ms, per_image_inference_time: 0.586ms
2022-08-06 02:29:58 - until epoch: 013, best_acc1: 60.904%
2022-08-06 02:29:58 - epoch 014 lr: 0.046859
2022-08-06 02:30:37 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.6681
2022-08-06 02:31:09 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.8388
2022-08-06 02:31:42 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.6566
2022-08-06 02:32:14 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.8627
2022-08-06 02:32:47 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.7839
2022-08-06 02:33:19 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.8890
2022-08-06 02:33:52 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.7612
2022-08-06 02:34:25 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.7728
2022-08-06 02:34:57 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.8824
2022-08-06 02:35:30 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 1.8786
2022-08-06 02:36:02 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.9247
2022-08-06 02:36:35 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 2.1225
2022-08-06 02:37:08 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.0358
2022-08-06 02:37:41 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.9577
2022-08-06 02:38:13 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.0100
2022-08-06 02:38:46 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.7830
2022-08-06 02:39:19 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9888
2022-08-06 02:39:52 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.0127
2022-08-06 02:40:25 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.9102
2022-08-06 02:40:58 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.7677
2022-08-06 02:41:31 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.1815
2022-08-06 02:42:03 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.7744
2022-08-06 02:42:36 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.8277
2022-08-06 02:43:09 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.9028
2022-08-06 02:43:42 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.7372
2022-08-06 02:44:15 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.9052
2022-08-06 02:44:48 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.7982
2022-08-06 02:45:20 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 2.0002
2022-08-06 02:45:53 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9284
2022-08-06 02:46:26 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.7974
2022-08-06 02:46:59 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.8298
2022-08-06 02:47:31 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 2.1655
2022-08-06 02:48:05 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 2.0453
2022-08-06 02:48:38 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 2.0970
2022-08-06 02:49:10 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.9100
2022-08-06 02:49:43 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.7516
2022-08-06 02:50:16 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.8189
2022-08-06 02:50:48 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 1.9321
2022-08-06 02:51:21 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 2.0395
2022-08-06 02:51:54 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.9030
2022-08-06 02:52:27 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.7040
2022-08-06 02:53:01 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.8630
2022-08-06 02:53:33 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.7117
2022-08-06 02:54:06 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.7748
2022-08-06 02:54:39 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.8542
2022-08-06 02:55:12 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.7543
2022-08-06 02:55:45 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.9639
2022-08-06 02:56:18 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.8461
2022-08-06 02:56:51 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.7831
2022-08-06 02:57:23 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.9477
2022-08-06 02:57:24 - train: epoch 014, train_loss: 1.9032
2022-08-06 02:58:38 - eval: epoch: 014, acc1: 61.406%, acc5: 84.328%, test_loss: 1.6143, per_image_load_time: 2.290ms, per_image_inference_time: 0.566ms
2022-08-06 02:58:39 - until epoch: 014, best_acc1: 61.406%
2022-08-06 02:58:39 - epoch 015 lr: 0.040630
2022-08-06 02:59:18 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.5510
2022-08-06 02:59:50 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.9961
2022-08-06 03:00:23 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 1.9312
2022-08-06 03:00:55 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.9703
2022-08-06 03:01:27 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.9865
2022-08-06 03:02:00 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 2.1016
2022-08-06 03:02:32 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.9954
2022-08-06 03:03:05 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.8366
2022-08-06 03:03:37 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.8479
2022-08-06 03:04:10 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.9875
2022-08-06 03:04:42 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7834
2022-08-06 03:05:14 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.8521
2022-08-06 03:05:47 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.9914
2022-08-06 03:06:20 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.6932
2022-08-06 03:06:52 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.7913
2022-08-06 03:07:25 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.8247
2022-08-06 03:07:58 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.8099
2022-08-06 03:08:30 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.7452
2022-08-06 03:09:03 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.7612
2022-08-06 03:09:36 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.7789
2022-08-06 03:10:09 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.6957
2022-08-06 03:10:42 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.8864
2022-08-06 03:11:15 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.7586
2022-08-06 03:11:48 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.9264
2022-08-06 03:12:20 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 2.0110
2022-08-06 03:12:53 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.7613
2022-08-06 03:13:26 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.9455
2022-08-06 03:13:59 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.9378
2022-08-06 03:14:32 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.9183
2022-08-06 03:15:04 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.7023
2022-08-06 03:15:37 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.8522
2022-08-06 03:16:10 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.9879
2022-08-06 03:16:43 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.6766
2022-08-06 03:17:16 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.6458
2022-08-06 03:17:48 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0379
2022-08-06 03:18:21 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.6496
2022-08-06 03:18:53 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6806
2022-08-06 03:19:26 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.8351
2022-08-06 03:19:59 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8419
2022-08-06 03:20:32 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.8140
2022-08-06 03:21:05 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.7000
2022-08-06 03:21:38 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6532
2022-08-06 03:22:11 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.6886
2022-08-06 03:22:44 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.9157
2022-08-06 03:23:16 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.8519
2022-08-06 03:23:50 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.9061
2022-08-06 03:24:22 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.9042
2022-08-06 03:24:55 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.6651
2022-08-06 03:25:28 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 2.0051
2022-08-06 03:26:00 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.9242
2022-08-06 03:26:02 - train: epoch 015, train_loss: 1.8457
2022-08-06 03:27:16 - eval: epoch: 015, acc1: 62.588%, acc5: 85.444%, test_loss: 1.5403, per_image_load_time: 2.052ms, per_image_inference_time: 0.593ms
2022-08-06 03:27:16 - until epoch: 015, best_acc1: 62.588%
2022-08-06 03:27:16 - epoch 016 lr: 0.034548
2022-08-06 03:27:55 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.6612
2022-08-06 03:28:26 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.6468
2022-08-06 03:28:59 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.6870
2022-08-06 03:29:30 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 1.8798
2022-08-06 03:30:03 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.7073
2022-08-06 03:30:35 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.7774
2022-08-06 03:31:08 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.6628
2022-08-06 03:31:41 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.7509
2022-08-06 03:32:13 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.7554
2022-08-06 03:32:46 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.7318
2022-08-06 03:33:18 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.5900
2022-08-06 03:33:51 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.7246
2022-08-06 03:34:23 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.8623
2022-08-06 03:34:56 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.7639
2022-08-06 03:35:28 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 2.0836
2022-08-06 03:36:01 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.7873
2022-08-06 03:36:33 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.9428
2022-08-06 03:37:06 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 2.0009
2022-08-06 03:37:38 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.6102
2022-08-06 03:38:11 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.6147
2022-08-06 03:38:44 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.9059
2022-08-06 03:39:16 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.8550
2022-08-06 03:39:49 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.6853
2022-08-06 03:40:21 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.7274
2022-08-06 03:40:54 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.8285
2022-08-06 03:41:27 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.9265
2022-08-06 03:42:00 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.7117
2022-08-06 03:42:32 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.8038
2022-08-06 03:43:05 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.5756
2022-08-06 03:43:38 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 2.0855
2022-08-06 03:44:10 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.7424
2022-08-06 03:44:43 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.9173
2022-08-06 03:45:16 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 2.0718
2022-08-06 03:45:49 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.6713
2022-08-06 03:46:21 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.8829
2022-08-06 03:46:54 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.7018
2022-08-06 03:47:27 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.8117
2022-08-06 03:48:00 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.0828
2022-08-06 03:48:33 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.7826
2022-08-06 03:49:05 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.9234
2022-08-06 03:49:38 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.8463
2022-08-06 03:50:11 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.6896
2022-08-06 03:50:44 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.8102
2022-08-06 03:51:17 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.6722
2022-08-06 03:51:50 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 2.0189
2022-08-06 03:52:23 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.4998
2022-08-06 03:52:55 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.8385
2022-08-06 03:53:28 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.7856
2022-08-06 03:54:01 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.9534
2022-08-06 03:54:33 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.7545
2022-08-06 03:54:34 - train: epoch 016, train_loss: 1.7759
2022-08-06 03:55:48 - eval: epoch: 016, acc1: 64.190%, acc5: 86.360%, test_loss: 1.4719, per_image_load_time: 1.796ms, per_image_inference_time: 0.578ms
2022-08-06 03:55:48 - until epoch: 016, best_acc1: 64.190%
2022-08-06 03:55:48 - epoch 017 lr: 0.028710
2022-08-06 03:56:27 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.6545
2022-08-06 03:56:59 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.8059
2022-08-06 03:57:31 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 2.0224
2022-08-06 03:58:03 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.4327
2022-08-06 03:58:35 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.8908
2022-08-06 03:59:08 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 1.9650
2022-08-06 03:59:41 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.8387
2022-08-06 04:00:13 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.6954
2022-08-06 04:00:46 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.5317
2022-08-06 04:01:19 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.7691
2022-08-06 04:01:52 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.9322
2022-08-06 04:02:24 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.7369
2022-08-06 04:02:57 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.7888
2022-08-06 04:03:30 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.6590
2022-08-06 04:04:03 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.5749
2022-08-06 04:04:35 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.6084
2022-08-06 04:05:08 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.5698
2022-08-06 04:05:41 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.7518
2022-08-06 04:06:13 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.7038
2022-08-06 04:06:46 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.6664
2022-08-06 04:07:19 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.6182
2022-08-06 04:07:51 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.3955
2022-08-06 04:08:24 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.6056
2022-08-06 04:08:56 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.6440
2022-08-06 04:09:29 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.7842
2022-08-06 04:10:02 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.5125
2022-08-06 04:10:34 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6003
2022-08-06 04:11:07 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.9105
2022-08-06 04:11:39 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.8464
2022-08-06 04:12:12 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.6335
2022-08-06 04:12:45 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8096
2022-08-06 04:13:18 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.6258
2022-08-06 04:13:50 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.9397
2022-08-06 04:14:23 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.6764
2022-08-06 04:14:56 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.6037
2022-08-06 04:15:29 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.7084
2022-08-06 04:16:02 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.6604
2022-08-06 04:16:34 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.7043
2022-08-06 04:17:07 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.5442
2022-08-06 04:17:40 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.4746
2022-08-06 04:18:13 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.9071
2022-08-06 04:18:45 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.6873
2022-08-06 04:19:18 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.6657
2022-08-06 04:19:51 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 2.0197
2022-08-06 04:20:23 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.7599
2022-08-06 04:20:56 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.5475
2022-08-06 04:21:29 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.8758
2022-08-06 04:22:02 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.8175
2022-08-06 04:22:35 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.5358
2022-08-06 04:23:07 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.3695
2022-08-06 04:23:08 - train: epoch 017, train_loss: 1.7076
2022-08-06 04:24:22 - eval: epoch: 017, acc1: 64.982%, acc5: 86.312%, test_loss: 1.4563, per_image_load_time: 2.133ms, per_image_inference_time: 0.572ms
2022-08-06 04:24:22 - until epoch: 017, best_acc1: 64.982%
2022-08-06 04:24:22 - epoch 018 lr: 0.023208
2022-08-06 04:25:00 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.6940
2022-08-06 04:25:32 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.7584
2022-08-06 04:26:05 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.7932
2022-08-06 04:26:37 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.7162
2022-08-06 04:27:10 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.7521
2022-08-06 04:27:43 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.7722
2022-08-06 04:28:15 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.4764
2022-08-06 04:28:48 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.7114
2022-08-06 04:29:21 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.6841
2022-08-06 04:29:54 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.5905
2022-08-06 04:30:26 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.6930
2022-08-06 04:30:59 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.6343
2022-08-06 04:31:32 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.7775
2022-08-06 04:32:04 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.6037
2022-08-06 04:32:37 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.7643
2022-08-06 04:33:09 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.4499
2022-08-06 04:33:42 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.6767
2022-08-06 04:34:15 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.5457
2022-08-06 04:34:48 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.5345
2022-08-06 04:35:21 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 2.2005
2022-08-06 04:35:54 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.8431
2022-08-06 04:36:27 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.8205
2022-08-06 04:36:59 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.7157
2022-08-06 04:37:32 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.5015
2022-08-06 04:38:04 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.5667
2022-08-06 04:38:37 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.5214
2022-08-06 04:39:10 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.6518
2022-08-06 04:39:42 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5139
2022-08-06 04:40:15 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.5490
2022-08-06 04:40:48 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.7543
2022-08-06 04:41:21 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.7728
2022-08-06 04:41:53 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.5169
2022-08-06 04:42:26 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.5968
2022-08-06 04:42:59 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.6285
2022-08-06 04:43:32 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.7528
2022-08-06 04:44:05 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.6314
2022-08-06 04:44:38 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.8551
2022-08-06 04:45:11 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.7302
2022-08-06 04:45:44 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.8854
2022-08-06 04:46:17 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.6453
2022-08-06 04:46:49 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7041
2022-08-06 04:47:23 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.6197
2022-08-06 04:47:55 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.3900
2022-08-06 04:48:28 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.6416
2022-08-06 04:49:01 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.6057
2022-08-06 04:49:34 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.7706
2022-08-06 04:50:07 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.7781
2022-08-06 04:50:40 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6862
2022-08-06 04:51:12 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.5806
2022-08-06 04:51:44 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.5378
2022-08-06 04:51:46 - train: epoch 018, train_loss: 1.6356
2022-08-06 04:52:59 - eval: epoch: 018, acc1: 66.764%, acc5: 87.710%, test_loss: 1.3761, per_image_load_time: 1.618ms, per_image_inference_time: 0.622ms
2022-08-06 04:52:59 - until epoch: 018, best_acc1: 66.764%
2022-08-06 04:52:59 - epoch 019 lr: 0.018128
2022-08-06 04:53:38 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.3767
2022-08-06 04:54:10 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.6576
2022-08-06 04:54:42 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.6596
2022-08-06 04:55:15 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.5398
2022-08-06 04:55:47 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.4576
2022-08-06 04:56:19 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.6841
2022-08-06 04:56:51 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.4617
2022-08-06 04:57:24 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.6054
2022-08-06 04:57:57 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.7181
2022-08-06 04:58:30 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.5532
2022-08-06 04:59:02 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.4469
2022-08-06 04:59:35 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.7916
2022-08-06 05:00:08 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.5156
2022-08-06 05:00:41 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.3466
2022-08-06 05:01:13 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.8469
2022-08-06 05:01:46 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.5021
2022-08-06 05:02:19 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.7806
2022-08-06 05:02:52 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.5624
2022-08-06 05:03:24 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.8321
2022-08-06 05:03:58 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.5182
2022-08-06 05:04:30 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.2959
2022-08-06 05:05:03 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.5802
2022-08-06 05:05:36 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.7076
2022-08-06 05:06:08 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.5709
2022-08-06 05:06:41 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.7073
2022-08-06 05:07:13 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.7166
2022-08-06 05:07:46 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.4448
2022-08-06 05:08:19 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.6487
2022-08-06 05:08:52 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.6593
2022-08-06 05:09:25 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.6930
2022-08-06 05:09:58 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.4734
2022-08-06 05:10:30 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.5058
2022-08-06 05:11:03 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.5011
2022-08-06 05:11:36 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.5914
2022-08-06 05:12:09 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.7104
2022-08-06 05:12:41 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.1777
2022-08-06 05:13:14 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.6253
2022-08-06 05:13:47 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.7437
2022-08-06 05:14:20 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.4844
2022-08-06 05:14:52 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.6953
2022-08-06 05:15:25 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.5756
2022-08-06 05:15:58 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.4629
2022-08-06 05:16:30 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.3124
2022-08-06 05:17:03 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.6868
2022-08-06 05:17:36 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.7756
2022-08-06 05:18:08 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.4065
2022-08-06 05:18:41 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.4881
2022-08-06 05:19:14 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.4256
2022-08-06 05:19:47 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.4240
2022-08-06 05:20:18 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.5518
2022-08-06 05:20:20 - train: epoch 019, train_loss: 1.5632
2022-08-06 05:21:32 - eval: epoch: 019, acc1: 68.064%, acc5: 88.176%, test_loss: 1.3177, per_image_load_time: 1.552ms, per_image_inference_time: 0.599ms
2022-08-06 05:21:32 - until epoch: 019, best_acc1: 68.064%
2022-08-06 05:21:32 - epoch 020 lr: 0.013551
2022-08-06 05:22:11 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.3223
2022-08-06 05:22:43 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.2369
2022-08-06 05:23:15 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.3681
2022-08-06 05:23:47 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.3015
2022-08-06 05:24:20 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.3210
2022-08-06 05:24:52 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.4089
2022-08-06 05:25:25 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.3818
2022-08-06 05:25:58 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.6118
2022-08-06 05:26:30 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.5621
2022-08-06 05:27:03 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.5608
2022-08-06 05:27:36 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.3372
2022-08-06 05:28:08 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.4053
2022-08-06 05:28:41 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.5366
2022-08-06 05:29:14 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.4152
2022-08-06 05:29:47 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.3581
2022-08-06 05:30:20 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.4321
2022-08-06 05:30:52 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.5249
2022-08-06 05:31:25 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.4207
2022-08-06 05:31:57 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.3895
2022-08-06 05:32:30 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.4839
2022-08-06 05:33:02 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.5344
2022-08-06 05:33:35 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.3604
2022-08-06 05:34:07 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.4117
2022-08-06 05:34:40 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.7907
2022-08-06 05:35:12 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.6353
2022-08-06 05:35:45 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.3696
2022-08-06 05:36:18 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.4448
2022-08-06 05:36:51 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.5280
2022-08-06 05:37:24 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.6072
2022-08-06 05:37:57 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.6711
2022-08-06 05:38:29 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.4687
2022-08-06 05:39:02 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.5172
2022-08-06 05:39:35 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3184
2022-08-06 05:40:08 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.4462
2022-08-06 05:40:40 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.1839
2022-08-06 05:41:13 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.5727
2022-08-06 05:41:46 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.2440
2022-08-06 05:42:19 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.5144
2022-08-06 05:42:52 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.4818
2022-08-06 05:43:25 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.3269
2022-08-06 05:43:58 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.4679
2022-08-06 05:44:30 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.3284
2022-08-06 05:45:03 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.4523
2022-08-06 05:45:36 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.3460
2022-08-06 05:46:09 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.4458
2022-08-06 05:46:42 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.4851
2022-08-06 05:47:14 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.4139
2022-08-06 05:47:47 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.3065
2022-08-06 05:48:20 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.2857
2022-08-06 05:48:52 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.4804
2022-08-06 05:48:53 - train: epoch 020, train_loss: 1.4867
2022-08-06 05:50:06 - eval: epoch: 020, acc1: 69.508%, acc5: 89.278%, test_loss: 1.2438, per_image_load_time: 2.221ms, per_image_inference_time: 0.587ms
2022-08-06 05:50:06 - until epoch: 020, best_acc1: 69.508%
2022-08-06 05:50:06 - epoch 021 lr: 0.009548
2022-08-06 05:50:44 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.4695
2022-08-06 05:51:17 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.4291
2022-08-06 05:51:48 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.1241
2022-08-06 05:52:21 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4296
2022-08-06 05:52:53 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.3751
2022-08-06 05:53:25 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.4447
2022-08-06 05:53:58 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3621
2022-08-06 05:54:31 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.5415
2022-08-06 05:55:03 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.3458
2022-08-06 05:55:36 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3887
2022-08-06 05:56:09 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.2808
2022-08-06 05:56:41 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.3681
2022-08-06 05:57:14 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.3101
2022-08-06 05:57:47 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.1013
2022-08-06 05:58:20 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.2916
2022-08-06 05:58:52 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.2961
2022-08-06 05:59:25 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5948
2022-08-06 05:59:58 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.2500
2022-08-06 06:00:31 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.4385
2022-08-06 06:01:03 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.7739
2022-08-06 06:01:36 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3944
2022-08-06 06:02:08 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.3636
2022-08-06 06:02:41 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.4305
2022-08-06 06:03:14 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.3733
2022-08-06 06:03:46 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.4689
2022-08-06 06:04:19 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.3869
2022-08-06 06:04:52 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.4284
2022-08-06 06:05:25 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.3974
2022-08-06 06:05:58 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.2340
2022-08-06 06:06:31 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.4786
2022-08-06 06:07:04 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.5220
2022-08-06 06:07:36 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.1800
2022-08-06 06:08:09 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.5228
2022-08-06 06:08:41 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.5321
2022-08-06 06:09:14 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4569
2022-08-06 06:09:47 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.4376
2022-08-06 06:10:19 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.1650
2022-08-06 06:10:52 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.4620
2022-08-06 06:11:25 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.4887
2022-08-06 06:11:58 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.5950
2022-08-06 06:12:31 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.3910
2022-08-06 06:13:03 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.2615
2022-08-06 06:13:36 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3005
2022-08-06 06:14:08 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.6800
2022-08-06 06:14:41 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.3131
2022-08-06 06:15:14 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.3782
2022-08-06 06:15:47 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.4202
2022-08-06 06:16:20 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.4675
2022-08-06 06:16:52 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.2437
2022-08-06 06:17:24 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.1931
2022-08-06 06:17:25 - train: epoch 021, train_loss: 1.4129
2022-08-06 06:18:38 - eval: epoch: 021, acc1: 70.398%, acc5: 89.726%, test_loss: 1.2014, per_image_load_time: 1.719ms, per_image_inference_time: 0.632ms
2022-08-06 06:18:38 - until epoch: 021, best_acc1: 70.398%
2022-08-06 06:18:38 - epoch 022 lr: 0.006184
2022-08-06 06:19:17 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.1686
2022-08-06 06:19:49 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.3930
2022-08-06 06:20:21 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.1613
2022-08-06 06:20:53 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.2616
2022-08-06 06:21:25 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.4183
2022-08-06 06:21:57 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.3267
2022-08-06 06:22:30 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.3157
2022-08-06 06:23:02 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.4418
2022-08-06 06:23:35 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.2361
2022-08-06 06:24:08 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.3775
2022-08-06 06:24:40 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.5049
2022-08-06 06:25:13 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.1104
2022-08-06 06:25:45 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.2430
2022-08-06 06:26:18 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.2146
2022-08-06 06:26:51 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.4042
2022-08-06 06:27:24 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.2313
2022-08-06 06:27:56 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.4948
2022-08-06 06:28:29 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.6014
2022-08-06 06:29:01 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.3581
2022-08-06 06:29:34 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4466
2022-08-06 06:30:06 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.4663
2022-08-06 06:30:39 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.1203
2022-08-06 06:31:12 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.3567
2022-08-06 06:31:44 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.4120
2022-08-06 06:32:17 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.1098
2022-08-06 06:32:50 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.2670
2022-08-06 06:33:22 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.2273
2022-08-06 06:33:55 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.3798
2022-08-06 06:34:28 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.1754
2022-08-06 06:35:01 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.2491
2022-08-06 06:35:33 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.5178
2022-08-06 06:36:06 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.3818
2022-08-06 06:36:39 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.2955
2022-08-06 06:37:12 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.2743
2022-08-06 06:37:44 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.5769
2022-08-06 06:38:17 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.4609
2022-08-06 06:38:50 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.4766
2022-08-06 06:39:23 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.5787
2022-08-06 06:39:56 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.2326
2022-08-06 06:40:29 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.4169
2022-08-06 06:41:01 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.2367
2022-08-06 06:41:34 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.4184
2022-08-06 06:42:07 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.3689
2022-08-06 06:42:40 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3626
2022-08-06 06:43:13 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.3232
2022-08-06 06:43:46 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.5582
2022-08-06 06:44:18 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.4065
2022-08-06 06:44:51 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.0882
2022-08-06 06:45:24 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.3898
2022-08-06 06:45:56 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.1439
2022-08-06 06:45:58 - train: epoch 022, train_loss: 1.3460
2022-08-06 06:47:11 - eval: epoch: 022, acc1: 71.462%, acc5: 90.226%, test_loss: 1.1541, per_image_load_time: 1.392ms, per_image_inference_time: 0.630ms
2022-08-06 06:47:12 - until epoch: 022, best_acc1: 71.462%
2022-08-06 06:47:12 - epoch 023 lr: 0.003511
2022-08-06 06:47:51 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.1487
2022-08-06 06:48:23 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.1163
2022-08-06 06:48:55 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.2884
2022-08-06 06:49:27 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.3759
2022-08-06 06:50:00 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.5824
2022-08-06 06:50:32 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.0971
2022-08-06 06:51:04 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.0620
2022-08-06 06:51:37 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.4000
2022-08-06 06:52:10 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4442
2022-08-06 06:52:43 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.2797
2022-08-06 06:53:16 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.2067
2022-08-06 06:53:48 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.2171
2022-08-06 06:54:21 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.1764
2022-08-06 06:54:53 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.3627
2022-08-06 06:55:26 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.4448
2022-08-06 06:55:58 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.2616
2022-08-06 06:56:31 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.2417
2022-08-06 06:57:03 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.1818
2022-08-06 06:57:35 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.4390
2022-08-06 06:58:08 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.1446
2022-08-06 06:58:40 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.2629
2022-08-06 06:59:13 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.1549
2022-08-06 06:59:46 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.1019
2022-08-06 07:00:19 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.2995
2022-08-06 07:00:51 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.3447
2022-08-06 07:01:24 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.3471
2022-08-06 07:01:57 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.3648
2022-08-06 07:02:30 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.1801
2022-08-06 07:03:02 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.5229
2022-08-06 07:03:35 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.4024
2022-08-06 07:04:08 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.2349
2022-08-06 07:04:41 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.5354
2022-08-06 07:05:13 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.3756
2022-08-06 07:05:46 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.2165
2022-08-06 07:06:19 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.1997
2022-08-06 07:06:52 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.1457
2022-08-06 07:07:24 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.1866
2022-08-06 07:07:57 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.4449
2022-08-06 07:08:29 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.2423
2022-08-06 07:09:02 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2197
2022-08-06 07:09:35 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.1277
2022-08-06 07:10:08 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 0.9946
2022-08-06 07:10:40 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.1622
2022-08-06 07:11:14 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.3077
2022-08-06 07:11:46 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.1423
2022-08-06 07:12:19 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.3432
2022-08-06 07:12:52 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.2656
2022-08-06 07:13:25 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.1032
2022-08-06 07:13:57 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.1345
2022-08-06 07:14:30 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.3376
2022-08-06 07:14:31 - train: epoch 023, train_loss: 1.2871
2022-08-06 07:15:44 - eval: epoch: 023, acc1: 72.298%, acc5: 90.592%, test_loss: 1.1228, per_image_load_time: 1.441ms, per_image_inference_time: 0.590ms
2022-08-06 07:15:44 - until epoch: 023, best_acc1: 72.298%
2022-08-06 07:15:44 - epoch 024 lr: 0.001571
2022-08-06 07:16:22 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.3767
2022-08-06 07:16:55 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.1820
2022-08-06 07:17:27 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.1055
2022-08-06 07:17:59 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.2175
2022-08-06 07:18:32 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.2679
2022-08-06 07:19:05 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.3683
2022-08-06 07:19:37 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.2155
2022-08-06 07:20:10 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.2237
2022-08-06 07:20:43 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.2740
2022-08-06 07:21:15 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.3609
2022-08-06 07:21:48 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 0.8754
2022-08-06 07:22:20 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.3086
2022-08-06 07:22:53 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.3978
2022-08-06 07:23:26 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.2721
2022-08-06 07:23:59 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.2480
2022-08-06 07:24:31 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.1166
2022-08-06 07:25:04 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.2467
2022-08-06 07:25:36 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.5283
2022-08-06 07:26:09 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.0594
2022-08-06 07:26:42 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.4177
2022-08-06 07:27:14 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.2005
2022-08-06 07:27:47 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.1801
2022-08-06 07:28:20 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.1219
2022-08-06 07:28:53 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.2996
2022-08-06 07:29:26 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.2597
2022-08-06 07:29:59 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.4565
2022-08-06 07:30:31 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.2912
2022-08-06 07:31:04 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.2106
2022-08-06 07:31:37 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.4627
2022-08-06 07:32:09 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.0565
2022-08-06 07:32:42 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.2300
2022-08-06 07:33:15 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.2970
2022-08-06 07:33:48 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.2067
2022-08-06 07:34:21 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.1467
2022-08-06 07:34:54 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.2039
2022-08-06 07:35:27 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.2447
2022-08-06 07:35:59 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.1372
2022-08-06 07:36:32 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.3621
2022-08-06 07:37:05 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.1670
2022-08-06 07:37:38 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.1663
2022-08-06 07:38:11 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.0071
2022-08-06 07:38:44 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.1709
2022-08-06 07:39:17 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.2387
2022-08-06 07:39:50 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.2583
2022-08-06 07:40:23 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.1374
2022-08-06 07:40:55 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.2910
2022-08-06 07:41:28 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.2274
2022-08-06 07:42:01 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.0562
2022-08-06 07:42:34 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.2500
2022-08-06 07:43:06 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.1946
2022-08-06 07:43:07 - train: epoch 024, train_loss: 1.2481
2022-08-06 07:44:19 - eval: epoch: 024, acc1: 72.642%, acc5: 90.804%, test_loss: 1.1080, per_image_load_time: 2.183ms, per_image_inference_time: 0.605ms
2022-08-06 07:44:19 - until epoch: 024, best_acc1: 72.642%
2022-08-06 07:44:19 - epoch 025 lr: 0.000394
2022-08-06 07:44:58 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.2370
2022-08-06 07:45:30 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1122
2022-08-06 07:46:01 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.1060
2022-08-06 07:46:33 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.3087
2022-08-06 07:47:05 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 0.9750
2022-08-06 07:47:37 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.2695
2022-08-06 07:48:09 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.2324
2022-08-06 07:48:42 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.2605
2022-08-06 07:49:14 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0917
2022-08-06 07:49:47 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.2811
2022-08-06 07:50:19 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.1563
2022-08-06 07:50:52 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.2472
2022-08-06 07:51:24 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.1602
2022-08-06 07:51:57 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.2806
2022-08-06 07:52:29 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.3250
2022-08-06 07:53:02 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1550
2022-08-06 07:53:35 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.3128
2022-08-06 07:54:08 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 0.9746
2022-08-06 07:54:40 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.1915
2022-08-06 07:55:14 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.2208
2022-08-06 07:55:46 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 0.9515
2022-08-06 07:56:19 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.1394
2022-08-06 07:56:52 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.2981
2022-08-06 07:57:25 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 0.9914
2022-08-06 07:57:58 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.2466
2022-08-06 07:58:31 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.1405
2022-08-06 07:59:04 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.3398
2022-08-06 07:59:37 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.1309
2022-08-06 08:00:10 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.4310
2022-08-06 08:00:43 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.2336
2022-08-06 08:01:15 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.2213
2022-08-06 08:01:48 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.2971
2022-08-06 08:02:21 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.2873
2022-08-06 08:02:54 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.3360
2022-08-06 08:03:27 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1054
2022-08-06 08:04:00 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.2392
2022-08-06 08:04:33 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.1189
2022-08-06 08:05:05 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3706
2022-08-06 08:05:38 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.1715
2022-08-06 08:06:11 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.3471
2022-08-06 08:06:44 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.5530
2022-08-06 08:07:17 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.4134
2022-08-06 08:07:49 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.0522
2022-08-06 08:08:22 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.2078
2022-08-06 08:08:55 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.1704
2022-08-06 08:09:28 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.1577
2022-08-06 08:10:01 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.2205
2022-08-06 08:10:33 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.1507
2022-08-06 08:11:06 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.2129
2022-08-06 08:11:38 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.2277
2022-08-06 08:11:39 - train: epoch 025, train_loss: 1.2304
2022-08-06 08:12:52 - eval: epoch: 025, acc1: 72.766%, acc5: 90.874%, test_loss: 1.1042, per_image_load_time: 2.215ms, per_image_inference_time: 0.611ms
2022-08-06 08:12:53 - until epoch: 025, best_acc1: 72.766%
2022-08-06 08:12:53 - train done. train time: 11.912 hours, best_acc1: 72.766%
2022-08-09 22:49:20 - net_idx: 4
2022-08-09 22:49:20 - net_config: {'stem_width': 64, 'depth': 15, 'w_0': 40, 'w_a': 17.36690916049138, 'w_m': 1.8119620337729927}
2022-08-09 22:49:20 - num_classes: 1000
2022-08-09 22:49:20 - input_image_size: 224
2022-08-09 22:49:20 - scale: 1.1428571428571428
2022-08-09 22:49:20 - seed: 0
2022-08-09 22:49:20 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-09 22:49:20 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-09 22:49:20 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce2b0>
2022-08-09 22:49:20 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce6d0>
2022-08-09 22:49:20 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce700>
2022-08-09 22:49:20 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce760>
2022-08-09 22:49:20 - batch_size: 256
2022-08-09 22:49:20 - num_workers: 16
2022-08-09 22:49:20 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-09 22:49:20 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-09 22:49:20 - epochs: 25
2022-08-09 22:49:20 - print_interval: 100
2022-08-09 22:49:20 - accumulation_steps: 1
2022-08-09 22:49:20 - sync_bn: False
2022-08-09 22:49:20 - apex: True
2022-08-09 22:49:20 - use_ema_model: False
2022-08-09 22:49:20 - ema_model_decay: 0.9999
2022-08-09 22:49:20 - log_dir: ./log
2022-08-09 22:49:20 - checkpoint_dir: ./checkpoints
2022-08-09 22:49:20 - gpus_type: NVIDIA RTX A5000
2022-08-09 22:49:20 - gpus_num: 2
2022-08-09 22:49:20 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5dddaeb570>
2022-08-09 22:49:20 - ema_model: None
2022-08-09 22:49:20 - --------------------parameters--------------------
2022-08-09 22:49:20 - name: conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.0.weight, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.weight, grad: True
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.bias, grad: True
2022-08-09 22:49:20 - name: fc.weight, grad: True
2022-08-09 22:49:20 - name: fc.bias, grad: True
2022-08-09 22:49:20 - --------------------buffers--------------------
2022-08-09 22:49:20 - name: conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.running_mean, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.running_var, grad: False
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.num_batches_tracked, grad: False
2022-08-09 22:49:20 - -----------no weight decay layers--------------
2022-08-09 22:49:20 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-09 22:49:20 - -------------weight decay layers---------------
2022-08-09 22:49:20 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: layer4.6.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-09 22:49:20 - resuming model from ./checkpoints/4/latest.pth. resume_epoch: 025, used_time: 11.912 hours, best_acc1: 72.766%, test_loss: 1.1042, lr: 0.000000
2022-08-09 22:49:20 - train done. train time: 11.912 hours, best_acc1: 72.766%
