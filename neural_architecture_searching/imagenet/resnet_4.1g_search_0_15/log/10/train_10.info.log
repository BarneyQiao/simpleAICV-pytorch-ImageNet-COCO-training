2022-08-11 21:04:36 - net_idx: 10
2022-08-11 21:04:36 - net_config: {'stem_width': 64, 'depth': 14, 'w_0': 40, 'w_a': 20.053025731231948, 'w_m': 1.8268982021440168}
2022-08-11 21:04:36 - num_classes: 1000
2022-08-11 21:04:36 - input_image_size: 224
2022-08-11 21:04:36 - scale: 1.1428571428571428
2022-08-11 21:04:36 - seed: 0
2022-08-11 21:04:36 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-11 21:04:36 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-11 21:04:36 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce2b0>
2022-08-11 21:04:36 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5dfd2ce6d0>
2022-08-11 21:04:36 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce700>
2022-08-11 21:04:36 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5dfd2ce760>
2022-08-11 21:04:36 - batch_size: 256
2022-08-11 21:04:36 - num_workers: 16
2022-08-11 21:04:36 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-11 21:04:36 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-11 21:04:36 - epochs: 25
2022-08-11 21:04:36 - print_interval: 100
2022-08-11 21:04:36 - accumulation_steps: 1
2022-08-11 21:04:36 - sync_bn: False
2022-08-11 21:04:36 - apex: True
2022-08-11 21:04:36 - use_ema_model: False
2022-08-11 21:04:36 - ema_model_decay: 0.9999
2022-08-11 21:04:36 - log_dir: ./log
2022-08-11 21:04:36 - checkpoint_dir: ./checkpoints
2022-08-11 21:04:36 - gpus_type: NVIDIA RTX A5000
2022-08-11 21:04:36 - gpus_num: 2
2022-08-11 21:04:36 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5dddaeb570>
2022-08-11 21:04:36 - ema_model: None
2022-08-11 21:04:36 - --------------------parameters--------------------
2022-08-11 21:04:36 - name: conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-11 21:04:36 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-11 21:04:36 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-11 21:04:36 - name: fc.weight, grad: True
2022-08-11 21:04:36 - name: fc.bias, grad: True
2022-08-11 21:04:36 - --------------------buffers--------------------
2022-08-11 21:04:36 - name: conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-11 21:04:36 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-11 21:04:36 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-11 21:04:36 - -----------no weight decay layers--------------
2022-08-11 21:04:36 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-11 21:04:36 - -------------weight decay layers---------------
2022-08-11 21:04:36 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-11 21:04:36 - epoch 001 lr: 0.100000
2022-08-11 21:05:16 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9012
2022-08-11 21:05:50 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9013
2022-08-11 21:06:23 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8639
2022-08-11 21:06:56 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.8694
2022-08-11 21:07:29 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.7720
2022-08-11 21:08:02 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.7195
2022-08-11 21:08:35 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.7402
2022-08-11 21:09:09 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.6407
2022-08-11 21:09:43 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.5160
2022-08-11 21:10:16 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.4697
2022-08-11 21:10:50 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.3631
2022-08-11 21:11:23 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.2178
2022-08-11 21:11:57 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.2143
2022-08-11 21:12:30 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.1042
2022-08-11 21:13:04 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 6.0376
2022-08-11 21:13:38 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.1065
2022-08-11 21:14:11 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.7344
2022-08-11 21:14:45 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.7672
2022-08-11 21:15:19 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.7673
2022-08-11 21:15:52 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.5321
2022-08-11 21:16:26 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.6286
2022-08-11 21:16:59 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.5968
2022-08-11 21:17:33 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.3873
2022-08-11 21:18:06 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.3332
2022-08-11 21:18:40 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.2954
2022-08-11 21:19:13 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.3785
2022-08-11 21:19:47 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.2896
2022-08-11 21:20:20 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.1749
2022-08-11 21:20:54 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.1617
2022-08-11 21:21:28 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.2201
2022-08-11 21:22:01 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.2049
2022-08-11 21:22:35 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.2044
2022-08-11 21:23:09 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 5.0278
2022-08-11 21:23:43 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.9988
2022-08-11 21:24:16 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.8788
2022-08-11 21:24:49 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.7947
2022-08-11 21:25:23 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.7994
2022-08-11 21:25:57 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.5466
2022-08-11 21:26:30 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.7581
2022-08-11 21:27:04 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.7171
2022-08-11 21:27:37 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.8131
2022-08-11 21:28:11 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.5745
2022-08-11 21:28:45 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.5707
2022-08-11 21:29:19 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.3844
2022-08-11 21:29:53 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.5632
2022-08-11 21:30:27 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.7091
2022-08-11 21:31:01 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.4500
2022-08-11 21:31:35 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.5695
2022-08-11 21:32:08 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.5224
2022-08-11 21:32:42 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.4746
2022-08-11 21:32:43 - train: epoch 001, train_loss: 5.5004
2022-08-11 21:34:00 - eval: epoch: 001, acc1: 17.128%, acc5: 38.712%, test_loss: 4.1984, per_image_load_time: 2.356ms, per_image_inference_time: 0.579ms
2022-08-11 21:34:00 - until epoch: 001, best_acc1: 17.128%
2022-08-11 21:34:00 - epoch 002 lr: 0.099606
2022-08-11 21:34:41 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.1437
2022-08-11 21:35:14 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.2548
2022-08-11 21:35:47 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.4166
2022-08-11 21:36:19 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.3898
2022-08-11 21:36:52 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.2676
2022-08-11 21:37:25 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.0037
2022-08-11 21:37:58 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.3745
2022-08-11 21:38:30 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 4.0920
2022-08-11 21:39:03 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.8289
2022-08-11 21:39:35 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.1938
2022-08-11 21:40:07 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.0969
2022-08-11 21:40:40 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.8961
2022-08-11 21:41:13 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.9671
2022-08-11 21:41:46 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.1217
2022-08-11 21:42:19 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 4.0679
2022-08-11 21:42:51 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.9013
2022-08-11 21:43:24 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 4.0157
2022-08-11 21:43:57 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.8597
2022-08-11 21:44:29 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.7135
2022-08-11 21:45:02 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.6545
2022-08-11 21:45:35 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.9637
2022-08-11 21:46:08 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.6242
2022-08-11 21:46:41 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.9174
2022-08-11 21:47:14 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.6891
2022-08-11 21:47:47 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.7091
2022-08-11 21:48:20 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.6045
2022-08-11 21:48:53 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.8283
2022-08-11 21:49:26 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.8134
2022-08-11 21:49:59 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.8052
2022-08-11 21:50:32 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.4196
2022-08-11 21:51:05 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.6910
2022-08-11 21:51:38 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.7665
2022-08-11 21:52:11 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.5210
2022-08-11 21:52:44 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.5408
2022-08-11 21:53:18 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.5494
2022-08-11 21:53:51 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.6615
2022-08-11 21:54:24 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.8119
2022-08-11 21:54:57 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.4090
2022-08-11 21:55:30 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.4829
2022-08-11 21:56:03 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.2374
2022-08-11 21:56:36 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.8274
2022-08-11 21:57:09 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.3757
2022-08-11 21:57:42 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.5095
2022-08-11 21:58:15 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.2602
2022-08-11 21:58:48 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.2183
2022-08-11 21:59:21 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.3034
2022-08-11 21:59:55 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.4014
2022-08-11 22:00:28 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.5036
2022-08-11 22:01:01 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.3601
2022-08-11 22:01:33 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.4791
2022-08-11 22:01:35 - train: epoch 002, train_loss: 3.7693
2022-08-11 22:02:52 - eval: epoch: 002, acc1: 32.176%, acc5: 58.162%, test_loss: 3.1849, per_image_load_time: 2.360ms, per_image_inference_time: 0.602ms
2022-08-11 22:02:52 - until epoch: 002, best_acc1: 32.176%
2022-08-11 22:02:52 - epoch 003 lr: 0.098429
2022-08-11 22:03:33 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.3354
2022-08-11 22:04:07 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.4691
2022-08-11 22:04:40 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.3208
2022-08-11 22:05:14 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.2655
2022-08-11 22:05:47 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.5311
2022-08-11 22:06:21 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.1738
2022-08-11 22:06:54 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.3903
2022-08-11 22:07:27 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.5403
2022-08-11 22:08:00 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.3932
2022-08-11 22:08:34 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.4250
2022-08-11 22:09:07 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.0490
2022-08-11 22:09:40 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.2735
2022-08-11 22:10:13 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.2256
2022-08-11 22:10:47 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.1268
2022-08-11 22:11:20 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.3390
2022-08-11 22:11:53 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.1333
2022-08-11 22:12:26 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.0879
2022-08-11 22:12:59 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.0783
2022-08-11 22:13:31 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.3949
2022-08-11 22:14:04 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 2.9723
2022-08-11 22:14:37 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.0695
2022-08-11 22:15:10 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.5085
2022-08-11 22:15:42 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.9880
2022-08-11 22:16:15 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.2445
2022-08-11 22:16:48 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.0823
2022-08-11 22:17:21 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.2931
2022-08-11 22:17:54 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.5088
2022-08-11 22:18:27 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.0339
2022-08-11 22:19:00 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.9593
2022-08-11 22:19:33 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 2.9308
2022-08-11 22:20:06 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.3504
2022-08-11 22:20:39 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.1671
2022-08-11 22:21:12 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0367
2022-08-11 22:21:45 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.1814
2022-08-11 22:22:17 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 3.0567
2022-08-11 22:22:51 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 3.0076
2022-08-11 22:23:24 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.2468
2022-08-11 22:23:57 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.1917
2022-08-11 22:24:31 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.3003
2022-08-11 22:25:04 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.8081
2022-08-11 22:25:37 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.9993
2022-08-11 22:26:10 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.0863
2022-08-11 22:26:44 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.9833
2022-08-11 22:27:17 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.9989
2022-08-11 22:27:50 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.0458
2022-08-11 22:28:24 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.8172
2022-08-11 22:28:57 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 3.0448
2022-08-11 22:29:30 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.1155
2022-08-11 22:30:03 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.1400
2022-08-11 22:30:36 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 3.2623
2022-08-11 22:30:37 - train: epoch 003, train_loss: 3.1419
2022-08-11 22:31:54 - eval: epoch: 003, acc1: 37.622%, acc5: 63.812%, test_loss: 2.8735, per_image_load_time: 2.325ms, per_image_inference_time: 0.606ms
2022-08-11 22:31:54 - until epoch: 003, best_acc1: 37.622%
2022-08-11 22:31:54 - epoch 004 lr: 0.096488
2022-08-11 22:32:34 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.9710
2022-08-11 22:33:07 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.8105
2022-08-11 22:33:41 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.8970
2022-08-11 22:34:14 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.8054
2022-08-11 22:34:48 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.6046
2022-08-11 22:35:21 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 2.9937
2022-08-11 22:35:54 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 3.0311
2022-08-11 22:36:27 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.8079
2022-08-11 22:37:01 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.6532
2022-08-11 22:37:34 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.7832
2022-08-11 22:38:07 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.0327
2022-08-11 22:38:41 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.7209
2022-08-11 22:39:14 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.7959
2022-08-11 22:39:48 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 3.0493
2022-08-11 22:40:21 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.9835
2022-08-11 22:40:55 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.9313
2022-08-11 22:41:28 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.8857
2022-08-11 22:42:02 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 3.0717
2022-08-11 22:42:36 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.7985
2022-08-11 22:43:09 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.9326
2022-08-11 22:43:42 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.7641
2022-08-11 22:44:16 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.8414
2022-08-11 22:44:49 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.5229
2022-08-11 22:45:23 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.7126
2022-08-11 22:45:56 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.5748
2022-08-11 22:46:29 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.7053
2022-08-11 22:47:03 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.6149
2022-08-11 22:47:36 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8040
2022-08-11 22:48:09 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8902
2022-08-11 22:48:42 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.7403
2022-08-11 22:49:15 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.7482
2022-08-11 22:49:49 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8492
2022-08-11 22:50:22 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8458
2022-08-11 22:50:55 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 3.0543
2022-08-11 22:51:28 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.8969
2022-08-11 22:52:01 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.6753
2022-08-11 22:52:35 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.8633
2022-08-11 22:53:08 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.6625
2022-08-11 22:53:42 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.4094
2022-08-11 22:54:15 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.5747
2022-08-11 22:54:48 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.7710
2022-08-11 22:55:22 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6157
2022-08-11 22:55:55 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.6530
2022-08-11 22:56:28 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.6009
2022-08-11 22:57:02 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.3267
2022-08-11 22:57:35 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.7847
2022-08-11 22:58:08 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.5396
2022-08-11 22:58:41 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.5180
2022-08-11 22:59:15 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.9023
2022-08-11 22:59:47 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.8019
2022-08-11 22:59:49 - train: epoch 004, train_loss: 2.8187
2022-08-11 23:01:05 - eval: epoch: 004, acc1: 43.940%, acc5: 70.288%, test_loss: 2.5123, per_image_load_time: 1.678ms, per_image_inference_time: 0.625ms
2022-08-11 23:01:05 - until epoch: 004, best_acc1: 43.940%
2022-08-11 23:01:05 - epoch 005 lr: 0.093815
2022-08-11 23:01:45 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.6811
2022-08-11 23:02:18 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 3.0184
2022-08-11 23:02:51 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.8895
2022-08-11 23:03:24 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.6608
2022-08-11 23:03:57 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.5277
2022-08-11 23:04:30 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.6363
2022-08-11 23:05:03 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.7719
2022-08-11 23:05:36 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.6003
2022-08-11 23:06:09 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.6631
2022-08-11 23:06:42 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.5376
2022-08-11 23:07:15 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.7108
2022-08-11 23:07:49 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.8995
2022-08-11 23:08:22 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.4057
2022-08-11 23:08:55 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.6698
2022-08-11 23:09:28 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.4899
2022-08-11 23:10:02 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.4838
2022-08-11 23:10:35 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.4536
2022-08-11 23:11:09 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.5944
2022-08-11 23:11:42 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.7430
2022-08-11 23:12:16 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.7141
2022-08-11 23:12:49 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.5180
2022-08-11 23:13:23 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.6189
2022-08-11 23:13:57 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.4519
2022-08-11 23:14:31 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.6552
2022-08-11 23:15:05 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.5320
2022-08-11 23:15:38 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.7633
2022-08-11 23:16:11 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.6194
2022-08-11 23:16:44 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.5689
2022-08-11 23:17:18 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.5494
2022-08-11 23:17:52 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.6500
2022-08-11 23:18:25 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.7732
2022-08-11 23:18:59 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.4831
2022-08-11 23:19:33 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.7391
2022-08-11 23:20:07 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.4342
2022-08-11 23:20:40 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.7008
2022-08-11 23:21:14 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6243
2022-08-11 23:21:47 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.5992
2022-08-11 23:22:21 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5007
2022-08-11 23:22:54 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.6712
2022-08-11 23:23:28 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.6568
2022-08-11 23:24:02 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.5008
2022-08-11 23:24:36 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.6910
2022-08-11 23:25:09 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.5764
2022-08-11 23:25:43 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.4286
2022-08-11 23:26:17 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.6186
2022-08-11 23:26:51 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.6367
2022-08-11 23:27:25 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.2981
2022-08-11 23:27:59 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.4378
2022-08-11 23:28:32 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.7586
2022-08-11 23:29:05 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.5950
2022-08-11 23:29:07 - train: epoch 005, train_loss: 2.6209
2022-08-11 23:30:24 - eval: epoch: 005, acc1: 47.164%, acc5: 73.582%, test_loss: 2.3093, per_image_load_time: 2.421ms, per_image_inference_time: 0.558ms
2022-08-11 23:30:24 - until epoch: 005, best_acc1: 47.164%
2022-08-11 23:30:24 - epoch 006 lr: 0.090450
2022-08-11 23:31:04 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.3713
2022-08-11 23:31:37 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.5499
2022-08-11 23:32:11 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.5167
2022-08-11 23:32:44 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.3126
2022-08-11 23:33:17 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.2385
2022-08-11 23:33:51 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.4653
2022-08-11 23:34:24 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.4465
2022-08-11 23:34:57 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.2957
2022-08-11 23:35:30 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.2818
2022-08-11 23:36:04 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.2116
2022-08-11 23:36:37 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.6619
2022-08-11 23:37:11 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.5910
2022-08-11 23:37:44 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.4896
2022-08-11 23:38:18 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.5455
2022-08-11 23:38:52 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.7930
2022-08-11 23:39:25 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3407
2022-08-11 23:39:58 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.5899
2022-08-11 23:40:31 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.4677
2022-08-11 23:41:04 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.4915
2022-08-11 23:41:38 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.6204
2022-08-11 23:42:11 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.5772
2022-08-11 23:42:44 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.3427
2022-08-11 23:43:18 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.3803
2022-08-11 23:43:51 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.5502
2022-08-11 23:44:25 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4616
2022-08-11 23:44:59 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.3569
2022-08-11 23:45:32 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.4577
2022-08-11 23:46:06 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.0853
2022-08-11 23:46:40 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.7069
2022-08-11 23:47:14 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.5736
2022-08-11 23:47:47 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.3462
2022-08-11 23:48:20 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.4187
2022-08-11 23:48:54 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.3103
2022-08-11 23:49:28 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.6021
2022-08-11 23:50:01 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.7026
2022-08-11 23:50:35 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.3347
2022-08-11 23:51:08 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.3840
2022-08-11 23:51:42 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.2078
2022-08-11 23:52:16 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.3450
2022-08-11 23:52:49 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.4887
2022-08-11 23:53:23 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.4388
2022-08-11 23:53:57 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.5469
2022-08-11 23:54:31 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.6272
2022-08-11 23:55:05 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.4474
2022-08-11 23:55:38 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.4604
2022-08-11 23:56:12 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.5196
2022-08-11 23:56:46 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.5376
2022-08-11 23:57:20 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.5341
2022-08-11 23:57:54 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.4555
2022-08-11 23:58:27 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.4637
2022-08-11 23:58:29 - train: epoch 006, train_loss: 2.4891
2022-08-11 23:59:45 - eval: epoch: 006, acc1: 50.064%, acc5: 75.740%, test_loss: 2.1777, per_image_load_time: 2.345ms, per_image_inference_time: 0.591ms
2022-08-11 23:59:45 - until epoch: 006, best_acc1: 50.064%
2022-08-11 23:59:45 - epoch 007 lr: 0.086448
2022-08-12 00:00:25 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.2609
2022-08-12 00:00:59 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.6009
2022-08-12 00:01:33 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.6680
2022-08-12 00:02:06 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.3843
2022-08-12 00:02:40 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.3032
2022-08-12 00:03:13 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.5094
2022-08-12 00:03:47 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.4191
2022-08-12 00:04:20 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.5473
2022-08-12 00:04:53 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.5419
2022-08-12 00:05:26 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.7176
2022-08-12 00:05:59 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.3423
2022-08-12 00:06:32 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.2652
2022-08-12 00:07:05 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.1281
2022-08-12 00:07:38 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.4350
2022-08-12 00:08:12 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.4781
2022-08-12 00:08:45 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.3145
2022-08-12 00:09:19 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.4899
2022-08-12 00:09:52 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.3706
2022-08-12 00:10:25 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.4667
2022-08-12 00:10:59 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.2320
2022-08-12 00:11:32 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.5300
2022-08-12 00:12:06 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.3474
2022-08-12 00:12:39 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.4007
2022-08-12 00:13:13 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.6157
2022-08-12 00:13:46 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.4425
2022-08-12 00:14:20 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.2034
2022-08-12 00:14:53 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.2166
2022-08-12 00:15:27 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.4870
2022-08-12 00:16:00 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.3621
2022-08-12 00:16:34 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.5457
2022-08-12 00:17:08 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.2357
2022-08-12 00:17:41 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.2635
2022-08-12 00:18:15 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.4209
2022-08-12 00:18:48 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.2788
2022-08-12 00:19:21 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.2921
2022-08-12 00:19:54 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.2600
2022-08-12 00:20:27 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.2717
2022-08-12 00:21:00 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.3799
2022-08-12 00:21:34 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.3145
2022-08-12 00:22:07 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.5705
2022-08-12 00:22:40 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.3416
2022-08-12 00:23:14 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2637
2022-08-12 00:23:47 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.3969
2022-08-12 00:24:21 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.2040
2022-08-12 00:24:54 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.4290
2022-08-12 00:25:27 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.5828
2022-08-12 00:26:01 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.3490
2022-08-12 00:26:35 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.5202
2022-08-12 00:27:08 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.4750
2022-08-12 00:27:41 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.3363
2022-08-12 00:27:43 - train: epoch 007, train_loss: 2.3880
2022-08-12 00:28:59 - eval: epoch: 007, acc1: 50.372%, acc5: 75.504%, test_loss: 2.1822, per_image_load_time: 2.237ms, per_image_inference_time: 0.622ms
2022-08-12 00:28:59 - until epoch: 007, best_acc1: 50.372%
2022-08-12 00:28:59 - epoch 008 lr: 0.081870
2022-08-12 00:29:39 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.3990
2022-08-12 00:30:13 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.5059
2022-08-12 00:30:46 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.1703
2022-08-12 00:31:19 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.1850
2022-08-12 00:31:53 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.1351
2022-08-12 00:32:26 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.3853
2022-08-12 00:32:59 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.4130
2022-08-12 00:33:32 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.0999
2022-08-12 00:34:06 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.1296
2022-08-12 00:34:40 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.5246
2022-08-12 00:35:13 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.1739
2022-08-12 00:35:47 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.2449
2022-08-12 00:36:21 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.5252
2022-08-12 00:36:54 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.1243
2022-08-12 00:37:27 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.3134
2022-08-12 00:38:01 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.6021
2022-08-12 00:38:35 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.4813
2022-08-12 00:39:08 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.3955
2022-08-12 00:39:42 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.3536
2022-08-12 00:40:16 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.3813
2022-08-12 00:40:50 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.3307
2022-08-12 00:41:23 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.3014
2022-08-12 00:41:57 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.2943
2022-08-12 00:42:31 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.0034
2022-08-12 00:43:05 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.3254
2022-08-12 00:43:38 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.1964
2022-08-12 00:44:12 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.5890
2022-08-12 00:44:46 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.3107
2022-08-12 00:45:20 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.2728
2022-08-12 00:45:53 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.3879
2022-08-12 00:46:27 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.5238
2022-08-12 00:47:01 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.5219
2022-08-12 00:47:34 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.4585
2022-08-12 00:48:08 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.5281
2022-08-12 00:48:42 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.4154
2022-08-12 00:49:15 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.2759
2022-08-12 00:49:49 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.1709
2022-08-12 00:50:23 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.4761
2022-08-12 00:50:56 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.2563
2022-08-12 00:51:30 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.5539
2022-08-12 00:52:04 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 1.9839
2022-08-12 00:52:38 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.3430
2022-08-12 00:53:12 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 1.9909
2022-08-12 00:53:45 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.2673
2022-08-12 00:54:19 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.2816
2022-08-12 00:54:53 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.3410
2022-08-12 00:55:26 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2097
2022-08-12 00:56:00 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.0468
2022-08-12 00:56:34 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.4818
2022-08-12 00:57:07 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.2116
2022-08-12 00:57:08 - train: epoch 008, train_loss: 2.3051
2022-08-12 00:58:24 - eval: epoch: 008, acc1: 51.484%, acc5: 77.030%, test_loss: 2.0964, per_image_load_time: 1.580ms, per_image_inference_time: 0.605ms
2022-08-12 00:58:24 - until epoch: 008, best_acc1: 51.484%
2022-08-12 00:58:24 - epoch 009 lr: 0.076790
2022-08-12 00:59:04 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 2.1149
2022-08-12 00:59:37 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.3168
2022-08-12 01:00:11 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.0418
2022-08-12 01:00:44 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.3991
2022-08-12 01:01:17 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.3459
2022-08-12 01:01:50 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.1815
2022-08-12 01:02:23 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.2192
2022-08-12 01:02:57 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.2587
2022-08-12 01:03:31 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.0329
2022-08-12 01:04:04 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.2282
2022-08-12 01:04:38 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.5031
2022-08-12 01:05:11 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.2299
2022-08-12 01:05:45 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.3474
2022-08-12 01:06:19 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9142
2022-08-12 01:06:53 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.2195
2022-08-12 01:07:26 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2203
2022-08-12 01:08:00 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 1.9571
2022-08-12 01:08:34 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.1641
2022-08-12 01:09:08 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.1380
2022-08-12 01:09:42 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.0315
2022-08-12 01:10:15 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.3231
2022-08-12 01:10:49 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.3995
2022-08-12 01:11:23 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.0025
2022-08-12 01:11:56 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.1527
2022-08-12 01:12:30 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.1014
2022-08-12 01:13:04 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.2620
2022-08-12 01:13:37 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.1902
2022-08-12 01:14:11 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2224
2022-08-12 01:14:45 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.0331
2022-08-12 01:15:19 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.0832
2022-08-12 01:15:53 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.3132
2022-08-12 01:16:26 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.1892
2022-08-12 01:17:00 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.4408
2022-08-12 01:17:34 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.4218
2022-08-12 01:18:08 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.3371
2022-08-12 01:18:42 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.1431
2022-08-12 01:19:16 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.2925
2022-08-12 01:19:50 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.2096
2022-08-12 01:20:23 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 1.9792
2022-08-12 01:20:57 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.3445
2022-08-12 01:21:31 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.1952
2022-08-12 01:22:05 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.0568
2022-08-12 01:22:38 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.0773
2022-08-12 01:23:12 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.1525
2022-08-12 01:23:46 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.4093
2022-08-12 01:24:20 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.3464
2022-08-12 01:24:54 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.2712
2022-08-12 01:25:28 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.2550
2022-08-12 01:26:02 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.0807
2022-08-12 01:26:35 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 1.9596
2022-08-12 01:26:36 - train: epoch 009, train_loss: 2.2353
2022-08-12 01:27:52 - eval: epoch: 009, acc1: 54.972%, acc5: 79.592%, test_loss: 1.9316, per_image_load_time: 2.385ms, per_image_inference_time: 0.560ms
2022-08-12 01:27:53 - until epoch: 009, best_acc1: 54.972%
2022-08-12 01:27:53 - epoch 010 lr: 0.071288
2022-08-12 01:28:33 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 1.9493
2022-08-12 01:29:06 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.2620
2022-08-12 01:29:39 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.0897
2022-08-12 01:30:12 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.4066
2022-08-12 01:30:45 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.1206
2022-08-12 01:31:19 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.2048
2022-08-12 01:31:52 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.2789
2022-08-12 01:32:25 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.0751
2022-08-12 01:32:59 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.1531
2022-08-12 01:33:32 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.0950
2022-08-12 01:34:06 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 1.8636
2022-08-12 01:34:39 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.1322
2022-08-12 01:35:12 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.0825
2022-08-12 01:35:45 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.2577
2022-08-12 01:36:19 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 2.0230
2022-08-12 01:36:52 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.1553
2022-08-12 01:37:25 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.2984
2022-08-12 01:37:58 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 1.8897
2022-08-12 01:38:32 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.2495
2022-08-12 01:39:05 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.2350
2022-08-12 01:39:38 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 2.1022
2022-08-12 01:40:12 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.3006
2022-08-12 01:40:45 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.1934
2022-08-12 01:41:19 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.2752
2022-08-12 01:41:52 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.0849
2022-08-12 01:42:26 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.2028
2022-08-12 01:43:00 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.8374
2022-08-12 01:43:33 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.1319
2022-08-12 01:44:07 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.3052
2022-08-12 01:44:40 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1862
2022-08-12 01:45:14 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.2723
2022-08-12 01:45:48 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.2464
2022-08-12 01:46:21 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.2876
2022-08-12 01:46:55 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.3447
2022-08-12 01:47:28 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.3386
2022-08-12 01:48:02 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.2282
2022-08-12 01:48:35 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 1.9582
2022-08-12 01:49:09 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1741
2022-08-12 01:49:43 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 2.0116
2022-08-12 01:50:17 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.0531
2022-08-12 01:50:50 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 1.9395
2022-08-12 01:51:24 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.2885
2022-08-12 01:51:57 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.1262
2022-08-12 01:52:31 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.1275
2022-08-12 01:53:06 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.0042
2022-08-12 01:53:40 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 1.9663
2022-08-12 01:54:13 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.2721
2022-08-12 01:54:47 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.1123
2022-08-12 01:55:21 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.1982
2022-08-12 01:55:54 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.0923
2022-08-12 01:55:55 - train: epoch 010, train_loss: 2.1707
2022-08-12 01:57:11 - eval: epoch: 010, acc1: 55.768%, acc5: 80.206%, test_loss: 1.8860, per_image_load_time: 2.365ms, per_image_inference_time: 0.587ms
2022-08-12 01:57:11 - until epoch: 010, best_acc1: 55.768%
2022-08-12 01:57:11 - epoch 011 lr: 0.065450
2022-08-12 01:57:52 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.0691
2022-08-12 01:58:25 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.1559
2022-08-12 01:58:59 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.1564
2022-08-12 01:59:32 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.2118
2022-08-12 02:00:06 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.0534
2022-08-12 02:00:39 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.1657
2022-08-12 02:01:12 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.1617
2022-08-12 02:01:46 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.1510
2022-08-12 02:02:19 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.2114
2022-08-12 02:02:52 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 1.9228
2022-08-12 02:03:26 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.3765
2022-08-12 02:03:59 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.3568
2022-08-12 02:04:33 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.3042
2022-08-12 02:05:06 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.0562
2022-08-12 02:05:40 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.1206
2022-08-12 02:06:14 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.3928
2022-08-12 02:06:48 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.2267
2022-08-12 02:07:21 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 1.9519
2022-08-12 02:07:55 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.0536
2022-08-12 02:08:28 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1616
2022-08-12 02:09:02 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 1.9608
2022-08-12 02:09:36 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 1.9933
2022-08-12 02:10:09 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.1426
2022-08-12 02:10:43 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.8912
2022-08-12 02:11:16 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.0976
2022-08-12 02:11:51 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.2012
2022-08-12 02:12:24 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 1.9615
2022-08-12 02:12:58 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.9693
2022-08-12 02:13:32 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.3005
2022-08-12 02:14:05 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.5830
2022-08-12 02:14:39 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.2098
2022-08-12 02:15:13 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.1429
2022-08-12 02:15:47 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.1601
2022-08-12 02:16:20 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 1.9873
2022-08-12 02:16:54 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.0548
2022-08-12 02:17:28 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.0496
2022-08-12 02:18:01 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.2412
2022-08-12 02:18:35 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.9531
2022-08-12 02:19:09 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.1067
2022-08-12 02:19:42 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.1603
2022-08-12 02:20:16 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.7915
2022-08-12 02:20:50 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 1.9712
2022-08-12 02:21:24 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 1.8729
2022-08-12 02:21:57 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.2176
2022-08-12 02:22:31 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 1.9962
2022-08-12 02:23:04 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 2.1178
2022-08-12 02:23:38 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.8517
2022-08-12 02:24:12 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.7790
2022-08-12 02:24:46 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.0714
2022-08-12 02:25:19 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.1511
2022-08-12 02:25:21 - train: epoch 011, train_loss: 2.1083
2022-08-12 02:26:37 - eval: epoch: 011, acc1: 57.108%, acc5: 81.452%, test_loss: 1.8233, per_image_load_time: 2.378ms, per_image_inference_time: 0.586ms
2022-08-12 02:26:37 - until epoch: 011, best_acc1: 57.108%
2022-08-12 02:26:37 - epoch 012 lr: 0.059368
2022-08-12 02:27:17 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.8849
2022-08-12 02:27:50 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.8858
2022-08-12 02:28:24 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 2.0716
2022-08-12 02:28:58 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.0068
2022-08-12 02:29:31 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.0583
2022-08-12 02:30:04 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.9400
2022-08-12 02:30:38 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.7226
2022-08-12 02:31:11 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.1316
2022-08-12 02:31:45 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 1.9409
2022-08-12 02:32:18 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9794
2022-08-12 02:32:52 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.4098
2022-08-12 02:33:25 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.9728
2022-08-12 02:33:59 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.8543
2022-08-12 02:34:33 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.0914
2022-08-12 02:35:06 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 1.8387
2022-08-12 02:35:40 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 1.9377
2022-08-12 02:36:14 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.9228
2022-08-12 02:36:48 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.0665
2022-08-12 02:37:21 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1431
2022-08-12 02:37:55 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.2297
2022-08-12 02:38:29 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.1403
2022-08-12 02:39:03 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.2541
2022-08-12 02:39:37 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 2.0452
2022-08-12 02:40:10 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.1544
2022-08-12 02:40:44 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.9009
2022-08-12 02:41:18 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.8305
2022-08-12 02:41:52 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 1.9467
2022-08-12 02:42:26 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.9299
2022-08-12 02:42:59 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.0514
2022-08-12 02:43:33 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.8924
2022-08-12 02:44:07 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.0382
2022-08-12 02:44:41 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.8644
2022-08-12 02:45:14 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.1460
2022-08-12 02:45:48 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.1266
2022-08-12 02:46:22 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.0890
2022-08-12 02:46:56 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.9291
2022-08-12 02:47:29 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.1913
2022-08-12 02:48:03 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.2113
2022-08-12 02:48:37 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8545
2022-08-12 02:49:10 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1310
2022-08-12 02:49:44 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.1760
2022-08-12 02:50:18 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.9652
2022-08-12 02:50:52 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.0984
2022-08-12 02:51:26 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 2.1045
2022-08-12 02:51:59 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.7983
2022-08-12 02:52:33 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.1466
2022-08-12 02:53:08 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.9974
2022-08-12 02:53:41 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.0128
2022-08-12 02:54:15 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.8804
2022-08-12 02:54:49 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.8640
2022-08-12 02:54:50 - train: epoch 012, train_loss: 2.0493
2022-08-12 02:56:07 - eval: epoch: 012, acc1: 58.876%, acc5: 82.836%, test_loss: 1.7314, per_image_load_time: 2.386ms, per_image_inference_time: 0.568ms
2022-08-12 02:56:07 - until epoch: 012, best_acc1: 58.876%
2022-08-12 02:56:07 - epoch 013 lr: 0.053138
2022-08-12 02:56:47 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.8671
2022-08-12 02:57:20 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.8067
2022-08-12 02:57:54 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8202
2022-08-12 02:58:27 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.8909
2022-08-12 02:59:01 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 1.9818
2022-08-12 02:59:34 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 1.8313
2022-08-12 03:00:07 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.8179
2022-08-12 03:00:41 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.0092
2022-08-12 03:01:14 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 2.0533
2022-08-12 03:01:47 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 2.0250
2022-08-12 03:02:21 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.0273
2022-08-12 03:02:55 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.0726
2022-08-12 03:03:29 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.8575
2022-08-12 03:04:02 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 1.9214
2022-08-12 03:04:36 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.2068
2022-08-12 03:05:09 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.9756
2022-08-12 03:05:43 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 1.9545
2022-08-12 03:06:17 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9943
2022-08-12 03:06:50 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 1.9377
2022-08-12 03:07:24 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.1640
2022-08-12 03:07:58 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.2224
2022-08-12 03:08:31 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 2.0047
2022-08-12 03:09:05 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 1.9230
2022-08-12 03:09:38 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.0167
2022-08-12 03:10:12 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.9346
2022-08-12 03:10:46 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.8310
2022-08-12 03:11:19 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 2.0527
2022-08-12 03:11:53 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.0244
2022-08-12 03:12:27 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 1.9252
2022-08-12 03:13:00 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.7352
2022-08-12 03:13:34 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.7953
2022-08-12 03:14:08 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 1.9927
2022-08-12 03:14:41 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.8439
2022-08-12 03:15:15 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.9344
2022-08-12 03:15:48 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.7944
2022-08-12 03:16:21 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 1.9778
2022-08-12 03:16:55 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.6360
2022-08-12 03:17:29 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 1.9568
2022-08-12 03:18:02 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.1235
2022-08-12 03:18:36 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.9758
2022-08-12 03:19:10 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.9409
2022-08-12 03:19:44 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 2.0971
2022-08-12 03:20:17 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.9589
2022-08-12 03:20:51 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.9202
2022-08-12 03:21:25 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.9184
2022-08-12 03:21:59 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 1.9272
2022-08-12 03:22:32 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.9759
2022-08-12 03:23:06 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.0930
2022-08-12 03:23:39 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0016
2022-08-12 03:24:13 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.1275
2022-08-12 03:24:14 - train: epoch 013, train_loss: 1.9867
2022-08-12 03:25:30 - eval: epoch: 013, acc1: 60.118%, acc5: 83.692%, test_loss: 1.6773, per_image_load_time: 2.357ms, per_image_inference_time: 0.574ms
2022-08-12 03:25:30 - until epoch: 013, best_acc1: 60.118%
2022-08-12 03:25:30 - epoch 014 lr: 0.046859
2022-08-12 03:26:10 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 2.0674
2022-08-12 03:26:44 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 2.0990
2022-08-12 03:27:17 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.8233
2022-08-12 03:27:50 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.8053
2022-08-12 03:28:23 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.8998
2022-08-12 03:28:56 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 2.0107
2022-08-12 03:29:29 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.7322
2022-08-12 03:30:03 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.7358
2022-08-12 03:30:36 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.9656
2022-08-12 03:31:09 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.1333
2022-08-12 03:31:43 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.8793
2022-08-12 03:32:16 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 2.2234
2022-08-12 03:32:50 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.0081
2022-08-12 03:33:23 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.9705
2022-08-12 03:33:57 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.0941
2022-08-12 03:34:31 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.8298
2022-08-12 03:35:04 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 2.1108
2022-08-12 03:35:38 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.0341
2022-08-12 03:36:11 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.9544
2022-08-12 03:36:45 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.8765
2022-08-12 03:37:18 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 1.9781
2022-08-12 03:37:52 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 2.0273
2022-08-12 03:38:26 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.7413
2022-08-12 03:38:59 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.9006
2022-08-12 03:39:33 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.8416
2022-08-12 03:40:07 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8483
2022-08-12 03:40:40 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 2.0393
2022-08-12 03:41:13 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 1.9555
2022-08-12 03:41:47 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9316
2022-08-12 03:42:21 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 2.0379
2022-08-12 03:42:54 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.9600
2022-08-12 03:43:28 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 2.0051
2022-08-12 03:44:01 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.9296
2022-08-12 03:44:35 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.8096
2022-08-12 03:45:09 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.9792
2022-08-12 03:45:43 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.7115
2022-08-12 03:46:16 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.8253
2022-08-12 03:46:50 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 2.1683
2022-08-12 03:47:23 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.8997
2022-08-12 03:47:57 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 2.0398
2022-08-12 03:48:31 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.9713
2022-08-12 03:49:05 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.9635
2022-08-12 03:49:38 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.9635
2022-08-12 03:50:12 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.9201
2022-08-12 03:50:45 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.9660
2022-08-12 03:51:19 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.8742
2022-08-12 03:51:53 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.7584
2022-08-12 03:52:27 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.9368
2022-08-12 03:53:00 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8107
2022-08-12 03:53:33 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.9447
2022-08-12 03:53:35 - train: epoch 014, train_loss: 1.9275
2022-08-12 03:54:50 - eval: epoch: 014, acc1: 60.790%, acc5: 84.070%, test_loss: 1.6487, per_image_load_time: 2.221ms, per_image_inference_time: 0.617ms
2022-08-12 03:54:50 - until epoch: 014, best_acc1: 60.790%
2022-08-12 03:54:50 - epoch 015 lr: 0.040630
2022-08-12 03:55:31 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.6941
2022-08-12 03:56:04 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.9500
2022-08-12 03:56:38 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.0783
2022-08-12 03:57:11 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.8004
2022-08-12 03:57:45 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.8302
2022-08-12 03:58:18 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 1.9235
2022-08-12 03:58:52 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.9272
2022-08-12 03:59:25 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.8725
2022-08-12 03:59:59 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.8989
2022-08-12 04:00:33 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.8174
2022-08-12 04:01:06 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.8571
2022-08-12 04:01:39 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.9867
2022-08-12 04:02:12 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.9644
2022-08-12 04:02:46 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.8620
2022-08-12 04:03:20 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.5815
2022-08-12 04:03:54 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.8477
2022-08-12 04:04:27 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.9815
2022-08-12 04:05:01 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.8786
2022-08-12 04:05:34 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.7837
2022-08-12 04:06:08 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.8318
2022-08-12 04:06:42 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.7014
2022-08-12 04:07:16 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9130
2022-08-12 04:07:49 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.8900
2022-08-12 04:08:23 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.7576
2022-08-12 04:08:57 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.9804
2022-08-12 04:09:30 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.4887
2022-08-12 04:10:04 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 2.0367
2022-08-12 04:10:38 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.9019
2022-08-12 04:11:11 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.8544
2022-08-12 04:11:45 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.7990
2022-08-12 04:12:19 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.7907
2022-08-12 04:12:52 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.9731
2022-08-12 04:13:26 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.7270
2022-08-12 04:13:59 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.6835
2022-08-12 04:14:33 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0838
2022-08-12 04:15:07 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.7649
2022-08-12 04:15:41 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.7217
2022-08-12 04:16:15 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.8579
2022-08-12 04:16:48 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.9363
2022-08-12 04:17:22 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.7625
2022-08-12 04:17:55 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.7563
2022-08-12 04:18:29 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.8036
2022-08-12 04:19:02 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.7811
2022-08-12 04:19:36 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.8842
2022-08-12 04:20:10 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.7624
2022-08-12 04:20:44 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.9169
2022-08-12 04:21:17 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.9235
2022-08-12 04:21:51 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.7908
2022-08-12 04:22:24 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.8710
2022-08-12 04:22:57 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 2.0887
2022-08-12 04:22:59 - train: epoch 015, train_loss: 1.8632
2022-08-12 04:24:16 - eval: epoch: 015, acc1: 61.700%, acc5: 84.718%, test_loss: 1.5939, per_image_load_time: 2.382ms, per_image_inference_time: 0.582ms
2022-08-12 04:24:16 - until epoch: 015, best_acc1: 61.700%
2022-08-12 04:24:16 - epoch 016 lr: 0.034548
2022-08-12 04:24:56 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.6618
2022-08-12 04:25:30 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.7727
2022-08-12 04:26:03 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.7015
2022-08-12 04:26:37 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 1.9188
2022-08-12 04:27:10 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.6901
2022-08-12 04:27:43 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.7586
2022-08-12 04:28:16 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.5828
2022-08-12 04:28:49 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.7242
2022-08-12 04:29:23 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.7336
2022-08-12 04:29:56 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.7868
2022-08-12 04:30:30 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.5890
2022-08-12 04:31:03 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.6581
2022-08-12 04:31:37 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.8821
2022-08-12 04:32:10 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.7507
2022-08-12 04:32:44 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 2.1322
2022-08-12 04:33:17 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.7616
2022-08-12 04:33:51 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.8342
2022-08-12 04:34:24 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.7843
2022-08-12 04:34:57 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.8651
2022-08-12 04:35:31 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.5509
2022-08-12 04:36:05 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 2.0405
2022-08-12 04:36:38 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.7544
2022-08-12 04:37:11 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 2.0493
2022-08-12 04:37:45 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.8503
2022-08-12 04:38:18 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.7413
2022-08-12 04:38:52 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.9676
2022-08-12 04:39:25 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.6782
2022-08-12 04:39:58 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.6516
2022-08-12 04:40:31 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.7580
2022-08-12 04:41:05 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 2.0252
2022-08-12 04:41:38 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.8438
2022-08-12 04:42:12 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.8381
2022-08-12 04:42:45 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.8494
2022-08-12 04:43:19 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.6773
2022-08-12 04:43:53 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.8498
2022-08-12 04:44:27 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.6388
2022-08-12 04:45:01 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.6016
2022-08-12 04:45:34 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.2327
2022-08-12 04:46:07 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.7722
2022-08-12 04:46:41 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.9250
2022-08-12 04:47:15 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.8988
2022-08-12 04:47:49 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.8079
2022-08-12 04:48:23 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.7224
2022-08-12 04:48:56 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.6017
2022-08-12 04:49:30 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.8026
2022-08-12 04:50:04 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.8687
2022-08-12 04:50:38 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.8304
2022-08-12 04:51:12 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.7201
2022-08-12 04:51:45 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.7188
2022-08-12 04:52:18 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.9439
2022-08-12 04:52:20 - train: epoch 016, train_loss: 1.7994
2022-08-12 04:53:36 - eval: epoch: 016, acc1: 64.050%, acc5: 86.128%, test_loss: 1.4927, per_image_load_time: 2.378ms, per_image_inference_time: 0.566ms
2022-08-12 04:53:36 - until epoch: 016, best_acc1: 64.050%
2022-08-12 04:53:36 - epoch 017 lr: 0.028710
2022-08-12 04:54:17 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.8522
2022-08-12 04:54:50 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.6563
2022-08-12 04:55:23 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.9487
2022-08-12 04:55:56 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.4869
2022-08-12 04:56:30 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.7470
2022-08-12 04:57:03 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 1.9120
2022-08-12 04:57:37 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.7006
2022-08-12 04:58:10 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.7942
2022-08-12 04:58:44 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.8407
2022-08-12 04:59:17 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.7040
2022-08-12 04:59:51 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.9490
2022-08-12 05:00:24 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.7237
2022-08-12 05:00:58 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.6166
2022-08-12 05:01:32 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.7352
2022-08-12 05:02:05 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.5151
2022-08-12 05:02:39 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.6477
2022-08-12 05:03:12 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.6195
2022-08-12 05:03:46 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.7250
2022-08-12 05:04:19 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.5284
2022-08-12 05:04:53 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.6792
2022-08-12 05:05:27 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.4303
2022-08-12 05:06:00 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.6087
2022-08-12 05:06:34 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.8657
2022-08-12 05:07:08 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.7251
2022-08-12 05:07:42 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.8584
2022-08-12 05:08:15 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.7902
2022-08-12 05:08:49 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6540
2022-08-12 05:09:22 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.6891
2022-08-12 05:09:56 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 2.0630
2022-08-12 05:10:30 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.8503
2022-08-12 05:11:04 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8753
2022-08-12 05:11:37 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.6597
2022-08-12 05:12:11 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.7795
2022-08-12 05:12:44 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.7489
2022-08-12 05:13:18 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.7780
2022-08-12 05:13:51 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.9489
2022-08-12 05:14:25 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.6914
2022-08-12 05:14:59 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.7311
2022-08-12 05:15:32 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.6199
2022-08-12 05:16:06 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.7563
2022-08-12 05:16:39 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.7491
2022-08-12 05:17:13 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.7089
2022-08-12 05:17:47 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.7196
2022-08-12 05:18:21 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8125
2022-08-12 05:18:54 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.9045
2022-08-12 05:19:28 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.5943
2022-08-12 05:20:02 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.9067
2022-08-12 05:20:36 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.7496
2022-08-12 05:21:09 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.6347
2022-08-12 05:21:42 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.4594
2022-08-12 05:21:43 - train: epoch 017, train_loss: 1.7304
2022-08-12 05:22:58 - eval: epoch: 017, acc1: 64.270%, acc5: 86.302%, test_loss: 1.4730, per_image_load_time: 2.350ms, per_image_inference_time: 0.542ms
2022-08-12 05:22:58 - until epoch: 017, best_acc1: 64.270%
2022-08-12 05:22:58 - epoch 018 lr: 0.023208
2022-08-12 05:23:38 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.7094
2022-08-12 05:24:11 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.7384
2022-08-12 05:24:45 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.7988
2022-08-12 05:25:19 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.9609
2022-08-12 05:25:52 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.5903
2022-08-12 05:26:25 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.7547
2022-08-12 05:26:58 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.4317
2022-08-12 05:27:30 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.7077
2022-08-12 05:28:03 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.7643
2022-08-12 05:28:36 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.6837
2022-08-12 05:29:10 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.8370
2022-08-12 05:29:43 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.6320
2022-08-12 05:30:16 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.7538
2022-08-12 05:30:50 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.7997
2022-08-12 05:31:23 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.8058
2022-08-12 05:31:56 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.7049
2022-08-12 05:32:30 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7328
2022-08-12 05:33:03 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6347
2022-08-12 05:33:37 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.5477
2022-08-12 05:34:10 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.7810
2022-08-12 05:34:43 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.8256
2022-08-12 05:35:16 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.9092
2022-08-12 05:35:49 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.7432
2022-08-12 05:36:22 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.6380
2022-08-12 05:36:55 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.4796
2022-08-12 05:37:29 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.6585
2022-08-12 05:38:03 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.7375
2022-08-12 05:38:36 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.6935
2022-08-12 05:39:09 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.4695
2022-08-12 05:39:43 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.7451
2022-08-12 05:40:17 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.8622
2022-08-12 05:40:50 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.5641
2022-08-12 05:41:24 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.6030
2022-08-12 05:41:58 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.6956
2022-08-12 05:42:32 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.8881
2022-08-12 05:43:06 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.7326
2022-08-12 05:43:39 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.8761
2022-08-12 05:44:13 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.8595
2022-08-12 05:44:47 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.8473
2022-08-12 05:45:20 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.7046
2022-08-12 05:45:54 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7916
2022-08-12 05:46:27 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.7100
2022-08-12 05:47:01 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.6029
2022-08-12 05:47:35 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.6824
2022-08-12 05:48:08 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.4968
2022-08-12 05:48:42 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.6336
2022-08-12 05:49:16 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.9067
2022-08-12 05:49:49 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.8832
2022-08-12 05:50:23 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.6159
2022-08-12 05:50:56 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.6750
2022-08-12 05:50:58 - train: epoch 018, train_loss: 1.6586
2022-08-12 05:52:14 - eval: epoch: 018, acc1: 66.260%, acc5: 87.500%, test_loss: 1.3858, per_image_load_time: 2.343ms, per_image_inference_time: 0.588ms
2022-08-12 05:52:14 - until epoch: 018, best_acc1: 66.260%
2022-08-12 05:52:14 - epoch 019 lr: 0.018128
2022-08-12 05:52:54 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4058
2022-08-12 05:53:28 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.5570
2022-08-12 05:54:01 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.6098
2022-08-12 05:54:35 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.4252
2022-08-12 05:55:08 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.7076
2022-08-12 05:55:41 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.6539
2022-08-12 05:56:15 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.5070
2022-08-12 05:56:48 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.6862
2022-08-12 05:57:22 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.5360
2022-08-12 05:57:56 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.6991
2022-08-12 05:58:29 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5340
2022-08-12 05:59:03 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.5259
2022-08-12 05:59:36 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.6513
2022-08-12 06:00:10 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.4512
2022-08-12 06:00:43 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.6922
2022-08-12 06:01:17 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.4911
2022-08-12 06:01:51 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.5366
2022-08-12 06:02:25 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.5967
2022-08-12 06:02:59 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.7416
2022-08-12 06:03:33 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.4795
2022-08-12 06:04:06 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.6065
2022-08-12 06:04:40 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.7121
2022-08-12 06:05:14 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.7610
2022-08-12 06:05:48 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.7892
2022-08-12 06:06:22 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.7116
2022-08-12 06:06:55 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.5022
2022-08-12 06:07:29 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.4367
2022-08-12 06:08:03 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.8456
2022-08-12 06:08:36 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.6515
2022-08-12 06:09:10 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.6928
2022-08-12 06:09:43 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.5009
2022-08-12 06:10:17 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.3164
2022-08-12 06:10:51 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.4888
2022-08-12 06:11:24 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.6896
2022-08-12 06:11:58 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.6646
2022-08-12 06:12:32 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.2533
2022-08-12 06:13:05 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.6517
2022-08-12 06:13:39 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.7057
2022-08-12 06:14:13 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.4402
2022-08-12 06:14:47 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.5697
2022-08-12 06:15:21 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.6428
2022-08-12 06:15:55 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.6381
2022-08-12 06:16:29 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.6247
2022-08-12 06:17:02 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.6425
2022-08-12 06:17:36 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.7449
2022-08-12 06:18:10 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.3859
2022-08-12 06:18:43 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.5642
2022-08-12 06:19:17 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.5563
2022-08-12 06:19:51 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.4607
2022-08-12 06:20:24 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.6684
2022-08-12 06:20:25 - train: epoch 019, train_loss: 1.5869
2022-08-12 06:21:41 - eval: epoch: 019, acc1: 67.578%, acc5: 88.372%, test_loss: 1.3210, per_image_load_time: 2.124ms, per_image_inference_time: 0.593ms
2022-08-12 06:21:41 - until epoch: 019, best_acc1: 67.578%
2022-08-12 06:21:41 - epoch 020 lr: 0.013551
2022-08-12 06:22:21 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.5556
2022-08-12 06:22:55 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.3979
2022-08-12 06:23:29 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.5399
2022-08-12 06:24:02 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.4297
2022-08-12 06:24:35 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.4491
2022-08-12 06:25:09 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.6150
2022-08-12 06:25:43 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.2917
2022-08-12 06:26:16 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.4537
2022-08-12 06:26:49 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.6966
2022-08-12 06:27:22 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.4247
2022-08-12 06:27:55 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.2836
2022-08-12 06:28:28 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.4557
2022-08-12 06:29:01 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.4012
2022-08-12 06:29:35 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.5024
2022-08-12 06:30:08 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.7267
2022-08-12 06:30:41 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.7977
2022-08-12 06:31:15 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.6074
2022-08-12 06:31:48 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.6145
2022-08-12 06:32:22 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.3623
2022-08-12 06:32:55 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.4944
2022-08-12 06:33:29 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.6575
2022-08-12 06:34:02 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.3937
2022-08-12 06:34:36 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.4112
2022-08-12 06:35:09 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.7924
2022-08-12 06:35:43 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.5291
2022-08-12 06:36:17 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.3971
2022-08-12 06:36:50 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.4544
2022-08-12 06:37:24 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.6550
2022-08-12 06:37:58 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.6250
2022-08-12 06:38:32 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.6201
2022-08-12 06:39:05 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.8066
2022-08-12 06:39:39 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.5670
2022-08-12 06:40:12 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3770
2022-08-12 06:40:46 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.5270
2022-08-12 06:41:20 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.2462
2022-08-12 06:41:53 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.5116
2022-08-12 06:42:27 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.3204
2022-08-12 06:43:00 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.4796
2022-08-12 06:43:34 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.6077
2022-08-12 06:44:06 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.2961
2022-08-12 06:44:40 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.5176
2022-08-12 06:45:13 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.4662
2022-08-12 06:45:47 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.6634
2022-08-12 06:46:20 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.4267
2022-08-12 06:46:53 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.4480
2022-08-12 06:47:27 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.5923
2022-08-12 06:48:00 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.4459
2022-08-12 06:48:34 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4582
2022-08-12 06:49:07 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.4272
2022-08-12 06:49:40 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.4826
2022-08-12 06:49:42 - train: epoch 020, train_loss: 1.5094
2022-08-12 06:50:58 - eval: epoch: 020, acc1: 68.832%, acc5: 88.846%, test_loss: 1.2740, per_image_load_time: 2.115ms, per_image_inference_time: 0.602ms
2022-08-12 06:50:58 - until epoch: 020, best_acc1: 68.832%
2022-08-12 06:50:58 - epoch 021 lr: 0.009548
2022-08-12 06:51:38 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.6239
2022-08-12 06:52:11 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.4568
2022-08-12 06:52:44 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.4440
2022-08-12 06:53:17 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4718
2022-08-12 06:53:51 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.3341
2022-08-12 06:54:24 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.4201
2022-08-12 06:54:58 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3167
2022-08-12 06:55:31 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.5402
2022-08-12 06:56:05 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.3055
2022-08-12 06:56:38 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3772
2022-08-12 06:57:11 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.2393
2022-08-12 06:57:45 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.3142
2022-08-12 06:58:18 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.3609
2022-08-12 06:58:52 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.0455
2022-08-12 06:59:25 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.3591
2022-08-12 06:59:58 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3838
2022-08-12 07:00:31 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.3853
2022-08-12 07:01:05 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.2648
2022-08-12 07:01:39 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.6710
2022-08-12 07:02:11 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.4985
2022-08-12 07:02:44 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3661
2022-08-12 07:03:18 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.3311
2022-08-12 07:03:51 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.4326
2022-08-12 07:04:25 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.3437
2022-08-12 07:04:58 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.6756
2022-08-12 07:05:31 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.5704
2022-08-12 07:06:05 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.3567
2022-08-12 07:06:38 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.3889
2022-08-12 07:07:11 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.2123
2022-08-12 07:07:45 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.4216
2022-08-12 07:08:18 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.6407
2022-08-12 07:08:52 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.2286
2022-08-12 07:09:25 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.5611
2022-08-12 07:09:59 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.5398
2022-08-12 07:10:32 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4312
2022-08-12 07:11:06 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.1581
2022-08-12 07:11:40 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.4028
2022-08-12 07:12:13 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.6410
2022-08-12 07:12:47 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.4502
2022-08-12 07:13:20 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.6840
2022-08-12 07:13:54 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.3679
2022-08-12 07:14:27 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.5300
2022-08-12 07:15:01 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.4671
2022-08-12 07:15:34 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.4038
2022-08-12 07:16:06 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.3428
2022-08-12 07:16:40 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.2849
2022-08-12 07:17:13 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.7122
2022-08-12 07:17:47 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.5002
2022-08-12 07:18:20 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.1532
2022-08-12 07:18:53 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.2539
2022-08-12 07:18:55 - train: epoch 021, train_loss: 1.4372
2022-08-12 07:20:09 - eval: epoch: 021, acc1: 69.880%, acc5: 89.492%, test_loss: 1.2152, per_image_load_time: 1.888ms, per_image_inference_time: 0.621ms
2022-08-12 07:20:09 - until epoch: 021, best_acc1: 69.880%
2022-08-12 07:20:09 - epoch 022 lr: 0.006184
2022-08-12 07:20:49 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.2212
2022-08-12 07:21:22 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.3541
2022-08-12 07:21:55 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.2897
2022-08-12 07:22:29 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.4256
2022-08-12 07:23:02 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3110
2022-08-12 07:23:35 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.3717
2022-08-12 07:24:09 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4259
2022-08-12 07:24:42 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.4220
2022-08-12 07:25:16 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.3891
2022-08-12 07:25:50 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.3789
2022-08-12 07:26:23 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.4004
2022-08-12 07:26:57 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.1727
2022-08-12 07:27:30 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.2367
2022-08-12 07:28:03 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.1609
2022-08-12 07:28:37 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.3435
2022-08-12 07:29:10 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.2556
2022-08-12 07:29:43 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.4208
2022-08-12 07:30:16 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.6546
2022-08-12 07:30:50 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.4051
2022-08-12 07:31:23 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4079
2022-08-12 07:31:56 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.4428
2022-08-12 07:32:29 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.1645
2022-08-12 07:33:02 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.4058
2022-08-12 07:33:36 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.6111
2022-08-12 07:34:09 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.3818
2022-08-12 07:34:43 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.2974
2022-08-12 07:35:16 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.1730
2022-08-12 07:35:50 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.3689
2022-08-12 07:36:23 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.3423
2022-08-12 07:36:57 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.4725
2022-08-12 07:37:30 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.4364
2022-08-12 07:38:04 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4108
2022-08-12 07:38:38 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.3264
2022-08-12 07:39:11 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.3340
2022-08-12 07:39:45 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.3465
2022-08-12 07:40:18 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.1865
2022-08-12 07:40:52 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.4409
2022-08-12 07:41:26 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.5877
2022-08-12 07:41:59 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.2903
2022-08-12 07:42:32 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.3311
2022-08-12 07:43:06 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.2746
2022-08-12 07:43:40 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.4027
2022-08-12 07:44:14 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4335
2022-08-12 07:44:47 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.2445
2022-08-12 07:45:21 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.3060
2022-08-12 07:45:54 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.5242
2022-08-12 07:46:28 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.2684
2022-08-12 07:47:01 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.1976
2022-08-12 07:47:35 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.2874
2022-08-12 07:48:07 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.3191
2022-08-12 07:48:09 - train: epoch 022, train_loss: 1.3674
2022-08-12 07:49:25 - eval: epoch: 022, acc1: 71.206%, acc5: 90.110%, test_loss: 1.1708, per_image_load_time: 2.290ms, per_image_inference_time: 0.616ms
2022-08-12 07:49:25 - until epoch: 022, best_acc1: 71.206%
2022-08-12 07:49:25 - epoch 023 lr: 0.003511
2022-08-12 07:50:05 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.2643
2022-08-12 07:50:38 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.2854
2022-08-12 07:51:11 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.2547
2022-08-12 07:51:45 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.3866
2022-08-12 07:52:18 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.5089
2022-08-12 07:52:51 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.2996
2022-08-12 07:53:25 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.2253
2022-08-12 07:53:59 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3570
2022-08-12 07:54:32 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.3984
2022-08-12 07:55:06 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.1146
2022-08-12 07:55:39 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.2768
2022-08-12 07:56:13 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.1696
2022-08-12 07:56:47 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.2481
2022-08-12 07:57:20 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.4331
2022-08-12 07:57:54 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.4381
2022-08-12 07:58:27 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.2617
2022-08-12 07:59:01 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.3986
2022-08-12 07:59:34 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.1773
2022-08-12 08:00:08 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.4508
2022-08-12 08:00:41 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.1364
2022-08-12 08:01:14 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.3676
2022-08-12 08:01:48 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.2784
2022-08-12 08:02:22 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.2311
2022-08-12 08:02:56 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.4175
2022-08-12 08:03:29 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.4553
2022-08-12 08:04:02 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.5433
2022-08-12 08:04:36 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.2411
2022-08-12 08:05:09 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.1727
2022-08-12 08:05:43 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.2214
2022-08-12 08:06:17 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.2034
2022-08-12 08:06:51 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.4592
2022-08-12 08:07:24 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.3056
2022-08-12 08:07:58 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.4321
2022-08-12 08:08:32 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.2354
2022-08-12 08:09:05 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.2895
2022-08-12 08:09:39 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.2887
2022-08-12 08:10:13 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.1196
2022-08-12 08:10:47 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.4450
2022-08-12 08:11:20 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3833
2022-08-12 08:11:54 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.1614
2022-08-12 08:12:28 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.2125
2022-08-12 08:13:02 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.1162
2022-08-12 08:13:36 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.0702
2022-08-12 08:14:09 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.3150
2022-08-12 08:14:43 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.2672
2022-08-12 08:15:17 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.4210
2022-08-12 08:15:50 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.2486
2022-08-12 08:16:24 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.2448
2022-08-12 08:16:58 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.1698
2022-08-12 08:17:31 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.2543
2022-08-12 08:17:32 - train: epoch 023, train_loss: 1.3140
2022-08-12 08:18:48 - eval: epoch: 023, acc1: 71.810%, acc5: 90.454%, test_loss: 1.1431, per_image_load_time: 1.818ms, per_image_inference_time: 0.631ms
2022-08-12 08:18:48 - until epoch: 023, best_acc1: 71.810%
2022-08-12 08:18:48 - epoch 024 lr: 0.001571
2022-08-12 08:19:28 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.5149
2022-08-12 08:20:00 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.4631
2022-08-12 08:20:33 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.1646
2022-08-12 08:21:05 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.2973
2022-08-12 08:21:38 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.3853
2022-08-12 08:22:11 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.2905
2022-08-12 08:22:44 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.2883
2022-08-12 08:23:17 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.2241
2022-08-12 08:23:50 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.3175
2022-08-12 08:24:23 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.3487
2022-08-12 08:24:56 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.0794
2022-08-12 08:25:30 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.3486
2022-08-12 08:26:03 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.5599
2022-08-12 08:26:36 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.4014
2022-08-12 08:27:10 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.3802
2022-08-12 08:27:44 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.1252
2022-08-12 08:28:17 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.3153
2022-08-12 08:28:50 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.4412
2022-08-12 08:29:23 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.0598
2022-08-12 08:29:56 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.1465
2022-08-12 08:30:30 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.2363
2022-08-12 08:31:03 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.1463
2022-08-12 08:31:36 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.1974
2022-08-12 08:32:10 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.3299
2022-08-12 08:32:43 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.3475
2022-08-12 08:33:16 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.3722
2022-08-12 08:33:50 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.1913
2022-08-12 08:34:23 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.2435
2022-08-12 08:34:56 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.2955
2022-08-12 08:35:29 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.2341
2022-08-12 08:36:02 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.1826
2022-08-12 08:36:36 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.2480
2022-08-12 08:37:09 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.1303
2022-08-12 08:37:42 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.2644
2022-08-12 08:38:15 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.1971
2022-08-12 08:38:48 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.2901
2022-08-12 08:39:22 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.3289
2022-08-12 08:39:55 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.6201
2022-08-12 08:40:29 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.4106
2022-08-12 08:41:03 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.2122
2022-08-12 08:41:37 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.1341
2022-08-12 08:42:10 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.2705
2022-08-12 08:42:44 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.4053
2022-08-12 08:43:17 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.2776
2022-08-12 08:43:51 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.1363
2022-08-12 08:44:24 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.1833
2022-08-12 08:44:58 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.2241
2022-08-12 08:45:31 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 0.9837
2022-08-12 08:46:05 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.2858
2022-08-12 08:46:38 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.1725
2022-08-12 08:46:39 - train: epoch 024, train_loss: 1.2768
2022-08-12 08:47:55 - eval: epoch: 024, acc1: 72.120%, acc5: 90.676%, test_loss: 1.1274, per_image_load_time: 2.058ms, per_image_inference_time: 0.616ms
2022-08-12 08:47:55 - until epoch: 024, best_acc1: 72.120%
2022-08-12 08:47:55 - epoch 025 lr: 0.000394
2022-08-12 08:48:35 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.1231
2022-08-12 08:49:08 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1291
2022-08-12 08:49:40 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.2901
2022-08-12 08:50:13 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.2422
2022-08-12 08:50:45 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.0723
2022-08-12 08:51:18 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3105
2022-08-12 08:51:50 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.3291
2022-08-12 08:52:23 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.2252
2022-08-12 08:52:56 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0889
2022-08-12 08:53:29 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3606
2022-08-12 08:54:02 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.3092
2022-08-12 08:54:35 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.2417
2022-08-12 08:55:08 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3612
2022-08-12 08:55:41 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.4592
2022-08-12 08:56:14 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.2054
2022-08-12 08:56:48 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.0682
2022-08-12 08:57:20 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.2519
2022-08-12 08:57:54 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.0762
2022-08-12 08:58:27 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.4543
2022-08-12 08:59:00 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.3045
2022-08-12 08:59:34 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 0.9315
2022-08-12 09:00:07 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.1201
2022-08-12 09:00:41 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.2654
2022-08-12 09:01:14 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.2040
2022-08-12 09:01:48 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.0344
2022-08-12 09:02:21 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.2086
2022-08-12 09:02:55 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.2492
2022-08-12 09:03:29 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.1901
2022-08-12 09:04:02 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.4337
2022-08-12 09:04:35 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.1979
2022-08-12 09:05:08 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.3466
2022-08-12 09:05:41 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.2537
2022-08-12 09:06:15 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.2007
2022-08-12 09:06:48 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.2994
2022-08-12 09:07:21 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1470
2022-08-12 09:07:54 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.2794
2022-08-12 09:08:28 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.2501
2022-08-12 09:09:01 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3020
2022-08-12 09:09:34 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.3331
2022-08-12 09:10:08 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.3595
2022-08-12 09:10:42 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.5254
2022-08-12 09:11:15 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.1444
2022-08-12 09:11:49 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.0846
2022-08-12 09:12:22 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.1423
2022-08-12 09:12:56 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.1983
2022-08-12 09:13:29 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 0.9871
2022-08-12 09:14:02 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.2203
2022-08-12 09:14:36 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.1961
2022-08-12 09:15:09 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.3115
2022-08-12 09:15:42 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.2645
2022-08-12 09:15:43 - train: epoch 025, train_loss: 1.2556
2022-08-12 09:16:58 - eval: epoch: 025, acc1: 72.186%, acc5: 90.744%, test_loss: 1.1242, per_image_load_time: 1.443ms, per_image_inference_time: 0.630ms
2022-08-12 09:16:59 - until epoch: 025, best_acc1: 72.186%
2022-08-12 09:16:59 - train done. train time: 12.205 hours, best_acc1: 72.186%
