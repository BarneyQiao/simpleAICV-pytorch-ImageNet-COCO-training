2022-08-16 00:00:29 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.1279
2022-08-16 00:01:02 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.2604
2022-08-16 00:01:36 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.4730
2022-08-16 00:02:10 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.5168
2022-08-16 00:02:45 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.4306
2022-08-16 00:03:18 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.3909
2022-08-16 00:03:19 - train: epoch 002, train_loss: 3.7561
2022-08-16 00:04:34 - eval: epoch: 002, acc1: 30.928%, acc5: 56.758%, test_loss: 3.2718, per_image_load_time: 2.442ms, per_image_inference_time: 0.494ms
2022-08-16 00:04:34 - until epoch: 002, best_acc1: 30.928%
2022-08-16 00:04:34 - epoch 003 lr: 0.098429
2022-08-16 00:05:14 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.2680
2022-08-16 00:05:48 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.4531
2022-08-16 00:06:22 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.3138
2022-08-16 00:06:55 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.3155
2022-08-16 00:07:28 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.3900
2022-08-16 00:08:03 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.0064
2022-08-16 00:08:36 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.3449
2022-08-16 00:09:09 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.3849
2022-08-16 00:09:43 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.1038
2022-08-16 00:10:17 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.2872
2022-08-16 00:10:51 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.1086
2022-08-16 00:11:25 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.1352
2022-08-16 00:11:59 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.0997
2022-08-16 00:12:33 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.0886
2022-08-16 00:13:06 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.4900
2022-08-16 00:13:40 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.2460
2022-08-16 00:14:15 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.1388
2022-08-16 00:14:49 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 2.9551
2022-08-16 00:15:23 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.3058
2022-08-16 00:15:56 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.2375
2022-08-16 00:16:30 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.2032
2022-08-16 00:17:05 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.4602
2022-08-16 00:17:38 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.9015
2022-08-16 00:18:12 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.0839
2022-08-16 00:18:46 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.1961
2022-08-16 00:19:20 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.1687
2022-08-16 00:19:54 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.1478
2022-08-16 00:20:27 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.0224
2022-08-16 00:21:01 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 3.0492
2022-08-16 00:21:35 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.1542
2022-08-16 00:22:11 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.4896
2022-08-16 00:22:44 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.0476
2022-08-16 00:23:18 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0924
2022-08-16 00:23:51 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.2981
2022-08-16 00:24:25 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.9711
2022-08-16 00:24:59 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.9735
2022-08-16 00:25:34 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.1720
2022-08-16 00:26:07 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 2.9592
2022-08-16 00:26:41 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.4170
2022-08-16 00:27:16 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7687
2022-08-16 00:27:50 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.8644
2022-08-16 00:28:23 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.1147
2022-08-16 00:28:57 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.8004
2022-08-16 00:29:31 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 3.0369
2022-08-16 00:30:05 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 2.8726
2022-08-16 00:30:39 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.8003
2022-08-16 00:31:13 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 3.1064
2022-08-16 00:31:47 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.1535
2022-08-16 00:32:21 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.1880
2022-08-16 00:32:54 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.9415
2022-08-16 00:32:55 - train: epoch 003, train_loss: 3.1341
2022-08-16 00:34:10 - eval: epoch: 003, acc1: 37.746%, acc5: 64.192%, test_loss: 2.8785, per_image_load_time: 2.023ms, per_image_inference_time: 0.520ms
2022-08-16 00:34:11 - until epoch: 003, best_acc1: 37.746%
2022-08-16 00:34:11 - epoch 004 lr: 0.096488
2022-08-16 00:34:50 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.9809
2022-08-16 00:35:23 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.9358
2022-08-16 00:35:58 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.9788
2022-08-16 00:36:32 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.8667
2022-08-16 00:37:05 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.8038
2022-08-16 00:37:39 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.0999
2022-08-16 00:38:14 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.8487
2022-08-16 00:38:47 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.8606
2022-08-16 00:39:21 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.8454
2022-08-16 00:39:55 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.9699
2022-08-16 00:40:29 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.0516
2022-08-16 00:41:04 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.6996
2022-08-16 00:41:38 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.6393
2022-08-16 00:42:12 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.8006
2022-08-16 00:42:46 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.8198
2022-08-16 00:43:20 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.8819
2022-08-16 00:43:54 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.9387
2022-08-16 00:44:28 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.9370
2022-08-16 00:45:02 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.8911
2022-08-16 00:45:36 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.8952
2022-08-16 00:46:10 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.8119
2022-08-16 00:46:43 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.8938
2022-08-16 00:47:18 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.4362
2022-08-16 00:47:53 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.6567
2022-08-16 00:48:26 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.8357
2022-08-16 00:49:00 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.7771
2022-08-16 00:49:35 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.5842
2022-08-16 00:50:09 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 3.0305
2022-08-16 00:50:42 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8099
2022-08-16 00:51:16 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.9457
2022-08-16 00:51:50 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.7825
2022-08-16 00:52:24 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8976
2022-08-16 00:52:58 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.9389
2022-08-16 00:53:32 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.8227
2022-08-16 00:54:05 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.6451
2022-08-16 00:54:39 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.7397
2022-08-16 00:55:13 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.7512
2022-08-16 00:55:48 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.5218
2022-08-16 00:56:21 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.5952
2022-08-16 00:56:55 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.4884
2022-08-16 00:57:29 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.6924
2022-08-16 00:58:03 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.8085
2022-08-16 00:58:37 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.5772
2022-08-16 00:59:11 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.8242
2022-08-16 00:59:45 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.2299
2022-08-16 01:00:20 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.8019
2022-08-16 01:00:54 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.6782
2022-08-16 01:01:28 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.5609
2022-08-16 01:02:02 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.7530
2022-08-16 01:02:35 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.9150
2022-08-16 01:02:36 - train: epoch 004, train_loss: 2.8214
2022-08-16 01:03:51 - eval: epoch: 004, acc1: 44.850%, acc5: 70.950%, test_loss: 2.4650, per_image_load_time: 2.428ms, per_image_inference_time: 0.497ms
2022-08-16 01:03:52 - until epoch: 004, best_acc1: 44.850%
2022-08-16 01:03:52 - epoch 005 lr: 0.093815
2022-08-16 01:04:31 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.7190
2022-08-16 01:05:06 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.7919
2022-08-16 01:05:40 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.8856
2022-08-16 01:06:13 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.5664
2022-08-16 01:06:47 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.4574
2022-08-16 01:07:21 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.8000
2022-08-16 01:07:55 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.5640
2022-08-16 01:08:29 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.8285
2022-08-16 01:09:02 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.6961
2022-08-16 01:09:35 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.4877
2022-08-16 01:10:09 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.7537
2022-08-16 01:10:43 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.6894
2022-08-16 01:11:17 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.5827
2022-08-16 01:11:50 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.7293
2022-08-16 01:12:24 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.5414
2022-08-16 01:12:58 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.4684
2022-08-16 01:13:32 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.5291
2022-08-16 01:14:05 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.4671
2022-08-16 01:14:39 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.6759
2022-08-16 01:15:13 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.8048
2022-08-16 01:15:47 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.4704
2022-08-16 01:16:21 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.5900
2022-08-16 01:16:55 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.5615
2022-08-16 01:17:29 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.4696
2022-08-16 01:18:02 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.5322
2022-08-16 01:18:36 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.6058
2022-08-16 01:19:11 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.7730
2022-08-16 01:19:44 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.7589
2022-08-16 01:20:18 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.5813
2022-08-16 01:20:53 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.6109
2022-08-16 01:21:26 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.5950
2022-08-16 01:21:59 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.4472
2022-08-16 01:22:33 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.6397
2022-08-16 01:23:08 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.6926
2022-08-16 01:23:42 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.6229
2022-08-16 01:24:15 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.5708
2022-08-16 01:24:49 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.5692
2022-08-16 01:25:23 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.6267
2022-08-16 01:25:56 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.7961
2022-08-16 01:26:30 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.6734
2022-08-16 01:27:05 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.6075
2022-08-16 01:27:39 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.7304
2022-08-16 01:28:12 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.7154
2022-08-16 01:28:46 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.5725
2022-08-16 01:29:20 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.5659
2022-08-16 01:29:53 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.6029
2022-08-16 01:30:27 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.3466
2022-08-16 01:31:01 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.3970
2022-08-16 01:31:34 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.7225
2022-08-16 01:32:07 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.3336
2022-08-16 01:32:08 - train: epoch 005, train_loss: 2.6230
2022-08-16 01:33:23 - eval: epoch: 005, acc1: 48.032%, acc5: 74.388%, test_loss: 2.2728, per_image_load_time: 2.372ms, per_image_inference_time: 0.521ms
2022-08-16 01:33:24 - until epoch: 005, best_acc1: 48.032%
2022-08-16 01:33:24 - epoch 006 lr: 0.090450
2022-08-16 01:34:03 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.3488
2022-08-16 01:34:37 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.4136
2022-08-16 01:35:11 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.5423
2022-08-16 01:35:45 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.2731
2022-08-16 01:36:19 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.4674
2022-08-16 01:36:52 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.5703
2022-08-16 01:37:26 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.6328
2022-08-16 01:37:59 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.6580
2022-08-16 01:38:33 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3998
2022-08-16 01:39:07 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.3331
2022-08-16 01:39:40 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.6967
2022-08-16 01:40:14 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.5747
2022-08-16 01:40:48 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5683
2022-08-16 01:41:22 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.5716
2022-08-16 01:41:56 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.8490
2022-08-16 01:42:30 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.1805
2022-08-16 01:43:03 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.5421
2022-08-16 01:43:37 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.4829
2022-08-16 01:44:10 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.3260
2022-08-16 01:44:44 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.7278
2022-08-16 01:45:19 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.4259
2022-08-16 01:45:52 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.3531
2022-08-16 01:46:26 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.4654
2022-08-16 01:47:01 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.3103
2022-08-16 01:47:35 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.5154
2022-08-16 01:48:09 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.3579
2022-08-16 01:48:44 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.5219
2022-08-16 01:49:18 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.2852
2022-08-16 01:49:51 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.6339
2022-08-16 01:50:25 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.5255
2022-08-16 01:50:58 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.3188
2022-08-16 01:51:32 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.5426
2022-08-16 01:52:05 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.3412
2022-08-16 01:52:39 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.6272
2022-08-16 01:53:13 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.5025
2022-08-16 01:53:48 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.4588
2022-08-16 01:54:21 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.5697
2022-08-16 01:54:55 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.1590
2022-08-16 01:55:29 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.4629
2022-08-16 01:56:02 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.4994
2022-08-16 01:56:36 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.2813
2022-08-16 01:57:10 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.3267
2022-08-16 01:57:44 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.5878
2022-08-16 01:58:18 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.4469
2022-08-16 01:58:52 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.4553
2022-08-16 01:59:26 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.4666
2022-08-16 02:00:00 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.3469
2022-08-16 02:00:33 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.5454
2022-08-16 02:01:08 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.5203
2022-08-16 02:01:40 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.4341
2022-08-16 02:01:41 - train: epoch 006, train_loss: 2.4810
2022-08-16 02:02:56 - eval: epoch: 006, acc1: 49.694%, acc5: 75.212%, test_loss: 2.2101, per_image_load_time: 1.545ms, per_image_inference_time: 0.510ms
2022-08-16 02:02:57 - until epoch: 006, best_acc1: 49.694%
2022-08-16 02:02:57 - epoch 007 lr: 0.086448
2022-08-16 02:03:36 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.2353
2022-08-16 02:04:09 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.6654
2022-08-16 02:04:43 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.7939
2022-08-16 02:05:17 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.6433
2022-08-16 02:05:51 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.5249
2022-08-16 02:06:24 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.5124
2022-08-16 02:06:58 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.5243
2022-08-16 02:07:33 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.4377
2022-08-16 02:08:06 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.4854
2022-08-16 02:08:40 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.4804
2022-08-16 02:09:14 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.4542
2022-08-16 02:09:48 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.3974
2022-08-16 02:10:22 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.2059
2022-08-16 02:10:56 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.3843
2022-08-16 02:11:29 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.6649
2022-08-16 02:12:04 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.1718
2022-08-16 02:12:38 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.4163
2022-08-16 02:13:11 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.4194
2022-08-16 02:13:46 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.5023
2022-08-16 02:14:19 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.2907
2022-08-16 02:14:53 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.5274
2022-08-16 02:15:27 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.3399
2022-08-16 02:16:00 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.4239
2022-08-16 02:16:34 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.6520
2022-08-16 02:17:08 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.3463
2022-08-16 02:17:41 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.2939
2022-08-16 02:18:16 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.1982
2022-08-16 02:18:50 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.2993
2022-08-16 02:19:24 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.2055
2022-08-16 02:19:57 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.4144
2022-08-16 02:20:31 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.3130
2022-08-16 02:21:05 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.2767
2022-08-16 02:21:39 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.4113
2022-08-16 02:22:13 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.2668
2022-08-16 02:22:47 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.3977
2022-08-16 02:23:21 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.3749
2022-08-16 02:23:55 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.2951
2022-08-16 02:24:29 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.3889
2022-08-16 02:25:03 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.5234
2022-08-16 02:25:36 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.4330
2022-08-16 02:26:10 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.2311
2022-08-16 02:26:44 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2113
2022-08-16 02:27:19 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.3888
2022-08-16 02:27:52 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.1664
2022-08-16 02:28:26 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.3948
2022-08-16 02:29:00 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.5442
2022-08-16 02:29:34 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.4047
2022-08-16 02:30:09 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.5629
2022-08-16 02:30:42 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.2548
2022-08-16 02:31:15 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.2065
2022-08-16 02:31:16 - train: epoch 007, train_loss: 2.3765
2022-08-16 02:32:32 - eval: epoch: 007, acc1: 50.346%, acc5: 76.384%, test_loss: 2.1368, per_image_load_time: 2.399ms, per_image_inference_time: 0.531ms
2022-08-16 02:32:32 - until epoch: 007, best_acc1: 50.346%
2022-08-16 02:32:32 - epoch 008 lr: 0.081870
2022-08-16 02:33:12 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.1821
2022-08-16 02:33:46 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.1572
2022-08-16 02:34:20 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.4435
2022-08-16 02:34:53 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.3031
2022-08-16 02:35:27 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.0755
2022-08-16 02:36:01 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.4516
2022-08-16 02:36:35 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.4225
2022-08-16 02:37:09 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.1152
2022-08-16 02:37:43 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.1725
2022-08-16 02:38:17 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.3410
2022-08-16 02:38:50 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.2674
2022-08-16 02:39:24 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.1675
2022-08-16 02:39:58 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.3247
2022-08-16 02:40:32 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.2135
2022-08-16 02:41:06 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.2631
2022-08-16 02:41:40 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.4035
2022-08-16 02:42:15 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.2024
2022-08-16 02:42:48 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.3916
2022-08-16 02:43:22 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.2777
2022-08-16 02:43:57 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.2610
2022-08-16 02:44:30 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.3325
2022-08-16 02:45:04 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.2920
2022-08-16 02:45:38 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.2056
2022-08-16 02:46:12 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.1287
2022-08-16 02:46:46 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.3380
2022-08-16 02:47:21 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.3345
2022-08-16 02:47:55 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.3567
2022-08-16 02:48:29 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.5715
2022-08-16 02:49:02 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.3741
2022-08-16 02:49:36 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.5474
2022-08-16 02:50:10 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.1963
2022-08-16 02:50:45 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.3040
2022-08-16 02:51:18 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.3074
2022-08-16 02:51:53 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.2341
2022-08-16 02:52:26 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.5044
2022-08-16 02:53:01 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.1735
2022-08-16 02:53:35 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.2019
2022-08-16 02:54:09 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.2674
2022-08-16 02:54:43 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.3363
2022-08-16 02:55:17 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.7132
2022-08-16 02:55:51 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.0698
2022-08-16 02:56:25 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.4096
2022-08-16 02:56:59 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 1.9659
2022-08-16 02:57:33 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.2767
2022-08-16 02:58:06 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.2853
2022-08-16 02:58:40 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.3843
2022-08-16 02:59:15 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2909
2022-08-16 02:59:49 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.2096
2022-08-16 03:00:23 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.5383
2022-08-16 03:00:56 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.3882
2022-08-16 03:00:57 - train: epoch 008, train_loss: 2.2950
2022-08-16 03:02:13 - eval: epoch: 008, acc1: 52.994%, acc5: 78.460%, test_loss: 2.0241, per_image_load_time: 2.121ms, per_image_inference_time: 0.512ms
2022-08-16 03:02:13 - until epoch: 008, best_acc1: 52.994%
2022-08-16 03:02:13 - epoch 009 lr: 0.076790
2022-08-16 03:02:52 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 1.9754
2022-08-16 03:03:27 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.2297
2022-08-16 03:04:00 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.1759
2022-08-16 03:04:35 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.4375
2022-08-16 03:05:08 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.1341
2022-08-16 03:05:42 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.2028
2022-08-16 03:06:16 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.0049
2022-08-16 03:06:50 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.1777
2022-08-16 03:07:23 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 1.9618
2022-08-16 03:07:58 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1673
2022-08-16 03:08:32 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.3947
2022-08-16 03:09:06 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.3172
2022-08-16 03:09:39 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.2441
2022-08-16 03:10:13 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9455
2022-08-16 03:10:47 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.1275
2022-08-16 03:11:21 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.3810
2022-08-16 03:11:55 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.1292
2022-08-16 03:12:29 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.2816
2022-08-16 03:13:03 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.0001
2022-08-16 03:13:37 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 1.9409
2022-08-16 03:14:11 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.2620
2022-08-16 03:14:45 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.2289
2022-08-16 03:15:19 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.2537
2022-08-16 03:15:53 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.1400
2022-08-16 03:16:27 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.2055
2022-08-16 03:17:01 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.3573
2022-08-16 03:17:35 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.1705
2022-08-16 03:18:09 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.0889
2022-08-16 03:18:42 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.1129
2022-08-16 03:19:16 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1281
2022-08-16 03:19:50 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.2300
2022-08-16 03:20:24 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.2713
2022-08-16 03:20:58 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.4206
2022-08-16 03:21:32 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.3960
2022-08-16 03:22:06 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.2565
2022-08-16 03:22:39 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.1576
2022-08-16 03:23:13 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.1140
2022-08-16 03:23:47 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.1571
2022-08-16 03:24:21 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 1.9145
2022-08-16 03:24:55 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.4880
2022-08-16 03:25:28 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.4431
2022-08-16 03:26:03 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 1.9941
2022-08-16 03:26:36 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.0098
2022-08-16 03:27:11 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.2087
2022-08-16 03:27:44 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.1983
2022-08-16 03:28:18 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.1334
2022-08-16 03:28:52 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.2375
2022-08-16 03:29:26 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.3061
2022-08-16 03:30:00 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.2324
2022-08-16 03:30:32 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.1042
2022-08-16 03:30:33 - train: epoch 009, train_loss: 2.2202
2022-08-16 03:31:49 - eval: epoch: 009, acc1: 53.426%, acc5: 78.266%, test_loss: 2.0161, per_image_load_time: 2.318ms, per_image_inference_time: 0.549ms
2022-08-16 03:31:49 - until epoch: 009, best_acc1: 53.426%
2022-08-16 03:31:49 - epoch 010 lr: 0.071288
2022-08-16 03:32:29 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.0083
2022-08-16 03:33:02 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.0850
2022-08-16 03:33:37 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.3677
2022-08-16 03:34:10 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.4780
2022-08-16 03:34:44 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.1389
2022-08-16 03:35:18 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.2527
2022-08-16 03:35:52 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.2472
2022-08-16 03:36:26 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.1165
2022-08-16 03:37:00 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.0858
2022-08-16 03:37:35 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.1906
2022-08-16 03:38:09 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 1.9986
2022-08-16 03:38:43 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.1008
2022-08-16 03:39:17 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.0459
2022-08-16 03:39:51 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.4020
2022-08-16 03:40:26 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.8274
2022-08-16 03:40:59 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.1802
2022-08-16 03:41:34 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.3224
2022-08-16 03:42:07 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 1.9405
2022-08-16 03:42:41 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.1458
2022-08-16 03:43:15 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.2057
2022-08-16 03:43:49 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 2.0526
2022-08-16 03:44:23 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.3343
2022-08-16 03:44:58 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.1895
2022-08-16 03:45:31 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.4020
2022-08-16 03:46:06 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.0350
2022-08-16 03:46:40 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.2221
2022-08-16 03:47:14 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.8155
2022-08-16 03:47:48 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.0601
2022-08-16 03:48:22 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.2914
2022-08-16 03:48:57 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1958
2022-08-16 03:49:31 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.3683
2022-08-16 03:50:06 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.2709
2022-08-16 03:50:39 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.1476
2022-08-16 03:51:13 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.2350
2022-08-16 03:51:47 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.2348
2022-08-16 03:52:22 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.5104
2022-08-16 03:52:55 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.2505
2022-08-16 03:53:30 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1040
2022-08-16 03:54:04 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 1.7915
2022-08-16 03:54:38 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 1.9286
2022-08-16 03:55:12 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 1.9737
2022-08-16 03:55:46 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.2544
2022-08-16 03:56:20 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.1091
2022-08-16 03:56:55 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 1.9410
2022-08-16 03:57:29 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.2143
2022-08-16 03:58:03 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 1.9678
2022-08-16 03:58:37 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1884
2022-08-16 03:59:11 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.0611
2022-08-16 03:59:46 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.1007
2022-08-16 04:00:19 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.2076
2022-08-16 04:00:20 - train: epoch 010, train_loss: 2.1551
2022-08-16 04:01:35 - eval: epoch: 010, acc1: 54.986%, acc5: 80.296%, test_loss: 1.9073, per_image_load_time: 2.345ms, per_image_inference_time: 0.539ms
2022-08-16 04:01:35 - until epoch: 010, best_acc1: 54.986%
2022-08-16 04:01:35 - epoch 011 lr: 0.065450
2022-08-16 04:02:15 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.0769
2022-08-16 04:02:48 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.0916
2022-08-16 04:03:21 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.1516
2022-08-16 04:03:55 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.0551
2022-08-16 04:04:29 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.0645
2022-08-16 04:05:03 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.0975
2022-08-16 04:05:38 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.1717
2022-08-16 04:06:12 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.2108
2022-08-16 04:06:45 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 1.9257
2022-08-16 04:07:20 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.0408
2022-08-16 04:07:54 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.1285
2022-08-16 04:08:28 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2561
2022-08-16 04:09:02 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.3050
2022-08-16 04:09:36 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.0810
2022-08-16 04:10:10 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 1.9288
2022-08-16 04:10:44 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.2195
2022-08-16 04:11:18 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.1711
2022-08-16 04:11:52 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.1060
2022-08-16 04:12:27 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.0280
2022-08-16 04:13:00 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.2065
2022-08-16 04:13:34 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.0135
2022-08-16 04:14:08 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 2.0136
2022-08-16 04:14:42 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.1354
2022-08-16 04:15:16 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.9667
2022-08-16 04:15:50 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.2432
2022-08-16 04:16:25 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 1.9786
2022-08-16 04:17:00 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.0008
2022-08-16 04:17:34 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.9179
2022-08-16 04:18:08 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.0777
2022-08-16 04:18:41 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.3466
2022-08-16 04:19:16 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.1068
2022-08-16 04:19:50 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.0583
2022-08-16 04:20:24 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.1339
2022-08-16 04:20:58 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.0720
2022-08-16 04:21:33 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.1403
2022-08-16 04:22:07 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 1.9545
2022-08-16 04:22:41 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.2931
2022-08-16 04:23:15 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.9116
2022-08-16 04:23:50 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.1980
2022-08-16 04:24:22 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 1.9926
2022-08-16 04:24:58 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.8307
2022-08-16 04:25:32 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 1.9921
2022-08-16 04:26:06 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.0226
2022-08-16 04:26:41 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.3297
2022-08-16 04:27:14 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.0235
2022-08-16 04:27:49 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 1.9032
2022-08-16 04:28:23 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.9004
2022-08-16 04:28:57 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.8858
2022-08-16 04:29:31 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 1.9815
2022-08-16 04:30:05 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.1995
2022-08-16 04:30:06 - train: epoch 011, train_loss: 2.0919
2022-08-16 04:31:21 - eval: epoch: 011, acc1: 57.304%, acc5: 81.458%, test_loss: 1.8139, per_image_load_time: 1.424ms, per_image_inference_time: 0.536ms
2022-08-16 04:31:21 - until epoch: 011, best_acc1: 57.304%
2022-08-16 04:31:21 - epoch 012 lr: 0.059368
2022-08-16 04:32:01 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.9095
2022-08-16 04:32:34 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.9206
2022-08-16 04:33:07 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 1.8826
2022-08-16 04:33:41 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.0806
2022-08-16 04:34:14 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.2309
2022-08-16 04:34:50 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.8215
2022-08-16 04:35:24 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.9494
2022-08-16 04:35:58 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.2017
2022-08-16 04:36:32 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.1443
2022-08-16 04:37:06 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9655
2022-08-16 04:37:39 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.1548
2022-08-16 04:38:13 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.7805
2022-08-16 04:38:47 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.9112
2022-08-16 04:39:22 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.0923
2022-08-16 04:39:56 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 1.9182
2022-08-16 04:40:30 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 2.1174
2022-08-16 04:41:04 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.9089
2022-08-16 04:41:38 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.1507
2022-08-16 04:42:12 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.0461
2022-08-16 04:42:46 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.2604
2022-08-16 04:43:19 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.0396
2022-08-16 04:43:53 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.1490
2022-08-16 04:44:28 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 1.9956
2022-08-16 04:45:02 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.0723
2022-08-16 04:45:36 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.6771
2022-08-16 04:46:09 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.8173
2022-08-16 04:46:44 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.0536
2022-08-16 04:47:19 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.9125
2022-08-16 04:47:52 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.1909
2022-08-16 04:48:26 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.9210
2022-08-16 04:49:00 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.0999
2022-08-16 04:49:34 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.9789
2022-08-16 04:50:08 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.0624
2022-08-16 04:50:42 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 1.8473
2022-08-16 04:51:16 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.2146
2022-08-16 04:51:51 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.9266
2022-08-16 04:52:24 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.0237
2022-08-16 04:52:58 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.1908
2022-08-16 04:53:33 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8540
2022-08-16 04:54:07 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.0231
2022-08-16 04:54:41 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.0509
2022-08-16 04:55:15 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.8311
2022-08-16 04:55:49 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.0103
2022-08-16 04:56:23 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.8796
2022-08-16 04:56:57 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 2.0494
2022-08-16 04:57:32 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 1.9477
2022-08-16 04:58:06 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.8031
2022-08-16 04:58:40 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.1351
2022-08-16 04:59:15 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.8776
2022-08-16 04:59:48 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.6934
2022-08-16 04:59:49 - train: epoch 012, train_loss: 2.0302
2022-08-16 05:01:04 - eval: epoch: 012, acc1: 59.214%, acc5: 83.056%, test_loss: 1.7106, per_image_load_time: 1.147ms, per_image_inference_time: 0.567ms
2022-08-16 05:01:04 - until epoch: 012, best_acc1: 59.214%
2022-08-16 05:01:04 - epoch 013 lr: 0.053138
2022-08-16 05:01:44 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.9093
2022-08-16 05:02:18 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.7939
2022-08-16 05:02:51 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8135
2022-08-16 05:03:24 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.9263
2022-08-16 05:03:58 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 1.8932
2022-08-16 05:04:32 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 2.0426
2022-08-16 05:05:06 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 2.0344
2022-08-16 05:05:40 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 1.9895
2022-08-16 05:06:14 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 2.0132
2022-08-16 05:06:48 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.9043
2022-08-16 05:07:22 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 1.9685
2022-08-16 05:07:57 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.0263
2022-08-16 05:08:31 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.9002
2022-08-16 05:09:04 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.0044
2022-08-16 05:09:39 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.0894
2022-08-16 05:10:12 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.7281
2022-08-16 05:10:46 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.1278
2022-08-16 05:11:21 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 2.1435
2022-08-16 05:11:55 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 1.9715
2022-08-16 05:12:28 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.1589
2022-08-16 05:13:03 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.1574
2022-08-16 05:13:37 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.9861
2022-08-16 05:14:11 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 2.0300
2022-08-16 05:14:45 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 1.9907
2022-08-16 05:15:19 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.9062
2022-08-16 05:15:53 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.8990
2022-08-16 05:16:27 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.9760
2022-08-16 05:17:01 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 1.8476
2022-08-16 05:17:36 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.0788
2022-08-16 05:18:09 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.8602
2022-08-16 05:18:44 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.8318
2022-08-16 05:19:18 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 1.9531
2022-08-16 05:19:53 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.9078
2022-08-16 05:20:27 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 2.0355
2022-08-16 05:21:00 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.8142
2022-08-16 05:21:35 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.0835
2022-08-16 05:22:09 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.7178
2022-08-16 05:22:43 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.0617
2022-08-16 05:23:18 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.2404
2022-08-16 05:23:52 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.9132
2022-08-16 05:24:26 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.8565
2022-08-16 05:24:59 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 2.0428
2022-08-16 05:25:34 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.7742
2022-08-16 05:26:09 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 2.0169
2022-08-16 05:26:43 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.7599
2022-08-16 05:27:17 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 1.9883
2022-08-16 05:27:51 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.9652
2022-08-16 05:28:25 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 1.9403
2022-08-16 05:28:59 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0055
2022-08-16 05:29:32 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 1.9190
2022-08-16 05:29:34 - train: epoch 013, train_loss: 1.9703
2022-08-16 05:30:49 - eval: epoch: 013, acc1: 59.656%, acc5: 83.174%, test_loss: 1.7073, per_image_load_time: 2.301ms, per_image_inference_time: 0.534ms
2022-08-16 05:30:49 - until epoch: 013, best_acc1: 59.656%
2022-08-16 05:30:49 - epoch 014 lr: 0.046859
2022-08-16 05:31:29 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.8372
2022-08-16 05:32:03 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.9368
2022-08-16 05:32:37 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 2.0004
2022-08-16 05:33:10 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.7621
2022-08-16 05:33:44 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.9275
2022-08-16 05:34:19 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.8475
2022-08-16 05:34:53 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.7408
2022-08-16 05:35:27 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.8590
2022-08-16 05:36:01 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.9045
2022-08-16 05:36:35 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.0648
2022-08-16 05:37:09 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.7960
2022-08-16 05:37:42 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 1.8895
2022-08-16 05:38:17 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.0819
2022-08-16 05:38:51 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.9268
2022-08-16 05:39:25 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 1.9058
2022-08-16 05:39:59 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.9926
2022-08-16 05:40:33 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9880
2022-08-16 05:41:08 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.0605
2022-08-16 05:41:42 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.9137
2022-08-16 05:42:16 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.6409
2022-08-16 05:42:50 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.0853
2022-08-16 05:43:24 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.7440
2022-08-16 05:43:58 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.7637
2022-08-16 05:44:32 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 2.0799
2022-08-16 05:45:07 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.8769
2022-08-16 05:45:41 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8982
2022-08-16 05:46:15 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.9902
2022-08-16 05:46:49 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 1.9080
2022-08-16 05:47:23 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.8553
2022-08-16 05:47:57 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.8313
2022-08-16 05:48:32 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.9545
2022-08-16 05:49:06 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.9952
2022-08-16 05:49:40 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.7759
2022-08-16 05:50:14 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.9677
2022-08-16 05:50:48 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.8864
2022-08-16 05:51:22 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.6822
2022-08-16 05:51:56 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.9795
2022-08-16 05:52:30 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 1.9138
2022-08-16 05:53:04 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.9889
2022-08-16 05:53:38 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.9581
2022-08-16 05:54:12 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.8405
2022-08-16 05:54:47 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.7108
2022-08-16 05:55:22 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.8357
2022-08-16 05:55:55 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.8212
2022-08-16 05:56:29 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.7939
2022-08-16 05:57:04 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.7106
2022-08-16 05:57:39 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.8505
2022-08-16 05:58:12 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.8982
2022-08-16 05:58:46 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.9785
2022-08-16 05:59:19 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 2.1293
2022-08-16 05:59:21 - train: epoch 014, train_loss: 1.9064
2022-08-16 06:00:36 - eval: epoch: 014, acc1: 61.084%, acc5: 84.078%, test_loss: 1.6381, per_image_load_time: 1.476ms, per_image_inference_time: 0.551ms
2022-08-16 06:00:36 - until epoch: 014, best_acc1: 61.084%
2022-08-16 06:00:36 - epoch 015 lr: 0.040630
2022-08-16 06:01:16 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.5536
2022-08-16 06:01:49 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.9646
2022-08-16 06:02:22 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 1.9043
2022-08-16 06:02:56 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.8397
2022-08-16 06:03:30 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.8089
2022-08-16 06:04:04 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 1.8329
2022-08-16 06:04:38 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.9343
2022-08-16 06:05:12 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.8544
2022-08-16 06:05:46 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.8459
2022-08-16 06:06:20 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.8039
2022-08-16 06:06:54 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7016
2022-08-16 06:07:28 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.8456
2022-08-16 06:08:03 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 2.0484
2022-08-16 06:08:36 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.7675
2022-08-16 06:09:11 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.8314
2022-08-16 06:09:44 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.8429
2022-08-16 06:10:18 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.7595
2022-08-16 06:10:53 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.7131
2022-08-16 06:11:26 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.8200
2022-08-16 06:12:01 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.7668
2022-08-16 06:12:34 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.6093
2022-08-16 06:13:09 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.8633
2022-08-16 06:13:43 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.7709
2022-08-16 06:14:17 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.8159
2022-08-16 06:14:52 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.9671
2022-08-16 06:15:26 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.7019
2022-08-16 06:15:59 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.9966
2022-08-16 06:16:33 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.8436
2022-08-16 06:17:07 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.8463
2022-08-16 06:17:41 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.6456
2022-08-16 06:18:15 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.6620
2022-08-16 06:18:49 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.7978
2022-08-16 06:19:23 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.6211
2022-08-16 06:19:58 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.9311
2022-08-16 06:20:31 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 1.9950
2022-08-16 06:21:05 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.7364
2022-08-16 06:21:40 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6551
2022-08-16 06:22:14 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.8092
2022-08-16 06:22:48 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.7792
2022-08-16 06:23:22 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.7384
2022-08-16 06:23:56 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.6486
2022-08-16 06:24:30 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.5931
2022-08-16 06:25:04 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.6146
2022-08-16 06:25:38 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.8950
2022-08-16 06:26:11 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.6511
2022-08-16 06:26:46 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.8704
2022-08-16 06:27:20 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.8483
2022-08-16 06:27:54 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.8441
2022-08-16 06:28:28 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.7496
2022-08-16 06:29:01 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 2.0166
2022-08-16 06:29:02 - train: epoch 015, train_loss: 1.8448
2022-08-16 06:30:17 - eval: epoch: 015, acc1: 62.302%, acc5: 84.972%, test_loss: 1.5714, per_image_load_time: 1.954ms, per_image_inference_time: 0.555ms
2022-08-16 06:30:17 - until epoch: 015, best_acc1: 62.302%
2022-08-16 06:30:17 - epoch 016 lr: 0.034548
2022-08-16 06:30:57 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.7462
2022-08-16 06:31:30 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.7268
2022-08-16 06:32:04 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.7917
2022-08-16 06:32:38 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 1.8610
2022-08-16 06:33:12 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.7350
2022-08-16 06:33:47 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.7550
2022-08-16 06:34:20 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.6408
2022-08-16 06:34:54 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.7633
2022-08-16 06:35:28 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.8060
2022-08-16 06:36:02 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.5452
2022-08-16 06:36:36 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.7900
2022-08-16 06:37:10 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.6720
2022-08-16 06:37:45 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.8770
2022-08-16 06:38:19 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.9500
2022-08-16 06:38:53 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.8521
2022-08-16 06:39:26 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.7575
2022-08-16 06:40:01 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.8604
2022-08-16 06:40:35 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.8530
2022-08-16 06:41:09 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.8522
2022-08-16 06:41:43 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.4528
2022-08-16 06:42:18 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.9086
2022-08-16 06:42:51 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.7150
2022-08-16 06:43:26 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.9439
2022-08-16 06:44:00 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.7528
2022-08-16 06:44:35 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.8490
2022-08-16 06:45:08 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.8385
2022-08-16 06:45:42 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.6816
2022-08-16 06:46:16 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.8293
2022-08-16 06:46:50 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.6629
2022-08-16 06:47:25 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.8927
2022-08-16 06:47:59 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.8354
2022-08-16 06:48:32 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.8645
2022-08-16 06:49:07 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9881
2022-08-16 06:49:41 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.7355
2022-08-16 06:50:15 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.7288
2022-08-16 06:50:49 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.5880
2022-08-16 06:51:23 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.9251
2022-08-16 06:51:58 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.0062
2022-08-16 06:52:31 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.8180
2022-08-16 06:53:05 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.8851
2022-08-16 06:53:40 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.7950
2022-08-16 06:54:15 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.8125
2022-08-16 06:54:49 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.7152
2022-08-16 06:55:23 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.6022
2022-08-16 06:55:57 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.9944
2022-08-16 06:56:32 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.7050
2022-08-16 06:57:06 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.9626
2022-08-16 06:57:40 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.8033
2022-08-16 06:58:15 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.8356
2022-08-16 06:58:47 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.7117
2022-08-16 06:58:49 - train: epoch 016, train_loss: 1.7762
2022-08-16 07:00:04 - eval: epoch: 016, acc1: 62.542%, acc5: 84.914%, test_loss: 1.5613, per_image_load_time: 2.012ms, per_image_inference_time: 0.564ms
2022-08-16 07:00:04 - until epoch: 016, best_acc1: 62.542%
2022-08-16 07:00:04 - epoch 017 lr: 0.028710
2022-08-16 07:00:43 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.7247
2022-08-16 07:01:17 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.7333
2022-08-16 07:01:51 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.9901
2022-08-16 07:02:24 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.5340
2022-08-16 07:02:59 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.7331
2022-08-16 07:03:33 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 2.0138
2022-08-16 07:04:07 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.5118
2022-08-16 07:04:41 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.7287
2022-08-16 07:05:15 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.5764
2022-08-16 07:05:49 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.6987
2022-08-16 07:06:23 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 2.0358
2022-08-16 07:06:56 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.6418
2022-08-16 07:07:31 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.7913
2022-08-16 07:08:05 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.6112
2022-08-16 07:08:38 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.5110
2022-08-16 07:09:13 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.6959
2022-08-16 07:09:47 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.7403
2022-08-16 07:10:21 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.6666
2022-08-16 07:10:55 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.7127
2022-08-16 07:11:28 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.5714
2022-08-16 07:12:03 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.7273
2022-08-16 07:12:38 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.4329
2022-08-16 07:13:12 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.7629
2022-08-16 07:13:46 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.5627
2022-08-16 07:14:19 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.8571
2022-08-16 07:14:53 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.7629
2022-08-16 07:15:28 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.7434
2022-08-16 07:16:01 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.7532
2022-08-16 07:16:36 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.8169
2022-08-16 07:17:10 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.5809
2022-08-16 07:17:44 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.9245
2022-08-16 07:18:18 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.7606
2022-08-16 07:18:53 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8223
2022-08-16 07:19:27 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.6508
2022-08-16 07:20:00 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.5174
2022-08-16 07:20:34 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.8792
2022-08-16 07:21:08 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.5821
2022-08-16 07:21:43 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.8120
2022-08-16 07:22:18 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.5496
2022-08-16 07:22:51 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.7403
2022-08-16 07:23:25 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.6330
2022-08-16 07:23:59 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.4556
2022-08-16 07:24:33 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.7662
2022-08-16 07:25:07 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.7695
2022-08-16 07:25:42 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.8158
2022-08-16 07:26:15 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.5548
2022-08-16 07:26:50 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.7411
2022-08-16 07:27:24 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.7917
2022-08-16 07:27:59 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.4858
2022-08-16 07:28:31 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.4835
2022-08-16 07:28:33 - train: epoch 017, train_loss: 1.7091
2022-08-16 07:29:47 - eval: epoch: 017, acc1: 65.058%, acc5: 86.726%, test_loss: 1.4426, per_image_load_time: 2.306ms, per_image_inference_time: 0.549ms
2022-08-16 07:29:47 - until epoch: 017, best_acc1: 65.058%
2022-08-16 07:29:47 - epoch 018 lr: 0.023208
2022-08-16 07:30:26 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.6223
2022-08-16 07:31:00 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.8797
2022-08-16 07:31:33 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.8661
2022-08-16 07:32:06 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.7129
2022-08-16 07:32:40 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.6498
2022-08-16 07:33:14 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.6985
2022-08-16 07:33:48 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.6078
2022-08-16 07:34:22 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.6901
2022-08-16 07:34:55 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.7322
2022-08-16 07:35:29 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.5778
2022-08-16 07:36:03 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.7289
2022-08-16 07:36:37 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.5808
2022-08-16 07:37:11 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.8232
2022-08-16 07:37:44 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.7174
2022-08-16 07:38:18 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.9597
2022-08-16 07:38:51 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.5037
2022-08-16 07:39:25 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7867
2022-08-16 07:39:59 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.7023
2022-08-16 07:40:33 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.5954
2022-08-16 07:41:06 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.8523
2022-08-16 07:41:41 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.6963
2022-08-16 07:42:15 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.8142
2022-08-16 07:42:49 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.5239
2022-08-16 07:43:23 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.5116
2022-08-16 07:43:57 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.5197
2022-08-16 07:44:31 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.6019
2022-08-16 07:45:05 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.7163
2022-08-16 07:45:39 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5629
2022-08-16 07:46:13 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.4817
2022-08-16 07:46:47 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.6344
2022-08-16 07:47:21 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.9227
2022-08-16 07:47:54 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.5820
2022-08-16 07:48:28 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.7081
2022-08-16 07:49:02 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.7378
2022-08-16 07:49:36 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.7357
2022-08-16 07:50:10 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.5221
2022-08-16 07:50:44 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 2.0505
2022-08-16 07:51:18 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.7760
2022-08-16 07:51:53 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.8507
2022-08-16 07:52:27 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.7627
2022-08-16 07:53:01 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.8304
2022-08-16 07:53:35 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.6835
2022-08-16 07:54:09 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.5031
2022-08-16 07:54:43 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.6696
2022-08-16 07:55:17 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.5854
2022-08-16 07:55:50 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.4317
2022-08-16 07:56:25 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.7893
2022-08-16 07:56:59 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6307
2022-08-16 07:57:33 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.4794
2022-08-16 07:58:06 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.8359
2022-08-16 07:58:08 - train: epoch 018, train_loss: 1.6371
2022-08-16 07:59:24 - eval: epoch: 018, acc1: 65.396%, acc5: 86.868%, test_loss: 1.4252, per_image_load_time: 2.171ms, per_image_inference_time: 0.560ms
2022-08-16 07:59:24 - until epoch: 018, best_acc1: 65.396%
2022-08-16 07:59:24 - epoch 019 lr: 0.018128
2022-08-16 08:00:03 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.6333
2022-08-16 08:00:37 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.5131
2022-08-16 08:01:10 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.6155
2022-08-16 08:01:43 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.5268
2022-08-16 08:02:16 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.5279
2022-08-16 08:02:50 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.7230
2022-08-16 08:03:24 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.4288
2022-08-16 08:03:58 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.6757
2022-08-16 08:04:31 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.6343
2022-08-16 08:05:05 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.4432
2022-08-16 08:05:39 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5171
2022-08-16 08:06:13 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.5667
2022-08-16 08:06:46 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.6634
2022-08-16 08:07:19 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.2868
2022-08-16 08:07:54 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.7130
2022-08-16 08:08:27 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.4774
2022-08-16 08:09:01 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.7397
2022-08-16 08:09:34 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.4833
2022-08-16 08:10:09 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.8353
2022-08-16 08:10:43 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.5922
2022-08-16 08:11:16 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.4929
2022-08-16 08:11:50 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.6469
2022-08-16 08:12:24 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.4615
2022-08-16 08:12:59 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.6346
2022-08-16 08:13:32 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.5381
2022-08-16 08:14:06 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.7380
2022-08-16 08:14:40 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.5073
2022-08-16 08:15:14 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.4519
2022-08-16 08:15:48 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.7347
2022-08-16 08:16:22 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.7699
2022-08-16 08:16:56 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.6238
2022-08-16 08:17:29 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.1465
2022-08-16 08:18:04 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.5314
2022-08-16 08:18:38 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.6101
2022-08-16 08:19:12 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.6842
2022-08-16 08:19:46 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.2791
2022-08-16 08:20:20 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.6495
2022-08-16 08:20:54 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.8544
2022-08-16 08:21:28 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.4153
2022-08-16 08:22:02 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.3749
2022-08-16 08:22:36 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.6104
2022-08-16 08:23:09 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.6111
2022-08-16 08:23:44 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.4935
2022-08-16 08:24:18 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.5676
2022-08-16 08:24:52 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.7864
2022-08-16 08:25:25 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.3391
2022-08-16 08:26:00 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.5377
2022-08-16 08:26:34 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.4508
2022-08-16 08:27:08 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.6792
2022-08-16 08:27:41 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.5823
2022-08-16 08:27:43 - train: epoch 019, train_loss: 1.5645
2022-08-16 08:28:57 - eval: epoch: 019, acc1: 67.834%, acc5: 88.394%, test_loss: 1.3141, per_image_load_time: 2.359ms, per_image_inference_time: 0.536ms
2022-08-16 08:28:58 - until epoch: 019, best_acc1: 67.834%
2022-08-16 08:28:58 - epoch 020 lr: 0.013551
2022-08-16 08:29:37 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.4549
2022-08-16 08:30:11 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.3966
2022-08-16 08:30:45 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.5021
2022-08-16 08:31:18 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.5249
2022-08-16 08:31:52 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.3998
2022-08-16 08:32:26 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.4227
2022-08-16 08:33:00 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.3578
2022-08-16 08:33:35 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.4966
2022-08-16 08:34:08 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.5592
2022-08-16 08:34:42 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.5678
2022-08-16 08:35:16 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.5589
2022-08-16 08:35:50 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.3155
2022-08-16 08:36:24 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.5058
2022-08-16 08:36:58 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.4899
2022-08-16 08:37:32 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.6227
2022-08-16 08:38:06 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.4904
2022-08-16 08:38:40 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.4609
2022-08-16 08:39:14 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.6362
2022-08-16 08:39:47 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.3311
2022-08-16 08:40:22 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.3229
2022-08-16 08:40:56 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.5857
2022-08-16 08:41:30 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.4830
2022-08-16 08:42:04 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.3299
2022-08-16 08:42:39 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.6808
2022-08-16 08:43:12 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.3995
2022-08-16 08:43:47 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.4969
2022-08-16 08:44:20 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.6242
2022-08-16 08:44:54 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.6550
2022-08-16 08:45:28 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.4856
2022-08-16 08:46:03 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.8333
2022-08-16 08:46:37 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.6215
2022-08-16 08:47:11 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.5650
2022-08-16 08:47:45 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3632
2022-08-16 08:48:19 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.5650
2022-08-16 08:48:53 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.4684
2022-08-16 08:49:26 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.4433
2022-08-16 08:50:00 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.2757
2022-08-16 08:50:35 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.3959
2022-08-16 08:51:09 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.5970
2022-08-16 08:51:43 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.3223
2022-08-16 08:52:17 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.5787
2022-08-16 08:52:51 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.4903
2022-08-16 08:53:25 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.5774
2022-08-16 08:53:59 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.3585
2022-08-16 08:54:32 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.5484
2022-08-16 08:55:07 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.4698
2022-08-16 08:55:41 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.4777
2022-08-16 08:56:14 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4488
2022-08-16 08:56:48 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.4778
2022-08-16 08:57:22 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.5391
2022-08-16 08:57:23 - train: epoch 020, train_loss: 1.4866
2022-08-16 08:58:39 - eval: epoch: 020, acc1: 69.270%, acc5: 89.220%, test_loss: 1.2494, per_image_load_time: 1.122ms, per_image_inference_time: 0.566ms
2022-08-16 08:58:39 - until epoch: 020, best_acc1: 69.270%
2022-08-16 08:58:39 - epoch 021 lr: 0.009548
2022-08-16 08:59:17 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.6013
2022-08-16 08:59:51 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.4240
2022-08-16 09:00:25 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.4667
2022-08-16 09:00:58 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4343
2022-08-16 09:01:32 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.2587
2022-08-16 09:02:06 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.3229
2022-08-16 09:02:39 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3169
2022-08-16 09:03:13 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.4058
2022-08-16 09:03:48 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.3021
2022-08-16 09:04:21 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.4123
2022-08-16 09:04:55 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.4344
2022-08-16 09:05:29 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.3797
2022-08-16 09:06:03 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.5094
2022-08-16 09:06:37 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.3828
2022-08-16 09:07:11 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.4692
2022-08-16 09:07:45 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3956
2022-08-16 09:08:20 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.3878
2022-08-16 09:08:53 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.2689
2022-08-16 09:09:28 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.5825
2022-08-16 09:10:02 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.3471
2022-08-16 09:10:36 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.4298
2022-08-16 09:11:10 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.4245
2022-08-16 09:11:44 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.4624
2022-08-16 09:12:18 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.2818
2022-08-16 09:12:52 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.3166
2022-08-16 09:13:26 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.3116
2022-08-16 09:14:01 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.5230
2022-08-16 09:14:35 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.4387
2022-08-16 09:15:08 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.3358
2022-08-16 09:15:43 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.4479
2022-08-16 09:16:17 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.5420
2022-08-16 09:16:51 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.4582
2022-08-16 09:17:25 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.7396
2022-08-16 09:17:59 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.5525
2022-08-16 09:18:33 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.3021
2022-08-16 09:19:08 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.2226
2022-08-16 09:19:42 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.4035
2022-08-16 09:20:16 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.4297
2022-08-16 09:20:51 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.4393
2022-08-16 09:21:25 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.5818
2022-08-16 09:21:59 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.4313
2022-08-16 09:22:33 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.4467
2022-08-16 09:23:07 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3945
2022-08-16 09:23:41 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.4582
2022-08-16 09:24:15 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.4142
2022-08-16 09:24:49 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.3097
2022-08-16 09:25:23 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.5390
2022-08-16 09:25:58 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.4815
2022-08-16 09:26:32 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.2778
2022-08-16 09:27:05 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.3236
2022-08-16 09:27:06 - train: epoch 021, train_loss: 1.4132
2022-08-16 09:28:21 - eval: epoch: 021, acc1: 70.388%, acc5: 89.680%, test_loss: 1.2077, per_image_load_time: 2.341ms, per_image_inference_time: 0.544ms
2022-08-16 09:28:22 - until epoch: 021, best_acc1: 70.388%
2022-08-16 09:28:22 - epoch 022 lr: 0.006184
2022-08-16 09:29:01 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.0621
2022-08-16 09:29:35 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.3403
2022-08-16 09:30:09 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.1664
2022-08-16 09:30:44 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.3389
2022-08-16 09:31:18 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3091
2022-08-16 09:31:52 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.2761
2022-08-16 09:32:26 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.3469
2022-08-16 09:33:00 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.4789
2022-08-16 09:33:34 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.3935
2022-08-16 09:34:08 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.2090
2022-08-16 09:34:42 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.2852
2022-08-16 09:35:17 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.1488
2022-08-16 09:35:50 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.2039
2022-08-16 09:36:25 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.1771
2022-08-16 09:36:59 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.3755
2022-08-16 09:37:33 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.2383
2022-08-16 09:38:07 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.4686
2022-08-16 09:38:42 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.4483
2022-08-16 09:39:16 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.4197
2022-08-16 09:39:51 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.3332
2022-08-16 09:40:25 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.2427
2022-08-16 09:40:59 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 0.9767
2022-08-16 09:41:34 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.2492
2022-08-16 09:42:07 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.5962
2022-08-16 09:42:42 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.3452
2022-08-16 09:43:16 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.2467
2022-08-16 09:43:50 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.2084
2022-08-16 09:44:24 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.3658
2022-08-16 09:44:58 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.2037
2022-08-16 09:45:33 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.2531
2022-08-16 09:46:06 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.5488
2022-08-16 09:46:41 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.3839
2022-08-16 09:47:15 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.3827
2022-08-16 09:47:49 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.2096
2022-08-16 09:48:24 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.4123
2022-08-16 09:48:59 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.5700
2022-08-16 09:49:32 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.3726
2022-08-16 09:50:07 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.3686
2022-08-16 09:50:41 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.3844
2022-08-16 09:51:16 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.3247
2022-08-16 09:51:49 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.3034
2022-08-16 09:52:24 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.3224
2022-08-16 09:52:59 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.6342
2022-08-16 09:53:32 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3078
2022-08-16 09:54:07 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.2765
2022-08-16 09:54:41 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.4388
2022-08-16 09:55:15 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.3443
2022-08-16 09:55:49 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.1999
2022-08-16 09:56:23 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.3735
2022-08-16 09:56:56 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.2325
2022-08-16 09:56:57 - train: epoch 022, train_loss: 1.3466
2022-08-16 09:58:13 - eval: epoch: 022, acc1: 71.406%, acc5: 90.332%, test_loss: 1.1585, per_image_load_time: 2.350ms, per_image_inference_time: 0.558ms
2022-08-16 09:58:13 - until epoch: 022, best_acc1: 71.406%
2022-08-16 09:58:13 - epoch 023 lr: 0.003511
2022-08-16 09:58:52 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.2230
2022-08-16 09:59:26 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.3692
2022-08-16 09:59:59 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.1786
2022-08-16 10:00:34 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.3575
2022-08-16 10:01:07 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.3841
2022-08-16 10:01:40 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.1112
2022-08-16 10:02:14 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.1298
2022-08-16 10:02:48 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3167
2022-08-16 10:03:22 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.3452
2022-08-16 10:03:56 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.1249
2022-08-16 10:04:30 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.2132
2022-08-16 10:05:04 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.1682
2022-08-16 10:05:38 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.2586
2022-08-16 10:06:13 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.2868
2022-08-16 10:06:47 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.2132
2022-08-16 10:07:21 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.3792
2022-08-16 10:07:54 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.3178
2022-08-16 10:08:28 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.2754
2022-08-16 10:09:02 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.3404
2022-08-16 10:09:36 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.0659
2022-08-16 10:10:11 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.2097
2022-08-16 10:10:46 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.2637
2022-08-16 10:11:19 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.0971
2022-08-16 10:11:53 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.4109
2022-08-16 10:12:28 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.2547
2022-08-16 10:13:01 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.3352
2022-08-16 10:13:36 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.3277
2022-08-16 10:14:09 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.2878
2022-08-16 10:14:43 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.3721
2022-08-16 10:15:18 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.3158
2022-08-16 10:15:52 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.2742
2022-08-16 10:16:27 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.3229
2022-08-16 10:17:00 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.3720
2022-08-16 10:17:35 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.1658
2022-08-16 10:18:08 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.3472
2022-08-16 10:18:42 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.2326
2022-08-16 10:19:16 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.2303
2022-08-16 10:19:51 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.3941
2022-08-16 10:20:25 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3380
2022-08-16 10:20:59 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2234
2022-08-16 10:21:33 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.1871
2022-08-16 10:22:08 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.4118
2022-08-16 10:22:41 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.1330
2022-08-16 10:23:15 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.1558
2022-08-16 10:23:50 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.1362
2022-08-16 10:24:24 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.3313
2022-08-16 10:24:58 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.1855
2022-08-16 10:25:32 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.2219
2022-08-16 10:26:07 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.1151
2022-08-16 10:26:40 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.2942
2022-08-16 10:26:41 - train: epoch 023, train_loss: 1.2894
2022-08-16 10:27:55 - eval: epoch: 023, acc1: 72.222%, acc5: 90.684%, test_loss: 1.1308, per_image_load_time: 1.581ms, per_image_inference_time: 0.569ms
2022-08-16 10:27:55 - until epoch: 023, best_acc1: 72.222%
2022-08-16 10:27:55 - epoch 024 lr: 0.001571
2022-08-16 10:28:35 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.3795
2022-08-16 10:29:08 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.2318
2022-08-16 10:29:42 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.1596
2022-08-16 10:30:16 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.3163
2022-08-16 10:30:50 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.2794
2022-08-16 10:31:23 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.3734
2022-08-16 10:31:57 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.1901
2022-08-16 10:32:32 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.1409
2022-08-16 10:33:06 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.2949
2022-08-16 10:33:40 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.2888
2022-08-16 10:34:14 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.0348
2022-08-16 10:34:47 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.1775
2022-08-16 10:35:22 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4815
2022-08-16 10:35:55 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.3237
2022-08-16 10:36:30 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.3280
2022-08-16 10:37:03 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.1963
2022-08-16 10:37:38 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.1904
2022-08-16 10:38:11 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.4989
2022-08-16 10:38:45 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.0779
2022-08-16 10:39:19 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.1840
2022-08-16 10:39:54 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.1039
2022-08-16 10:40:27 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.1845
2022-08-16 10:41:01 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.3429
2022-08-16 10:41:35 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.3767
2022-08-16 10:42:09 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.2742
2022-08-16 10:42:43 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.5081
2022-08-16 10:43:17 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.3122
2022-08-16 10:43:52 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.1546
2022-08-16 10:44:25 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.4438
2022-08-16 10:44:59 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.2739
2022-08-16 10:45:34 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.0519
2022-08-16 10:46:08 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.2941
2022-08-16 10:46:42 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.0200
2022-08-16 10:47:15 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.1089
2022-08-16 10:47:49 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.1169
2022-08-16 10:48:23 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.2127
2022-08-16 10:48:57 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.3502
2022-08-16 10:49:31 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.2364
2022-08-16 10:50:06 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.1849
2022-08-16 10:50:39 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.3148
2022-08-16 10:51:14 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.3219
2022-08-16 10:51:47 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.2772
2022-08-16 10:52:21 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.2582
2022-08-16 10:52:55 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.3102
2022-08-16 10:53:30 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.1431
2022-08-16 10:54:04 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.1885
2022-08-16 10:54:38 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.2397
2022-08-16 10:55:12 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 0.9138
2022-08-16 10:55:47 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.3818
2022-08-16 10:56:18 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.2663
2022-08-16 10:56:20 - train: epoch 024, train_loss: 1.2547
2022-08-16 10:57:34 - eval: epoch: 024, acc1: 72.446%, acc5: 90.836%, test_loss: 1.1170, per_image_load_time: 1.051ms, per_image_inference_time: 0.537ms
2022-08-16 10:57:34 - until epoch: 024, best_acc1: 72.446%
2022-08-16 10:57:34 - epoch 025 lr: 0.000394
2022-08-16 10:58:14 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.1103
2022-08-16 10:58:48 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1541
2022-08-16 10:59:22 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.2460
2022-08-16 10:59:55 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.1918
2022-08-16 11:00:30 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.0198
2022-08-16 11:01:04 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.2489
2022-08-16 11:01:37 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.1120
2022-08-16 11:02:11 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.2158
2022-08-16 11:02:45 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0432
2022-08-16 11:03:19 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.4012
2022-08-16 11:03:53 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.2479
2022-08-16 11:04:27 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.1474
2022-08-16 11:05:01 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3261
2022-08-16 11:05:36 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.3143
2022-08-16 11:06:10 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.2035
2022-08-16 11:06:44 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1869
2022-08-16 11:07:18 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.1695
2022-08-16 11:07:53 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.0098
2022-08-16 11:08:27 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.0889
2022-08-16 11:09:01 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.3164
2022-08-16 11:09:35 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.0133
2022-08-16 11:10:09 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.1823
2022-08-16 11:10:44 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.2440
2022-08-16 11:11:17 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.0486
2022-08-16 11:11:52 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.1035
2022-08-16 11:12:26 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.1861
2022-08-16 11:13:00 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.2154
2022-08-16 11:13:33 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.1737
2022-08-16 11:14:08 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.2779
2022-08-16 11:14:42 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.3744
2022-08-16 11:15:16 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.2531
2022-08-16 11:15:50 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.3239
2022-08-16 11:16:24 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.1192
2022-08-16 11:16:59 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.2581
2022-08-16 11:17:33 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1102
2022-08-16 11:18:07 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.1010
2022-08-16 11:18:42 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.0788
2022-08-16 11:19:16 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.4727
2022-08-16 11:19:50 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.4225
2022-08-16 11:20:24 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.1846
2022-08-16 11:20:59 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.3539
2022-08-16 11:21:32 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.4357
2022-08-16 11:22:07 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.1728
2022-08-16 11:22:42 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.3154
2022-08-16 11:23:15 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.2691
2022-08-16 11:23:50 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.1661
2022-08-16 11:24:24 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.2050
2022-08-16 11:24:58 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.2001
2022-08-16 11:25:33 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.2298
2022-08-16 11:26:06 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.1970
2022-08-16 11:26:08 - train: epoch 025, train_loss: 1.2315
2022-08-16 11:27:23 - eval: epoch: 025, acc1: 72.500%, acc5: 90.872%, test_loss: 1.1138, per_image_load_time: 2.321ms, per_image_inference_time: 0.533ms
2022-08-16 11:27:24 - until epoch: 025, best_acc1: 72.500%
2022-08-16 11:27:24 - train done. train time: 12.369 hours, best_acc1: 72.500%
