2022-08-19 14:04:21 - net_idx: 9
2022-08-19 14:04:21 - net_config: {'stem_width': 64, 'depth': 12, 'w_0': 48, 'w_a': 23.410581912946537, 'w_m': 1.7151011761493211}
2022-08-19 14:04:21 - num_classes: 1000
2022-08-19 14:04:21 - input_image_size: 224
2022-08-19 14:04:21 - scale: 1.1428571428571428
2022-08-19 14:04:21 - seed: 0
2022-08-19 14:04:21 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-19 14:04:21 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-19 14:04:21 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c2100>
2022-08-19 14:04:21 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c23d0>
2022-08-19 14:04:21 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2400>
2022-08-19 14:04:21 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2460>
2022-08-19 14:04:21 - batch_size: 256
2022-08-19 14:04:21 - num_workers: 16
2022-08-19 14:04:21 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-19 14:04:21 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-19 14:04:21 - epochs: 25
2022-08-19 14:04:21 - print_interval: 100
2022-08-19 14:04:21 - accumulation_steps: 1
2022-08-19 14:04:21 - sync_bn: False
2022-08-19 14:04:21 - apex: True
2022-08-19 14:04:21 - use_ema_model: False
2022-08-19 14:04:21 - ema_model_decay: 0.9999
2022-08-19 14:04:21 - log_dir: ./log
2022-08-19 14:04:21 - checkpoint_dir: ./checkpoints
2022-08-19 14:04:21 - gpus_type: NVIDIA RTX A5000
2022-08-19 14:04:21 - gpus_num: 2
2022-08-19 14:04:21 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f4efca864b0>
2022-08-19 14:04:21 - ema_model: None
2022-08-19 14:04:21 - --------------------parameters--------------------
2022-08-19 14:04:21 - name: conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-19 14:04:21 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-19 14:04:21 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-19 14:04:21 - name: fc.weight, grad: True
2022-08-19 14:04:21 - name: fc.bias, grad: True
2022-08-19 14:04:21 - --------------------buffers--------------------
2022-08-19 14:04:21 - name: conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-19 14:04:21 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-19 14:04:21 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 14:04:21 - -----------no weight decay layers--------------
2022-08-19 14:04:21 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 14:04:21 - -------------weight decay layers---------------
2022-08-19 14:04:21 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 14:04:21 - epoch 001 lr: 0.100000
2022-08-19 14:05:02 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9042
2022-08-19 14:05:38 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.8983
2022-08-19 14:06:14 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8662
2022-08-19 14:06:49 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.8394
2022-08-19 14:07:23 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.7539
2022-08-19 14:07:59 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.6059
2022-08-19 14:08:32 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.6813
2022-08-19 14:09:07 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.5191
2022-08-19 14:09:42 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.4538
2022-08-19 14:10:18 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.4364
2022-08-19 14:10:52 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.3572
2022-08-19 14:11:27 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.3301
2022-08-19 14:12:02 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.3575
2022-08-19 14:12:37 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.1566
2022-08-19 14:13:13 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 6.0496
2022-08-19 14:13:47 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.2072
2022-08-19 14:14:21 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.8642
2022-08-19 14:14:57 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.8167
2022-08-19 14:15:32 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.7270
2022-08-19 14:16:08 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.6986
2022-08-19 14:16:43 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.6436
2022-08-19 14:17:18 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.5555
2022-08-19 14:17:53 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.3469
2022-08-19 14:18:28 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.4649
2022-08-19 14:19:02 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.4048
2022-08-19 14:19:37 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.5145
2022-08-19 14:20:13 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.3381
2022-08-19 14:20:48 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.2211
2022-08-19 14:21:23 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.0967
2022-08-19 14:21:58 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.3805
2022-08-19 14:22:34 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.2143
2022-08-19 14:23:09 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.2238
2022-08-19 14:23:45 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 5.0537
2022-08-19 14:24:19 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.9486
2022-08-19 14:24:53 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.8904
2022-08-19 14:25:28 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.9625
2022-08-19 14:26:04 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.9611
2022-08-19 14:26:39 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.6061
2022-08-19 14:27:14 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.7291
2022-08-19 14:27:50 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.7576
2022-08-19 14:28:25 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.8315
2022-08-19 14:29:00 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.8115
2022-08-19 14:29:35 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.5934
2022-08-19 14:30:09 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.2630
2022-08-19 14:30:44 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.5107
2022-08-19 14:31:20 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.6312
2022-08-19 14:31:55 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.4031
2022-08-19 14:32:30 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.5197
2022-08-19 14:33:05 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.5563
2022-08-19 14:33:40 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.2138
2022-08-19 14:33:41 - train: epoch 001, train_loss: 5.5188
2022-08-19 14:35:00 - eval: epoch: 001, acc1: 17.072%, acc5: 38.292%, test_loss: 4.6228, per_image_load_time: 2.489ms, per_image_inference_time: 0.553ms
2022-08-19 14:35:00 - until epoch: 001, best_acc1: 17.072%
2022-08-19 14:35:00 - epoch 002 lr: 0.099606
2022-08-19 14:35:41 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.4476
2022-08-19 14:36:15 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.1892
2022-08-19 14:36:50 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.4213
2022-08-19 14:37:25 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.3594
2022-08-19 14:38:00 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.0688
2022-08-19 14:38:35 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.0786
2022-08-19 14:39:10 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.2912
2022-08-19 14:39:45 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 3.8967
2022-08-19 14:40:20 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.8765
2022-08-19 14:40:54 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.1755
2022-08-19 14:41:28 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.1953
2022-08-19 14:42:04 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 4.0083
2022-08-19 14:42:39 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.9409
2022-08-19 14:43:13 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.1764
2022-08-19 14:43:48 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 3.9215
2022-08-19 14:44:23 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.7830
2022-08-19 14:44:58 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.8690
2022-08-19 14:45:33 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 4.0235
2022-08-19 14:46:07 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.8045
2022-08-19 14:46:40 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.6163
2022-08-19 14:47:16 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.7565
2022-08-19 14:47:51 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.5190
2022-08-19 14:48:26 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.8519
2022-08-19 14:49:01 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.6916
2022-08-19 14:49:36 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.6716
2022-08-19 14:50:11 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.6248
2022-08-19 14:50:46 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.8531
2022-08-19 14:51:20 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.8858
2022-08-19 14:51:55 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.7109
2022-08-19 14:52:30 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.5119
2022-08-19 14:53:05 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.6531
2022-08-19 14:53:40 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.4843
2022-08-19 14:54:16 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.6812
2022-08-19 14:54:51 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.5750
2022-08-19 14:55:26 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.5866
2022-08-19 14:56:01 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.4402
2022-08-19 14:56:36 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.5702
2022-08-19 14:57:10 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.3047
2022-08-19 14:57:46 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.5334
2022-08-19 14:58:21 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.3620
2022-08-19 14:58:57 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.7181
2022-08-19 14:59:32 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.4511
2022-08-19 15:00:07 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.4846
2022-08-19 15:00:42 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.5104
2022-08-19 15:01:17 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.2816
2022-08-19 15:01:51 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.2912
2022-08-19 15:02:25 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.4723
2022-08-19 15:03:00 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.4282
2022-08-19 15:03:35 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.3151
2022-08-19 15:04:09 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.2103
2022-08-19 15:04:11 - train: epoch 002, train_loss: 3.7601
2022-08-19 15:05:28 - eval: epoch: 002, acc1: 29.278%, acc5: 54.686%, test_loss: 3.4375, per_image_load_time: 1.847ms, per_image_inference_time: 0.563ms
2022-08-19 15:05:28 - until epoch: 002, best_acc1: 29.278%
2022-08-19 15:05:28 - epoch 003 lr: 0.098429
2022-08-19 15:06:09 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.3939
2022-08-19 15:06:45 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.3023
2022-08-19 15:07:19 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.4420
2022-08-19 15:07:52 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.2559
2022-08-19 15:08:27 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.2827
2022-08-19 15:09:02 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.0765
2022-08-19 15:09:37 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.3901
2022-08-19 15:10:12 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.4376
2022-08-19 15:10:47 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.1019
2022-08-19 15:11:22 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.3023
2022-08-19 15:11:57 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.1477
2022-08-19 15:12:32 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.3036
2022-08-19 15:13:05 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.0033
2022-08-19 15:13:39 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 2.9801
2022-08-19 15:14:14 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.2790
2022-08-19 15:14:50 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.1962
2022-08-19 15:15:25 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.0553
2022-08-19 15:15:59 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.0784
2022-08-19 15:16:34 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.2546
2022-08-19 15:17:09 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.1487
2022-08-19 15:17:44 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.0972
2022-08-19 15:18:19 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.4205
2022-08-19 15:18:53 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.9126
2022-08-19 15:19:28 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.0308
2022-08-19 15:20:03 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.1001
2022-08-19 15:20:38 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.0599
2022-08-19 15:21:13 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.4705
2022-08-19 15:21:48 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.0575
2022-08-19 15:22:24 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 3.0535
2022-08-19 15:22:59 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.2092
2022-08-19 15:23:34 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.3271
2022-08-19 15:24:08 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.1609
2022-08-19 15:24:44 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0658
2022-08-19 15:25:20 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.2170
2022-08-19 15:25:55 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.8831
2022-08-19 15:26:30 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 3.1931
2022-08-19 15:27:06 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.2037
2022-08-19 15:27:42 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.1321
2022-08-19 15:28:17 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.3467
2022-08-19 15:28:52 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.9296
2022-08-19 15:29:26 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 3.0620
2022-08-19 15:30:01 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.0712
2022-08-19 15:30:37 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 3.0034
2022-08-19 15:31:12 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.9881
2022-08-19 15:31:48 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.0217
2022-08-19 15:32:23 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 3.0143
2022-08-19 15:32:58 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 3.0003
2022-08-19 15:33:33 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.2068
2022-08-19 15:34:08 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.2360
2022-08-19 15:34:42 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.9652
2022-08-19 15:34:43 - train: epoch 003, train_loss: 3.1291
2022-08-19 15:36:01 - eval: epoch: 003, acc1: 38.500%, acc5: 65.064%, test_loss: 2.8186, per_image_load_time: 2.463ms, per_image_inference_time: 0.557ms
2022-08-19 15:36:01 - until epoch: 003, best_acc1: 38.500%
2022-08-19 15:36:01 - epoch 004 lr: 0.096488
2022-08-19 15:36:43 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.9352
2022-08-19 15:37:18 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.9074
2022-08-19 15:37:53 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.9534
2022-08-19 15:38:28 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.8933
2022-08-19 15:39:02 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.8016
2022-08-19 15:39:37 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.2415
2022-08-19 15:40:12 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.9333
2022-08-19 15:40:47 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.6751
2022-08-19 15:41:22 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.5406
2022-08-19 15:41:56 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.9949
2022-08-19 15:42:31 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.1360
2022-08-19 15:43:06 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.7963
2022-08-19 15:43:41 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.7850
2022-08-19 15:44:16 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.7349
2022-08-19 15:44:51 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 3.1286
2022-08-19 15:45:26 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.9488
2022-08-19 15:46:01 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.7363
2022-08-19 15:46:36 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.8259
2022-08-19 15:47:11 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.8857
2022-08-19 15:47:46 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.7900
2022-08-19 15:48:21 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.9524
2022-08-19 15:48:56 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.6255
2022-08-19 15:49:31 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.3665
2022-08-19 15:50:07 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.6438
2022-08-19 15:50:41 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.6183
2022-08-19 15:51:16 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.8004
2022-08-19 15:51:51 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.6639
2022-08-19 15:52:26 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.7062
2022-08-19 15:53:02 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.6628
2022-08-19 15:53:37 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.9581
2022-08-19 15:54:12 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.6500
2022-08-19 15:54:47 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8754
2022-08-19 15:55:22 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8483
2022-08-19 15:55:56 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.7363
2022-08-19 15:56:31 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.8284
2022-08-19 15:57:06 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.6491
2022-08-19 15:57:41 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.7253
2022-08-19 15:58:16 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.5671
2022-08-19 15:58:51 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.6406
2022-08-19 15:59:26 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.4571
2022-08-19 16:00:01 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.7001
2022-08-19 16:00:37 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6414
2022-08-19 16:01:12 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.6639
2022-08-19 16:01:47 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.8739
2022-08-19 16:02:22 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.2854
2022-08-19 16:02:57 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.7607
2022-08-19 16:03:32 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.7903
2022-08-19 16:04:07 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.6127
2022-08-19 16:04:43 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.6986
2022-08-19 16:05:17 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.7370
2022-08-19 16:05:18 - train: epoch 004, train_loss: 2.8198
2022-08-19 16:06:36 - eval: epoch: 004, acc1: 43.678%, acc5: 69.768%, test_loss: 2.5575, per_image_load_time: 2.433ms, per_image_inference_time: 0.568ms
2022-08-19 16:06:37 - until epoch: 004, best_acc1: 43.678%
2022-08-19 16:06:37 - epoch 005 lr: 0.093815
2022-08-19 16:07:18 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.7981
2022-08-19 16:07:53 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.8235
2022-08-19 16:08:28 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.9574
2022-08-19 16:09:02 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.5200
2022-08-19 16:09:35 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.6227
2022-08-19 16:10:09 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.7062
2022-08-19 16:10:43 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.5735
2022-08-19 16:11:17 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.9038
2022-08-19 16:11:51 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.7840
2022-08-19 16:12:24 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.6378
2022-08-19 16:12:58 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.9125
2022-08-19 16:13:32 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.7082
2022-08-19 16:14:06 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.6332
2022-08-19 16:14:41 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.6649
2022-08-19 16:15:15 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.4566
2022-08-19 16:15:49 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.4729
2022-08-19 16:16:23 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.6221
2022-08-19 16:16:57 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.5957
2022-08-19 16:17:31 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.5543
2022-08-19 16:18:06 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.5999
2022-08-19 16:18:40 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.5399
2022-08-19 16:19:14 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.5593
2022-08-19 16:19:48 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.6041
2022-08-19 16:20:23 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.7986
2022-08-19 16:20:57 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.7054
2022-08-19 16:21:31 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.8000
2022-08-19 16:22:06 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.7783
2022-08-19 16:22:39 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.5439
2022-08-19 16:23:14 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.5689
2022-08-19 16:23:48 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.5417
2022-08-19 16:24:23 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.5414
2022-08-19 16:24:57 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.7155
2022-08-19 16:25:32 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.6409
2022-08-19 16:26:06 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.5026
2022-08-19 16:26:41 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.5850
2022-08-19 16:27:15 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6240
2022-08-19 16:27:49 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.5928
2022-08-19 16:28:23 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.6289
2022-08-19 16:28:58 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.8344
2022-08-19 16:29:32 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.5982
2022-08-19 16:30:07 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.8228
2022-08-19 16:30:41 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.6706
2022-08-19 16:31:16 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.6596
2022-08-19 16:31:50 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.5408
2022-08-19 16:32:24 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.8162
2022-08-19 16:32:59 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.5787
2022-08-19 16:33:34 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.4214
2022-08-19 16:34:08 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.4392
2022-08-19 16:34:43 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.8689
2022-08-19 16:35:16 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.3889
2022-08-19 16:35:18 - train: epoch 005, train_loss: 2.6339
2022-08-19 16:36:34 - eval: epoch: 005, acc1: 47.362%, acc5: 73.850%, test_loss: 2.3025, per_image_load_time: 2.426ms, per_image_inference_time: 0.539ms
2022-08-19 16:36:34 - until epoch: 005, best_acc1: 47.362%
2022-08-19 16:36:34 - epoch 006 lr: 0.090450
2022-08-19 16:37:15 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.6066
2022-08-19 16:37:49 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.6190
2022-08-19 16:38:24 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.5525
2022-08-19 16:38:58 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.6102
2022-08-19 16:39:33 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.5706
2022-08-19 16:40:07 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.5267
2022-08-19 16:40:41 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.7094
2022-08-19 16:41:15 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.5004
2022-08-19 16:41:49 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3399
2022-08-19 16:42:23 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.3684
2022-08-19 16:42:58 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.4991
2022-08-19 16:43:32 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.5882
2022-08-19 16:44:06 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5543
2022-08-19 16:44:40 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.5816
2022-08-19 16:45:15 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.6972
2022-08-19 16:45:49 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3536
2022-08-19 16:46:23 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.4879
2022-08-19 16:46:57 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.5037
2022-08-19 16:47:32 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.3460
2022-08-19 16:48:06 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.7144
2022-08-19 16:48:41 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.3703
2022-08-19 16:49:15 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.3460
2022-08-19 16:49:49 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.4694
2022-08-19 16:50:24 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.5646
2022-08-19 16:50:58 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4572
2022-08-19 16:51:32 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.3596
2022-08-19 16:52:06 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.4860
2022-08-19 16:52:41 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.2550
2022-08-19 16:53:16 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.6274
2022-08-19 16:53:50 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.5726
2022-08-19 16:54:24 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.3007
2022-08-19 16:54:58 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.5757
2022-08-19 16:55:32 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.5014
2022-08-19 16:56:07 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.7261
2022-08-19 16:56:41 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.5558
2022-08-19 16:57:15 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.4813
2022-08-19 16:57:50 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.4801
2022-08-19 16:58:24 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.2479
2022-08-19 16:58:58 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.4951
2022-08-19 16:59:33 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.7070
2022-08-19 17:00:07 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.4219
2022-08-19 17:00:42 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.2798
2022-08-19 17:01:16 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.3926
2022-08-19 17:01:50 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.4747
2022-08-19 17:02:25 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.4637
2022-08-19 17:02:59 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.6336
2022-08-19 17:03:33 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4001
2022-08-19 17:04:08 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.5607
2022-08-19 17:04:42 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.5624
2022-08-19 17:05:16 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.2285
2022-08-19 17:05:17 - train: epoch 006, train_loss: 2.5020
2022-08-19 17:06:34 - eval: epoch: 006, acc1: 47.878%, acc5: 73.698%, test_loss: 2.3079, per_image_load_time: 2.439ms, per_image_inference_time: 0.545ms
2022-08-19 17:06:34 - until epoch: 006, best_acc1: 47.878%
2022-08-19 17:06:34 - epoch 007 lr: 0.086448
2022-08-19 17:07:14 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.3669
2022-08-19 17:07:48 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.4678
2022-08-19 17:08:23 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.7428
2022-08-19 17:08:57 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.5483
2022-08-19 17:09:31 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.4588
2022-08-19 17:10:05 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.3487
2022-08-19 17:10:39 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.4302
2022-08-19 17:11:14 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.4437
2022-08-19 17:11:48 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.6974
2022-08-19 17:12:22 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.3815
2022-08-19 17:12:57 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.3655
2022-08-19 17:13:31 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.3341
2022-08-19 17:14:05 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.2246
2022-08-19 17:14:40 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.5068
2022-08-19 17:15:14 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.5059
2022-08-19 17:15:49 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.4751
2022-08-19 17:16:23 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.3378
2022-08-19 17:16:57 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.3289
2022-08-19 17:17:31 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.5125
2022-08-19 17:18:05 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.0032
2022-08-19 17:18:39 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.6411
2022-08-19 17:19:14 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.4254
2022-08-19 17:19:48 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.4580
2022-08-19 17:20:22 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.6480
2022-08-19 17:20:56 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.4070
2022-08-19 17:21:30 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.3839
2022-08-19 17:22:05 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.2914
2022-08-19 17:22:39 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.3227
2022-08-19 17:23:14 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.4478
2022-08-19 17:23:47 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.4287
2022-08-19 17:24:22 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.4380
2022-08-19 17:24:56 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.3809
2022-08-19 17:25:31 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.2916
2022-08-19 17:26:05 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.4809
2022-08-19 17:26:39 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.6028
2022-08-19 17:27:13 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.1834
2022-08-19 17:27:48 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.1646
2022-08-19 17:28:22 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.6271
2022-08-19 17:28:57 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.3880
2022-08-19 17:29:31 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.6248
2022-08-19 17:30:05 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.2591
2022-08-19 17:30:39 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2889
2022-08-19 17:31:14 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.3340
2022-08-19 17:31:48 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.3239
2022-08-19 17:32:22 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.3306
2022-08-19 17:32:57 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.3303
2022-08-19 17:33:31 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.1778
2022-08-19 17:34:06 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.5656
2022-08-19 17:34:40 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.3834
2022-08-19 17:35:14 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.2794
2022-08-19 17:35:16 - train: epoch 007, train_loss: 2.4005
2022-08-19 17:36:32 - eval: epoch: 007, acc1: 51.444%, acc5: 76.986%, test_loss: 2.1041, per_image_load_time: 2.415ms, per_image_inference_time: 0.532ms
2022-08-19 17:36:32 - until epoch: 007, best_acc1: 51.444%
2022-08-19 17:36:32 - epoch 008 lr: 0.081870
2022-08-19 17:37:12 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.3579
2022-08-19 17:37:46 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.3860
2022-08-19 17:38:19 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.0696
2022-08-19 17:38:53 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.1554
2022-08-19 17:39:27 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.1607
2022-08-19 17:40:00 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.4187
2022-08-19 17:40:34 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.4296
2022-08-19 17:41:08 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.1959
2022-08-19 17:41:42 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.1600
2022-08-19 17:42:16 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.2324
2022-08-19 17:42:50 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.3135
2022-08-19 17:43:24 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.0775
2022-08-19 17:43:58 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.4839
2022-08-19 17:44:32 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.3172
2022-08-19 17:45:06 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.2691
2022-08-19 17:45:40 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.4366
2022-08-19 17:46:14 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.3781
2022-08-19 17:46:48 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.1857
2022-08-19 17:47:21 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.1443
2022-08-19 17:47:55 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.4485
2022-08-19 17:48:29 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.3495
2022-08-19 17:49:04 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.3700
2022-08-19 17:49:38 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.3698
2022-08-19 17:50:11 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.2802
2022-08-19 17:50:45 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.2200
2022-08-19 17:51:20 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.3310
2022-08-19 17:51:53 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.6524
2022-08-19 17:52:27 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.4170
2022-08-19 17:53:01 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.3012
2022-08-19 17:53:35 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.3534
2022-08-19 17:54:09 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.5212
2022-08-19 17:54:43 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.5596
2022-08-19 17:55:17 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.4025
2022-08-19 17:55:51 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.1653
2022-08-19 17:56:24 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.3725
2022-08-19 17:56:58 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.4299
2022-08-19 17:57:33 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.1265
2022-08-19 17:58:07 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.3297
2022-08-19 17:58:41 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.4848
2022-08-19 17:59:15 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.6645
2022-08-19 17:59:49 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.0498
2022-08-19 18:00:23 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.3151
2022-08-19 18:00:57 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.0354
2022-08-19 18:01:31 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.1483
2022-08-19 18:02:05 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.2356
2022-08-19 18:02:39 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.3910
2022-08-19 18:03:13 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2435
2022-08-19 18:03:46 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.3938
2022-08-19 18:04:20 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.2859
2022-08-19 18:04:54 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.2194
2022-08-19 18:04:55 - train: epoch 008, train_loss: 2.3211
2022-08-19 18:06:12 - eval: epoch: 008, acc1: 53.362%, acc5: 78.628%, test_loss: 2.0062, per_image_load_time: 2.421ms, per_image_inference_time: 0.540ms
2022-08-19 18:06:12 - until epoch: 008, best_acc1: 53.362%
2022-08-19 18:06:12 - epoch 009 lr: 0.076790
2022-08-19 18:06:52 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 1.9952
2022-08-19 18:07:26 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.2931
2022-08-19 18:08:00 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.0747
2022-08-19 18:08:35 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.6084
2022-08-19 18:09:09 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.3829
2022-08-19 18:09:43 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.1913
2022-08-19 18:10:18 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.1681
2022-08-19 18:10:52 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.1645
2022-08-19 18:11:26 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 1.9270
2022-08-19 18:12:00 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1334
2022-08-19 18:12:35 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.5513
2022-08-19 18:13:09 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.3225
2022-08-19 18:13:43 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.3554
2022-08-19 18:14:18 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 2.0857
2022-08-19 18:14:52 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.2078
2022-08-19 18:15:27 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2162
2022-08-19 18:16:01 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.1202
2022-08-19 18:16:36 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.2598
2022-08-19 18:17:10 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.0486
2022-08-19 18:17:44 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.0032
2022-08-19 18:18:19 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.2173
2022-08-19 18:18:54 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.3744
2022-08-19 18:19:28 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.3032
2022-08-19 18:20:02 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.2082
2022-08-19 18:20:37 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.0526
2022-08-19 18:21:11 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.3008
2022-08-19 18:21:45 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.1623
2022-08-19 18:22:20 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2500
2022-08-19 18:22:54 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.0604
2022-08-19 18:23:28 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.0816
2022-08-19 18:24:02 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.2474
2022-08-19 18:24:36 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.1128
2022-08-19 18:25:10 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.2847
2022-08-19 18:25:44 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.3251
2022-08-19 18:26:19 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.3032
2022-08-19 18:26:53 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.0890
2022-08-19 18:27:28 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.5199
2022-08-19 18:28:02 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.3512
2022-08-19 18:28:36 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.0678
2022-08-19 18:29:11 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.5384
2022-08-19 18:29:45 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.1877
2022-08-19 18:30:19 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 1.9909
2022-08-19 18:30:54 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.1052
2022-08-19 18:31:29 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.3477
2022-08-19 18:32:03 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.2070
2022-08-19 18:32:38 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.2657
2022-08-19 18:33:12 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.3812
2022-08-19 18:33:46 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.1924
2022-08-19 18:34:21 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.3123
2022-08-19 18:34:54 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.0471
2022-08-19 18:34:56 - train: epoch 009, train_loss: 2.2528
2022-08-19 18:36:12 - eval: epoch: 009, acc1: 52.882%, acc5: 78.236%, test_loss: 2.0252, per_image_load_time: 2.408ms, per_image_inference_time: 0.552ms
2022-08-19 18:36:12 - until epoch: 009, best_acc1: 53.362%
2022-08-19 18:36:12 - epoch 010 lr: 0.071288
2022-08-19 18:36:53 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.0622
2022-08-19 18:37:27 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.3166
2022-08-19 18:38:01 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.3359
2022-08-19 18:38:34 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.4206
2022-08-19 18:39:07 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.0406
2022-08-19 18:39:40 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.1965
2022-08-19 18:40:13 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.1844
2022-08-19 18:40:47 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.1136
2022-08-19 18:41:19 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.1331
2022-08-19 18:41:53 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.0661
2022-08-19 18:42:27 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.0766
2022-08-19 18:43:00 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.2571
2022-08-19 18:43:34 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.0461
2022-08-19 18:44:07 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.3208
2022-08-19 18:44:41 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.9414
2022-08-19 18:45:14 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.0350
2022-08-19 18:45:47 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.2269
2022-08-19 18:46:21 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 1.9683
2022-08-19 18:46:54 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.1298
2022-08-19 18:47:28 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.2217
2022-08-19 18:48:01 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 2.0623
2022-08-19 18:48:34 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.2730
2022-08-19 18:49:07 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.4413
2022-08-19 18:49:41 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.3984
2022-08-19 18:50:14 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.2050
2022-08-19 18:50:48 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.3468
2022-08-19 18:51:21 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.8626
2022-08-19 18:51:55 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.2527
2022-08-19 18:52:28 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.4205
2022-08-19 18:53:02 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1427
2022-08-19 18:53:35 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.3853
2022-08-19 18:54:09 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.1136
2022-08-19 18:54:42 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.2226
2022-08-19 18:55:16 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.2304
2022-08-19 18:55:49 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.5870
2022-08-19 18:56:22 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.2653
2022-08-19 18:56:55 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.1066
2022-08-19 18:57:29 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1933
2022-08-19 18:58:02 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 2.3365
2022-08-19 18:58:36 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.1999
2022-08-19 18:59:09 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 1.9719
2022-08-19 18:59:43 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.2433
2022-08-19 19:00:16 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.2189
2022-08-19 19:00:49 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 1.9399
2022-08-19 19:01:23 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.1408
2022-08-19 19:01:57 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.0884
2022-08-19 19:02:30 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.2231
2022-08-19 19:03:04 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.0776
2022-08-19 19:03:37 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.1192
2022-08-19 19:04:10 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 1.9918
2022-08-19 19:04:11 - train: epoch 010, train_loss: 2.1911
2022-08-19 19:05:26 - eval: epoch: 010, acc1: 54.920%, acc5: 79.636%, test_loss: 1.9239, per_image_load_time: 2.297ms, per_image_inference_time: 0.581ms
2022-08-19 19:05:26 - until epoch: 010, best_acc1: 54.920%
2022-08-19 19:05:26 - epoch 011 lr: 0.065450
2022-08-19 19:06:06 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.0615
2022-08-19 19:06:39 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.0609
2022-08-19 19:07:12 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.2087
2022-08-19 19:07:45 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.1789
2022-08-19 19:08:19 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.1016
2022-08-19 19:08:52 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.1973
2022-08-19 19:09:26 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.0766
2022-08-19 19:09:59 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.0621
2022-08-19 19:10:32 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.1625
2022-08-19 19:11:05 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 1.9284
2022-08-19 19:11:39 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2553
2022-08-19 19:12:12 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2717
2022-08-19 19:12:46 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.3478
2022-08-19 19:13:19 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.0732
2022-08-19 19:13:52 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.1178
2022-08-19 19:14:26 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.2028
2022-08-19 19:14:59 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.2479
2022-08-19 19:15:33 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.0502
2022-08-19 19:16:06 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.2201
2022-08-19 19:16:39 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.0611
2022-08-19 19:17:13 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.1589
2022-08-19 19:17:46 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 2.0302
2022-08-19 19:18:20 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.2070
2022-08-19 19:18:53 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 2.0692
2022-08-19 19:19:27 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.0422
2022-08-19 19:20:00 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.1241
2022-08-19 19:20:34 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.0136
2022-08-19 19:21:07 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.8546
2022-08-19 19:21:41 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.1988
2022-08-19 19:22:14 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.3376
2022-08-19 19:22:48 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.1415
2022-08-19 19:23:21 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.0498
2022-08-19 19:23:55 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.1737
2022-08-19 19:24:28 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.0786
2022-08-19 19:25:01 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.1656
2022-08-19 19:25:35 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.1267
2022-08-19 19:26:08 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.2100
2022-08-19 19:26:42 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 2.0764
2022-08-19 19:27:15 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.1050
2022-08-19 19:27:49 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 1.8239
2022-08-19 19:28:22 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 2.0071
2022-08-19 19:28:56 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 2.2496
2022-08-19 19:29:30 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.1468
2022-08-19 19:30:03 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.0438
2022-08-19 19:30:37 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.0951
2022-08-19 19:31:11 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 2.0885
2022-08-19 19:31:44 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.8706
2022-08-19 19:32:18 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.9148
2022-08-19 19:32:51 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.1617
2022-08-19 19:33:24 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 1.9553
2022-08-19 19:33:25 - train: epoch 011, train_loss: 2.1290
2022-08-19 19:34:40 - eval: epoch: 011, acc1: 56.134%, acc5: 80.614%, test_loss: 1.8665, per_image_load_time: 2.191ms, per_image_inference_time: 0.611ms
2022-08-19 19:34:40 - until epoch: 011, best_acc1: 56.134%
2022-08-19 19:34:40 - epoch 012 lr: 0.059368
2022-08-19 19:35:20 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 2.0976
2022-08-19 19:35:53 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.8304
2022-08-19 19:36:26 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 1.9468
2022-08-19 19:36:59 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.2592
2022-08-19 19:37:33 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.1097
2022-08-19 19:38:06 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.9321
2022-08-19 19:38:40 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.8347
2022-08-19 19:39:13 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.2147
2022-08-19 19:39:47 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.1619
2022-08-19 19:40:21 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 2.1823
2022-08-19 19:40:54 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.2964
2022-08-19 19:41:27 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.7746
2022-08-19 19:42:01 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 2.1273
2022-08-19 19:42:34 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.2942
2022-08-19 19:43:08 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 2.1454
2022-08-19 19:43:41 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 2.1765
2022-08-19 19:44:15 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 2.1350
2022-08-19 19:44:48 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.0317
2022-08-19 19:45:22 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1605
2022-08-19 19:45:55 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.3472
2022-08-19 19:46:29 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.1462
2022-08-19 19:47:02 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.1983
2022-08-19 19:47:37 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 1.8565
2022-08-19 19:48:11 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.0271
2022-08-19 19:48:45 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 2.0380
2022-08-19 19:49:18 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.9152
2022-08-19 19:49:52 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.0900
2022-08-19 19:50:26 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 2.0547
2022-08-19 19:51:00 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 1.9964
2022-08-19 19:51:34 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.8445
2022-08-19 19:52:08 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.0066
2022-08-19 19:52:41 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.9688
2022-08-19 19:53:15 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.1850
2022-08-19 19:53:49 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.0551
2022-08-19 19:54:23 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.0308
2022-08-19 19:54:57 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.9753
2022-08-19 19:55:31 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.0378
2022-08-19 19:56:05 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.1579
2022-08-19 19:56:39 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8249
2022-08-19 19:57:13 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.2585
2022-08-19 19:57:47 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.1626
2022-08-19 19:58:22 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.9559
2022-08-19 19:58:56 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.0687
2022-08-19 19:59:30 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 2.0892
2022-08-19 20:00:05 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 2.0538
2022-08-19 20:00:39 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.0905
2022-08-19 20:01:14 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.9739
2022-08-19 20:01:48 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.0476
2022-08-19 20:02:23 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.8050
2022-08-19 20:02:56 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.9467
2022-08-19 20:02:57 - train: epoch 012, train_loss: 2.0719
2022-08-19 20:04:13 - eval: epoch: 012, acc1: 57.122%, acc5: 81.456%, test_loss: 1.8150, per_image_load_time: 2.389ms, per_image_inference_time: 0.552ms
2022-08-19 20:04:14 - until epoch: 012, best_acc1: 57.122%
2022-08-19 20:04:14 - epoch 013 lr: 0.053138
2022-08-19 20:04:54 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.9028
2022-08-19 20:05:29 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.8746
2022-08-19 20:06:03 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.9307
2022-08-19 20:06:37 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.9051
2022-08-19 20:07:10 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 2.0793
2022-08-19 20:07:44 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 2.0642
2022-08-19 20:08:18 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 2.1298
2022-08-19 20:08:52 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.1221
2022-08-19 20:09:27 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.9463
2022-08-19 20:10:01 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.8318
2022-08-19 20:10:35 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.1039
2022-08-19 20:11:10 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.1617
2022-08-19 20:11:44 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.8261
2022-08-19 20:12:19 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 1.9710
2022-08-19 20:12:53 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.2309
2022-08-19 20:13:28 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.8475
2022-08-19 20:14:02 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.1801
2022-08-19 20:14:37 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9242
2022-08-19 20:15:11 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.3915
2022-08-19 20:15:46 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.1266
2022-08-19 20:16:20 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.3051
2022-08-19 20:16:55 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 2.1974
2022-08-19 20:17:30 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 1.9526
2022-08-19 20:18:04 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.0701
2022-08-19 20:18:39 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 2.0955
2022-08-19 20:19:13 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 2.0995
2022-08-19 20:19:48 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 2.0001
2022-08-19 20:20:22 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.2011
2022-08-19 20:20:57 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 1.9937
2022-08-19 20:21:31 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.8848
2022-08-19 20:22:06 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.8230
2022-08-19 20:22:40 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 1.8867
2022-08-19 20:23:15 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.9151
2022-08-19 20:23:50 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.9996
2022-08-19 20:24:24 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.8713
2022-08-19 20:24:59 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.1052
2022-08-19 20:25:33 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.7379
2022-08-19 20:26:08 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1275
2022-08-19 20:26:42 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.1729
2022-08-19 20:27:17 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.9370
2022-08-19 20:27:51 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 2.0179
2022-08-19 20:28:26 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 2.0624
2022-08-19 20:29:01 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.8672
2022-08-19 20:29:35 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.9248
2022-08-19 20:30:10 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 2.0147
2022-08-19 20:30:45 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 1.9564
2022-08-19 20:31:19 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.9120
2022-08-19 20:31:54 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.2529
2022-08-19 20:32:28 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.1947
2022-08-19 20:33:02 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.1009
2022-08-19 20:33:03 - train: epoch 013, train_loss: 2.0134
2022-08-19 20:34:19 - eval: epoch: 013, acc1: 59.644%, acc5: 83.090%, test_loss: 1.7035, per_image_load_time: 2.392ms, per_image_inference_time: 0.539ms
2022-08-19 20:34:19 - until epoch: 013, best_acc1: 59.644%
2022-08-19 20:34:19 - epoch 014 lr: 0.046859
2022-08-19 20:35:00 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.8088
2022-08-19 20:35:34 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 2.0466
2022-08-19 20:36:08 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.8139
2022-08-19 20:36:43 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 2.0954
2022-08-19 20:37:17 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.7729
2022-08-19 20:37:51 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 2.0187
2022-08-19 20:38:25 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.8415
2022-08-19 20:39:00 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.8622
2022-08-19 20:39:34 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.9854
2022-08-19 20:40:08 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 1.8633
2022-08-19 20:40:43 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.8411
2022-08-19 20:41:17 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 2.0045
2022-08-19 20:41:51 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.1548
2022-08-19 20:42:26 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.8800
2022-08-19 20:43:01 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.1646
2022-08-19 20:43:35 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.7789
2022-08-19 20:44:10 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 2.0570
2022-08-19 20:44:44 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.0318
2022-08-19 20:45:18 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.8017
2022-08-19 20:45:52 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.8425
2022-08-19 20:46:26 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.0136
2022-08-19 20:47:00 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.8476
2022-08-19 20:47:35 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.9603
2022-08-19 20:48:09 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 2.1247
2022-08-19 20:48:43 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.7603
2022-08-19 20:49:17 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8953
2022-08-19 20:49:51 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.9560
2022-08-19 20:50:26 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 1.9841
2022-08-19 20:51:00 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9727
2022-08-19 20:51:34 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.9472
2022-08-19 20:52:08 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.7987
2022-08-19 20:52:42 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 2.1350
2022-08-19 20:53:16 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.8639
2022-08-19 20:53:50 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.7760
2022-08-19 20:54:24 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.8277
2022-08-19 20:54:58 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.8386
2022-08-19 20:55:32 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.9879
2022-08-19 20:56:06 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 2.0280
2022-08-19 20:56:40 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.9187
2022-08-19 20:57:14 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 2.0823
2022-08-19 20:57:48 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.8360
2022-08-19 20:58:22 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 2.0716
2022-08-19 20:58:56 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.8393
2022-08-19 20:59:30 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.9899
2022-08-19 21:00:04 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.8906
2022-08-19 21:00:38 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 2.0637
2022-08-19 21:01:12 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.9285
2022-08-19 21:01:46 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.9706
2022-08-19 21:02:20 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8240
2022-08-19 21:02:53 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 2.4515
2022-08-19 21:02:55 - train: epoch 014, train_loss: 1.9515
2022-08-19 21:04:11 - eval: epoch: 014, acc1: 59.844%, acc5: 83.214%, test_loss: 1.7037, per_image_load_time: 2.344ms, per_image_inference_time: 0.547ms
2022-08-19 21:04:11 - until epoch: 014, best_acc1: 59.844%
2022-08-19 21:04:11 - epoch 015 lr: 0.040630
2022-08-19 21:04:52 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.6662
2022-08-19 21:05:26 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 2.0336
2022-08-19 21:06:00 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.0490
2022-08-19 21:06:34 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.9372
2022-08-19 21:07:08 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.7865
2022-08-19 21:07:42 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 2.0456
2022-08-19 21:08:16 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 2.1227
2022-08-19 21:08:50 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.8883
2022-08-19 21:09:24 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 2.0771
2022-08-19 21:09:58 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.9618
2022-08-19 21:10:33 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.6816
2022-08-19 21:11:07 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 2.0163
2022-08-19 21:11:41 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 2.1237
2022-08-19 21:12:15 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.7747
2022-08-19 21:12:49 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.9055
2022-08-19 21:13:24 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.8101
2022-08-19 21:13:58 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.7914
2022-08-19 21:14:32 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.7749
2022-08-19 21:15:07 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.8450
2022-08-19 21:15:41 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.8182
2022-08-19 21:16:16 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.8536
2022-08-19 21:16:50 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9184
2022-08-19 21:17:25 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.6803
2022-08-19 21:17:59 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.9592
2022-08-19 21:18:34 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.9497
2022-08-19 21:19:08 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.8030
2022-08-19 21:19:43 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 2.0639
2022-08-19 21:20:17 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.8933
2022-08-19 21:20:51 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.9031
2022-08-19 21:21:25 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.7635
2022-08-19 21:22:00 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.8914
2022-08-19 21:22:34 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 2.0703
2022-08-19 21:23:08 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.6602
2022-08-19 21:23:42 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.9766
2022-08-19 21:24:17 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 1.8364
2022-08-19 21:24:51 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.7057
2022-08-19 21:25:25 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6953
2022-08-19 21:26:00 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.8152
2022-08-19 21:26:34 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.9000
2022-08-19 21:27:08 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.8400
2022-08-19 21:27:43 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.9047
2022-08-19 21:28:17 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6634
2022-08-19 21:28:51 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.7009
2022-08-19 21:29:26 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.8132
2022-08-19 21:30:00 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 2.1026
2022-08-19 21:30:35 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 2.0485
2022-08-19 21:31:09 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.7512
2022-08-19 21:31:43 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.9100
2022-08-19 21:32:17 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.7193
2022-08-19 21:32:51 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.9168
2022-08-19 21:32:52 - train: epoch 015, train_loss: 1.8863
2022-08-19 21:34:09 - eval: epoch: 015, acc1: 61.522%, acc5: 84.600%, test_loss: 1.6085, per_image_load_time: 2.356ms, per_image_inference_time: 0.570ms
2022-08-19 21:34:09 - until epoch: 015, best_acc1: 61.522%
2022-08-19 21:34:09 - epoch 016 lr: 0.034548
2022-08-19 21:34:50 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.7933
2022-08-19 21:35:24 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.7771
2022-08-19 21:35:57 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.8181
2022-08-19 21:36:31 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.1146
2022-08-19 21:37:04 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.8185
2022-08-19 21:37:38 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.9364
2022-08-19 21:38:11 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.7032
2022-08-19 21:38:45 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.8162
2022-08-19 21:39:18 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.7915
2022-08-19 21:39:52 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.7693
2022-08-19 21:40:25 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.7513
2022-08-19 21:40:59 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.7912
2022-08-19 21:41:33 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 2.0669
2022-08-19 21:42:06 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.7006
2022-08-19 21:42:40 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.9557
2022-08-19 21:43:13 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.9041
2022-08-19 21:43:47 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.7270
2022-08-19 21:44:21 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.8910
2022-08-19 21:44:54 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.9687
2022-08-19 21:45:28 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.6962
2022-08-19 21:46:02 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8325
2022-08-19 21:46:36 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.7521
2022-08-19 21:47:09 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.8799
2022-08-19 21:47:43 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.8072
2022-08-19 21:48:17 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.7433
2022-08-19 21:48:50 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.8927
2022-08-19 21:49:24 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.7658
2022-08-19 21:49:57 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.6907
2022-08-19 21:50:31 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.8637
2022-08-19 21:51:04 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.8182
2022-08-19 21:51:38 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.8433
2022-08-19 21:52:12 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 2.0777
2022-08-19 21:52:46 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9209
2022-08-19 21:53:20 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.8122
2022-08-19 21:53:54 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.8582
2022-08-19 21:54:27 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.6616
2022-08-19 21:55:01 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.8388
2022-08-19 21:55:35 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.1084
2022-08-19 21:56:08 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.7996
2022-08-19 21:56:41 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.9081
2022-08-19 21:57:14 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.9055
2022-08-19 21:57:48 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.6877
2022-08-19 21:58:21 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.9090
2022-08-19 21:58:54 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.6034
2022-08-19 21:59:27 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.7379
2022-08-19 22:00:01 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.6531
2022-08-19 22:00:34 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.7685
2022-08-19 22:01:08 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.8310
2022-08-19 22:01:41 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 2.0276
2022-08-19 22:02:14 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.8799
2022-08-19 22:02:15 - train: epoch 016, train_loss: 1.8262
2022-08-19 22:03:31 - eval: epoch: 016, acc1: 63.004%, acc5: 85.498%, test_loss: 1.5283, per_image_load_time: 2.395ms, per_image_inference_time: 0.540ms
2022-08-19 22:03:31 - until epoch: 016, best_acc1: 63.004%
2022-08-19 22:03:31 - epoch 017 lr: 0.028710
2022-08-19 22:04:12 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.7733
2022-08-19 22:04:45 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.7214
2022-08-19 22:05:18 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 2.1401
2022-08-19 22:05:52 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.4857
2022-08-19 22:06:25 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.6712
2022-08-19 22:06:59 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 2.1068
2022-08-19 22:07:33 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.7435
2022-08-19 22:08:07 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.9228
2022-08-19 22:08:40 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.6680
2022-08-19 22:09:14 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.7055
2022-08-19 22:09:49 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.8854
2022-08-19 22:10:23 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.5845
2022-08-19 22:10:57 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.8147
2022-08-19 22:11:31 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.6958
2022-08-19 22:12:05 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.7781
2022-08-19 22:12:40 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.6757
2022-08-19 22:13:14 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.8166
2022-08-19 22:13:48 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.8667
2022-08-19 22:14:23 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.7021
2022-08-19 22:14:57 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.7318
2022-08-19 22:15:32 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.7370
2022-08-19 22:16:06 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.5874
2022-08-19 22:16:41 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.7937
2022-08-19 22:17:15 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.7350
2022-08-19 22:17:50 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.8807
2022-08-19 22:18:24 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.7722
2022-08-19 22:18:58 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.9351
2022-08-19 22:19:33 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.8078
2022-08-19 22:20:07 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 2.0066
2022-08-19 22:20:41 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.5560
2022-08-19 22:21:16 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8424
2022-08-19 22:21:50 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.7268
2022-08-19 22:22:25 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8853
2022-08-19 22:22:59 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.7805
2022-08-19 22:23:33 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.7504
2022-08-19 22:24:08 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.6416
2022-08-19 22:24:42 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.6185
2022-08-19 22:25:17 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.8084
2022-08-19 22:25:51 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.6203
2022-08-19 22:26:25 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.7249
2022-08-19 22:27:00 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.8236
2022-08-19 22:27:34 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.6366
2022-08-19 22:28:08 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.6758
2022-08-19 22:28:43 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8572
2022-08-19 22:29:17 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.9309
2022-08-19 22:29:52 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.8039
2022-08-19 22:30:26 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.9269
2022-08-19 22:31:00 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.8770
2022-08-19 22:31:35 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.5622
2022-08-19 22:32:08 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.5890
2022-08-19 22:32:10 - train: epoch 017, train_loss: 1.7558
2022-08-19 22:33:27 - eval: epoch: 017, acc1: 64.582%, acc5: 86.402%, test_loss: 1.4691, per_image_load_time: 2.314ms, per_image_inference_time: 0.590ms
2022-08-19 22:33:27 - until epoch: 017, best_acc1: 64.582%
2022-08-19 22:33:27 - epoch 018 lr: 0.023208
2022-08-19 22:34:09 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.7726
2022-08-19 22:34:43 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.7939
2022-08-19 22:35:17 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.9094
2022-08-19 22:35:52 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.7243
2022-08-19 22:36:26 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.7363
2022-08-19 22:37:00 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.6575
2022-08-19 22:37:34 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.6151
2022-08-19 22:38:08 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.6963
2022-08-19 22:38:42 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.7355
2022-08-19 22:39:17 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.5663
2022-08-19 22:39:51 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.6672
2022-08-19 22:40:25 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.5436
2022-08-19 22:41:00 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.8292
2022-08-19 22:41:34 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.8483
2022-08-19 22:42:08 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.8111
2022-08-19 22:42:43 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.5874
2022-08-19 22:43:17 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.8107
2022-08-19 22:43:51 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6547
2022-08-19 22:44:25 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.8260
2022-08-19 22:45:00 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.8751
2022-08-19 22:45:34 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.8919
2022-08-19 22:46:08 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.8661
2022-08-19 22:46:43 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6241
2022-08-19 22:47:17 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.6309
2022-08-19 22:47:51 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.4737
2022-08-19 22:48:26 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.6572
2022-08-19 22:49:00 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.9679
2022-08-19 22:49:34 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.6080
2022-08-19 22:50:09 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.5127
2022-08-19 22:50:43 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.4844
2022-08-19 22:51:18 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.9038
2022-08-19 22:51:52 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.6385
2022-08-19 22:52:26 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.6013
2022-08-19 22:53:00 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.6618
2022-08-19 22:53:35 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.5661
2022-08-19 22:54:09 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.6609
2022-08-19 22:54:43 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.7901
2022-08-19 22:55:18 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.8373
2022-08-19 22:55:52 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.8774
2022-08-19 22:56:26 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.7420
2022-08-19 22:57:01 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.9599
2022-08-19 22:57:35 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.6960
2022-08-19 22:58:10 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.4896
2022-08-19 22:58:44 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.6101
2022-08-19 22:59:19 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.6389
2022-08-19 22:59:54 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.7377
2022-08-19 23:00:28 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.8903
2022-08-19 23:01:03 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.7508
2022-08-19 23:01:38 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.6519
2022-08-19 23:02:12 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.7325
2022-08-19 23:02:13 - train: epoch 018, train_loss: 1.6853
2022-08-19 23:03:32 - eval: epoch: 018, acc1: 65.780%, acc5: 87.024%, test_loss: 1.4158, per_image_load_time: 2.478ms, per_image_inference_time: 0.553ms
2022-08-19 23:03:32 - until epoch: 018, best_acc1: 65.780%
2022-08-19 23:03:32 - epoch 019 lr: 0.018128
2022-08-19 23:04:14 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4861
2022-08-19 23:04:48 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.5745
2022-08-19 23:05:22 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.5640
2022-08-19 23:05:56 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.6056
2022-08-19 23:06:31 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.5783
2022-08-19 23:07:05 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.6976
2022-08-19 23:07:39 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.5663
2022-08-19 23:08:14 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.7327
2022-08-19 23:08:48 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.7364
2022-08-19 23:09:23 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.6011
2022-08-19 23:09:58 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.4796
2022-08-19 23:10:32 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.5539
2022-08-19 23:11:07 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.5990
2022-08-19 23:11:41 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.4329
2022-08-19 23:12:15 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.7024
2022-08-19 23:12:49 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.5821
2022-08-19 23:13:24 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.7454
2022-08-19 23:13:58 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.5291
2022-08-19 23:14:33 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.8538
2022-08-19 23:15:07 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.5639
2022-08-19 23:15:42 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.4680
2022-08-19 23:16:16 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.5811
2022-08-19 23:16:51 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.7543
2022-08-19 23:17:25 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.7353
2022-08-19 23:18:00 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.5815
2022-08-19 23:18:34 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.7205
2022-08-19 23:19:08 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.3836
2022-08-19 23:19:43 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.5646
2022-08-19 23:20:17 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.7526
2022-08-19 23:20:52 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.8690
2022-08-19 23:21:26 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.5383
2022-08-19 23:22:01 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.3114
2022-08-19 23:22:35 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.6335
2022-08-19 23:23:10 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.5787
2022-08-19 23:23:45 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.6466
2022-08-19 23:24:19 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.3304
2022-08-19 23:24:53 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.6641
2022-08-19 23:25:28 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.8539
2022-08-19 23:26:03 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.6202
2022-08-19 23:26:37 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.4581
2022-08-19 23:27:12 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.7539
2022-08-19 23:27:46 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.4466
2022-08-19 23:28:21 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.6686
2022-08-19 23:28:55 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.7326
2022-08-19 23:29:30 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.7695
2022-08-19 23:30:04 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.4159
2022-08-19 23:30:38 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.4758
2022-08-19 23:31:13 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.5408
2022-08-19 23:31:47 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.5102
2022-08-19 23:32:21 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.6460
2022-08-19 23:32:22 - train: epoch 019, train_loss: 1.6141
2022-08-19 23:33:40 - eval: epoch: 019, acc1: 67.002%, acc5: 87.822%, test_loss: 1.3516, per_image_load_time: 2.351ms, per_image_inference_time: 0.551ms
2022-08-19 23:33:40 - until epoch: 019, best_acc1: 67.002%
2022-08-19 23:33:40 - epoch 020 lr: 0.013551
2022-08-19 23:34:22 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.6567
2022-08-19 23:34:57 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.5676
2022-08-19 23:35:31 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.5780
2022-08-19 23:36:06 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.4634
2022-08-19 23:36:40 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.4391
2022-08-19 23:37:15 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.6804
2022-08-19 23:37:50 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.2391
2022-08-19 23:38:24 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.6189
2022-08-19 23:38:59 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.5996
2022-08-19 23:39:33 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.6486
2022-08-19 23:40:08 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.5878
2022-08-19 23:40:43 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.5022
2022-08-19 23:41:17 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.3969
2022-08-19 23:41:51 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.5794
2022-08-19 23:42:26 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.5753
2022-08-19 23:43:00 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.5426
2022-08-19 23:43:35 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.6488
2022-08-19 23:44:09 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.5913
2022-08-19 23:44:43 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.4919
2022-08-19 23:45:18 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.5622
2022-08-19 23:45:52 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.5940
2022-08-19 23:46:26 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.3540
2022-08-19 23:47:01 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.4546
2022-08-19 23:47:35 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.7439
2022-08-19 23:48:10 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.4873
2022-08-19 23:48:44 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.5458
2022-08-19 23:49:19 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.5839
2022-08-19 23:49:53 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.6511
2022-08-19 23:50:27 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.7094
2022-08-19 23:51:02 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.7103
2022-08-19 23:51:36 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.7347
2022-08-19 23:52:10 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.6223
2022-08-19 23:52:44 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3284
2022-08-19 23:53:19 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.5702
2022-08-19 23:53:53 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.4106
2022-08-19 23:54:28 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.5801
2022-08-19 23:55:02 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.4686
2022-08-19 23:55:36 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.6444
2022-08-19 23:56:11 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.7114
2022-08-19 23:56:45 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.4243
2022-08-19 23:57:19 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.5385
2022-08-19 23:57:54 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.4566
2022-08-19 23:58:28 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.5322
2022-08-19 23:59:02 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.4077
2022-08-19 23:59:37 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.6221
2022-08-20 00:00:12 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.5498
2022-08-20 00:00:46 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.6455
2022-08-20 00:01:21 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4621
2022-08-20 00:01:55 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.6110
2022-08-20 00:02:29 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.4885
2022-08-20 00:02:30 - train: epoch 020, train_loss: 1.5369
2022-08-20 00:03:49 - eval: epoch: 020, acc1: 68.576%, acc5: 88.780%, test_loss: 1.2900, per_image_load_time: 2.469ms, per_image_inference_time: 0.560ms
2022-08-20 00:03:49 - until epoch: 020, best_acc1: 68.576%
2022-08-20 00:03:49 - epoch 021 lr: 0.009548
2022-08-20 00:04:32 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.4305
2022-08-20 00:05:05 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.5872
2022-08-20 00:05:39 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.2722
2022-08-20 00:06:12 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4927
2022-08-20 00:06:46 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.4884
2022-08-20 00:07:20 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.4470
2022-08-20 00:07:54 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3745
2022-08-20 00:08:29 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.5573
2022-08-20 00:09:03 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.3916
2022-08-20 00:09:37 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.4629
2022-08-20 00:10:11 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.3532
2022-08-20 00:10:46 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.4255
2022-08-20 00:11:20 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.3177
2022-08-20 00:11:54 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.1643
2022-08-20 00:12:28 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.4363
2022-08-20 00:13:03 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3889
2022-08-20 00:13:37 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.3991
2022-08-20 00:14:11 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.4409
2022-08-20 00:14:46 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.5899
2022-08-20 00:15:20 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.5392
2022-08-20 00:15:54 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.2973
2022-08-20 00:16:29 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.5040
2022-08-20 00:17:03 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.4577
2022-08-20 00:17:37 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.5118
2022-08-20 00:18:11 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.5824
2022-08-20 00:18:46 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.4299
2022-08-20 00:19:20 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.4127
2022-08-20 00:19:54 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.4581
2022-08-20 00:20:28 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.4450
2022-08-20 00:21:03 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.5702
2022-08-20 00:21:37 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.4898
2022-08-20 00:22:12 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.1513
2022-08-20 00:22:46 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.8357
2022-08-20 00:23:20 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.7478
2022-08-20 00:23:54 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4992
2022-08-20 00:24:29 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.5246
2022-08-20 00:25:04 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.3868
2022-08-20 00:25:38 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.5306
2022-08-20 00:26:13 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.6264
2022-08-20 00:26:47 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.5734
2022-08-20 00:27:22 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.3348
2022-08-20 00:27:56 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.4685
2022-08-20 00:28:31 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.4242
2022-08-20 00:29:05 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.5569
2022-08-20 00:29:39 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.5179
2022-08-20 00:30:14 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.4492
2022-08-20 00:30:48 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.4658
2022-08-20 00:31:23 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.5879
2022-08-20 00:31:57 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.3531
2022-08-20 00:32:31 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.5859
2022-08-20 00:32:32 - train: epoch 021, train_loss: 1.4690
2022-08-20 00:33:50 - eval: epoch: 021, acc1: 69.404%, acc5: 89.210%, test_loss: 1.2453, per_image_load_time: 2.435ms, per_image_inference_time: 0.559ms
2022-08-20 00:33:50 - until epoch: 021, best_acc1: 69.404%
2022-08-20 00:33:50 - epoch 022 lr: 0.006184
2022-08-20 00:34:31 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.2190
2022-08-20 00:35:05 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.4867
2022-08-20 00:35:40 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.2559
2022-08-20 00:36:14 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.2517
2022-08-20 00:36:48 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.4588
2022-08-20 00:37:22 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.6075
2022-08-20 00:37:57 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4137
2022-08-20 00:38:31 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.5017
2022-08-20 00:39:05 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.4552
2022-08-20 00:39:40 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.5191
2022-08-20 00:40:14 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3539
2022-08-20 00:40:48 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.2271
2022-08-20 00:41:23 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.3433
2022-08-20 00:41:57 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.2332
2022-08-20 00:42:31 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.5618
2022-08-20 00:43:05 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.3618
2022-08-20 00:43:40 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.4502
2022-08-20 00:44:14 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.6539
2022-08-20 00:44:48 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.4051
2022-08-20 00:45:22 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4322
2022-08-20 00:45:56 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.5136
2022-08-20 00:46:30 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.3162
2022-08-20 00:47:05 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.6022
2022-08-20 00:47:39 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.4296
2022-08-20 00:48:14 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.4282
2022-08-20 00:48:48 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.2588
2022-08-20 00:49:23 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.1748
2022-08-20 00:49:57 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.4213
2022-08-20 00:50:31 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.3048
2022-08-20 00:51:05 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.3332
2022-08-20 00:51:40 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.6844
2022-08-20 00:52:14 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4940
2022-08-20 00:52:48 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.5741
2022-08-20 00:53:22 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.2485
2022-08-20 00:53:56 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.4881
2022-08-20 00:54:31 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.4845
2022-08-20 00:55:05 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.4660
2022-08-20 00:55:40 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.4571
2022-08-20 00:56:15 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.2735
2022-08-20 00:56:49 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.5007
2022-08-20 00:57:23 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.3423
2022-08-20 00:57:58 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.3932
2022-08-20 00:58:32 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4141
2022-08-20 00:59:06 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3096
2022-08-20 00:59:41 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.4142
2022-08-20 01:00:15 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.5864
2022-08-20 01:00:50 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.4290
2022-08-20 01:01:24 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.1882
2022-08-20 01:01:58 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.3668
2022-08-20 01:02:32 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.3240
2022-08-20 01:02:33 - train: epoch 022, train_loss: 1.4003
2022-08-20 01:03:50 - eval: epoch: 022, acc1: 70.612%, acc5: 89.792%, test_loss: 1.1933, per_image_load_time: 2.201ms, per_image_inference_time: 0.580ms
2022-08-20 01:03:50 - until epoch: 022, best_acc1: 70.612%
2022-08-20 01:03:50 - epoch 023 lr: 0.003511
2022-08-20 01:04:32 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.2281
2022-08-20 01:05:06 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.3075
2022-08-20 01:05:41 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.3376
2022-08-20 01:06:15 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.2842
2022-08-20 01:06:49 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.5431
2022-08-20 01:07:23 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.2288
2022-08-20 01:07:58 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.3082
2022-08-20 01:08:32 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.4192
2022-08-20 01:09:07 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4777
2022-08-20 01:09:42 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.2182
2022-08-20 01:10:16 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.4334
2022-08-20 01:10:51 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.2206
2022-08-20 01:11:26 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.3578
2022-08-20 01:12:00 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.4863
2022-08-20 01:12:34 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.1884
2022-08-20 01:13:09 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.4491
2022-08-20 01:13:43 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.4060
2022-08-20 01:14:17 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.2272
2022-08-20 01:14:51 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.2991
2022-08-20 01:15:26 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.3494
2022-08-20 01:16:00 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.5043
2022-08-20 01:16:34 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.1911
2022-08-20 01:17:09 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.3004
2022-08-20 01:17:43 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.4173
2022-08-20 01:18:17 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.3866
2022-08-20 01:18:52 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.4664
2022-08-20 01:19:26 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.5389
2022-08-20 01:20:00 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.2735
2022-08-20 01:20:35 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.4347
2022-08-20 01:21:09 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.2053
2022-08-20 01:21:43 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.5096
2022-08-20 01:22:18 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.5104
2022-08-20 01:22:52 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.4504
2022-08-20 01:23:26 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.3862
2022-08-20 01:24:00 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.3375
2022-08-20 01:24:34 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.4060
2022-08-20 01:25:08 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.2100
2022-08-20 01:25:43 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.3535
2022-08-20 01:26:17 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3055
2022-08-20 01:26:51 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.3603
2022-08-20 01:27:26 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.3229
2022-08-20 01:28:00 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.2598
2022-08-20 01:28:34 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.3757
2022-08-20 01:29:09 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.3745
2022-08-20 01:29:43 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.2939
2022-08-20 01:30:17 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.3843
2022-08-20 01:30:52 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.1716
2022-08-20 01:31:26 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.2553
2022-08-20 01:32:00 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.0966
2022-08-20 01:32:33 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.3530
2022-08-20 01:32:35 - train: epoch 023, train_loss: 1.3465
2022-08-20 01:33:52 - eval: epoch: 023, acc1: 71.342%, acc5: 90.170%, test_loss: 1.1630, per_image_load_time: 1.768ms, per_image_inference_time: 0.578ms
2022-08-20 01:33:52 - until epoch: 023, best_acc1: 71.342%
2022-08-20 01:33:52 - epoch 024 lr: 0.001571
2022-08-20 01:34:34 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.5573
2022-08-20 01:35:08 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.3142
2022-08-20 01:35:42 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.3623
2022-08-20 01:36:16 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.2983
2022-08-20 01:36:49 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.3974
2022-08-20 01:37:24 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.2370
2022-08-20 01:37:58 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.4277
2022-08-20 01:38:32 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.1758
2022-08-20 01:39:07 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.2074
2022-08-20 01:39:42 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.3245
2022-08-20 01:40:17 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.1304
2022-08-20 01:40:52 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.1745
2022-08-20 01:41:27 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4552
2022-08-20 01:42:02 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.4551
2022-08-20 01:42:37 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.2884
2022-08-20 01:43:12 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.2314
2022-08-20 01:43:48 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.1651
2022-08-20 01:44:22 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.4025
2022-08-20 01:44:58 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.1147
2022-08-20 01:45:33 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.4234
2022-08-20 01:46:08 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.3778
2022-08-20 01:46:43 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.1595
2022-08-20 01:47:18 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.2689
2022-08-20 01:47:54 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.3576
2022-08-20 01:48:29 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.2455
2022-08-20 01:49:04 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.5673
2022-08-20 01:49:39 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.4594
2022-08-20 01:50:14 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.4313
2022-08-20 01:50:49 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.3563
2022-08-20 01:51:24 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.1999
2022-08-20 01:52:00 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.2562
2022-08-20 01:52:35 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.3669
2022-08-20 01:53:10 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.1737
2022-08-20 01:53:45 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.2163
2022-08-20 01:54:20 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.1748
2022-08-20 01:54:55 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.3195
2022-08-20 01:55:31 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.2752
2022-08-20 01:56:06 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.5753
2022-08-20 01:56:41 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.3724
2022-08-20 01:57:16 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.1553
2022-08-20 01:57:52 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.1737
2022-08-20 01:58:27 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.1434
2022-08-20 01:59:03 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.5612
2022-08-20 01:59:38 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.2794
2022-08-20 02:00:14 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.2786
2022-08-20 02:00:49 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.3802
2022-08-20 02:01:25 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.5323
2022-08-20 02:02:00 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.0157
2022-08-20 02:02:35 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.4292
2022-08-20 02:03:09 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.2311
2022-08-20 02:03:11 - train: epoch 024, train_loss: 1.3087
2022-08-20 02:04:29 - eval: epoch: 024, acc1: 71.666%, acc5: 90.356%, test_loss: 1.1481, per_image_load_time: 0.650ms, per_image_inference_time: 0.600ms
2022-08-20 02:04:29 - until epoch: 024, best_acc1: 71.666%
2022-08-20 02:04:29 - epoch 025 lr: 0.000394
2022-08-20 02:05:12 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.1993
2022-08-20 02:05:48 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.2547
2022-08-20 02:06:23 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.2654
2022-08-20 02:06:58 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.3893
2022-08-20 02:07:33 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.1080
2022-08-20 02:08:08 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.2406
2022-08-20 02:08:42 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.1824
2022-08-20 02:09:17 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.1991
2022-08-20 02:09:52 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.1750
2022-08-20 02:10:27 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3367
2022-08-20 02:11:02 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.3101
2022-08-20 02:11:37 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.3669
2022-08-20 02:12:12 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3225
2022-08-20 02:12:47 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.2939
2022-08-20 02:13:22 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.2168
2022-08-20 02:13:57 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1794
2022-08-20 02:14:32 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.2866
2022-08-20 02:15:07 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.0633
2022-08-20 02:15:42 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.3141
2022-08-20 02:16:18 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.1579
2022-08-20 02:16:53 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.1977
2022-08-20 02:17:28 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.1967
2022-08-20 02:18:03 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.3880
2022-08-20 02:18:38 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.0736
2022-08-20 02:19:12 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.2605
2022-08-20 02:19:47 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.3364
2022-08-20 02:20:22 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.3248
2022-08-20 02:20:56 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.0898
2022-08-20 02:21:31 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.4399
2022-08-20 02:22:05 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.2102
2022-08-20 02:22:39 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.4150
2022-08-20 02:23:14 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.2907
2022-08-20 02:23:48 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.2792
2022-08-20 02:24:23 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.3827
2022-08-20 02:24:57 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.2202
2022-08-20 02:25:32 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.2523
2022-08-20 02:26:07 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.1377
2022-08-20 02:26:41 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.4593
2022-08-20 02:27:16 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.4196
2022-08-20 02:27:50 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.2473
2022-08-20 02:28:25 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.3335
2022-08-20 02:29:00 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.2530
2022-08-20 02:29:35 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.1547
2022-08-20 02:30:09 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.1887
2022-08-20 02:30:44 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.3343
2022-08-20 02:31:19 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.0931
2022-08-20 02:31:53 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.2763
2022-08-20 02:32:28 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.1856
2022-08-20 02:33:02 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.3652
2022-08-20 02:33:36 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.3328
2022-08-20 02:33:37 - train: epoch 025, train_loss: 1.2906
2022-08-20 02:34:55 - eval: epoch: 025, acc1: 71.746%, acc5: 90.402%, test_loss: 1.1448, per_image_load_time: 2.368ms, per_image_inference_time: 0.613ms
2022-08-20 02:34:55 - until epoch: 025, best_acc1: 71.746%
2022-08-20 02:34:55 - train done. train time: 12.508 hours, best_acc1: 71.746%
