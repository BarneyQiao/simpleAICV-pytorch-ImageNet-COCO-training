2022-08-18 00:33:07 - net_idx: 6
2022-08-18 00:33:07 - net_config: {'stem_width': 64, 'depth': 14, 'w_0': 40, 'w_a': 18.883277747086293, 'w_m': 1.8285316801722313}
2022-08-18 00:33:07 - num_classes: 1000
2022-08-18 00:33:07 - input_image_size: 224
2022-08-18 00:33:07 - scale: 1.1428571428571428
2022-08-18 00:33:07 - seed: 0
2022-08-18 00:33:07 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-18 00:33:07 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-18 00:33:07 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c2100>
2022-08-18 00:33:07 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c23d0>
2022-08-18 00:33:07 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2400>
2022-08-18 00:33:07 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2460>
2022-08-18 00:33:07 - batch_size: 256
2022-08-18 00:33:07 - num_workers: 16
2022-08-18 00:33:07 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-18 00:33:07 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-18 00:33:07 - epochs: 25
2022-08-18 00:33:07 - print_interval: 100
2022-08-18 00:33:07 - accumulation_steps: 1
2022-08-18 00:33:07 - sync_bn: False
2022-08-18 00:33:07 - apex: True
2022-08-18 00:33:07 - use_ema_model: False
2022-08-18 00:33:07 - ema_model_decay: 0.9999
2022-08-18 00:33:07 - log_dir: ./log
2022-08-18 00:33:07 - checkpoint_dir: ./checkpoints
2022-08-18 00:33:07 - gpus_type: NVIDIA RTX A5000
2022-08-18 00:33:07 - gpus_num: 2
2022-08-18 00:33:07 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f4efca864b0>
2022-08-18 00:33:07 - ema_model: None
2022-08-18 00:33:08 - --------------------parameters--------------------
2022-08-18 00:33:08 - name: conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-18 00:33:08 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-18 00:33:08 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-18 00:33:08 - name: fc.weight, grad: True
2022-08-18 00:33:08 - name: fc.bias, grad: True
2022-08-18 00:33:08 - --------------------buffers--------------------
2022-08-18 00:33:08 - name: conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-18 00:33:08 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-18 00:33:08 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 00:33:08 - -----------no weight decay layers--------------
2022-08-18 00:33:08 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 00:33:08 - -------------weight decay layers---------------
2022-08-18 00:33:08 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 00:33:08 - epoch 001 lr: 0.100000
2022-08-18 00:33:48 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9177
2022-08-18 00:34:22 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9043
2022-08-18 00:34:56 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8697
2022-08-18 00:35:30 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.7787
2022-08-18 00:36:04 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.6250
2022-08-18 00:36:39 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.4295
2022-08-18 00:37:13 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.5126
2022-08-18 00:37:47 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.3230
2022-08-18 00:38:21 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.2885
2022-08-18 00:38:55 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.1805
2022-08-18 00:39:29 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 5.9043
2022-08-18 00:40:03 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 5.9211
2022-08-18 00:40:37 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 5.8071
2022-08-18 00:41:11 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 5.7761
2022-08-18 00:41:45 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 5.6349
2022-08-18 00:42:19 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 5.6672
2022-08-18 00:42:54 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.4879
2022-08-18 00:43:28 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.6322
2022-08-18 00:44:03 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.5324
2022-08-18 00:44:37 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.2330
2022-08-18 00:45:11 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.2138
2022-08-18 00:45:45 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.0840
2022-08-18 00:46:19 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 4.9574
2022-08-18 00:46:53 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.0781
2022-08-18 00:47:28 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.0613
2022-08-18 00:48:02 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.2221
2022-08-18 00:48:37 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 4.9298
2022-08-18 00:49:11 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 4.8857
2022-08-18 00:49:46 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 4.7544
2022-08-18 00:50:20 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 4.9996
2022-08-18 00:50:54 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 4.9393
2022-08-18 00:51:29 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 4.8508
2022-08-18 00:52:03 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.7177
2022-08-18 00:52:37 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.5459
2022-08-18 00:53:12 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.6140
2022-08-18 00:53:46 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.5529
2022-08-18 00:54:20 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.7077
2022-08-18 00:54:54 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.4593
2022-08-18 00:55:29 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.5147
2022-08-18 00:56:03 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.5141
2022-08-18 00:56:38 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.5303
2022-08-18 00:57:12 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.3471
2022-08-18 00:57:46 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.3110
2022-08-18 00:58:21 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.1095
2022-08-18 00:58:55 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.2240
2022-08-18 00:59:30 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.4390
2022-08-18 01:00:04 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.0649
2022-08-18 01:00:38 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.3420
2022-08-18 01:01:13 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.3790
2022-08-18 01:01:46 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.1046
2022-08-18 01:01:48 - train: epoch 001, train_loss: 5.2224
2022-08-18 01:03:06 - eval: epoch: 001, acc1: 19.330%, acc5: 41.466%, test_loss: 4.0586, per_image_load_time: 1.351ms, per_image_inference_time: 0.587ms
2022-08-18 01:03:06 - until epoch: 001, best_acc1: 19.330%
2022-08-18 01:03:06 - epoch 002 lr: 0.099606
2022-08-18 01:03:47 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 3.8680
2022-08-18 01:04:21 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 3.9326
2022-08-18 01:04:54 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.1142
2022-08-18 01:05:28 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.0885
2022-08-18 01:06:02 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 3.8910
2022-08-18 01:06:36 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 3.7057
2022-08-18 01:07:09 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.2526
2022-08-18 01:07:43 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 3.6358
2022-08-18 01:08:18 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.7774
2022-08-18 01:08:51 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.0046
2022-08-18 01:09:25 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 3.8898
2022-08-18 01:10:00 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.7742
2022-08-18 01:10:34 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.7774
2022-08-18 01:11:08 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 3.9451
2022-08-18 01:11:42 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 3.7919
2022-08-18 01:12:16 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.6660
2022-08-18 01:12:51 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.7838
2022-08-18 01:13:25 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.6755
2022-08-18 01:13:58 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.5107
2022-08-18 01:14:32 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.4860
2022-08-18 01:15:07 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.7544
2022-08-18 01:15:41 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.4122
2022-08-18 01:16:16 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.7494
2022-08-18 01:16:50 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.5006
2022-08-18 01:17:24 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.4403
2022-08-18 01:17:59 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.5581
2022-08-18 01:18:33 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.7875
2022-08-18 01:19:07 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.6737
2022-08-18 01:19:42 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.5087
2022-08-18 01:20:16 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.3009
2022-08-18 01:20:51 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.5384
2022-08-18 01:21:25 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.6758
2022-08-18 01:21:59 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.4434
2022-08-18 01:22:33 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.5063
2022-08-18 01:23:08 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.3037
2022-08-18 01:23:42 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.5277
2022-08-18 01:24:16 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.5131
2022-08-18 01:24:50 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.0230
2022-08-18 01:25:25 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.4534
2022-08-18 01:26:00 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.2239
2022-08-18 01:26:34 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.5214
2022-08-18 01:27:09 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.3202
2022-08-18 01:27:43 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.3489
2022-08-18 01:28:17 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.1467
2022-08-18 01:28:52 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.1805
2022-08-18 01:29:26 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.1934
2022-08-18 01:30:00 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.4405
2022-08-18 01:30:35 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.1838
2022-08-18 01:31:09 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.2420
2022-08-18 01:31:42 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.4076
2022-08-18 01:31:44 - train: epoch 002, train_loss: 3.5846
2022-08-18 01:33:01 - eval: epoch: 002, acc1: 32.192%, acc5: 58.290%, test_loss: 3.1685, per_image_load_time: 2.096ms, per_image_inference_time: 0.594ms
2022-08-18 01:33:02 - until epoch: 002, best_acc1: 32.192%
2022-08-18 01:33:02 - epoch 003 lr: 0.098429
2022-08-18 01:33:43 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.2792
2022-08-18 01:34:17 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.2244
2022-08-18 01:34:50 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.1211
2022-08-18 01:35:24 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.0738
2022-08-18 01:35:58 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.4717
2022-08-18 01:36:32 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 2.9712
2022-08-18 01:37:06 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.3314
2022-08-18 01:37:40 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.4261
2022-08-18 01:38:14 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.0189
2022-08-18 01:38:48 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.3737
2022-08-18 01:39:22 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.0689
2022-08-18 01:39:57 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.0361
2022-08-18 01:40:31 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 2.9593
2022-08-18 01:41:05 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 2.9822
2022-08-18 01:41:39 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.0368
2022-08-18 01:42:13 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.1158
2022-08-18 01:42:48 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.1518
2022-08-18 01:43:22 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 2.9350
2022-08-18 01:43:55 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 2.9929
2022-08-18 01:44:30 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 2.9445
2022-08-18 01:45:04 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.1267
2022-08-18 01:45:38 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.3357
2022-08-18 01:46:11 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.8321
2022-08-18 01:46:45 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 2.9977
2022-08-18 01:47:20 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.0467
2022-08-18 01:47:54 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.0973
2022-08-18 01:48:28 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.3665
2022-08-18 01:49:02 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 2.8961
2022-08-18 01:49:36 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.7929
2022-08-18 01:50:10 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.0869
2022-08-18 01:50:45 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.2254
2022-08-18 01:51:19 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 2.9737
2022-08-18 01:51:53 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 2.9160
2022-08-18 01:52:27 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.0892
2022-08-18 01:53:02 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.7813
2022-08-18 01:53:36 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.8126
2022-08-18 01:54:10 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 2.8874
2022-08-18 01:54:45 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.0815
2022-08-18 01:55:19 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.0979
2022-08-18 01:55:53 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.6044
2022-08-18 01:56:28 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.9007
2022-08-18 01:57:02 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 2.9518
2022-08-18 01:57:36 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.6965
2022-08-18 01:58:11 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.8207
2022-08-18 01:58:45 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 2.8854
2022-08-18 01:59:20 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.9066
2022-08-18 01:59:54 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 2.8061
2022-08-18 02:00:28 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 2.9784
2022-08-18 02:01:03 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.1756
2022-08-18 02:01:36 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.7186
2022-08-18 02:01:38 - train: epoch 003, train_loss: 3.0123
2022-08-18 02:02:56 - eval: epoch: 003, acc1: 38.894%, acc5: 65.464%, test_loss: 2.8127, per_image_load_time: 1.263ms, per_image_inference_time: 0.584ms
2022-08-18 02:02:57 - until epoch: 003, best_acc1: 38.894%
2022-08-18 02:02:57 - epoch 004 lr: 0.096488
2022-08-18 02:03:37 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.8359
2022-08-18 02:04:11 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.6163
2022-08-18 02:04:45 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.6140
2022-08-18 02:05:19 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.8557
2022-08-18 02:05:53 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.6403
2022-08-18 02:06:28 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 2.9889
2022-08-18 02:07:02 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.7583
2022-08-18 02:07:36 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.5188
2022-08-18 02:08:11 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.6444
2022-08-18 02:08:45 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.9127
2022-08-18 02:09:20 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 2.8922
2022-08-18 02:09:54 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.6148
2022-08-18 02:10:29 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.5641
2022-08-18 02:11:03 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.8236
2022-08-18 02:11:38 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.7562
2022-08-18 02:12:12 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.9081
2022-08-18 02:12:46 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.7722
2022-08-18 02:13:21 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.9707
2022-08-18 02:13:56 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.8912
2022-08-18 02:14:30 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.7038
2022-08-18 02:15:04 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.7718
2022-08-18 02:15:39 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.6813
2022-08-18 02:16:13 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.3759
2022-08-18 02:16:48 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.4827
2022-08-18 02:17:22 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.6438
2022-08-18 02:17:57 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.6076
2022-08-18 02:18:31 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.5121
2022-08-18 02:19:06 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.7073
2022-08-18 02:19:40 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.7534
2022-08-18 02:20:15 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.7515
2022-08-18 02:20:49 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.6521
2022-08-18 02:21:23 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8125
2022-08-18 02:21:58 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.7745
2022-08-18 02:22:32 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.6365
2022-08-18 02:23:06 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.8127
2022-08-18 02:23:40 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.5685
2022-08-18 02:24:15 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.8116
2022-08-18 02:24:49 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.5287
2022-08-18 02:25:24 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.6478
2022-08-18 02:25:58 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.6193
2022-08-18 02:26:32 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.4969
2022-08-18 02:27:07 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.4678
2022-08-18 02:27:41 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.3708
2022-08-18 02:28:15 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.5823
2022-08-18 02:28:50 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.1924
2022-08-18 02:29:24 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.5337
2022-08-18 02:29:58 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.7214
2022-08-18 02:30:32 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.3877
2022-08-18 02:31:06 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.7259
2022-08-18 02:31:40 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.8150
2022-08-18 02:31:42 - train: epoch 004, train_loss: 2.7249
2022-08-18 02:32:59 - eval: epoch: 004, acc1: 43.216%, acc5: 69.702%, test_loss: 2.5575, per_image_load_time: 2.383ms, per_image_inference_time: 0.579ms
2022-08-18 02:32:59 - until epoch: 004, best_acc1: 43.216%
2022-08-18 02:32:59 - epoch 005 lr: 0.093815
2022-08-18 02:33:40 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.6419
2022-08-18 02:34:14 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.5446
2022-08-18 02:34:48 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.7285
2022-08-18 02:35:22 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.6320
2022-08-18 02:35:56 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.5139
2022-08-18 02:36:30 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.8080
2022-08-18 02:37:04 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.5917
2022-08-18 02:37:38 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.6224
2022-08-18 02:38:12 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.4954
2022-08-18 02:38:46 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.4587
2022-08-18 02:39:21 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.5490
2022-08-18 02:39:55 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.5553
2022-08-18 02:40:29 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.5223
2022-08-18 02:41:03 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.6525
2022-08-18 02:41:37 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.4017
2022-08-18 02:42:11 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.3489
2022-08-18 02:42:45 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.4791
2022-08-18 02:43:19 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.4918
2022-08-18 02:43:53 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.6351
2022-08-18 02:44:28 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.6866
2022-08-18 02:45:02 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.3428
2022-08-18 02:45:36 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.4882
2022-08-18 02:46:10 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.4271
2022-08-18 02:46:44 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5950
2022-08-18 02:47:18 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.6279
2022-08-18 02:47:52 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.6264
2022-08-18 02:48:27 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.5568
2022-08-18 02:49:01 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.4255
2022-08-18 02:49:35 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.4973
2022-08-18 02:50:09 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.4213
2022-08-18 02:50:43 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.4824
2022-08-18 02:51:17 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.5592
2022-08-18 02:51:52 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.3652
2022-08-18 02:52:26 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.5274
2022-08-18 02:53:00 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.6614
2022-08-18 02:53:34 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.4602
2022-08-18 02:54:08 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.5932
2022-08-18 02:54:43 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5155
2022-08-18 02:55:17 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.7550
2022-08-18 02:55:51 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.7486
2022-08-18 02:56:25 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.4949
2022-08-18 02:57:00 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.5237
2022-08-18 02:57:34 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.5436
2022-08-18 02:58:08 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.5940
2022-08-18 02:58:42 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.6034
2022-08-18 02:59:16 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.5868
2022-08-18 02:59:50 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.3248
2022-08-18 03:00:25 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.4495
2022-08-18 03:00:59 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.8066
2022-08-18 03:01:33 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.4787
2022-08-18 03:01:34 - train: epoch 005, train_loss: 2.5544
2022-08-18 03:02:51 - eval: epoch: 005, acc1: 48.398%, acc5: 74.818%, test_loss: 2.2332, per_image_load_time: 2.384ms, per_image_inference_time: 0.599ms
2022-08-18 03:02:52 - until epoch: 005, best_acc1: 48.398%
2022-08-18 03:02:52 - epoch 006 lr: 0.090450
2022-08-18 03:03:33 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.4444
2022-08-18 03:04:06 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.6346
2022-08-18 03:04:40 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.4511
2022-08-18 03:05:14 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.5142
2022-08-18 03:05:48 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.3344
2022-08-18 03:06:22 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.5014
2022-08-18 03:06:56 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.7560
2022-08-18 03:07:29 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.4822
2022-08-18 03:08:03 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3317
2022-08-18 03:08:37 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.2654
2022-08-18 03:09:11 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.4750
2022-08-18 03:09:45 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.3366
2022-08-18 03:10:19 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5791
2022-08-18 03:10:53 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.3215
2022-08-18 03:11:28 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.7639
2022-08-18 03:12:02 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3497
2022-08-18 03:12:36 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.5149
2022-08-18 03:13:10 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.2483
2022-08-18 03:13:45 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.3594
2022-08-18 03:14:19 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.5009
2022-08-18 03:14:53 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.4253
2022-08-18 03:15:27 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.1919
2022-08-18 03:16:01 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.4560
2022-08-18 03:16:35 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.3760
2022-08-18 03:17:10 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.3071
2022-08-18 03:17:44 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.3995
2022-08-18 03:18:18 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.6181
2022-08-18 03:18:52 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.0223
2022-08-18 03:19:26 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.4788
2022-08-18 03:20:00 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.2486
2022-08-18 03:20:34 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.3095
2022-08-18 03:21:08 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.5374
2022-08-18 03:21:43 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.3617
2022-08-18 03:22:17 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.6824
2022-08-18 03:22:51 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.4500
2022-08-18 03:23:24 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.3816
2022-08-18 03:23:58 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.4511
2022-08-18 03:24:32 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.2002
2022-08-18 03:25:06 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.5007
2022-08-18 03:25:40 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.4846
2022-08-18 03:26:14 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.4521
2022-08-18 03:26:49 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.2565
2022-08-18 03:27:22 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.4003
2022-08-18 03:27:57 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.3021
2022-08-18 03:28:31 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.4166
2022-08-18 03:29:05 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.4025
2022-08-18 03:29:39 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.3364
2022-08-18 03:30:14 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.4553
2022-08-18 03:30:47 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.4140
2022-08-18 03:31:21 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.3219
2022-08-18 03:31:22 - train: epoch 006, train_loss: 2.4326
2022-08-18 03:32:39 - eval: epoch: 006, acc1: 48.398%, acc5: 74.488%, test_loss: 2.2678, per_image_load_time: 2.361ms, per_image_inference_time: 0.571ms
2022-08-18 03:32:39 - until epoch: 006, best_acc1: 48.398%
2022-08-18 03:32:39 - epoch 007 lr: 0.086448
2022-08-18 03:33:20 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.2895
2022-08-18 03:33:54 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.5509
2022-08-18 03:34:28 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.4444
2022-08-18 03:35:02 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.3612
2022-08-18 03:35:36 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.2248
2022-08-18 03:36:10 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.3245
2022-08-18 03:36:44 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.3052
2022-08-18 03:37:18 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.4596
2022-08-18 03:37:52 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.1443
2022-08-18 03:38:26 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.4052
2022-08-18 03:39:00 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.2768
2022-08-18 03:39:34 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.3072
2022-08-18 03:40:08 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.0212
2022-08-18 03:40:42 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.5271
2022-08-18 03:41:16 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.4245
2022-08-18 03:41:51 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.3955
2022-08-18 03:42:25 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.1405
2022-08-18 03:42:59 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.2086
2022-08-18 03:43:33 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.5463
2022-08-18 03:44:08 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.1039
2022-08-18 03:44:42 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.4737
2022-08-18 03:45:16 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.3893
2022-08-18 03:45:50 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.4327
2022-08-18 03:46:24 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.4971
2022-08-18 03:46:58 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.3412
2022-08-18 03:47:32 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.2291
2022-08-18 03:48:06 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.1644
2022-08-18 03:48:41 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.2701
2022-08-18 03:49:15 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.2747
2022-08-18 03:49:49 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.4433
2022-08-18 03:50:24 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.3881
2022-08-18 03:50:58 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.0308
2022-08-18 03:51:32 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.4300
2022-08-18 03:52:07 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.3394
2022-08-18 03:52:41 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.3805
2022-08-18 03:53:15 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.1504
2022-08-18 03:53:49 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.4294
2022-08-18 03:54:23 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.3770
2022-08-18 03:54:58 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.2346
2022-08-18 03:55:32 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.5319
2022-08-18 03:56:06 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.1275
2022-08-18 03:56:41 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.3418
2022-08-18 03:57:15 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.3437
2022-08-18 03:57:49 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.3129
2022-08-18 03:58:23 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.3950
2022-08-18 03:58:58 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.5704
2022-08-18 03:59:32 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.2478
2022-08-18 04:00:06 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.4601
2022-08-18 04:00:41 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.1855
2022-08-18 04:01:14 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.2816
2022-08-18 04:01:16 - train: epoch 007, train_loss: 2.3409
2022-08-18 04:02:33 - eval: epoch: 007, acc1: 53.430%, acc5: 78.822%, test_loss: 1.9960, per_image_load_time: 2.354ms, per_image_inference_time: 0.587ms
2022-08-18 04:02:33 - until epoch: 007, best_acc1: 53.430%
2022-08-18 04:02:33 - epoch 008 lr: 0.081870
2022-08-18 04:03:13 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.3187
2022-08-18 04:03:47 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.2688
2022-08-18 04:04:20 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.3994
2022-08-18 04:04:54 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.2224
2022-08-18 04:05:28 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.2952
2022-08-18 04:06:02 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.2145
2022-08-18 04:06:35 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.2020
2022-08-18 04:07:10 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.2447
2022-08-18 04:07:43 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.2572
2022-08-18 04:08:17 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.3212
2022-08-18 04:08:51 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.0560
2022-08-18 04:09:25 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.2192
2022-08-18 04:09:59 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.3438
2022-08-18 04:10:33 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.3215
2022-08-18 04:11:06 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.3394
2022-08-18 04:11:40 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.4140
2022-08-18 04:12:15 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.2006
2022-08-18 04:12:48 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.2574
2022-08-18 04:13:22 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.0978
2022-08-18 04:13:56 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.1831
2022-08-18 04:14:30 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.2687
2022-08-18 04:15:04 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.1548
2022-08-18 04:15:38 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.0866
2022-08-18 04:16:12 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.1554
2022-08-18 04:16:46 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.3336
2022-08-18 04:17:20 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.1955
2022-08-18 04:17:54 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.4848
2022-08-18 04:18:28 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.4733
2022-08-18 04:19:02 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.3292
2022-08-18 04:19:36 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.3458
2022-08-18 04:20:10 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.2740
2022-08-18 04:20:44 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.5915
2022-08-18 04:21:19 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.3884
2022-08-18 04:21:53 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.3634
2022-08-18 04:22:27 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.2191
2022-08-18 04:23:00 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.2974
2022-08-18 04:23:35 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.1849
2022-08-18 04:24:09 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.1772
2022-08-18 04:24:42 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.3389
2022-08-18 04:25:16 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.6291
2022-08-18 04:25:51 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.1767
2022-08-18 04:26:25 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.1878
2022-08-18 04:26:59 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 1.8809
2022-08-18 04:27:32 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.1713
2022-08-18 04:28:06 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.3095
2022-08-18 04:28:40 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.3285
2022-08-18 04:29:14 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2741
2022-08-18 04:29:48 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.2953
2022-08-18 04:30:22 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.1956
2022-08-18 04:30:55 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.1187
2022-08-18 04:30:57 - train: epoch 008, train_loss: 2.2636
2022-08-18 04:32:14 - eval: epoch: 008, acc1: 52.636%, acc5: 78.352%, test_loss: 2.0233, per_image_load_time: 2.349ms, per_image_inference_time: 0.568ms
2022-08-18 04:32:15 - until epoch: 008, best_acc1: 53.430%
2022-08-18 04:32:15 - epoch 009 lr: 0.076790
2022-08-18 04:32:55 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 1.9440
2022-08-18 04:33:28 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.2102
2022-08-18 04:34:01 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.0860
2022-08-18 04:34:35 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.4893
2022-08-18 04:35:08 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.1692
2022-08-18 04:35:42 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.0429
2022-08-18 04:36:16 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.0985
2022-08-18 04:36:50 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.2601
2022-08-18 04:37:24 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.0352
2022-08-18 04:37:57 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1537
2022-08-18 04:38:31 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.3455
2022-08-18 04:39:05 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.4924
2022-08-18 04:39:39 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.3729
2022-08-18 04:40:13 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9136
2022-08-18 04:40:47 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 1.8946
2022-08-18 04:41:21 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2184
2022-08-18 04:41:55 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.1782
2022-08-18 04:42:29 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.1781
2022-08-18 04:43:03 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 1.9678
2022-08-18 04:43:37 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.1294
2022-08-18 04:44:11 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.3035
2022-08-18 04:44:45 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.2575
2022-08-18 04:45:19 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.1470
2022-08-18 04:45:53 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.4224
2022-08-18 04:46:27 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.0623
2022-08-18 04:47:01 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.2946
2022-08-18 04:47:35 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.2670
2022-08-18 04:48:09 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.3287
2022-08-18 04:48:44 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 1.9660
2022-08-18 04:49:17 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1561
2022-08-18 04:49:52 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.3276
2022-08-18 04:50:26 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.1498
2022-08-18 04:51:00 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.1399
2022-08-18 04:51:34 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.2574
2022-08-18 04:52:08 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.2412
2022-08-18 04:52:42 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.0337
2022-08-18 04:53:16 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.3361
2022-08-18 04:53:50 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.1439
2022-08-18 04:54:24 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.1377
2022-08-18 04:54:58 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.3412
2022-08-18 04:55:32 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.0687
2022-08-18 04:56:06 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.1199
2022-08-18 04:56:40 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 1.8486
2022-08-18 04:57:15 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.3237
2022-08-18 04:57:48 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.4357
2022-08-18 04:58:22 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.2615
2022-08-18 04:58:56 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.2488
2022-08-18 04:59:31 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.3859
2022-08-18 05:00:04 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.2951
2022-08-18 05:00:38 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.0888
2022-08-18 05:00:39 - train: epoch 009, train_loss: 2.2000
2022-08-18 05:01:57 - eval: epoch: 009, acc1: 53.030%, acc5: 78.194%, test_loss: 2.0228, per_image_load_time: 2.388ms, per_image_inference_time: 0.570ms
2022-08-18 05:01:57 - until epoch: 009, best_acc1: 53.430%
2022-08-18 05:01:57 - epoch 010 lr: 0.071288
2022-08-18 05:02:38 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 1.9896
2022-08-18 05:03:11 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.1683
2022-08-18 05:03:45 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.1566
2022-08-18 05:04:19 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.2248
2022-08-18 05:04:53 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.0971
2022-08-18 05:05:26 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.2323
2022-08-18 05:06:00 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.1886
2022-08-18 05:06:34 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.1563
2022-08-18 05:07:08 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.0495
2022-08-18 05:07:42 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.1301
2022-08-18 05:08:16 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 1.9654
2022-08-18 05:08:49 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.0181
2022-08-18 05:09:24 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 1.9981
2022-08-18 05:09:57 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.4406
2022-08-18 05:10:31 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.9470
2022-08-18 05:11:05 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 1.9674
2022-08-18 05:11:39 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.2356
2022-08-18 05:12:12 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 1.8760
2022-08-18 05:12:47 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.0362
2022-08-18 05:13:20 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.2443
2022-08-18 05:13:54 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 1.8882
2022-08-18 05:14:28 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.3404
2022-08-18 05:15:02 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.2011
2022-08-18 05:15:36 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.4104
2022-08-18 05:16:10 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.0796
2022-08-18 05:16:44 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.0416
2022-08-18 05:17:18 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 2.0525
2022-08-18 05:17:52 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.2069
2022-08-18 05:18:26 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.4221
2022-08-18 05:19:01 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.2041
2022-08-18 05:19:34 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.4643
2022-08-18 05:20:09 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.0859
2022-08-18 05:20:43 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.2574
2022-08-18 05:21:17 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.4831
2022-08-18 05:21:51 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.2589
2022-08-18 05:22:25 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.3580
2022-08-18 05:22:59 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.1187
2022-08-18 05:23:33 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1698
2022-08-18 05:24:07 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 2.0836
2022-08-18 05:24:41 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 1.8637
2022-08-18 05:25:15 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 1.9870
2022-08-18 05:25:49 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.2236
2022-08-18 05:26:23 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.0904
2022-08-18 05:26:57 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.1523
2022-08-18 05:27:31 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.0368
2022-08-18 05:28:06 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.1018
2022-08-18 05:28:40 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1624
2022-08-18 05:29:14 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.1075
2022-08-18 05:29:48 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.2272
2022-08-18 05:30:21 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.0989
2022-08-18 05:30:23 - train: epoch 010, train_loss: 2.1371
2022-08-18 05:31:41 - eval: epoch: 010, acc1: 56.232%, acc5: 80.916%, test_loss: 1.8472, per_image_load_time: 2.405ms, per_image_inference_time: 0.574ms
2022-08-18 05:31:41 - until epoch: 010, best_acc1: 56.232%
2022-08-18 05:31:41 - epoch 011 lr: 0.065450
2022-08-18 05:32:22 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 1.8576
2022-08-18 05:32:55 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.2691
2022-08-18 05:33:29 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.2023
2022-08-18 05:34:03 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.1484
2022-08-18 05:34:36 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.0749
2022-08-18 05:35:10 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 1.9652
2022-08-18 05:35:44 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.1180
2022-08-18 05:36:17 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.1340
2022-08-18 05:36:52 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.0441
2022-08-18 05:37:25 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.1152
2022-08-18 05:37:59 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.3861
2022-08-18 05:38:33 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.0535
2022-08-18 05:39:07 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.1331
2022-08-18 05:39:41 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.0392
2022-08-18 05:40:15 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.0079
2022-08-18 05:40:49 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.1957
2022-08-18 05:41:23 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.2127
2022-08-18 05:41:57 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 1.8714
2022-08-18 05:42:31 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.2305
2022-08-18 05:43:05 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1048
2022-08-18 05:43:39 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.0974
2022-08-18 05:44:13 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 1.9080
2022-08-18 05:44:47 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.1702
2022-08-18 05:45:21 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 2.0985
2022-08-18 05:45:55 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.1375
2022-08-18 05:46:29 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.2666
2022-08-18 05:47:03 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.0453
2022-08-18 05:47:37 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.7750
2022-08-18 05:48:12 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.1989
2022-08-18 05:48:45 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.3113
2022-08-18 05:49:19 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.0816
2022-08-18 05:49:53 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.0072
2022-08-18 05:50:28 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.1544
2022-08-18 05:51:02 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.0124
2022-08-18 05:51:36 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.0140
2022-08-18 05:52:10 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.0563
2022-08-18 05:52:44 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.0586
2022-08-18 05:53:18 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.8786
2022-08-18 05:53:52 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 1.9232
2022-08-18 05:54:26 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.1717
2022-08-18 05:55:00 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.9523
2022-08-18 05:55:35 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 2.0695
2022-08-18 05:56:09 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 1.9753
2022-08-18 05:56:43 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.1796
2022-08-18 05:57:17 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.0469
2022-08-18 05:57:51 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 2.0365
2022-08-18 05:58:25 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.7826
2022-08-18 05:58:59 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.7634
2022-08-18 05:59:33 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 1.8954
2022-08-18 06:00:06 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.2031
2022-08-18 06:00:08 - train: epoch 011, train_loss: 2.0756
2022-08-18 06:01:25 - eval: epoch: 011, acc1: 57.694%, acc5: 81.932%, test_loss: 1.7835, per_image_load_time: 2.417ms, per_image_inference_time: 0.581ms
2022-08-18 06:01:25 - until epoch: 011, best_acc1: 57.694%
2022-08-18 06:01:25 - epoch 012 lr: 0.059368
2022-08-18 06:02:06 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.9324
2022-08-18 06:02:39 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.8389
2022-08-18 06:03:13 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 2.0358
2022-08-18 06:03:47 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.1822
2022-08-18 06:04:21 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.1404
2022-08-18 06:04:54 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.8729
2022-08-18 06:05:28 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.6487
2022-08-18 06:06:02 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.1858
2022-08-18 06:06:36 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.0062
2022-08-18 06:07:10 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.8831
2022-08-18 06:07:43 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.2695
2022-08-18 06:08:16 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.8969
2022-08-18 06:08:50 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.9203
2022-08-18 06:09:24 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.0822
2022-08-18 06:09:58 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 2.0891
2022-08-18 06:10:32 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 2.0338
2022-08-18 06:11:05 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 2.0279
2022-08-18 06:11:40 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.0368
2022-08-18 06:12:13 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.0802
2022-08-18 06:12:48 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.2102
2022-08-18 06:13:22 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.1378
2022-08-18 06:13:55 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.0828
2022-08-18 06:14:29 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 2.0653
2022-08-18 06:15:03 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.1172
2022-08-18 06:15:37 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.9602
2022-08-18 06:16:11 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.9266
2022-08-18 06:16:45 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.1095
2022-08-18 06:17:20 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.9445
2022-08-18 06:17:54 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 1.9700
2022-08-18 06:18:27 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.5944
2022-08-18 06:19:01 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.1390
2022-08-18 06:19:35 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.8812
2022-08-18 06:20:09 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.1233
2022-08-18 06:20:43 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.2165
2022-08-18 06:21:17 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.1264
2022-08-18 06:21:51 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.8356
2022-08-18 06:22:25 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 1.9003
2022-08-18 06:22:59 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.1139
2022-08-18 06:23:33 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.9065
2022-08-18 06:24:07 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1344
2022-08-18 06:24:41 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.0266
2022-08-18 06:25:15 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 2.0686
2022-08-18 06:25:49 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 1.9385
2022-08-18 06:26:23 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 2.0867
2022-08-18 06:26:58 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 2.0770
2022-08-18 06:27:32 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.1433
2022-08-18 06:28:06 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.8302
2022-08-18 06:28:40 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.1479
2022-08-18 06:29:14 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.8092
2022-08-18 06:29:47 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.7316
2022-08-18 06:29:49 - train: epoch 012, train_loss: 2.0193
2022-08-18 06:31:06 - eval: epoch: 012, acc1: 58.198%, acc5: 82.264%, test_loss: 1.7580, per_image_load_time: 2.369ms, per_image_inference_time: 0.611ms
2022-08-18 06:31:06 - until epoch: 012, best_acc1: 58.198%
2022-08-18 06:31:06 - epoch 013 lr: 0.053138
2022-08-18 06:31:47 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.8632
2022-08-18 06:32:21 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.6803
2022-08-18 06:32:55 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.9754
2022-08-18 06:33:28 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.9619
2022-08-18 06:34:02 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 2.0034
2022-08-18 06:34:36 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 2.0225
2022-08-18 06:35:10 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.9588
2022-08-18 06:35:44 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 1.9905
2022-08-18 06:36:18 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.7691
2022-08-18 06:36:53 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 2.0082
2022-08-18 06:37:27 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.0130
2022-08-18 06:38:02 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 1.9759
2022-08-18 06:38:36 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.9285
2022-08-18 06:39:11 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 1.8927
2022-08-18 06:39:45 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.2544
2022-08-18 06:40:20 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.8366
2022-08-18 06:40:54 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 1.9169
2022-08-18 06:41:29 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.8104
2022-08-18 06:42:03 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.0626
2022-08-18 06:42:38 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 1.9274
2022-08-18 06:43:12 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.3031
2022-08-18 06:43:47 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.8722
2022-08-18 06:44:22 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 2.0057
2022-08-18 06:44:56 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 1.9438
2022-08-18 06:45:30 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.8682
2022-08-18 06:46:05 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.9373
2022-08-18 06:46:39 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.9471
2022-08-18 06:47:14 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.0701
2022-08-18 06:47:48 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.0047
2022-08-18 06:48:23 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.5988
2022-08-18 06:48:57 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.7405
2022-08-18 06:49:32 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 1.9543
2022-08-18 06:50:06 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.9218
2022-08-18 06:50:41 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.8916
2022-08-18 06:51:15 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.8564
2022-08-18 06:51:49 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.1337
2022-08-18 06:52:24 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.7517
2022-08-18 06:52:58 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1488
2022-08-18 06:53:33 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 1.8652
2022-08-18 06:54:08 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.8810
2022-08-18 06:54:42 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 2.0101
2022-08-18 06:55:17 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 2.0362
2022-08-18 06:55:52 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.8637
2022-08-18 06:56:26 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 2.0407
2022-08-18 06:57:01 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.9353
2022-08-18 06:57:35 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.0146
2022-08-18 06:58:10 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 2.0461
2022-08-18 06:58:44 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.0952
2022-08-18 06:59:18 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0650
2022-08-18 06:59:52 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 1.9698
2022-08-18 06:59:53 - train: epoch 013, train_loss: 1.9591
2022-08-18 07:01:11 - eval: epoch: 013, acc1: 60.438%, acc5: 83.994%, test_loss: 1.6459, per_image_load_time: 2.418ms, per_image_inference_time: 0.583ms
2022-08-18 07:01:11 - until epoch: 013, best_acc1: 60.438%
2022-08-18 07:01:11 - epoch 014 lr: 0.046859
2022-08-18 07:01:52 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.9034
2022-08-18 07:02:27 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.8302
2022-08-18 07:03:01 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.9596
2022-08-18 07:03:35 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.9073
2022-08-18 07:04:10 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.7920
2022-08-18 07:04:44 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.9875
2022-08-18 07:05:18 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.7547
2022-08-18 07:05:53 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.7088
2022-08-18 07:06:27 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.7782
2022-08-18 07:07:01 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.0007
2022-08-18 07:07:35 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.8179
2022-08-18 07:08:09 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 2.0166
2022-08-18 07:08:43 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.0301
2022-08-18 07:09:17 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.7874
2022-08-18 07:09:52 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.0803
2022-08-18 07:10:26 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.6639
2022-08-18 07:11:00 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9082
2022-08-18 07:11:34 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.1520
2022-08-18 07:12:08 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.7577
2022-08-18 07:12:42 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.8999
2022-08-18 07:13:16 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.0881
2022-08-18 07:13:50 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.8462
2022-08-18 07:14:25 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.7321
2022-08-18 07:14:59 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.9843
2022-08-18 07:15:34 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.8612
2022-08-18 07:16:09 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.7827
2022-08-18 07:16:43 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.9341
2022-08-18 07:17:17 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 1.9361
2022-08-18 07:17:51 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 2.0616
2022-08-18 07:18:26 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.8061
2022-08-18 07:19:00 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 2.1096
2022-08-18 07:19:35 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.8914
2022-08-18 07:20:09 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.7882
2022-08-18 07:20:44 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.8604
2022-08-18 07:21:18 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.8552
2022-08-18 07:21:53 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.7617
2022-08-18 07:22:27 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.8729
2022-08-18 07:23:01 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 2.0455
2022-08-18 07:23:36 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.8780
2022-08-18 07:24:10 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.8934
2022-08-18 07:24:45 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.7046
2022-08-18 07:25:19 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.8123
2022-08-18 07:25:54 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.8166
2022-08-18 07:26:28 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.9034
2022-08-18 07:27:03 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 2.0738
2022-08-18 07:27:37 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.9779
2022-08-18 07:28:12 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.8294
2022-08-18 07:28:46 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.9295
2022-08-18 07:29:21 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.6840
2022-08-18 07:29:54 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.9060
2022-08-18 07:29:56 - train: epoch 014, train_loss: 1.8969
2022-08-18 07:31:13 - eval: epoch: 014, acc1: 61.192%, acc5: 84.030%, test_loss: 1.6359, per_image_load_time: 2.432ms, per_image_inference_time: 0.583ms
2022-08-18 07:31:14 - until epoch: 014, best_acc1: 61.192%
2022-08-18 07:31:14 - epoch 015 lr: 0.040630
2022-08-18 07:31:55 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.7449
2022-08-18 07:32:29 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.8964
2022-08-18 07:33:02 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.0376
2022-08-18 07:33:36 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.7822
2022-08-18 07:34:10 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.8788
2022-08-18 07:34:44 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 1.9336
2022-08-18 07:35:18 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.7942
2022-08-18 07:35:52 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.8010
2022-08-18 07:36:27 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.7711
2022-08-18 07:37:01 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.7434
2022-08-18 07:37:35 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7106
2022-08-18 07:38:09 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.8764
2022-08-18 07:38:43 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 2.0031
2022-08-18 07:39:17 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.8189
2022-08-18 07:39:51 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.6773
2022-08-18 07:40:25 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 2.0349
2022-08-18 07:40:59 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.8689
2022-08-18 07:41:34 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.7107
2022-08-18 07:42:08 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.9164
2022-08-18 07:42:43 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.6729
2022-08-18 07:43:17 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.9127
2022-08-18 07:43:52 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.8907
2022-08-18 07:44:26 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.9045
2022-08-18 07:45:00 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.9058
2022-08-18 07:45:35 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 2.1166
2022-08-18 07:46:10 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.6462
2022-08-18 07:46:44 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.8275
2022-08-18 07:47:19 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.8377
2022-08-18 07:47:53 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.7997
2022-08-18 07:48:28 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.7390
2022-08-18 07:49:03 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.7325
2022-08-18 07:49:37 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.8309
2022-08-18 07:50:12 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.5817
2022-08-18 07:50:47 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.8320
2022-08-18 07:51:21 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0044
2022-08-18 07:51:56 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.8052
2022-08-18 07:52:30 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6416
2022-08-18 07:53:05 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.9399
2022-08-18 07:53:39 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.9527
2022-08-18 07:54:14 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.8140
2022-08-18 07:54:48 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.6443
2022-08-18 07:55:23 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6119
2022-08-18 07:55:57 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.7607
2022-08-18 07:56:32 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.6509
2022-08-18 07:57:07 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.8487
2022-08-18 07:57:41 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.9573
2022-08-18 07:58:15 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.8149
2022-08-18 07:58:50 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.8546
2022-08-18 07:59:24 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.6509
2022-08-18 07:59:58 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.8953
2022-08-18 07:59:59 - train: epoch 015, train_loss: 1.8360
2022-08-18 08:01:18 - eval: epoch: 015, acc1: 62.912%, acc5: 85.784%, test_loss: 1.5207, per_image_load_time: 2.405ms, per_image_inference_time: 0.579ms
2022-08-18 08:01:18 - until epoch: 015, best_acc1: 62.912%
2022-08-18 08:01:18 - epoch 016 lr: 0.034548
2022-08-18 08:01:59 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.7658
2022-08-18 08:02:33 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.7108
2022-08-18 08:03:07 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.6394
2022-08-18 08:03:41 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 1.9911
2022-08-18 08:04:15 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.6163
2022-08-18 08:04:49 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.6324
2022-08-18 08:05:23 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.5036
2022-08-18 08:05:58 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.8151
2022-08-18 08:06:32 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.6912
2022-08-18 08:07:06 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.6353
2022-08-18 08:07:40 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.6479
2022-08-18 08:08:14 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.6739
2022-08-18 08:08:49 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.7576
2022-08-18 08:09:23 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.5492
2022-08-18 08:09:57 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.9507
2022-08-18 08:10:31 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.9660
2022-08-18 08:11:05 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.7961
2022-08-18 08:11:40 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.7438
2022-08-18 08:12:14 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 2.0221
2022-08-18 08:12:48 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.7156
2022-08-18 08:13:22 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8955
2022-08-18 08:13:57 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.7675
2022-08-18 08:14:31 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.7510
2022-08-18 08:15:06 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.7759
2022-08-18 08:15:41 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.6926
2022-08-18 08:16:15 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.8190
2022-08-18 08:16:50 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.6729
2022-08-18 08:17:24 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.6494
2022-08-18 08:17:59 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.6559
2022-08-18 08:18:34 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.8487
2022-08-18 08:19:08 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.7653
2022-08-18 08:19:42 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.8709
2022-08-18 08:20:17 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.8541
2022-08-18 08:20:52 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.7683
2022-08-18 08:21:26 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.7135
2022-08-18 08:22:00 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.6490
2022-08-18 08:22:35 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.6770
2022-08-18 08:23:09 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 1.9336
2022-08-18 08:23:44 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.8366
2022-08-18 08:24:18 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.9946
2022-08-18 08:24:53 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.7677
2022-08-18 08:25:27 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.6921
2022-08-18 08:26:02 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.7010
2022-08-18 08:26:36 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.6280
2022-08-18 08:27:11 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.7557
2022-08-18 08:27:46 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.5403
2022-08-18 08:28:20 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.9804
2022-08-18 08:28:55 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.5359
2022-08-18 08:29:29 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.8461
2022-08-18 08:30:03 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.7550
2022-08-18 08:30:04 - train: epoch 016, train_loss: 1.7701
2022-08-18 08:31:22 - eval: epoch: 016, acc1: 64.688%, acc5: 86.462%, test_loss: 1.4574, per_image_load_time: 2.433ms, per_image_inference_time: 0.578ms
2022-08-18 08:31:23 - until epoch: 016, best_acc1: 64.688%
2022-08-18 08:31:23 - epoch 017 lr: 0.028710
2022-08-18 08:32:03 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.8023
2022-08-18 08:32:38 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.5373
2022-08-18 08:33:12 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.9025
2022-08-18 08:33:46 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.3633
2022-08-18 08:34:20 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.7976
2022-08-18 08:34:54 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 2.0810
2022-08-18 08:35:28 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.7432
2022-08-18 08:36:02 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.7089
2022-08-18 08:36:37 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.7338
2022-08-18 08:37:11 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.7078
2022-08-18 08:37:45 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.9330
2022-08-18 08:38:19 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.6565
2022-08-18 08:38:54 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.7987
2022-08-18 08:39:28 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.6450
2022-08-18 08:40:02 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.3574
2022-08-18 08:40:36 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.8042
2022-08-18 08:41:10 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.7038
2022-08-18 08:41:44 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.7820
2022-08-18 08:42:18 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.6971
2022-08-18 08:42:52 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.6082
2022-08-18 08:43:27 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.7412
2022-08-18 08:44:01 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.4720
2022-08-18 08:44:35 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.7953
2022-08-18 08:45:09 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.7010
2022-08-18 08:45:44 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.7383
2022-08-18 08:46:19 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.7277
2022-08-18 08:46:53 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6433
2022-08-18 08:47:27 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.8460
2022-08-18 08:48:02 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.7497
2022-08-18 08:48:36 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.3932
2022-08-18 08:49:10 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8965
2022-08-18 08:49:45 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.6706
2022-08-18 08:50:19 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8465
2022-08-18 08:50:54 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.6147
2022-08-18 08:51:28 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.7729
2022-08-18 08:52:02 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.8459
2022-08-18 08:52:37 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.6096
2022-08-18 08:53:11 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.6916
2022-08-18 08:53:46 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.6153
2022-08-18 08:54:20 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.6545
2022-08-18 08:54:55 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.6369
2022-08-18 08:55:29 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.5910
2022-08-18 08:56:04 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.8047
2022-08-18 08:56:38 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.7587
2022-08-18 08:57:12 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.7083
2022-08-18 08:57:46 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.5546
2022-08-18 08:58:21 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.7527
2022-08-18 08:58:55 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.8637
2022-08-18 08:59:29 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.7465
2022-08-18 09:00:03 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.5647
2022-08-18 09:00:04 - train: epoch 017, train_loss: 1.7034
2022-08-18 09:01:22 - eval: epoch: 017, acc1: 65.278%, acc5: 86.774%, test_loss: 1.4374, per_image_load_time: 2.256ms, per_image_inference_time: 0.637ms
2022-08-18 09:01:23 - until epoch: 017, best_acc1: 65.278%
2022-08-18 09:01:23 - epoch 018 lr: 0.023208
2022-08-18 09:02:05 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.4083
2022-08-18 09:02:38 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.8293
2022-08-18 09:03:13 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.7537
2022-08-18 09:03:47 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.8394
2022-08-18 09:04:21 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.6869
2022-08-18 09:04:55 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.7438
2022-08-18 09:05:29 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.5517
2022-08-18 09:06:03 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.5722
2022-08-18 09:06:37 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.5770
2022-08-18 09:07:11 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.6413
2022-08-18 09:07:45 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.6058
2022-08-18 09:08:19 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.5759
2022-08-18 09:08:53 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.7634
2022-08-18 09:09:27 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.7619
2022-08-18 09:10:02 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.6440
2022-08-18 09:10:36 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.5377
2022-08-18 09:11:11 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7727
2022-08-18 09:11:45 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.7484
2022-08-18 09:12:19 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.5434
2022-08-18 09:12:53 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.8811
2022-08-18 09:13:27 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.7185
2022-08-18 09:14:01 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.8516
2022-08-18 09:14:36 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6050
2022-08-18 09:15:10 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.4881
2022-08-18 09:15:44 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.6178
2022-08-18 09:16:19 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.6376
2022-08-18 09:16:53 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.7688
2022-08-18 09:17:27 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5855
2022-08-18 09:18:01 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.6103
2022-08-18 09:18:35 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.6172
2022-08-18 09:19:10 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.9275
2022-08-18 09:19:44 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.4054
2022-08-18 09:20:18 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.5295
2022-08-18 09:20:52 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.7089
2022-08-18 09:21:26 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.4581
2022-08-18 09:22:00 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.7555
2022-08-18 09:22:35 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.8143
2022-08-18 09:23:09 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.7275
2022-08-18 09:23:43 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.5638
2022-08-18 09:24:17 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.6618
2022-08-18 09:24:51 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7819
2022-08-18 09:25:26 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.6038
2022-08-18 09:26:00 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.6133
2022-08-18 09:26:34 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.5377
2022-08-18 09:27:08 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.6419
2022-08-18 09:27:43 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.5352
2022-08-18 09:28:17 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.8219
2022-08-18 09:28:51 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6414
2022-08-18 09:29:26 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.5519
2022-08-18 09:30:00 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.8974
2022-08-18 09:30:01 - train: epoch 018, train_loss: 1.6326
2022-08-18 09:31:21 - eval: epoch: 018, acc1: 66.920%, acc5: 87.880%, test_loss: 1.3584, per_image_load_time: 2.425ms, per_image_inference_time: 0.632ms
2022-08-18 09:31:21 - until epoch: 018, best_acc1: 66.920%
2022-08-18 09:31:21 - epoch 019 lr: 0.018128
2022-08-18 09:32:02 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4139
2022-08-18 09:32:37 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.5915
2022-08-18 09:33:11 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.6366
2022-08-18 09:33:45 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.3912
2022-08-18 09:34:19 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.4033
2022-08-18 09:34:53 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.5814
2022-08-18 09:35:27 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.4608
2022-08-18 09:36:01 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.7415
2022-08-18 09:36:36 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.8984
2022-08-18 09:37:10 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.4964
2022-08-18 09:37:44 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5459
2022-08-18 09:38:18 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.6014
2022-08-18 09:38:52 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.7353
2022-08-18 09:39:26 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.3971
2022-08-18 09:40:00 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.7521
2022-08-18 09:40:34 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.5458
2022-08-18 09:41:08 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.5291
2022-08-18 09:41:43 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.6076
2022-08-18 09:42:17 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.6673
2022-08-18 09:42:52 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.4673
2022-08-18 09:43:26 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.4576
2022-08-18 09:44:01 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.5505
2022-08-18 09:44:35 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.5638
2022-08-18 09:45:09 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.7836
2022-08-18 09:45:44 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.6121
2022-08-18 09:46:18 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.7831
2022-08-18 09:46:53 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.5390
2022-08-18 09:47:27 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.7558
2022-08-18 09:48:02 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.7194
2022-08-18 09:48:36 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.6891
2022-08-18 09:49:11 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.4845
2022-08-18 09:49:45 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.2138
2022-08-18 09:50:19 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.5893
2022-08-18 09:50:54 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.6102
2022-08-18 09:51:28 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.6394
2022-08-18 09:52:02 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.3556
2022-08-18 09:52:36 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.6525
2022-08-18 09:53:11 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.7834
2022-08-18 09:53:45 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.4101
2022-08-18 09:54:20 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.4616
2022-08-18 09:54:54 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.7004
2022-08-18 09:55:28 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.4905
2022-08-18 09:56:03 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.3361
2022-08-18 09:56:37 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.6114
2022-08-18 09:57:11 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.8542
2022-08-18 09:57:45 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.4915
2022-08-18 09:58:19 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.5127
2022-08-18 09:58:53 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.5406
2022-08-18 09:59:27 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.3587
2022-08-18 10:00:00 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.7175
2022-08-18 10:00:01 - train: epoch 019, train_loss: 1.5593
2022-08-18 10:01:17 - eval: epoch: 019, acc1: 68.198%, acc5: 88.530%, test_loss: 1.2984, per_image_load_time: 2.334ms, per_image_inference_time: 0.600ms
2022-08-18 10:01:18 - until epoch: 019, best_acc1: 68.198%
2022-08-18 10:01:18 - epoch 020 lr: 0.013551
2022-08-18 10:01:58 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.5562
2022-08-18 10:02:31 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.5563
2022-08-18 10:03:05 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.3199
2022-08-18 10:03:39 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.4358
2022-08-18 10:04:13 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.4170
2022-08-18 10:04:47 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.6767
2022-08-18 10:05:21 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.2702
2022-08-18 10:05:55 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.4625
2022-08-18 10:06:28 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.5056
2022-08-18 10:07:02 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.5798
2022-08-18 10:07:36 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.4040
2022-08-18 10:08:10 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.4111
2022-08-18 10:08:44 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.3817
2022-08-18 10:09:17 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.6874
2022-08-18 10:09:51 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.6451
2022-08-18 10:10:24 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.6274
2022-08-18 10:10:58 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.3537
2022-08-18 10:11:31 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.5840
2022-08-18 10:12:05 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.4818
2022-08-18 10:12:38 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.5502
2022-08-18 10:13:11 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.5869
2022-08-18 10:13:45 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.3346
2022-08-18 10:14:18 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.5869
2022-08-18 10:14:52 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.5295
2022-08-18 10:15:26 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.5012
2022-08-18 10:16:00 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.3637
2022-08-18 10:16:34 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.3747
2022-08-18 10:17:08 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.5088
2022-08-18 10:17:42 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.7074
2022-08-18 10:18:16 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.5523
2022-08-18 10:18:50 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.4907
2022-08-18 10:19:23 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.4643
2022-08-18 10:19:57 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3587
2022-08-18 10:20:32 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.4241
2022-08-18 10:21:06 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.4429
2022-08-18 10:21:40 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.4685
2022-08-18 10:22:14 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.3420
2022-08-18 10:22:48 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.5207
2022-08-18 10:23:22 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.6120
2022-08-18 10:23:56 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.4133
2022-08-18 10:24:30 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.3991
2022-08-18 10:25:05 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.4931
2022-08-18 10:25:39 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.4735
2022-08-18 10:26:13 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.3622
2022-08-18 10:26:47 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.6070
2022-08-18 10:27:21 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.5125
2022-08-18 10:27:55 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.3538
2022-08-18 10:28:30 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4113
2022-08-18 10:29:04 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.4207
2022-08-18 10:29:37 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.3031
2022-08-18 10:29:39 - train: epoch 020, train_loss: 1.4888
2022-08-18 10:30:56 - eval: epoch: 020, acc1: 69.600%, acc5: 89.456%, test_loss: 1.2349, per_image_load_time: 2.358ms, per_image_inference_time: 0.613ms
2022-08-18 10:30:56 - until epoch: 020, best_acc1: 69.600%
2022-08-18 10:30:56 - epoch 021 lr: 0.009548
2022-08-18 10:31:37 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.3915
2022-08-18 10:32:11 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.4229
2022-08-18 10:32:45 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.3160
2022-08-18 10:33:19 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.5061
2022-08-18 10:33:53 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.3709
2022-08-18 10:34:27 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.4534
2022-08-18 10:35:01 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.2561
2022-08-18 10:35:35 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.5703
2022-08-18 10:36:08 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.4140
2022-08-18 10:36:42 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.4141
2022-08-18 10:37:16 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.2683
2022-08-18 10:37:50 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.4144
2022-08-18 10:38:24 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.3125
2022-08-18 10:38:58 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.3943
2022-08-18 10:39:31 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.2543
2022-08-18 10:40:04 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3233
2022-08-18 10:40:37 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5145
2022-08-18 10:41:11 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.2151
2022-08-18 10:41:44 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.6474
2022-08-18 10:42:18 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.5691
2022-08-18 10:42:51 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.2311
2022-08-18 10:43:26 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.3292
2022-08-18 10:43:59 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.3401
2022-08-18 10:44:33 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.2919
2022-08-18 10:45:07 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.4513
2022-08-18 10:45:41 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.4952
2022-08-18 10:46:15 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.4549
2022-08-18 10:46:49 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.6245
2022-08-18 10:47:23 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.3140
2022-08-18 10:47:56 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.4551
2022-08-18 10:48:30 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.4257
2022-08-18 10:49:04 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.2814
2022-08-18 10:49:37 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.6620
2022-08-18 10:50:11 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.5252
2022-08-18 10:50:44 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.5510
2022-08-18 10:51:18 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.2649
2022-08-18 10:51:51 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.4113
2022-08-18 10:52:25 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.3932
2022-08-18 10:52:59 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.6355
2022-08-18 10:53:32 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.5705
2022-08-18 10:54:06 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.3333
2022-08-18 10:54:40 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.4674
2022-08-18 10:55:13 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3559
2022-08-18 10:55:47 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.4993
2022-08-18 10:56:21 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.3645
2022-08-18 10:56:54 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.3593
2022-08-18 10:57:28 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.6382
2022-08-18 10:58:02 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.4916
2022-08-18 10:58:36 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.1869
2022-08-18 10:59:09 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.3036
2022-08-18 10:59:10 - train: epoch 021, train_loss: 1.4140
2022-08-18 11:00:26 - eval: epoch: 021, acc1: 70.200%, acc5: 89.944%, test_loss: 1.1878, per_image_load_time: 2.238ms, per_image_inference_time: 0.607ms
2022-08-18 11:00:26 - until epoch: 021, best_acc1: 70.200%
2022-08-18 11:00:26 - epoch 022 lr: 0.006184
2022-08-18 11:01:05 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.1121
2022-08-18 11:01:38 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.3740
2022-08-18 11:02:12 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.2270
2022-08-18 11:02:46 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.3825
2022-08-18 11:03:19 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.2432
2022-08-18 11:03:52 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.4707
2022-08-18 11:04:25 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.3168
2022-08-18 11:04:59 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.4990
2022-08-18 11:05:33 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.4891
2022-08-18 11:06:07 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.3922
2022-08-18 11:06:41 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.4370
2022-08-18 11:07:15 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 0.9409
2022-08-18 11:07:48 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.2378
2022-08-18 11:08:22 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.4047
2022-08-18 11:08:56 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.5022
2022-08-18 11:09:29 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.2981
2022-08-18 11:10:03 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.4198
2022-08-18 11:10:37 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.4210
2022-08-18 11:11:10 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.3469
2022-08-18 11:11:44 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.2764
2022-08-18 11:12:18 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.3165
2022-08-18 11:12:52 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.0634
2022-08-18 11:13:26 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.4288
2022-08-18 11:13:59 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.4450
2022-08-18 11:14:33 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.2598
2022-08-18 11:15:07 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.3362
2022-08-18 11:15:40 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.1840
2022-08-18 11:16:14 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.3734
2022-08-18 11:16:48 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.3124
2022-08-18 11:17:22 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.2343
2022-08-18 11:17:56 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.3572
2022-08-18 11:18:30 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4889
2022-08-18 11:19:04 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.1450
2022-08-18 11:19:37 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.0699
2022-08-18 11:20:11 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.5541
2022-08-18 11:20:45 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.3145
2022-08-18 11:21:18 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.5221
2022-08-18 11:21:52 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.5519
2022-08-18 11:22:26 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.3013
2022-08-18 11:23:00 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.4076
2022-08-18 11:23:33 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.1600
2022-08-18 11:24:07 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.3798
2022-08-18 11:24:41 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4761
2022-08-18 11:25:15 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.4863
2022-08-18 11:25:48 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.2541
2022-08-18 11:26:22 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.4066
2022-08-18 11:26:56 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.4789
2022-08-18 11:27:30 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.1958
2022-08-18 11:28:03 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.4337
2022-08-18 11:28:36 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.1694
2022-08-18 11:28:38 - train: epoch 022, train_loss: 1.3493
2022-08-18 11:29:54 - eval: epoch: 022, acc1: 71.554%, acc5: 90.334%, test_loss: 1.1490, per_image_load_time: 2.319ms, per_image_inference_time: 0.616ms
2022-08-18 11:29:54 - until epoch: 022, best_acc1: 71.554%
2022-08-18 11:29:54 - epoch 023 lr: 0.003511
2022-08-18 11:30:34 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.1312
2022-08-18 11:31:08 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.3126
2022-08-18 11:31:41 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.2871
2022-08-18 11:32:14 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.3122
2022-08-18 11:32:48 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.3905
2022-08-18 11:33:21 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.0617
2022-08-18 11:33:54 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.1609
2022-08-18 11:34:27 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.2781
2022-08-18 11:35:00 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4004
2022-08-18 11:35:33 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.4488
2022-08-18 11:36:07 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.3565
2022-08-18 11:36:40 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.2035
2022-08-18 11:37:14 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.3107
2022-08-18 11:37:47 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.3627
2022-08-18 11:38:21 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.4510
2022-08-18 11:38:55 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.2268
2022-08-18 11:39:28 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.3398
2022-08-18 11:40:02 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.1005
2022-08-18 11:40:35 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.2334
2022-08-18 11:41:09 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.0165
2022-08-18 11:41:43 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.2517
2022-08-18 11:42:17 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.1736
2022-08-18 11:42:50 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.2901
2022-08-18 11:43:24 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.2860
2022-08-18 11:43:58 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.2916
2022-08-18 11:44:32 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.3827
2022-08-18 11:45:06 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.3636
2022-08-18 11:45:39 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.1937
2022-08-18 11:46:13 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.1840
2022-08-18 11:46:46 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.3152
2022-08-18 11:47:20 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.3933
2022-08-18 11:47:54 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.3016
2022-08-18 11:48:27 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.2771
2022-08-18 11:49:01 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.3248
2022-08-18 11:49:35 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.1823
2022-08-18 11:50:09 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.3167
2022-08-18 11:50:43 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.1200
2022-08-18 11:51:17 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.3826
2022-08-18 11:51:50 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3919
2022-08-18 11:52:25 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.1837
2022-08-18 11:52:59 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.2242
2022-08-18 11:53:33 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.2357
2022-08-18 11:54:06 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.0216
2022-08-18 11:54:40 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.2717
2022-08-18 11:55:14 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.0717
2022-08-18 11:55:48 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.4498
2022-08-18 11:56:22 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.2293
2022-08-18 11:56:56 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.2339
2022-08-18 11:57:30 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.1336
2022-08-18 11:58:03 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.5012
2022-08-18 11:58:04 - train: epoch 023, train_loss: 1.2888
2022-08-18 11:59:20 - eval: epoch: 023, acc1: 72.466%, acc5: 90.854%, test_loss: 1.1129, per_image_load_time: 2.300ms, per_image_inference_time: 0.615ms
2022-08-18 11:59:20 - until epoch: 023, best_acc1: 72.466%
2022-08-18 11:59:20 - epoch 024 lr: 0.001571
2022-08-18 12:00:00 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.3249
2022-08-18 12:00:33 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.3405
2022-08-18 12:01:07 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.3098
2022-08-18 12:01:40 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.2425
2022-08-18 12:02:14 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.3541
2022-08-18 12:02:47 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.3279
2022-08-18 12:03:21 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.2022
2022-08-18 12:03:55 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.1239
2022-08-18 12:04:28 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.3087
2022-08-18 12:05:02 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.2291
2022-08-18 12:05:36 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 0.9803
2022-08-18 12:06:09 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.2225
2022-08-18 12:06:43 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4262
2022-08-18 12:07:16 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.3010
2022-08-18 12:07:50 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.3732
2022-08-18 12:08:23 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.0699
2022-08-18 12:08:57 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.1371
2022-08-18 12:09:31 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.4270
2022-08-18 12:10:04 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.0855
2022-08-18 12:10:38 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.3002
2022-08-18 12:11:12 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.1848
2022-08-18 12:11:46 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.3281
2022-08-18 12:12:20 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.1400
2022-08-18 12:12:53 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.3977
2022-08-18 12:13:27 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.3678
2022-08-18 12:14:00 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.4703
2022-08-18 12:14:33 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.3881
2022-08-18 12:15:07 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.3029
2022-08-18 12:15:40 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.3281
2022-08-18 12:16:13 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.1812
2022-08-18 12:16:47 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.1381
2022-08-18 12:17:20 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.3443
2022-08-18 12:17:54 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.0905
2022-08-18 12:18:28 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.2937
2022-08-18 12:19:01 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.2159
2022-08-18 12:19:34 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.1364
2022-08-18 12:20:08 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.5138
2022-08-18 12:20:42 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.3699
2022-08-18 12:21:15 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.3204
2022-08-18 12:21:48 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.4106
2022-08-18 12:22:22 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.2145
2022-08-18 12:22:55 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.1516
2022-08-18 12:23:28 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.3034
2022-08-18 12:24:02 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.2922
2022-08-18 12:24:35 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.1402
2022-08-18 12:25:08 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.4467
2022-08-18 12:25:42 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.2247
2022-08-18 12:26:15 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.1507
2022-08-18 12:26:49 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.2605
2022-08-18 12:27:22 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.2501
2022-08-18 12:27:23 - train: epoch 024, train_loss: 1.2525
2022-08-18 12:28:40 - eval: epoch: 024, acc1: 72.716%, acc5: 91.004%, test_loss: 1.0976, per_image_load_time: 2.347ms, per_image_inference_time: 0.578ms
2022-08-18 12:28:40 - until epoch: 024, best_acc1: 72.716%
2022-08-18 12:28:40 - epoch 025 lr: 0.000394
2022-08-18 12:29:20 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.1454
2022-08-18 12:29:53 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1321
2022-08-18 12:30:26 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.2079
2022-08-18 12:30:59 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.3502
2022-08-18 12:31:31 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.0616
2022-08-18 12:32:04 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3674
2022-08-18 12:32:36 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.1239
2022-08-18 12:33:09 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.2567
2022-08-18 12:33:42 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.2892
2022-08-18 12:34:16 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3473
2022-08-18 12:34:49 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.4086
2022-08-18 12:35:22 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.3311
2022-08-18 12:35:56 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.1406
2022-08-18 12:36:29 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.3835
2022-08-18 12:37:03 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.2421
2022-08-18 12:37:36 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1509
2022-08-18 12:38:09 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.2677
2022-08-18 12:38:42 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 0.9960
2022-08-18 12:39:16 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.3260
2022-08-18 12:39:49 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.3361
2022-08-18 12:40:23 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.1802
2022-08-18 12:40:56 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.1855
2022-08-18 12:41:29 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.1081
2022-08-18 12:42:03 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.2324
2022-08-18 12:42:37 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.2039
2022-08-18 12:43:11 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.1342
2022-08-18 12:43:45 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.4349
2022-08-18 12:44:19 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.3852
2022-08-18 12:44:52 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.2906
2022-08-18 12:45:26 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.2481
2022-08-18 12:46:00 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.1771
2022-08-18 12:46:33 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.2469
2022-08-18 12:47:06 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.1055
2022-08-18 12:47:39 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.3537
2022-08-18 12:48:13 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1377
2022-08-18 12:48:46 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.4245
2022-08-18 12:49:19 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.1512
2022-08-18 12:49:52 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3894
2022-08-18 12:50:26 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.3565
2022-08-18 12:50:59 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.2887
2022-08-18 12:51:32 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.4435
2022-08-18 12:52:06 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.2702
2022-08-18 12:52:39 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.0735
2022-08-18 12:53:12 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.1411
2022-08-18 12:53:47 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.1090
2022-08-18 12:54:21 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.1126
2022-08-18 12:54:55 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.1452
2022-08-18 12:55:29 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.2588
2022-08-18 12:56:04 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.3102
2022-08-18 12:56:37 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.2090
2022-08-18 12:56:39 - train: epoch 025, train_loss: 1.2350
2022-08-18 12:57:56 - eval: epoch: 025, acc1: 72.734%, acc5: 91.058%, test_loss: 1.0954, per_image_load_time: 1.611ms, per_image_inference_time: 0.586ms
2022-08-18 12:57:56 - until epoch: 025, best_acc1: 72.734%
2022-08-18 12:57:56 - train done. train time: 12.412 hours, best_acc1: 72.734%
