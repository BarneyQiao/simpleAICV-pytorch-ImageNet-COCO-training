2022-08-22 04:12:27 - net_idx: 14
2022-08-22 04:12:27 - net_config: {'stem_width': 64, 'depth': 13, 'w_0': 40, 'w_a': 18.096607541181776, 'w_m': 1.8504615808901832}
2022-08-22 04:12:27 - num_classes: 1000
2022-08-22 04:12:27 - input_image_size: 224
2022-08-22 04:12:27 - scale: 1.1428571428571428
2022-08-22 04:12:27 - seed: 0
2022-08-22 04:12:27 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-22 04:12:27 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-22 04:12:27 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c2100>
2022-08-22 04:12:27 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c23d0>
2022-08-22 04:12:27 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2400>
2022-08-22 04:12:27 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2460>
2022-08-22 04:12:27 - batch_size: 256
2022-08-22 04:12:27 - num_workers: 16
2022-08-22 04:12:27 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-22 04:12:27 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-22 04:12:27 - epochs: 25
2022-08-22 04:12:27 - print_interval: 100
2022-08-22 04:12:27 - accumulation_steps: 1
2022-08-22 04:12:27 - sync_bn: False
2022-08-22 04:12:27 - apex: True
2022-08-22 04:12:27 - use_ema_model: False
2022-08-22 04:12:27 - ema_model_decay: 0.9999
2022-08-22 04:12:27 - log_dir: ./log
2022-08-22 04:12:27 - checkpoint_dir: ./checkpoints
2022-08-22 04:12:27 - gpus_type: NVIDIA RTX A5000
2022-08-22 04:12:27 - gpus_num: 2
2022-08-22 04:12:27 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f4efca864b0>
2022-08-22 04:12:27 - ema_model: None
2022-08-22 04:12:27 - --------------------parameters--------------------
2022-08-22 04:12:27 - name: conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-22 04:12:27 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-22 04:12:27 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-22 04:12:27 - name: fc.weight, grad: True
2022-08-22 04:12:27 - name: fc.bias, grad: True
2022-08-22 04:12:27 - --------------------buffers--------------------
2022-08-22 04:12:27 - name: conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-22 04:12:27 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-22 04:12:27 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 04:12:27 - -----------no weight decay layers--------------
2022-08-22 04:12:27 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 04:12:27 - -------------weight decay layers---------------
2022-08-22 04:12:27 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 04:12:27 - epoch 001 lr: 0.100000
2022-08-22 04:13:10 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9105
2022-08-22 04:13:43 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9011
2022-08-22 04:14:16 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.7841
2022-08-22 04:14:49 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.7730
2022-08-22 04:15:22 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.6878
2022-08-22 04:15:55 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.5048
2022-08-22 04:16:27 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.6158
2022-08-22 04:17:00 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.3888
2022-08-22 04:17:33 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.3320
2022-08-22 04:18:06 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.3206
2022-08-22 04:18:39 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.0865
2022-08-22 04:19:12 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 5.9392
2022-08-22 04:19:45 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 5.8944
2022-08-22 04:20:18 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 5.8651
2022-08-22 04:20:51 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 5.6836
2022-08-22 04:21:24 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 5.7781
2022-08-22 04:21:58 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.4535
2022-08-22 04:22:31 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.5264
2022-08-22 04:23:05 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.4495
2022-08-22 04:23:38 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.2867
2022-08-22 04:24:12 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.2616
2022-08-22 04:24:45 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.1717
2022-08-22 04:25:18 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.1525
2022-08-22 04:25:51 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.1666
2022-08-22 04:26:24 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.1375
2022-08-22 04:26:57 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.1979
2022-08-22 04:27:30 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.0852
2022-08-22 04:28:04 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.0259
2022-08-22 04:28:37 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 4.9219
2022-08-22 04:29:10 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.0297
2022-08-22 04:29:44 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.0030
2022-08-22 04:30:17 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 4.8788
2022-08-22 04:30:50 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.5867
2022-08-22 04:31:23 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.7045
2022-08-22 04:31:57 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.6069
2022-08-22 04:32:30 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.6577
2022-08-22 04:33:03 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.6625
2022-08-22 04:33:37 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.3731
2022-08-22 04:34:10 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.5237
2022-08-22 04:34:43 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.3474
2022-08-22 04:35:16 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.5572
2022-08-22 04:35:49 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.3084
2022-08-22 04:36:22 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.4251
2022-08-22 04:36:55 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.1028
2022-08-22 04:37:29 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.2601
2022-08-22 04:38:02 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.4958
2022-08-22 04:38:35 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.3332
2022-08-22 04:39:08 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.3925
2022-08-22 04:39:42 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.3192
2022-08-22 04:40:14 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.1463
2022-08-22 04:40:16 - train: epoch 001, train_loss: 5.2686
2022-08-22 04:41:36 - eval: epoch: 001, acc1: 18.942%, acc5: 41.300%, test_loss: 4.2188, per_image_load_time: 1.459ms, per_image_inference_time: 0.635ms
2022-08-22 04:41:36 - until epoch: 001, best_acc1: 18.942%
2022-08-22 04:41:36 - epoch 002 lr: 0.099606
2022-08-22 04:42:18 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.0113
2022-08-22 04:42:51 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 3.8626
2022-08-22 04:43:24 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 3.9957
2022-08-22 04:43:57 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.1057
2022-08-22 04:44:30 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 3.8766
2022-08-22 04:45:03 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 3.8727
2022-08-22 04:45:36 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.0514
2022-08-22 04:46:09 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 4.0050
2022-08-22 04:46:43 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.7711
2022-08-22 04:47:16 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.0001
2022-08-22 04:47:49 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.0051
2022-08-22 04:48:22 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.7740
2022-08-22 04:48:55 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.8529
2022-08-22 04:49:28 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.0543
2022-08-22 04:50:01 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 3.8125
2022-08-22 04:50:35 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.7290
2022-08-22 04:51:07 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.6762
2022-08-22 04:51:41 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.6859
2022-08-22 04:52:14 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.6620
2022-08-22 04:52:48 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.3236
2022-08-22 04:53:21 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.7556
2022-08-22 04:53:54 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.6170
2022-08-22 04:54:28 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.7439
2022-08-22 04:55:01 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.5705
2022-08-22 04:55:35 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.3940
2022-08-22 04:56:08 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.5297
2022-08-22 04:56:41 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.8300
2022-08-22 04:57:14 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.6907
2022-08-22 04:57:48 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.6212
2022-08-22 04:58:21 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.4888
2022-08-22 04:58:54 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.3570
2022-08-22 04:59:27 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.4989
2022-08-22 05:00:01 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.5080
2022-08-22 05:00:34 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.3506
2022-08-22 05:01:08 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.3834
2022-08-22 05:01:41 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.3593
2022-08-22 05:02:15 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.4953
2022-08-22 05:02:48 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.2831
2022-08-22 05:03:22 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.4806
2022-08-22 05:03:55 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.1348
2022-08-22 05:04:28 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.5081
2022-08-22 05:05:01 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.3142
2022-08-22 05:05:35 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.2061
2022-08-22 05:06:07 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.3793
2022-08-22 05:06:41 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.1485
2022-08-22 05:07:14 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.0680
2022-08-22 05:07:47 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.2241
2022-08-22 05:08:20 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.4867
2022-08-22 05:08:54 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.1945
2022-08-22 05:09:27 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.3276
2022-08-22 05:09:29 - train: epoch 002, train_loss: 3.6041
2022-08-22 05:10:47 - eval: epoch: 002, acc1: 31.790%, acc5: 58.308%, test_loss: 3.2009, per_image_load_time: 2.451ms, per_image_inference_time: 0.608ms
2022-08-22 05:10:48 - until epoch: 002, best_acc1: 31.790%
2022-08-22 05:10:48 - epoch 003 lr: 0.098429
2022-08-22 05:11:30 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.1793
2022-08-22 05:12:03 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.2215
2022-08-22 05:12:36 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.3507
2022-08-22 05:13:09 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.1734
2022-08-22 05:13:42 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.3925
2022-08-22 05:14:15 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.1498
2022-08-22 05:14:49 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.2941
2022-08-22 05:15:22 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.3899
2022-08-22 05:15:55 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.3432
2022-08-22 05:16:29 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.2517
2022-08-22 05:17:01 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 2.9811
2022-08-22 05:17:34 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.0160
2022-08-22 05:18:06 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.0703
2022-08-22 05:18:39 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.0572
2022-08-22 05:19:11 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.2870
2022-08-22 05:19:44 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.1008
2022-08-22 05:20:16 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.0856
2022-08-22 05:20:49 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.0014
2022-08-22 05:21:22 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.0806
2022-08-22 05:21:54 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 2.9713
2022-08-22 05:22:27 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.1151
2022-08-22 05:22:59 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.4381
2022-08-22 05:23:31 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.8943
2022-08-22 05:24:04 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 2.9563
2022-08-22 05:24:37 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.1442
2022-08-22 05:25:09 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.1050
2022-08-22 05:25:42 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.2295
2022-08-22 05:26:15 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 2.9899
2022-08-22 05:26:48 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.9350
2022-08-22 05:27:21 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.0953
2022-08-22 05:27:54 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.2564
2022-08-22 05:28:27 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.1015
2022-08-22 05:29:00 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.1161
2022-08-22 05:29:33 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.1655
2022-08-22 05:30:07 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 3.0245
2022-08-22 05:30:40 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.8694
2022-08-22 05:31:13 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 2.8954
2022-08-22 05:31:46 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 2.8758
2022-08-22 05:32:20 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.3383
2022-08-22 05:32:53 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7093
2022-08-22 05:33:26 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.9955
2022-08-22 05:34:00 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 2.9496
2022-08-22 05:34:33 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.9768
2022-08-22 05:35:06 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 3.1257
2022-08-22 05:35:40 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.0064
2022-08-22 05:36:14 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.9357
2022-08-22 05:36:47 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 2.9326
2022-08-22 05:37:20 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.1310
2022-08-22 05:37:54 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.0763
2022-08-22 05:38:27 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.7767
2022-08-22 05:38:28 - train: epoch 003, train_loss: 3.0706
2022-08-22 05:39:44 - eval: epoch: 003, acc1: 40.388%, acc5: 66.962%, test_loss: 2.7083, per_image_load_time: 1.317ms, per_image_inference_time: 0.667ms
2022-08-22 05:39:45 - until epoch: 003, best_acc1: 40.388%
2022-08-22 05:39:45 - epoch 004 lr: 0.096488
2022-08-22 05:40:24 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 3.0036
2022-08-22 05:40:56 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.7448
2022-08-22 05:41:29 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.8673
2022-08-22 05:42:01 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.7455
2022-08-22 05:42:33 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.7550
2022-08-22 05:43:06 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.1768
2022-08-22 05:43:38 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.7609
2022-08-22 05:44:11 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.7026
2022-08-22 05:44:44 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.7207
2022-08-22 05:45:16 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.9546
2022-08-22 05:45:49 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.1094
2022-08-22 05:46:22 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.6547
2022-08-22 05:46:55 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.5652
2022-08-22 05:47:28 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.6939
2022-08-22 05:48:01 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.9935
2022-08-22 05:48:34 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.8076
2022-08-22 05:49:07 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.6481
2022-08-22 05:49:41 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.9574
2022-08-22 05:50:15 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.9528
2022-08-22 05:50:48 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.9380
2022-08-22 05:51:21 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.9709
2022-08-22 05:51:54 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.7305
2022-08-22 05:52:28 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.3828
2022-08-22 05:53:01 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.6546
2022-08-22 05:53:34 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.7324
2022-08-22 05:54:08 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.8834
2022-08-22 05:54:41 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.5493
2022-08-22 05:55:15 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8999
2022-08-22 05:55:48 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8236
2022-08-22 05:56:21 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.7621
2022-08-22 05:56:55 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.8239
2022-08-22 05:57:28 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8331
2022-08-22 05:58:02 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.9387
2022-08-22 05:58:35 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.8518
2022-08-22 05:59:09 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.9920
2022-08-22 05:59:42 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.7005
2022-08-22 06:00:16 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.7725
2022-08-22 06:00:49 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.5002
2022-08-22 06:01:23 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.5629
2022-08-22 06:01:56 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.5561
2022-08-22 06:02:30 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.5808
2022-08-22 06:03:03 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6102
2022-08-22 06:03:37 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.5473
2022-08-22 06:04:10 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.6435
2022-08-22 06:04:44 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.2631
2022-08-22 06:05:18 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.6221
2022-08-22 06:05:51 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.6644
2022-08-22 06:06:25 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.4927
2022-08-22 06:06:58 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.7119
2022-08-22 06:07:31 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.6091
2022-08-22 06:07:33 - train: epoch 004, train_loss: 2.7984
2022-08-22 06:08:50 - eval: epoch: 004, acc1: 44.794%, acc5: 71.506%, test_loss: 2.4438, per_image_load_time: 2.211ms, per_image_inference_time: 0.681ms
2022-08-22 06:08:50 - until epoch: 004, best_acc1: 44.794%
2022-08-22 06:08:50 - epoch 005 lr: 0.093815
2022-08-22 06:09:30 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.7317
2022-08-22 06:10:03 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.8757
2022-08-22 06:10:36 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.8203
2022-08-22 06:11:10 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.6903
2022-08-22 06:11:43 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.3719
2022-08-22 06:12:17 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.7001
2022-08-22 06:12:50 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.7544
2022-08-22 06:13:24 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.7318
2022-08-22 06:13:58 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.5936
2022-08-22 06:14:31 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.6794
2022-08-22 06:15:05 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.6160
2022-08-22 06:15:39 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.7413
2022-08-22 06:16:12 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.6844
2022-08-22 06:16:46 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.5411
2022-08-22 06:17:20 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.3449
2022-08-22 06:17:53 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.2179
2022-08-22 06:18:27 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.5861
2022-08-22 06:19:01 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.5177
2022-08-22 06:19:34 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.5586
2022-08-22 06:20:08 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.6559
2022-08-22 06:20:42 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.4704
2022-08-22 06:21:16 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.5303
2022-08-22 06:21:49 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.6199
2022-08-22 06:22:23 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5845
2022-08-22 06:22:57 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.6339
2022-08-22 06:23:30 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.8338
2022-08-22 06:24:04 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.4963
2022-08-22 06:24:38 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.5023
2022-08-22 06:25:12 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.4339
2022-08-22 06:25:45 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.4930
2022-08-22 06:26:19 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.4788
2022-08-22 06:26:52 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.7208
2022-08-22 06:27:26 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.4009
2022-08-22 06:28:00 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.5820
2022-08-22 06:28:34 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.6353
2022-08-22 06:29:07 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.7254
2022-08-22 06:29:41 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.6015
2022-08-22 06:30:14 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5865
2022-08-22 06:30:48 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.8766
2022-08-22 06:31:21 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.6585
2022-08-22 06:31:55 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.5886
2022-08-22 06:32:29 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.6331
2022-08-22 06:33:02 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.7245
2022-08-22 06:33:36 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.5463
2022-08-22 06:34:10 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.7959
2022-08-22 06:34:44 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.5310
2022-08-22 06:35:17 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.3031
2022-08-22 06:35:51 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.3977
2022-08-22 06:36:25 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.7967
2022-08-22 06:36:58 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.3943
2022-08-22 06:37:00 - train: epoch 005, train_loss: 2.6257
2022-08-22 06:38:15 - eval: epoch: 005, acc1: 47.784%, acc5: 74.032%, test_loss: 2.2881, per_image_load_time: 1.684ms, per_image_inference_time: 0.669ms
2022-08-22 06:38:16 - until epoch: 005, best_acc1: 47.784%
2022-08-22 06:38:16 - epoch 006 lr: 0.090450
2022-08-22 06:38:57 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.5007
2022-08-22 06:39:30 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.5803
2022-08-22 06:40:03 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.4697
2022-08-22 06:40:35 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.5021
2022-08-22 06:41:09 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.4498
2022-08-22 06:41:42 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.4859
2022-08-22 06:42:15 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.5972
2022-08-22 06:42:48 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.4679
2022-08-22 06:43:21 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3000
2022-08-22 06:43:54 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.3667
2022-08-22 06:44:28 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.4647
2022-08-22 06:45:01 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.6257
2022-08-22 06:45:34 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5149
2022-08-22 06:46:08 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.7382
2022-08-22 06:46:41 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.6074
2022-08-22 06:47:15 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3203
2022-08-22 06:47:48 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.5838
2022-08-22 06:48:22 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.4366
2022-08-22 06:48:56 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.3198
2022-08-22 06:49:29 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.7075
2022-08-22 06:50:03 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.5326
2022-08-22 06:50:37 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.2958
2022-08-22 06:51:10 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.4817
2022-08-22 06:51:44 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.4050
2022-08-22 06:52:17 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4315
2022-08-22 06:52:50 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.4662
2022-08-22 06:53:24 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.6251
2022-08-22 06:53:58 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.2148
2022-08-22 06:54:31 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.5979
2022-08-22 06:55:05 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.5704
2022-08-22 06:55:38 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.2680
2022-08-22 06:56:11 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.4276
2022-08-22 06:56:45 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.3455
2022-08-22 06:57:18 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.6908
2022-08-22 06:57:52 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.6731
2022-08-22 06:58:25 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.5810
2022-08-22 06:58:59 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.5568
2022-08-22 06:59:32 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.3930
2022-08-22 07:00:06 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.4350
2022-08-22 07:00:40 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.6139
2022-08-22 07:01:14 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.4590
2022-08-22 07:01:47 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.2217
2022-08-22 07:02:20 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.4874
2022-08-22 07:02:54 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.5449
2022-08-22 07:03:28 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.5040
2022-08-22 07:04:01 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.4752
2022-08-22 07:04:34 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.5429
2022-08-22 07:05:08 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.5619
2022-08-22 07:05:41 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.4642
2022-08-22 07:06:14 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.2130
2022-08-22 07:06:16 - train: epoch 006, train_loss: 2.5052
2022-08-22 07:07:32 - eval: epoch: 006, acc1: 49.196%, acc5: 75.230%, test_loss: 2.2199, per_image_load_time: 1.986ms, per_image_inference_time: 0.676ms
2022-08-22 07:07:32 - until epoch: 006, best_acc1: 49.196%
2022-08-22 07:07:32 - epoch 007 lr: 0.086448
2022-08-22 07:08:12 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.3164
2022-08-22 07:08:45 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.5872
2022-08-22 07:09:18 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.4046
2022-08-22 07:09:51 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.4767
2022-08-22 07:10:24 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.1184
2022-08-22 07:10:57 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.4560
2022-08-22 07:11:30 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.4393
2022-08-22 07:12:04 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.3389
2022-08-22 07:12:37 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.3832
2022-08-22 07:13:10 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.5243
2022-08-22 07:13:44 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.2545
2022-08-22 07:14:17 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.3940
2022-08-22 07:14:50 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.1398
2022-08-22 07:15:24 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.4722
2022-08-22 07:15:57 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.4577
2022-08-22 07:16:31 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.3223
2022-08-22 07:17:04 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.6911
2022-08-22 07:17:38 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.5213
2022-08-22 07:18:12 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.3619
2022-08-22 07:18:46 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.1764
2022-08-22 07:19:19 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.4817
2022-08-22 07:19:53 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.4926
2022-08-22 07:20:26 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.4890
2022-08-22 07:21:00 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.6480
2022-08-22 07:21:33 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.3925
2022-08-22 07:22:07 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.2259
2022-08-22 07:22:40 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.3918
2022-08-22 07:23:14 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.4825
2022-08-22 07:23:47 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.3763
2022-08-22 07:24:21 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.5951
2022-08-22 07:24:55 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.2564
2022-08-22 07:25:28 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.2653
2022-08-22 07:26:02 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.6389
2022-08-22 07:26:35 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.4111
2022-08-22 07:27:08 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.4217
2022-08-22 07:27:42 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.2257
2022-08-22 07:28:15 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.2469
2022-08-22 07:28:48 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.4990
2022-08-22 07:29:22 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.5227
2022-08-22 07:29:55 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.3785
2022-08-22 07:30:28 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.2597
2022-08-22 07:31:02 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.3360
2022-08-22 07:31:36 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.4727
2022-08-22 07:32:10 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.2925
2022-08-22 07:32:43 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.5993
2022-08-22 07:33:17 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.4168
2022-08-22 07:33:50 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.3667
2022-08-22 07:34:24 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.5399
2022-08-22 07:34:57 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.2079
2022-08-22 07:35:30 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.2364
2022-08-22 07:35:32 - train: epoch 007, train_loss: 2.4163
2022-08-22 07:36:49 - eval: epoch: 007, acc1: 51.128%, acc5: 76.848%, test_loss: 2.1094, per_image_load_time: 2.344ms, per_image_inference_time: 0.661ms
2022-08-22 07:36:49 - until epoch: 007, best_acc1: 51.128%
2022-08-22 07:36:49 - epoch 008 lr: 0.081870
2022-08-22 07:37:30 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.3398
2022-08-22 07:38:02 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.4551
2022-08-22 07:38:35 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.4006
2022-08-22 07:39:08 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.1993
2022-08-22 07:39:41 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.2475
2022-08-22 07:40:14 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.4794
2022-08-22 07:40:47 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.5408
2022-08-22 07:41:20 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.0418
2022-08-22 07:41:53 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.2471
2022-08-22 07:42:26 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.4539
2022-08-22 07:42:59 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.1935
2022-08-22 07:43:31 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.1488
2022-08-22 07:44:04 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.5467
2022-08-22 07:44:37 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.2125
2022-08-22 07:45:10 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.4653
2022-08-22 07:45:44 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.5146
2022-08-22 07:46:16 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.3791
2022-08-22 07:46:49 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.4763
2022-08-22 07:47:22 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.2557
2022-08-22 07:47:56 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.4931
2022-08-22 07:48:29 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.6014
2022-08-22 07:49:02 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.2587
2022-08-22 07:49:36 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.4918
2022-08-22 07:50:09 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.3411
2022-08-22 07:50:42 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.3773
2022-08-22 07:51:15 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.2820
2022-08-22 07:51:48 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.5095
2022-08-22 07:52:22 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.4281
2022-08-22 07:52:55 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.5731
2022-08-22 07:53:28 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.6011
2022-08-22 07:54:02 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.3439
2022-08-22 07:54:35 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.5707
2022-08-22 07:55:09 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.5098
2022-08-22 07:55:42 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.3337
2022-08-22 07:56:16 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.5018
2022-08-22 07:56:49 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.3943
2022-08-22 07:57:23 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.1148
2022-08-22 07:57:56 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.2860
2022-08-22 07:58:30 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.6229
2022-08-22 07:59:03 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.5554
2022-08-22 07:59:37 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.1359
2022-08-22 08:00:10 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.2626
2022-08-22 08:00:44 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.0469
2022-08-22 08:01:17 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.2793
2022-08-22 08:01:50 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.2777
2022-08-22 08:02:23 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.4337
2022-08-22 08:02:57 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2858
2022-08-22 08:03:30 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.2658
2022-08-22 08:04:04 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.4174
2022-08-22 08:04:37 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.3205
2022-08-22 08:04:38 - train: epoch 008, train_loss: 2.3427
2022-08-22 08:05:56 - eval: epoch: 008, acc1: 52.724%, acc5: 78.456%, test_loss: 2.0283, per_image_load_time: 1.593ms, per_image_inference_time: 0.675ms
2022-08-22 08:05:56 - until epoch: 008, best_acc1: 52.724%
2022-08-22 08:05:56 - epoch 009 lr: 0.076790
2022-08-22 08:06:36 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 2.0364
2022-08-22 08:07:08 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.2270
2022-08-22 08:07:41 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.1960
2022-08-22 08:08:13 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.2361
2022-08-22 08:08:46 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.2710
2022-08-22 08:09:18 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.1562
2022-08-22 08:09:51 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.1104
2022-08-22 08:10:24 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.3025
2022-08-22 08:10:56 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.0712
2022-08-22 08:11:29 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.2683
2022-08-22 08:12:02 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.4259
2022-08-22 08:12:35 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.3087
2022-08-22 08:13:08 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.3372
2022-08-22 08:13:41 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9733
2022-08-22 08:14:15 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.1893
2022-08-22 08:14:47 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.3429
2022-08-22 08:15:20 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.3371
2022-08-22 08:15:53 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.3408
2022-08-22 08:16:27 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 1.8471
2022-08-22 08:17:00 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.0603
2022-08-22 08:17:33 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.4414
2022-08-22 08:18:06 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.3239
2022-08-22 08:18:39 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.3272
2022-08-22 08:19:12 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.4921
2022-08-22 08:19:45 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.2211
2022-08-22 08:20:18 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.3014
2022-08-22 08:20:51 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.2472
2022-08-22 08:21:25 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2096
2022-08-22 08:21:58 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.1245
2022-08-22 08:22:31 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1812
2022-08-22 08:23:04 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.3439
2022-08-22 08:23:38 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.1633
2022-08-22 08:24:11 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.2254
2022-08-22 08:24:44 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.2763
2022-08-22 08:25:18 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.3013
2022-08-22 08:25:51 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.0291
2022-08-22 08:26:25 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.3645
2022-08-22 08:26:58 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.3216
2022-08-22 08:27:32 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 1.9719
2022-08-22 08:28:05 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.3836
2022-08-22 08:28:39 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.2347
2022-08-22 08:29:13 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.0774
2022-08-22 08:29:46 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.1854
2022-08-22 08:30:19 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.2555
2022-08-22 08:30:53 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.3334
2022-08-22 08:31:26 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.3760
2022-08-22 08:32:00 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.4758
2022-08-22 08:32:33 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.3364
2022-08-22 08:33:07 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.4121
2022-08-22 08:33:40 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.1022
2022-08-22 08:33:42 - train: epoch 009, train_loss: 2.2754
2022-08-22 08:34:59 - eval: epoch: 009, acc1: 52.926%, acc5: 78.010%, test_loss: 2.0270, per_image_load_time: 2.370ms, per_image_inference_time: 0.624ms
2022-08-22 08:34:59 - until epoch: 009, best_acc1: 52.926%
2022-08-22 08:34:59 - epoch 010 lr: 0.071288
2022-08-22 08:35:41 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.3892
2022-08-22 08:36:14 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.0910
2022-08-22 08:36:47 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.1328
2022-08-22 08:37:19 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.3003
2022-08-22 08:37:53 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.0923
2022-08-22 08:38:26 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.2759
2022-08-22 08:39:00 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.1733
2022-08-22 08:39:34 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.2054
2022-08-22 08:40:07 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.1888
2022-08-22 08:40:40 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.2352
2022-08-22 08:41:14 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.0461
2022-08-22 08:41:48 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.2969
2022-08-22 08:42:22 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.1567
2022-08-22 08:42:55 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.4155
2022-08-22 08:43:29 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.9546
2022-08-22 08:44:03 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.3208
2022-08-22 08:44:36 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.4637
2022-08-22 08:45:10 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 1.9757
2022-08-22 08:45:44 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.4036
2022-08-22 08:46:17 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.3648
2022-08-22 08:46:51 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 2.2301
2022-08-22 08:47:24 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.4437
2022-08-22 08:47:58 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.4130
2022-08-22 08:48:32 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.4082
2022-08-22 08:49:05 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.1132
2022-08-22 08:49:39 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.1905
2022-08-22 08:50:13 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.8871
2022-08-22 08:50:47 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.0917
2022-08-22 08:51:21 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.5659
2022-08-22 08:51:54 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.2878
2022-08-22 08:52:28 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.4107
2022-08-22 08:53:02 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.2270
2022-08-22 08:53:36 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.2920
2022-08-22 08:54:10 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.2854
2022-08-22 08:54:44 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.4382
2022-08-22 08:55:18 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.4873
2022-08-22 08:55:52 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.1922
2022-08-22 08:56:26 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1493
2022-08-22 08:57:00 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 1.9878
2022-08-22 08:57:34 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.1954
2022-08-22 08:58:08 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.1708
2022-08-22 08:58:42 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.1246
2022-08-22 08:59:16 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.1813
2022-08-22 08:59:50 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.0706
2022-08-22 09:00:24 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.3862
2022-08-22 09:00:58 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 1.9916
2022-08-22 09:01:32 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1176
2022-08-22 09:02:06 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.1364
2022-08-22 09:02:40 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.0688
2022-08-22 09:03:13 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.1284
2022-08-22 09:03:15 - train: epoch 010, train_loss: 2.2173
2022-08-22 09:04:32 - eval: epoch: 010, acc1: 54.708%, acc5: 79.640%, test_loss: 1.9395, per_image_load_time: 2.348ms, per_image_inference_time: 0.629ms
2022-08-22 09:04:33 - until epoch: 010, best_acc1: 54.708%
2022-08-22 09:04:33 - epoch 011 lr: 0.065450
2022-08-22 09:05:13 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 1.9628
2022-08-22 09:05:46 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.2423
2022-08-22 09:06:20 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.3118
2022-08-22 09:06:53 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.1973
2022-08-22 09:07:27 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.0784
2022-08-22 09:08:00 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.2219
2022-08-22 09:08:34 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.2664
2022-08-22 09:09:08 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.2746
2022-08-22 09:09:42 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.2195
2022-08-22 09:10:16 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.0150
2022-08-22 09:10:50 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2142
2022-08-22 09:11:24 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.3804
2022-08-22 09:11:57 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.3853
2022-08-22 09:12:31 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.3342
2022-08-22 09:13:05 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.1038
2022-08-22 09:13:39 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.3241
2022-08-22 09:14:13 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.3008
2022-08-22 09:14:47 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 1.9989
2022-08-22 09:15:21 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.0536
2022-08-22 09:15:54 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1959
2022-08-22 09:16:28 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.1311
2022-08-22 09:17:02 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 1.9682
2022-08-22 09:17:36 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.0937
2022-08-22 09:18:10 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.9546
2022-08-22 09:18:43 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.1017
2022-08-22 09:19:17 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.1785
2022-08-22 09:19:51 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 1.9727
2022-08-22 09:20:25 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.9352
2022-08-22 09:20:59 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.0965
2022-08-22 09:21:33 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.2420
2022-08-22 09:22:07 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.1742
2022-08-22 09:22:41 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.0246
2022-08-22 09:23:15 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.2515
2022-08-22 09:23:49 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.0313
2022-08-22 09:24:23 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.1775
2022-08-22 09:24:57 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.1733
2022-08-22 09:25:31 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.3393
2022-08-22 09:26:05 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 2.0875
2022-08-22 09:26:39 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 1.8707
2022-08-22 09:27:13 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 1.9712
2022-08-22 09:27:47 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 2.0805
2022-08-22 09:28:21 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 2.3238
2022-08-22 09:28:54 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.2428
2022-08-22 09:29:29 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.3207
2022-08-22 09:30:03 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 1.8932
2022-08-22 09:30:37 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 1.9790
2022-08-22 09:31:11 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.8085
2022-08-22 09:31:45 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.8207
2022-08-22 09:32:19 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.1672
2022-08-22 09:32:53 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.1224
2022-08-22 09:32:54 - train: epoch 011, train_loss: 2.1573
2022-08-22 09:34:12 - eval: epoch: 011, acc1: 56.798%, acc5: 80.978%, test_loss: 1.8368, per_image_load_time: 1.644ms, per_image_inference_time: 0.635ms
2022-08-22 09:34:12 - until epoch: 011, best_acc1: 56.798%
2022-08-22 09:34:12 - epoch 012 lr: 0.059368
2022-08-22 09:34:52 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 2.0620
2022-08-22 09:35:25 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.9338
2022-08-22 09:35:59 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 1.8913
2022-08-22 09:36:32 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.0988
2022-08-22 09:37:05 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.0193
2022-08-22 09:37:38 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.8918
2022-08-22 09:38:12 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 2.1519
2022-08-22 09:38:46 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.2078
2022-08-22 09:39:19 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.1384
2022-08-22 09:39:53 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 2.1271
2022-08-22 09:40:26 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.2919
2022-08-22 09:40:59 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.8370
2022-08-22 09:41:33 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.9240
2022-08-22 09:42:06 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 1.9352
2022-08-22 09:42:40 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 2.0341
2022-08-22 09:43:14 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 1.9822
2022-08-22 09:43:47 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.8830
2022-08-22 09:44:21 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.3372
2022-08-22 09:44:54 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1035
2022-08-22 09:45:28 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.3411
2022-08-22 09:46:02 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.2669
2022-08-22 09:46:35 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.1450
2022-08-22 09:47:09 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 1.9412
2022-08-22 09:47:43 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 1.9869
2022-08-22 09:48:17 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.8045
2022-08-22 09:48:51 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.9030
2022-08-22 09:49:24 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.0848
2022-08-22 09:49:58 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 2.2702
2022-08-22 09:50:32 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.0889
2022-08-22 09:51:06 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.9254
2022-08-22 09:51:40 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.0079
2022-08-22 09:52:13 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 2.1231
2022-08-22 09:52:47 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.1607
2022-08-22 09:53:21 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.0405
2022-08-22 09:53:54 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 1.9884
2022-08-22 09:54:28 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.9181
2022-08-22 09:55:01 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.1454
2022-08-22 09:55:35 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.1164
2022-08-22 09:56:09 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.9370
2022-08-22 09:56:43 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1960
2022-08-22 09:57:17 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.0264
2022-08-22 09:57:51 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.9287
2022-08-22 09:58:24 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.1085
2022-08-22 09:58:58 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.9982
2022-08-22 09:59:32 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.9247
2022-08-22 10:00:06 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.4504
2022-08-22 10:00:40 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 2.1479
2022-08-22 10:01:14 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.0247
2022-08-22 10:01:48 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.9157
2022-08-22 10:02:21 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.8518
2022-08-22 10:02:23 - train: epoch 012, train_loss: 2.1000
2022-08-22 10:03:40 - eval: epoch: 012, acc1: 58.052%, acc5: 81.998%, test_loss: 1.7789, per_image_load_time: 2.195ms, per_image_inference_time: 0.632ms
2022-08-22 10:03:40 - until epoch: 012, best_acc1: 58.052%
2022-08-22 10:03:40 - epoch 013 lr: 0.053138
2022-08-22 10:04:22 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.9319
2022-08-22 10:04:55 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.9757
2022-08-22 10:05:28 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8734
2022-08-22 10:06:01 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.9792
2022-08-22 10:06:34 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 2.0055
2022-08-22 10:07:07 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 2.0889
2022-08-22 10:07:41 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 2.0986
2022-08-22 10:08:14 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.0879
2022-08-22 10:08:48 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 2.1446
2022-08-22 10:09:22 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 2.0772
2022-08-22 10:09:55 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.0344
2022-08-22 10:10:29 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 1.9493
2022-08-22 10:11:03 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.8615
2022-08-22 10:11:37 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.0872
2022-08-22 10:12:10 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.2487
2022-08-22 10:12:44 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.8106
2022-08-22 10:13:18 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 1.9620
2022-08-22 10:13:52 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 2.0222
2022-08-22 10:14:26 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.2314
2022-08-22 10:15:00 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.2493
2022-08-22 10:15:34 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.1867
2022-08-22 10:16:07 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 2.0499
2022-08-22 10:16:41 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 1.9507
2022-08-22 10:17:15 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.1410
2022-08-22 10:17:49 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.8373
2022-08-22 10:18:22 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 2.1099
2022-08-22 10:18:56 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.9015
2022-08-22 10:19:30 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.0155
2022-08-22 10:20:04 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 1.9564
2022-08-22 10:20:38 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.7882
2022-08-22 10:21:12 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.9264
2022-08-22 10:21:45 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 2.1748
2022-08-22 10:22:19 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.9908
2022-08-22 10:22:52 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.9941
2022-08-22 10:23:26 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.9319
2022-08-22 10:24:00 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.0191
2022-08-22 10:24:34 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.7194
2022-08-22 10:25:07 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.2801
2022-08-22 10:25:41 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.1527
2022-08-22 10:26:16 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.8520
2022-08-22 10:26:50 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 2.0299
2022-08-22 10:27:24 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 2.0559
2022-08-22 10:27:57 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.8652
2022-08-22 10:28:31 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 2.1163
2022-08-22 10:29:06 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.9549
2022-08-22 10:29:39 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.0904
2022-08-22 10:30:13 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.9578
2022-08-22 10:30:47 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.0698
2022-08-22 10:31:21 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0773
2022-08-22 10:31:55 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.0856
2022-08-22 10:31:56 - train: epoch 013, train_loss: 2.0420
2022-08-22 10:33:13 - eval: epoch: 013, acc1: 59.110%, acc5: 82.868%, test_loss: 1.7205, per_image_load_time: 2.227ms, per_image_inference_time: 0.675ms
2022-08-22 10:33:13 - until epoch: 013, best_acc1: 59.110%
2022-08-22 10:33:13 - epoch 014 lr: 0.046859
2022-08-22 10:33:54 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 2.0243
2022-08-22 10:34:28 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 2.1033
2022-08-22 10:35:02 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.8447
2022-08-22 10:35:35 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 2.0515
2022-08-22 10:36:09 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.9149
2022-08-22 10:36:43 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.8354
2022-08-22 10:37:16 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 2.0335
2022-08-22 10:37:50 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.9359
2022-08-22 10:38:24 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.9840
2022-08-22 10:38:57 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 1.8962
2022-08-22 10:39:31 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.7517
2022-08-22 10:40:04 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 2.0615
2022-08-22 10:40:38 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 1.9981
2022-08-22 10:41:12 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 2.0768
2022-08-22 10:41:45 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.0515
2022-08-22 10:42:18 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.9428
2022-08-22 10:42:52 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9448
2022-08-22 10:43:26 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 1.9808
2022-08-22 10:43:59 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.9372
2022-08-22 10:44:33 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.6840
2022-08-22 10:45:07 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.1379
2022-08-22 10:45:40 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.9685
2022-08-22 10:46:14 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.9735
2022-08-22 10:46:48 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.9342
2022-08-22 10:47:21 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.9582
2022-08-22 10:47:55 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.9522
2022-08-22 10:48:29 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.9132
2022-08-22 10:49:02 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 2.1148
2022-08-22 10:49:36 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 2.0873
2022-08-22 10:50:09 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 2.0783
2022-08-22 10:50:43 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 2.0930
2022-08-22 10:51:17 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.9217
2022-08-22 10:51:51 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.9796
2022-08-22 10:52:25 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.9815
2022-08-22 10:52:59 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 2.0645
2022-08-22 10:53:33 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.9388
2022-08-22 10:54:06 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 2.0933
2022-08-22 10:54:40 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 2.0082
2022-08-22 10:55:14 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.8816
2022-08-22 10:55:48 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.9288
2022-08-22 10:56:22 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.9635
2022-08-22 10:56:56 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.9533
2022-08-22 10:57:30 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.8089
2022-08-22 10:58:04 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.8812
2022-08-22 10:58:37 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 2.0462
2022-08-22 10:59:11 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.9183
2022-08-22 10:59:45 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.8405
2022-08-22 11:00:19 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.9216
2022-08-22 11:00:53 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8791
2022-08-22 11:01:27 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.9789
2022-08-22 11:01:28 - train: epoch 014, train_loss: 1.9812
2022-08-22 11:02:46 - eval: epoch: 014, acc1: 59.836%, acc5: 83.304%, test_loss: 1.6920, per_image_load_time: 1.778ms, per_image_inference_time: 0.655ms
2022-08-22 11:02:46 - until epoch: 014, best_acc1: 59.836%
2022-08-22 11:02:46 - epoch 015 lr: 0.040630
2022-08-22 11:03:28 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.6308
2022-08-22 11:04:01 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.9723
2022-08-22 11:04:34 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.1189
2022-08-22 11:05:07 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.9742
2022-08-22 11:05:40 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 2.0043
2022-08-22 11:06:14 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 1.8076
2022-08-22 11:06:47 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.9924
2022-08-22 11:07:21 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.7920
2022-08-22 11:07:55 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.9218
2022-08-22 11:08:29 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.9090
2022-08-22 11:09:02 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.8144
2022-08-22 11:09:36 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.9257
2022-08-22 11:10:09 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.9658
2022-08-22 11:10:43 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.9258
2022-08-22 11:11:17 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.8858
2022-08-22 11:11:51 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 2.0176
2022-08-22 11:12:24 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.9557
2022-08-22 11:12:58 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.8774
2022-08-22 11:13:31 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.7434
2022-08-22 11:14:04 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.8485
2022-08-22 11:14:38 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.8514
2022-08-22 11:15:12 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9416
2022-08-22 11:15:45 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.7278
2022-08-22 11:16:19 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.7508
2022-08-22 11:16:52 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.9917
2022-08-22 11:17:26 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.7425
2022-08-22 11:18:00 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.8686
2022-08-22 11:18:34 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.9794
2022-08-22 11:19:08 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.9736
2022-08-22 11:19:42 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.8439
2022-08-22 11:20:16 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.8459
2022-08-22 11:20:50 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 2.0370
2022-08-22 11:21:24 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.8659
2022-08-22 11:21:58 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 2.0823
2022-08-22 11:22:31 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 1.9894
2022-08-22 11:23:05 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.8787
2022-08-22 11:23:39 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6566
2022-08-22 11:24:13 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.9431
2022-08-22 11:24:47 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8040
2022-08-22 11:25:21 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.8757
2022-08-22 11:25:55 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.8230
2022-08-22 11:26:29 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.8308
2022-08-22 11:27:03 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.9050
2022-08-22 11:27:37 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.6972
2022-08-22 11:28:11 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.8153
2022-08-22 11:28:45 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.8788
2022-08-22 11:29:19 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.7697
2022-08-22 11:29:53 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.9227
2022-08-22 11:30:27 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.9852
2022-08-22 11:31:00 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.9949
2022-08-22 11:31:02 - train: epoch 015, train_loss: 1.9211
2022-08-22 11:32:19 - eval: epoch: 015, acc1: 61.874%, acc5: 84.804%, test_loss: 1.5910, per_image_load_time: 2.335ms, per_image_inference_time: 0.630ms
2022-08-22 11:32:19 - until epoch: 015, best_acc1: 61.874%
2022-08-22 11:32:19 - epoch 016 lr: 0.034548
2022-08-22 11:33:01 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.8126
2022-08-22 11:33:34 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.7149
2022-08-22 11:34:08 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.8528
2022-08-22 11:34:41 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.1165
2022-08-22 11:35:14 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.7489
2022-08-22 11:35:48 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.7751
2022-08-22 11:36:21 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.7140
2022-08-22 11:36:55 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.9808
2022-08-22 11:37:28 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.8345
2022-08-22 11:38:02 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.7895
2022-08-22 11:38:36 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.8414
2022-08-22 11:39:10 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.6932
2022-08-22 11:39:43 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.8024
2022-08-22 11:40:17 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.6121
2022-08-22 11:40:51 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 2.0616
2022-08-22 11:41:24 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 2.1128
2022-08-22 11:41:58 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.8353
2022-08-22 11:42:32 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 2.1426
2022-08-22 11:43:05 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.9981
2022-08-22 11:43:39 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.5006
2022-08-22 11:44:13 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.9211
2022-08-22 11:44:47 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 2.0157
2022-08-22 11:45:20 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.9958
2022-08-22 11:45:54 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 2.0502
2022-08-22 11:46:28 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.7719
2022-08-22 11:47:01 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.8267
2022-08-22 11:47:35 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.6335
2022-08-22 11:48:08 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.8150
2022-08-22 11:48:42 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.9008
2022-08-22 11:49:16 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.9108
2022-08-22 11:49:49 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.9864
2022-08-22 11:50:23 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.9293
2022-08-22 11:50:56 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9898
2022-08-22 11:51:30 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.6622
2022-08-22 11:52:04 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.8119
2022-08-22 11:52:37 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.8476
2022-08-22 11:53:10 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.8087
2022-08-22 11:53:44 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.0685
2022-08-22 11:54:17 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.8757
2022-08-22 11:54:50 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 2.0035
2022-08-22 11:55:24 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 2.0115
2022-08-22 11:55:58 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.7303
2022-08-22 11:56:31 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.8008
2022-08-22 11:57:05 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.7734
2022-08-22 11:57:39 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.9039
2022-08-22 11:58:13 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.6672
2022-08-22 11:58:46 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 2.0361
2022-08-22 11:59:20 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.8139
2022-08-22 11:59:54 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.8513
2022-08-22 12:00:27 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.8184
2022-08-22 12:00:28 - train: epoch 016, train_loss: 1.8584
2022-08-22 12:01:46 - eval: epoch: 016, acc1: 63.116%, acc5: 85.478%, test_loss: 1.5329, per_image_load_time: 2.148ms, per_image_inference_time: 0.634ms
2022-08-22 12:01:46 - until epoch: 016, best_acc1: 63.116%
2022-08-22 12:01:46 - epoch 017 lr: 0.028710
2022-08-22 12:02:28 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.9285
2022-08-22 12:03:01 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.9575
2022-08-22 12:03:34 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 2.1319
2022-08-22 12:04:08 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.4974
2022-08-22 12:04:41 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.8609
2022-08-22 12:05:14 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 2.3080
2022-08-22 12:05:48 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.6794
2022-08-22 12:06:21 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.8535
2022-08-22 12:06:54 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.6767
2022-08-22 12:07:29 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.7150
2022-08-22 12:08:03 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 2.0387
2022-08-22 12:08:37 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.7100
2022-08-22 12:09:11 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 2.0215
2022-08-22 12:09:45 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.8562
2022-08-22 12:10:18 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.8482
2022-08-22 12:10:52 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.5536
2022-08-22 12:11:26 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.6428
2022-08-22 12:12:00 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.9329
2022-08-22 12:12:34 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.6059
2022-08-22 12:13:07 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.9854
2022-08-22 12:13:41 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.7104
2022-08-22 12:14:15 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.7553
2022-08-22 12:14:49 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.7394
2022-08-22 12:15:23 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.7409
2022-08-22 12:15:57 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.8415
2022-08-22 12:16:31 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.7220
2022-08-22 12:17:04 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.7473
2022-08-22 12:17:38 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.7879
2022-08-22 12:18:13 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.8722
2022-08-22 12:18:47 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.7895
2022-08-22 12:19:21 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.9024
2022-08-22 12:19:55 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.7820
2022-08-22 12:20:29 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8285
2022-08-22 12:21:03 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.7936
2022-08-22 12:21:37 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.7737
2022-08-22 12:22:10 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.8159
2022-08-22 12:22:44 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.5135
2022-08-22 12:23:18 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.6962
2022-08-22 12:23:52 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.7923
2022-08-22 12:24:26 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.7164
2022-08-22 12:25:00 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.7930
2022-08-22 12:25:34 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.6891
2022-08-22 12:26:08 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.8829
2022-08-22 12:26:42 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8857
2022-08-22 12:27:15 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.7537
2022-08-22 12:27:49 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.5919
2022-08-22 12:28:23 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.8803
2022-08-22 12:28:57 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.8739
2022-08-22 12:29:31 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.6512
2022-08-22 12:30:05 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.5384
2022-08-22 12:30:06 - train: epoch 017, train_loss: 1.7912
2022-08-22 12:31:23 - eval: epoch: 017, acc1: 64.170%, acc5: 86.058%, test_loss: 1.4832, per_image_load_time: 2.365ms, per_image_inference_time: 0.611ms
2022-08-22 12:31:23 - until epoch: 017, best_acc1: 64.170%
2022-08-22 12:31:23 - epoch 018 lr: 0.023208
2022-08-22 12:32:05 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.6616
2022-08-22 12:32:38 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.8113
2022-08-22 12:33:11 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.7599
2022-08-22 12:33:45 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 2.1715
2022-08-22 12:34:18 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.7655
2022-08-22 12:34:52 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.7252
2022-08-22 12:35:25 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.7300
2022-08-22 12:35:58 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.6906
2022-08-22 12:36:32 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.9135
2022-08-22 12:37:06 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.7277
2022-08-22 12:37:39 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.9635
2022-08-22 12:38:13 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.7224
2022-08-22 12:38:46 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.7965
2022-08-22 12:39:20 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.9474
2022-08-22 12:39:54 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.9170
2022-08-22 12:40:28 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.7424
2022-08-22 12:41:02 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7589
2022-08-22 12:41:35 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6941
2022-08-22 12:42:09 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.7078
2022-08-22 12:42:43 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.9930
2022-08-22 12:43:17 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.9361
2022-08-22 12:43:50 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.8149
2022-08-22 12:44:24 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6110
2022-08-22 12:44:58 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.5434
2022-08-22 12:45:31 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.5530
2022-08-22 12:46:05 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.8474
2022-08-22 12:46:39 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.7038
2022-08-22 12:47:13 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.6482
2022-08-22 12:47:46 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.6228
2022-08-22 12:48:20 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.6932
2022-08-22 12:48:54 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 2.1080
2022-08-22 12:49:28 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.6917
2022-08-22 12:50:01 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.5314
2022-08-22 12:50:35 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.6063
2022-08-22 12:51:09 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.7183
2022-08-22 12:51:43 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.7507
2022-08-22 12:52:17 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.7664
2022-08-22 12:52:51 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.9554
2022-08-22 12:53:24 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.6781
2022-08-22 12:53:58 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.7355
2022-08-22 12:54:32 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7422
2022-08-22 12:55:05 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.7203
2022-08-22 12:55:39 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.6617
2022-08-22 12:56:13 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.5977
2022-08-22 12:56:47 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.6960
2022-08-22 12:57:22 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.7424
2022-08-22 12:57:55 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.9863
2022-08-22 12:58:29 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.9225
2022-08-22 12:59:03 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.8186
2022-08-22 12:59:36 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.7774
2022-08-22 12:59:38 - train: epoch 018, train_loss: 1.7220
2022-08-22 13:00:55 - eval: epoch: 018, acc1: 65.488%, acc5: 86.950%, test_loss: 1.4273, per_image_load_time: 0.626ms, per_image_inference_time: 0.625ms
2022-08-22 13:00:55 - until epoch: 018, best_acc1: 65.488%
2022-08-22 13:00:55 - epoch 019 lr: 0.018128
2022-08-22 13:01:36 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4843
2022-08-22 13:02:10 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.7526
2022-08-22 13:02:43 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.8274
2022-08-22 13:03:17 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.6342
2022-08-22 13:03:50 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.5671
2022-08-22 13:04:24 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.6243
2022-08-22 13:04:58 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.5605
2022-08-22 13:05:32 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.7571
2022-08-22 13:06:06 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.7592
2022-08-22 13:06:40 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.4817
2022-08-22 13:07:13 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5664
2022-08-22 13:07:47 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.6468
2022-08-22 13:08:21 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.6851
2022-08-22 13:08:55 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.3064
2022-08-22 13:09:29 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.7825
2022-08-22 13:10:03 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.5636
2022-08-22 13:10:36 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.8632
2022-08-22 13:11:10 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.6241
2022-08-22 13:11:44 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.9976
2022-08-22 13:12:18 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.7372
2022-08-22 13:12:52 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.4300
2022-08-22 13:13:26 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.7776
2022-08-22 13:14:00 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.7136
2022-08-22 13:14:34 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.7695
2022-08-22 13:15:08 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.5929
2022-08-22 13:15:42 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.6693
2022-08-22 13:16:16 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.6462
2022-08-22 13:16:50 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.6633
2022-08-22 13:17:24 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.7889
2022-08-22 13:17:58 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.6617
2022-08-22 13:18:32 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.8303
2022-08-22 13:19:06 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.3922
2022-08-22 13:19:40 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.6611
2022-08-22 13:20:14 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.6002
2022-08-22 13:20:48 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.7369
2022-08-22 13:21:22 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.3811
2022-08-22 13:21:56 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.7933
2022-08-22 13:22:30 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.8521
2022-08-22 13:23:04 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.5016
2022-08-22 13:23:38 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.6091
2022-08-22 13:24:12 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.7821
2022-08-22 13:24:46 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.6354
2022-08-22 13:25:20 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.7483
2022-08-22 13:25:54 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.7543
2022-08-22 13:26:28 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 2.0167
2022-08-22 13:27:02 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.5257
2022-08-22 13:27:36 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.6128
2022-08-22 13:28:10 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.7324
2022-08-22 13:28:44 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.6601
2022-08-22 13:29:18 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.7464
2022-08-22 13:29:20 - train: epoch 019, train_loss: 1.6522
2022-08-22 13:30:36 - eval: epoch: 019, acc1: 66.642%, acc5: 87.586%, test_loss: 1.3722, per_image_load_time: 0.559ms, per_image_inference_time: 0.628ms
2022-08-22 13:30:36 - until epoch: 019, best_acc1: 66.642%
2022-08-22 13:30:36 - epoch 020 lr: 0.013551
2022-08-22 13:31:17 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.5890
2022-08-22 13:31:50 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.4933
2022-08-22 13:32:24 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.4225
2022-08-22 13:32:58 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.3660
2022-08-22 13:33:32 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.4863
2022-08-22 13:34:06 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.8027
2022-08-22 13:34:40 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.4248
2022-08-22 13:35:14 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.6333
2022-08-22 13:35:47 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.6806
2022-08-22 13:36:21 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.6014
2022-08-22 13:36:55 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.5081
2022-08-22 13:37:29 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.4984
2022-08-22 13:38:03 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.4854
2022-08-22 13:38:37 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.5916
2022-08-22 13:39:11 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.6178
2022-08-22 13:39:45 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.7012
2022-08-22 13:40:19 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.5320
2022-08-22 13:40:53 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.5393
2022-08-22 13:41:27 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.3608
2022-08-22 13:42:01 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.8032
2022-08-22 13:42:35 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.7067
2022-08-22 13:43:09 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.4284
2022-08-22 13:43:43 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.5148
2022-08-22 13:44:17 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.6606
2022-08-22 13:44:51 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.5552
2022-08-22 13:45:25 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.4065
2022-08-22 13:45:59 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.4852
2022-08-22 13:46:33 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.5301
2022-08-22 13:47:07 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.7097
2022-08-22 13:47:41 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.8272
2022-08-22 13:48:14 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.7121
2022-08-22 13:48:48 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.6086
2022-08-22 13:49:22 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.4539
2022-08-22 13:49:56 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.7650
2022-08-22 13:50:30 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.3373
2022-08-22 13:51:04 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.5685
2022-08-22 13:51:38 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.4973
2022-08-22 13:52:12 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.4999
2022-08-22 13:52:46 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.7803
2022-08-22 13:53:19 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.4373
2022-08-22 13:53:53 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.4569
2022-08-22 13:54:27 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.5226
2022-08-22 13:55:02 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.6505
2022-08-22 13:55:36 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.4831
2022-08-22 13:56:10 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.6453
2022-08-22 13:56:44 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.5949
2022-08-22 13:57:18 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.4322
2022-08-22 13:57:52 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.5504
2022-08-22 13:58:26 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.5037
2022-08-22 13:58:59 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.4366
2022-08-22 13:59:01 - train: epoch 020, train_loss: 1.5825
2022-08-22 14:00:17 - eval: epoch: 020, acc1: 68.024%, acc5: 88.486%, test_loss: 1.3086, per_image_load_time: 0.810ms, per_image_inference_time: 0.665ms
2022-08-22 14:00:17 - until epoch: 020, best_acc1: 68.024%
2022-08-22 14:00:17 - epoch 021 lr: 0.009548
2022-08-22 14:00:57 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.4342
2022-08-22 14:01:30 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.4766
2022-08-22 14:02:03 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.4420
2022-08-22 14:02:37 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4795
2022-08-22 14:03:10 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.4790
2022-08-22 14:03:44 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.3364
2022-08-22 14:04:18 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3824
2022-08-22 14:04:51 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.5521
2022-08-22 14:05:24 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.4565
2022-08-22 14:05:58 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3602
2022-08-22 14:06:31 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.4921
2022-08-22 14:07:05 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.5723
2022-08-22 14:07:39 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.3561
2022-08-22 14:08:13 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.3234
2022-08-22 14:08:46 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.4569
2022-08-22 14:09:20 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3961
2022-08-22 14:09:54 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.6274
2022-08-22 14:10:28 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.5166
2022-08-22 14:11:02 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.7540
2022-08-22 14:11:35 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.6717
2022-08-22 14:12:09 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.4236
2022-08-22 14:12:43 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.5434
2022-08-22 14:13:17 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.5864
2022-08-22 14:13:50 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.3861
2022-08-22 14:14:24 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.6134
2022-08-22 14:14:58 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.6534
2022-08-22 14:15:32 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.4801
2022-08-22 14:16:05 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.5057
2022-08-22 14:16:39 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.2158
2022-08-22 14:17:13 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.7032
2022-08-22 14:17:46 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.4012
2022-08-22 14:18:20 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.3844
2022-08-22 14:18:54 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.6779
2022-08-22 14:19:28 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.6808
2022-08-22 14:20:02 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4727
2022-08-22 14:20:36 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.3121
2022-08-22 14:21:10 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.3484
2022-08-22 14:21:44 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.6142
2022-08-22 14:22:18 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.3769
2022-08-22 14:22:51 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.8313
2022-08-22 14:23:25 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.4342
2022-08-22 14:23:59 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.6696
2022-08-22 14:24:33 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.5550
2022-08-22 14:25:06 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.4967
2022-08-22 14:25:40 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.5578
2022-08-22 14:26:14 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.5174
2022-08-22 14:26:48 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.8325
2022-08-22 14:27:22 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.5005
2022-08-22 14:27:56 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.3318
2022-08-22 14:28:30 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.3742
2022-08-22 14:28:31 - train: epoch 021, train_loss: 1.5117
2022-08-22 14:29:48 - eval: epoch: 021, acc1: 68.918%, acc5: 88.998%, test_loss: 1.2650, per_image_load_time: 0.675ms, per_image_inference_time: 0.649ms
2022-08-22 14:29:48 - until epoch: 021, best_acc1: 68.918%
2022-08-22 14:29:48 - epoch 022 lr: 0.006184
2022-08-22 14:30:28 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.2476
2022-08-22 14:31:02 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.5151
2022-08-22 14:31:36 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.3700
2022-08-22 14:32:09 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.5562
2022-08-22 14:32:43 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.4924
2022-08-22 14:33:16 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.5722
2022-08-22 14:33:49 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.6003
2022-08-22 14:34:23 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.5050
2022-08-22 14:34:57 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.4950
2022-08-22 14:35:31 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.5106
2022-08-22 14:36:04 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3840
2022-08-22 14:36:39 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.1761
2022-08-22 14:37:12 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.6084
2022-08-22 14:37:46 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.3049
2022-08-22 14:38:20 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.4887
2022-08-22 14:38:54 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.4720
2022-08-22 14:39:27 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.5190
2022-08-22 14:40:01 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.6350
2022-08-22 14:40:35 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.4634
2022-08-22 14:41:09 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.3275
2022-08-22 14:41:42 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.3089
2022-08-22 14:42:16 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.2726
2022-08-22 14:42:49 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.4740
2022-08-22 14:43:23 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.5335
2022-08-22 14:43:57 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.5001
2022-08-22 14:44:31 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.3623
2022-08-22 14:45:05 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.2833
2022-08-22 14:45:39 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.4713
2022-08-22 14:46:12 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.3208
2022-08-22 14:46:46 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.6645
2022-08-22 14:47:20 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.6336
2022-08-22 14:47:53 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4556
2022-08-22 14:48:27 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.3577
2022-08-22 14:49:01 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.4068
2022-08-22 14:49:34 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.4680
2022-08-22 14:50:08 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.5054
2022-08-22 14:50:42 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.4717
2022-08-22 14:51:16 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.6511
2022-08-22 14:51:50 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.1374
2022-08-22 14:52:24 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.5235
2022-08-22 14:52:58 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.3301
2022-08-22 14:53:32 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.4135
2022-08-22 14:54:06 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4567
2022-08-22 14:54:40 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3311
2022-08-22 14:55:13 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.4402
2022-08-22 14:55:47 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.7815
2022-08-22 14:56:21 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.3862
2022-08-22 14:56:54 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.3381
2022-08-22 14:57:28 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.3180
2022-08-22 14:58:02 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.2923
2022-08-22 14:58:03 - train: epoch 022, train_loss: 1.4472
2022-08-22 14:59:20 - eval: epoch: 022, acc1: 69.890%, acc5: 89.514%, test_loss: 1.2231, per_image_load_time: 2.311ms, per_image_inference_time: 0.640ms
2022-08-22 14:59:20 - until epoch: 022, best_acc1: 69.890%
2022-08-22 14:59:20 - epoch 023 lr: 0.003511
2022-08-22 15:00:01 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.2552
2022-08-22 15:00:34 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.2944
2022-08-22 15:01:07 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.3621
2022-08-22 15:01:40 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.4758
2022-08-22 15:02:13 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.5040
2022-08-22 15:02:47 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.1705
2022-08-22 15:03:20 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.3194
2022-08-22 15:03:54 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.5562
2022-08-22 15:04:28 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4130
2022-08-22 15:05:01 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.4838
2022-08-22 15:05:35 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.4195
2022-08-22 15:06:09 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.3821
2022-08-22 15:06:42 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.4943
2022-08-22 15:07:16 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.3852
2022-08-22 15:07:50 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.4147
2022-08-22 15:08:23 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.4598
2022-08-22 15:08:57 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.3478
2022-08-22 15:09:30 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.2364
2022-08-22 15:10:04 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.4992
2022-08-22 15:10:38 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.3095
2022-08-22 15:11:12 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.5303
2022-08-22 15:11:46 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.3106
2022-08-22 15:12:20 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.2664
2022-08-22 15:12:54 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.4360
2022-08-22 15:13:28 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.1873
2022-08-22 15:14:01 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.3890
2022-08-22 15:14:35 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.4388
2022-08-22 15:15:09 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.2707
2022-08-22 15:15:43 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.3806
2022-08-22 15:16:17 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.6102
2022-08-22 15:16:50 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.4932
2022-08-22 15:17:24 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.5289
2022-08-22 15:17:57 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.4062
2022-08-22 15:18:31 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.5746
2022-08-22 15:19:05 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.2855
2022-08-22 15:19:38 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.1942
2022-08-22 15:20:12 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.2962
2022-08-22 15:20:46 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.4154
2022-08-22 15:21:20 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3814
2022-08-22 15:21:54 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2520
2022-08-22 15:22:27 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.2318
2022-08-22 15:23:01 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.3098
2022-08-22 15:23:35 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.2013
2022-08-22 15:24:09 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.4239
2022-08-22 15:24:43 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.4596
2022-08-22 15:25:16 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.4595
2022-08-22 15:25:50 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.3365
2022-08-22 15:26:24 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.2813
2022-08-22 15:26:57 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.2923
2022-08-22 15:27:30 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.6916
2022-08-22 15:27:32 - train: epoch 023, train_loss: 1.3928
2022-08-22 15:28:47 - eval: epoch: 023, acc1: 70.626%, acc5: 89.924%, test_loss: 1.1904, per_image_load_time: 1.997ms, per_image_inference_time: 0.613ms
2022-08-22 15:28:48 - until epoch: 023, best_acc1: 70.626%
2022-08-22 15:28:48 - epoch 024 lr: 0.001571
2022-08-22 15:29:29 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.4051
2022-08-22 15:30:02 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.5202
2022-08-22 15:30:36 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.3373
2022-08-22 15:31:10 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.3611
2022-08-22 15:31:43 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.3257
2022-08-22 15:32:16 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.3784
2022-08-22 15:32:50 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.3887
2022-08-22 15:33:23 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.3829
2022-08-22 15:33:56 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.3356
2022-08-22 15:34:30 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.4061
2022-08-22 15:35:03 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.0336
2022-08-22 15:35:37 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.3114
2022-08-22 15:36:10 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.3502
2022-08-22 15:36:43 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.3328
2022-08-22 15:37:17 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.4981
2022-08-22 15:37:50 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.3883
2022-08-22 15:38:23 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.2399
2022-08-22 15:38:57 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.6204
2022-08-22 15:39:30 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.2515
2022-08-22 15:40:04 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.2856
2022-08-22 15:40:37 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.3252
2022-08-22 15:41:11 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.3632
2022-08-22 15:41:44 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.3697
2022-08-22 15:42:18 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.3547
2022-08-22 15:42:51 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.4927
2022-08-22 15:43:25 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.6038
2022-08-22 15:43:58 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.3113
2022-08-22 15:44:32 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.3590
2022-08-22 15:45:05 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.3035
2022-08-22 15:45:39 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.3932
2022-08-22 15:46:13 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.2126
2022-08-22 15:46:46 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.5343
2022-08-22 15:47:20 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.2890
2022-08-22 15:47:53 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.2406
2022-08-22 15:48:27 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.3356
2022-08-22 15:49:01 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.3894
2022-08-22 15:49:35 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.3687
2022-08-22 15:50:09 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.4614
2022-08-22 15:50:43 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.2924
2022-08-22 15:51:17 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.3202
2022-08-22 15:51:50 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.2828
2022-08-22 15:52:24 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.2684
2022-08-22 15:52:58 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.3774
2022-08-22 15:53:31 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.2546
2022-08-22 15:54:05 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.2791
2022-08-22 15:54:39 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.3639
2022-08-22 15:55:12 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.3001
2022-08-22 15:55:46 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.0212
2022-08-22 15:56:20 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.5854
2022-08-22 15:56:53 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.2359
2022-08-22 15:56:54 - train: epoch 024, train_loss: 1.3558
2022-08-22 15:58:11 - eval: epoch: 024, acc1: 71.050%, acc5: 90.062%, test_loss: 1.1751, per_image_load_time: 2.378ms, per_image_inference_time: 0.589ms
2022-08-22 15:58:11 - until epoch: 024, best_acc1: 71.050%
2022-08-22 15:58:11 - epoch 025 lr: 0.000394
2022-08-22 15:58:52 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.2385
2022-08-22 15:59:26 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.2073
2022-08-22 15:59:59 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.3461
2022-08-22 16:00:33 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.3610
2022-08-22 16:01:06 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.3094
2022-08-22 16:01:40 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3594
2022-08-22 16:02:14 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.2779
2022-08-22 16:02:47 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.1986
2022-08-22 16:03:21 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.1060
2022-08-22 16:03:54 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3623
2022-08-22 16:04:28 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.2208
2022-08-22 16:05:02 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.5778
2022-08-22 16:05:35 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3531
2022-08-22 16:06:09 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.3187
2022-08-22 16:06:42 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.4091
2022-08-22 16:07:16 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.2753
2022-08-22 16:07:50 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.5525
2022-08-22 16:08:23 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.0394
2022-08-22 16:08:57 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.2320
2022-08-22 16:09:31 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.3476
2022-08-22 16:10:05 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.0705
2022-08-22 16:10:39 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.2840
2022-08-22 16:11:12 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.3047
2022-08-22 16:11:46 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.1407
2022-08-22 16:12:20 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.2641
2022-08-22 16:12:54 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.4062
2022-08-22 16:13:27 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.3950
2022-08-22 16:14:01 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.3143
2022-08-22 16:14:34 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.2487
2022-08-22 16:15:08 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.3923
2022-08-22 16:15:41 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.3524
2022-08-22 16:16:15 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.3637
2022-08-22 16:16:48 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.1726
2022-08-22 16:17:22 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.4747
2022-08-22 16:17:55 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1464
2022-08-22 16:18:29 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.4459
2022-08-22 16:19:02 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.4541
2022-08-22 16:19:36 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.5624
2022-08-22 16:20:09 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.4609
2022-08-22 16:20:43 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.3624
2022-08-22 16:21:16 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.5027
2022-08-22 16:21:50 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.4363
2022-08-22 16:22:23 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.1285
2022-08-22 16:22:56 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.1977
2022-08-22 16:23:30 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.2663
2022-08-22 16:24:03 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.2828
2022-08-22 16:24:37 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.1105
2022-08-22 16:25:11 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.2316
2022-08-22 16:25:44 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.4054
2022-08-22 16:26:18 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.3970
2022-08-22 16:26:19 - train: epoch 025, train_loss: 1.3364
2022-08-22 16:27:36 - eval: epoch: 025, acc1: 71.042%, acc5: 90.154%, test_loss: 1.1735, per_image_load_time: 2.360ms, per_image_inference_time: 0.609ms
2022-08-22 16:27:36 - until epoch: 025, best_acc1: 71.050%
2022-08-22 16:27:36 - train done. train time: 12.251 hours, best_acc1: 71.050%
