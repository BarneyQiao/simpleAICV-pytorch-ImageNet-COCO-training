2022-08-21 03:24:18 - net_idx: 12
2022-08-21 03:24:18 - net_config: {'stem_width': 64, 'depth': 12, 'w_0': 32, 'w_a': 20.806982986031304, 'w_m': 2.125204201916835}
2022-08-21 03:24:18 - num_classes: 1000
2022-08-21 03:24:18 - input_image_size: 224
2022-08-21 03:24:18 - scale: 1.1428571428571428
2022-08-21 03:24:18 - seed: 0
2022-08-21 03:24:18 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-21 03:24:18 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-21 03:24:18 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c2100>
2022-08-21 03:24:18 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c23d0>
2022-08-21 03:24:18 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2400>
2022-08-21 03:24:18 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2460>
2022-08-21 03:24:18 - batch_size: 256
2022-08-21 03:24:18 - num_workers: 16
2022-08-21 03:24:18 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-21 03:24:18 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-21 03:24:18 - epochs: 25
2022-08-21 03:24:18 - print_interval: 100
2022-08-21 03:24:18 - accumulation_steps: 1
2022-08-21 03:24:18 - sync_bn: False
2022-08-21 03:24:18 - apex: True
2022-08-21 03:24:18 - use_ema_model: False
2022-08-21 03:24:18 - ema_model_decay: 0.9999
2022-08-21 03:24:18 - log_dir: ./log
2022-08-21 03:24:18 - checkpoint_dir: ./checkpoints
2022-08-21 03:24:18 - gpus_type: NVIDIA RTX A5000
2022-08-21 03:24:18 - gpus_num: 2
2022-08-21 03:24:18 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f4efca864b0>
2022-08-21 03:24:18 - ema_model: None
2022-08-21 03:24:18 - --------------------parameters--------------------
2022-08-21 03:24:18 - name: conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-21 03:24:18 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-21 03:24:18 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-21 03:24:18 - name: fc.weight, grad: True
2022-08-21 03:24:18 - name: fc.bias, grad: True
2022-08-21 03:24:18 - --------------------buffers--------------------
2022-08-21 03:24:18 - name: conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-21 03:24:18 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-21 03:24:18 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 03:24:18 - -----------no weight decay layers--------------
2022-08-21 03:24:18 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 03:24:18 - -------------weight decay layers---------------
2022-08-21 03:24:18 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 03:24:18 - epoch 001 lr: 0.100000
2022-08-21 03:24:59 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8919
2022-08-21 03:25:32 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9114
2022-08-21 03:26:06 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8170
2022-08-21 03:26:39 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.7543
2022-08-21 03:27:13 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.6898
2022-08-21 03:27:47 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.4692
2022-08-21 03:28:21 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.6192
2022-08-21 03:28:55 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.4739
2022-08-21 03:29:29 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.3441
2022-08-21 03:30:02 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.2813
2022-08-21 03:30:36 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.2385
2022-08-21 03:31:10 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.0445
2022-08-21 03:31:44 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.0422
2022-08-21 03:32:18 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 5.8559
2022-08-21 03:32:52 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 5.8947
2022-08-21 03:33:26 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 5.9435
2022-08-21 03:34:00 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.6380
2022-08-21 03:34:34 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.6274
2022-08-21 03:35:08 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.5586
2022-08-21 03:35:42 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.3855
2022-08-21 03:36:16 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.4076
2022-08-21 03:36:49 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.4246
2022-08-21 03:37:24 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.2493
2022-08-21 03:37:57 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.2655
2022-08-21 03:38:31 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.2332
2022-08-21 03:39:05 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.3416
2022-08-21 03:39:39 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.0986
2022-08-21 03:40:13 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.0336
2022-08-21 03:40:47 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 4.9050
2022-08-21 03:41:20 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 4.9875
2022-08-21 03:41:54 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.0018
2022-08-21 03:42:28 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 4.9322
2022-08-21 03:43:02 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.8544
2022-08-21 03:43:36 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.8102
2022-08-21 03:44:10 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.6939
2022-08-21 03:44:44 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.6627
2022-08-21 03:45:19 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.8115
2022-08-21 03:45:52 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.5758
2022-08-21 03:46:26 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.5704
2022-08-21 03:47:00 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.6055
2022-08-21 03:47:34 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.6392
2022-08-21 03:48:08 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.4278
2022-08-21 03:48:43 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.5900
2022-08-21 03:49:17 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.2495
2022-08-21 03:49:51 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.2671
2022-08-21 03:50:26 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.5566
2022-08-21 03:51:00 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.3220
2022-08-21 03:51:34 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.3329
2022-08-21 03:52:08 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.4204
2022-08-21 03:52:41 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.1602
2022-08-21 03:52:43 - train: epoch 001, train_loss: 5.3381
2022-08-21 03:54:01 - eval: epoch: 001, acc1: 15.168%, acc5: 35.432%, test_loss: 4.7757, per_image_load_time: 1.971ms, per_image_inference_time: 0.628ms
2022-08-21 03:54:01 - until epoch: 001, best_acc1: 15.168%
2022-08-21 03:54:01 - epoch 002 lr: 0.099606
2022-08-21 03:54:42 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.0791
2022-08-21 03:55:15 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.0420
2022-08-21 03:55:49 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.1212
2022-08-21 03:56:22 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.2135
2022-08-21 03:56:56 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 3.9002
2022-08-21 03:57:30 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 3.9542
2022-08-21 03:58:04 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.1167
2022-08-21 03:58:38 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 3.8929
2022-08-21 03:59:12 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.7926
2022-08-21 03:59:45 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.0183
2022-08-21 04:00:19 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 3.9755
2022-08-21 04:00:53 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.9113
2022-08-21 04:01:27 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.8878
2022-08-21 04:02:01 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.0188
2022-08-21 04:02:35 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 3.8793
2022-08-21 04:03:08 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.7938
2022-08-21 04:03:42 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.8421
2022-08-21 04:04:16 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.6877
2022-08-21 04:04:50 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.6535
2022-08-21 04:05:24 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.3437
2022-08-21 04:05:58 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.7579
2022-08-21 04:06:32 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.5475
2022-08-21 04:07:06 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.6952
2022-08-21 04:07:40 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.6079
2022-08-21 04:08:14 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.4790
2022-08-21 04:08:47 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.5274
2022-08-21 04:09:22 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.6822
2022-08-21 04:09:55 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.6361
2022-08-21 04:10:30 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.6529
2022-08-21 04:11:03 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.5365
2022-08-21 04:11:37 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.4750
2022-08-21 04:12:11 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.6311
2022-08-21 04:12:45 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.5129
2022-08-21 04:13:19 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.4987
2022-08-21 04:13:53 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.3050
2022-08-21 04:14:27 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.4792
2022-08-21 04:15:01 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.5845
2022-08-21 04:15:35 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.2585
2022-08-21 04:16:10 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.4194
2022-08-21 04:16:44 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.3414
2022-08-21 04:17:18 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.4309
2022-08-21 04:17:52 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.3693
2022-08-21 04:18:26 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.3447
2022-08-21 04:19:00 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.2096
2022-08-21 04:19:34 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.1925
2022-08-21 04:20:08 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.1688
2022-08-21 04:20:42 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.2524
2022-08-21 04:21:16 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.4437
2022-08-21 04:21:50 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.2909
2022-08-21 04:22:23 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.3793
2022-08-21 04:22:25 - train: epoch 002, train_loss: 3.6538
2022-08-21 04:23:43 - eval: epoch: 002, acc1: 30.470%, acc5: 56.604%, test_loss: 3.3639, per_image_load_time: 2.410ms, per_image_inference_time: 0.624ms
2022-08-21 04:23:43 - until epoch: 002, best_acc1: 30.470%
2022-08-21 04:23:43 - epoch 003 lr: 0.098429
2022-08-21 04:24:25 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.3145
2022-08-21 04:24:59 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.2179
2022-08-21 04:25:32 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.2355
2022-08-21 04:26:06 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.2059
2022-08-21 04:26:39 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.3507
2022-08-21 04:27:13 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.1560
2022-08-21 04:27:47 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.4128
2022-08-21 04:28:21 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.5144
2022-08-21 04:28:55 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.0797
2022-08-21 04:29:29 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.3055
2022-08-21 04:30:03 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 2.9643
2022-08-21 04:30:37 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.2736
2022-08-21 04:31:11 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.0959
2022-08-21 04:31:45 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.0976
2022-08-21 04:32:19 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.3070
2022-08-21 04:32:53 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.0149
2022-08-21 04:33:27 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.0286
2022-08-21 04:34:01 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.1003
2022-08-21 04:34:35 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.0470
2022-08-21 04:35:09 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 2.9627
2022-08-21 04:35:43 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.1908
2022-08-21 04:36:17 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.3799
2022-08-21 04:36:50 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.9247
2022-08-21 04:37:24 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 2.9320
2022-08-21 04:37:58 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.2113
2022-08-21 04:38:32 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.0178
2022-08-21 04:39:06 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.2639
2022-08-21 04:39:40 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 2.8382
2022-08-21 04:40:13 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.9266
2022-08-21 04:40:48 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.1830
2022-08-21 04:41:21 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.2580
2022-08-21 04:41:56 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.0005
2022-08-21 04:42:30 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0008
2022-08-21 04:43:04 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.0114
2022-08-21 04:43:39 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 3.0453
2022-08-21 04:44:12 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.8444
2022-08-21 04:44:46 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.0122
2022-08-21 04:45:21 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.0399
2022-08-21 04:45:55 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.2527
2022-08-21 04:46:29 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.8298
2022-08-21 04:47:03 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.8672
2022-08-21 04:47:37 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 2.8829
2022-08-21 04:48:11 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 3.0115
2022-08-21 04:48:45 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.9130
2022-08-21 04:49:20 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 2.8708
2022-08-21 04:49:54 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.7464
2022-08-21 04:50:28 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 2.8757
2022-08-21 04:51:02 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.0898
2022-08-21 04:51:36 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 2.9867
2022-08-21 04:52:09 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 3.0479
2022-08-21 04:52:11 - train: epoch 003, train_loss: 3.0859
2022-08-21 04:53:28 - eval: epoch: 003, acc1: 37.384%, acc5: 64.220%, test_loss: 2.8944, per_image_load_time: 2.343ms, per_image_inference_time: 0.595ms
2022-08-21 04:53:28 - until epoch: 003, best_acc1: 37.384%
2022-08-21 04:53:28 - epoch 004 lr: 0.096488
2022-08-21 04:54:10 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.9974
2022-08-21 04:54:43 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.7876
2022-08-21 04:55:16 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.8453
2022-08-21 04:55:49 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.8239
2022-08-21 04:56:23 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.5861
2022-08-21 04:56:57 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.1975
2022-08-21 04:57:31 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.9157
2022-08-21 04:58:05 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.7878
2022-08-21 04:58:39 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.6940
2022-08-21 04:59:13 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.9617
2022-08-21 04:59:48 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 2.9693
2022-08-21 05:00:22 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.7645
2022-08-21 05:00:56 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.8285
2022-08-21 05:01:30 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.7341
2022-08-21 05:02:03 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 3.0708
2022-08-21 05:02:37 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.8809
2022-08-21 05:03:11 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.9422
2022-08-21 05:03:45 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.9353
2022-08-21 05:04:19 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.7971
2022-08-21 05:04:53 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.8858
2022-08-21 05:05:26 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.8513
2022-08-21 05:06:00 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.8637
2022-08-21 05:06:34 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.3486
2022-08-21 05:07:08 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.5114
2022-08-21 05:07:41 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.7410
2022-08-21 05:08:15 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.6653
2022-08-21 05:08:48 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.6148
2022-08-21 05:09:22 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8454
2022-08-21 05:09:56 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.7845
2022-08-21 05:10:30 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.9076
2022-08-21 05:11:04 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.5998
2022-08-21 05:11:38 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8351
2022-08-21 05:12:12 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8846
2022-08-21 05:12:45 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.8999
2022-08-21 05:13:19 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.7548
2022-08-21 05:13:53 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.6595
2022-08-21 05:14:27 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.7967
2022-08-21 05:15:01 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.5106
2022-08-21 05:15:35 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.8745
2022-08-21 05:16:08 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.6314
2022-08-21 05:16:42 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.5470
2022-08-21 05:17:17 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.5330
2022-08-21 05:17:51 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.5168
2022-08-21 05:18:25 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.6300
2022-08-21 05:18:59 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.1211
2022-08-21 05:19:33 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.7470
2022-08-21 05:20:07 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.7280
2022-08-21 05:20:40 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.5150
2022-08-21 05:21:14 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.7795
2022-08-21 05:21:48 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.7445
2022-08-21 05:21:49 - train: epoch 004, train_loss: 2.7994
2022-08-21 05:23:07 - eval: epoch: 004, acc1: 44.732%, acc5: 70.900%, test_loss: 2.4802, per_image_load_time: 2.115ms, per_image_inference_time: 0.650ms
2022-08-21 05:23:07 - until epoch: 004, best_acc1: 44.732%
2022-08-21 05:23:07 - epoch 005 lr: 0.093815
2022-08-21 05:23:48 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.7311
2022-08-21 05:24:21 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.8572
2022-08-21 05:24:54 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.8392
2022-08-21 05:25:27 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.6608
2022-08-21 05:26:01 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.3892
2022-08-21 05:26:34 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.7304
2022-08-21 05:27:08 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.7356
2022-08-21 05:27:41 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.7604
2022-08-21 05:28:15 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.6949
2022-08-21 05:28:49 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.7202
2022-08-21 05:29:22 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.7671
2022-08-21 05:29:56 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.8660
2022-08-21 05:30:29 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.4897
2022-08-21 05:31:03 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.7586
2022-08-21 05:31:37 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.3872
2022-08-21 05:32:11 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.4189
2022-08-21 05:32:45 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.5410
2022-08-21 05:33:19 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.5451
2022-08-21 05:33:53 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.7732
2022-08-21 05:34:28 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.4757
2022-08-21 05:35:02 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.4420
2022-08-21 05:35:36 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.4584
2022-08-21 05:36:10 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.3170
2022-08-21 05:36:44 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5361
2022-08-21 05:37:18 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.5064
2022-08-21 05:37:52 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.6703
2022-08-21 05:38:26 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.6758
2022-08-21 05:39:00 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.7003
2022-08-21 05:39:34 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.3833
2022-08-21 05:40:08 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.5877
2022-08-21 05:40:42 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.7394
2022-08-21 05:41:16 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.5627
2022-08-21 05:41:50 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.5105
2022-08-21 05:42:24 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.6476
2022-08-21 05:42:58 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.6048
2022-08-21 05:43:32 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.5509
2022-08-21 05:44:07 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.4700
2022-08-21 05:44:40 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5431
2022-08-21 05:45:15 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.6876
2022-08-21 05:45:49 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.7461
2022-08-21 05:46:23 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.6962
2022-08-21 05:46:57 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.7470
2022-08-21 05:47:31 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.5916
2022-08-21 05:48:05 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.6829
2022-08-21 05:48:39 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.7734
2022-08-21 05:49:13 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.4643
2022-08-21 05:49:47 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.3608
2022-08-21 05:50:21 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.4637
2022-08-21 05:50:55 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.8110
2022-08-21 05:51:28 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.5457
2022-08-21 05:51:30 - train: epoch 005, train_loss: 2.6255
2022-08-21 05:52:48 - eval: epoch: 005, acc1: 47.914%, acc5: 74.392%, test_loss: 2.2826, per_image_load_time: 2.277ms, per_image_inference_time: 0.624ms
2022-08-21 05:52:48 - until epoch: 005, best_acc1: 47.914%
2022-08-21 05:52:48 - epoch 006 lr: 0.090450
2022-08-21 05:53:30 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.4843
2022-08-21 05:54:03 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.5416
2022-08-21 05:54:36 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.4345
2022-08-21 05:55:10 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.6143
2022-08-21 05:55:43 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.1983
2022-08-21 05:56:17 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.5770
2022-08-21 05:56:50 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.4725
2022-08-21 05:57:24 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.4078
2022-08-21 05:57:58 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.4504
2022-08-21 05:58:31 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.4896
2022-08-21 05:59:05 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.5823
2022-08-21 05:59:38 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.5187
2022-08-21 06:00:12 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.6540
2022-08-21 06:00:46 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.7586
2022-08-21 06:01:20 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.8186
2022-08-21 06:01:54 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3920
2022-08-21 06:02:27 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.6494
2022-08-21 06:03:01 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.4160
2022-08-21 06:03:35 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.3818
2022-08-21 06:04:09 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.6860
2022-08-21 06:04:43 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.2568
2022-08-21 06:05:17 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.3068
2022-08-21 06:05:50 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.3332
2022-08-21 06:06:24 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.2788
2022-08-21 06:06:58 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4085
2022-08-21 06:07:32 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.3888
2022-08-21 06:08:06 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.4012
2022-08-21 06:08:41 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.0763
2022-08-21 06:09:14 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.6537
2022-08-21 06:09:48 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.5550
2022-08-21 06:10:22 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.2535
2022-08-21 06:10:56 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.4924
2022-08-21 06:11:30 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.5150
2022-08-21 06:12:04 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.6804
2022-08-21 06:12:38 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.4465
2022-08-21 06:13:11 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.6428
2022-08-21 06:13:45 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.5614
2022-08-21 06:14:19 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.0684
2022-08-21 06:14:53 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.4488
2022-08-21 06:15:27 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.5684
2022-08-21 06:16:01 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.3931
2022-08-21 06:16:35 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.3360
2022-08-21 06:17:09 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.3608
2022-08-21 06:17:43 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.4679
2022-08-21 06:18:17 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.3839
2022-08-21 06:18:51 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.5831
2022-08-21 06:19:26 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4465
2022-08-21 06:20:00 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.5348
2022-08-21 06:20:34 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.4787
2022-08-21 06:21:07 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.4105
2022-08-21 06:21:08 - train: epoch 006, train_loss: 2.5041
2022-08-21 06:22:25 - eval: epoch: 006, acc1: 49.102%, acc5: 75.100%, test_loss: 2.2263, per_image_load_time: 1.920ms, per_image_inference_time: 0.646ms
2022-08-21 06:22:25 - until epoch: 006, best_acc1: 49.102%
2022-08-21 06:22:25 - epoch 007 lr: 0.086448
2022-08-21 06:23:07 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.3000
2022-08-21 06:23:40 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.5930
2022-08-21 06:24:14 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.7112
2022-08-21 06:24:48 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.4053
2022-08-21 06:25:21 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.2646
2022-08-21 06:25:54 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.3366
2022-08-21 06:26:28 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.3729
2022-08-21 06:27:02 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.4832
2022-08-21 06:27:35 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.5016
2022-08-21 06:28:09 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.3813
2022-08-21 06:28:43 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.3384
2022-08-21 06:29:17 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.4445
2022-08-21 06:29:51 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.1978
2022-08-21 06:30:25 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.3406
2022-08-21 06:30:58 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.4490
2022-08-21 06:31:32 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.4980
2022-08-21 06:32:06 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.3916
2022-08-21 06:32:40 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.4536
2022-08-21 06:33:14 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.5624
2022-08-21 06:33:48 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.2528
2022-08-21 06:34:22 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.4349
2022-08-21 06:34:55 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.4572
2022-08-21 06:35:29 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.6375
2022-08-21 06:36:03 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.7195
2022-08-21 06:36:37 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.3872
2022-08-21 06:37:10 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.2888
2022-08-21 06:37:44 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.3479
2022-08-21 06:38:18 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.2649
2022-08-21 06:38:52 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.0935
2022-08-21 06:39:26 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.5818
2022-08-21 06:40:00 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.2175
2022-08-21 06:40:35 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.3367
2022-08-21 06:41:09 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.6355
2022-08-21 06:41:43 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.4631
2022-08-21 06:42:17 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.4161
2022-08-21 06:42:51 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.1960
2022-08-21 06:43:25 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.2645
2022-08-21 06:43:59 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.4942
2022-08-21 06:44:33 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.4487
2022-08-21 06:45:06 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.4880
2022-08-21 06:45:40 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.2866
2022-08-21 06:46:15 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2337
2022-08-21 06:46:49 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.5350
2022-08-21 06:47:23 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.2516
2022-08-21 06:47:57 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.4754
2022-08-21 06:48:31 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.4893
2022-08-21 06:49:05 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.3452
2022-08-21 06:49:39 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.5351
2022-08-21 06:50:13 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.3947
2022-08-21 06:50:46 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.1594
2022-08-21 06:50:48 - train: epoch 007, train_loss: 2.4118
2022-08-21 06:52:06 - eval: epoch: 007, acc1: 50.038%, acc5: 75.412%, test_loss: 2.1797, per_image_load_time: 1.467ms, per_image_inference_time: 0.658ms
2022-08-21 06:52:06 - until epoch: 007, best_acc1: 50.038%
2022-08-21 06:52:06 - epoch 008 lr: 0.081870
2022-08-21 06:52:47 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.4498
2022-08-21 06:53:20 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.4190
2022-08-21 06:53:53 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.2369
2022-08-21 06:54:27 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.5435
2022-08-21 06:55:00 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.2374
2022-08-21 06:55:33 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.1300
2022-08-21 06:56:07 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.3859
2022-08-21 06:56:41 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.2611
2022-08-21 06:57:14 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.2006
2022-08-21 06:57:48 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.3492
2022-08-21 06:58:22 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.2379
2022-08-21 06:58:56 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.3056
2022-08-21 06:59:30 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.3374
2022-08-21 07:00:04 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.2797
2022-08-21 07:00:38 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.4776
2022-08-21 07:01:12 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.3675
2022-08-21 07:01:46 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.3357
2022-08-21 07:02:20 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.0967
2022-08-21 07:02:54 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.2683
2022-08-21 07:03:28 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.2116
2022-08-21 07:04:02 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.2830
2022-08-21 07:04:36 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.3542
2022-08-21 07:05:10 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.4453
2022-08-21 07:05:44 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.1589
2022-08-21 07:06:18 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.2551
2022-08-21 07:06:52 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.2434
2022-08-21 07:07:26 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.5742
2022-08-21 07:08:00 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.3687
2022-08-21 07:08:34 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.2823
2022-08-21 07:09:08 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.5944
2022-08-21 07:09:41 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.3072
2022-08-21 07:10:16 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.5622
2022-08-21 07:10:50 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.3681
2022-08-21 07:11:24 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.3423
2022-08-21 07:11:58 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.5664
2022-08-21 07:12:32 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.4911
2022-08-21 07:13:06 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.0265
2022-08-21 07:13:40 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.1019
2022-08-21 07:14:14 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.3546
2022-08-21 07:14:49 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.6282
2022-08-21 07:15:23 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.1122
2022-08-21 07:15:57 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.3456
2022-08-21 07:16:31 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.1114
2022-08-21 07:17:05 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.2974
2022-08-21 07:17:39 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.3486
2022-08-21 07:18:13 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.3758
2022-08-21 07:18:47 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2523
2022-08-21 07:19:21 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.3002
2022-08-21 07:19:55 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.4946
2022-08-21 07:20:29 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.2671
2022-08-21 07:20:30 - train: epoch 008, train_loss: 2.3320
2022-08-21 07:21:47 - eval: epoch: 008, acc1: 53.454%, acc5: 78.710%, test_loss: 1.9941, per_image_load_time: 2.264ms, per_image_inference_time: 0.655ms
2022-08-21 07:21:47 - until epoch: 008, best_acc1: 53.454%
2022-08-21 07:21:47 - epoch 009 lr: 0.076790
2022-08-21 07:22:28 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 2.0115
2022-08-21 07:23:01 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.2576
2022-08-21 07:23:33 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.0715
2022-08-21 07:24:06 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.5505
2022-08-21 07:24:39 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.2007
2022-08-21 07:25:13 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.2480
2022-08-21 07:25:46 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.1207
2022-08-21 07:26:20 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.3140
2022-08-21 07:26:53 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.1501
2022-08-21 07:27:27 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1104
2022-08-21 07:28:01 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.4030
2022-08-21 07:28:34 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.3177
2022-08-21 07:29:08 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.4066
2022-08-21 07:29:41 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 2.1500
2022-08-21 07:30:16 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.1241
2022-08-21 07:30:50 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2613
2022-08-21 07:31:23 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.4699
2022-08-21 07:31:57 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.3693
2022-08-21 07:32:31 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.1308
2022-08-21 07:33:05 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.2250
2022-08-21 07:33:39 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.2753
2022-08-21 07:34:13 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.2949
2022-08-21 07:34:47 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.1881
2022-08-21 07:35:21 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.1793
2022-08-21 07:35:55 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.2228
2022-08-21 07:36:29 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.3256
2022-08-21 07:37:03 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.0492
2022-08-21 07:37:37 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2929
2022-08-21 07:38:11 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.1059
2022-08-21 07:38:45 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.0894
2022-08-21 07:39:19 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.3277
2022-08-21 07:39:53 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.1872
2022-08-21 07:40:27 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.3675
2022-08-21 07:41:01 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.4748
2022-08-21 07:41:35 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.3089
2022-08-21 07:42:09 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.1047
2022-08-21 07:42:43 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.3393
2022-08-21 07:43:17 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.2988
2022-08-21 07:43:51 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.0527
2022-08-21 07:44:25 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.4313
2022-08-21 07:44:59 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.0598
2022-08-21 07:45:33 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.0733
2022-08-21 07:46:07 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 1.9802
2022-08-21 07:46:41 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.4059
2022-08-21 07:47:15 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.3404
2022-08-21 07:47:49 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.1782
2022-08-21 07:48:23 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.3727
2022-08-21 07:48:57 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.1380
2022-08-21 07:49:31 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.3303
2022-08-21 07:50:04 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.1324
2022-08-21 07:50:05 - train: epoch 009, train_loss: 2.2646
2022-08-21 07:51:22 - eval: epoch: 009, acc1: 53.592%, acc5: 78.714%, test_loss: 1.9858, per_image_load_time: 2.013ms, per_image_inference_time: 0.649ms
2022-08-21 07:51:22 - until epoch: 009, best_acc1: 53.592%
2022-08-21 07:51:22 - epoch 010 lr: 0.071288
2022-08-21 07:52:03 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.0198
2022-08-21 07:52:36 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.2158
2022-08-21 07:53:09 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.3640
2022-08-21 07:53:43 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.3263
2022-08-21 07:54:16 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.0355
2022-08-21 07:54:50 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.3758
2022-08-21 07:55:23 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.2170
2022-08-21 07:55:57 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.1665
2022-08-21 07:56:31 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.1841
2022-08-21 07:57:05 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.1142
2022-08-21 07:57:39 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.2556
2022-08-21 07:58:13 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.1300
2022-08-21 07:58:47 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.0799
2022-08-21 07:59:21 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.4799
2022-08-21 07:59:55 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.9190
2022-08-21 08:00:29 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 1.9767
2022-08-21 08:01:02 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.1696
2022-08-21 08:01:36 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 2.0806
2022-08-21 08:02:10 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.3258
2022-08-21 08:02:43 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.0744
2022-08-21 08:03:17 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 1.9170
2022-08-21 08:03:51 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.3315
2022-08-21 08:04:25 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.2348
2022-08-21 08:04:58 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.5116
2022-08-21 08:05:32 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.3161
2022-08-21 08:06:06 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.3038
2022-08-21 08:06:40 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.9435
2022-08-21 08:07:14 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.2554
2022-08-21 08:07:48 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.2075
2022-08-21 08:08:22 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1792
2022-08-21 08:08:56 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.2083
2022-08-21 08:09:30 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.1774
2022-08-21 08:10:04 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.4100
2022-08-21 08:10:38 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.3340
2022-08-21 08:11:12 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.3839
2022-08-21 08:11:46 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.2843
2022-08-21 08:12:20 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.2057
2022-08-21 08:12:54 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1081
2022-08-21 08:13:28 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 1.8167
2022-08-21 08:14:03 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.2636
2022-08-21 08:14:37 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.2443
2022-08-21 08:15:11 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.0633
2022-08-21 08:15:45 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.1768
2022-08-21 08:16:19 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.0297
2022-08-21 08:16:53 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.1445
2022-08-21 08:17:27 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.2098
2022-08-21 08:18:01 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1064
2022-08-21 08:18:35 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.0742
2022-08-21 08:19:10 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.2106
2022-08-21 08:19:42 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 1.9850
2022-08-21 08:19:44 - train: epoch 010, train_loss: 2.2025
2022-08-21 08:21:00 - eval: epoch: 010, acc1: 55.378%, acc5: 80.112%, test_loss: 1.9043, per_image_load_time: 0.605ms, per_image_inference_time: 0.642ms
2022-08-21 08:21:00 - until epoch: 010, best_acc1: 55.378%
2022-08-21 08:21:00 - epoch 011 lr: 0.065450
2022-08-21 08:21:40 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 1.9762
2022-08-21 08:22:13 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.1174
2022-08-21 08:22:46 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.0582
2022-08-21 08:23:19 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.2879
2022-08-21 08:23:52 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.1047
2022-08-21 08:24:26 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.2458
2022-08-21 08:24:59 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.0482
2022-08-21 08:25:33 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.2310
2022-08-21 08:26:06 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.1465
2022-08-21 08:26:40 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 1.9120
2022-08-21 08:27:14 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2117
2022-08-21 08:27:48 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2554
2022-08-21 08:28:22 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.3092
2022-08-21 08:28:55 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.2518
2022-08-21 08:29:29 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 1.9924
2022-08-21 08:30:03 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.2374
2022-08-21 08:30:37 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.1967
2022-08-21 08:31:11 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.0162
2022-08-21 08:31:45 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.0587
2022-08-21 08:32:19 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.0135
2022-08-21 08:32:53 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.1418
2022-08-21 08:33:27 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 1.9277
2022-08-21 08:34:01 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.3240
2022-08-21 08:34:35 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.9340
2022-08-21 08:35:09 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.3218
2022-08-21 08:35:43 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.0397
2022-08-21 08:36:17 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.1676
2022-08-21 08:36:52 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.8669
2022-08-21 08:37:26 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.2430
2022-08-21 08:38:00 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.4464
2022-08-21 08:38:34 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.0550
2022-08-21 08:39:08 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.3556
2022-08-21 08:39:42 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.0851
2022-08-21 08:40:16 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 1.9446
2022-08-21 08:40:50 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.0521
2022-08-21 08:41:25 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.0746
2022-08-21 08:41:59 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.3265
2022-08-21 08:42:33 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.8847
2022-08-21 08:43:07 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.1383
2022-08-21 08:43:41 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.0005
2022-08-21 08:44:15 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.9483
2022-08-21 08:44:50 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 1.9577
2022-08-21 08:45:24 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.1972
2022-08-21 08:45:58 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 1.9470
2022-08-21 08:46:32 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.0109
2022-08-21 08:47:06 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 2.1954
2022-08-21 08:47:40 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.9348
2022-08-21 08:48:14 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.9157
2022-08-21 08:48:49 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.1741
2022-08-21 08:49:22 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.1193
2022-08-21 08:49:24 - train: epoch 011, train_loss: 2.1418
2022-08-21 08:50:40 - eval: epoch: 011, acc1: 55.758%, acc5: 80.576%, test_loss: 1.8744, per_image_load_time: 0.985ms, per_image_inference_time: 0.655ms
2022-08-21 08:50:40 - until epoch: 011, best_acc1: 55.758%
2022-08-21 08:50:40 - epoch 012 lr: 0.059368
2022-08-21 08:51:20 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.9447
2022-08-21 08:51:53 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.7713
2022-08-21 08:52:26 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 1.9839
2022-08-21 08:53:00 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 1.9425
2022-08-21 08:53:34 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.1530
2022-08-21 08:54:07 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.9057
2022-08-21 08:54:41 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 2.0318
2022-08-21 08:55:15 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.1705
2022-08-21 08:55:48 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.2505
2022-08-21 08:56:22 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9473
2022-08-21 08:56:56 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.3628
2022-08-21 08:57:30 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.8761
2022-08-21 08:58:03 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.9553
2022-08-21 08:58:37 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.1302
2022-08-21 08:59:11 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 1.9304
2022-08-21 08:59:45 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 2.0568
2022-08-21 09:00:19 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.8684
2022-08-21 09:00:53 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.1565
2022-08-21 09:01:27 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1926
2022-08-21 09:02:01 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.2052
2022-08-21 09:02:35 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.1568
2022-08-21 09:03:09 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.0074
2022-08-21 09:03:42 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 2.1299
2022-08-21 09:04:16 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.1750
2022-08-21 09:04:50 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.9754
2022-08-21 09:05:25 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 2.0260
2022-08-21 09:05:59 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.0478
2022-08-21 09:06:33 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 2.1657
2022-08-21 09:07:07 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.0034
2022-08-21 09:07:41 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.9121
2022-08-21 09:08:15 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 1.9922
2022-08-21 09:08:49 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.9867
2022-08-21 09:09:23 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.0888
2022-08-21 09:09:57 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.0102
2022-08-21 09:10:32 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.0567
2022-08-21 09:11:05 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.9661
2022-08-21 09:11:39 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.1385
2022-08-21 09:12:13 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.1889
2022-08-21 09:12:47 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.7073
2022-08-21 09:13:21 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.3758
2022-08-21 09:13:55 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.1950
2022-08-21 09:14:29 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 2.0475
2022-08-21 09:15:03 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.1800
2022-08-21 09:15:37 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.9001
2022-08-21 09:16:10 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 2.0210
2022-08-21 09:16:44 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 1.8448
2022-08-21 09:17:18 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.9342
2022-08-21 09:17:52 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.1082
2022-08-21 09:18:26 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 2.0187
2022-08-21 09:18:59 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.8141
2022-08-21 09:19:01 - train: epoch 012, train_loss: 2.0818
2022-08-21 09:20:18 - eval: epoch: 012, acc1: 58.850%, acc5: 82.646%, test_loss: 1.7382, per_image_load_time: 1.155ms, per_image_inference_time: 0.632ms
2022-08-21 09:20:18 - until epoch: 012, best_acc1: 58.850%
2022-08-21 09:20:18 - epoch 013 lr: 0.053138
2022-08-21 09:20:58 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.9993
2022-08-21 09:21:31 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.9011
2022-08-21 09:22:04 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8239
2022-08-21 09:22:38 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.9024
2022-08-21 09:23:11 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 2.0373
2022-08-21 09:23:45 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 2.0632
2022-08-21 09:24:19 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.8848
2022-08-21 09:24:53 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.1721
2022-08-21 09:25:27 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 2.1367
2022-08-21 09:26:01 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 2.0854
2022-08-21 09:26:35 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.0426
2022-08-21 09:27:09 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.0540
2022-08-21 09:27:43 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 2.0288
2022-08-21 09:28:17 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.0730
2022-08-21 09:28:51 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.0564
2022-08-21 09:29:25 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.7815
2022-08-21 09:29:59 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.3352
2022-08-21 09:30:33 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 2.0800
2022-08-21 09:31:07 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.0323
2022-08-21 09:31:41 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.1019
2022-08-21 09:32:15 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.1309
2022-08-21 09:32:50 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 2.0957
2022-08-21 09:33:24 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 2.1358
2022-08-21 09:33:58 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.1164
2022-08-21 09:34:32 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 2.0464
2022-08-21 09:35:07 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 2.0731
2022-08-21 09:35:41 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 2.0619
2022-08-21 09:36:15 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.0754
2022-08-21 09:36:49 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.0397
2022-08-21 09:37:23 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.9275
2022-08-21 09:37:57 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.8559
2022-08-21 09:38:32 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 1.9588
2022-08-21 09:39:06 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.8159
2022-08-21 09:39:40 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 2.0585
2022-08-21 09:40:14 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.8827
2022-08-21 09:40:48 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.0800
2022-08-21 09:41:22 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.9441
2022-08-21 09:41:56 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.2334
2022-08-21 09:42:31 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.2323
2022-08-21 09:43:05 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.9653
2022-08-21 09:43:39 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.9249
2022-08-21 09:44:13 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 1.9727
2022-08-21 09:44:47 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 2.0643
2022-08-21 09:45:21 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.8912
2022-08-21 09:45:56 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.8578
2022-08-21 09:46:30 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.0888
2022-08-21 09:47:04 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 2.0149
2022-08-21 09:47:38 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.1442
2022-08-21 09:48:12 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0034
2022-08-21 09:48:45 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.2076
2022-08-21 09:48:47 - train: epoch 013, train_loss: 2.0243
2022-08-21 09:50:03 - eval: epoch: 013, acc1: 59.792%, acc5: 83.420%, test_loss: 1.6943, per_image_load_time: 0.618ms, per_image_inference_time: 0.654ms
2022-08-21 09:50:03 - until epoch: 013, best_acc1: 59.792%
2022-08-21 09:50:03 - epoch 014 lr: 0.046859
2022-08-21 09:50:43 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 2.1266
2022-08-21 09:51:16 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.9809
2022-08-21 09:51:49 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.8611
2022-08-21 09:52:23 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.8724
2022-08-21 09:52:57 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.7781
2022-08-21 09:53:31 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.8473
2022-08-21 09:54:05 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.8339
2022-08-21 09:54:38 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.8318
2022-08-21 09:55:12 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.8805
2022-08-21 09:55:45 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.1381
2022-08-21 09:56:19 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.8147
2022-08-21 09:56:53 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 2.0786
2022-08-21 09:57:26 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.1485
2022-08-21 09:58:00 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.9049
2022-08-21 09:58:34 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.1282
2022-08-21 09:59:08 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 2.0093
2022-08-21 09:59:42 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 2.1196
2022-08-21 10:00:16 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 1.9763
2022-08-21 10:00:49 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.7624
2022-08-21 10:01:23 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.8672
2022-08-21 10:01:56 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 1.9034
2022-08-21 10:02:30 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 2.0071
2022-08-21 10:03:04 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 2.0466
2022-08-21 10:03:38 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 2.0445
2022-08-21 10:04:12 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.8935
2022-08-21 10:04:46 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.7777
2022-08-21 10:05:20 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.9279
2022-08-21 10:05:54 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 2.0044
2022-08-21 10:06:28 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9220
2022-08-21 10:07:02 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.9194
2022-08-21 10:07:36 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.9683
2022-08-21 10:08:10 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 2.0212
2022-08-21 10:08:44 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.9036
2022-08-21 10:09:18 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 2.0306
2022-08-21 10:09:52 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.9988
2022-08-21 10:10:26 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.8002
2022-08-21 10:11:00 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.7897
2022-08-21 10:11:34 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 2.0267
2022-08-21 10:12:08 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.9636
2022-08-21 10:12:42 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.9894
2022-08-21 10:13:16 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.9273
2022-08-21 10:13:50 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.8432
2022-08-21 10:14:24 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.9814
2022-08-21 10:14:58 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.9278
2022-08-21 10:15:32 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.8290
2022-08-21 10:16:06 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.8023
2022-08-21 10:16:40 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.9542
2022-08-21 10:17:14 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.9467
2022-08-21 10:17:48 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.7445
2022-08-21 10:18:22 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.9732
2022-08-21 10:18:23 - train: epoch 014, train_loss: 1.9637
2022-08-21 10:19:39 - eval: epoch: 014, acc1: 59.828%, acc5: 83.304%, test_loss: 1.7050, per_image_load_time: 1.336ms, per_image_inference_time: 0.652ms
2022-08-21 10:19:39 - until epoch: 014, best_acc1: 59.828%
2022-08-21 10:19:39 - epoch 015 lr: 0.040630
2022-08-21 10:20:19 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.6439
2022-08-21 10:20:53 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.8918
2022-08-21 10:21:26 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.1005
2022-08-21 10:21:59 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.9134
2022-08-21 10:22:33 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.9415
2022-08-21 10:23:06 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 2.0093
2022-08-21 10:23:40 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 2.1182
2022-08-21 10:24:14 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.8436
2022-08-21 10:24:49 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.9553
2022-08-21 10:25:22 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.8965
2022-08-21 10:25:56 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.6889
2022-08-21 10:26:30 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 2.0174
2022-08-21 10:27:04 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 2.0783
2022-08-21 10:27:38 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.8605
2022-08-21 10:28:12 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.9051
2022-08-21 10:28:46 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.5372
2022-08-21 10:29:20 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.8740
2022-08-21 10:29:54 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.8654
2022-08-21 10:30:28 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.9209
2022-08-21 10:31:01 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.7780
2022-08-21 10:31:35 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.8158
2022-08-21 10:32:09 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9177
2022-08-21 10:32:43 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.8730
2022-08-21 10:33:17 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.8197
2022-08-21 10:33:51 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 2.2477
2022-08-21 10:34:25 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.8455
2022-08-21 10:34:59 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 2.0018
2022-08-21 10:35:33 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.9328
2022-08-21 10:36:07 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.7765
2022-08-21 10:36:41 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.8780
2022-08-21 10:37:15 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.9685
2022-08-21 10:37:49 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.8723
2022-08-21 10:38:23 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.5291
2022-08-21 10:38:57 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.7388
2022-08-21 10:39:31 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0515
2022-08-21 10:40:05 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.7517
2022-08-21 10:40:38 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6989
2022-08-21 10:41:13 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.9022
2022-08-21 10:41:47 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8355
2022-08-21 10:42:21 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.9140
2022-08-21 10:42:55 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.8445
2022-08-21 10:43:28 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.7595
2022-08-21 10:44:02 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.7603
2022-08-21 10:44:37 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.8290
2022-08-21 10:45:10 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.9146
2022-08-21 10:45:44 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.8184
2022-08-21 10:46:18 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.7515
2022-08-21 10:46:52 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.8858
2022-08-21 10:47:26 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.8122
2022-08-21 10:47:59 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.9696
2022-08-21 10:48:01 - train: epoch 015, train_loss: 1.8986
2022-08-21 10:49:17 - eval: epoch: 015, acc1: 62.468%, acc5: 85.062%, test_loss: 1.5649, per_image_load_time: 0.991ms, per_image_inference_time: 0.649ms
2022-08-21 10:49:18 - until epoch: 015, best_acc1: 62.468%
2022-08-21 10:49:18 - epoch 016 lr: 0.034548
2022-08-21 10:49:58 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.8131
2022-08-21 10:50:31 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.7438
2022-08-21 10:51:04 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.7922
2022-08-21 10:51:38 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.1177
2022-08-21 10:52:11 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.6634
2022-08-21 10:52:45 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.6858
2022-08-21 10:53:19 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.6494
2022-08-21 10:53:52 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.7204
2022-08-21 10:54:26 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.9392
2022-08-21 10:55:00 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.5859
2022-08-21 10:55:34 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.7373
2022-08-21 10:56:08 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.8327
2022-08-21 10:56:42 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.8458
2022-08-21 10:57:15 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.7074
2022-08-21 10:57:49 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.9859
2022-08-21 10:58:22 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.8536
2022-08-21 10:58:56 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.7724
2022-08-21 10:59:30 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 2.0489
2022-08-21 11:00:04 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.9222
2022-08-21 11:00:37 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.6610
2022-08-21 11:01:11 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.9123
2022-08-21 11:01:45 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.9169
2022-08-21 11:02:19 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.9899
2022-08-21 11:02:53 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.7757
2022-08-21 11:03:27 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.6387
2022-08-21 11:04:01 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.9681
2022-08-21 11:04:34 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.7839
2022-08-21 11:05:08 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.6982
2022-08-21 11:05:42 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.8454
2022-08-21 11:06:16 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 2.0061
2022-08-21 11:06:50 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.9574
2022-08-21 11:07:24 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.9659
2022-08-21 11:07:58 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9915
2022-08-21 11:08:32 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.7440
2022-08-21 11:09:06 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.8401
2022-08-21 11:09:40 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.6989
2022-08-21 11:10:13 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.9190
2022-08-21 11:10:48 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.1799
2022-08-21 11:11:21 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.8129
2022-08-21 11:11:55 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 2.0770
2022-08-21 11:12:29 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.8020
2022-08-21 11:13:03 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.7144
2022-08-21 11:13:37 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.7419
2022-08-21 11:14:11 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.7832
2022-08-21 11:14:45 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.9973
2022-08-21 11:15:19 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.6116
2022-08-21 11:15:53 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.9295
2022-08-21 11:16:27 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.8028
2022-08-21 11:17:01 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.9231
2022-08-21 11:17:34 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.8741
2022-08-21 11:17:35 - train: epoch 016, train_loss: 1.8368
2022-08-21 11:18:50 - eval: epoch: 016, acc1: 63.380%, acc5: 85.456%, test_loss: 1.5245, per_image_load_time: 0.834ms, per_image_inference_time: 0.641ms
2022-08-21 11:18:51 - until epoch: 016, best_acc1: 63.380%
2022-08-21 11:18:51 - epoch 017 lr: 0.028710
2022-08-21 11:19:30 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.7081
2022-08-21 11:20:04 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.8047
2022-08-21 11:20:37 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.9937
2022-08-21 11:21:10 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.6074
2022-08-21 11:21:44 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.8889
2022-08-21 11:22:18 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 2.1067
2022-08-21 11:22:52 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.7712
2022-08-21 11:23:25 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.8075
2022-08-21 11:23:59 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.7447
2022-08-21 11:24:33 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.8624
2022-08-21 11:25:06 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.9330
2022-08-21 11:25:40 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.6822
2022-08-21 11:26:14 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 2.0932
2022-08-21 11:26:47 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.7298
2022-08-21 11:27:21 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.8046
2022-08-21 11:27:55 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.6880
2022-08-21 11:28:29 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.7778
2022-08-21 11:29:03 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.6649
2022-08-21 11:29:37 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.6423
2022-08-21 11:30:11 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.7305
2022-08-21 11:30:45 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.7989
2022-08-21 11:31:18 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.5336
2022-08-21 11:31:53 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.8402
2022-08-21 11:32:27 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.7691
2022-08-21 11:33:01 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.9102
2022-08-21 11:33:35 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.6644
2022-08-21 11:34:08 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6676
2022-08-21 11:34:42 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.8692
2022-08-21 11:35:16 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.9530
2022-08-21 11:35:50 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.8062
2022-08-21 11:36:24 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8452
2022-08-21 11:36:58 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.6943
2022-08-21 11:37:31 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 2.1241
2022-08-21 11:38:05 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.7282
2022-08-21 11:38:39 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.9994
2022-08-21 11:39:13 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.8656
2022-08-21 11:39:47 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.7655
2022-08-21 11:40:21 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.8100
2022-08-21 11:40:55 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.5510
2022-08-21 11:41:29 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.7612
2022-08-21 11:42:03 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.9984
2022-08-21 11:42:37 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.7944
2022-08-21 11:43:11 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.7636
2022-08-21 11:43:46 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8801
2022-08-21 11:44:20 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.8020
2022-08-21 11:44:54 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.5489
2022-08-21 11:45:28 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.7652
2022-08-21 11:46:02 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.8298
2022-08-21 11:46:36 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.7533
2022-08-21 11:47:10 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.3919
2022-08-21 11:47:12 - train: epoch 017, train_loss: 1.7699
2022-08-21 11:48:28 - eval: epoch: 017, acc1: 64.832%, acc5: 86.418%, test_loss: 1.4638, per_image_load_time: 1.262ms, per_image_inference_time: 0.650ms
2022-08-21 11:48:28 - until epoch: 017, best_acc1: 64.832%
2022-08-21 11:48:28 - epoch 018 lr: 0.023208
2022-08-21 11:49:08 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.6234
2022-08-21 11:49:41 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.8611
2022-08-21 11:50:15 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.6737
2022-08-21 11:50:48 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.8164
2022-08-21 11:51:22 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.8505
2022-08-21 11:51:55 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.6024
2022-08-21 11:52:28 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.6252
2022-08-21 11:53:02 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.7877
2022-08-21 11:53:35 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.7924
2022-08-21 11:54:09 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.5575
2022-08-21 11:54:42 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.6606
2022-08-21 11:55:16 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.7988
2022-08-21 11:55:49 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.8933
2022-08-21 11:56:23 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.7502
2022-08-21 11:56:56 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.9980
2022-08-21 11:57:30 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.7491
2022-08-21 11:58:04 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.8644
2022-08-21 11:58:37 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.5852
2022-08-21 11:59:11 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.8092
2022-08-21 11:59:45 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.9051
2022-08-21 12:00:19 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.9543
2022-08-21 12:00:52 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.8846
2022-08-21 12:01:26 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6164
2022-08-21 12:02:00 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.6551
2022-08-21 12:02:34 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.4600
2022-08-21 12:03:08 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.6579
2022-08-21 12:03:42 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.7506
2022-08-21 12:04:16 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5342
2022-08-21 12:04:50 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.5994
2022-08-21 12:05:24 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.6817
2022-08-21 12:05:57 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 2.0207
2022-08-21 12:06:31 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.7767
2022-08-21 12:07:05 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.5869
2022-08-21 12:07:39 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.7763
2022-08-21 12:08:13 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.8274
2022-08-21 12:08:47 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.6328
2022-08-21 12:09:21 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.9180
2022-08-21 12:09:55 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.8977
2022-08-21 12:10:29 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.7427
2022-08-21 12:11:03 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.8140
2022-08-21 12:11:37 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7532
2022-08-21 12:12:11 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.6859
2022-08-21 12:12:45 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.5806
2022-08-21 12:13:19 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.6164
2022-08-21 12:13:53 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.4640
2022-08-21 12:14:27 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.7674
2022-08-21 12:15:01 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.9405
2022-08-21 12:15:35 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.7362
2022-08-21 12:16:09 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.6624
2022-08-21 12:16:42 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.8347
2022-08-21 12:16:43 - train: epoch 018, train_loss: 1.7001
2022-08-21 12:17:59 - eval: epoch: 018, acc1: 65.930%, acc5: 87.048%, test_loss: 1.4142, per_image_load_time: 1.316ms, per_image_inference_time: 0.646ms
2022-08-21 12:17:59 - until epoch: 018, best_acc1: 65.930%
2022-08-21 12:17:59 - epoch 019 lr: 0.018128
2022-08-21 12:18:40 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.5641
2022-08-21 12:19:13 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.6839
2022-08-21 12:19:47 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.6169
2022-08-21 12:20:20 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.4908
2022-08-21 12:20:54 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.4218
2022-08-21 12:21:27 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.7410
2022-08-21 12:22:01 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.4649
2022-08-21 12:22:35 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.6692
2022-08-21 12:23:09 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.6571
2022-08-21 12:23:43 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.5451
2022-08-21 12:24:17 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5769
2022-08-21 12:24:50 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.5836
2022-08-21 12:25:24 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.5943
2022-08-21 12:25:58 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.5111
2022-08-21 12:26:32 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.8500
2022-08-21 12:27:05 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.5239
2022-08-21 12:27:39 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.6995
2022-08-21 12:28:13 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.6363
2022-08-21 12:28:47 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.9679
2022-08-21 12:29:21 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.4782
2022-08-21 12:29:55 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.5187
2022-08-21 12:30:29 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.6979
2022-08-21 12:31:03 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.6103
2022-08-21 12:31:37 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.6006
2022-08-21 12:32:11 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.7707
2022-08-21 12:32:44 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.5449
2022-08-21 12:33:18 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.5454
2022-08-21 12:33:52 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.6209
2022-08-21 12:34:26 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.8841
2022-08-21 12:35:00 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.8079
2022-08-21 12:35:34 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.5976
2022-08-21 12:36:08 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.4106
2022-08-21 12:36:42 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.5112
2022-08-21 12:37:16 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.6543
2022-08-21 12:37:50 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.5071
2022-08-21 12:38:23 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.4809
2022-08-21 12:38:58 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.5180
2022-08-21 12:39:32 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.9024
2022-08-21 12:40:06 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.5552
2022-08-21 12:40:40 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.6358
2022-08-21 12:41:14 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.7444
2022-08-21 12:41:48 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.4460
2022-08-21 12:42:22 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.4908
2022-08-21 12:42:57 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.5886
2022-08-21 12:43:31 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.8225
2022-08-21 12:44:05 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.5704
2022-08-21 12:44:39 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.5219
2022-08-21 12:45:13 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.5194
2022-08-21 12:45:47 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.5212
2022-08-21 12:46:20 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.8071
2022-08-21 12:46:22 - train: epoch 019, train_loss: 1.6270
2022-08-21 12:47:38 - eval: epoch: 019, acc1: 66.944%, acc5: 87.746%, test_loss: 1.3662, per_image_load_time: 0.549ms, per_image_inference_time: 0.643ms
2022-08-21 12:47:38 - until epoch: 019, best_acc1: 66.944%
2022-08-21 12:47:38 - epoch 020 lr: 0.013551
2022-08-21 12:48:19 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.7362
2022-08-21 12:48:52 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.3950
2022-08-21 12:49:26 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.5730
2022-08-21 12:49:59 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.6078
2022-08-21 12:50:33 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.4543
2022-08-21 12:51:06 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.5019
2022-08-21 12:51:40 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.2622
2022-08-21 12:52:13 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.7672
2022-08-21 12:52:47 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.6623
2022-08-21 12:53:21 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.6811
2022-08-21 12:53:55 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.5494
2022-08-21 12:54:29 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.4309
2022-08-21 12:55:03 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.5451
2022-08-21 12:55:37 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.5903
2022-08-21 12:56:11 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.6225
2022-08-21 12:56:45 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.7655
2022-08-21 12:57:19 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.6699
2022-08-21 12:57:53 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.7002
2022-08-21 12:58:26 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.3512
2022-08-21 12:59:00 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.4428
2022-08-21 12:59:34 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.7522
2022-08-21 13:00:08 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.4376
2022-08-21 13:00:42 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.5710
2022-08-21 13:01:16 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.8901
2022-08-21 13:01:50 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.3839
2022-08-21 13:02:24 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.5623
2022-08-21 13:02:58 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.4420
2022-08-21 13:03:32 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.5950
2022-08-21 13:04:05 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.8699
2022-08-21 13:04:39 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.8702
2022-08-21 13:05:13 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.7907
2022-08-21 13:05:47 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.7250
2022-08-21 13:06:21 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3654
2022-08-21 13:06:55 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.5598
2022-08-21 13:07:29 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.4001
2022-08-21 13:08:03 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.5095
2022-08-21 13:08:37 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.3951
2022-08-21 13:09:11 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.4815
2022-08-21 13:09:45 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.6234
2022-08-21 13:10:19 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.3768
2022-08-21 13:10:53 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.4631
2022-08-21 13:11:27 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.5793
2022-08-21 13:12:01 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.6395
2022-08-21 13:12:35 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.3628
2022-08-21 13:13:09 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.5730
2022-08-21 13:13:43 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.6241
2022-08-21 13:14:17 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.5118
2022-08-21 13:14:51 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.5273
2022-08-21 13:15:25 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.4099
2022-08-21 13:15:59 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.5138
2022-08-21 13:16:00 - train: epoch 020, train_loss: 1.5566
2022-08-21 13:17:16 - eval: epoch: 020, acc1: 68.480%, acc5: 88.704%, test_loss: 1.2930, per_image_load_time: 1.202ms, per_image_inference_time: 0.663ms
2022-08-21 13:17:16 - until epoch: 020, best_acc1: 68.480%
2022-08-21 13:17:16 - epoch 021 lr: 0.009548
2022-08-21 13:17:56 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.4853
2022-08-21 13:18:29 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.5485
2022-08-21 13:19:03 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.3591
2022-08-21 13:19:36 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4659
2022-08-21 13:20:10 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.3315
2022-08-21 13:20:43 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.4316
2022-08-21 13:21:17 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.4021
2022-08-21 13:21:51 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.4027
2022-08-21 13:22:25 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.5078
2022-08-21 13:22:58 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3087
2022-08-21 13:23:32 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.4311
2022-08-21 13:24:06 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.4832
2022-08-21 13:24:40 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.3492
2022-08-21 13:25:14 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.3208
2022-08-21 13:25:48 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.3760
2022-08-21 13:26:22 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3436
2022-08-21 13:26:55 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5750
2022-08-21 13:27:29 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.3115
2022-08-21 13:28:04 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.5191
2022-08-21 13:28:38 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.5626
2022-08-21 13:29:11 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3635
2022-08-21 13:29:45 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.4768
2022-08-21 13:30:19 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.4929
2022-08-21 13:30:53 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.5069
2022-08-21 13:31:27 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.5466
2022-08-21 13:32:01 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.4258
2022-08-21 13:32:35 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.4059
2022-08-21 13:33:09 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.4714
2022-08-21 13:33:43 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.4080
2022-08-21 13:34:17 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.6294
2022-08-21 13:34:51 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.4579
2022-08-21 13:35:25 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.3504
2022-08-21 13:35:59 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.6181
2022-08-21 13:36:33 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.6699
2022-08-21 13:37:06 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.5966
2022-08-21 13:37:40 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.3023
2022-08-21 13:38:15 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.3044
2022-08-21 13:38:49 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.3780
2022-08-21 13:39:23 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.4358
2022-08-21 13:39:57 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.7456
2022-08-21 13:40:31 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.2983
2022-08-21 13:41:05 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.4676
2022-08-21 13:41:39 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.4113
2022-08-21 13:42:13 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.4975
2022-08-21 13:42:47 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.4665
2022-08-21 13:43:21 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.4382
2022-08-21 13:43:55 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.6711
2022-08-21 13:44:29 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.5332
2022-08-21 13:45:03 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.2158
2022-08-21 13:45:36 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.4325
2022-08-21 13:45:38 - train: epoch 021, train_loss: 1.4828
2022-08-21 13:46:54 - eval: epoch: 021, acc1: 69.530%, acc5: 89.300%, test_loss: 1.2457, per_image_load_time: 0.689ms, per_image_inference_time: 0.647ms
2022-08-21 13:46:54 - until epoch: 021, best_acc1: 69.530%
2022-08-21 13:46:54 - epoch 022 lr: 0.006184
2022-08-21 13:47:34 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.2747
2022-08-21 13:48:07 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.5285
2022-08-21 13:48:41 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.1949
2022-08-21 13:49:14 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.3833
2022-08-21 13:49:47 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.4797
2022-08-21 13:50:21 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.4222
2022-08-21 13:50:55 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.7320
2022-08-21 13:51:28 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.3802
2022-08-21 13:52:02 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.4728
2022-08-21 13:52:36 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.3662
2022-08-21 13:53:09 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3967
2022-08-21 13:53:43 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.1952
2022-08-21 13:54:17 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.3462
2022-08-21 13:54:51 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.1992
2022-08-21 13:55:24 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.3863
2022-08-21 13:55:58 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.3873
2022-08-21 13:56:32 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.5517
2022-08-21 13:57:06 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.6571
2022-08-21 13:57:39 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.4257
2022-08-21 13:58:13 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4533
2022-08-21 13:58:47 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.3961
2022-08-21 13:59:21 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.1734
2022-08-21 13:59:55 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.5951
2022-08-21 14:00:29 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.5889
2022-08-21 14:01:02 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.4620
2022-08-21 14:01:36 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.3624
2022-08-21 14:02:10 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.2695
2022-08-21 14:02:44 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.3782
2022-08-21 14:03:18 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.2814
2022-08-21 14:03:52 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.4552
2022-08-21 14:04:25 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.5935
2022-08-21 14:04:59 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4574
2022-08-21 14:05:33 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.4155
2022-08-21 14:06:07 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.3770
2022-08-21 14:06:41 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.2936
2022-08-21 14:07:15 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.3889
2022-08-21 14:07:49 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.4774
2022-08-21 14:08:23 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.7615
2022-08-21 14:08:57 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.2679
2022-08-21 14:09:31 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.5606
2022-08-21 14:10:05 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.3130
2022-08-21 14:10:39 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.3565
2022-08-21 14:11:13 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.6404
2022-08-21 14:11:47 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3157
2022-08-21 14:12:21 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.2406
2022-08-21 14:12:55 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.6056
2022-08-21 14:13:29 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.3681
2022-08-21 14:14:03 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.1816
2022-08-21 14:14:37 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.3788
2022-08-21 14:15:10 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.2985
2022-08-21 14:15:12 - train: epoch 022, train_loss: 1.4177
2022-08-21 14:16:27 - eval: epoch: 022, acc1: 70.630%, acc5: 89.708%, test_loss: 1.2005, per_image_load_time: 0.942ms, per_image_inference_time: 0.642ms
2022-08-21 14:16:28 - until epoch: 022, best_acc1: 70.630%
2022-08-21 14:16:28 - epoch 023 lr: 0.003511
2022-08-21 14:17:08 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.3327
2022-08-21 14:17:41 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.2323
2022-08-21 14:18:15 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.3712
2022-08-21 14:18:48 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.4819
2022-08-21 14:19:21 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.6236
2022-08-21 14:19:55 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.3128
2022-08-21 14:20:28 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.2875
2022-08-21 14:21:02 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.4744
2022-08-21 14:21:35 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4434
2022-08-21 14:22:09 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.2822
2022-08-21 14:22:43 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.2827
2022-08-21 14:23:17 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.1923
2022-08-21 14:23:51 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.3317
2022-08-21 14:24:25 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.3516
2022-08-21 14:24:58 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.3577
2022-08-21 14:25:32 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.4790
2022-08-21 14:26:06 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.5012
2022-08-21 14:26:39 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.4179
2022-08-21 14:27:13 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.3904
2022-08-21 14:27:47 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.2188
2022-08-21 14:28:21 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.2796
2022-08-21 14:28:55 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.3577
2022-08-21 14:29:29 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.3160
2022-08-21 14:30:03 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.3886
2022-08-21 14:30:37 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.3223
2022-08-21 14:31:11 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.5084
2022-08-21 14:31:45 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.5732
2022-08-21 14:32:19 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.3264
2022-08-21 14:32:53 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.2861
2022-08-21 14:33:27 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.3561
2022-08-21 14:34:02 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.4478
2022-08-21 14:34:36 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.3492
2022-08-21 14:35:10 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.3574
2022-08-21 14:35:44 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.5577
2022-08-21 14:36:18 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.3477
2022-08-21 14:36:51 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.3062
2022-08-21 14:37:25 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.4037
2022-08-21 14:37:59 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.3727
2022-08-21 14:38:33 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3548
2022-08-21 14:39:07 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.1235
2022-08-21 14:39:41 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.3615
2022-08-21 14:40:15 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.1128
2022-08-21 14:40:49 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.2642
2022-08-21 14:41:23 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.2657
2022-08-21 14:41:57 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.2601
2022-08-21 14:42:31 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.6943
2022-08-21 14:43:05 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.2129
2022-08-21 14:43:39 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.2587
2022-08-21 14:44:13 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.2838
2022-08-21 14:44:47 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.3155
2022-08-21 14:44:48 - train: epoch 023, train_loss: 1.3650
2022-08-21 14:46:04 - eval: epoch: 023, acc1: 71.270%, acc5: 90.162%, test_loss: 1.1700, per_image_load_time: 0.816ms, per_image_inference_time: 0.627ms
2022-08-21 14:46:04 - until epoch: 023, best_acc1: 71.270%
2022-08-21 14:46:04 - epoch 024 lr: 0.001571
2022-08-21 14:46:45 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.3179
2022-08-21 14:47:18 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.3634
2022-08-21 14:47:51 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.4008
2022-08-21 14:48:25 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.3598
2022-08-21 14:48:58 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.4975
2022-08-21 14:49:32 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.4031
2022-08-21 14:50:06 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.4561
2022-08-21 14:50:39 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.3591
2022-08-21 14:51:13 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.4012
2022-08-21 14:51:46 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.4980
2022-08-21 14:52:20 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.1548
2022-08-21 14:52:53 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.2991
2022-08-21 14:53:27 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.5165
2022-08-21 14:54:01 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.3865
2022-08-21 14:54:34 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.3323
2022-08-21 14:55:08 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.2514
2022-08-21 14:55:42 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.2162
2022-08-21 14:56:15 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.6123
2022-08-21 14:56:49 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.1248
2022-08-21 14:57:23 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.3714
2022-08-21 14:57:57 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.4688
2022-08-21 14:58:31 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.3323
2022-08-21 14:59:05 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.4763
2022-08-21 14:59:39 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.5295
2022-08-21 15:00:13 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.3717
2022-08-21 15:00:47 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.5277
2022-08-21 15:01:21 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.3801
2022-08-21 15:01:55 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.2734
2022-08-21 15:02:28 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.4122
2022-08-21 15:03:02 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.1298
2022-08-21 15:03:36 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.2048
2022-08-21 15:04:10 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.2450
2022-08-21 15:04:44 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.1446
2022-08-21 15:05:18 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.3032
2022-08-21 15:05:52 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.3031
2022-08-21 15:06:26 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.4386
2022-08-21 15:07:01 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.3182
2022-08-21 15:07:35 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.3424
2022-08-21 15:08:09 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.2975
2022-08-21 15:08:42 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.3145
2022-08-21 15:09:17 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.2282
2022-08-21 15:09:51 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.1291
2022-08-21 15:10:25 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.3695
2022-08-21 15:10:59 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.3091
2022-08-21 15:11:33 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.1956
2022-08-21 15:12:07 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.3376
2022-08-21 15:12:41 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.2704
2022-08-21 15:13:15 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.0842
2022-08-21 15:13:49 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.4748
2022-08-21 15:14:23 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.3656
2022-08-21 15:14:24 - train: epoch 024, train_loss: 1.3279
2022-08-21 15:15:40 - eval: epoch: 024, acc1: 71.710%, acc5: 90.410%, test_loss: 1.1530, per_image_load_time: 1.207ms, per_image_inference_time: 0.655ms
2022-08-21 15:15:40 - until epoch: 024, best_acc1: 71.710%
2022-08-21 15:15:40 - epoch 025 lr: 0.000394
2022-08-21 15:16:20 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.2052
2022-08-21 15:16:54 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1652
2022-08-21 15:17:27 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.3237
2022-08-21 15:18:00 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.4069
2022-08-21 15:18:34 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.1047
2022-08-21 15:19:07 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3370
2022-08-21 15:19:41 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.2317
2022-08-21 15:20:15 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.2061
2022-08-21 15:20:48 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.1323
2022-08-21 15:21:22 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3288
2022-08-21 15:21:56 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.1699
2022-08-21 15:22:30 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.3511
2022-08-21 15:23:03 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.2784
2022-08-21 15:23:37 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.2270
2022-08-21 15:24:11 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.3912
2022-08-21 15:24:44 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1980
2022-08-21 15:25:18 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.4046
2022-08-21 15:25:52 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.1199
2022-08-21 15:26:25 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.4334
2022-08-21 15:26:59 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.3561
2022-08-21 15:27:33 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.0852
2022-08-21 15:28:07 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.2131
2022-08-21 15:28:41 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.3988
2022-08-21 15:29:15 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.1252
2022-08-21 15:29:50 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.0717
2022-08-21 15:30:24 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.2696
2022-08-21 15:30:58 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.3578
2022-08-21 15:31:32 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.3714
2022-08-21 15:32:06 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.2873
2022-08-21 15:32:40 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.3328
2022-08-21 15:33:14 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.3855
2022-08-21 15:33:48 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.3293
2022-08-21 15:34:22 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.2233
2022-08-21 15:34:56 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.2876
2022-08-21 15:35:29 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.4224
2022-08-21 15:36:03 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.4117
2022-08-21 15:36:37 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.1512
2022-08-21 15:37:11 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.4030
2022-08-21 15:37:45 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.5708
2022-08-21 15:38:20 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.1831
2022-08-21 15:38:54 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.4077
2022-08-21 15:39:28 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.3332
2022-08-21 15:40:01 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.3912
2022-08-21 15:40:35 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.2159
2022-08-21 15:41:09 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.2013
2022-08-21 15:41:43 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.0978
2022-08-21 15:42:17 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.2605
2022-08-21 15:42:51 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.1542
2022-08-21 15:43:25 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.3897
2022-08-21 15:43:59 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.2570
2022-08-21 15:44:00 - train: epoch 025, train_loss: 1.3086
2022-08-21 15:45:16 - eval: epoch: 025, acc1: 71.758%, acc5: 90.382%, test_loss: 1.1514, per_image_load_time: 1.278ms, per_image_inference_time: 0.663ms
2022-08-21 15:45:16 - until epoch: 025, best_acc1: 71.758%
2022-08-21 15:45:16 - train done. train time: 12.348 hours, best_acc1: 71.758%
