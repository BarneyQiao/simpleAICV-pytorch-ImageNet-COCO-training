2022-08-22 16:27:36 - net_idx: 15
2022-08-22 16:27:36 - net_config: {'stem_width': 64, 'depth': 12, 'w_0': 48, 'w_a': 18.57073015636926, 'w_m': 1.7046627186576484}
2022-08-22 16:27:36 - num_classes: 1000
2022-08-22 16:27:36 - input_image_size: 224
2022-08-22 16:27:36 - scale: 1.1428571428571428
2022-08-22 16:27:36 - seed: 0
2022-08-22 16:27:36 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-22 16:27:36 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-22 16:27:36 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c2100>
2022-08-22 16:27:36 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c23d0>
2022-08-22 16:27:36 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2400>
2022-08-22 16:27:36 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2460>
2022-08-22 16:27:36 - batch_size: 256
2022-08-22 16:27:36 - num_workers: 16
2022-08-22 16:27:36 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-22 16:27:36 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-22 16:27:36 - epochs: 25
2022-08-22 16:27:36 - print_interval: 100
2022-08-22 16:27:36 - accumulation_steps: 1
2022-08-22 16:27:36 - sync_bn: False
2022-08-22 16:27:36 - apex: True
2022-08-22 16:27:36 - use_ema_model: False
2022-08-22 16:27:36 - ema_model_decay: 0.9999
2022-08-22 16:27:36 - log_dir: ./log
2022-08-22 16:27:36 - checkpoint_dir: ./checkpoints
2022-08-22 16:27:36 - gpus_type: NVIDIA RTX A5000
2022-08-22 16:27:36 - gpus_num: 2
2022-08-22 16:27:36 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f4efca864b0>
2022-08-22 16:27:36 - ema_model: None
2022-08-22 16:27:36 - --------------------parameters--------------------
2022-08-22 16:27:36 - name: conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-22 16:27:36 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-22 16:27:36 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-22 16:27:36 - name: fc.weight, grad: True
2022-08-22 16:27:36 - name: fc.bias, grad: True
2022-08-22 16:27:36 - --------------------buffers--------------------
2022-08-22 16:27:36 - name: conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-22 16:27:36 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-22 16:27:36 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-22 16:27:36 - -----------no weight decay layers--------------
2022-08-22 16:27:36 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-22 16:27:36 - -------------weight decay layers---------------
2022-08-22 16:27:36 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-22 16:27:36 - epoch 001 lr: 0.100000
2022-08-22 16:28:18 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9110
2022-08-22 16:28:52 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9023
2022-08-22 16:29:25 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.7921
2022-08-22 16:29:59 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.7051
2022-08-22 16:30:32 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.6244
2022-08-22 16:31:06 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.4629
2022-08-22 16:31:39 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.5677
2022-08-22 16:32:13 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.3932
2022-08-22 16:32:46 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.3334
2022-08-22 16:33:19 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.2630
2022-08-22 16:33:53 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.1560
2022-08-22 16:34:27 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 5.9177
2022-08-22 16:35:01 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 5.9381
2022-08-22 16:35:34 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 5.8625
2022-08-22 16:36:08 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 5.8172
2022-08-22 16:36:42 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 5.7945
2022-08-22 16:37:16 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.5950
2022-08-22 16:37:49 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.6028
2022-08-22 16:38:23 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.4737
2022-08-22 16:38:57 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.3309
2022-08-22 16:39:31 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.2321
2022-08-22 16:40:05 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.3208
2022-08-22 16:40:39 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.0657
2022-08-22 16:41:12 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.1563
2022-08-22 16:41:46 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.1992
2022-08-22 16:42:19 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.2039
2022-08-22 16:42:53 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 4.9546
2022-08-22 16:43:26 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.0381
2022-08-22 16:44:00 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 4.8233
2022-08-22 16:44:34 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.0380
2022-08-22 16:45:08 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.0911
2022-08-22 16:45:41 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 4.8112
2022-08-22 16:46:15 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.6375
2022-08-22 16:46:49 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.7388
2022-08-22 16:47:22 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.7081
2022-08-22 16:47:56 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.7150
2022-08-22 16:48:30 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.6310
2022-08-22 16:49:03 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.5779
2022-08-22 16:49:37 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.4243
2022-08-22 16:50:11 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.4693
2022-08-22 16:50:44 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.6430
2022-08-22 16:51:17 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.4164
2022-08-22 16:51:51 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.3630
2022-08-22 16:52:24 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.1647
2022-08-22 16:52:58 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.2107
2022-08-22 16:53:32 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.7165
2022-08-22 16:54:06 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.0779
2022-08-22 16:54:40 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.3872
2022-08-22 16:55:13 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.2157
2022-08-22 16:55:46 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.0574
2022-08-22 16:55:48 - train: epoch 001, train_loss: 5.2821
2022-08-22 16:57:04 - eval: epoch: 001, acc1: 19.272%, acc5: 41.450%, test_loss: 4.0605, per_image_load_time: 2.397ms, per_image_inference_time: 0.581ms
2022-08-22 16:57:04 - until epoch: 001, best_acc1: 19.272%
2022-08-22 16:57:04 - epoch 002 lr: 0.099606
2022-08-22 16:57:46 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.0245
2022-08-22 16:58:20 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 3.9638
2022-08-22 16:58:53 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.0781
2022-08-22 16:59:26 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.1065
2022-08-22 16:59:59 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 3.9188
2022-08-22 17:00:32 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.0090
2022-08-22 17:01:06 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.2336
2022-08-22 17:01:39 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 3.8517
2022-08-22 17:02:13 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.7943
2022-08-22 17:02:46 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 3.7693
2022-08-22 17:03:20 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.0375
2022-08-22 17:03:53 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.8773
2022-08-22 17:04:27 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.8426
2022-08-22 17:05:01 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.1551
2022-08-22 17:05:35 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 3.7670
2022-08-22 17:06:08 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.6258
2022-08-22 17:06:41 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.7907
2022-08-22 17:07:15 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.7369
2022-08-22 17:07:49 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.6385
2022-08-22 17:08:22 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.4656
2022-08-22 17:08:56 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.8788
2022-08-22 17:09:29 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.4830
2022-08-22 17:10:03 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.5486
2022-08-22 17:10:36 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.4874
2022-08-22 17:11:10 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.3466
2022-08-22 17:11:43 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.6864
2022-08-22 17:12:17 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.8703
2022-08-22 17:12:50 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.8500
2022-08-22 17:13:24 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.6275
2022-08-22 17:13:57 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.3555
2022-08-22 17:14:31 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.4790
2022-08-22 17:15:05 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.5993
2022-08-22 17:15:38 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.5966
2022-08-22 17:16:12 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.4682
2022-08-22 17:16:46 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.3980
2022-08-22 17:17:19 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.5282
2022-08-22 17:17:53 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.7160
2022-08-22 17:18:26 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.4894
2022-08-22 17:19:00 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.5006
2022-08-22 17:19:34 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.2711
2022-08-22 17:20:08 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.5081
2022-08-22 17:20:41 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.3797
2022-08-22 17:21:15 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.2600
2022-08-22 17:21:49 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.1333
2022-08-22 17:22:23 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.0774
2022-08-22 17:22:56 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.1426
2022-08-22 17:23:30 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.4072
2022-08-22 17:24:04 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.4586
2022-08-22 17:24:37 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.3409
2022-08-22 17:25:11 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.4136
2022-08-22 17:25:12 - train: epoch 002, train_loss: 3.6264
2022-08-22 17:26:29 - eval: epoch: 002, acc1: 33.784%, acc5: 59.916%, test_loss: 3.0846, per_image_load_time: 2.236ms, per_image_inference_time: 0.596ms
2022-08-22 17:26:29 - until epoch: 002, best_acc1: 33.784%
2022-08-22 17:26:29 - epoch 003 lr: 0.098429
2022-08-22 17:27:11 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.1830
2022-08-22 17:27:44 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.4820
2022-08-22 17:28:17 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.2022
2022-08-22 17:28:50 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.1067
2022-08-22 17:29:23 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.2484
2022-08-22 17:29:57 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.2559
2022-08-22 17:30:30 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.2452
2022-08-22 17:31:04 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.2736
2022-08-22 17:31:37 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.0306
2022-08-22 17:32:11 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.3058
2022-08-22 17:32:44 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.0466
2022-08-22 17:33:18 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.1844
2022-08-22 17:33:51 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.0441
2022-08-22 17:34:25 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 2.7194
2022-08-22 17:34:59 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.3997
2022-08-22 17:35:33 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.1239
2022-08-22 17:36:06 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 2.9599
2022-08-22 17:36:40 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 2.9995
2022-08-22 17:37:13 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.0723
2022-08-22 17:37:47 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 2.9116
2022-08-22 17:38:20 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.1705
2022-08-22 17:38:54 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.4675
2022-08-22 17:39:27 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.8104
2022-08-22 17:40:01 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 2.9825
2022-08-22 17:40:34 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.1403
2022-08-22 17:41:08 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.2526
2022-08-22 17:41:41 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.3363
2022-08-22 17:42:15 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.1135
2022-08-22 17:42:49 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.9140
2022-08-22 17:43:22 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 2.9283
2022-08-22 17:43:56 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.3416
2022-08-22 17:44:30 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.0328
2022-08-22 17:45:03 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 2.9626
2022-08-22 17:45:37 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.1254
2022-08-22 17:46:11 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.7170
2022-08-22 17:46:44 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 3.1536
2022-08-22 17:47:17 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 2.9718
2022-08-22 17:47:51 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.0325
2022-08-22 17:48:25 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.2580
2022-08-22 17:48:58 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7173
2022-08-22 17:49:32 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.9581
2022-08-22 17:50:05 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.0997
2022-08-22 17:50:39 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.7381
2022-08-22 17:51:12 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.9493
2022-08-22 17:51:46 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 2.9135
2022-08-22 17:52:19 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.8616
2022-08-22 17:52:53 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 3.0463
2022-08-22 17:53:27 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.2186
2022-08-22 17:54:01 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.0052
2022-08-22 17:54:34 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.8129
2022-08-22 17:54:35 - train: epoch 003, train_loss: 3.0660
2022-08-22 17:55:52 - eval: epoch: 003, acc1: 38.458%, acc5: 65.082%, test_loss: 2.8078, per_image_load_time: 2.357ms, per_image_inference_time: 0.611ms
2022-08-22 17:55:52 - until epoch: 003, best_acc1: 38.458%
2022-08-22 17:55:52 - epoch 004 lr: 0.096488
2022-08-22 17:56:33 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.9070
2022-08-22 17:57:06 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.7060
2022-08-22 17:57:39 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.8988
2022-08-22 17:58:13 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.7713
2022-08-22 17:58:46 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.7090
2022-08-22 17:59:20 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 2.9601
2022-08-22 17:59:53 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.8646
2022-08-22 18:00:27 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.8006
2022-08-22 18:01:01 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.5168
2022-08-22 18:01:34 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.9686
2022-08-22 18:02:08 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.0328
2022-08-22 18:02:42 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.6169
2022-08-22 18:03:15 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.6352
2022-08-22 18:03:49 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.8885
2022-08-22 18:04:23 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.9686
2022-08-22 18:04:56 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.6955
2022-08-22 18:05:30 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.8408
2022-08-22 18:06:04 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.8869
2022-08-22 18:06:37 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.8543
2022-08-22 18:07:11 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.8535
2022-08-22 18:07:45 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.8812
2022-08-22 18:08:18 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.8882
2022-08-22 18:08:52 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.5364
2022-08-22 18:09:25 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.6288
2022-08-22 18:09:59 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.7622
2022-08-22 18:10:33 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.7306
2022-08-22 18:11:07 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.7354
2022-08-22 18:11:40 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.6523
2022-08-22 18:12:14 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8329
2022-08-22 18:12:48 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.9891
2022-08-22 18:13:21 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.7210
2022-08-22 18:13:55 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8268
2022-08-22 18:14:28 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8845
2022-08-22 18:15:01 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.7462
2022-08-22 18:15:35 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.7860
2022-08-22 18:16:08 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.7521
2022-08-22 18:16:42 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.7092
2022-08-22 18:17:16 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.4873
2022-08-22 18:17:50 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.5784
2022-08-22 18:18:23 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.4466
2022-08-22 18:18:57 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.6994
2022-08-22 18:19:31 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6810
2022-08-22 18:20:04 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.5515
2022-08-22 18:20:38 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.6705
2022-08-22 18:21:12 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.2808
2022-08-22 18:21:45 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.6894
2022-08-22 18:22:19 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.6741
2022-08-22 18:22:52 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.5288
2022-08-22 18:23:25 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.5916
2022-08-22 18:23:58 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.7475
2022-08-22 18:24:00 - train: epoch 004, train_loss: 2.7897
2022-08-22 18:25:16 - eval: epoch: 004, acc1: 43.262%, acc5: 69.784%, test_loss: 2.5564, per_image_load_time: 2.362ms, per_image_inference_time: 0.598ms
2022-08-22 18:25:16 - until epoch: 004, best_acc1: 43.262%
2022-08-22 18:25:16 - epoch 005 lr: 0.093815
2022-08-22 18:25:57 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.5457
2022-08-22 18:26:30 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.7235
2022-08-22 18:27:02 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.9696
2022-08-22 18:27:35 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.7564
2022-08-22 18:28:08 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.5729
2022-08-22 18:28:41 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.5390
2022-08-22 18:29:13 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.7183
2022-08-22 18:29:46 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.8366
2022-08-22 18:30:19 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.7590
2022-08-22 18:30:51 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.6788
2022-08-22 18:31:24 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.5242
2022-08-22 18:31:57 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.5416
2022-08-22 18:32:30 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.4399
2022-08-22 18:33:03 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.5254
2022-08-22 18:33:36 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.5371
2022-08-22 18:34:09 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.4181
2022-08-22 18:34:42 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.7212
2022-08-22 18:35:15 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.4957
2022-08-22 18:35:48 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.5704
2022-08-22 18:36:22 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.5683
2022-08-22 18:36:55 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.4546
2022-08-22 18:37:28 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.6474
2022-08-22 18:38:01 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.3590
2022-08-22 18:38:34 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5984
2022-08-22 18:39:07 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.6112
2022-08-22 18:39:39 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.8979
2022-08-22 18:40:12 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.7928
2022-08-22 18:40:45 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.5189
2022-08-22 18:41:18 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.4798
2022-08-22 18:41:51 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.4622
2022-08-22 18:42:24 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.6884
2022-08-22 18:42:56 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.5091
2022-08-22 18:43:30 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.5388
2022-08-22 18:44:03 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.5948
2022-08-22 18:44:36 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.5828
2022-08-22 18:45:08 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6992
2022-08-22 18:45:41 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.5533
2022-08-22 18:46:14 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.4277
2022-08-22 18:46:47 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.7446
2022-08-22 18:47:20 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.6148
2022-08-22 18:47:53 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.4685
2022-08-22 18:48:26 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.6794
2022-08-22 18:48:59 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.7264
2022-08-22 18:49:32 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.6401
2022-08-22 18:50:05 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.7184
2022-08-22 18:50:38 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.3782
2022-08-22 18:51:11 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.5166
2022-08-22 18:51:44 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.3987
2022-08-22 18:52:16 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.8400
2022-08-22 18:52:49 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.3233
2022-08-22 18:52:50 - train: epoch 005, train_loss: 2.6223
2022-08-22 18:54:07 - eval: epoch: 005, acc1: 45.784%, acc5: 72.760%, test_loss: 2.3828, per_image_load_time: 0.737ms, per_image_inference_time: 0.600ms
2022-08-22 18:54:07 - until epoch: 005, best_acc1: 45.784%
2022-08-22 18:54:07 - epoch 006 lr: 0.090450
2022-08-22 18:54:48 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.4535
2022-08-22 18:55:21 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.5306
2022-08-22 18:55:54 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.4032
2022-08-22 18:56:27 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.5344
2022-08-22 18:57:00 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.3496
2022-08-22 18:57:33 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.5503
2022-08-22 18:58:06 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.5599
2022-08-22 18:58:39 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.4680
2022-08-22 18:59:13 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.4802
2022-08-22 18:59:46 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.4374
2022-08-22 19:00:19 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.6572
2022-08-22 19:00:52 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.3515
2022-08-22 19:01:25 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5254
2022-08-22 19:01:58 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.3894
2022-08-22 19:02:32 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.5846
2022-08-22 19:03:05 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3575
2022-08-22 19:03:38 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.6043
2022-08-22 19:04:11 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.4257
2022-08-22 19:04:45 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.4869
2022-08-22 19:05:17 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.6348
2022-08-22 19:05:50 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.5812
2022-08-22 19:06:24 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.2487
2022-08-22 19:06:57 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.4804
2022-08-22 19:07:30 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.6097
2022-08-22 19:08:03 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4561
2022-08-22 19:08:36 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.2081
2022-08-22 19:09:09 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.5705
2022-08-22 19:09:42 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.4048
2022-08-22 19:10:15 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.6560
2022-08-22 19:10:49 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.5445
2022-08-22 19:11:22 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.3262
2022-08-22 19:11:56 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.4327
2022-08-22 19:12:29 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.3127
2022-08-22 19:13:02 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.7582
2022-08-22 19:13:35 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.5336
2022-08-22 19:14:08 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.5592
2022-08-22 19:14:40 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.5307
2022-08-22 19:15:13 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.0585
2022-08-22 19:15:46 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.2948
2022-08-22 19:16:19 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.4399
2022-08-22 19:16:52 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.5546
2022-08-22 19:17:25 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.3240
2022-08-22 19:17:58 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.3797
2022-08-22 19:18:30 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.4529
2022-08-22 19:19:03 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.4769
2022-08-22 19:19:36 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.4325
2022-08-22 19:20:08 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.4392
2022-08-22 19:20:41 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.5552
2022-08-22 19:21:14 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.6051
2022-08-22 19:21:46 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.4585
2022-08-22 19:21:48 - train: epoch 006, train_loss: 2.5091
2022-08-22 19:23:05 - eval: epoch: 006, acc1: 48.574%, acc5: 74.640%, test_loss: 2.2666, per_image_load_time: 1.965ms, per_image_inference_time: 0.605ms
2022-08-22 19:23:05 - until epoch: 006, best_acc1: 48.574%
2022-08-22 19:23:05 - epoch 007 lr: 0.086448
2022-08-22 19:23:46 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.4895
2022-08-22 19:24:18 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.5127
2022-08-22 19:24:51 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.5477
2022-08-22 19:25:23 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.4441
2022-08-22 19:25:56 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.1800
2022-08-22 19:26:28 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.7128
2022-08-22 19:27:01 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.4628
2022-08-22 19:27:33 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.6375
2022-08-22 19:28:06 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.4918
2022-08-22 19:28:39 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.4164
2022-08-22 19:29:12 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.3949
2022-08-22 19:29:45 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.3637
2022-08-22 19:30:17 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.2873
2022-08-22 19:30:50 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.4351
2022-08-22 19:31:23 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.5219
2022-08-22 19:31:56 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.3330
2022-08-22 19:32:29 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.5480
2022-08-22 19:33:02 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.5095
2022-08-22 19:33:35 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.4555
2022-08-22 19:34:08 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.1848
2022-08-22 19:34:41 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.6389
2022-08-22 19:35:14 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.3547
2022-08-22 19:35:46 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.3228
2022-08-22 19:36:19 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.5617
2022-08-22 19:36:51 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.4882
2022-08-22 19:37:24 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.2445
2022-08-22 19:37:57 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.2184
2022-08-22 19:38:30 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.2818
2022-08-22 19:39:02 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.2828
2022-08-22 19:39:35 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.6055
2022-08-22 19:40:08 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.4640
2022-08-22 19:40:40 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.2187
2022-08-22 19:41:13 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.6433
2022-08-22 19:41:46 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.4742
2022-08-22 19:42:19 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.4069
2022-08-22 19:42:51 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.4571
2022-08-22 19:43:24 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.3940
2022-08-22 19:43:57 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.4632
2022-08-22 19:44:30 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.2693
2022-08-22 19:45:03 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.5129
2022-08-22 19:45:36 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.2424
2022-08-22 19:46:09 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2521
2022-08-22 19:46:41 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.4874
2022-08-22 19:47:14 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.2184
2022-08-22 19:47:47 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.4479
2022-08-22 19:48:20 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.6236
2022-08-22 19:48:52 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.2987
2022-08-22 19:49:25 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.4614
2022-08-22 19:49:58 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.2552
2022-08-22 19:50:31 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.3552
2022-08-22 19:50:32 - train: epoch 007, train_loss: 2.4164
2022-08-22 19:51:48 - eval: epoch: 007, acc1: 49.440%, acc5: 75.592%, test_loss: 2.2029, per_image_load_time: 2.067ms, per_image_inference_time: 0.643ms
2022-08-22 19:51:48 - until epoch: 007, best_acc1: 49.440%
2022-08-22 19:51:48 - epoch 008 lr: 0.081870
2022-08-22 19:52:29 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.2400
2022-08-22 19:53:01 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.3414
2022-08-22 19:53:34 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.4329
2022-08-22 19:54:07 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.3180
2022-08-22 19:54:39 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.2277
2022-08-22 19:55:12 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.3312
2022-08-22 19:55:45 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.3213
2022-08-22 19:56:18 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.2603
2022-08-22 19:56:51 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.3309
2022-08-22 19:57:24 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.4196
2022-08-22 19:57:57 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.2899
2022-08-22 19:58:29 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.2461
2022-08-22 19:59:02 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.4183
2022-08-22 19:59:35 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.2749
2022-08-22 20:00:08 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.3554
2022-08-22 20:00:41 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.4986
2022-08-22 20:01:14 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.4027
2022-08-22 20:01:47 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.3577
2022-08-22 20:02:20 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.3097
2022-08-22 20:02:53 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.3773
2022-08-22 20:03:26 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.1845
2022-08-22 20:03:59 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.1247
2022-08-22 20:04:32 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.4053
2022-08-22 20:05:04 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.1373
2022-08-22 20:05:37 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.3243
2022-08-22 20:06:10 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.4980
2022-08-22 20:06:43 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.4172
2022-08-22 20:07:16 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.4335
2022-08-22 20:07:49 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.2806
2022-08-22 20:08:22 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.5861
2022-08-22 20:08:55 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.3085
2022-08-22 20:09:28 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.5327
2022-08-22 20:10:01 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.3943
2022-08-22 20:10:34 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.3661
2022-08-22 20:11:07 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.4567
2022-08-22 20:11:39 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.4037
2022-08-22 20:12:12 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.2312
2022-08-22 20:12:45 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.3648
2022-08-22 20:13:18 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.2583
2022-08-22 20:13:50 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.3891
2022-08-22 20:14:23 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 1.9932
2022-08-22 20:14:56 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.4496
2022-08-22 20:15:29 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.0823
2022-08-22 20:16:01 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.3325
2022-08-22 20:16:34 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.2519
2022-08-22 20:17:06 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.2283
2022-08-22 20:17:39 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2765
2022-08-22 20:18:11 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.4270
2022-08-22 20:18:44 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.3959
2022-08-22 20:19:17 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.2939
2022-08-22 20:19:18 - train: epoch 008, train_loss: 2.3398
2022-08-22 20:20:34 - eval: epoch: 008, acc1: 53.364%, acc5: 78.748%, test_loss: 1.9938, per_image_load_time: 1.952ms, per_image_inference_time: 0.645ms
2022-08-22 20:20:34 - until epoch: 008, best_acc1: 53.364%
2022-08-22 20:20:34 - epoch 009 lr: 0.076790
2022-08-22 20:21:14 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 2.0947
2022-08-22 20:21:47 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.3596
2022-08-22 20:22:19 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.1228
2022-08-22 20:22:52 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.4102
2022-08-22 20:23:25 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.1593
2022-08-22 20:23:58 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.0438
2022-08-22 20:24:31 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.0973
2022-08-22 20:25:04 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.2617
2022-08-22 20:25:37 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.0905
2022-08-22 20:26:10 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1821
2022-08-22 20:26:42 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.4610
2022-08-22 20:27:15 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.2347
2022-08-22 20:27:48 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.4997
2022-08-22 20:28:21 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9534
2022-08-22 20:28:54 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.2122
2022-08-22 20:29:27 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.0984
2022-08-22 20:30:00 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.5355
2022-08-22 20:30:33 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.2112
2022-08-22 20:31:06 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.0371
2022-08-22 20:31:39 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.1018
2022-08-22 20:32:12 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.2396
2022-08-22 20:32:45 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.2507
2022-08-22 20:33:18 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.1951
2022-08-22 20:33:51 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.3199
2022-08-22 20:34:24 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.1381
2022-08-22 20:34:57 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.2572
2022-08-22 20:35:30 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.2089
2022-08-22 20:36:03 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.3171
2022-08-22 20:36:36 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.0395
2022-08-22 20:37:09 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1208
2022-08-22 20:37:41 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.2969
2022-08-22 20:38:14 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.1157
2022-08-22 20:38:48 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.3298
2022-08-22 20:39:21 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.4700
2022-08-22 20:39:54 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.2341
2022-08-22 20:40:26 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.1082
2022-08-22 20:40:59 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.3445
2022-08-22 20:41:32 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.4566
2022-08-22 20:42:05 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 1.9874
2022-08-22 20:42:39 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.3421
2022-08-22 20:43:12 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.3726
2022-08-22 20:43:45 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.1583
2022-08-22 20:44:18 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.0759
2022-08-22 20:44:51 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.5548
2022-08-22 20:45:24 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.5171
2022-08-22 20:45:56 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.3108
2022-08-22 20:46:29 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.4247
2022-08-22 20:47:03 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.3109
2022-08-22 20:47:36 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.3920
2022-08-22 20:48:08 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 1.9841
2022-08-22 20:48:10 - train: epoch 009, train_loss: 2.2743
2022-08-22 20:49:26 - eval: epoch: 009, acc1: 53.062%, acc5: 78.474%, test_loss: 2.0322, per_image_load_time: 1.737ms, per_image_inference_time: 0.666ms
2022-08-22 20:49:26 - until epoch: 009, best_acc1: 53.364%
2022-08-22 20:49:26 - epoch 010 lr: 0.071288
2022-08-22 20:50:06 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.1697
2022-08-22 20:50:39 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.1357
2022-08-22 20:51:12 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.2204
2022-08-22 20:51:46 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.6049
2022-08-22 20:52:19 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.0891
2022-08-22 20:52:53 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.5454
2022-08-22 20:53:26 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.2660
2022-08-22 20:54:00 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.4105
2022-08-22 20:54:33 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.1922
2022-08-22 20:55:06 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.1041
2022-08-22 20:55:39 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.2127
2022-08-22 20:56:13 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.2154
2022-08-22 20:56:47 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 1.9921
2022-08-22 20:57:20 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.3371
2022-08-22 20:57:54 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 2.0777
2022-08-22 20:58:27 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.1085
2022-08-22 20:59:01 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.2457
2022-08-22 20:59:34 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 2.0698
2022-08-22 21:00:08 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.2920
2022-08-22 21:00:41 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.3291
2022-08-22 21:01:15 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 2.0972
2022-08-22 21:01:48 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.2809
2022-08-22 21:02:21 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.3886
2022-08-22 21:02:54 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.4226
2022-08-22 21:03:28 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.2178
2022-08-22 21:04:01 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.3324
2022-08-22 21:04:34 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.9045
2022-08-22 21:05:07 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.2832
2022-08-22 21:05:41 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.1641
2022-08-22 21:06:14 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1460
2022-08-22 21:06:47 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.4272
2022-08-22 21:07:21 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.2726
2022-08-22 21:07:54 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.1755
2022-08-22 21:08:27 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.4398
2022-08-22 21:08:59 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.3191
2022-08-22 21:09:33 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.2650
2022-08-22 21:10:06 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.1275
2022-08-22 21:10:39 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.2010
2022-08-22 21:11:13 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 2.0265
2022-08-22 21:11:46 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.1626
2022-08-22 21:12:19 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.3129
2022-08-22 21:12:51 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.1600
2022-08-22 21:13:24 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.1656
2022-08-22 21:13:56 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.2550
2022-08-22 21:14:30 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.0244
2022-08-22 21:15:03 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.1365
2022-08-22 21:15:36 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1642
2022-08-22 21:16:10 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.1995
2022-08-22 21:16:43 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.1143
2022-08-22 21:17:15 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.1724
2022-08-22 21:17:17 - train: epoch 010, train_loss: 2.2132
2022-08-22 21:18:33 - eval: epoch: 010, acc1: 55.684%, acc5: 80.114%, test_loss: 1.9009, per_image_load_time: 2.294ms, per_image_inference_time: 0.655ms
2022-08-22 21:18:33 - until epoch: 010, best_acc1: 55.684%
2022-08-22 21:18:33 - epoch 011 lr: 0.065450
2022-08-22 21:19:13 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.0529
2022-08-22 21:19:46 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.2709
2022-08-22 21:20:19 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.1675
2022-08-22 21:20:53 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.3677
2022-08-22 21:21:26 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.2750
2022-08-22 21:21:59 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.3038
2022-08-22 21:22:32 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.1574
2022-08-22 21:23:05 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.2324
2022-08-22 21:23:38 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.1792
2022-08-22 21:24:11 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 1.9441
2022-08-22 21:24:44 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2084
2022-08-22 21:25:17 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2295
2022-08-22 21:25:51 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.3507
2022-08-22 21:26:24 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.1100
2022-08-22 21:26:57 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.2134
2022-08-22 21:27:30 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.3752
2022-08-22 21:28:04 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.3566
2022-08-22 21:28:37 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.0844
2022-08-22 21:29:11 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.2235
2022-08-22 21:29:44 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1426
2022-08-22 21:30:17 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.0558
2022-08-22 21:30:51 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 2.0545
2022-08-22 21:31:24 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.3361
2022-08-22 21:31:57 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 2.0182
2022-08-22 21:32:29 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.0654
2022-08-22 21:33:03 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 1.9716
2022-08-22 21:33:36 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.0776
2022-08-22 21:34:10 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 2.0898
2022-08-22 21:34:43 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.1711
2022-08-22 21:35:16 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.4614
2022-08-22 21:35:49 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.0879
2022-08-22 21:36:22 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.1401
2022-08-22 21:36:55 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.1964
2022-08-22 21:37:28 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.0096
2022-08-22 21:38:01 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.1527
2022-08-22 21:38:34 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 1.9830
2022-08-22 21:39:07 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.2869
2022-08-22 21:39:40 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 2.0713
2022-08-22 21:40:13 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.1617
2022-08-22 21:40:46 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.0218
2022-08-22 21:41:19 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.9094
2022-08-22 21:41:52 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 2.2166
2022-08-22 21:42:25 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.2488
2022-08-22 21:42:58 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.0319
2022-08-22 21:43:31 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.1910
2022-08-22 21:44:04 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 2.1726
2022-08-22 21:44:38 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.9317
2022-08-22 21:45:10 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.8630
2022-08-22 21:45:43 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.1976
2022-08-22 21:46:16 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.0279
2022-08-22 21:46:17 - train: epoch 011, train_loss: 2.1581
2022-08-22 21:47:33 - eval: epoch: 011, acc1: 56.562%, acc5: 80.960%, test_loss: 1.8514, per_image_load_time: 1.376ms, per_image_inference_time: 0.653ms
2022-08-22 21:47:33 - until epoch: 011, best_acc1: 56.562%
2022-08-22 21:47:33 - epoch 012 lr: 0.059368
2022-08-22 21:48:14 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 2.0758
2022-08-22 21:48:47 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.8829
2022-08-22 21:49:20 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 2.1110
2022-08-22 21:49:53 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 1.8777
2022-08-22 21:50:27 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.0595
2022-08-22 21:51:01 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 2.0051
2022-08-22 21:51:34 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.8394
2022-08-22 21:52:08 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.1912
2022-08-22 21:52:41 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.2435
2022-08-22 21:53:15 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.8934
2022-08-22 21:53:48 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.2923
2022-08-22 21:54:21 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.8943
2022-08-22 21:54:54 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.9860
2022-08-22 21:55:28 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.1438
2022-08-22 21:56:01 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 2.0338
2022-08-22 21:56:34 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 2.0623
2022-08-22 21:57:07 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 2.1241
2022-08-22 21:57:40 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.2051
2022-08-22 21:58:14 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1148
2022-08-22 21:58:47 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.3325
2022-08-22 21:59:21 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.0453
2022-08-22 21:59:54 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.0635
2022-08-22 22:00:27 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 1.9929
2022-08-22 22:01:00 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.1071
2022-08-22 22:01:33 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.8595
2022-08-22 22:02:06 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.9186
2022-08-22 22:02:40 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.2618
2022-08-22 22:03:13 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.9866
2022-08-22 22:03:46 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.0277
2022-08-22 22:04:19 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.9394
2022-08-22 22:04:53 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.1320
2022-08-22 22:05:26 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 2.0024
2022-08-22 22:05:59 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 1.9678
2022-08-22 22:06:32 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.1223
2022-08-22 22:07:05 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.1441
2022-08-22 22:07:39 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.9938
2022-08-22 22:08:12 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.0180
2022-08-22 22:08:45 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.1846
2022-08-22 22:09:19 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8369
2022-08-22 22:09:52 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1055
2022-08-22 22:10:25 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.1442
2022-08-22 22:10:59 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 2.0581
2022-08-22 22:11:32 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.1184
2022-08-22 22:12:06 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 2.0391
2022-08-22 22:12:39 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 2.1150
2022-08-22 22:13:12 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.1492
2022-08-22 22:13:46 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.8958
2022-08-22 22:14:19 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.0485
2022-08-22 22:14:52 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.9777
2022-08-22 22:15:25 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.7988
2022-08-22 22:15:27 - train: epoch 012, train_loss: 2.0999
2022-08-22 22:16:43 - eval: epoch: 012, acc1: 58.530%, acc5: 82.216%, test_loss: 1.7540, per_image_load_time: 2.217ms, per_image_inference_time: 0.631ms
2022-08-22 22:16:43 - until epoch: 012, best_acc1: 58.530%
2022-08-22 22:16:43 - epoch 013 lr: 0.053138
2022-08-22 22:17:23 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.9726
2022-08-22 22:17:55 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.9725
2022-08-22 22:18:28 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8512
2022-08-22 22:19:00 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.7817
2022-08-22 22:19:33 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 2.1200
2022-08-22 22:20:06 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 1.9998
2022-08-22 22:20:39 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.8082
2022-08-22 22:21:12 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.1872
2022-08-22 22:21:46 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 2.1245
2022-08-22 22:22:19 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 2.0002
2022-08-22 22:22:53 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.1668
2022-08-22 22:23:26 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 1.9968
2022-08-22 22:23:59 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.9716
2022-08-22 22:24:33 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.0167
2022-08-22 22:25:07 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.3095
2022-08-22 22:25:40 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.7659
2022-08-22 22:26:13 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.1557
2022-08-22 22:26:46 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.8355
2022-08-22 22:27:20 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.1108
2022-08-22 22:27:53 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.1937
2022-08-22 22:28:26 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.1015
2022-08-22 22:28:59 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.9072
2022-08-22 22:29:33 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 1.9489
2022-08-22 22:30:06 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.2066
2022-08-22 22:30:39 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.9911
2022-08-22 22:31:12 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.9443
2022-08-22 22:31:45 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.9694
2022-08-22 22:32:19 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.0959
2022-08-22 22:32:52 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.0954
2022-08-22 22:33:25 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.8090
2022-08-22 22:33:58 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.9341
2022-08-22 22:34:31 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 2.1521
2022-08-22 22:35:05 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 2.0301
2022-08-22 22:35:38 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.9954
2022-08-22 22:36:11 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.9570
2022-08-22 22:36:44 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.2068
2022-08-22 22:37:18 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.7200
2022-08-22 22:37:51 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1774
2022-08-22 22:38:24 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.2081
2022-08-22 22:38:58 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 2.0481
2022-08-22 22:39:32 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.9379
2022-08-22 22:40:05 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 1.9777
2022-08-22 22:40:39 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.8962
2022-08-22 22:41:12 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.9841
2022-08-22 22:41:45 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 2.0074
2022-08-22 22:42:18 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.1428
2022-08-22 22:42:52 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 2.0289
2022-08-22 22:43:25 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.0130
2022-08-22 22:43:58 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0685
2022-08-22 22:44:31 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.2389
2022-08-22 22:44:33 - train: epoch 013, train_loss: 2.0415
2022-08-22 22:45:48 - eval: epoch: 013, acc1: 59.600%, acc5: 83.168%, test_loss: 1.7032, per_image_load_time: 2.200ms, per_image_inference_time: 0.626ms
2022-08-22 22:45:49 - until epoch: 013, best_acc1: 59.600%
2022-08-22 22:45:49 - epoch 014 lr: 0.046859
2022-08-22 22:46:29 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.8243
2022-08-22 22:47:02 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 2.1406
2022-08-22 22:47:35 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.8352
2022-08-22 22:48:08 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.9111
2022-08-22 22:48:41 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.8721
2022-08-22 22:49:14 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 2.1236
2022-08-22 22:49:47 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.9019
2022-08-22 22:50:21 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.8095
2022-08-22 22:50:54 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 2.0492
2022-08-22 22:51:27 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.1380
2022-08-22 22:52:00 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.9682
2022-08-22 22:52:33 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 1.9549
2022-08-22 22:53:06 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.0509
2022-08-22 22:53:39 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 2.0701
2022-08-22 22:54:12 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.1339
2022-08-22 22:54:45 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 2.0203
2022-08-22 22:55:18 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9958
2022-08-22 22:55:52 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.2013
2022-08-22 22:56:25 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.9144
2022-08-22 22:56:58 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.8920
2022-08-22 22:57:32 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.0688
2022-08-22 22:58:05 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 2.0842
2022-08-22 22:58:38 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.9827
2022-08-22 22:59:11 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 2.1290
2022-08-22 22:59:44 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.8680
2022-08-22 23:00:17 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.9149
2022-08-22 23:00:51 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.9598
2022-08-22 23:01:24 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 2.0844
2022-08-22 23:01:57 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 2.0297
2022-08-22 23:02:31 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 2.1054
2022-08-22 23:03:04 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 2.0161
2022-08-22 23:03:37 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 2.2133
2022-08-22 23:04:11 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.8784
2022-08-22 23:04:45 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.9831
2022-08-22 23:05:18 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.9856
2022-08-22 23:05:51 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.8572
2022-08-22 23:06:24 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.8032
2022-08-22 23:06:57 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 2.1384
2022-08-22 23:07:30 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 2.0477
2022-08-22 23:08:03 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.9640
2022-08-22 23:08:36 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.8911
2022-08-22 23:09:10 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.9380
2022-08-22 23:09:43 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.7007
2022-08-22 23:10:16 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.8958
2022-08-22 23:10:49 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 2.0795
2022-08-22 23:11:22 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.9270
2022-08-22 23:11:56 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.9628
2022-08-22 23:12:29 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.9610
2022-08-22 23:13:02 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8781
2022-08-22 23:13:35 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 2.0433
2022-08-22 23:13:37 - train: epoch 014, train_loss: 1.9851
2022-08-22 23:14:52 - eval: epoch: 014, acc1: 59.672%, acc5: 83.214%, test_loss: 1.6984, per_image_load_time: 1.894ms, per_image_inference_time: 0.636ms
2022-08-22 23:14:52 - until epoch: 014, best_acc1: 59.672%
2022-08-22 23:14:52 - epoch 015 lr: 0.040630
2022-08-22 23:15:32 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.6217
2022-08-22 23:16:05 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.9477
2022-08-22 23:16:37 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.2258
2022-08-22 23:17:09 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 2.0092
2022-08-22 23:17:42 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.9563
2022-08-22 23:18:15 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 2.1120
2022-08-22 23:18:47 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 2.1530
2022-08-22 23:19:20 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.8853
2022-08-22 23:19:53 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.8563
2022-08-22 23:20:26 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.8557
2022-08-22 23:20:59 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.6467
2022-08-22 23:21:32 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.9888
2022-08-22 23:22:06 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 2.1152
2022-08-22 23:22:39 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.7621
2022-08-22 23:23:12 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.7964
2022-08-22 23:23:45 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.9606
2022-08-22 23:24:18 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 2.0618
2022-08-22 23:24:51 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.8051
2022-08-22 23:25:24 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.8936
2022-08-22 23:25:57 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.7406
2022-08-22 23:26:30 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.6555
2022-08-22 23:27:03 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9407
2022-08-22 23:27:37 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.8902
2022-08-22 23:28:10 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.8876
2022-08-22 23:28:43 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.9659
2022-08-22 23:29:16 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.6214
2022-08-22 23:29:49 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.9447
2022-08-22 23:30:22 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 2.0172
2022-08-22 23:30:55 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 2.0098
2022-08-22 23:31:29 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.7796
2022-08-22 23:32:02 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.9185
2022-08-22 23:32:35 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.9182
2022-08-22 23:33:09 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.8392
2022-08-22 23:33:42 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.9164
2022-08-22 23:34:15 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0409
2022-08-22 23:34:49 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.8803
2022-08-22 23:35:22 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6732
2022-08-22 23:35:56 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.8515
2022-08-22 23:36:29 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8180
2022-08-22 23:37:02 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.9688
2022-08-22 23:37:36 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 2.0042
2022-08-22 23:38:09 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.9838
2022-08-22 23:38:42 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.8424
2022-08-22 23:39:15 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.8690
2022-08-22 23:39:48 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.9205
2022-08-22 23:40:22 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.8823
2022-08-22 23:40:55 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.8297
2022-08-22 23:41:28 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.8214
2022-08-22 23:42:02 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.6202
2022-08-22 23:42:35 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 2.0911
2022-08-22 23:42:36 - train: epoch 015, train_loss: 1.9218
2022-08-22 23:43:51 - eval: epoch: 015, acc1: 61.412%, acc5: 84.286%, test_loss: 1.6172, per_image_load_time: 2.258ms, per_image_inference_time: 0.635ms
2022-08-22 23:43:51 - until epoch: 015, best_acc1: 61.412%
2022-08-22 23:43:51 - epoch 016 lr: 0.034548
2022-08-22 23:44:31 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.8329
2022-08-22 23:45:04 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.5451
2022-08-22 23:45:36 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.7767
2022-08-22 23:46:09 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.1476
2022-08-22 23:46:42 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.6966
2022-08-22 23:47:15 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.7366
2022-08-22 23:47:48 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.6896
2022-08-22 23:48:21 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.8531
2022-08-22 23:48:54 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.7585
2022-08-22 23:49:27 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.7274
2022-08-22 23:50:00 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.7249
2022-08-22 23:50:33 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.7644
2022-08-22 23:51:06 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.8819
2022-08-22 23:51:39 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.9394
2022-08-22 23:52:12 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 2.0540
2022-08-22 23:52:46 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.9175
2022-08-22 23:53:19 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.9076
2022-08-22 23:53:52 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 2.2367
2022-08-22 23:54:26 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.8684
2022-08-22 23:54:59 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.5840
2022-08-22 23:55:32 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8810
2022-08-22 23:56:06 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 2.0320
2022-08-22 23:56:39 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 2.0553
2022-08-22 23:57:12 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.8904
2022-08-22 23:57:45 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.9625
2022-08-22 23:58:18 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 2.0841
2022-08-22 23:58:51 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.6663
2022-08-22 23:59:25 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.7162
2022-08-22 23:59:58 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.7595
