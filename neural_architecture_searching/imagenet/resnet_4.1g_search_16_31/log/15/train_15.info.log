2022-08-23 00:00:31 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.9907
2022-08-23 00:01:05 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.8586
2022-08-23 00:01:38 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.9242
2022-08-23 00:02:11 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9243
2022-08-23 00:02:44 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.8075
2022-08-23 00:03:18 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.8983
2022-08-23 00:03:51 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.7118
2022-08-23 00:04:24 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.7145
2022-08-23 00:04:58 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.1963
2022-08-23 00:05:31 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.7912
2022-08-23 00:06:04 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.8728
2022-08-23 00:06:37 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.6778
2022-08-23 00:07:11 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.6530
2022-08-23 00:07:44 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.6925
2022-08-23 00:08:18 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.6458
2022-08-23 00:08:51 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.8056
2022-08-23 00:09:24 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.8248
2022-08-23 00:09:58 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 2.0058
2022-08-23 00:10:31 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.7862
2022-08-23 00:11:04 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.8497
2022-08-23 00:11:37 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.8278
2022-08-23 00:11:39 - train: epoch 016, train_loss: 1.8593
2022-08-23 00:12:54 - eval: epoch: 016, acc1: 62.218%, acc5: 84.768%, test_loss: 1.5742, per_image_load_time: 1.975ms, per_image_inference_time: 0.659ms
2022-08-23 00:12:54 - until epoch: 016, best_acc1: 62.218%
2022-08-23 00:12:54 - epoch 017 lr: 0.028710
2022-08-23 00:13:34 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 2.0167
2022-08-23 00:14:07 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.8299
2022-08-23 00:14:40 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 2.0160
2022-08-23 00:15:13 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.5331
2022-08-23 00:15:45 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.8845
2022-08-23 00:16:18 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 2.2149
2022-08-23 00:16:51 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.7409
2022-08-23 00:17:23 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.8111
2022-08-23 00:17:56 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.6509
2022-08-23 00:18:29 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.7944
2022-08-23 00:19:02 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 2.0075
2022-08-23 00:19:35 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.6934
2022-08-23 00:20:08 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.8178
2022-08-23 00:20:41 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.6323
2022-08-23 00:21:14 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.6600
2022-08-23 00:21:47 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.5677
2022-08-23 00:22:20 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.6288
2022-08-23 00:22:53 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.8320
2022-08-23 00:23:27 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.6438
2022-08-23 00:24:00 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.5752
2022-08-23 00:24:33 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.7146
2022-08-23 00:25:06 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.5771
2022-08-23 00:25:39 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.7651
2022-08-23 00:26:12 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.8059
2022-08-23 00:26:45 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.8030
2022-08-23 00:27:19 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.7161
2022-08-23 00:27:51 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.7587
2022-08-23 00:28:24 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.7941
2022-08-23 00:28:57 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.9646
2022-08-23 00:29:30 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.6885
2022-08-23 00:30:03 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8027
2022-08-23 00:30:36 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.7650
2022-08-23 00:31:09 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8792
2022-08-23 00:31:42 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.8309
2022-08-23 00:32:15 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.8902
2022-08-23 00:32:47 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.9616
2022-08-23 00:33:20 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.6911
2022-08-23 00:33:53 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.8805
2022-08-23 00:34:26 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.6869
2022-08-23 00:34:58 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.6310
2022-08-23 00:35:31 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.8519
2022-08-23 00:36:04 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.6160
2022-08-23 00:36:37 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.6677
2022-08-23 00:37:09 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.7430
2022-08-23 00:37:42 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.9078
2022-08-23 00:38:15 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.7537
2022-08-23 00:38:48 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.7812
2022-08-23 00:39:21 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.9008
2022-08-23 00:39:54 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.6335
2022-08-23 00:40:26 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.5601
2022-08-23 00:40:27 - train: epoch 017, train_loss: 1.7931
2022-08-23 00:41:42 - eval: epoch: 017, acc1: 64.248%, acc5: 86.106%, test_loss: 1.4923, per_image_load_time: 2.079ms, per_image_inference_time: 0.618ms
2022-08-23 00:41:43 - until epoch: 017, best_acc1: 64.248%
2022-08-23 00:41:43 - epoch 018 lr: 0.023208
2022-08-23 00:42:23 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.8169
2022-08-23 00:42:56 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.9227
2022-08-23 00:43:29 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.9659
2022-08-23 00:44:02 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.9850
2022-08-23 00:44:35 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.7190
2022-08-23 00:45:08 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.8700
2022-08-23 00:45:40 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.4771
2022-08-23 00:46:13 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.8928
2022-08-23 00:46:45 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.8239
2022-08-23 00:47:18 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.5523
2022-08-23 00:47:51 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.7686
2022-08-23 00:48:24 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.6440
2022-08-23 00:48:56 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.7577
2022-08-23 00:49:29 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.7625
2022-08-23 00:50:02 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.7610
2022-08-23 00:50:34 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.6136
2022-08-23 00:51:07 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7139
2022-08-23 00:51:40 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6619
2022-08-23 00:52:12 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.6913
2022-08-23 00:52:44 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 2.0871
2022-08-23 00:53:17 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 2.0650
2022-08-23 00:53:49 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.8929
2022-08-23 00:54:22 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.7922
2022-08-23 00:54:55 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.5921
2022-08-23 00:55:27 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.4815
2022-08-23 00:56:00 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.8197
2022-08-23 00:56:33 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.8890
2022-08-23 00:57:06 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.7817
2022-08-23 00:57:38 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.7703
2022-08-23 00:58:11 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.7344
2022-08-23 00:58:44 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.9408
2022-08-23 00:59:16 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.5447
2022-08-23 00:59:49 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.5631
2022-08-23 01:00:22 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.7728
2022-08-23 01:00:55 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.9974
2022-08-23 01:01:28 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.7834
2022-08-23 01:02:01 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.9946
2022-08-23 01:02:34 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 2.0862
2022-08-23 01:03:07 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.6702
2022-08-23 01:03:40 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.8435
2022-08-23 01:04:13 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.8145
2022-08-23 01:04:46 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.6485
2022-08-23 01:05:19 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.6744
2022-08-23 01:05:52 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.8188
2022-08-23 01:06:25 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.6741
2022-08-23 01:06:58 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.7968
2022-08-23 01:07:31 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.9969
2022-08-23 01:08:04 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.7011
2022-08-23 01:08:37 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.5190
2022-08-23 01:09:10 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.8066
2022-08-23 01:09:11 - train: epoch 018, train_loss: 1.7242
2022-08-23 01:10:27 - eval: epoch: 018, acc1: 65.260%, acc5: 86.832%, test_loss: 1.4388, per_image_load_time: 1.644ms, per_image_inference_time: 0.663ms
2022-08-23 01:10:27 - until epoch: 018, best_acc1: 65.260%
2022-08-23 01:10:27 - epoch 019 lr: 0.018128
2022-08-23 01:11:07 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.6598
2022-08-23 01:11:40 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.6780
2022-08-23 01:12:13 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.6197
2022-08-23 01:12:47 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.7965
2022-08-23 01:13:20 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.6261
2022-08-23 01:13:53 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.6830
2022-08-23 01:14:26 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.5635
2022-08-23 01:14:59 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.7475
2022-08-23 01:15:32 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.6034
2022-08-23 01:16:05 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.6696
2022-08-23 01:16:38 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.6636
2022-08-23 01:17:12 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.7477
2022-08-23 01:17:45 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.6875
2022-08-23 01:18:18 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.5333
2022-08-23 01:18:51 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.9821
2022-08-23 01:19:24 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.4503
2022-08-23 01:19:58 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.7008
2022-08-23 01:20:31 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.7111
2022-08-23 01:21:04 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.8697
2022-08-23 01:21:38 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.6313
2022-08-23 01:22:11 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.6399
2022-08-23 01:22:45 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.7145
2022-08-23 01:23:18 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.6149
2022-08-23 01:23:51 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.6983
2022-08-23 01:24:25 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.6723
2022-08-23 01:24:58 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.7409
2022-08-23 01:25:31 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.5198
2022-08-23 01:26:04 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.8118
2022-08-23 01:26:37 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.7022
2022-08-23 01:27:11 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.8328
2022-08-23 01:27:44 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.6880
2022-08-23 01:28:17 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.3652
2022-08-23 01:28:51 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.4667
2022-08-23 01:29:24 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.6253
2022-08-23 01:29:57 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.8753
2022-08-23 01:30:30 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.3533
2022-08-23 01:31:04 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.5393
2022-08-23 01:31:37 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.8708
2022-08-23 01:32:10 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.4241
2022-08-23 01:32:44 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.5378
2022-08-23 01:33:17 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.6238
2022-08-23 01:33:50 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.6422
2022-08-23 01:34:24 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.5427
2022-08-23 01:34:57 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.5921
2022-08-23 01:35:31 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.8998
2022-08-23 01:36:04 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.5840
2022-08-23 01:36:37 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.7016
2022-08-23 01:37:11 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.7416
2022-08-23 01:37:44 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.4448
2022-08-23 01:38:17 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.5303
2022-08-23 01:38:19 - train: epoch 019, train_loss: 1.6517
2022-08-23 01:39:35 - eval: epoch: 019, acc1: 66.268%, acc5: 87.456%, test_loss: 1.3875, per_image_load_time: 0.839ms, per_image_inference_time: 0.656ms
2022-08-23 01:39:35 - until epoch: 019, best_acc1: 66.268%
2022-08-23 01:39:35 - epoch 020 lr: 0.013551
2022-08-23 01:40:15 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.7180
2022-08-23 01:40:48 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.5168
2022-08-23 01:41:21 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.6688
2022-08-23 01:41:54 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.5041
2022-08-23 01:42:27 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.4718
2022-08-23 01:43:00 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.7300
2022-08-23 01:43:33 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.3803
2022-08-23 01:44:06 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.6267
2022-08-23 01:44:39 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.7707
2022-08-23 01:45:12 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.6622
2022-08-23 01:45:45 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.4533
2022-08-23 01:46:18 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.4784
2022-08-23 01:46:51 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.6506
2022-08-23 01:47:24 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.6108
2022-08-23 01:47:57 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.5978
2022-08-23 01:48:31 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.7063
2022-08-23 01:49:04 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.8054
2022-08-23 01:49:37 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.5785
2022-08-23 01:50:10 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.3509
2022-08-23 01:50:43 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.5414
2022-08-23 01:51:16 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.8393
2022-08-23 01:51:50 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.5380
2022-08-23 01:52:23 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.3806
2022-08-23 01:52:56 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.7515
2022-08-23 01:53:30 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.5864
2022-08-23 01:54:03 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.6717
2022-08-23 01:54:36 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.5790
2022-08-23 01:55:10 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.6209
2022-08-23 01:55:43 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.7314
2022-08-23 01:56:16 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.7389
2022-08-23 01:56:50 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.6555
2022-08-23 01:57:23 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.5531
2022-08-23 01:57:56 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.5936
2022-08-23 01:58:30 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.6206
2022-08-23 01:59:03 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.5773
2022-08-23 01:59:37 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.6939
2022-08-23 02:00:10 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.4927
2022-08-23 02:00:44 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.6274
2022-08-23 02:01:18 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.8933
2022-08-23 02:01:51 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.4911
2022-08-23 02:02:24 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.6011
2022-08-23 02:02:58 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.4642
2022-08-23 02:03:31 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.6411
2022-08-23 02:04:05 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.4358
2022-08-23 02:04:38 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.6315
2022-08-23 02:05:12 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.4849
2022-08-23 02:05:45 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.5561
2022-08-23 02:06:19 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.6074
2022-08-23 02:06:53 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.6295
2022-08-23 02:07:26 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.5240
2022-08-23 02:07:28 - train: epoch 020, train_loss: 1.5836
2022-08-23 02:08:43 - eval: epoch: 020, acc1: 68.240%, acc5: 88.548%, test_loss: 1.2999, per_image_load_time: 0.690ms, per_image_inference_time: 0.627ms
2022-08-23 02:08:43 - until epoch: 020, best_acc1: 68.240%
2022-08-23 02:08:43 - epoch 021 lr: 0.009548
2022-08-23 02:09:23 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.5526
2022-08-23 02:09:56 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.6231
2022-08-23 02:10:30 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.4225
2022-08-23 02:11:04 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4674
2022-08-23 02:11:37 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.3566
2022-08-23 02:12:11 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.4914
2022-08-23 02:12:45 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3934
2022-08-23 02:13:20 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.7039
2022-08-23 02:13:53 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.4531
2022-08-23 02:14:27 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3666
2022-08-23 02:15:01 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.4617
2022-08-23 02:15:36 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.6027
2022-08-23 02:16:10 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.4410
2022-08-23 02:16:44 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.4667
2022-08-23 02:17:18 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.4769
2022-08-23 02:17:53 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.4673
2022-08-23 02:18:27 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5941
2022-08-23 02:19:01 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.3248
2022-08-23 02:19:35 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.7594
2022-08-23 02:20:09 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.6726
2022-08-23 02:20:43 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.5348
2022-08-23 02:21:17 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.3960
2022-08-23 02:21:51 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.3897
2022-08-23 02:22:25 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.4316
2022-08-23 02:22:59 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.6174
2022-08-23 02:23:34 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.7041
2022-08-23 02:24:08 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.5214
2022-08-23 02:24:42 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.5313
2022-08-23 02:25:16 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.4860
2022-08-23 02:25:50 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.6613
2022-08-23 02:26:24 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.4831
2022-08-23 02:26:58 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.3080
2022-08-23 02:27:32 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.7034
2022-08-23 02:28:06 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.7443
2022-08-23 02:28:41 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4731
2022-08-23 02:29:15 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.4391
2022-08-23 02:29:49 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.4352
2022-08-23 02:30:23 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.6279
2022-08-23 02:30:58 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.5289
2022-08-23 02:31:32 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.8117
2022-08-23 02:32:06 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.4350
2022-08-23 02:32:40 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.5730
2022-08-23 02:33:14 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.4147
2022-08-23 02:33:48 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.5985
2022-08-23 02:34:23 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.5685
2022-08-23 02:34:57 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.4034
2022-08-23 02:35:31 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.6172
2022-08-23 02:36:06 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.6869
2022-08-23 02:36:40 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.5076
2022-08-23 02:37:14 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.3962
2022-08-23 02:37:15 - train: epoch 021, train_loss: 1.5122
2022-08-23 02:38:32 - eval: epoch: 021, acc1: 69.000%, acc5: 89.016%, test_loss: 1.2585, per_image_load_time: 2.337ms, per_image_inference_time: 0.607ms
2022-08-23 02:38:32 - until epoch: 021, best_acc1: 69.000%
2022-08-23 02:38:32 - epoch 022 lr: 0.006184
2022-08-23 02:39:12 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.2156
2022-08-23 02:39:45 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.4740
2022-08-23 02:40:20 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.3872
2022-08-23 02:40:54 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.3606
2022-08-23 02:41:27 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3935
2022-08-23 02:42:01 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.7284
2022-08-23 02:42:35 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4796
2022-08-23 02:43:09 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.5364
2022-08-23 02:43:43 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.4706
2022-08-23 02:44:18 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.5356
2022-08-23 02:44:52 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3500
2022-08-23 02:45:26 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.2858
2022-08-23 02:46:00 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.3370
2022-08-23 02:46:34 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.3988
2022-08-23 02:47:08 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.4427
2022-08-23 02:47:42 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.3847
2022-08-23 02:48:16 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.3854
2022-08-23 02:48:50 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.7812
2022-08-23 02:49:24 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.5565
2022-08-23 02:49:59 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4673
2022-08-23 02:50:32 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.4413
2022-08-23 02:51:07 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.1712
2022-08-23 02:51:41 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.4968
2022-08-23 02:52:15 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.5809
2022-08-23 02:52:49 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.3987
2022-08-23 02:53:24 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.3787
2022-08-23 02:53:58 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.3098
2022-08-23 02:54:32 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.7339
2022-08-23 02:55:06 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.2046
2022-08-23 02:55:40 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.4993
2022-08-23 02:56:15 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.5007
2022-08-23 02:56:49 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.5333
2022-08-23 02:57:23 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.4479
2022-08-23 02:57:58 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.3103
2022-08-23 02:58:32 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.5833
2022-08-23 02:59:06 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.4107
2022-08-23 02:59:40 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.3610
2022-08-23 03:00:15 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.6133
2022-08-23 03:00:50 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.3872
2022-08-23 03:01:24 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.5445
2022-08-23 03:01:58 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.4652
2022-08-23 03:02:33 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.4094
2022-08-23 03:03:07 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.7369
2022-08-23 03:03:41 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.5295
2022-08-23 03:04:15 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.2800
2022-08-23 03:04:49 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.7085
2022-08-23 03:05:24 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.5610
2022-08-23 03:05:58 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.3071
2022-08-23 03:06:32 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.5339
2022-08-23 03:07:06 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.3556
2022-08-23 03:07:07 - train: epoch 022, train_loss: 1.4462
2022-08-23 03:08:24 - eval: epoch: 022, acc1: 70.094%, acc5: 89.648%, test_loss: 1.2177, per_image_load_time: 2.374ms, per_image_inference_time: 0.630ms
2022-08-23 03:08:24 - until epoch: 022, best_acc1: 70.094%
2022-08-23 03:08:24 - epoch 023 lr: 0.003511
2022-08-23 03:09:05 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.2695
2022-08-23 03:09:39 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.3516
2022-08-23 03:10:12 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.4529
2022-08-23 03:10:46 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.3580
2022-08-23 03:11:20 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.6398
2022-08-23 03:11:54 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.2284
2022-08-23 03:12:28 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.3002
2022-08-23 03:13:02 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.4564
2022-08-23 03:13:36 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.7052
2022-08-23 03:14:10 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.4138
2022-08-23 03:14:44 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.4580
2022-08-23 03:15:18 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.4103
2022-08-23 03:15:52 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.4060
2022-08-23 03:16:26 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.4166
2022-08-23 03:17:00 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.5343
2022-08-23 03:17:34 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.4673
2022-08-23 03:18:08 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.3867
2022-08-23 03:18:42 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.3396
2022-08-23 03:19:16 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.3922
2022-08-23 03:19:50 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.2566
2022-08-23 03:20:24 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.3938
2022-08-23 03:20:58 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.4361
2022-08-23 03:21:33 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.4274
2022-08-23 03:22:07 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.4797
2022-08-23 03:22:41 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.3529
2022-08-23 03:23:16 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.4799
2022-08-23 03:23:50 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.3834
2022-08-23 03:24:24 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.3787
2022-08-23 03:24:58 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.3357
2022-08-23 03:25:33 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.3139
2022-08-23 03:26:07 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.3868
2022-08-23 03:26:42 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.4439
2022-08-23 03:27:15 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.5114
2022-08-23 03:27:50 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.4324
2022-08-23 03:28:24 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.3071
2022-08-23 03:28:58 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.3515
2022-08-23 03:29:32 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.3624
2022-08-23 03:30:07 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.4428
2022-08-23 03:30:41 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3859
2022-08-23 03:31:15 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2225
2022-08-23 03:31:49 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.2966
2022-08-23 03:32:23 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.2573
2022-08-23 03:32:58 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.2739
2022-08-23 03:33:32 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.2732
2022-08-23 03:34:06 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.1869
2022-08-23 03:34:40 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.4344
2022-08-23 03:35:15 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.4061
2022-08-23 03:35:48 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.3167
2022-08-23 03:36:23 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.1707
2022-08-23 03:36:56 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.4411
2022-08-23 03:36:58 - train: epoch 023, train_loss: 1.3954
2022-08-23 03:38:15 - eval: epoch: 023, acc1: 70.924%, acc5: 89.986%, test_loss: 1.1855, per_image_load_time: 1.637ms, per_image_inference_time: 0.632ms
2022-08-23 03:38:15 - until epoch: 023, best_acc1: 70.924%
2022-08-23 03:38:15 - epoch 024 lr: 0.001571
2022-08-23 03:38:55 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.4101
2022-08-23 03:39:29 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.3380
2022-08-23 03:40:02 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.3081
2022-08-23 03:40:36 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.3457
2022-08-23 03:41:10 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.3141
2022-08-23 03:41:44 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.2423
2022-08-23 03:42:18 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.3177
2022-08-23 03:42:51 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.2182
2022-08-23 03:43:25 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.4181
2022-08-23 03:43:58 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.2984
2022-08-23 03:44:32 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.1848
2022-08-23 03:45:06 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.3644
2022-08-23 03:45:40 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4652
2022-08-23 03:46:14 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.3897
2022-08-23 03:46:48 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.5177
2022-08-23 03:47:22 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.2911
2022-08-23 03:47:56 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.2160
2022-08-23 03:48:30 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.5112
2022-08-23 03:49:04 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.0964
2022-08-23 03:49:38 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.5104
2022-08-23 03:50:12 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.3405
2022-08-23 03:50:46 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.3625
2022-08-23 03:51:20 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.3912
2022-08-23 03:51:55 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.4090
2022-08-23 03:52:29 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.4052
2022-08-23 03:53:03 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.5467
2022-08-23 03:53:37 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.6147
2022-08-23 03:54:12 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.4341
2022-08-23 03:54:46 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.4250
2022-08-23 03:55:20 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.3990
2022-08-23 03:55:54 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.3377
2022-08-23 03:56:28 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.4677
2022-08-23 03:57:03 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.2042
2022-08-23 03:57:37 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.3098
2022-08-23 03:58:11 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.4036
2022-08-23 03:58:45 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.4765
2022-08-23 03:59:19 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.3840
2022-08-23 03:59:54 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.5137
2022-08-23 04:00:28 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.4581
2022-08-23 04:01:03 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.3491
2022-08-23 04:01:36 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.2186
2022-08-23 04:02:11 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.2370
2022-08-23 04:02:45 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.4101
2022-08-23 04:03:19 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.2844
2022-08-23 04:03:54 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.2991
2022-08-23 04:04:28 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.3355
2022-08-23 04:05:02 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.4020
2022-08-23 04:05:36 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.1541
2022-08-23 04:06:10 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.2771
2022-08-23 04:06:43 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.3577
2022-08-23 04:06:45 - train: epoch 024, train_loss: 1.3580
2022-08-23 04:08:01 - eval: epoch: 024, acc1: 71.200%, acc5: 90.284%, test_loss: 1.1697, per_image_load_time: 0.493ms, per_image_inference_time: 0.593ms
2022-08-23 04:08:01 - until epoch: 024, best_acc1: 71.200%
2022-08-23 04:08:01 - epoch 025 lr: 0.000394
2022-08-23 04:08:41 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.1093
2022-08-23 04:09:15 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.2752
2022-08-23 04:09:49 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.3137
2022-08-23 04:10:23 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.3595
2022-08-23 04:10:57 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.0249
2022-08-23 04:11:31 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3633
2022-08-23 04:12:05 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.2838
2022-08-23 04:12:38 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.2442
2022-08-23 04:13:12 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.1447
2022-08-23 04:13:47 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.4652
2022-08-23 04:14:20 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.3766
2022-08-23 04:14:54 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.2983
2022-08-23 04:15:28 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3989
2022-08-23 04:16:02 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.2741
2022-08-23 04:16:36 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.4075
2022-08-23 04:17:10 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.0950
2022-08-23 04:17:44 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.4731
2022-08-23 04:18:18 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.0931
2022-08-23 04:18:52 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.4100
2022-08-23 04:19:26 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.3556
2022-08-23 04:20:00 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.0585
2022-08-23 04:20:34 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.2796
2022-08-23 04:21:09 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.2794
2022-08-23 04:21:43 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.1949
2022-08-23 04:22:17 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.1774
2022-08-23 04:22:51 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.4249
2022-08-23 04:23:25 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.3619
2022-08-23 04:24:00 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.2867
2022-08-23 04:24:34 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.3391
2022-08-23 04:25:08 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.3284
2022-08-23 04:25:42 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.3304
2022-08-23 04:26:16 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.3705
2022-08-23 04:26:50 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.2907
2022-08-23 04:27:24 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.4849
2022-08-23 04:27:58 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.2341
2022-08-23 04:28:32 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.4266
2022-08-23 04:29:06 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.3532
2022-08-23 04:29:40 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3621
2022-08-23 04:30:14 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.3973
2022-08-23 04:30:48 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.3916
2022-08-23 04:31:22 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.5218
2022-08-23 04:31:56 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.3267
2022-08-23 04:32:30 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.0643
2022-08-23 04:33:04 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.4156
2022-08-23 04:33:38 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.2961
2022-08-23 04:34:12 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.1867
2022-08-23 04:34:46 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.2411
2022-08-23 04:35:20 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.2192
2022-08-23 04:35:54 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.3510
2022-08-23 04:36:27 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.5228
2022-08-23 04:36:29 - train: epoch 025, train_loss: 1.3377
2022-08-23 04:37:46 - eval: epoch: 025, acc1: 71.286%, acc5: 90.300%, test_loss: 1.1661, per_image_load_time: 0.659ms, per_image_inference_time: 0.626ms
2022-08-23 04:37:46 - until epoch: 025, best_acc1: 71.286%
2022-08-23 04:37:46 - train done. train time: 12.168 hours, best_acc1: 71.286%
