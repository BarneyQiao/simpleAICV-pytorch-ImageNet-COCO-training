2022-08-17 12:12:31 - net_idx: 5
2022-08-17 12:12:31 - net_config: {'stem_width': 64, 'depth': 14, 'w_0': 40, 'w_a': 17.19122406546614, 'w_m': 1.8415927382057373}
2022-08-17 12:12:31 - num_classes: 1000
2022-08-17 12:12:31 - input_image_size: 224
2022-08-17 12:12:31 - scale: 1.1428571428571428
2022-08-17 12:12:31 - seed: 0
2022-08-17 12:12:31 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-17 12:12:31 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-17 12:12:31 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c2100>
2022-08-17 12:12:31 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c23d0>
2022-08-17 12:12:31 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2400>
2022-08-17 12:12:31 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2460>
2022-08-17 12:12:31 - batch_size: 256
2022-08-17 12:12:31 - num_workers: 16
2022-08-17 12:12:31 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-17 12:12:31 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-17 12:12:31 - epochs: 25
2022-08-17 12:12:31 - print_interval: 100
2022-08-17 12:12:31 - accumulation_steps: 1
2022-08-17 12:12:31 - sync_bn: False
2022-08-17 12:12:31 - apex: True
2022-08-17 12:12:31 - use_ema_model: False
2022-08-17 12:12:31 - ema_model_decay: 0.9999
2022-08-17 12:12:31 - log_dir: ./log
2022-08-17 12:12:31 - checkpoint_dir: ./checkpoints
2022-08-17 12:12:31 - gpus_type: NVIDIA RTX A5000
2022-08-17 12:12:31 - gpus_num: 2
2022-08-17 12:12:31 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f4efca864b0>
2022-08-17 12:12:31 - ema_model: None
2022-08-17 12:12:31 - --------------------parameters--------------------
2022-08-17 12:12:31 - name: conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-17 12:12:31 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-17 12:12:31 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-17 12:12:31 - name: fc.weight, grad: True
2022-08-17 12:12:31 - name: fc.bias, grad: True
2022-08-17 12:12:31 - --------------------buffers--------------------
2022-08-17 12:12:31 - name: conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-17 12:12:31 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-17 12:12:31 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-17 12:12:31 - -----------no weight decay layers--------------
2022-08-17 12:12:31 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-17 12:12:31 - -------------weight decay layers---------------
2022-08-17 12:12:31 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-17 12:12:31 - epoch 001 lr: 0.100000
2022-08-17 12:13:12 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9146
2022-08-17 12:13:45 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9078
2022-08-17 12:14:18 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8669
2022-08-17 12:14:52 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.7788
2022-08-17 12:15:26 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.6951
2022-08-17 12:15:59 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.4644
2022-08-17 12:16:33 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.5888
2022-08-17 12:17:07 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.4059
2022-08-17 12:17:41 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.3665
2022-08-17 12:18:14 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.3662
2022-08-17 12:18:48 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.2457
2022-08-17 12:19:22 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.1052
2022-08-17 12:19:55 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 5.9329
2022-08-17 12:20:29 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 5.8852
2022-08-17 12:21:03 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 5.7950
2022-08-17 12:21:37 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 5.7529
2022-08-17 12:22:11 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.6191
2022-08-17 12:22:45 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.6086
2022-08-17 12:23:18 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.5212
2022-08-17 12:23:52 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.5141
2022-08-17 12:24:26 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.3852
2022-08-17 12:24:59 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.2745
2022-08-17 12:25:33 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.3057
2022-08-17 12:26:07 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.2092
2022-08-17 12:26:41 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.2403
2022-08-17 12:27:15 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.3476
2022-08-17 12:27:49 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.0220
2022-08-17 12:28:23 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.0507
2022-08-17 12:28:57 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 4.8834
2022-08-17 12:29:31 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.0192
2022-08-17 12:30:05 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 4.9626
2022-08-17 12:30:39 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.0203
2022-08-17 12:31:13 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.7371
2022-08-17 12:31:47 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.8875
2022-08-17 12:32:20 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.6713
2022-08-17 12:32:54 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.6592
2022-08-17 12:33:28 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.7285
2022-08-17 12:34:02 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.4686
2022-08-17 12:34:36 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.4272
2022-08-17 12:35:10 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.5386
2022-08-17 12:35:43 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.5946
2022-08-17 12:36:17 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.4973
2022-08-17 12:36:51 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.3993
2022-08-17 12:37:25 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.1316
2022-08-17 12:37:59 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.2921
2022-08-17 12:38:33 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.4832
2022-08-17 12:39:07 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.1285
2022-08-17 12:39:41 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.3939
2022-08-17 12:40:14 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.3545
2022-08-17 12:40:47 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.1383
2022-08-17 12:40:49 - train: epoch 001, train_loss: 5.3268
2022-08-17 12:42:05 - eval: epoch: 001, acc1: 19.180%, acc5: 42.018%, test_loss: 4.0565, per_image_load_time: 2.339ms, per_image_inference_time: 0.608ms
2022-08-17 12:42:05 - until epoch: 001, best_acc1: 19.180%
2022-08-17 12:42:05 - epoch 002 lr: 0.099606
2022-08-17 12:42:47 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 3.9796
2022-08-17 12:43:21 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.0733
2022-08-17 12:43:54 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.0763
2022-08-17 12:44:28 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.1562
2022-08-17 12:45:01 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.0521
2022-08-17 12:45:35 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 3.8537
2022-08-17 12:46:08 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.1552
2022-08-17 12:46:42 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 3.8274
2022-08-17 12:47:16 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.7940
2022-08-17 12:47:49 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 3.9752
2022-08-17 12:48:23 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.0312
2022-08-17 12:48:56 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.7342
2022-08-17 12:49:30 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.7233
2022-08-17 12:50:04 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 3.9346
2022-08-17 12:50:38 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 3.9190
2022-08-17 12:51:12 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.6863
2022-08-17 12:51:46 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.8380
2022-08-17 12:52:20 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.7256
2022-08-17 12:52:54 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.6830
2022-08-17 12:53:28 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.5652
2022-08-17 12:54:02 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.7940
2022-08-17 12:54:36 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.5731
2022-08-17 12:55:10 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.6241
2022-08-17 12:55:44 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.5731
2022-08-17 12:56:18 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.4554
2022-08-17 12:56:51 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.4885
2022-08-17 12:57:25 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.6871
2022-08-17 12:57:59 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.7639
2022-08-17 12:58:32 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.6168
2022-08-17 12:59:06 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.4844
2022-08-17 12:59:40 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.4880
2022-08-17 13:00:14 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.5100
2022-08-17 13:00:48 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.5277
2022-08-17 13:01:22 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.5301
2022-08-17 13:01:56 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.2376
2022-08-17 13:02:30 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.2952
2022-08-17 13:03:04 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.6641
2022-08-17 13:03:38 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.3122
2022-08-17 13:04:12 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.2855
2022-08-17 13:04:46 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.1962
2022-08-17 13:05:20 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.6073
2022-08-17 13:05:54 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.4181
2022-08-17 13:06:28 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.3753
2022-08-17 13:07:02 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.3160
2022-08-17 13:07:36 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.2224
2022-08-17 13:08:10 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.1572
2022-08-17 13:08:44 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.2279
2022-08-17 13:09:18 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.1706
2022-08-17 13:09:51 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.3051
2022-08-17 13:10:25 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.2914
2022-08-17 13:10:26 - train: epoch 002, train_loss: 3.6182
2022-08-17 13:11:42 - eval: epoch: 002, acc1: 33.644%, acc5: 60.308%, test_loss: 3.1454, per_image_load_time: 1.005ms, per_image_inference_time: 0.596ms
2022-08-17 13:11:42 - until epoch: 002, best_acc1: 33.644%
2022-08-17 13:11:42 - epoch 003 lr: 0.098429
2022-08-17 13:12:24 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.2154
2022-08-17 13:12:58 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.3514
2022-08-17 13:13:31 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.1669
2022-08-17 13:14:05 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.2417
2022-08-17 13:14:38 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.2969
2022-08-17 13:15:12 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 2.9858
2022-08-17 13:15:45 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.4256
2022-08-17 13:16:19 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.3146
2022-08-17 13:16:52 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.2051
2022-08-17 13:17:26 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.1842
2022-08-17 13:18:00 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.1254
2022-08-17 13:18:33 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.0821
2022-08-17 13:19:07 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.0302
2022-08-17 13:19:41 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 2.9052
2022-08-17 13:20:14 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.3287
2022-08-17 13:20:48 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.0904
2022-08-17 13:21:22 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 2.9805
2022-08-17 13:21:56 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 2.8732
2022-08-17 13:22:29 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.0482
2022-08-17 13:23:03 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 2.8501
2022-08-17 13:23:37 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.1790
2022-08-17 13:24:10 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.5020
2022-08-17 13:24:44 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.7511
2022-08-17 13:25:18 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 2.9267
2022-08-17 13:25:51 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.1162
2022-08-17 13:26:25 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.0609
2022-08-17 13:26:59 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.3117
2022-08-17 13:27:33 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 2.8897
2022-08-17 13:28:07 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 2.9668
2022-08-17 13:28:41 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.1373
2022-08-17 13:29:14 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.4102
2022-08-17 13:29:48 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 2.9398
2022-08-17 13:30:22 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0021
2022-08-17 13:30:56 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.1888
2022-08-17 13:31:30 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.7179
2022-08-17 13:32:04 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.8047
2022-08-17 13:32:37 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.1112
2022-08-17 13:33:11 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.0397
2022-08-17 13:33:45 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.3258
2022-08-17 13:34:19 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7076
2022-08-17 13:34:53 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.7468
2022-08-17 13:35:26 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 2.9316
2022-08-17 13:36:00 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.6852
2022-08-17 13:36:34 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.8151
2022-08-17 13:37:08 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.0683
2022-08-17 13:37:42 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.8702
2022-08-17 13:38:15 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 2.7643
2022-08-17 13:38:49 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.0633
2022-08-17 13:39:23 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.0688
2022-08-17 13:39:56 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.8937
2022-08-17 13:39:57 - train: epoch 003, train_loss: 3.0339
2022-08-17 13:41:14 - eval: epoch: 003, acc1: 38.186%, acc5: 64.618%, test_loss: 2.8253, per_image_load_time: 2.219ms, per_image_inference_time: 0.616ms
2022-08-17 13:41:14 - until epoch: 003, best_acc1: 38.186%
2022-08-17 13:41:14 - epoch 004 lr: 0.096488
2022-08-17 13:41:56 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.8802
2022-08-17 13:42:29 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.6295
2022-08-17 13:43:02 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.7323
2022-08-17 13:43:36 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.7154
2022-08-17 13:44:09 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.6788
2022-08-17 13:44:43 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 2.9410
2022-08-17 13:45:17 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.9459
2022-08-17 13:45:50 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.6830
2022-08-17 13:46:24 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.5782
2022-08-17 13:46:57 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.8448
2022-08-17 13:47:31 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.0092
2022-08-17 13:48:04 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.4634
2022-08-17 13:48:38 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.5492
2022-08-17 13:49:11 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.7382
2022-08-17 13:49:45 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.9031
2022-08-17 13:50:19 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.6579
2022-08-17 13:50:53 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.9312
2022-08-17 13:51:27 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.8860
2022-08-17 13:52:01 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.9066
2022-08-17 13:52:34 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.6978
2022-08-17 13:53:08 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.7122
2022-08-17 13:53:42 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.8386
2022-08-17 13:54:16 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.2734
2022-08-17 13:54:50 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.4662
2022-08-17 13:55:23 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.7445
2022-08-17 13:55:57 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.6235
2022-08-17 13:56:31 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.6303
2022-08-17 13:57:05 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8338
2022-08-17 13:57:38 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.7445
2022-08-17 13:58:12 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.7553
2022-08-17 13:58:46 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.7435
2022-08-17 13:59:20 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8004
2022-08-17 13:59:54 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.7964
2022-08-17 14:00:27 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.6017
2022-08-17 14:01:01 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.7839
2022-08-17 14:01:34 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.6184
2022-08-17 14:02:08 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.6050
2022-08-17 14:02:42 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.4531
2022-08-17 14:03:16 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.6571
2022-08-17 14:03:50 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.5669
2022-08-17 14:04:24 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.6588
2022-08-17 14:04:57 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.5934
2022-08-17 14:05:31 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.6233
2022-08-17 14:06:05 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.4261
2022-08-17 14:06:39 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.2816
2022-08-17 14:07:12 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.6418
2022-08-17 14:07:46 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.6817
2022-08-17 14:08:20 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.3987
2022-08-17 14:08:53 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.6008
2022-08-17 14:09:27 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.7323
2022-08-17 14:09:28 - train: epoch 004, train_loss: 2.7420
2022-08-17 14:10:44 - eval: epoch: 004, acc1: 46.612%, acc5: 73.168%, test_loss: 2.3516, per_image_load_time: 2.083ms, per_image_inference_time: 0.588ms
2022-08-17 14:10:44 - until epoch: 004, best_acc1: 46.612%
2022-08-17 14:10:44 - epoch 005 lr: 0.093815
2022-08-17 14:11:26 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.5113
2022-08-17 14:11:59 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.7391
2022-08-17 14:12:32 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.9855
2022-08-17 14:13:06 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.5121
2022-08-17 14:13:39 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.4092
2022-08-17 14:14:13 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.8033
2022-08-17 14:14:47 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.7441
2022-08-17 14:15:20 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.7084
2022-08-17 14:15:54 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.5939
2022-08-17 14:16:28 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.7415
2022-08-17 14:17:01 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.5493
2022-08-17 14:17:35 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.5243
2022-08-17 14:18:09 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.5334
2022-08-17 14:18:43 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.6229
2022-08-17 14:19:16 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.4918
2022-08-17 14:19:50 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.3780
2022-08-17 14:20:24 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.4796
2022-08-17 14:20:57 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.4779
2022-08-17 14:21:31 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.6089
2022-08-17 14:22:05 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.6512
2022-08-17 14:22:39 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.3819
2022-08-17 14:23:13 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.4061
2022-08-17 14:23:47 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.3198
2022-08-17 14:24:21 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5299
2022-08-17 14:24:55 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.4838
2022-08-17 14:25:28 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.6804
2022-08-17 14:26:02 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.6493
2022-08-17 14:26:36 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.5662
2022-08-17 14:27:10 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.4773
2022-08-17 14:27:43 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.6642
2022-08-17 14:28:18 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.6648
2022-08-17 14:28:51 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.6593
2022-08-17 14:29:25 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.4772
2022-08-17 14:29:59 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.5677
2022-08-17 14:30:33 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.5850
2022-08-17 14:31:07 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.4946
2022-08-17 14:31:40 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.5226
2022-08-17 14:32:14 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5136
2022-08-17 14:32:48 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.8251
2022-08-17 14:33:22 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.4160
2022-08-17 14:33:56 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.3689
2022-08-17 14:34:30 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.5028
2022-08-17 14:35:03 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.4154
2022-08-17 14:35:37 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.4714
2022-08-17 14:36:11 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.5481
2022-08-17 14:36:45 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.6031
2022-08-17 14:37:19 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.3035
2022-08-17 14:37:53 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.4968
2022-08-17 14:38:27 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.6541
2022-08-17 14:39:00 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.5738
2022-08-17 14:39:02 - train: epoch 005, train_loss: 2.5646
2022-08-17 14:40:18 - eval: epoch: 005, acc1: 48.726%, acc5: 74.866%, test_loss: 2.2395, per_image_load_time: 2.141ms, per_image_inference_time: 0.608ms
2022-08-17 14:40:19 - until epoch: 005, best_acc1: 48.726%
2022-08-17 14:40:19 - epoch 006 lr: 0.090450
2022-08-17 14:41:00 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.5934
2022-08-17 14:41:34 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.6332
2022-08-17 14:42:07 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.4331
2022-08-17 14:42:40 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.5391
2022-08-17 14:43:13 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.4294
2022-08-17 14:43:47 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.5074
2022-08-17 14:44:20 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.5786
2022-08-17 14:44:54 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.4986
2022-08-17 14:45:28 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.2123
2022-08-17 14:46:02 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.3186
2022-08-17 14:46:35 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.5661
2022-08-17 14:47:09 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.4603
2022-08-17 14:47:43 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.4786
2022-08-17 14:48:17 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.4162
2022-08-17 14:48:51 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.7177
2022-08-17 14:49:24 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3792
2022-08-17 14:49:58 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.5470
2022-08-17 14:50:32 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.2752
2022-08-17 14:51:06 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.4646
2022-08-17 14:51:40 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.5781
2022-08-17 14:52:14 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.3369
2022-08-17 14:52:47 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.2848
2022-08-17 14:53:21 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.1865
2022-08-17 14:53:54 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.3468
2022-08-17 14:54:28 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.3210
2022-08-17 14:55:02 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.2128
2022-08-17 14:55:35 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.5124
2022-08-17 14:56:09 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.0686
2022-08-17 14:56:43 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.4740
2022-08-17 14:57:17 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.5086
2022-08-17 14:57:51 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.2996
2022-08-17 14:58:24 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.3503
2022-08-17 14:58:58 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.2747
2022-08-17 14:59:32 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.6948
2022-08-17 15:00:06 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.6946
2022-08-17 15:00:40 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.6604
2022-08-17 15:01:14 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.5079
2022-08-17 15:01:48 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.2817
2022-08-17 15:02:21 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.3981
2022-08-17 15:02:55 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.5735
2022-08-17 15:03:29 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.3253
2022-08-17 15:04:03 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.1660
2022-08-17 15:04:37 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.4661
2022-08-17 15:05:11 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.3360
2022-08-17 15:05:45 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.3339
2022-08-17 15:06:19 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.7047
2022-08-17 15:06:53 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.5720
2022-08-17 15:07:27 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.6140
2022-08-17 15:08:01 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.2881
2022-08-17 15:08:34 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.1563
2022-08-17 15:08:35 - train: epoch 006, train_loss: 2.4443
2022-08-17 15:09:52 - eval: epoch: 006, acc1: 48.292%, acc5: 74.590%, test_loss: 2.2704, per_image_load_time: 2.311ms, per_image_inference_time: 0.590ms
2022-08-17 15:09:52 - until epoch: 006, best_acc1: 48.726%
2022-08-17 15:09:52 - epoch 007 lr: 0.086448
2022-08-17 15:10:34 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.2649
2022-08-17 15:11:08 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.6956
2022-08-17 15:11:41 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.6506
2022-08-17 15:12:15 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.3560
2022-08-17 15:12:49 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.3092
2022-08-17 15:13:22 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.2711
2022-08-17 15:13:56 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.4018
2022-08-17 15:14:30 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.3788
2022-08-17 15:15:04 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.3645
2022-08-17 15:15:37 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.3782
2022-08-17 15:16:11 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.2793
2022-08-17 15:16:45 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.2011
2022-08-17 15:17:19 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.2161
2022-08-17 15:17:53 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.3009
2022-08-17 15:18:26 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.6889
2022-08-17 15:19:00 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.2563
2022-08-17 15:19:34 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.3289
2022-08-17 15:20:08 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.2618
2022-08-17 15:20:42 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.4206
2022-08-17 15:21:16 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.2104
2022-08-17 15:21:49 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.5853
2022-08-17 15:22:23 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.2302
2022-08-17 15:22:57 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.3108
2022-08-17 15:23:30 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.4503
2022-08-17 15:24:04 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.3782
2022-08-17 15:24:38 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.3384
2022-08-17 15:25:11 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.3693
2022-08-17 15:25:45 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.3859
2022-08-17 15:26:19 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.2632
2022-08-17 15:26:53 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.4792
2022-08-17 15:27:27 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.5614
2022-08-17 15:28:01 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.4041
2022-08-17 15:28:35 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.5051
2022-08-17 15:29:08 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.2958
2022-08-17 15:29:42 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.2528
2022-08-17 15:30:15 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.1096
2022-08-17 15:30:49 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.3536
2022-08-17 15:31:23 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.4785
2022-08-17 15:31:57 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.2330
2022-08-17 15:32:30 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.4811
2022-08-17 15:33:04 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.2104
2022-08-17 15:33:38 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2655
2022-08-17 15:34:12 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.3849
2022-08-17 15:34:46 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.1994
2022-08-17 15:35:19 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.2459
2022-08-17 15:35:53 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.3639
2022-08-17 15:36:27 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.1205
2022-08-17 15:37:01 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.5507
2022-08-17 15:37:35 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.3673
2022-08-17 15:38:09 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.3309
2022-08-17 15:38:10 - train: epoch 007, train_loss: 2.3556
2022-08-17 15:39:26 - eval: epoch: 007, acc1: 52.758%, acc5: 78.350%, test_loss: 2.0289, per_image_load_time: 2.346ms, per_image_inference_time: 0.600ms
2022-08-17 15:39:26 - until epoch: 007, best_acc1: 52.758%
2022-08-17 15:39:26 - epoch 008 lr: 0.081870
2022-08-17 15:40:07 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.2188
2022-08-17 15:40:41 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.5366
2022-08-17 15:41:14 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.1597
2022-08-17 15:41:48 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.2022
2022-08-17 15:42:21 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.1937
2022-08-17 15:42:54 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.1940
2022-08-17 15:43:28 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.3040
2022-08-17 15:44:01 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.1017
2022-08-17 15:44:34 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.2201
2022-08-17 15:45:08 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.1698
2022-08-17 15:45:41 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.2014
2022-08-17 15:46:15 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.0328
2022-08-17 15:46:48 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.4553
2022-08-17 15:47:22 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.3505
2022-08-17 15:47:56 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.5264
2022-08-17 15:48:29 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.2976
2022-08-17 15:49:03 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.2794
2022-08-17 15:49:36 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.3891
2022-08-17 15:50:09 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.1936
2022-08-17 15:50:43 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.3461
2022-08-17 15:51:17 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.3241
2022-08-17 15:51:50 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.1261
2022-08-17 15:52:24 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.2893
2022-08-17 15:52:58 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.0724
2022-08-17 15:53:31 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.2838
2022-08-17 15:54:05 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.3772
2022-08-17 15:54:39 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.4122
2022-08-17 15:55:12 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.2635
2022-08-17 15:55:46 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.1846
2022-08-17 15:56:20 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.5059
2022-08-17 15:56:54 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.2750
2022-08-17 15:57:28 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.3899
2022-08-17 15:58:01 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.4078
2022-08-17 15:58:35 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.1232
2022-08-17 15:59:08 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.2579
2022-08-17 15:59:42 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.3929
2022-08-17 16:00:16 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.3268
2022-08-17 16:00:50 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.0934
2022-08-17 16:01:23 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.2627
2022-08-17 16:01:57 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.5422
2022-08-17 16:02:31 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.0081
2022-08-17 16:03:04 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.1508
2022-08-17 16:03:38 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 1.9656
2022-08-17 16:04:12 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.0509
2022-08-17 16:04:45 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.2525
2022-08-17 16:05:19 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.0234
2022-08-17 16:05:53 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2709
2022-08-17 16:06:25 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.3996
2022-08-17 16:06:58 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.2863
2022-08-17 16:07:31 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.1801
2022-08-17 16:07:33 - train: epoch 008, train_loss: 2.2791
2022-08-17 16:08:49 - eval: epoch: 008, acc1: 53.970%, acc5: 79.226%, test_loss: 1.9651, per_image_load_time: 1.962ms, per_image_inference_time: 0.658ms
2022-08-17 16:08:49 - until epoch: 008, best_acc1: 53.970%
2022-08-17 16:08:49 - epoch 009 lr: 0.076790
2022-08-17 16:09:29 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 1.9170
2022-08-17 16:10:02 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.1344
2022-08-17 16:10:35 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.0867
2022-08-17 16:11:08 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.4858
2022-08-17 16:11:41 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.2104
2022-08-17 16:12:14 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.1894
2022-08-17 16:12:48 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.1012
2022-08-17 16:13:22 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.2603
2022-08-17 16:13:55 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.1015
2022-08-17 16:14:29 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1893
2022-08-17 16:15:03 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.4481
2022-08-17 16:15:37 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.4119
2022-08-17 16:16:11 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.3316
2022-08-17 16:16:45 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 2.1719
2022-08-17 16:17:19 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.1069
2022-08-17 16:17:53 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2252
2022-08-17 16:18:27 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.2035
2022-08-17 16:19:01 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.2050
2022-08-17 16:19:35 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.1150
2022-08-17 16:20:09 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.1051
2022-08-17 16:20:43 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.2802
2022-08-17 16:21:16 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.2404
2022-08-17 16:21:50 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.0963
2022-08-17 16:22:24 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.2482
2022-08-17 16:22:58 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.2628
2022-08-17 16:23:32 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.0640
2022-08-17 16:24:06 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.1257
2022-08-17 16:24:40 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2361
2022-08-17 16:25:14 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.0551
2022-08-17 16:25:47 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1192
2022-08-17 16:26:21 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.2572
2022-08-17 16:26:55 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.1282
2022-08-17 16:27:29 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.1048
2022-08-17 16:28:02 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.2610
2022-08-17 16:28:37 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.3151
2022-08-17 16:29:10 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 1.9235
2022-08-17 16:29:44 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.2535
2022-08-17 16:30:18 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.3760
2022-08-17 16:30:52 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 1.9573
2022-08-17 16:31:26 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.2978
2022-08-17 16:32:00 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.2389
2022-08-17 16:32:34 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 1.9917
2022-08-17 16:33:08 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.0778
2022-08-17 16:33:41 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.2815
2022-08-17 16:34:13 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.1701
2022-08-17 16:34:46 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.3782
2022-08-17 16:35:19 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.2933
2022-08-17 16:35:52 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.1686
2022-08-17 16:36:25 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.3659
2022-08-17 16:36:58 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.0115
2022-08-17 16:36:59 - train: epoch 009, train_loss: 2.2160
2022-08-17 16:38:17 - eval: epoch: 009, acc1: 54.540%, acc5: 79.364%, test_loss: 1.9461, per_image_load_time: 2.244ms, per_image_inference_time: 0.604ms
2022-08-17 16:38:17 - until epoch: 009, best_acc1: 54.540%
2022-08-17 16:38:17 - epoch 010 lr: 0.071288
2022-08-17 16:38:58 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.1140
2022-08-17 16:39:31 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.2311
2022-08-17 16:40:03 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.0014
2022-08-17 16:40:36 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.1881
2022-08-17 16:41:09 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 1.9859
2022-08-17 16:41:43 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.2171
2022-08-17 16:42:16 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.0747
2022-08-17 16:42:49 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.2660
2022-08-17 16:43:22 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.1831
2022-08-17 16:43:54 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 1.9343
2022-08-17 16:44:28 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.1168
2022-08-17 16:45:00 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.1232
2022-08-17 16:45:34 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 1.8853
2022-08-17 16:46:07 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.3225
2022-08-17 16:46:40 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 2.0507
2022-08-17 16:47:13 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.0755
2022-08-17 16:47:46 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.4020
2022-08-17 16:48:19 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 2.2235
2022-08-17 16:48:53 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.0713
2022-08-17 16:49:26 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.1025
2022-08-17 16:49:59 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 2.0654
2022-08-17 16:50:33 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.3266
2022-08-17 16:51:06 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.3368
2022-08-17 16:51:39 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.3910
2022-08-17 16:52:13 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.3054
2022-08-17 16:52:46 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.2667
2022-08-17 16:53:19 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 2.0537
2022-08-17 16:53:53 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.0428
2022-08-17 16:54:26 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.1594
2022-08-17 16:54:59 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1122
2022-08-17 16:55:32 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.2193
2022-08-17 16:56:06 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 1.9869
2022-08-17 16:56:39 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.2957
2022-08-17 16:57:12 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.1325
2022-08-17 16:57:45 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.2424
2022-08-17 16:58:19 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.3091
2022-08-17 16:58:52 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.0336
2022-08-17 16:59:25 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1279
2022-08-17 16:59:59 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 2.1257
2022-08-17 17:00:32 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.0916
2022-08-17 17:01:05 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.2686
2022-08-17 17:01:39 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.1606
2022-08-17 17:02:12 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.0865
2022-08-17 17:02:46 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.0442
2022-08-17 17:03:19 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.2093
2022-08-17 17:03:53 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.2200
2022-08-17 17:04:26 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1635
2022-08-17 17:04:59 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.1954
2022-08-17 17:05:33 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.3006
2022-08-17 17:06:05 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 1.9979
2022-08-17 17:06:07 - train: epoch 010, train_loss: 2.1520
2022-08-17 17:07:24 - eval: epoch: 010, acc1: 56.328%, acc5: 80.732%, test_loss: 1.8680, per_image_load_time: 2.319ms, per_image_inference_time: 0.600ms
2022-08-17 17:07:24 - until epoch: 010, best_acc1: 56.328%
2022-08-17 17:07:24 - epoch 011 lr: 0.065450
2022-08-17 17:08:05 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.0166
2022-08-17 17:08:38 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.1963
2022-08-17 17:09:10 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 1.9433
2022-08-17 17:09:42 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.2261
2022-08-17 17:10:16 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 1.9809
2022-08-17 17:10:49 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.2042
2022-08-17 17:11:22 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.0022
2022-08-17 17:11:55 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.0158
2022-08-17 17:12:28 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.1134
2022-08-17 17:13:01 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 1.9870
2022-08-17 17:13:35 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2072
2022-08-17 17:14:09 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.3049
2022-08-17 17:14:42 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.2238
2022-08-17 17:15:16 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.0438
2022-08-17 17:15:49 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.1145
2022-08-17 17:16:22 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.0714
2022-08-17 17:16:56 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.2761
2022-08-17 17:17:29 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.1140
2022-08-17 17:18:03 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.1431
2022-08-17 17:18:36 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 1.9766
2022-08-17 17:19:10 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 1.9026
2022-08-17 17:19:43 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 2.0542
2022-08-17 17:20:16 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.3916
2022-08-17 17:20:50 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.9989
2022-08-17 17:21:23 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.1278
2022-08-17 17:21:56 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.0024
2022-08-17 17:22:30 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.2933
2022-08-17 17:23:03 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.9406
2022-08-17 17:23:37 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.1342
2022-08-17 17:24:10 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.2424
2022-08-17 17:24:43 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.1321
2022-08-17 17:25:16 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.0348
2022-08-17 17:25:49 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.2183
2022-08-17 17:26:23 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.0230
2022-08-17 17:26:56 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.1602
2022-08-17 17:27:30 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.0000
2022-08-17 17:28:04 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.2458
2022-08-17 17:28:37 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 2.0051
2022-08-17 17:29:10 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.1032
2022-08-17 17:29:44 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.1009
2022-08-17 17:30:17 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.8097
2022-08-17 17:30:50 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 1.9980
2022-08-17 17:31:24 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.0551
2022-08-17 17:31:57 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.1176
2022-08-17 17:32:31 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.0987
2022-08-17 17:33:04 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 2.0277
2022-08-17 17:33:37 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 2.0058
2022-08-17 17:34:11 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.9396
2022-08-17 17:34:44 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.0840
2022-08-17 17:35:16 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.0049
2022-08-17 17:35:17 - train: epoch 011, train_loss: 2.0920
2022-08-17 17:36:34 - eval: epoch: 011, acc1: 57.416%, acc5: 81.754%, test_loss: 1.8022, per_image_load_time: 2.347ms, per_image_inference_time: 0.608ms
2022-08-17 17:36:34 - until epoch: 011, best_acc1: 57.416%
2022-08-17 17:36:34 - epoch 012 lr: 0.059368
2022-08-17 17:37:16 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.7314
2022-08-17 17:37:48 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.8127
2022-08-17 17:38:21 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 2.0036
2022-08-17 17:38:54 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.0254
2022-08-17 17:39:27 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.0981
2022-08-17 17:40:00 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.7415
2022-08-17 17:40:33 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.9345
2022-08-17 17:41:07 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.0830
2022-08-17 17:41:41 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 1.9449
2022-08-17 17:42:15 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.8847
2022-08-17 17:42:49 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.2754
2022-08-17 17:43:22 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.8371
2022-08-17 17:43:55 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 2.0500
2022-08-17 17:44:29 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.1254
2022-08-17 17:45:03 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 2.0266
2022-08-17 17:45:36 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 2.1020
2022-08-17 17:46:10 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.7948
2022-08-17 17:46:44 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.0657
2022-08-17 17:47:19 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.0649
2022-08-17 17:47:53 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.2429
2022-08-17 17:48:26 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 1.9983
2022-08-17 17:49:00 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 1.9705
2022-08-17 17:49:34 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 2.0382
2022-08-17 17:50:08 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.1352
2022-08-17 17:50:42 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.8355
2022-08-17 17:51:16 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.8109
2022-08-17 17:51:50 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.0033
2022-08-17 17:52:24 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.8974
2022-08-17 17:52:59 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.0890
2022-08-17 17:53:32 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.8270
2022-08-17 17:54:06 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 1.8757
2022-08-17 17:54:40 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.8952
2022-08-17 17:55:14 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.1551
2022-08-17 17:55:48 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.0117
2022-08-17 17:56:22 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.0386
2022-08-17 17:56:56 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 2.0112
2022-08-17 17:57:30 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.0349
2022-08-17 17:58:04 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.0499
2022-08-17 17:58:38 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8832
2022-08-17 17:59:12 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1040
2022-08-17 17:59:46 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 1.9780
2022-08-17 18:00:20 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.9510
2022-08-17 18:00:54 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.2218
2022-08-17 18:01:28 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 2.0302
2022-08-17 18:02:02 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.8962
2022-08-17 18:02:37 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.0528
2022-08-17 18:03:11 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.8863
2022-08-17 18:03:45 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.1495
2022-08-17 18:04:19 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.9711
2022-08-17 18:04:52 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.8148
2022-08-17 18:04:54 - train: epoch 012, train_loss: 2.0328
2022-08-17 18:06:11 - eval: epoch: 012, acc1: 59.464%, acc5: 82.872%, test_loss: 1.7155, per_image_load_time: 2.244ms, per_image_inference_time: 0.599ms
2022-08-17 18:06:11 - until epoch: 012, best_acc1: 59.464%
2022-08-17 18:06:11 - epoch 013 lr: 0.053138
2022-08-17 18:06:52 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.8504
2022-08-17 18:07:26 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.9295
2022-08-17 18:07:59 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.9837
2022-08-17 18:08:33 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.9627
2022-08-17 18:09:06 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 2.0772
2022-08-17 18:09:40 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 1.9312
2022-08-17 18:10:13 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.8555
2022-08-17 18:10:47 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.0716
2022-08-17 18:11:21 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.9715
2022-08-17 18:11:55 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 2.0377
2022-08-17 18:12:29 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 1.8922
2022-08-17 18:13:03 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 1.9382
2022-08-17 18:13:36 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.8533
2022-08-17 18:14:10 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 1.9311
2022-08-17 18:14:44 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.1019
2022-08-17 18:15:18 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.9700
2022-08-17 18:15:51 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.0894
2022-08-17 18:16:25 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 2.0108
2022-08-17 18:16:59 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.1330
2022-08-17 18:17:33 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 1.9132
2022-08-17 18:18:07 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.1615
2022-08-17 18:18:40 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.8501
2022-08-17 18:19:14 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 2.0671
2022-08-17 18:19:48 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.0285
2022-08-17 18:20:22 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.9153
2022-08-17 18:20:55 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.9358
2022-08-17 18:21:29 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 2.0676
2022-08-17 18:22:03 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.0749
2022-08-17 18:22:37 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.1456
2022-08-17 18:23:10 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.9383
2022-08-17 18:23:44 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.9940
2022-08-17 18:24:18 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 2.2072
2022-08-17 18:24:52 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.9211
2022-08-17 18:25:26 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.8769
2022-08-17 18:25:59 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.9788
2022-08-17 18:26:33 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 1.8298
2022-08-17 18:27:07 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.8303
2022-08-17 18:27:40 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1530
2022-08-17 18:28:14 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.0602
2022-08-17 18:28:48 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.8722
2022-08-17 18:29:22 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.8501
2022-08-17 18:29:56 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 2.1548
2022-08-17 18:30:29 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.8767
2022-08-17 18:31:03 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.9559
2022-08-17 18:31:37 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.9266
2022-08-17 18:32:11 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.0634
2022-08-17 18:32:45 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 2.0850
2022-08-17 18:33:19 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.1415
2022-08-17 18:33:52 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 1.8906
2022-08-17 18:34:26 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.2365
2022-08-17 18:34:27 - train: epoch 013, train_loss: 1.9771
2022-08-17 18:35:44 - eval: epoch: 013, acc1: 59.522%, acc5: 83.360%, test_loss: 1.6870, per_image_load_time: 1.946ms, per_image_inference_time: 0.586ms
2022-08-17 18:35:45 - until epoch: 013, best_acc1: 59.522%
2022-08-17 18:35:45 - epoch 014 lr: 0.046859
2022-08-17 18:36:26 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.9757
2022-08-17 18:36:59 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.9700
2022-08-17 18:37:33 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.9834
2022-08-17 18:38:07 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.8347
2022-08-17 18:38:41 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.8516
2022-08-17 18:39:15 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 2.0020
2022-08-17 18:39:48 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.6979
2022-08-17 18:40:22 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.8847
2022-08-17 18:40:55 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.7952
2022-08-17 18:41:28 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 1.9921
2022-08-17 18:42:02 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.7472
2022-08-17 18:42:35 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 1.9479
2022-08-17 18:43:09 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.0515
2022-08-17 18:43:43 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.9357
2022-08-17 18:44:17 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.0129
2022-08-17 18:44:51 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.8855
2022-08-17 18:45:25 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9147
2022-08-17 18:45:59 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.0977
2022-08-17 18:46:32 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.7468
2022-08-17 18:47:06 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.5940
2022-08-17 18:47:39 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 1.9740
2022-08-17 18:48:12 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.9940
2022-08-17 18:48:46 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.7761
2022-08-17 18:49:19 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 2.0214
2022-08-17 18:49:53 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.8211
2022-08-17 18:50:27 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.9316
2022-08-17 18:51:01 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.9477
2022-08-17 18:51:34 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 1.9879
2022-08-17 18:52:08 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.9483
2022-08-17 18:52:42 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.8243
2022-08-17 18:53:15 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.9766
2022-08-17 18:53:49 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.9817
2022-08-17 18:54:23 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.8307
2022-08-17 18:54:56 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.8924
2022-08-17 18:55:30 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.9673
2022-08-17 18:56:04 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.7918
2022-08-17 18:56:37 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 2.0588
2022-08-17 18:57:11 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 1.9387
2022-08-17 18:57:44 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.8627
2022-08-17 18:58:18 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.8059
2022-08-17 18:58:51 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.8634
2022-08-17 18:59:25 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.8289
2022-08-17 18:59:58 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.7381
2022-08-17 19:00:32 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.9664
2022-08-17 19:01:06 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 2.0468
2022-08-17 19:01:39 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.6353
2022-08-17 19:02:13 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.8266
2022-08-17 19:02:47 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.8700
2022-08-17 19:03:20 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8264
2022-08-17 19:03:53 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.9249
2022-08-17 19:03:55 - train: epoch 014, train_loss: 1.9166
2022-08-17 19:05:12 - eval: epoch: 014, acc1: 60.592%, acc5: 83.954%, test_loss: 1.6532, per_image_load_time: 2.354ms, per_image_inference_time: 0.586ms
2022-08-17 19:05:12 - until epoch: 014, best_acc1: 60.592%
2022-08-17 19:05:12 - epoch 015 lr: 0.040630
2022-08-17 19:05:52 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.7152
2022-08-17 19:06:26 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.9882
2022-08-17 19:06:59 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.0209
2022-08-17 19:07:33 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 1.9362
2022-08-17 19:08:07 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.7771
2022-08-17 19:08:40 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 2.0477
2022-08-17 19:09:14 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 2.0931
2022-08-17 19:09:47 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.8019
2022-08-17 19:10:22 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.7974
2022-08-17 19:10:55 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.7298
2022-08-17 19:11:29 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7592
2022-08-17 19:12:02 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.9772
2022-08-17 19:12:36 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.9153
2022-08-17 19:13:10 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.7757
2022-08-17 19:13:44 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.7855
2022-08-17 19:14:17 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.7325
2022-08-17 19:14:51 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.8232
2022-08-17 19:15:25 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.8356
2022-08-17 19:15:58 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.9148
2022-08-17 19:16:32 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.9246
2022-08-17 19:17:06 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.8199
2022-08-17 19:17:39 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9935
2022-08-17 19:18:13 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.8544
2022-08-17 19:18:46 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.7851
2022-08-17 19:19:20 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.8994
2022-08-17 19:19:54 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.7427
2022-08-17 19:20:28 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.8040
2022-08-17 19:21:02 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.8629
2022-08-17 19:21:36 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.7811
2022-08-17 19:22:09 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.6628
2022-08-17 19:22:43 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.8090
2022-08-17 19:23:16 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.8838
2022-08-17 19:23:50 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.6529
2022-08-17 19:24:24 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.8733
2022-08-17 19:24:57 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 1.9971
2022-08-17 19:25:31 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.9026
2022-08-17 19:26:05 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6899
2022-08-17 19:26:38 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.7836
2022-08-17 19:27:12 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8696
2022-08-17 19:27:46 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.9107
2022-08-17 19:28:19 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.9460
2022-08-17 19:28:53 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6384
2022-08-17 19:29:27 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.7984
2022-08-17 19:30:00 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.8750
2022-08-17 19:30:34 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.6476
2022-08-17 19:31:08 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.9756
2022-08-17 19:31:42 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.8609
2022-08-17 19:32:15 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.9259
2022-08-17 19:32:49 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.9088
2022-08-17 19:33:22 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 2.0264
2022-08-17 19:33:24 - train: epoch 015, train_loss: 1.8548
2022-08-17 19:34:41 - eval: epoch: 015, acc1: 62.176%, acc5: 84.938%, test_loss: 1.5736, per_image_load_time: 2.177ms, per_image_inference_time: 0.576ms
2022-08-17 19:34:41 - until epoch: 015, best_acc1: 62.176%
2022-08-17 19:34:41 - epoch 016 lr: 0.034548
2022-08-17 19:35:22 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.7831
2022-08-17 19:35:56 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.5810
2022-08-17 19:36:30 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.6624
2022-08-17 19:37:04 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.0756
2022-08-17 19:37:38 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.6507
2022-08-17 19:38:12 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.8069
2022-08-17 19:38:46 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.6611
2022-08-17 19:39:20 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.7644
2022-08-17 19:39:54 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.6663
2022-08-17 19:40:28 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.7160
2022-08-17 19:41:02 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.6245
2022-08-17 19:41:36 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.5962
2022-08-17 19:42:10 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.8279
2022-08-17 19:42:44 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.5485
2022-08-17 19:43:17 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.9127
2022-08-17 19:43:51 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.8978
2022-08-17 19:44:25 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.7126
2022-08-17 19:44:59 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.9186
2022-08-17 19:45:33 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.9413
2022-08-17 19:46:07 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.4736
2022-08-17 19:46:40 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8589
2022-08-17 19:47:14 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.9157
2022-08-17 19:47:48 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.9307
2022-08-17 19:48:22 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.6712
2022-08-17 19:48:56 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.7958
2022-08-17 19:49:30 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.8799
2022-08-17 19:50:04 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.7693
2022-08-17 19:50:38 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.7644
2022-08-17 19:51:12 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.6336
2022-08-17 19:51:46 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.9837
2022-08-17 19:52:20 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 2.0167
2022-08-17 19:52:54 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.9190
2022-08-17 19:53:28 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9171
2022-08-17 19:54:02 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.8108
2022-08-17 19:54:36 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.7027
2022-08-17 19:55:10 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.6661
2022-08-17 19:55:44 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.7788
2022-08-17 19:56:18 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.0261
2022-08-17 19:56:52 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.6661
2022-08-17 19:57:26 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.8145
2022-08-17 19:57:59 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.8142
2022-08-17 19:58:34 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.6028
2022-08-17 19:59:08 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.7414
2022-08-17 19:59:42 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.5066
2022-08-17 20:00:16 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.7888
2022-08-17 20:00:50 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.6870
2022-08-17 20:01:24 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.8348
2022-08-17 20:01:58 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.6770
2022-08-17 20:02:32 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.6720
2022-08-17 20:03:05 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.6739
2022-08-17 20:03:06 - train: epoch 016, train_loss: 1.7915
2022-08-17 20:04:24 - eval: epoch: 016, acc1: 63.696%, acc5: 86.128%, test_loss: 1.4901, per_image_load_time: 2.350ms, per_image_inference_time: 0.577ms
2022-08-17 20:04:24 - until epoch: 016, best_acc1: 63.696%
2022-08-17 20:04:24 - epoch 017 lr: 0.028710
2022-08-17 20:05:04 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.6680
2022-08-17 20:05:38 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.5745
2022-08-17 20:06:12 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 2.0595
2022-08-17 20:06:46 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.6530
2022-08-17 20:07:20 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.7317
2022-08-17 20:07:54 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 1.8308
2022-08-17 20:08:28 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.7157
2022-08-17 20:09:01 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.5821
2022-08-17 20:09:35 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.5561
2022-08-17 20:10:09 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.5963
2022-08-17 20:10:43 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.9533
2022-08-17 20:11:17 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.7597
2022-08-17 20:11:51 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.7393
2022-08-17 20:12:24 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.6036
2022-08-17 20:12:58 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.5494
2022-08-17 20:13:32 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.6133
2022-08-17 20:14:06 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.5370
2022-08-17 20:14:39 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.5430
2022-08-17 20:15:13 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.7086
2022-08-17 20:15:47 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.4961
2022-08-17 20:16:21 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.4026
2022-08-17 20:16:55 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.5320
2022-08-17 20:17:29 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.6992
2022-08-17 20:18:02 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.6706
2022-08-17 20:18:36 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.7218
2022-08-17 20:19:10 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.5819
2022-08-17 20:19:44 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.5774
2022-08-17 20:20:18 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.9081
2022-08-17 20:20:51 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.8827
2022-08-17 20:21:25 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.5932
2022-08-17 20:21:58 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.6864
2022-08-17 20:22:32 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.6330
2022-08-17 20:23:06 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.6388
2022-08-17 20:23:39 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.6481
2022-08-17 20:24:13 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.6280
2022-08-17 20:24:46 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.7492
2022-08-17 20:25:21 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.5709
2022-08-17 20:25:55 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.7258
2022-08-17 20:26:28 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.6989
2022-08-17 20:27:02 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.7259
2022-08-17 20:27:36 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.6704
2022-08-17 20:28:09 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.6699
2022-08-17 20:28:43 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.7040
2022-08-17 20:29:17 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8221
2022-08-17 20:29:50 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.8627
2022-08-17 20:30:24 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.7076
2022-08-17 20:30:58 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.7944
2022-08-17 20:31:31 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.7828
2022-08-17 20:32:05 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.4441
2022-08-17 20:32:38 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.3816
2022-08-17 20:32:40 - train: epoch 017, train_loss: 1.7227
2022-08-17 20:33:58 - eval: epoch: 017, acc1: 65.006%, acc5: 86.422%, test_loss: 1.4530, per_image_load_time: 2.417ms, per_image_inference_time: 0.596ms
2022-08-17 20:33:58 - until epoch: 017, best_acc1: 65.006%
2022-08-17 20:33:58 - epoch 018 lr: 0.023208
2022-08-17 20:34:38 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.5654
2022-08-17 20:35:12 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.9018
2022-08-17 20:35:46 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.8517
2022-08-17 20:36:20 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.8996
2022-08-17 20:36:53 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.8498
2022-08-17 20:37:27 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.6294
2022-08-17 20:38:01 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.5694
2022-08-17 20:38:35 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.8265
2022-08-17 20:39:08 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.7687
2022-08-17 20:39:42 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.7574
2022-08-17 20:40:16 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.9413
2022-08-17 20:40:50 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.6752
2022-08-17 20:41:23 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.8092
2022-08-17 20:41:57 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.7366
2022-08-17 20:42:31 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.7425
2022-08-17 20:43:05 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.5107
2022-08-17 20:43:39 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.8161
2022-08-17 20:44:12 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6615
2022-08-17 20:44:46 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.6804
2022-08-17 20:45:20 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.8150
2022-08-17 20:45:54 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.8850
2022-08-17 20:46:27 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.7265
2022-08-17 20:47:01 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.6505
2022-08-17 20:47:35 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.6032
2022-08-17 20:48:09 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.6170
2022-08-17 20:48:42 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.7315
2022-08-17 20:49:16 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.6469
2022-08-17 20:49:50 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5186
2022-08-17 20:50:24 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.7160
2022-08-17 20:50:58 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.6468
2022-08-17 20:51:32 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 2.0623
2022-08-17 20:52:05 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.6145
2022-08-17 20:52:39 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.6694
2022-08-17 20:53:13 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.5992
2022-08-17 20:53:47 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.6657
2022-08-17 20:54:21 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.8634
2022-08-17 20:54:55 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.8693
2022-08-17 20:55:29 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.7963
2022-08-17 20:56:02 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.6849
2022-08-17 20:56:36 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.5442
2022-08-17 20:57:10 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.8682
2022-08-17 20:57:43 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.6426
2022-08-17 20:58:17 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.6127
2022-08-17 20:58:51 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.5994
2022-08-17 20:59:25 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.6204
2022-08-17 20:59:59 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.5182
2022-08-17 21:00:33 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.8843
2022-08-17 21:01:07 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6413
2022-08-17 21:01:41 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.5190
2022-08-17 21:02:14 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.7225
2022-08-17 21:02:16 - train: epoch 018, train_loss: 1.6536
2022-08-17 21:03:32 - eval: epoch: 018, acc1: 66.416%, acc5: 87.600%, test_loss: 1.3735, per_image_load_time: 1.717ms, per_image_inference_time: 0.576ms
2022-08-17 21:03:32 - until epoch: 018, best_acc1: 66.416%
2022-08-17 21:03:32 - epoch 019 lr: 0.018128
2022-08-17 21:04:12 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.5891
2022-08-17 21:04:45 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.4376
2022-08-17 21:05:18 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.6009
2022-08-17 21:05:52 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.6355
2022-08-17 21:06:25 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.5488
2022-08-17 21:06:58 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.5841
2022-08-17 21:07:31 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.4806
2022-08-17 21:08:05 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.6609
2022-08-17 21:08:38 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.7414
2022-08-17 21:09:12 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.5445
2022-08-17 21:09:46 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5290
2022-08-17 21:10:20 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.5384
2022-08-17 21:10:53 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.5493
2022-08-17 21:11:27 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.4950
2022-08-17 21:12:01 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.7761
2022-08-17 21:12:35 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.5658
2022-08-17 21:13:09 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.6216
2022-08-17 21:13:44 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.5345
2022-08-17 21:14:18 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.8862
2022-08-17 21:14:52 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.6107
2022-08-17 21:15:26 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.2988
2022-08-17 21:16:00 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.4714
2022-08-17 21:16:34 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.6476
2022-08-17 21:17:09 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.7439
2022-08-17 21:17:43 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.4425
2022-08-17 21:18:17 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.5707
2022-08-17 21:18:52 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.4409
2022-08-17 21:19:26 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.5533
2022-08-17 21:20:00 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.7467
2022-08-17 21:20:33 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.7017
2022-08-17 21:21:07 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.6289
2022-08-17 21:21:42 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.3858
2022-08-17 21:22:16 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.5170
2022-08-17 21:22:50 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.4774
2022-08-17 21:23:25 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.7074
2022-08-17 21:23:59 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.4060
2022-08-17 21:24:33 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.5279
2022-08-17 21:25:07 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.7368
2022-08-17 21:25:41 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.3471
2022-08-17 21:26:16 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.6057
2022-08-17 21:26:50 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.6111
2022-08-17 21:27:24 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.6084
2022-08-17 21:27:58 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.5701
2022-08-17 21:28:32 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.6004
2022-08-17 21:29:06 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.6682
2022-08-17 21:29:41 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.4669
2022-08-17 21:30:15 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.4851
2022-08-17 21:30:49 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.5306
2022-08-17 21:31:23 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.4169
2022-08-17 21:31:57 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.6131
2022-08-17 21:31:58 - train: epoch 019, train_loss: 1.5822
2022-08-17 21:33:16 - eval: epoch: 019, acc1: 67.828%, acc5: 88.330%, test_loss: 1.3155, per_image_load_time: 1.212ms, per_image_inference_time: 0.599ms
2022-08-17 21:33:16 - until epoch: 019, best_acc1: 67.828%
2022-08-17 21:33:16 - epoch 020 lr: 0.013551
2022-08-17 21:33:57 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.5384
2022-08-17 21:34:31 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.4507
2022-08-17 21:35:05 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.3839
2022-08-17 21:35:39 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.3887
2022-08-17 21:36:13 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.4784
2022-08-17 21:36:47 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.5342
2022-08-17 21:37:21 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.3477
2022-08-17 21:37:55 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.4849
2022-08-17 21:38:30 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.5252
2022-08-17 21:39:04 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.5730
2022-08-17 21:39:38 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.4832
2022-08-17 21:40:12 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.4385
2022-08-17 21:40:46 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.5553
2022-08-17 21:41:21 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.4890
2022-08-17 21:41:55 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.6949
2022-08-17 21:42:30 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.2924
2022-08-17 21:43:04 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.4137
2022-08-17 21:43:38 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.5385
2022-08-17 21:44:13 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.2599
2022-08-17 21:44:47 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.4770
2022-08-17 21:45:21 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.7693
2022-08-17 21:45:55 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.4711
2022-08-17 21:46:29 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.3734
2022-08-17 21:47:04 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.7630
2022-08-17 21:47:38 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.4081
2022-08-17 21:48:12 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.5748
2022-08-17 21:48:46 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.3704
2022-08-17 21:49:20 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.6164
2022-08-17 21:49:54 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.6166
2022-08-17 21:50:28 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.6037
2022-08-17 21:51:02 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.5859
2022-08-17 21:51:36 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.5471
2022-08-17 21:52:10 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.5355
2022-08-17 21:52:44 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.5046
2022-08-17 21:53:18 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.3768
2022-08-17 21:53:53 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.3996
2022-08-17 21:54:27 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.2102
2022-08-17 21:55:01 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.5184
2022-08-17 21:55:35 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.7262
2022-08-17 21:56:09 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.3724
2022-08-17 21:56:43 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.4592
2022-08-17 21:57:17 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.5266
2022-08-17 21:57:52 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.6414
2022-08-17 21:58:26 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.6563
2022-08-17 21:59:00 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.5755
2022-08-17 21:59:34 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.5706
2022-08-17 22:00:09 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.4266
2022-08-17 22:00:43 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4400
2022-08-17 22:01:17 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.5419
2022-08-17 22:01:51 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.4170
2022-08-17 22:01:52 - train: epoch 020, train_loss: 1.5100
2022-08-17 22:03:10 - eval: epoch: 020, acc1: 69.256%, acc5: 89.198%, test_loss: 1.2460, per_image_load_time: 2.195ms, per_image_inference_time: 0.618ms
2022-08-17 22:03:11 - until epoch: 020, best_acc1: 69.256%
2022-08-17 22:03:11 - epoch 021 lr: 0.009548
2022-08-17 22:03:51 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.5330
2022-08-17 22:04:25 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.2962
2022-08-17 22:04:59 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.1569
2022-08-17 22:05:33 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4054
2022-08-17 22:06:07 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.3747
2022-08-17 22:06:41 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.3696
2022-08-17 22:07:15 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3837
2022-08-17 22:07:49 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.4794
2022-08-17 22:08:23 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.4985
2022-08-17 22:08:57 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3365
2022-08-17 22:09:31 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.3729
2022-08-17 22:10:05 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.3523
2022-08-17 22:10:39 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.3298
2022-08-17 22:11:14 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.1312
2022-08-17 22:11:48 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.3390
2022-08-17 22:12:22 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3286
2022-08-17 22:12:56 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5798
2022-08-17 22:13:31 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.2260
2022-08-17 22:14:05 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.6023
2022-08-17 22:14:39 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.5943
2022-08-17 22:15:13 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3803
2022-08-17 22:15:47 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.5748
2022-08-17 22:16:21 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.4060
2022-08-17 22:16:55 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.4108
2022-08-17 22:17:30 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.4584
2022-08-17 22:18:04 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.4314
2022-08-17 22:18:39 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.5032
2022-08-17 22:19:13 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.3213
2022-08-17 22:19:47 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.3247
2022-08-17 22:20:22 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.6013
2022-08-17 22:20:56 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.5477
2022-08-17 22:21:30 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.2896
2022-08-17 22:22:05 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.5827
2022-08-17 22:22:39 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.4967
2022-08-17 22:23:13 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4610
2022-08-17 22:23:48 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.3287
2022-08-17 22:24:22 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.5275
2022-08-17 22:24:57 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.4705
2022-08-17 22:25:31 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.3820
2022-08-17 22:26:06 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.6268
2022-08-17 22:26:40 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.2261
2022-08-17 22:27:15 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.5060
2022-08-17 22:27:49 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.2974
2022-08-17 22:28:23 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.5186
2022-08-17 22:28:57 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.5889
2022-08-17 22:29:32 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.3477
2022-08-17 22:30:06 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.5576
2022-08-17 22:30:41 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.6406
2022-08-17 22:31:15 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.2766
2022-08-17 22:31:48 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.2972
2022-08-17 22:31:50 - train: epoch 021, train_loss: 1.4378
2022-08-17 22:33:08 - eval: epoch: 021, acc1: 70.024%, acc5: 89.762%, test_loss: 1.2079, per_image_load_time: 1.246ms, per_image_inference_time: 0.604ms
2022-08-17 22:33:09 - until epoch: 021, best_acc1: 70.024%
2022-08-17 22:33:09 - epoch 022 lr: 0.006184
2022-08-17 22:33:49 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.1511
2022-08-17 22:34:23 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.4192
2022-08-17 22:34:57 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.3436
2022-08-17 22:35:31 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.4869
2022-08-17 22:36:05 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3244
2022-08-17 22:36:40 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.3173
2022-08-17 22:37:14 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4214
2022-08-17 22:37:48 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.5663
2022-08-17 22:38:22 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.4863
2022-08-17 22:38:57 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.4299
2022-08-17 22:39:31 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.4468
2022-08-17 22:40:06 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.0348
2022-08-17 22:40:40 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.1930
2022-08-17 22:41:13 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.2757
2022-08-17 22:41:48 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.4595
2022-08-17 22:42:22 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.3059
2022-08-17 22:42:56 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.3376
2022-08-17 22:43:31 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.5440
2022-08-17 22:44:05 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.4759
2022-08-17 22:44:39 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4432
2022-08-17 22:45:14 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.4086
2022-08-17 22:45:48 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.0778
2022-08-17 22:46:22 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.3638
2022-08-17 22:46:56 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.5038
2022-08-17 22:47:31 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.3603
2022-08-17 22:48:05 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.1850
2022-08-17 22:48:39 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.2641
2022-08-17 22:49:13 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.5074
2022-08-17 22:49:48 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.2789
2022-08-17 22:50:22 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.2195
2022-08-17 22:50:56 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.5182
2022-08-17 22:51:30 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4980
2022-08-17 22:52:05 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.4992
2022-08-17 22:52:39 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.2689
2022-08-17 22:53:13 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.4455
2022-08-17 22:53:47 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.4031
2022-08-17 22:54:21 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.4573
2022-08-17 22:54:55 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.4139
2022-08-17 22:55:30 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.3685
2022-08-17 22:56:04 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.4661
2022-08-17 22:56:38 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.2924
2022-08-17 22:57:13 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.3233
2022-08-17 22:57:47 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4152
2022-08-17 22:58:21 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3122
2022-08-17 22:58:55 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.1356
2022-08-17 22:59:29 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.6932
2022-08-17 23:00:04 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.3593
2022-08-17 23:00:38 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.4369
2022-08-17 23:01:12 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.4221
2022-08-17 23:01:46 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.3020
2022-08-17 23:01:47 - train: epoch 022, train_loss: 1.3706
2022-08-17 23:03:04 - eval: epoch: 022, acc1: 71.168%, acc5: 90.242%, test_loss: 1.1619, per_image_load_time: 2.270ms, per_image_inference_time: 0.579ms
2022-08-17 23:03:05 - until epoch: 022, best_acc1: 71.168%
2022-08-17 23:03:05 - epoch 023 lr: 0.003511
2022-08-17 23:03:46 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.2960
2022-08-17 23:04:20 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.2454
2022-08-17 23:04:54 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.2217
2022-08-17 23:05:28 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.4465
2022-08-17 23:06:02 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.4560
2022-08-17 23:06:35 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.3512
2022-08-17 23:07:09 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.2821
2022-08-17 23:07:43 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3124
2022-08-17 23:08:18 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4115
2022-08-17 23:08:52 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.1752
2022-08-17 23:09:26 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.2630
2022-08-17 23:10:01 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.1882
2022-08-17 23:10:35 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.3167
2022-08-17 23:11:10 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.4869
2022-08-17 23:11:44 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.1845
2022-08-17 23:12:18 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.4217
2022-08-17 23:12:52 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.3721
2022-08-17 23:13:27 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.2188
2022-08-17 23:14:01 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.1752
2022-08-17 23:14:35 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.2449
2022-08-17 23:15:10 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.3348
2022-08-17 23:15:44 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.2230
2022-08-17 23:16:19 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.1840
2022-08-17 23:16:53 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.2382
2022-08-17 23:17:28 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.3872
2022-08-17 23:18:02 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.4888
2022-08-17 23:18:37 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.2858
2022-08-17 23:19:11 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.2479
2022-08-17 23:19:46 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.1083
2022-08-17 23:20:20 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.3112
2022-08-17 23:20:55 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.2813
2022-08-17 23:21:29 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.4349
2022-08-17 23:22:03 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.2722
2022-08-17 23:22:38 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.4823
2022-08-17 23:23:12 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.3737
2022-08-17 23:23:46 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.2953
2022-08-17 23:24:21 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.3628
2022-08-17 23:24:55 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.5224
2022-08-17 23:25:29 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3691
2022-08-17 23:26:03 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2244
2022-08-17 23:26:38 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.2122
2022-08-17 23:27:12 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.0896
2022-08-17 23:27:47 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.1571
2022-08-17 23:28:21 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.2634
2022-08-17 23:28:56 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.1454
2022-08-17 23:29:30 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.4553
2022-08-17 23:30:05 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.1299
2022-08-17 23:30:39 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.1344
2022-08-17 23:31:14 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.1727
2022-08-17 23:31:47 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.3413
2022-08-17 23:31:49 - train: epoch 023, train_loss: 1.3164
2022-08-17 23:33:07 - eval: epoch: 023, acc1: 71.896%, acc5: 90.636%, test_loss: 1.1308, per_image_load_time: 1.208ms, per_image_inference_time: 0.606ms
2022-08-17 23:33:07 - until epoch: 023, best_acc1: 71.896%
2022-08-17 23:33:07 - epoch 024 lr: 0.001571
2022-08-17 23:33:48 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.2301
2022-08-17 23:34:21 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.3090
2022-08-17 23:34:55 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.2775
2022-08-17 23:35:29 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.1996
2022-08-17 23:36:03 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.1583
2022-08-17 23:36:37 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.1632
2022-08-17 23:37:11 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.1925
2022-08-17 23:37:46 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.2234
2022-08-17 23:38:20 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.1227
2022-08-17 23:38:54 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.2681
2022-08-17 23:39:28 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 0.9939
2022-08-17 23:40:02 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.3071
2022-08-17 23:40:36 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4171
2022-08-17 23:41:10 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.3305
2022-08-17 23:41:44 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.3165
2022-08-17 23:42:18 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.2115
2022-08-17 23:42:53 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.1995
2022-08-17 23:43:27 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.5233
2022-08-17 23:44:01 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.2578
2022-08-17 23:44:35 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.4283
2022-08-17 23:45:10 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.3666
2022-08-17 23:45:44 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.1637
2022-08-17 23:46:18 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.2511
2022-08-17 23:46:52 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.3232
2022-08-17 23:47:27 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.2746
2022-08-17 23:48:01 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.4689
2022-08-17 23:48:36 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.5408
2022-08-17 23:49:10 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.1612
2022-08-17 23:49:45 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.3541
2022-08-17 23:50:19 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.3419
2022-08-17 23:50:54 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.1328
2022-08-17 23:51:28 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.1951
2022-08-17 23:52:02 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.2581
2022-08-17 23:52:37 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.1880
2022-08-17 23:53:11 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.1664
2022-08-17 23:53:46 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.2997
2022-08-17 23:54:20 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.4000
2022-08-17 23:54:55 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.3351
2022-08-17 23:55:29 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.0838
2022-08-17 23:56:04 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.2603
2022-08-17 23:56:39 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.2057
2022-08-17 23:57:13 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.0739
2022-08-17 23:57:47 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.4731
2022-08-17 23:58:22 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.2010
2022-08-17 23:58:57 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.1811
2022-08-17 23:59:31 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.3084
2022-08-18 00:00:06 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.2792
2022-08-18 00:00:40 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.0983
2022-08-18 00:01:15 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.2144
2022-08-18 00:01:48 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.2849
2022-08-18 00:01:50 - train: epoch 024, train_loss: 1.2803
2022-08-18 00:03:08 - eval: epoch: 024, acc1: 72.312%, acc5: 90.850%, test_loss: 1.1156, per_image_load_time: 2.030ms, per_image_inference_time: 0.609ms
2022-08-18 00:03:08 - until epoch: 024, best_acc1: 72.312%
2022-08-18 00:03:08 - epoch 025 lr: 0.000394
2022-08-18 00:03:49 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.2531
2022-08-18 00:04:23 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1369
2022-08-18 00:04:57 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.2152
2022-08-18 00:05:31 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.3229
2022-08-18 00:06:05 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.1710
2022-08-18 00:06:39 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3430
2022-08-18 00:07:13 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.3269
2022-08-18 00:07:47 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.1811
2022-08-18 00:08:21 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0719
2022-08-18 00:08:56 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3922
2022-08-18 00:09:30 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.3453
2022-08-18 00:10:04 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.2631
2022-08-18 00:10:38 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.2316
2022-08-18 00:11:13 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.2842
2022-08-18 00:11:47 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.2964
2022-08-18 00:12:21 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 0.9918
2022-08-18 00:12:55 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.3229
2022-08-18 00:13:30 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 0.9489
2022-08-18 00:14:04 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.1450
2022-08-18 00:14:38 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.2455
2022-08-18 00:15:13 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.1090
2022-08-18 00:15:47 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.0531
2022-08-18 00:16:21 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.3229
2022-08-18 00:16:56 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.1908
2022-08-18 00:17:30 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.2153
2022-08-18 00:18:04 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.3678
2022-08-18 00:18:38 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.1999
2022-08-18 00:19:12 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.2823
2022-08-18 00:19:46 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.3307
2022-08-18 00:20:20 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.2126
2022-08-18 00:20:55 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.2255
2022-08-18 00:21:29 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.3704
2022-08-18 00:22:03 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.1213
2022-08-18 00:22:38 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.2971
2022-08-18 00:23:12 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1005
2022-08-18 00:23:47 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.2471
2022-08-18 00:24:21 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.2971
2022-08-18 00:24:55 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3406
2022-08-18 00:25:30 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.2691
2022-08-18 00:26:04 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.3831
2022-08-18 00:26:39 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.3831
2022-08-18 00:27:13 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.2692
2022-08-18 00:27:48 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.0607
2022-08-18 00:28:23 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.2559
2022-08-18 00:28:57 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.3424
2022-08-18 00:29:31 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.0288
2022-08-18 00:30:06 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.3073
2022-08-18 00:30:41 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.1213
2022-08-18 00:31:15 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.3425
2022-08-18 00:31:49 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.3030
2022-08-18 00:31:50 - train: epoch 025, train_loss: 1.2615
2022-08-18 00:33:07 - eval: epoch: 025, acc1: 72.322%, acc5: 90.830%, test_loss: 1.1132, per_image_load_time: 1.629ms, per_image_inference_time: 0.590ms
2022-08-18 00:33:07 - until epoch: 025, best_acc1: 72.322%
2022-08-18 00:33:07 - train done. train time: 12.342 hours, best_acc1: 72.322%
