2022-08-19 01:29:54 - net_idx: 8
2022-08-19 01:29:54 - net_config: {'stem_width': 64, 'depth': 13, 'w_0': 48, 'w_a': 18.408368166139063, 'w_m': 1.6708008682303224}
2022-08-19 01:29:54 - num_classes: 1000
2022-08-19 01:29:54 - input_image_size: 224
2022-08-19 01:29:54 - scale: 1.1428571428571428
2022-08-19 01:29:54 - seed: 0
2022-08-19 01:29:54 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-19 01:29:54 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-19 01:29:54 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c2100>
2022-08-19 01:29:54 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c23d0>
2022-08-19 01:29:54 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2400>
2022-08-19 01:29:54 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2460>
2022-08-19 01:29:54 - batch_size: 256
2022-08-19 01:29:54 - num_workers: 16
2022-08-19 01:29:54 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-19 01:29:54 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-19 01:29:54 - epochs: 25
2022-08-19 01:29:54 - print_interval: 100
2022-08-19 01:29:54 - accumulation_steps: 1
2022-08-19 01:29:54 - sync_bn: False
2022-08-19 01:29:54 - apex: True
2022-08-19 01:29:54 - use_ema_model: False
2022-08-19 01:29:54 - ema_model_decay: 0.9999
2022-08-19 01:29:54 - log_dir: ./log
2022-08-19 01:29:54 - checkpoint_dir: ./checkpoints
2022-08-19 01:29:54 - gpus_type: NVIDIA RTX A5000
2022-08-19 01:29:54 - gpus_num: 2
2022-08-19 01:29:54 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f4efca864b0>
2022-08-19 01:29:54 - ema_model: None
2022-08-19 01:29:55 - --------------------parameters--------------------
2022-08-19 01:29:55 - name: conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-19 01:29:55 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-19 01:29:55 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-19 01:29:55 - name: fc.weight, grad: True
2022-08-19 01:29:55 - name: fc.bias, grad: True
2022-08-19 01:29:55 - --------------------buffers--------------------
2022-08-19 01:29:55 - name: conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-19 01:29:55 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-19 01:29:55 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-19 01:29:55 - -----------no weight decay layers--------------
2022-08-19 01:29:55 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-19 01:29:55 - -------------weight decay layers---------------
2022-08-19 01:29:55 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-19 01:29:55 - epoch 001 lr: 0.100000
2022-08-19 01:30:36 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9093
2022-08-19 01:31:09 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9120
2022-08-19 01:31:44 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8854
2022-08-19 01:32:18 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.8658
2022-08-19 01:32:52 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.7896
2022-08-19 01:33:26 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.6568
2022-08-19 01:34:01 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.6663
2022-08-19 01:34:35 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.5807
2022-08-19 01:35:10 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.4930
2022-08-19 01:35:44 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.4771
2022-08-19 01:36:19 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.4525
2022-08-19 01:36:53 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.2433
2022-08-19 01:37:27 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.1822
2022-08-19 01:38:02 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.2351
2022-08-19 01:38:36 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 6.0754
2022-08-19 01:39:10 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.1048
2022-08-19 01:39:44 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.8082
2022-08-19 01:40:19 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.9147
2022-08-19 01:40:53 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.8196
2022-08-19 01:41:28 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.6176
2022-08-19 01:42:02 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.6242
2022-08-19 01:42:36 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.4534
2022-08-19 01:43:11 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.4085
2022-08-19 01:43:46 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.5122
2022-08-19 01:44:20 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.3894
2022-08-19 01:44:55 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.4579
2022-08-19 01:45:30 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.3463
2022-08-19 01:46:05 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.2552
2022-08-19 01:46:39 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.0136
2022-08-19 01:47:14 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.3138
2022-08-19 01:47:49 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.1438
2022-08-19 01:48:23 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.2178
2022-08-19 01:48:58 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.8995
2022-08-19 01:49:32 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.9659
2022-08-19 01:50:07 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.8321
2022-08-19 01:50:41 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.8333
2022-08-19 01:51:16 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 5.0230
2022-08-19 01:51:50 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.6340
2022-08-19 01:52:25 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.6643
2022-08-19 01:52:59 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.6053
2022-08-19 01:53:33 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.7700
2022-08-19 01:54:08 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.5787
2022-08-19 01:54:43 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.4622
2022-08-19 01:55:17 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.3736
2022-08-19 01:55:52 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.5193
2022-08-19 01:56:27 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.7096
2022-08-19 01:57:01 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.3322
2022-08-19 01:57:36 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.4625
2022-08-19 01:58:10 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.6195
2022-08-19 01:58:44 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.3227
2022-08-19 01:58:46 - train: epoch 001, train_loss: 5.5030
2022-08-19 02:00:03 - eval: epoch: 001, acc1: 16.328%, acc5: 37.644%, test_loss: 4.2710, per_image_load_time: 2.360ms, per_image_inference_time: 0.579ms
2022-08-19 02:00:03 - until epoch: 001, best_acc1: 16.328%
2022-08-19 02:00:03 - epoch 002 lr: 0.099606
2022-08-19 02:00:44 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.2360
2022-08-19 02:01:19 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.0486
2022-08-19 02:01:53 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.3760
2022-08-19 02:02:27 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.1937
2022-08-19 02:03:01 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.1737
2022-08-19 02:03:36 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.0248
2022-08-19 02:04:10 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.2900
2022-08-19 02:04:44 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 4.0834
2022-08-19 02:05:19 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.8625
2022-08-19 02:05:53 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.1276
2022-08-19 02:06:27 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.2059
2022-08-19 02:07:02 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.8997
2022-08-19 02:07:37 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.8126
2022-08-19 02:08:11 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.0794
2022-08-19 02:08:45 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 3.9992
2022-08-19 02:09:19 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.8576
2022-08-19 02:09:53 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.9132
2022-08-19 02:10:28 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 4.0602
2022-08-19 02:11:02 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.6362
2022-08-19 02:11:36 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.4420
2022-08-19 02:12:11 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.8483
2022-08-19 02:12:46 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.4892
2022-08-19 02:13:20 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.7203
2022-08-19 02:13:55 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.6242
2022-08-19 02:14:29 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.5114
2022-08-19 02:15:04 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.6243
2022-08-19 02:15:38 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.8494
2022-08-19 02:16:13 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.7025
2022-08-19 02:16:47 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.6851
2022-08-19 02:17:22 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.5427
2022-08-19 02:17:56 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.4710
2022-08-19 02:18:30 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.5381
2022-08-19 02:19:05 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.4952
2022-08-19 02:19:39 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.5977
2022-08-19 02:20:14 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.3758
2022-08-19 02:20:48 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.4788
2022-08-19 02:21:23 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.7185
2022-08-19 02:21:58 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.2766
2022-08-19 02:22:32 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.3576
2022-08-19 02:23:07 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.2870
2022-08-19 02:23:41 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.4475
2022-08-19 02:24:16 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.2869
2022-08-19 02:24:50 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.4988
2022-08-19 02:25:24 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.2598
2022-08-19 02:25:59 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.0662
2022-08-19 02:26:33 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.2405
2022-08-19 02:27:08 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.3375
2022-08-19 02:27:42 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.3855
2022-08-19 02:28:17 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.2859
2022-08-19 02:28:51 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.3911
2022-08-19 02:28:53 - train: epoch 002, train_loss: 3.7040
2022-08-19 02:30:10 - eval: epoch: 002, acc1: 29.854%, acc5: 56.146%, test_loss: 3.3208, per_image_load_time: 2.400ms, per_image_inference_time: 0.559ms
2022-08-19 02:30:10 - until epoch: 002, best_acc1: 29.854%
2022-08-19 02:30:10 - epoch 003 lr: 0.098429
2022-08-19 02:30:51 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.1854
2022-08-19 02:31:26 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.2789
2022-08-19 02:32:00 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.1977
2022-08-19 02:32:34 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.1914
2022-08-19 02:33:08 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.3668
2022-08-19 02:33:43 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 2.9666
2022-08-19 02:34:17 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.3639
2022-08-19 02:34:51 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.5181
2022-08-19 02:35:26 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.1875
2022-08-19 02:36:00 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.3787
2022-08-19 02:36:35 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.0612
2022-08-19 02:37:09 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.1520
2022-08-19 02:37:44 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.1174
2022-08-19 02:38:18 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.0953
2022-08-19 02:38:53 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.4588
2022-08-19 02:39:27 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.1287
2022-08-19 02:40:02 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 2.9780
2022-08-19 02:40:36 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.1629
2022-08-19 02:41:10 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.0377
2022-08-19 02:41:45 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.0803
2022-08-19 02:42:19 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.1066
2022-08-19 02:42:53 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.6395
2022-08-19 02:43:28 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.9046
2022-08-19 02:44:02 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 2.9495
2022-08-19 02:44:37 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 2.9998
2022-08-19 02:45:11 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.2137
2022-08-19 02:45:46 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.3405
2022-08-19 02:46:20 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 3.0057
2022-08-19 02:46:55 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 3.0014
2022-08-19 02:47:30 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.0664
2022-08-19 02:48:04 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.2359
2022-08-19 02:48:39 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.0054
2022-08-19 02:49:14 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0258
2022-08-19 02:49:48 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.1029
2022-08-19 02:50:23 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.8594
2022-08-19 02:50:57 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.8137
2022-08-19 02:51:32 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.0338
2022-08-19 02:52:07 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 2.9647
2022-08-19 02:52:41 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.0595
2022-08-19 02:53:15 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7642
2022-08-19 02:53:50 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.9166
2022-08-19 02:54:24 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.0323
2022-08-19 02:54:59 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.8655
2022-08-19 02:55:34 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.9530
2022-08-19 02:56:08 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 2.9540
2022-08-19 02:56:42 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.9282
2022-08-19 02:57:17 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 2.8495
2022-08-19 02:57:51 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.1559
2022-08-19 02:58:26 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.0052
2022-08-19 02:58:59 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.9244
2022-08-19 02:59:01 - train: epoch 003, train_loss: 3.0815
2022-08-19 03:00:18 - eval: epoch: 003, acc1: 39.666%, acc5: 66.464%, test_loss: 2.7242, per_image_load_time: 2.374ms, per_image_inference_time: 0.604ms
2022-08-19 03:00:18 - until epoch: 003, best_acc1: 39.666%
2022-08-19 03:00:18 - epoch 004 lr: 0.096488
2022-08-19 03:00:59 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 3.0042
2022-08-19 03:01:33 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.7434
2022-08-19 03:02:07 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.9075
2022-08-19 03:02:42 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.7929
2022-08-19 03:03:16 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.7993
2022-08-19 03:03:50 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 2.9599
2022-08-19 03:04:24 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.8561
2022-08-19 03:04:58 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.5773
2022-08-19 03:05:32 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.6014
2022-08-19 03:06:06 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.8572
2022-08-19 03:06:40 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.1787
2022-08-19 03:07:15 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.7165
2022-08-19 03:07:49 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.7022
2022-08-19 03:08:23 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.7804
2022-08-19 03:08:58 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 3.0092
2022-08-19 03:09:33 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.8580
2022-08-19 03:10:07 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.9658
2022-08-19 03:10:41 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 3.0701
2022-08-19 03:11:16 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.9077
2022-08-19 03:11:50 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.7286
2022-08-19 03:12:24 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.6590
2022-08-19 03:12:59 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.9038
2022-08-19 03:13:33 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.6642
2022-08-19 03:14:07 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.6851
2022-08-19 03:14:42 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.7181
2022-08-19 03:15:16 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.7599
2022-08-19 03:15:50 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.6590
2022-08-19 03:16:24 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8675
2022-08-19 03:16:59 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.7387
2022-08-19 03:17:33 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.8539
2022-08-19 03:18:07 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.7257
2022-08-19 03:18:42 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.7034
2022-08-19 03:19:16 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8451
2022-08-19 03:19:51 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.7978
2022-08-19 03:20:25 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.7135
2022-08-19 03:20:59 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.7543
2022-08-19 03:21:34 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.6138
2022-08-19 03:22:08 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.5627
2022-08-19 03:22:43 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.5519
2022-08-19 03:23:17 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.6573
2022-08-19 03:23:51 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.6668
2022-08-19 03:24:26 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6348
2022-08-19 03:25:00 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.6276
2022-08-19 03:25:34 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.5447
2022-08-19 03:26:09 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.3099
2022-08-19 03:26:43 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.8609
2022-08-19 03:27:18 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.6599
2022-08-19 03:27:52 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.5284
2022-08-19 03:28:27 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.7186
2022-08-19 03:29:01 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.5990
2022-08-19 03:29:03 - train: epoch 004, train_loss: 2.7843
2022-08-19 03:30:21 - eval: epoch: 004, acc1: 43.802%, acc5: 70.286%, test_loss: 2.5132, per_image_load_time: 2.416ms, per_image_inference_time: 0.591ms
2022-08-19 03:30:21 - until epoch: 004, best_acc1: 43.802%
2022-08-19 03:30:21 - epoch 005 lr: 0.093815
2022-08-19 03:31:01 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.6101
2022-08-19 03:31:35 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.7118
2022-08-19 03:32:09 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.8367
2022-08-19 03:32:43 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.7674
2022-08-19 03:33:17 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.5847
2022-08-19 03:33:52 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.8371
2022-08-19 03:34:26 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.5551
2022-08-19 03:35:00 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.7605
2022-08-19 03:35:34 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.6461
2022-08-19 03:36:08 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.4655
2022-08-19 03:36:42 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.6315
2022-08-19 03:37:16 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.7426
2022-08-19 03:37:50 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.6158
2022-08-19 03:38:25 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.7703
2022-08-19 03:38:59 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.3908
2022-08-19 03:39:33 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.5026
2022-08-19 03:40:07 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.5972
2022-08-19 03:40:41 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.6740
2022-08-19 03:41:15 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.5846
2022-08-19 03:41:50 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.5124
2022-08-19 03:42:24 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.3643
2022-08-19 03:42:58 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.4607
2022-08-19 03:43:33 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.6504
2022-08-19 03:44:08 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5717
2022-08-19 03:44:42 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.6272
2022-08-19 03:45:17 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.5394
2022-08-19 03:45:51 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.5489
2022-08-19 03:46:26 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.4646
2022-08-19 03:47:01 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.5809
2022-08-19 03:47:35 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.6301
2022-08-19 03:48:10 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.7047
2022-08-19 03:48:44 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.5769
2022-08-19 03:49:19 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.5327
2022-08-19 03:49:54 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.3495
2022-08-19 03:50:28 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.4856
2022-08-19 03:51:03 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6343
2022-08-19 03:51:37 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.4259
2022-08-19 03:52:11 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.6390
2022-08-19 03:52:46 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.8031
2022-08-19 03:53:20 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.6865
2022-08-19 03:53:55 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.4425
2022-08-19 03:54:29 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.7114
2022-08-19 03:55:04 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.6369
2022-08-19 03:55:39 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.6120
2022-08-19 03:56:14 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.5710
2022-08-19 03:56:49 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.5516
2022-08-19 03:57:24 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.2772
2022-08-19 03:57:58 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.4540
2022-08-19 03:58:33 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.6955
2022-08-19 03:59:07 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.7196
2022-08-19 03:59:08 - train: epoch 005, train_loss: 2.6075
2022-08-19 04:00:25 - eval: epoch: 005, acc1: 46.736%, acc5: 73.506%, test_loss: 2.3149, per_image_load_time: 1.990ms, per_image_inference_time: 0.604ms
2022-08-19 04:00:25 - until epoch: 005, best_acc1: 46.736%
2022-08-19 04:00:25 - epoch 006 lr: 0.090450
2022-08-19 04:01:07 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.3811
2022-08-19 04:01:41 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.6022
2022-08-19 04:02:15 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.5378
2022-08-19 04:02:50 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.6208
2022-08-19 04:03:24 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.2818
2022-08-19 04:03:59 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.4819
2022-08-19 04:04:33 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.6460
2022-08-19 04:05:08 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.5846
2022-08-19 04:05:42 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.4057
2022-08-19 04:06:17 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.1921
2022-08-19 04:06:51 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.4990
2022-08-19 04:07:26 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.6946
2022-08-19 04:08:00 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5544
2022-08-19 04:08:35 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.6769
2022-08-19 04:09:09 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.6294
2022-08-19 04:09:44 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.4202
2022-08-19 04:10:18 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.7407
2022-08-19 04:10:53 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.5884
2022-08-19 04:11:28 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.4802
2022-08-19 04:12:02 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.6000
2022-08-19 04:12:37 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.6088
2022-08-19 04:13:11 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.4986
2022-08-19 04:13:46 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.2364
2022-08-19 04:14:21 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.3621
2022-08-19 04:14:56 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.6267
2022-08-19 04:15:30 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.2471
2022-08-19 04:16:05 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.4402
2022-08-19 04:16:39 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.1397
2022-08-19 04:17:14 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.6813
2022-08-19 04:17:48 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.5348
2022-08-19 04:18:22 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.4547
2022-08-19 04:18:56 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.4242
2022-08-19 04:19:31 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.4363
2022-08-19 04:20:05 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.6707
2022-08-19 04:20:40 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.5193
2022-08-19 04:21:14 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.6532
2022-08-19 04:21:49 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.4081
2022-08-19 04:22:23 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.2265
2022-08-19 04:22:58 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.4970
2022-08-19 04:23:32 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.5581
2022-08-19 04:24:07 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.4729
2022-08-19 04:24:41 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.2920
2022-08-19 04:25:15 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.3877
2022-08-19 04:25:50 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.4330
2022-08-19 04:26:24 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.4139
2022-08-19 04:26:59 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.7747
2022-08-19 04:27:34 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.3904
2022-08-19 04:28:08 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.4243
2022-08-19 04:28:43 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.6275
2022-08-19 04:29:17 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.4380
2022-08-19 04:29:18 - train: epoch 006, train_loss: 2.4870
2022-08-19 04:30:36 - eval: epoch: 006, acc1: 48.306%, acc5: 74.418%, test_loss: 2.2647, per_image_load_time: 2.235ms, per_image_inference_time: 0.610ms
2022-08-19 04:30:36 - until epoch: 006, best_acc1: 48.306%
2022-08-19 04:30:36 - epoch 007 lr: 0.086448
2022-08-19 04:31:17 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.2267
2022-08-19 04:31:51 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.6481
2022-08-19 04:32:25 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.8297
2022-08-19 04:32:59 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.5798
2022-08-19 04:33:34 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.3146
2022-08-19 04:34:08 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.4511
2022-08-19 04:34:42 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.3171
2022-08-19 04:35:17 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.2868
2022-08-19 04:35:51 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.3768
2022-08-19 04:36:25 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.5233
2022-08-19 04:37:00 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.4010
2022-08-19 04:37:34 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.4097
2022-08-19 04:38:08 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.1495
2022-08-19 04:38:43 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.3666
2022-08-19 04:39:17 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.5511
2022-08-19 04:39:52 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.4237
2022-08-19 04:40:27 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.3106
2022-08-19 04:41:02 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.4254
2022-08-19 04:41:36 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.3900
2022-08-19 04:42:11 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.2814
2022-08-19 04:42:46 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.6345
2022-08-19 04:43:20 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.4068
2022-08-19 04:43:55 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.4782
2022-08-19 04:44:30 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.6042
2022-08-19 04:45:05 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.4264
2022-08-19 04:45:39 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.4393
2022-08-19 04:46:14 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.3959
2022-08-19 04:46:48 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.3124
2022-08-19 04:47:23 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.2649
2022-08-19 04:47:57 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.3778
2022-08-19 04:48:31 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.3343
2022-08-19 04:49:06 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.2676
2022-08-19 04:49:40 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.4129
2022-08-19 04:50:15 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.4308
2022-08-19 04:50:49 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.3951
2022-08-19 04:51:24 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.2914
2022-08-19 04:51:58 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.2438
2022-08-19 04:52:32 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.5497
2022-08-19 04:53:07 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.3796
2022-08-19 04:53:41 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.5264
2022-08-19 04:54:16 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.3226
2022-08-19 04:54:50 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2767
2022-08-19 04:55:25 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.5006
2022-08-19 04:55:59 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.2415
2022-08-19 04:56:34 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.5046
2022-08-19 04:57:09 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.4868
2022-08-19 04:57:43 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.2027
2022-08-19 04:58:18 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.5013
2022-08-19 04:58:53 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.1972
2022-08-19 04:59:26 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.3613
2022-08-19 04:59:28 - train: epoch 007, train_loss: 2.3937
2022-08-19 05:00:45 - eval: epoch: 007, acc1: 52.592%, acc5: 78.100%, test_loss: 2.0400, per_image_load_time: 2.385ms, per_image_inference_time: 0.600ms
2022-08-19 05:00:45 - until epoch: 007, best_acc1: 52.592%
2022-08-19 05:00:45 - epoch 008 lr: 0.081870
2022-08-19 05:01:26 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.2873
2022-08-19 05:02:00 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.3538
2022-08-19 05:02:34 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.3550
2022-08-19 05:03:08 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.1604
2022-08-19 05:03:42 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.3078
2022-08-19 05:04:16 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.4377
2022-08-19 05:04:50 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.3866
2022-08-19 05:05:25 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.0851
2022-08-19 05:05:59 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.3796
2022-08-19 05:06:34 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.4252
2022-08-19 05:07:08 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.1241
2022-08-19 05:07:42 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.1983
2022-08-19 05:08:16 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.4942
2022-08-19 05:08:50 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.4417
2022-08-19 05:09:24 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.3984
2022-08-19 05:09:59 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.3223
2022-08-19 05:10:33 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.3478
2022-08-19 05:11:07 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.1058
2022-08-19 05:11:41 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.1763
2022-08-19 05:12:15 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.3596
2022-08-19 05:12:50 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.3302
2022-08-19 05:13:24 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.1785
2022-08-19 05:13:59 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.2577
2022-08-19 05:14:33 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.2189
2022-08-19 05:15:07 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.2433
2022-08-19 05:15:42 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.3838
2022-08-19 05:16:16 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.4392
2022-08-19 05:16:50 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.3274
2022-08-19 05:17:25 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.4142
2022-08-19 05:17:59 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.3637
2022-08-19 05:18:34 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.5065
2022-08-19 05:19:09 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.5532
2022-08-19 05:19:43 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.3924
2022-08-19 05:20:18 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.2992
2022-08-19 05:20:52 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.3004
2022-08-19 05:21:26 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.6466
2022-08-19 05:22:01 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.1670
2022-08-19 05:22:35 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.3698
2022-08-19 05:23:10 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.3334
2022-08-19 05:23:45 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.3923
2022-08-19 05:24:19 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.0934
2022-08-19 05:24:54 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.3347
2022-08-19 05:25:28 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.1125
2022-08-19 05:26:03 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.3052
2022-08-19 05:26:37 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.3534
2022-08-19 05:27:12 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.2918
2022-08-19 05:27:46 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.3670
2022-08-19 05:28:21 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.1954
2022-08-19 05:28:56 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.4972
2022-08-19 05:29:29 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.3442
2022-08-19 05:29:31 - train: epoch 008, train_loss: 2.3245
2022-08-19 05:30:48 - eval: epoch: 008, acc1: 51.776%, acc5: 77.666%, test_loss: 2.0600, per_image_load_time: 2.331ms, per_image_inference_time: 0.606ms
2022-08-19 05:30:48 - until epoch: 008, best_acc1: 52.592%
2022-08-19 05:30:48 - epoch 009 lr: 0.076790
2022-08-19 05:31:29 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 2.0564
2022-08-19 05:32:03 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.2886
2022-08-19 05:32:38 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.1166
2022-08-19 05:33:12 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.5689
2022-08-19 05:33:46 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.1439
2022-08-19 05:34:20 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.2585
2022-08-19 05:34:54 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.1138
2022-08-19 05:35:28 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.1745
2022-08-19 05:36:03 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.0069
2022-08-19 05:36:37 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1717
2022-08-19 05:37:11 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.6204
2022-08-19 05:37:45 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.3420
2022-08-19 05:38:19 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.3280
2022-08-19 05:38:53 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9128
2022-08-19 05:39:28 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.1319
2022-08-19 05:40:02 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.3560
2022-08-19 05:40:36 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.3410
2022-08-19 05:41:11 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.1467
2022-08-19 05:41:46 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.1032
2022-08-19 05:42:20 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.1548
2022-08-19 05:42:54 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.2997
2022-08-19 05:43:29 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.2933
2022-08-19 05:44:03 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.2501
2022-08-19 05:44:38 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.3142
2022-08-19 05:45:12 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.1511
2022-08-19 05:45:46 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.3047
2022-08-19 05:46:21 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.0499
2022-08-19 05:46:55 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2529
2022-08-19 05:47:29 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.2165
2022-08-19 05:48:04 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.0997
2022-08-19 05:48:38 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.4820
2022-08-19 05:49:13 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.2219
2022-08-19 05:49:48 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.3783
2022-08-19 05:50:22 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.1862
2022-08-19 05:50:57 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.2663
2022-08-19 05:51:31 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 1.9602
2022-08-19 05:52:06 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.3737
2022-08-19 05:52:40 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.3982
2022-08-19 05:53:15 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.2087
2022-08-19 05:53:50 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.5087
2022-08-19 05:54:24 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.3338
2022-08-19 05:54:59 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.0872
2022-08-19 05:55:34 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.1001
2022-08-19 05:56:09 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.4679
2022-08-19 05:56:44 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.4337
2022-08-19 05:57:19 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.3031
2022-08-19 05:57:53 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.4963
2022-08-19 05:58:28 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.2168
2022-08-19 05:59:02 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.3217
2022-08-19 05:59:36 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 1.9585
2022-08-19 05:59:37 - train: epoch 009, train_loss: 2.2570
2022-08-19 06:00:55 - eval: epoch: 009, acc1: 52.064%, acc5: 77.462%, test_loss: 2.0799, per_image_load_time: 2.396ms, per_image_inference_time: 0.611ms
2022-08-19 06:00:55 - until epoch: 009, best_acc1: 52.592%
2022-08-19 06:00:55 - epoch 010 lr: 0.071288
2022-08-19 06:01:36 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.1365
2022-08-19 06:02:11 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.1421
2022-08-19 06:02:45 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.2029
2022-08-19 06:03:19 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.2401
2022-08-19 06:03:54 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.2711
2022-08-19 06:04:28 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.1328
2022-08-19 06:05:02 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.3308
2022-08-19 06:05:36 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.0194
2022-08-19 06:06:11 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 1.9637
2022-08-19 06:06:45 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.1556
2022-08-19 06:07:19 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.2006
2022-08-19 06:07:54 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.1542
2022-08-19 06:08:28 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.1938
2022-08-19 06:09:03 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.4165
2022-08-19 06:09:37 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.9186
2022-08-19 06:10:11 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 1.9803
2022-08-19 06:10:46 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.2893
2022-08-19 06:11:20 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 1.9519
2022-08-19 06:11:54 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.1419
2022-08-19 06:12:28 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.1274
2022-08-19 06:13:03 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 2.0880
2022-08-19 06:13:37 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.3737
2022-08-19 06:14:12 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.1954
2022-08-19 06:14:46 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.4446
2022-08-19 06:15:21 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.2273
2022-08-19 06:15:55 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.2413
2022-08-19 06:16:30 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.9638
2022-08-19 06:17:04 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.2670
2022-08-19 06:17:39 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.4156
2022-08-19 06:18:13 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.0777
2022-08-19 06:18:48 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.2219
2022-08-19 06:19:22 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.3445
2022-08-19 06:19:57 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.2726
2022-08-19 06:20:31 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.2518
2022-08-19 06:21:05 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.4676
2022-08-19 06:21:40 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.5088
2022-08-19 06:22:14 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 1.9865
2022-08-19 06:22:49 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.3618
2022-08-19 06:23:23 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 1.8842
2022-08-19 06:23:58 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.1936
2022-08-19 06:24:32 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 1.8902
2022-08-19 06:25:07 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.1743
2022-08-19 06:25:41 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.0962
2022-08-19 06:26:16 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.1010
2022-08-19 06:26:51 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.2142
2022-08-19 06:27:25 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.1756
2022-08-19 06:28:00 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.0895
2022-08-19 06:28:34 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.0457
2022-08-19 06:29:09 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.2559
2022-08-19 06:29:43 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.0681
2022-08-19 06:29:44 - train: epoch 010, train_loss: 2.1942
2022-08-19 06:31:01 - eval: epoch: 010, acc1: 55.910%, acc5: 80.324%, test_loss: 1.8899, per_image_load_time: 2.156ms, per_image_inference_time: 0.609ms
2022-08-19 06:31:01 - until epoch: 010, best_acc1: 55.910%
2022-08-19 06:31:01 - epoch 011 lr: 0.065450
2022-08-19 06:31:42 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.0000
2022-08-19 06:32:16 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.3105
2022-08-19 06:32:51 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.2084
2022-08-19 06:33:25 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.4571
2022-08-19 06:33:59 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.0455
2022-08-19 06:34:33 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.1952
2022-08-19 06:35:07 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.1061
2022-08-19 06:35:41 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.3164
2022-08-19 06:36:16 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.2819
2022-08-19 06:36:50 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.1482
2022-08-19 06:37:24 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2630
2022-08-19 06:37:58 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2203
2022-08-19 06:38:33 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.4231
2022-08-19 06:39:07 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.0839
2022-08-19 06:39:42 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.0659
2022-08-19 06:40:16 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.2864
2022-08-19 06:40:51 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.0313
2022-08-19 06:41:25 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.1753
2022-08-19 06:42:00 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.1471
2022-08-19 06:42:34 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.0678
2022-08-19 06:43:09 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.1198
2022-08-19 06:43:43 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 1.9032
2022-08-19 06:44:17 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.2862
2022-08-19 06:44:51 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.8885
2022-08-19 06:45:25 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.3062
2022-08-19 06:46:00 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.1078
2022-08-19 06:46:34 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.0208
2022-08-19 06:47:09 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.8187
2022-08-19 06:47:43 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.2325
2022-08-19 06:48:18 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.3998
2022-08-19 06:48:52 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.2496
2022-08-19 06:49:27 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.1511
2022-08-19 06:50:01 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.2023
2022-08-19 06:50:36 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.0696
2022-08-19 06:51:10 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.1339
2022-08-19 06:51:44 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.1485
2022-08-19 06:52:19 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.1684
2022-08-19 06:52:53 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 2.0963
2022-08-19 06:53:28 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.1549
2022-08-19 06:54:02 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.1752
2022-08-19 06:54:37 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.9633
2022-08-19 06:55:11 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 2.1045
2022-08-19 06:55:46 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.0279
2022-08-19 06:56:20 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.2222
2022-08-19 06:56:55 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 1.9591
2022-08-19 06:57:30 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 1.9753
2022-08-19 06:58:04 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 2.0307
2022-08-19 06:58:39 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.9239
2022-08-19 06:59:13 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.0776
2022-08-19 06:59:47 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.0347
2022-08-19 06:59:48 - train: epoch 011, train_loss: 2.1340
2022-08-19 07:01:06 - eval: epoch: 011, acc1: 56.048%, acc5: 80.704%, test_loss: 1.8608, per_image_load_time: 2.403ms, per_image_inference_time: 0.578ms
2022-08-19 07:01:06 - until epoch: 011, best_acc1: 56.048%
2022-08-19 07:01:06 - epoch 012 lr: 0.059368
2022-08-19 07:01:47 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.9872
2022-08-19 07:02:21 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.9550
2022-08-19 07:02:55 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 2.0850
2022-08-19 07:03:29 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.2206
2022-08-19 07:04:04 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.1412
2022-08-19 07:04:37 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 2.1082
2022-08-19 07:05:11 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.9920
2022-08-19 07:05:45 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.1086
2022-08-19 07:06:19 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.1415
2022-08-19 07:06:54 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9585
2022-08-19 07:07:27 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.2338
2022-08-19 07:08:02 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.9187
2022-08-19 07:08:36 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.8418
2022-08-19 07:09:11 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.1951
2022-08-19 07:09:45 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 1.9487
2022-08-19 07:10:19 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 2.0643
2022-08-19 07:10:54 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 2.0105
2022-08-19 07:11:28 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.0969
2022-08-19 07:12:03 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1291
2022-08-19 07:12:38 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.1000
2022-08-19 07:13:12 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.0853
2022-08-19 07:13:47 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.0837
2022-08-19 07:14:22 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 1.9801
2022-08-19 07:14:56 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.2120
2022-08-19 07:15:31 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.8514
2022-08-19 07:16:06 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.8485
2022-08-19 07:16:41 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 1.9648
2022-08-19 07:17:16 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.9854
2022-08-19 07:17:51 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.1622
2022-08-19 07:18:25 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.9215
2022-08-19 07:19:00 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.0698
2022-08-19 07:19:35 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.8970
2022-08-19 07:20:09 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.0203
2022-08-19 07:20:44 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.0556
2022-08-19 07:21:19 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 1.9519
2022-08-19 07:21:53 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 2.3109
2022-08-19 07:22:28 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.0922
2022-08-19 07:23:02 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.3435
2022-08-19 07:23:37 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 2.0526
2022-08-19 07:24:12 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1743
2022-08-19 07:24:46 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.1994
2022-08-19 07:25:21 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 2.0060
2022-08-19 07:25:56 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.0313
2022-08-19 07:26:30 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 2.0180
2022-08-19 07:27:05 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 2.1011
2022-08-19 07:27:39 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.0554
2022-08-19 07:28:14 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 2.0729
2022-08-19 07:28:49 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 1.9921
2022-08-19 07:29:23 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.7917
2022-08-19 07:29:57 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.6179
2022-08-19 07:29:59 - train: epoch 012, train_loss: 2.0743
2022-08-19 07:31:17 - eval: epoch: 012, acc1: 58.176%, acc5: 81.886%, test_loss: 1.7821, per_image_load_time: 2.381ms, per_image_inference_time: 0.590ms
2022-08-19 07:31:17 - until epoch: 012, best_acc1: 58.176%
2022-08-19 07:31:17 - epoch 013 lr: 0.053138
2022-08-19 07:31:58 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.9984
2022-08-19 07:32:32 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.9360
2022-08-19 07:33:06 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.9768
2022-08-19 07:33:41 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.7054
2022-08-19 07:34:15 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 2.0444
2022-08-19 07:34:50 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 2.1545
2022-08-19 07:35:24 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.9942
2022-08-19 07:35:58 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.1040
2022-08-19 07:36:32 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.9151
2022-08-19 07:37:07 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.9019
2022-08-19 07:37:41 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.0059
2022-08-19 07:38:16 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.0801
2022-08-19 07:38:50 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.8461
2022-08-19 07:39:25 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.1946
2022-08-19 07:39:59 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.1770
2022-08-19 07:40:34 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.6723
2022-08-19 07:41:08 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 1.9533
2022-08-19 07:41:43 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.9674
2022-08-19 07:42:17 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.0971
2022-08-19 07:42:51 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.0877
2022-08-19 07:43:26 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.4407
2022-08-19 07:44:01 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.8314
2022-08-19 07:44:36 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 2.1100
2022-08-19 07:45:10 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.2031
2022-08-19 07:45:45 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 2.0067
2022-08-19 07:46:19 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.9071
2022-08-19 07:46:54 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.9401
2022-08-19 07:47:29 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.1332
2022-08-19 07:48:03 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 1.8096
2022-08-19 07:48:38 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.8451
2022-08-19 07:49:13 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.9996
2022-08-19 07:49:48 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 1.9400
2022-08-19 07:50:23 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.9757
2022-08-19 07:50:57 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.8641
2022-08-19 07:51:32 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.9346
2022-08-19 07:52:06 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.1424
2022-08-19 07:52:41 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.8032
2022-08-19 07:53:16 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1730
2022-08-19 07:53:50 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.3149
2022-08-19 07:54:25 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.9620
2022-08-19 07:54:59 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.9806
2022-08-19 07:55:34 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 2.1026
2022-08-19 07:56:09 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.9123
2022-08-19 07:56:43 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.9259
2022-08-19 07:57:18 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.7973
2022-08-19 07:57:53 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 1.9281
2022-08-19 07:58:28 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.9546
2022-08-19 07:59:02 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.0352
2022-08-19 07:59:37 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.2248
2022-08-19 08:00:11 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 1.9045
2022-08-19 08:00:12 - train: epoch 013, train_loss: 2.0142
2022-08-19 08:01:31 - eval: epoch: 013, acc1: 59.218%, acc5: 83.068%, test_loss: 1.7017, per_image_load_time: 2.422ms, per_image_inference_time: 0.592ms
2022-08-19 08:01:31 - until epoch: 013, best_acc1: 59.218%
2022-08-19 08:01:31 - epoch 014 lr: 0.046859
2022-08-19 08:02:12 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.9117
2022-08-19 08:02:46 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 2.1148
2022-08-19 08:03:20 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.8952
2022-08-19 08:03:54 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.8770
2022-08-19 08:04:29 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.7400
2022-08-19 08:05:03 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 2.0829
2022-08-19 08:05:37 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.8706
2022-08-19 08:06:12 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.7697
2022-08-19 08:06:46 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.8832
2022-08-19 08:07:20 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.0874
2022-08-19 08:07:54 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 2.0934
2022-08-19 08:08:28 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 1.9927
2022-08-19 08:09:03 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.1156
2022-08-19 08:09:37 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.8435
2022-08-19 08:10:12 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.0872
2022-08-19 08:10:46 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.9833
2022-08-19 08:11:21 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9847
2022-08-19 08:11:55 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.2123
2022-08-19 08:12:30 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.8367
2022-08-19 08:13:04 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.9228
2022-08-19 08:13:38 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.0106
2022-08-19 08:14:13 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.9738
2022-08-19 08:14:47 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.9245
2022-08-19 08:15:21 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 2.0169
2022-08-19 08:15:55 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.8894
2022-08-19 08:16:30 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.9881
2022-08-19 08:17:04 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.7706
2022-08-19 08:17:39 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 2.0460
2022-08-19 08:18:13 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 2.0889
2022-08-19 08:18:47 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.9000
2022-08-19 08:19:22 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.9162
2022-08-19 08:19:56 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.8533
2022-08-19 08:20:31 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.9285
2022-08-19 08:21:05 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 2.0081
2022-08-19 08:21:40 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.9484
2022-08-19 08:22:14 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.8019
2022-08-19 08:22:49 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.7837
2022-08-19 08:23:23 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 1.9765
2022-08-19 08:23:57 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 2.0776
2022-08-19 08:24:31 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.9066
2022-08-19 08:25:06 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.9617
2022-08-19 08:25:40 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.9487
2022-08-19 08:26:14 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.6717
2022-08-19 08:26:48 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 2.0632
2022-08-19 08:27:22 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.7406
2022-08-19 08:27:57 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.7866
2022-08-19 08:28:31 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 2.0249
2022-08-19 08:29:06 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 2.0416
2022-08-19 08:29:40 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 2.0020
2022-08-19 08:30:13 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 2.0668
2022-08-19 08:30:15 - train: epoch 014, train_loss: 1.9578
2022-08-19 08:31:33 - eval: epoch: 014, acc1: 60.444%, acc5: 83.530%, test_loss: 1.6620, per_image_load_time: 2.378ms, per_image_inference_time: 0.610ms
2022-08-19 08:31:34 - until epoch: 014, best_acc1: 60.444%
2022-08-19 08:31:34 - epoch 015 lr: 0.040630
2022-08-19 08:32:16 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.7196
2022-08-19 08:32:50 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 2.0133
2022-08-19 08:33:24 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.0860
2022-08-19 08:33:59 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 2.0233
2022-08-19 08:34:33 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.9083
2022-08-19 08:35:07 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 2.0514
2022-08-19 08:35:41 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 2.0764
2022-08-19 08:36:16 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.7911
2022-08-19 08:36:50 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.7882
2022-08-19 08:37:25 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 2.1253
2022-08-19 08:37:59 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7210
2022-08-19 08:38:33 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.8281
2022-08-19 08:39:08 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.9569
2022-08-19 08:39:42 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.9417
2022-08-19 08:40:17 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.8688
2022-08-19 08:40:51 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.9982
2022-08-19 08:41:26 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.9072
2022-08-19 08:42:00 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.7996
2022-08-19 08:42:35 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.6786
2022-08-19 08:43:10 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.8510
2022-08-19 08:43:45 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.6498
2022-08-19 08:44:19 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9485
2022-08-19 08:44:54 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.8627
2022-08-19 08:45:29 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.8032
2022-08-19 08:46:04 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 2.0327
2022-08-19 08:46:38 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.6886
2022-08-19 08:47:12 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.8618
2022-08-19 08:47:47 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.8212
2022-08-19 08:48:21 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.9984
2022-08-19 08:48:56 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.9541
2022-08-19 08:49:31 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.8621
2022-08-19 08:50:06 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.9351
2022-08-19 08:50:41 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.7410
2022-08-19 08:51:16 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.7984
2022-08-19 08:51:51 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0034
2022-08-19 08:52:26 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.7647
2022-08-19 08:53:01 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.7503
2022-08-19 08:53:36 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.8269
2022-08-19 08:54:11 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8481
2022-08-19 08:54:45 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.8896
2022-08-19 08:55:20 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.8740
2022-08-19 08:55:55 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6898
2022-08-19 08:56:30 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.7120
2022-08-19 08:57:05 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.8773
2022-08-19 08:57:40 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.6607
2022-08-19 08:58:14 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.9533
2022-08-19 08:58:49 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.7678
2022-08-19 08:59:23 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.6565
2022-08-19 08:59:58 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.7036
2022-08-19 09:00:32 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 2.0286
2022-08-19 09:00:33 - train: epoch 015, train_loss: 1.8951
2022-08-19 09:01:51 - eval: epoch: 015, acc1: 62.360%, acc5: 85.016%, test_loss: 1.5692, per_image_load_time: 2.416ms, per_image_inference_time: 0.576ms
2022-08-19 09:01:51 - until epoch: 015, best_acc1: 62.360%
2022-08-19 09:01:51 - epoch 016 lr: 0.034548
2022-08-19 09:02:33 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.6251
2022-08-19 09:03:07 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.8964
2022-08-19 09:03:42 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.7883
2022-08-19 09:04:17 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.0268
2022-08-19 09:04:51 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.6756
2022-08-19 09:05:25 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.7276
2022-08-19 09:06:00 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.6668
2022-08-19 09:06:35 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.9742
2022-08-19 09:07:09 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.9418
2022-08-19 09:07:44 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.9080
2022-08-19 09:08:19 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.7981
2022-08-19 09:08:54 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.8065
2022-08-19 09:09:29 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.7772
2022-08-19 09:10:04 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.6282
2022-08-19 09:10:38 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 2.0363
2022-08-19 09:11:13 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.8720
2022-08-19 09:11:48 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.6768
2022-08-19 09:12:23 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.9875
2022-08-19 09:12:57 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.9058
2022-08-19 09:13:32 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.5838
2022-08-19 09:14:07 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 2.0701
2022-08-19 09:14:42 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.8647
2022-08-19 09:15:17 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.9715
2022-08-19 09:15:52 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.9957
2022-08-19 09:16:27 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.7940
2022-08-19 09:17:02 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.9369
2022-08-19 09:17:37 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.9202
2022-08-19 09:18:12 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.7088
2022-08-19 09:18:47 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.9141
2022-08-19 09:19:22 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.9335
2022-08-19 09:19:57 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 2.0798
2022-08-19 09:20:32 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.8597
2022-08-19 09:21:07 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 2.0343
2022-08-19 09:21:42 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.7302
2022-08-19 09:22:17 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.7885
2022-08-19 09:22:52 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.8655
2022-08-19 09:23:27 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.8734
2022-08-19 09:24:02 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.2133
2022-08-19 09:24:37 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.8961
2022-08-19 09:25:12 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.9071
2022-08-19 09:25:47 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.8897
2022-08-19 09:26:22 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.7832
2022-08-19 09:26:57 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.7802
2022-08-19 09:27:32 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.5484
2022-08-19 09:28:07 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.8178
2022-08-19 09:28:42 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.5972
2022-08-19 09:29:17 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.9902
2022-08-19 09:29:52 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.8073
2022-08-19 09:30:27 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.9265
2022-08-19 09:31:01 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.7933
2022-08-19 09:31:03 - train: epoch 016, train_loss: 1.8303
2022-08-19 09:32:22 - eval: epoch: 016, acc1: 63.546%, acc5: 85.446%, test_loss: 1.5147, per_image_load_time: 2.412ms, per_image_inference_time: 0.622ms
2022-08-19 09:32:22 - until epoch: 016, best_acc1: 63.546%
2022-08-19 09:32:22 - epoch 017 lr: 0.028710
2022-08-19 09:33:03 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.6917
2022-08-19 09:33:37 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.7165
2022-08-19 09:34:11 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 2.0818
2022-08-19 09:34:46 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.5319
2022-08-19 09:35:21 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.7882
2022-08-19 09:35:56 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 2.0452
2022-08-19 09:36:31 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.7094
2022-08-19 09:37:05 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.9172
2022-08-19 09:37:40 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.6342
2022-08-19 09:38:15 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.6970
2022-08-19 09:38:50 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.9320
2022-08-19 09:39:25 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.8775
2022-08-19 09:40:00 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.8058
2022-08-19 09:40:35 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.6140
2022-08-19 09:41:10 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.5913
2022-08-19 09:41:45 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.6448
2022-08-19 09:42:21 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.8246
2022-08-19 09:42:56 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.8399
2022-08-19 09:43:31 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.8091
2022-08-19 09:44:05 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.7187
2022-08-19 09:44:41 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.5350
2022-08-19 09:45:15 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.5584
2022-08-19 09:45:50 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.7649
2022-08-19 09:46:25 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.6298
2022-08-19 09:47:00 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 2.0229
2022-08-19 09:47:35 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.6348
2022-08-19 09:48:09 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6551
2022-08-19 09:48:44 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.7640
2022-08-19 09:49:19 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.7904
2022-08-19 09:49:54 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.5970
2022-08-19 09:50:29 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8755
2022-08-19 09:51:03 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.4376
2022-08-19 09:51:38 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.8573
2022-08-19 09:52:13 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.5479
2022-08-19 09:52:48 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.6509
2022-08-19 09:53:23 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.8186
2022-08-19 09:53:57 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.6775
2022-08-19 09:54:32 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.7532
2022-08-19 09:55:07 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.6723
2022-08-19 09:55:42 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.8574
2022-08-19 09:56:17 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.9011
2022-08-19 09:56:52 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.6305
2022-08-19 09:57:27 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.7037
2022-08-19 09:58:01 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8178
2022-08-19 09:58:36 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.8248
2022-08-19 09:59:11 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.4763
2022-08-19 09:59:46 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.8992
2022-08-19 10:00:21 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.7835
2022-08-19 10:00:55 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.7085
2022-08-19 10:01:29 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.6533
2022-08-19 10:01:31 - train: epoch 017, train_loss: 1.7635
2022-08-19 10:02:49 - eval: epoch: 017, acc1: 64.342%, acc5: 86.222%, test_loss: 1.4756, per_image_load_time: 2.395ms, per_image_inference_time: 0.588ms
2022-08-19 10:02:49 - until epoch: 017, best_acc1: 64.342%
2022-08-19 10:02:49 - epoch 018 lr: 0.023208
2022-08-19 10:03:30 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.6172
2022-08-19 10:04:04 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.9535
2022-08-19 10:04:39 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.8858
2022-08-19 10:05:14 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.7346
2022-08-19 10:05:48 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.6258
2022-08-19 10:06:23 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.7915
2022-08-19 10:06:58 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.6387
2022-08-19 10:07:32 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.8479
2022-08-19 10:08:07 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.6243
2022-08-19 10:08:41 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.7462
2022-08-19 10:09:16 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.8330
2022-08-19 10:09:50 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.6698
2022-08-19 10:10:25 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.8938
2022-08-19 10:10:59 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.6360
2022-08-19 10:11:34 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.7500
2022-08-19 10:12:08 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.7438
2022-08-19 10:12:43 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.8420
2022-08-19 10:13:17 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6799
2022-08-19 10:13:52 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.6584
2022-08-19 10:14:27 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.9346
2022-08-19 10:15:02 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.8884
2022-08-19 10:15:36 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.7826
2022-08-19 10:16:11 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.7122
2022-08-19 10:16:46 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.6871
2022-08-19 10:17:21 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.7224
2022-08-19 10:17:56 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.7619
2022-08-19 10:18:30 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.7797
2022-08-19 10:19:05 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.6263
2022-08-19 10:19:40 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.6253
2022-08-19 10:20:15 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.7121
2022-08-19 10:20:49 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.9712
2022-08-19 10:21:24 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.5321
2022-08-19 10:21:59 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.5910
2022-08-19 10:22:34 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.7300
2022-08-19 10:23:09 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.9200
2022-08-19 10:23:44 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.6141
2022-08-19 10:24:19 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.8131
2022-08-19 10:24:54 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.7811
2022-08-19 10:25:29 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.7779
2022-08-19 10:26:04 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.8046
2022-08-19 10:26:38 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7771
2022-08-19 10:27:13 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.7513
2022-08-19 10:27:48 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.4674
2022-08-19 10:28:23 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.5594
2022-08-19 10:28:59 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.7850
2022-08-19 10:29:34 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.7521
2022-08-19 10:30:09 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.8625
2022-08-19 10:30:44 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6215
2022-08-19 10:31:19 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.4990
2022-08-19 10:31:53 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.5565
2022-08-19 10:31:55 - train: epoch 018, train_loss: 1.6945
2022-08-19 10:33:13 - eval: epoch: 018, acc1: 66.114%, acc5: 87.412%, test_loss: 1.3894, per_image_load_time: 2.434ms, per_image_inference_time: 0.609ms
2022-08-19 10:33:14 - until epoch: 018, best_acc1: 66.114%
2022-08-19 10:33:14 - epoch 019 lr: 0.018128
2022-08-19 10:33:55 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4638
2022-08-19 10:34:29 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.5464
2022-08-19 10:35:04 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.7222
2022-08-19 10:35:38 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.5080
2022-08-19 10:36:13 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.7575
2022-08-19 10:36:47 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.6224
2022-08-19 10:37:22 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.4955
2022-08-19 10:37:56 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.8307
2022-08-19 10:38:31 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.6568
2022-08-19 10:39:06 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.7296
2022-08-19 10:39:40 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.4440
2022-08-19 10:40:15 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.5569
2022-08-19 10:40:49 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.9028
2022-08-19 10:41:24 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.3039
2022-08-19 10:41:59 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.8129
2022-08-19 10:42:33 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.3765
2022-08-19 10:43:08 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.8422
2022-08-19 10:43:42 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.5453
2022-08-19 10:44:17 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.7629
2022-08-19 10:44:52 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.4597
2022-08-19 10:45:27 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.4094
2022-08-19 10:46:02 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.6849
2022-08-19 10:46:37 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.6623
2022-08-19 10:47:11 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.7161
2022-08-19 10:47:46 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.7169
2022-08-19 10:48:21 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.6590
2022-08-19 10:48:56 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.5393
2022-08-19 10:49:31 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.7059
2022-08-19 10:50:05 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.8563
2022-08-19 10:50:40 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.9097
2022-08-19 10:51:15 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.5572
2022-08-19 10:51:50 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.2897
2022-08-19 10:52:25 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.5432
2022-08-19 10:53:00 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.5218
2022-08-19 10:53:35 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.7443
2022-08-19 10:54:09 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.3694
2022-08-19 10:54:44 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.5672
2022-08-19 10:55:18 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.6897
2022-08-19 10:55:53 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.5155
2022-08-19 10:56:28 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.4744
2022-08-19 10:57:02 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.7531
2022-08-19 10:57:37 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.7130
2022-08-19 10:58:12 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.5720
2022-08-19 10:58:46 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.8097
2022-08-19 10:59:20 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.9213
2022-08-19 10:59:54 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.5971
2022-08-19 11:00:28 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.6103
2022-08-19 11:01:02 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.4996
2022-08-19 11:01:36 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.3926
2022-08-19 11:02:09 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.6717
2022-08-19 11:02:11 - train: epoch 019, train_loss: 1.6226
2022-08-19 11:03:28 - eval: epoch: 019, acc1: 66.898%, acc5: 87.860%, test_loss: 1.3496, per_image_load_time: 2.426ms, per_image_inference_time: 0.584ms
2022-08-19 11:03:29 - until epoch: 019, best_acc1: 66.898%
2022-08-19 11:03:29 - epoch 020 lr: 0.013551
2022-08-19 11:04:09 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.6758
2022-08-19 11:04:43 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.3177
2022-08-19 11:05:17 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.6434
2022-08-19 11:05:52 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.5754
2022-08-19 11:06:26 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.3445
2022-08-19 11:07:00 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.5843
2022-08-19 11:07:35 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.3063
2022-08-19 11:08:09 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.7441
2022-08-19 11:08:43 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.8427
2022-08-19 11:09:18 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.6956
2022-08-19 11:09:52 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.4428
2022-08-19 11:10:26 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.4768
2022-08-19 11:11:00 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.5842
2022-08-19 11:11:35 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.3231
2022-08-19 11:12:09 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.4983
2022-08-19 11:12:44 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.7241
2022-08-19 11:13:18 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.4165
2022-08-19 11:13:53 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.6268
2022-08-19 11:14:27 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.4297
2022-08-19 11:15:02 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.5956
2022-08-19 11:15:36 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.6594
2022-08-19 11:16:11 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.3642
2022-08-19 11:16:45 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.6407
2022-08-19 11:17:19 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.8322
2022-08-19 11:17:52 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.6098
2022-08-19 11:18:27 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.5995
2022-08-19 11:19:00 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.5925
2022-08-19 11:19:34 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.6339
2022-08-19 11:20:09 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.6181
2022-08-19 11:20:42 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.7577
2022-08-19 11:21:17 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.6513
2022-08-19 11:21:50 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.7252
2022-08-19 11:22:24 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3854
2022-08-19 11:22:59 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.6691
2022-08-19 11:23:33 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.3984
2022-08-19 11:24:07 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.7205
2022-08-19 11:24:41 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.4118
2022-08-19 11:25:16 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.4590
2022-08-19 11:25:50 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.7085
2022-08-19 11:26:24 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.5949
2022-08-19 11:26:58 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.6006
2022-08-19 11:27:32 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.4149
2022-08-19 11:28:06 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.7125
2022-08-19 11:28:40 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.2802
2022-08-19 11:29:14 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.5779
2022-08-19 11:29:49 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.5547
2022-08-19 11:30:23 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.4001
2022-08-19 11:30:57 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.5048
2022-08-19 11:31:31 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.4866
2022-08-19 11:32:05 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.5348
2022-08-19 11:32:07 - train: epoch 020, train_loss: 1.5500
2022-08-19 11:33:24 - eval: epoch: 020, acc1: 68.510%, acc5: 88.868%, test_loss: 1.2798, per_image_load_time: 2.373ms, per_image_inference_time: 0.588ms
2022-08-19 11:33:24 - until epoch: 020, best_acc1: 68.510%
2022-08-19 11:33:24 - epoch 021 lr: 0.009548
2022-08-19 11:34:05 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.4436
2022-08-19 11:34:39 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.4408
2022-08-19 11:35:14 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.5431
2022-08-19 11:35:48 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4424
2022-08-19 11:36:22 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.2940
2022-08-19 11:36:56 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.4368
2022-08-19 11:37:31 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3650
2022-08-19 11:38:05 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.5937
2022-08-19 11:38:40 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.3608
2022-08-19 11:39:14 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.5350
2022-08-19 11:39:49 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.4767
2022-08-19 11:40:23 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.4096
2022-08-19 11:40:58 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.2336
2022-08-19 11:41:32 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.5199
2022-08-19 11:42:07 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.4043
2022-08-19 11:42:41 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.4768
2022-08-19 11:43:16 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5890
2022-08-19 11:43:50 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.4058
2022-08-19 11:44:25 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.6912
2022-08-19 11:44:59 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.6013
2022-08-19 11:45:33 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3384
2022-08-19 11:46:08 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.5440
2022-08-19 11:46:43 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.4913
2022-08-19 11:47:17 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.3681
2022-08-19 11:47:52 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.5356
2022-08-19 11:48:26 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.5234
2022-08-19 11:49:01 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.4237
2022-08-19 11:49:36 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.4547
2022-08-19 11:50:11 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.2673
2022-08-19 11:50:45 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.6579
2022-08-19 11:51:20 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.5078
2022-08-19 11:51:54 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.4562
2022-08-19 11:52:28 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.4411
2022-08-19 11:53:02 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.5777
2022-08-19 11:53:36 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4019
2022-08-19 11:54:10 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.4315
2022-08-19 11:54:44 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.3460
2022-08-19 11:55:19 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.4598
2022-08-19 11:55:53 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.3649
2022-08-19 11:56:28 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.5539
2022-08-19 11:57:01 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.4786
2022-08-19 11:57:35 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.3383
2022-08-19 11:58:08 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3975
2022-08-19 11:58:42 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.6105
2022-08-19 11:59:15 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.4828
2022-08-19 11:59:48 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.4528
2022-08-19 12:00:22 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.7709
2022-08-19 12:00:56 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.5273
2022-08-19 12:01:30 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.3722
2022-08-19 12:02:03 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.4303
2022-08-19 12:02:04 - train: epoch 021, train_loss: 1.4781
2022-08-19 12:03:21 - eval: epoch: 021, acc1: 69.758%, acc5: 89.500%, test_loss: 1.2288, per_image_load_time: 2.065ms, per_image_inference_time: 0.601ms
2022-08-19 12:03:21 - until epoch: 021, best_acc1: 69.758%
2022-08-19 12:03:21 - epoch 022 lr: 0.006184
2022-08-19 12:04:01 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.2255
2022-08-19 12:04:35 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.5894
2022-08-19 12:05:09 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.2447
2022-08-19 12:05:42 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.2813
2022-08-19 12:06:16 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.4596
2022-08-19 12:06:50 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.5211
2022-08-19 12:07:24 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4067
2022-08-19 12:07:58 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.5827
2022-08-19 12:08:32 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.6121
2022-08-19 12:09:06 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.4886
2022-08-19 12:09:40 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3637
2022-08-19 12:10:14 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.2195
2022-08-19 12:10:47 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.2902
2022-08-19 12:11:21 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.3115
2022-08-19 12:11:56 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.4652
2022-08-19 12:12:30 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.4162
2022-08-19 12:13:05 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.6994
2022-08-19 12:13:39 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.7115
2022-08-19 12:14:14 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.4515
2022-08-19 12:14:48 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4345
2022-08-19 12:15:22 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.4967
2022-08-19 12:15:56 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.1037
2022-08-19 12:16:30 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.5509
2022-08-19 12:17:05 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.4544
2022-08-19 12:17:39 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.4655
2022-08-19 12:18:14 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.3536
2022-08-19 12:18:49 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.1866
2022-08-19 12:19:23 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.4772
2022-08-19 12:19:58 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.4076
2022-08-19 12:20:32 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.5275
2022-08-19 12:21:06 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.4656
2022-08-19 12:21:41 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4064
2022-08-19 12:22:15 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.3488
2022-08-19 12:22:50 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.3292
2022-08-19 12:23:24 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.7577
2022-08-19 12:23:59 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.3702
2022-08-19 12:24:34 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.3915
2022-08-19 12:25:08 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.7428
2022-08-19 12:25:43 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.4542
2022-08-19 12:26:17 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.3683
2022-08-19 12:26:52 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.4305
2022-08-19 12:27:27 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.4631
2022-08-19 12:28:02 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4404
2022-08-19 12:28:36 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.4729
2022-08-19 12:29:11 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.1879
2022-08-19 12:29:46 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.6131
2022-08-19 12:30:20 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.5432
2022-08-19 12:30:54 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.4099
2022-08-19 12:31:28 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.3487
2022-08-19 12:32:02 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.3901
2022-08-19 12:32:03 - train: epoch 022, train_loss: 1.4124
2022-08-19 12:33:21 - eval: epoch: 022, acc1: 70.728%, acc5: 89.904%, test_loss: 1.1859, per_image_load_time: 2.356ms, per_image_inference_time: 0.568ms
2022-08-19 12:33:21 - until epoch: 022, best_acc1: 70.728%
2022-08-19 12:33:21 - epoch 023 lr: 0.003511
2022-08-19 12:34:03 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.2290
2022-08-19 12:34:37 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.3496
2022-08-19 12:35:12 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.4534
2022-08-19 12:35:46 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.3332
2022-08-19 12:36:20 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.4137
2022-08-19 12:36:54 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.3786
2022-08-19 12:37:29 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.1743
2022-08-19 12:38:03 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.4096
2022-08-19 12:38:38 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.2557
2022-08-19 12:39:12 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.3906
2022-08-19 12:39:47 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.3723
2022-08-19 12:40:21 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.3376
2022-08-19 12:40:57 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.3349
2022-08-19 12:41:31 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.3532
2022-08-19 12:42:05 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.2552
2022-08-19 12:42:39 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.2634
2022-08-19 12:43:14 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.3843
2022-08-19 12:43:48 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.3660
2022-08-19 12:44:23 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.3406
2022-08-19 12:44:58 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.1270
2022-08-19 12:45:32 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.4996
2022-08-19 12:46:07 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.2234
2022-08-19 12:46:42 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.1629
2022-08-19 12:47:16 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.2681
2022-08-19 12:47:50 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.3951
2022-08-19 12:48:25 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.3230
2022-08-19 12:49:00 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.4385
2022-08-19 12:49:35 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.3092
2022-08-19 12:50:10 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.4810
2022-08-19 12:50:45 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.4972
2022-08-19 12:51:20 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.4262
2022-08-19 12:51:55 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.4520
2022-08-19 12:52:29 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.5121
2022-08-19 12:53:04 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.4741
2022-08-19 12:53:38 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.2455
2022-08-19 12:54:13 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.3015
2022-08-19 12:54:49 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.3100
2022-08-19 12:55:24 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.3652
2022-08-19 12:55:59 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.4801
2022-08-19 12:56:34 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2388
2022-08-19 12:57:08 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.4030
2022-08-19 12:57:43 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.0621
2022-08-19 12:58:17 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.1752
2022-08-19 12:58:51 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.2242
2022-08-19 12:59:26 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.2159
2022-08-19 13:00:01 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.2964
2022-08-19 13:00:36 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.2151
2022-08-19 13:01:11 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.1880
2022-08-19 13:01:46 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.2334
2022-08-19 13:02:20 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.4728
2022-08-19 13:02:21 - train: epoch 023, train_loss: 1.3584
2022-08-19 13:03:37 - eval: epoch: 023, acc1: 71.366%, acc5: 90.320%, test_loss: 1.1550, per_image_load_time: 2.201ms, per_image_inference_time: 0.556ms
2022-08-19 13:03:37 - until epoch: 023, best_acc1: 71.366%
2022-08-19 13:03:37 - epoch 024 lr: 0.001571
2022-08-19 13:04:19 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.3137
2022-08-19 13:04:53 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.3643
2022-08-19 13:05:28 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.3199
2022-08-19 13:06:02 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.3812
2022-08-19 13:06:37 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.2917
2022-08-19 13:07:12 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.3160
2022-08-19 13:07:46 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.4327
2022-08-19 13:08:21 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.2600
2022-08-19 13:08:55 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.1328
2022-08-19 13:09:29 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.1961
2022-08-19 13:10:04 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.0677
2022-08-19 13:10:38 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.2357
2022-08-19 13:11:13 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.5600
2022-08-19 13:11:47 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.4044
2022-08-19 13:12:22 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.5698
2022-08-19 13:12:57 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.1493
2022-08-19 13:13:32 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.0697
2022-08-19 13:14:06 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.4367
2022-08-19 13:14:40 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.1605
2022-08-19 13:15:15 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.4660
2022-08-19 13:15:50 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.1732
2022-08-19 13:16:25 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.2280
2022-08-19 13:16:59 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.3756
2022-08-19 13:17:34 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.4180
2022-08-19 13:18:09 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.3533
2022-08-19 13:18:44 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.5151
2022-08-19 13:19:18 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.4430
2022-08-19 13:19:52 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.3248
2022-08-19 13:20:27 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.3357
2022-08-19 13:21:02 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.1831
2022-08-19 13:21:37 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.2404
2022-08-19 13:22:12 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.3312
2022-08-19 13:22:47 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.0324
2022-08-19 13:23:22 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.3024
2022-08-19 13:23:57 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.3836
2022-08-19 13:24:32 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.2443
2022-08-19 13:25:06 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.2403
2022-08-19 13:25:41 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.4549
2022-08-19 13:26:16 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.2237
2022-08-19 13:26:51 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.2303
2022-08-19 13:27:26 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.2531
2022-08-19 13:28:02 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.1253
2022-08-19 13:28:37 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.2731
2022-08-19 13:29:12 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.4240
2022-08-19 13:29:47 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.3297
2022-08-19 13:30:22 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.1780
2022-08-19 13:30:56 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.3447
2022-08-19 13:31:31 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.1145
2022-08-19 13:32:06 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.3892
2022-08-19 13:32:40 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.4327
2022-08-19 13:32:42 - train: epoch 024, train_loss: 1.3209
2022-08-19 13:34:00 - eval: epoch: 024, acc1: 71.704%, acc5: 90.490%, test_loss: 1.1387, per_image_load_time: 2.362ms, per_image_inference_time: 0.571ms
2022-08-19 13:34:00 - until epoch: 024, best_acc1: 71.704%
2022-08-19 13:34:00 - epoch 025 lr: 0.000394
2022-08-19 13:34:42 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.1215
2022-08-19 13:35:17 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.0946
2022-08-19 13:35:50 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.1906
2022-08-19 13:36:24 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.3333
2022-08-19 13:36:59 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.1632
2022-08-19 13:37:34 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.4577
2022-08-19 13:38:09 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.2505
2022-08-19 13:38:43 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.2191
2022-08-19 13:39:18 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0421
2022-08-19 13:39:52 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3735
2022-08-19 13:40:27 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.3073
2022-08-19 13:41:01 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.3049
2022-08-19 13:41:35 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3255
2022-08-19 13:42:10 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.4362
2022-08-19 13:42:44 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.2017
2022-08-19 13:43:19 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.0865
2022-08-19 13:43:54 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.2546
2022-08-19 13:44:29 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.0610
2022-08-19 13:45:04 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.3639
2022-08-19 13:45:39 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.4435
2022-08-19 13:46:13 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.1926
2022-08-19 13:46:48 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.1841
2022-08-19 13:47:22 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.3238
2022-08-19 13:47:57 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.0752
2022-08-19 13:48:32 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.2663
2022-08-19 13:49:07 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.1993
2022-08-19 13:49:42 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.5982
2022-08-19 13:50:17 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.3391
2022-08-19 13:50:52 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.4137
2022-08-19 13:51:27 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.3091
2022-08-19 13:52:01 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.3658
2022-08-19 13:52:35 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.3027
2022-08-19 13:53:10 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.1723
2022-08-19 13:53:45 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.3917
2022-08-19 13:54:19 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1303
2022-08-19 13:54:54 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.3680
2022-08-19 13:55:29 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.2128
2022-08-19 13:56:04 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3965
2022-08-19 13:56:39 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.4230
2022-08-19 13:57:14 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.3166
2022-08-19 13:57:49 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.2935
2022-08-19 13:58:23 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.3356
2022-08-19 13:58:58 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.1592
2022-08-19 13:59:33 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.1543
2022-08-19 14:00:08 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.3152
2022-08-19 14:00:44 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.2068
2022-08-19 14:01:18 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.3108
2022-08-19 14:01:54 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.4077
2022-08-19 14:02:28 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.3506
2022-08-19 14:03:02 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.4093
2022-08-19 14:03:03 - train: epoch 025, train_loss: 1.3007
2022-08-19 14:04:21 - eval: epoch: 025, acc1: 71.778%, acc5: 90.478%, test_loss: 1.1346, per_image_load_time: 2.442ms, per_image_inference_time: 0.555ms
2022-08-19 14:04:21 - until epoch: 025, best_acc1: 71.778%
2022-08-19 14:04:21 - train done. train time: 12.573 hours, best_acc1: 71.778%
