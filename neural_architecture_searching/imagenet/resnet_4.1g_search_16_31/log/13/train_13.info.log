2022-08-21 15:45:16 - net_idx: 13
2022-08-21 15:45:16 - net_config: {'stem_width': 64, 'depth': 12, 'w_0': 40, 'w_a': 29.938894320103735, 'w_m': 1.8879421573693784}
2022-08-21 15:45:16 - num_classes: 1000
2022-08-21 15:45:16 - input_image_size: 224
2022-08-21 15:45:16 - scale: 1.1428571428571428
2022-08-21 15:45:16 - seed: 0
2022-08-21 15:45:16 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-21 15:45:16 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-21 15:45:16 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c2100>
2022-08-21 15:45:16 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c23d0>
2022-08-21 15:45:16 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2400>
2022-08-21 15:45:16 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2460>
2022-08-21 15:45:16 - batch_size: 256
2022-08-21 15:45:16 - num_workers: 16
2022-08-21 15:45:16 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-21 15:45:16 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-21 15:45:16 - epochs: 25
2022-08-21 15:45:16 - print_interval: 100
2022-08-21 15:45:16 - accumulation_steps: 1
2022-08-21 15:45:16 - sync_bn: False
2022-08-21 15:45:16 - apex: True
2022-08-21 15:45:16 - use_ema_model: False
2022-08-21 15:45:16 - ema_model_decay: 0.9999
2022-08-21 15:45:16 - log_dir: ./log
2022-08-21 15:45:16 - checkpoint_dir: ./checkpoints
2022-08-21 15:45:16 - gpus_type: NVIDIA RTX A5000
2022-08-21 15:45:16 - gpus_num: 2
2022-08-21 15:45:16 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f4efca864b0>
2022-08-21 15:45:16 - ema_model: None
2022-08-21 15:45:16 - --------------------parameters--------------------
2022-08-21 15:45:16 - name: conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-21 15:45:16 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-21 15:45:16 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-21 15:45:16 - name: fc.weight, grad: True
2022-08-21 15:45:16 - name: fc.bias, grad: True
2022-08-21 15:45:16 - --------------------buffers--------------------
2022-08-21 15:45:16 - name: conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-21 15:45:16 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-21 15:45:16 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-21 15:45:16 - -----------no weight decay layers--------------
2022-08-21 15:45:16 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-21 15:45:16 - -------------weight decay layers---------------
2022-08-21 15:45:16 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-21 15:45:16 - epoch 001 lr: 0.100000
2022-08-21 15:45:56 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9299
2022-08-21 15:46:30 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9011
2022-08-21 15:47:03 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8652
2022-08-21 15:47:36 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.8820
2022-08-21 15:48:10 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.8325
2022-08-21 15:48:44 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.6222
2022-08-21 15:49:18 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.6440
2022-08-21 15:49:52 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.5673
2022-08-21 15:50:26 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.4885
2022-08-21 15:51:00 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.4294
2022-08-21 15:51:33 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.4474
2022-08-21 15:52:07 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.3025
2022-08-21 15:52:41 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.3268
2022-08-21 15:53:15 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.1693
2022-08-21 15:53:49 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 6.0314
2022-08-21 15:54:22 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.1453
2022-08-21 15:54:56 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.9202
2022-08-21 15:55:30 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.7763
2022-08-21 15:56:04 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.7602
2022-08-21 15:56:38 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.5706
2022-08-21 15:57:12 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.6558
2022-08-21 15:57:46 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.6000
2022-08-21 15:58:20 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.3872
2022-08-21 15:58:54 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.4926
2022-08-21 15:59:28 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.3310
2022-08-21 16:00:02 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.4896
2022-08-21 16:00:36 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.3243
2022-08-21 16:01:10 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.2376
2022-08-21 16:01:44 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.1090
2022-08-21 16:02:18 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.3600
2022-08-21 16:02:52 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.2670
2022-08-21 16:03:26 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.1751
2022-08-21 16:04:00 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 5.0709
2022-08-21 16:04:35 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.9207
2022-08-21 16:05:09 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.7547
2022-08-21 16:05:43 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.7876
2022-08-21 16:06:17 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.9739
2022-08-21 16:06:51 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.6992
2022-08-21 16:07:25 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.6783
2022-08-21 16:07:59 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.7637
2022-08-21 16:08:33 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.7985
2022-08-21 16:09:07 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.7502
2022-08-21 16:09:42 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.6169
2022-08-21 16:10:16 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.3565
2022-08-21 16:10:50 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.5843
2022-08-21 16:11:24 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.8733
2022-08-21 16:11:58 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.4856
2022-08-21 16:12:32 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.4857
2022-08-21 16:13:06 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.4342
2022-08-21 16:13:40 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.2647
2022-08-21 16:13:41 - train: epoch 001, train_loss: 5.5148
2022-08-21 16:14:58 - eval: epoch: 001, acc1: 15.566%, acc5: 35.848%, test_loss: 4.4518, per_image_load_time: 1.331ms, per_image_inference_time: 0.637ms
2022-08-21 16:14:58 - until epoch: 001, best_acc1: 15.566%
2022-08-21 16:14:58 - epoch 002 lr: 0.099606
2022-08-21 16:15:38 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.0486
2022-08-21 16:16:11 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.2985
2022-08-21 16:16:45 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.3390
2022-08-21 16:17:18 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.2554
2022-08-21 16:17:52 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.1069
2022-08-21 16:18:26 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.0259
2022-08-21 16:18:59 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.4603
2022-08-21 16:19:33 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 4.0555
2022-08-21 16:20:07 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.9464
2022-08-21 16:20:41 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.0902
2022-08-21 16:21:15 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.0918
2022-08-21 16:21:50 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 4.0485
2022-08-21 16:22:24 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.9989
2022-08-21 16:22:58 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.1877
2022-08-21 16:23:32 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 4.0017
2022-08-21 16:24:07 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.8078
2022-08-21 16:24:40 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.8734
2022-08-21 16:25:14 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.8756
2022-08-21 16:25:48 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.7074
2022-08-21 16:26:23 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.5824
2022-08-21 16:26:57 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.8495
2022-08-21 16:27:31 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.7518
2022-08-21 16:28:05 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.8106
2022-08-21 16:28:39 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.8193
2022-08-21 16:29:13 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.5512
2022-08-21 16:29:47 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.5940
2022-08-21 16:30:21 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.8206
2022-08-21 16:30:55 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.7287
2022-08-21 16:31:29 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.7557
2022-08-21 16:32:03 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.4865
2022-08-21 16:32:37 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.6441
2022-08-21 16:33:11 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.5662
2022-08-21 16:33:45 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.6399
2022-08-21 16:34:19 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.4154
2022-08-21 16:34:54 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.5462
2022-08-21 16:35:28 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.6955
2022-08-21 16:36:02 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.6426
2022-08-21 16:36:36 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.2993
2022-08-21 16:37:11 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.5241
2022-08-21 16:37:45 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.3012
2022-08-21 16:38:19 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.5743
2022-08-21 16:38:53 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.3255
2022-08-21 16:39:28 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.3677
2022-08-21 16:40:02 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.2916
2022-08-21 16:40:36 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.2848
2022-08-21 16:41:10 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.2119
2022-08-21 16:41:44 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.3540
2022-08-21 16:42:19 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.4634
2022-08-21 16:42:53 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.3730
2022-08-21 16:43:26 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.4286
2022-08-21 16:43:28 - train: epoch 002, train_loss: 3.7340
2022-08-21 16:44:45 - eval: epoch: 002, acc1: 31.750%, acc5: 58.074%, test_loss: 3.2544, per_image_load_time: 0.649ms, per_image_inference_time: 0.605ms
2022-08-21 16:44:45 - until epoch: 002, best_acc1: 31.750%
2022-08-21 16:44:45 - epoch 003 lr: 0.098429
2022-08-21 16:45:25 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.3119
2022-08-21 16:45:59 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.3865
2022-08-21 16:46:32 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.3582
2022-08-21 16:47:06 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.2511
2022-08-21 16:47:39 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.4111
2022-08-21 16:48:12 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 2.9781
2022-08-21 16:48:46 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.1550
2022-08-21 16:49:20 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.3832
2022-08-21 16:49:54 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.2536
2022-08-21 16:50:28 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.2636
2022-08-21 16:51:02 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.0500
2022-08-21 16:51:36 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.0715
2022-08-21 16:52:10 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 2.9242
2022-08-21 16:52:44 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 2.9941
2022-08-21 16:53:18 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.3391
2022-08-21 16:53:52 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.0064
2022-08-21 16:54:27 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.0546
2022-08-21 16:55:01 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.0655
2022-08-21 16:55:35 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.0138
2022-08-21 16:56:09 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.0973
2022-08-21 16:56:43 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.2116
2022-08-21 16:57:17 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.3592
2022-08-21 16:57:51 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.9334
2022-08-21 16:58:25 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.1682
2022-08-21 16:58:59 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.1735
2022-08-21 16:59:33 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.1728
2022-08-21 17:00:08 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.1451
2022-08-21 17:00:42 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 2.8863
2022-08-21 17:01:16 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 3.1761
2022-08-21 17:01:50 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.0468
2022-08-21 17:02:24 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.1778
2022-08-21 17:02:58 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 2.9828
2022-08-21 17:03:33 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0124
2022-08-21 17:04:07 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.0673
2022-08-21 17:04:41 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.9921
2022-08-21 17:05:15 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 2.8806
2022-08-21 17:05:49 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.1444
2022-08-21 17:06:24 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.0692
2022-08-21 17:06:58 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.2762
2022-08-21 17:07:32 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.8157
2022-08-21 17:08:06 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 2.8479
2022-08-21 17:08:41 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.0740
2022-08-21 17:09:15 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.8359
2022-08-21 17:09:49 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 2.8787
2022-08-21 17:10:23 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 2.9215
2022-08-21 17:10:58 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.9887
2022-08-21 17:11:32 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 2.8031
2022-08-21 17:12:06 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.1078
2022-08-21 17:12:40 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.2111
2022-08-21 17:13:14 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 2.9754
2022-08-21 17:13:15 - train: epoch 003, train_loss: 3.0926
2022-08-21 17:14:31 - eval: epoch: 003, acc1: 37.934%, acc5: 64.898%, test_loss: 2.8378, per_image_load_time: 0.898ms, per_image_inference_time: 0.614ms
2022-08-21 17:14:31 - until epoch: 003, best_acc1: 37.934%
2022-08-21 17:14:31 - epoch 004 lr: 0.096488
2022-08-21 17:15:12 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.8454
2022-08-21 17:15:45 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.5623
2022-08-21 17:16:18 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.8909
2022-08-21 17:16:52 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.7606
2022-08-21 17:17:26 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.6211
2022-08-21 17:18:00 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.1124
2022-08-21 17:18:33 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.8070
2022-08-21 17:19:07 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.7046
2022-08-21 17:19:41 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.6962
2022-08-21 17:20:15 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.8544
2022-08-21 17:20:50 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.1798
2022-08-21 17:21:24 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.5727
2022-08-21 17:21:58 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.9622
2022-08-21 17:22:32 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.8025
2022-08-21 17:23:06 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.8918
2022-08-21 17:23:40 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.8649
2022-08-21 17:24:14 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.7618
2022-08-21 17:24:48 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.9522
2022-08-21 17:25:22 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.8121
2022-08-21 17:25:56 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.7607
2022-08-21 17:26:30 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.7760
2022-08-21 17:27:04 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.6917
2022-08-21 17:27:38 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.4233
2022-08-21 17:28:12 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.6097
2022-08-21 17:28:46 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.5799
2022-08-21 17:29:20 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.7040
2022-08-21 17:29:55 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.6577
2022-08-21 17:30:29 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8473
2022-08-21 17:31:03 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8215
2022-08-21 17:31:37 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.7480
2022-08-21 17:32:12 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.7603
2022-08-21 17:32:46 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8299
2022-08-21 17:33:20 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8638
2022-08-21 17:33:54 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.9302
2022-08-21 17:34:28 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.7385
2022-08-21 17:35:03 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.7732
2022-08-21 17:35:37 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.8722
2022-08-21 17:36:11 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.5910
2022-08-21 17:36:46 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.7737
2022-08-21 17:37:20 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.5946
2022-08-21 17:37:54 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.6704
2022-08-21 17:38:28 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6993
2022-08-21 17:39:02 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.6747
2022-08-21 17:39:36 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.6722
2022-08-21 17:40:11 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.4491
2022-08-21 17:40:45 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.6026
2022-08-21 17:41:19 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.8640
2022-08-21 17:41:53 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.4789
2022-08-21 17:42:28 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.7192
2022-08-21 17:43:01 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.7398
2022-08-21 17:43:02 - train: epoch 004, train_loss: 2.7793
2022-08-21 17:44:19 - eval: epoch: 004, acc1: 44.146%, acc5: 70.990%, test_loss: 2.4675, per_image_load_time: 1.905ms, per_image_inference_time: 0.627ms
2022-08-21 17:44:19 - until epoch: 004, best_acc1: 44.146%
2022-08-21 17:44:19 - epoch 005 lr: 0.093815
2022-08-21 17:44:59 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.7309
2022-08-21 17:45:32 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.7731
2022-08-21 17:46:06 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.8952
2022-08-21 17:46:40 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.6364
2022-08-21 17:47:14 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.4863
2022-08-21 17:47:48 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.6689
2022-08-21 17:48:22 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.7302
2022-08-21 17:48:56 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.8947
2022-08-21 17:49:30 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.8297
2022-08-21 17:50:04 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.6657
2022-08-21 17:50:38 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.7070
2022-08-21 17:51:12 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.4896
2022-08-21 17:51:46 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.4446
2022-08-21 17:52:20 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.7335
2022-08-21 17:52:54 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.3777
2022-08-21 17:53:28 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.5195
2022-08-21 17:54:02 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.4968
2022-08-21 17:54:37 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.7105
2022-08-21 17:55:11 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.6188
2022-08-21 17:55:45 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.4512
2022-08-21 17:56:19 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.3647
2022-08-21 17:56:53 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.5439
2022-08-21 17:57:28 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.5973
2022-08-21 17:58:02 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5951
2022-08-21 17:58:36 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.6142
2022-08-21 17:59:11 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.6173
2022-08-21 17:59:45 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.6467
2022-08-21 18:00:19 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.4702
2022-08-21 18:00:53 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.4213
2022-08-21 18:01:27 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.5282
2022-08-21 18:02:02 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.6334
2022-08-21 18:02:36 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.7613
2022-08-21 18:03:10 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.6128
2022-08-21 18:03:44 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.4368
2022-08-21 18:04:19 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.4975
2022-08-21 18:04:53 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.6524
2022-08-21 18:05:27 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.7115
2022-08-21 18:06:02 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.5665
2022-08-21 18:06:36 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.7597
2022-08-21 18:07:10 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.4988
2022-08-21 18:07:45 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.5893
2022-08-21 18:08:19 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.6388
2022-08-21 18:08:53 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.6326
2022-08-21 18:09:28 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.4353
2022-08-21 18:10:02 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.4971
2022-08-21 18:10:36 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.5759
2022-08-21 18:11:11 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.3573
2022-08-21 18:11:45 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.3571
2022-08-21 18:12:19 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.7081
2022-08-21 18:12:53 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.3559
2022-08-21 18:12:54 - train: epoch 005, train_loss: 2.5873
2022-08-21 18:14:10 - eval: epoch: 005, acc1: 47.122%, acc5: 73.182%, test_loss: 2.3279, per_image_load_time: 0.953ms, per_image_inference_time: 0.626ms
2022-08-21 18:14:11 - until epoch: 005, best_acc1: 47.122%
2022-08-21 18:14:11 - epoch 006 lr: 0.090450
2022-08-21 18:14:51 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.4777
2022-08-21 18:15:24 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.6587
2022-08-21 18:15:58 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.3939
2022-08-21 18:16:32 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.6378
2022-08-21 18:17:06 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.4710
2022-08-21 18:17:40 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.6743
2022-08-21 18:18:14 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.8165
2022-08-21 18:18:48 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.4356
2022-08-21 18:19:22 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.2725
2022-08-21 18:19:56 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.4604
2022-08-21 18:20:29 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.4272
2022-08-21 18:21:03 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.6507
2022-08-21 18:21:37 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5835
2022-08-21 18:22:11 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.6020
2022-08-21 18:22:45 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.9229
2022-08-21 18:23:19 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3188
2022-08-21 18:23:53 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.4983
2022-08-21 18:24:27 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.7026
2022-08-21 18:25:02 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.3362
2022-08-21 18:25:36 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.7188
2022-08-21 18:26:10 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.2875
2022-08-21 18:26:44 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.2562
2022-08-21 18:27:18 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.5600
2022-08-21 18:27:52 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.3282
2022-08-21 18:28:26 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.3842
2022-08-21 18:29:01 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.3680
2022-08-21 18:29:35 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.5374
2022-08-21 18:30:09 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.0222
2022-08-21 18:30:43 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.5387
2022-08-21 18:31:17 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.4661
2022-08-21 18:31:51 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.2698
2022-08-21 18:32:25 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.2447
2022-08-21 18:32:59 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.4286
2022-08-21 18:33:34 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.4255
2022-08-21 18:34:08 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.4519
2022-08-21 18:34:42 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.4401
2022-08-21 18:35:16 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.4611
2022-08-21 18:35:51 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.3223
2022-08-21 18:36:25 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.4725
2022-08-21 18:36:59 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.7592
2022-08-21 18:37:34 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.2706
2022-08-21 18:38:08 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.3168
2022-08-21 18:38:42 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.4990
2022-08-21 18:39:16 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.5032
2022-08-21 18:39:50 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.5731
2022-08-21 18:40:25 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.5900
2022-08-21 18:40:59 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.2557
2022-08-21 18:41:34 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.5229
2022-08-21 18:42:08 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.3403
2022-08-21 18:42:41 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.3708
2022-08-21 18:42:43 - train: epoch 006, train_loss: 2.4583
2022-08-21 18:43:58 - eval: epoch: 006, acc1: 49.390%, acc5: 75.416%, test_loss: 2.2088, per_image_load_time: 1.044ms, per_image_inference_time: 0.636ms
2022-08-21 18:43:59 - until epoch: 006, best_acc1: 49.390%
2022-08-21 18:43:59 - epoch 007 lr: 0.086448
2022-08-21 18:44:39 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.1760
2022-08-21 18:45:12 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.5289
2022-08-21 18:45:46 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.4977
2022-08-21 18:46:20 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.2739
2022-08-21 18:46:54 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.3026
2022-08-21 18:47:27 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.3704
2022-08-21 18:48:01 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.1578
2022-08-21 18:48:35 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.5209
2022-08-21 18:49:09 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.3640
2022-08-21 18:49:43 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.1939
2022-08-21 18:50:17 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.4289
2022-08-21 18:50:51 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.2357
2022-08-21 18:51:24 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.0580
2022-08-21 18:51:58 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.3640
2022-08-21 18:52:32 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.4557
2022-08-21 18:53:06 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.3169
2022-08-21 18:53:41 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.5141
2022-08-21 18:54:15 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.4311
2022-08-21 18:54:49 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.5407
2022-08-21 18:55:23 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.0551
2022-08-21 18:55:57 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.7017
2022-08-21 18:56:32 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.3934
2022-08-21 18:57:06 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.4784
2022-08-21 18:57:40 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.5091
2022-08-21 18:58:14 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.4260
2022-08-21 18:58:49 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.3836
2022-08-21 18:59:23 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.3196
2022-08-21 18:59:57 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.1742
2022-08-21 19:00:31 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.3054
2022-08-21 19:01:05 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.5342
2022-08-21 19:01:40 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.3200
2022-08-21 19:02:14 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.2207
2022-08-21 19:02:48 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.2763
2022-08-21 19:03:22 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.4959
2022-08-21 19:03:56 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.3665
2022-08-21 19:04:30 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.4000
2022-08-21 19:05:04 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.3261
2022-08-21 19:05:37 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.4546
2022-08-21 19:06:11 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.3071
2022-08-21 19:06:45 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.4578
2022-08-21 19:07:19 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.1421
2022-08-21 19:07:53 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.1775
2022-08-21 19:08:27 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.6095
2022-08-21 19:09:00 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.2973
2022-08-21 19:09:34 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.4970
2022-08-21 19:10:08 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.5769
2022-08-21 19:10:42 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.1827
2022-08-21 19:11:16 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.7038
2022-08-21 19:11:50 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.2488
2022-08-21 19:12:23 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.3183
2022-08-21 19:12:25 - train: epoch 007, train_loss: 2.3618
2022-08-21 19:13:40 - eval: epoch: 007, acc1: 52.010%, acc5: 77.424%, test_loss: 2.0663, per_image_load_time: 1.731ms, per_image_inference_time: 0.611ms
2022-08-21 19:13:40 - until epoch: 007, best_acc1: 52.010%
2022-08-21 19:13:40 - epoch 008 lr: 0.081870
2022-08-21 19:14:21 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.2838
2022-08-21 19:14:54 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.5247
2022-08-21 19:15:27 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.2251
2022-08-21 19:16:00 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.2225
2022-08-21 19:16:33 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.1751
2022-08-21 19:17:06 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.3012
2022-08-21 19:17:40 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.3945
2022-08-21 19:18:13 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.1502
2022-08-21 19:18:46 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.2214
2022-08-21 19:19:20 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.2336
2022-08-21 19:19:53 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.2224
2022-08-21 19:20:27 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.1695
2022-08-21 19:21:01 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.4917
2022-08-21 19:21:34 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.3562
2022-08-21 19:22:08 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.2361
2022-08-21 19:22:42 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.4811
2022-08-21 19:23:15 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.3966
2022-08-21 19:23:49 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.3298
2022-08-21 19:24:23 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.1010
2022-08-21 19:24:57 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.2099
2022-08-21 19:25:31 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.3999
2022-08-21 19:26:05 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.0935
2022-08-21 19:26:39 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.2414
2022-08-21 19:27:13 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.2332
2022-08-21 19:27:46 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.1187
2022-08-21 19:28:20 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.2772
2022-08-21 19:28:54 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.3709
2022-08-21 19:29:28 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.3902
2022-08-21 19:30:02 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.3473
2022-08-21 19:30:36 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.3698
2022-08-21 19:31:10 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.1517
2022-08-21 19:31:44 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.5713
2022-08-21 19:32:18 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.3521
2022-08-21 19:32:51 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.5017
2022-08-21 19:33:25 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.2099
2022-08-21 19:33:59 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.2711
2022-08-21 19:34:33 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.2359
2022-08-21 19:35:07 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.3644
2022-08-21 19:35:41 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.2270
2022-08-21 19:36:15 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.5879
2022-08-21 19:36:49 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.1441
2022-08-21 19:37:24 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.1922
2022-08-21 19:37:58 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 2.0735
2022-08-21 19:38:32 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.2777
2022-08-21 19:39:06 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.0639
2022-08-21 19:39:40 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.2546
2022-08-21 19:40:14 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2029
2022-08-21 19:40:48 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.1976
2022-08-21 19:41:22 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.4201
2022-08-21 19:41:56 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.1565
2022-08-21 19:41:57 - train: epoch 008, train_loss: 2.2822
2022-08-21 19:43:13 - eval: epoch: 008, acc1: 52.706%, acc5: 78.178%, test_loss: 2.0391, per_image_load_time: 0.627ms, per_image_inference_time: 0.593ms
2022-08-21 19:43:13 - until epoch: 008, best_acc1: 52.706%
2022-08-21 19:43:13 - epoch 009 lr: 0.076790
2022-08-21 19:43:54 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 1.8848
2022-08-21 19:44:27 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.1645
2022-08-21 19:45:01 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.1403
2022-08-21 19:45:35 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.6077
2022-08-21 19:46:09 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.1172
2022-08-21 19:46:43 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.1655
2022-08-21 19:47:17 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.1735
2022-08-21 19:47:51 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.2586
2022-08-21 19:48:25 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.0514
2022-08-21 19:49:00 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.0820
2022-08-21 19:49:34 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.3977
2022-08-21 19:50:08 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.2153
2022-08-21 19:50:42 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.3009
2022-08-21 19:51:16 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9399
2022-08-21 19:51:51 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.0899
2022-08-21 19:52:25 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.1795
2022-08-21 19:52:58 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.4254
2022-08-21 19:53:32 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.2113
2022-08-21 19:54:06 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.0448
2022-08-21 19:54:40 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 1.9430
2022-08-21 19:55:15 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.3264
2022-08-21 19:55:49 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.1776
2022-08-21 19:56:23 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 1.9948
2022-08-21 19:56:57 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.3268
2022-08-21 19:57:32 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.2035
2022-08-21 19:58:06 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.2775
2022-08-21 19:58:40 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.0100
2022-08-21 19:59:14 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.3303
2022-08-21 19:59:48 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.0321
2022-08-21 20:00:23 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1077
2022-08-21 20:00:57 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.3453
2022-08-21 20:01:31 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.2064
2022-08-21 20:02:05 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.2166
2022-08-21 20:02:39 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.2931
2022-08-21 20:03:13 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.3473
2022-08-21 20:03:47 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.2554
2022-08-21 20:04:21 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.4440
2022-08-21 20:04:55 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.3044
2022-08-21 20:05:29 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.0235
2022-08-21 20:06:04 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.5361
2022-08-21 20:06:38 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.1570
2022-08-21 20:07:13 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 1.9912
2022-08-21 20:07:47 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.0017
2022-08-21 20:08:21 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.2451
2022-08-21 20:08:56 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.2698
2022-08-21 20:09:30 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.1632
2022-08-21 20:10:04 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.3223
2022-08-21 20:10:38 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.2413
2022-08-21 20:11:12 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.3989
2022-08-21 20:11:46 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.1295
2022-08-21 20:11:47 - train: epoch 009, train_loss: 2.2126
2022-08-21 20:13:03 - eval: epoch: 009, acc1: 54.186%, acc5: 79.210%, test_loss: 1.9613, per_image_load_time: 0.781ms, per_image_inference_time: 0.553ms
2022-08-21 20:13:03 - until epoch: 009, best_acc1: 54.186%
2022-08-21 20:13:03 - epoch 010 lr: 0.071288
2022-08-21 20:13:43 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.1381
2022-08-21 20:14:17 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.2243
2022-08-21 20:14:51 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.2151
2022-08-21 20:15:25 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.1361
2022-08-21 20:15:59 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.0880
2022-08-21 20:16:33 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.3596
2022-08-21 20:17:06 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.3676
2022-08-21 20:17:40 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.0691
2022-08-21 20:18:14 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.1644
2022-08-21 20:18:48 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.1924
2022-08-21 20:19:23 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 1.9971
2022-08-21 20:19:57 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.0663
2022-08-21 20:20:32 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.0267
2022-08-21 20:21:05 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.3162
2022-08-21 20:21:39 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 2.1185
2022-08-21 20:22:13 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.2290
2022-08-21 20:22:47 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.2813
2022-08-21 20:23:21 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 2.0679
2022-08-21 20:23:56 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.1380
2022-08-21 20:24:30 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.0839
2022-08-21 20:25:04 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 2.0149
2022-08-21 20:25:38 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.2916
2022-08-21 20:26:12 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.1888
2022-08-21 20:26:46 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.2984
2022-08-21 20:27:20 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.1687
2022-08-21 20:27:55 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.2201
2022-08-21 20:28:29 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 2.0008
2022-08-21 20:29:03 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.2308
2022-08-21 20:29:37 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.1722
2022-08-21 20:30:11 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 1.9791
2022-08-21 20:30:45 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.2968
2022-08-21 20:31:18 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.0129
2022-08-21 20:31:52 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.1626
2022-08-21 20:32:26 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.1385
2022-08-21 20:33:01 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.3636
2022-08-21 20:33:35 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.2931
2022-08-21 20:34:09 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.3370
2022-08-21 20:34:43 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1855
2022-08-21 20:35:18 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 1.9223
2022-08-21 20:35:52 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.0600
2022-08-21 20:36:26 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 1.9663
2022-08-21 20:37:00 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.0304
2022-08-21 20:37:35 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.0671
2022-08-21 20:38:09 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.0157
2022-08-21 20:38:43 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.1039
2022-08-21 20:39:18 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.0553
2022-08-21 20:39:52 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1732
2022-08-21 20:40:26 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.0859
2022-08-21 20:40:59 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.1188
2022-08-21 20:41:33 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.2854
2022-08-21 20:41:34 - train: epoch 010, train_loss: 2.1479
2022-08-21 20:42:50 - eval: epoch: 010, acc1: 56.492%, acc5: 81.002%, test_loss: 1.8478, per_image_load_time: 0.720ms, per_image_inference_time: 0.579ms
2022-08-21 20:42:50 - until epoch: 010, best_acc1: 56.492%
2022-08-21 20:42:50 - epoch 011 lr: 0.065450
2022-08-21 20:43:31 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 1.9490
2022-08-21 20:44:05 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 1.9672
2022-08-21 20:44:39 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.1127
2022-08-21 20:45:13 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.3569
2022-08-21 20:45:47 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.2921
2022-08-21 20:46:21 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.1535
2022-08-21 20:46:54 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.0784
2022-08-21 20:47:28 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.3781
2022-08-21 20:48:02 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.1979
2022-08-21 20:48:36 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.0309
2022-08-21 20:49:10 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2877
2022-08-21 20:49:44 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2180
2022-08-21 20:50:18 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.1777
2022-08-21 20:50:52 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.1081
2022-08-21 20:51:26 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 2.0345
2022-08-21 20:52:00 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.0755
2022-08-21 20:52:35 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.1705
2022-08-21 20:53:09 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.0552
2022-08-21 20:53:43 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.0312
2022-08-21 20:54:17 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1167
2022-08-21 20:54:51 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 1.9233
2022-08-21 20:55:25 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 1.9614
2022-08-21 20:55:59 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.2505
2022-08-21 20:56:33 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 2.1034
2022-08-21 20:57:08 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.1880
2022-08-21 20:57:42 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.2098
2022-08-21 20:58:16 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.3344
2022-08-21 20:58:50 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.8213
2022-08-21 20:59:24 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 1.9041
2022-08-21 20:59:58 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.2902
2022-08-21 21:00:32 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.0211
2022-08-21 21:01:06 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.0925
2022-08-21 21:01:40 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.3186
2022-08-21 21:02:14 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.2167
2022-08-21 21:02:48 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.1730
2022-08-21 21:03:22 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.0602
2022-08-21 21:03:56 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.1412
2022-08-21 21:04:31 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.9245
2022-08-21 21:05:04 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.0508
2022-08-21 21:05:38 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.1100
2022-08-21 21:06:13 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.8684
2022-08-21 21:06:47 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 1.8874
2022-08-21 21:07:21 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.0993
2022-08-21 21:07:56 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.0440
2022-08-21 21:08:30 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 1.9057
2022-08-21 21:09:03 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 1.9523
2022-08-21 21:09:37 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.7839
2022-08-21 21:10:11 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.7236
2022-08-21 21:10:45 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.2551
2022-08-21 21:11:19 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.1501
2022-08-21 21:11:20 - train: epoch 011, train_loss: 2.0878
2022-08-21 21:12:37 - eval: epoch: 011, acc1: 57.322%, acc5: 81.206%, test_loss: 1.8180, per_image_load_time: 0.850ms, per_image_inference_time: 0.582ms
2022-08-21 21:12:37 - until epoch: 011, best_acc1: 57.322%
2022-08-21 21:12:37 - epoch 012 lr: 0.059368
2022-08-21 21:13:18 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 2.0770
2022-08-21 21:13:52 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.8294
2022-08-21 21:14:25 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 2.0440
2022-08-21 21:14:59 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 1.9994
2022-08-21 21:15:33 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.2060
2022-08-21 21:16:07 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.7893
2022-08-21 21:16:41 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.7624
2022-08-21 21:17:15 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.0600
2022-08-21 21:17:49 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.1758
2022-08-21 21:18:23 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9205
2022-08-21 21:18:57 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.3949
2022-08-21 21:19:31 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.9700
2022-08-21 21:20:05 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.9554
2022-08-21 21:20:39 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.1907
2022-08-21 21:21:13 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 2.0253
2022-08-21 21:21:47 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 2.0379
2022-08-21 21:22:21 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.8858
2022-08-21 21:22:55 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.0356
2022-08-21 21:23:29 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.0842
2022-08-21 21:24:03 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.1428
2022-08-21 21:24:37 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.1280
2022-08-21 21:25:12 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.0760
2022-08-21 21:25:46 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 1.9536
2022-08-21 21:26:20 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 1.9612
2022-08-21 21:26:54 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.9691
2022-08-21 21:27:28 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.8997
2022-08-21 21:28:02 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.0734
2022-08-21 21:28:36 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 2.0419
2022-08-21 21:29:10 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.0061
2022-08-21 21:29:43 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.9275
2022-08-21 21:30:18 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.0229
2022-08-21 21:30:51 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.9967
2022-08-21 21:31:25 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.0716
2022-08-21 21:31:59 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 1.8614
2022-08-21 21:32:33 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.0653
2022-08-21 21:33:07 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.9994
2022-08-21 21:33:42 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.0742
2022-08-21 21:34:16 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 1.9483
2022-08-21 21:34:50 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8351
2022-08-21 21:35:25 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 1.9611
2022-08-21 21:35:59 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 1.8542
2022-08-21 21:36:33 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.9812
2022-08-21 21:37:07 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.0439
2022-08-21 21:37:42 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.8713
2022-08-21 21:38:15 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.7867
2022-08-21 21:38:50 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.1414
2022-08-21 21:39:24 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.8415
2022-08-21 21:39:59 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.0753
2022-08-21 21:40:33 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.8723
2022-08-21 21:41:06 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.7773
2022-08-21 21:41:08 - train: epoch 012, train_loss: 2.0289
2022-08-21 21:42:24 - eval: epoch: 012, acc1: 60.018%, acc5: 83.364%, test_loss: 1.6871, per_image_load_time: 0.491ms, per_image_inference_time: 0.554ms
2022-08-21 21:42:24 - until epoch: 012, best_acc1: 60.018%
2022-08-21 21:42:24 - epoch 013 lr: 0.053138
2022-08-21 21:43:05 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.6817
2022-08-21 21:43:40 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.9553
2022-08-21 21:44:14 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.7640
2022-08-21 21:44:48 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.7535
2022-08-21 21:45:21 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 1.8535
2022-08-21 21:45:55 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 1.8972
2022-08-21 21:46:29 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.9139
2022-08-21 21:47:03 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.0429
2022-08-21 21:47:36 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.8884
2022-08-21 21:48:10 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 2.0324
2022-08-21 21:48:44 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 1.9471
2022-08-21 21:49:18 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.0014
2022-08-21 21:49:52 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 2.0258
2022-08-21 21:50:27 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 1.9808
2022-08-21 21:51:01 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.0776
2022-08-21 21:51:35 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.8545
2022-08-21 21:52:09 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 1.8761
2022-08-21 21:52:43 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.8264
2022-08-21 21:53:17 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.1457
2022-08-21 21:53:52 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.4077
2022-08-21 21:54:26 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.2772
2022-08-21 21:55:00 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.7891
2022-08-21 21:55:35 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 2.0314
2022-08-21 21:56:09 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.1834
2022-08-21 21:56:43 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.9887
2022-08-21 21:57:16 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.8409
2022-08-21 21:57:50 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.9620
2022-08-21 21:58:25 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.1487
2022-08-21 21:58:59 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.0040
2022-08-21 21:59:33 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.8567
2022-08-21 22:00:07 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.7135
2022-08-21 22:00:41 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 1.9846
2022-08-21 22:01:15 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.7767
2022-08-21 22:01:49 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.8445
2022-08-21 22:02:23 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.8585
2022-08-21 22:02:58 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.0495
2022-08-21 22:03:32 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.7683
2022-08-21 22:04:06 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.0689
2022-08-21 22:04:40 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.2109
2022-08-21 22:05:15 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.9971
2022-08-21 22:05:49 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.9265
2022-08-21 22:06:23 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 1.9581
2022-08-21 22:06:56 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.9792
2022-08-21 22:07:31 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.8464
2022-08-21 22:08:05 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.8479
2022-08-21 22:08:40 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 1.9366
2022-08-21 22:09:14 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.9218
2022-08-21 22:09:48 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.0427
2022-08-21 22:10:23 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0531
2022-08-21 22:10:56 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.0874
2022-08-21 22:10:58 - train: epoch 013, train_loss: 1.9681
2022-08-21 22:12:14 - eval: epoch: 013, acc1: 60.614%, acc5: 84.120%, test_loss: 1.6450, per_image_load_time: 0.509ms, per_image_inference_time: 0.557ms
2022-08-21 22:12:15 - until epoch: 013, best_acc1: 60.614%
2022-08-21 22:12:15 - epoch 014 lr: 0.046859
2022-08-21 22:12:55 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.8763
2022-08-21 22:13:29 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.9821
2022-08-21 22:14:03 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.9346
2022-08-21 22:14:37 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.9108
2022-08-21 22:15:11 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.8065
2022-08-21 22:15:45 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.9251
2022-08-21 22:16:18 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.8129
2022-08-21 22:16:51 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.8305
2022-08-21 22:17:25 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.7009
2022-08-21 22:17:59 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.0994
2022-08-21 22:18:33 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.8565
2022-08-21 22:19:07 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 1.9286
2022-08-21 22:19:40 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.0456
2022-08-21 22:20:14 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.8119
2022-08-21 22:20:49 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 1.8890
2022-08-21 22:21:23 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.8864
2022-08-21 22:21:57 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 2.0226
2022-08-21 22:22:31 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 1.9912
2022-08-21 22:23:05 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.8470
2022-08-21 22:23:39 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.6453
2022-08-21 22:24:13 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 1.9366
2022-08-21 22:24:47 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.9776
2022-08-21 22:25:21 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.7264
2022-08-21 22:25:55 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 2.0788
2022-08-21 22:26:28 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.9082
2022-08-21 22:27:02 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8349
2022-08-21 22:27:36 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.8406
2022-08-21 22:28:10 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 1.8461
2022-08-21 22:28:44 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 2.0081
2022-08-21 22:29:18 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.9715
2022-08-21 22:29:52 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.8875
2022-08-21 22:30:27 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.9874
2022-08-21 22:31:01 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.7254
2022-08-21 22:31:35 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.8620
2022-08-21 22:32:09 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.9895
2022-08-21 22:32:43 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.6477
2022-08-21 22:33:17 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.8983
2022-08-21 22:33:51 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 2.1124
2022-08-21 22:34:25 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 2.0313
2022-08-21 22:34:59 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.8174
2022-08-21 22:35:33 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.8143
2022-08-21 22:36:07 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.8115
2022-08-21 22:36:41 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.6398
2022-08-21 22:37:15 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.8316
2022-08-21 22:37:49 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.9515
2022-08-21 22:38:24 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.7970
2022-08-21 22:38:58 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.8460
2022-08-21 22:39:32 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 2.0338
2022-08-21 22:40:06 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.6393
2022-08-21 22:40:40 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.8823
2022-08-21 22:40:41 - train: epoch 014, train_loss: 1.9064
2022-08-21 22:41:58 - eval: epoch: 014, acc1: 61.744%, acc5: 84.582%, test_loss: 1.6012, per_image_load_time: 0.586ms, per_image_inference_time: 0.551ms
2022-08-21 22:41:58 - until epoch: 014, best_acc1: 61.744%
2022-08-21 22:41:58 - epoch 015 lr: 0.040630
2022-08-21 22:42:40 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.4450
2022-08-21 22:43:14 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.9935
2022-08-21 22:43:47 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 1.9884
2022-08-21 22:44:21 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 2.0070
2022-08-21 22:44:54 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.7109
2022-08-21 22:45:28 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 1.8098
2022-08-21 22:46:01 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.9114
2022-08-21 22:46:35 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.7515
2022-08-21 22:47:09 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 2.0157
2022-08-21 22:47:43 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.7631
2022-08-21 22:48:17 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7825
2022-08-21 22:48:51 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 2.0029
2022-08-21 22:49:25 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 1.8411
2022-08-21 22:49:59 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.8185
2022-08-21 22:50:33 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.7933
2022-08-21 22:51:07 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.7864
2022-08-21 22:51:41 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.8713
2022-08-21 22:52:15 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.6131
2022-08-21 22:52:49 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.8490
2022-08-21 22:53:23 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.9584
2022-08-21 22:53:56 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.7717
2022-08-21 22:54:30 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.9194
2022-08-21 22:55:03 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.7070
2022-08-21 22:55:37 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.8782
2022-08-21 22:56:10 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 1.8976
2022-08-21 22:56:44 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.7301
2022-08-21 22:57:18 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.8201
2022-08-21 22:57:52 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.9095
2022-08-21 22:58:26 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.8019
2022-08-21 22:59:00 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.7969
2022-08-21 22:59:34 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.8074
2022-08-21 23:00:08 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.8647
2022-08-21 23:00:42 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.6173
2022-08-21 23:01:16 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.9506
2022-08-21 23:01:50 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0243
2022-08-21 23:02:25 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.7865
2022-08-21 23:02:59 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.8242
2022-08-21 23:03:33 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.9870
2022-08-21 23:04:06 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8016
2022-08-21 23:04:40 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.7417
2022-08-21 23:05:15 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.6693
2022-08-21 23:05:49 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6148
2022-08-21 23:06:23 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.6497
2022-08-21 23:06:57 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.7831
2022-08-21 23:07:32 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.7443
2022-08-21 23:08:06 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 1.8220
2022-08-21 23:08:40 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.6412
2022-08-21 23:09:14 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.7437
2022-08-21 23:09:48 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.7318
2022-08-21 23:10:22 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 2.0190
2022-08-21 23:10:23 - train: epoch 015, train_loss: 1.8441
2022-08-21 23:11:40 - eval: epoch: 015, acc1: 62.116%, acc5: 84.746%, test_loss: 1.5832, per_image_load_time: 0.770ms, per_image_inference_time: 0.580ms
2022-08-21 23:11:40 - until epoch: 015, best_acc1: 62.116%
2022-08-21 23:11:40 - epoch 016 lr: 0.034548
2022-08-21 23:12:22 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.7695
2022-08-21 23:12:55 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.7230
2022-08-21 23:13:28 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.7943
2022-08-21 23:14:02 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.0330
2022-08-21 23:14:35 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.8364
2022-08-21 23:15:09 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.6330
2022-08-21 23:15:43 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.6865
2022-08-21 23:16:17 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.7538
2022-08-21 23:16:51 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.8438
2022-08-21 23:17:25 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.6958
2022-08-21 23:17:58 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.7055
2022-08-21 23:18:32 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.6590
2022-08-21 23:19:06 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.8000
2022-08-21 23:19:40 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.7442
2022-08-21 23:20:14 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.9263
2022-08-21 23:20:48 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.8611
2022-08-21 23:21:23 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.7832
2022-08-21 23:21:57 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 1.9609
2022-08-21 23:22:32 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.8521
2022-08-21 23:23:06 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.5659
2022-08-21 23:23:41 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.8560
2022-08-21 23:24:15 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.8837
2022-08-21 23:24:49 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.8554
2022-08-21 23:25:24 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 2.0331
2022-08-21 23:25:58 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.7334
2022-08-21 23:26:33 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.9030
2022-08-21 23:27:07 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.6711
2022-08-21 23:27:41 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.6196
2022-08-21 23:28:16 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.6570
2022-08-21 23:28:50 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.8950
2022-08-21 23:29:25 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.8819
2022-08-21 23:29:59 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.8856
2022-08-21 23:30:33 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9593
2022-08-21 23:31:08 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.7239
2022-08-21 23:31:43 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.8465
2022-08-21 23:32:17 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.7837
2022-08-21 23:32:51 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.6975
2022-08-21 23:33:25 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 1.8296
2022-08-21 23:34:00 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.7752
2022-08-21 23:34:34 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.9002
2022-08-21 23:35:09 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.7460
2022-08-21 23:35:43 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.6017
2022-08-21 23:36:17 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.6824
2022-08-21 23:36:52 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.5790
2022-08-21 23:37:26 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.9593
2022-08-21 23:38:01 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.6518
2022-08-21 23:38:36 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.8899
2022-08-21 23:39:10 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.8384
2022-08-21 23:39:45 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.8433
2022-08-21 23:40:18 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.7871
2022-08-21 23:40:20 - train: epoch 016, train_loss: 1.7767
2022-08-21 23:41:39 - eval: epoch: 016, acc1: 64.416%, acc5: 86.520%, test_loss: 1.4603, per_image_load_time: 2.517ms, per_image_inference_time: 0.551ms
2022-08-21 23:41:39 - until epoch: 016, best_acc1: 64.416%
2022-08-21 23:41:39 - epoch 017 lr: 0.028710
2022-08-21 23:42:22 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.7122
2022-08-21 23:42:56 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.8340
2022-08-21 23:43:30 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.8489
2022-08-21 23:44:05 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.5648
2022-08-21 23:44:39 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.8858
2022-08-21 23:45:13 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 1.9417
2022-08-21 23:45:48 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.5412
2022-08-21 23:46:22 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.6114
2022-08-21 23:46:56 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.8114
2022-08-21 23:47:31 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.7326
2022-08-21 23:48:05 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.9752
2022-08-21 23:48:39 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.6751
2022-08-21 23:49:14 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.7724
2022-08-21 23:49:48 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.9378
2022-08-21 23:50:23 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.7126
2022-08-21 23:50:57 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.4876
2022-08-21 23:51:31 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.5942
2022-08-21 23:52:06 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.6141
2022-08-21 23:52:40 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.7682
2022-08-21 23:53:14 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.5476
2022-08-21 23:53:49 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.6354
2022-08-21 23:54:23 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.5467
2022-08-21 23:54:57 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.7003
2022-08-21 23:55:32 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.6967
2022-08-21 23:56:06 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.9142
2022-08-21 23:56:41 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.6386
2022-08-21 23:57:15 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6993
2022-08-21 23:57:49 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.6457
2022-08-21 23:58:24 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.9021
2022-08-21 23:58:58 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.4332
2022-08-21 23:59:32 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 2.0198
2022-08-22 00:00:07 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.6005
2022-08-22 00:00:42 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.7455
2022-08-22 00:01:17 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.5307
2022-08-22 00:01:51 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.8287
2022-08-22 00:02:25 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.7346
2022-08-22 00:03:00 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.6062
2022-08-22 00:03:35 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.8523
2022-08-22 00:04:09 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.6446
2022-08-22 00:04:44 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.5574
2022-08-22 00:05:18 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.7247
2022-08-22 00:05:53 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.7633
2022-08-22 00:06:27 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.7000
2022-08-22 00:07:01 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8285
2022-08-22 00:07:36 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.8089
2022-08-22 00:08:11 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.5717
2022-08-22 00:08:45 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.8389
2022-08-22 00:09:19 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.8293
2022-08-22 00:09:54 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.6641
2022-08-22 00:10:27 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.5097
2022-08-22 00:10:29 - train: epoch 017, train_loss: 1.7107
2022-08-22 00:11:47 - eval: epoch: 017, acc1: 65.696%, acc5: 86.982%, test_loss: 1.4108, per_image_load_time: 2.493ms, per_image_inference_time: 0.541ms
2022-08-22 00:11:47 - until epoch: 017, best_acc1: 65.696%
2022-08-22 00:11:47 - epoch 018 lr: 0.023208
2022-08-22 00:12:29 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.5515
2022-08-22 00:13:03 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.6935
2022-08-22 00:13:37 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.7101
2022-08-22 00:14:11 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.9697
2022-08-22 00:14:46 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.7362
2022-08-22 00:15:20 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.8076
2022-08-22 00:15:54 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.3639
2022-08-22 00:16:28 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.7449
2022-08-22 00:17:02 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.6458
2022-08-22 00:17:36 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.3520
2022-08-22 00:18:10 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.7401
2022-08-22 00:18:45 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.6637
2022-08-22 00:19:19 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.8552
2022-08-22 00:19:53 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.7067
2022-08-22 00:20:27 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.6072
2022-08-22 00:21:01 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.6866
2022-08-22 00:21:35 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.9711
2022-08-22 00:22:10 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.7058
2022-08-22 00:22:44 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.7409
2022-08-22 00:23:18 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.8672
2022-08-22 00:23:52 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.8732
2022-08-22 00:24:27 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.7380
2022-08-22 00:25:01 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.5637
2022-08-22 00:25:36 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.6304
2022-08-22 00:26:11 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.3713
2022-08-22 00:26:45 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.7498
2022-08-22 00:27:20 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.7319
2022-08-22 00:27:54 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.4714
2022-08-22 00:28:29 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.7440
2022-08-22 00:29:03 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.5773
2022-08-22 00:29:37 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 2.0652
2022-08-22 00:30:11 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.4432
2022-08-22 00:30:46 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.6080
2022-08-22 00:31:20 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.6675
2022-08-22 00:31:55 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.6296
2022-08-22 00:32:29 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.5653
2022-08-22 00:33:03 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.8126
2022-08-22 00:33:38 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.7414
2022-08-22 00:34:12 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.7953
2022-08-22 00:34:47 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.6716
2022-08-22 00:35:21 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.8196
2022-08-22 00:35:55 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.6925
2022-08-22 00:36:30 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.5877
2022-08-22 00:37:04 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.5437
2022-08-22 00:37:39 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.5552
2022-08-22 00:38:14 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.6006
2022-08-22 00:38:48 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.6103
2022-08-22 00:39:23 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6183
2022-08-22 00:39:57 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.5247
2022-08-22 00:40:30 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.3843
2022-08-22 00:40:32 - train: epoch 018, train_loss: 1.6368
2022-08-22 00:41:50 - eval: epoch: 018, acc1: 66.606%, acc5: 87.514%, test_loss: 1.3751, per_image_load_time: 2.026ms, per_image_inference_time: 0.559ms
2022-08-22 00:41:50 - until epoch: 018, best_acc1: 66.606%
2022-08-22 00:41:50 - epoch 019 lr: 0.018128
2022-08-22 00:42:33 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4191
2022-08-22 00:43:07 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.3870
2022-08-22 00:43:41 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.6525
2022-08-22 00:44:15 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.8731
2022-08-22 00:44:49 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.5266
2022-08-22 00:45:23 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.5133
2022-08-22 00:45:57 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.3630
2022-08-22 00:46:32 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.7429
2022-08-22 00:47:06 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.4647
2022-08-22 00:47:41 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.6704
2022-08-22 00:48:15 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5700
2022-08-22 00:48:50 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.7401
2022-08-22 00:49:24 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.5901
2022-08-22 00:49:58 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.5692
2022-08-22 00:50:33 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.7477
2022-08-22 00:51:07 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.5534
2022-08-22 00:51:41 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.6231
2022-08-22 00:52:16 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.6166
2022-08-22 00:52:50 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.8271
2022-08-22 00:53:25 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.5923
2022-08-22 00:53:59 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.4967
2022-08-22 00:54:34 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.6819
2022-08-22 00:55:08 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.5801
2022-08-22 00:55:43 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.6254
2022-08-22 00:56:17 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.6127
2022-08-22 00:56:52 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.8960
2022-08-22 00:57:26 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.3627
2022-08-22 00:58:01 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.5624
2022-08-22 00:58:35 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.6175
2022-08-22 00:59:10 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.8089
2022-08-22 00:59:44 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.6552
2022-08-22 01:00:19 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.2922
2022-08-22 01:00:53 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.5845
2022-08-22 01:01:28 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.6005
2022-08-22 01:02:02 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.7291
2022-08-22 01:02:37 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.3596
2022-08-22 01:03:11 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.5049
2022-08-22 01:03:46 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.6546
2022-08-22 01:04:21 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.4990
2022-08-22 01:04:55 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.5532
2022-08-22 01:05:29 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.7760
2022-08-22 01:06:04 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.5567
2022-08-22 01:06:38 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.4534
2022-08-22 01:07:13 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.6603
2022-08-22 01:07:47 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.8314
2022-08-22 01:08:22 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.5531
2022-08-22 01:08:56 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.4007
2022-08-22 01:09:31 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.3998
2022-08-22 01:10:05 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.4692
2022-08-22 01:10:39 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.6148
2022-08-22 01:10:41 - train: epoch 019, train_loss: 1.5653
2022-08-22 01:11:58 - eval: epoch: 019, acc1: 67.780%, acc5: 88.372%, test_loss: 1.3148, per_image_load_time: 2.111ms, per_image_inference_time: 0.543ms
2022-08-22 01:11:59 - until epoch: 019, best_acc1: 67.780%
2022-08-22 01:11:59 - epoch 020 lr: 0.013551
2022-08-22 01:12:41 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.6842
2022-08-22 01:13:15 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.3485
2022-08-22 01:13:49 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.4066
2022-08-22 01:14:23 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.4423
2022-08-22 01:14:58 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.5161
2022-08-22 01:15:32 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.6307
2022-08-22 01:16:06 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.3590
2022-08-22 01:16:41 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.4883
2022-08-22 01:17:15 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.6725
2022-08-22 01:17:49 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.5567
2022-08-22 01:18:24 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.3585
2022-08-22 01:18:58 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.2539
2022-08-22 01:19:32 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.3727
2022-08-22 01:20:06 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.3515
2022-08-22 01:20:41 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.5140
2022-08-22 01:21:15 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.6254
2022-08-22 01:21:50 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.5609
2022-08-22 01:22:24 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.5647
2022-08-22 01:22:59 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.2485
2022-08-22 01:23:33 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.5557
2022-08-22 01:24:08 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.6528
2022-08-22 01:24:42 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.3914
2022-08-22 01:25:17 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.4623
2022-08-22 01:25:51 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.6304
2022-08-22 01:26:26 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.4510
2022-08-22 01:27:00 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.3956
2022-08-22 01:27:35 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.4866
2022-08-22 01:28:09 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.3781
2022-08-22 01:28:43 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.5881
2022-08-22 01:29:17 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.5954
2022-08-22 01:29:52 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.5628
2022-08-22 01:30:26 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.6377
2022-08-22 01:31:00 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.3336
2022-08-22 01:31:35 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.4032
2022-08-22 01:32:09 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.3796
2022-08-22 01:32:44 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.4649
2022-08-22 01:33:18 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.4177
2022-08-22 01:33:53 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.4874
2022-08-22 01:34:27 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.6566
2022-08-22 01:35:01 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.3682
2022-08-22 01:35:36 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.5063
2022-08-22 01:36:10 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.4734
2022-08-22 01:36:44 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.5574
2022-08-22 01:37:19 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.2246
2022-08-22 01:37:53 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.5361
2022-08-22 01:38:28 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.4031
2022-08-22 01:39:02 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.5566
2022-08-22 01:39:37 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4556
2022-08-22 01:40:11 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.6524
2022-08-22 01:40:45 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.3825
2022-08-22 01:40:47 - train: epoch 020, train_loss: 1.4903
2022-08-22 01:42:05 - eval: epoch: 020, acc1: 69.290%, acc5: 89.218%, test_loss: 1.2484, per_image_load_time: 1.858ms, per_image_inference_time: 0.567ms
2022-08-22 01:42:05 - until epoch: 020, best_acc1: 69.290%
2022-08-22 01:42:05 - epoch 021 lr: 0.009548
2022-08-22 01:42:47 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.5279
2022-08-22 01:43:21 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.5093
2022-08-22 01:43:55 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.3655
2022-08-22 01:44:29 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.3867
2022-08-22 01:45:03 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.3754
2022-08-22 01:45:37 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.3056
2022-08-22 01:46:11 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.3470
2022-08-22 01:46:46 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.4138
2022-08-22 01:47:20 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.4467
2022-08-22 01:47:54 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3426
2022-08-22 01:48:28 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.2540
2022-08-22 01:49:02 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.3184
2022-08-22 01:49:37 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.4164
2022-08-22 01:50:11 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.3013
2022-08-22 01:50:45 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.3579
2022-08-22 01:51:20 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.4576
2022-08-22 01:51:54 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5402
2022-08-22 01:52:28 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.3112
2022-08-22 01:53:03 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.7169
2022-08-22 01:53:37 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.4750
2022-08-22 01:54:11 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3297
2022-08-22 01:54:46 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.4242
2022-08-22 01:55:20 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.5102
2022-08-22 01:55:55 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.4118
2022-08-22 01:56:29 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.2923
2022-08-22 01:57:04 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.4850
2022-08-22 01:57:38 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.4251
2022-08-22 01:58:13 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.2812
2022-08-22 01:58:47 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.2072
2022-08-22 01:59:22 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.6970
2022-08-22 01:59:57 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.3832
2022-08-22 02:00:32 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.2373
2022-08-22 02:01:06 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.5887
2022-08-22 02:01:41 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.5058
2022-08-22 02:02:16 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.4514
2022-08-22 02:02:51 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.2317
2022-08-22 02:03:26 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.4022
2022-08-22 02:04:00 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.5774
2022-08-22 02:04:35 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.5830
2022-08-22 02:05:10 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.6411
2022-08-22 02:05:45 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.3140
2022-08-22 02:06:19 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.4093
2022-08-22 02:06:54 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3567
2022-08-22 02:07:29 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.5172
2022-08-22 02:08:03 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.3429
2022-08-22 02:08:37 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.2979
2022-08-22 02:09:12 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.5152
2022-08-22 02:09:46 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.4226
2022-08-22 02:10:21 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.2110
2022-08-22 02:10:55 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.3449
2022-08-22 02:10:56 - train: epoch 021, train_loss: 1.4170
2022-08-22 02:12:15 - eval: epoch: 021, acc1: 70.368%, acc5: 89.804%, test_loss: 1.1988, per_image_load_time: 2.460ms, per_image_inference_time: 0.592ms
2022-08-22 02:12:15 - until epoch: 021, best_acc1: 70.368%
2022-08-22 02:12:15 - epoch 022 lr: 0.006184
2022-08-22 02:12:58 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.1540
2022-08-22 02:13:32 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.5432
2022-08-22 02:14:06 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.2391
2022-08-22 02:14:40 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.3221
2022-08-22 02:15:14 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3640
2022-08-22 02:15:48 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.4959
2022-08-22 02:16:22 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4417
2022-08-22 02:16:56 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.4102
2022-08-22 02:17:30 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.4137
2022-08-22 02:18:04 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.5096
2022-08-22 02:18:38 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3281
2022-08-22 02:19:12 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.0919
2022-08-22 02:19:46 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.2865
2022-08-22 02:20:21 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.1569
2022-08-22 02:20:56 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.3769
2022-08-22 02:21:30 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.2098
2022-08-22 02:22:04 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.4129
2022-08-22 02:22:38 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.6964
2022-08-22 02:23:13 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.4698
2022-08-22 02:23:47 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.1816
2022-08-22 02:24:21 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.2277
2022-08-22 02:24:56 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.0537
2022-08-22 02:25:30 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.3629
2022-08-22 02:26:04 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.4448
2022-08-22 02:26:39 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.3117
2022-08-22 02:27:13 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.5270
2022-08-22 02:27:48 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.1880
2022-08-22 02:28:22 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.4038
2022-08-22 02:28:57 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.2365
2022-08-22 02:29:31 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.1324
2022-08-22 02:30:05 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.5878
2022-08-22 02:30:39 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.2988
2022-08-22 02:31:14 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.2826
2022-08-22 02:31:48 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.4649
2022-08-22 02:32:22 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.5210
2022-08-22 02:32:56 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.4567
2022-08-22 02:33:31 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.5176
2022-08-22 02:34:05 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.5697
2022-08-22 02:34:39 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.3238
2022-08-22 02:35:13 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.4422
2022-08-22 02:35:47 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.2807
2022-08-22 02:36:22 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.2714
2022-08-22 02:36:56 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.3782
2022-08-22 02:37:30 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3548
2022-08-22 02:38:04 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.3025
2022-08-22 02:38:38 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.4134
2022-08-22 02:39:12 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.4646
2022-08-22 02:39:47 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.1971
2022-08-22 02:40:21 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.2708
2022-08-22 02:40:54 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.2916
2022-08-22 02:40:56 - train: epoch 022, train_loss: 1.3502
2022-08-22 02:42:15 - eval: epoch: 022, acc1: 71.528%, acc5: 90.456%, test_loss: 1.1560, per_image_load_time: 2.459ms, per_image_inference_time: 0.605ms
2022-08-22 02:42:16 - until epoch: 022, best_acc1: 71.528%
2022-08-22 02:42:16 - epoch 023 lr: 0.003511
2022-08-22 02:42:58 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.2334
2022-08-22 02:43:31 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.1849
2022-08-22 02:44:05 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.2612
2022-08-22 02:44:39 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.2725
2022-08-22 02:45:12 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.3711
2022-08-22 02:45:46 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.2816
2022-08-22 02:46:20 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.1650
2022-08-22 02:46:54 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3555
2022-08-22 02:47:28 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4262
2022-08-22 02:48:02 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.3540
2022-08-22 02:48:36 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.1970
2022-08-22 02:49:10 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.1542
2022-08-22 02:49:44 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.3024
2022-08-22 02:50:18 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.2989
2022-08-22 02:50:52 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.2524
2022-08-22 02:51:26 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.3050
2022-08-22 02:52:00 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.4176
2022-08-22 02:52:34 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.2849
2022-08-22 02:53:08 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.4025
2022-08-22 02:53:42 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.2831
2022-08-22 02:54:17 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.4283
2022-08-22 02:54:51 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.1059
2022-08-22 02:55:25 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.0748
2022-08-22 02:55:59 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.3156
2022-08-22 02:56:34 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.2732
2022-08-22 02:57:08 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.5362
2022-08-22 02:57:42 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.3728
2022-08-22 02:58:16 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.3804
2022-08-22 02:58:50 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.2582
2022-08-22 02:59:24 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.4486
2022-08-22 02:59:59 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.3241
2022-08-22 03:00:33 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.3198
2022-08-22 03:01:07 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.2469
2022-08-22 03:01:41 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.5307
2022-08-22 03:02:15 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.2014
2022-08-22 03:02:49 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.2965
2022-08-22 03:03:23 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.1424
2022-08-22 03:03:57 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.2480
2022-08-22 03:04:31 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.3523
2022-08-22 03:05:05 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.1271
2022-08-22 03:05:39 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.3101
2022-08-22 03:06:13 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.1976
2022-08-22 03:06:47 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.3968
2022-08-22 03:07:21 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.2556
2022-08-22 03:07:55 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.2254
2022-08-22 03:08:29 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.3869
2022-08-22 03:09:03 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.1444
2022-08-22 03:09:37 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.2029
2022-08-22 03:10:11 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.2203
2022-08-22 03:10:44 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.4834
2022-08-22 03:10:46 - train: epoch 023, train_loss: 1.2963
2022-08-22 03:12:04 - eval: epoch: 023, acc1: 72.192%, acc5: 90.768%, test_loss: 1.1276, per_image_load_time: 0.620ms, per_image_inference_time: 0.585ms
2022-08-22 03:12:04 - until epoch: 023, best_acc1: 72.192%
2022-08-22 03:12:04 - epoch 024 lr: 0.001571
2022-08-22 03:12:47 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.4417
2022-08-22 03:13:21 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.2486
2022-08-22 03:13:54 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.1379
2022-08-22 03:14:29 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.2946
2022-08-22 03:15:03 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.3373
2022-08-22 03:15:37 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.3653
2022-08-22 03:16:12 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.3169
2022-08-22 03:16:46 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.1394
2022-08-22 03:17:20 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.3309
2022-08-22 03:17:54 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.2735
2022-08-22 03:18:28 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.1769
2022-08-22 03:19:02 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.2657
2022-08-22 03:19:36 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4569
2022-08-22 03:20:10 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.2789
2022-08-22 03:20:44 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.1772
2022-08-22 03:21:18 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.3019
2022-08-22 03:21:53 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.1132
2022-08-22 03:22:27 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.4490
2022-08-22 03:23:02 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.0875
2022-08-22 03:23:36 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.2595
2022-08-22 03:24:11 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.3764
2022-08-22 03:24:45 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.1113
2022-08-22 03:25:19 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.3292
2022-08-22 03:25:54 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.4470
2022-08-22 03:26:28 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.3852
2022-08-22 03:27:03 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.5520
2022-08-22 03:27:37 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.3488
2022-08-22 03:28:12 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.2252
2022-08-22 03:28:46 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.1829
2022-08-22 03:29:21 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.1217
2022-08-22 03:29:56 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.1862
2022-08-22 03:30:30 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.2782
2022-08-22 03:31:05 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.0382
2022-08-22 03:31:39 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.2163
2022-08-22 03:32:14 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.1836
2022-08-22 03:32:48 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.3111
2022-08-22 03:33:23 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.2786
2022-08-22 03:33:58 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.2640
2022-08-22 03:34:33 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.2278
2022-08-22 03:35:07 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.2967
2022-08-22 03:35:42 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.0703
2022-08-22 03:36:17 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.2599
2022-08-22 03:36:51 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.1805
2022-08-22 03:37:26 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.1709
2022-08-22 03:38:01 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.0405
2022-08-22 03:38:36 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.3516
2022-08-22 03:39:11 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.1049
2022-08-22 03:39:45 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.0274
2022-08-22 03:40:20 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.2603
2022-08-22 03:40:54 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.2269
2022-08-22 03:40:55 - train: epoch 024, train_loss: 1.2564
2022-08-22 03:42:14 - eval: epoch: 024, acc1: 72.460%, acc5: 90.872%, test_loss: 1.1118, per_image_load_time: 2.449ms, per_image_inference_time: 0.595ms
2022-08-22 03:42:14 - until epoch: 024, best_acc1: 72.460%
2022-08-22 03:42:14 - epoch 025 lr: 0.000394
2022-08-22 03:42:57 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.2136
2022-08-22 03:43:32 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1673
2022-08-22 03:44:06 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.1684
2022-08-22 03:44:40 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.3897
2022-08-22 03:45:14 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.0818
2022-08-22 03:45:48 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.4215
2022-08-22 03:46:23 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.2228
2022-08-22 03:46:57 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.3914
2022-08-22 03:47:32 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0575
2022-08-22 03:48:06 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3677
2022-08-22 03:48:40 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.3001
2022-08-22 03:49:14 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.1652
2022-08-22 03:49:48 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.2486
2022-08-22 03:50:22 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.2571
2022-08-22 03:50:56 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.2975
2022-08-22 03:51:30 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1150
2022-08-22 03:52:04 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.2249
2022-08-22 03:52:39 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.1602
2022-08-22 03:53:13 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.1077
2022-08-22 03:53:48 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.2911
2022-08-22 03:54:22 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.1098
2022-08-22 03:54:57 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.0560
2022-08-22 03:55:32 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.1097
2022-08-22 03:56:06 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.0749
2022-08-22 03:56:41 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.1430
2022-08-22 03:57:16 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.2304
2022-08-22 03:57:50 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.2825
2022-08-22 03:58:25 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.1785
2022-08-22 03:58:59 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.2043
2022-08-22 03:59:33 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.2760
2022-08-22 04:00:07 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.2367
2022-08-22 04:00:42 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.3027
2022-08-22 04:01:17 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.1386
2022-08-22 04:01:52 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.2646
2022-08-22 04:02:26 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.1357
2022-08-22 04:03:01 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.2822
2022-08-22 04:03:35 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.2380
2022-08-22 04:04:10 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3361
2022-08-22 04:04:44 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.4189
2022-08-22 04:05:19 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.1464
2022-08-22 04:05:54 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.3132
2022-08-22 04:06:29 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.2323
2022-08-22 04:07:03 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.0582
2022-08-22 04:07:38 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.2259
2022-08-22 04:08:13 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.2190
2022-08-22 04:08:48 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.1772
2022-08-22 04:09:22 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.3720
2022-08-22 04:09:57 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.1251
2022-08-22 04:10:32 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.3560
2022-08-22 04:11:06 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.2786
2022-08-22 04:11:07 - train: epoch 025, train_loss: 1.2387
2022-08-22 04:12:27 - eval: epoch: 025, acc1: 72.542%, acc5: 90.922%, test_loss: 1.1094, per_image_load_time: 2.497ms, per_image_inference_time: 0.588ms
2022-08-22 04:12:27 - until epoch: 025, best_acc1: 72.542%
2022-08-22 04:12:27 - train done. train time: 12.452 hours, best_acc1: 72.542%
