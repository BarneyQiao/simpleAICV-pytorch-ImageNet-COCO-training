2022-08-20 15:01:09 - net_idx: 11
2022-08-20 15:01:09 - net_config: {'stem_width': 64, 'depth': 17, 'w_0': 32, 'w_a': 16.874425465939442, 'w_m': 1.9607251944520512}
2022-08-20 15:01:09 - num_classes: 1000
2022-08-20 15:01:09 - input_image_size: 224
2022-08-20 15:01:09 - scale: 1.1428571428571428
2022-08-20 15:01:09 - seed: 0
2022-08-20 15:01:09 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-20 15:01:09 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-20 15:01:09 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c2100>
2022-08-20 15:01:09 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c23d0>
2022-08-20 15:01:09 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2400>
2022-08-20 15:01:09 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2460>
2022-08-20 15:01:09 - batch_size: 256
2022-08-20 15:01:09 - num_workers: 16
2022-08-20 15:01:09 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-20 15:01:09 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-20 15:01:09 - epochs: 25
2022-08-20 15:01:09 - print_interval: 100
2022-08-20 15:01:09 - accumulation_steps: 1
2022-08-20 15:01:09 - sync_bn: False
2022-08-20 15:01:09 - apex: True
2022-08-20 15:01:09 - use_ema_model: False
2022-08-20 15:01:09 - ema_model_decay: 0.9999
2022-08-20 15:01:09 - log_dir: ./log
2022-08-20 15:01:09 - checkpoint_dir: ./checkpoints
2022-08-20 15:01:09 - gpus_type: NVIDIA RTX A5000
2022-08-20 15:01:09 - gpus_num: 2
2022-08-20 15:01:09 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f4efca864b0>
2022-08-20 15:01:09 - ema_model: None
2022-08-20 15:01:09 - --------------------parameters--------------------
2022-08-20 15:01:09 - name: conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.6.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.6.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.6.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.6.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.6.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.6.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.6.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.6.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.6.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.7.conv1.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.7.conv1.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.7.conv1.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.7.conv2.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.7.conv2.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.7.conv2.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: layer4.7.conv3.layer.0.weight, grad: True
2022-08-20 15:01:09 - name: layer4.7.conv3.layer.1.weight, grad: True
2022-08-20 15:01:09 - name: layer4.7.conv3.layer.1.bias, grad: True
2022-08-20 15:01:09 - name: fc.weight, grad: True
2022-08-20 15:01:09 - name: fc.bias, grad: True
2022-08-20 15:01:09 - --------------------buffers--------------------
2022-08-20 15:01:09 - name: conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.6.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.6.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.6.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.6.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.6.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.6.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.6.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.6.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.6.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.7.conv1.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.7.conv1.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.7.conv1.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.7.conv2.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.7.conv2.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.7.conv2.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - name: layer4.7.conv3.layer.1.running_mean, grad: False
2022-08-20 15:01:09 - name: layer4.7.conv3.layer.1.running_var, grad: False
2022-08-20 15:01:09 - name: layer4.7.conv3.layer.1.num_batches_tracked, grad: False
2022-08-20 15:01:09 - -----------no weight decay layers--------------
2022-08-20 15:01:09 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.7.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.7.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.7.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.7.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.7.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.7.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-20 15:01:09 - -------------weight decay layers---------------
2022-08-20 15:01:09 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.6.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.6.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.6.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.7.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.7.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: layer4.7.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-20 15:01:09 - epoch 001 lr: 0.100000
2022-08-20 15:01:49 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9070
2022-08-20 15:02:23 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.9215
2022-08-20 15:02:56 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8873
2022-08-20 15:03:30 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.9065
2022-08-20 15:04:04 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.8582
2022-08-20 15:04:38 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.8525
2022-08-20 15:05:11 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.8604
2022-08-20 15:05:45 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.8128
2022-08-20 15:06:19 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.8078
2022-08-20 15:06:53 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.7290
2022-08-20 15:07:27 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.7292
2022-08-20 15:08:01 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.6735
2022-08-20 15:08:34 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.6504
2022-08-20 15:09:08 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.5322
2022-08-20 15:09:42 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 6.4629
2022-08-20 15:10:16 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 6.5064
2022-08-20 15:10:50 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 6.3433
2022-08-20 15:11:24 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 6.2922
2022-08-20 15:11:58 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 6.2673
2022-08-20 15:12:33 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 6.2264
2022-08-20 15:13:07 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 6.1938
2022-08-20 15:13:41 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 6.0508
2022-08-20 15:14:15 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.9777
2022-08-20 15:14:49 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 6.0370
2022-08-20 15:15:23 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.8649
2022-08-20 15:15:57 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.8965
2022-08-20 15:16:32 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.8414
2022-08-20 15:17:06 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.7310
2022-08-20 15:17:40 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 5.7259
2022-08-20 15:18:14 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.7463
2022-08-20 15:18:48 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.6362
2022-08-20 15:19:22 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 5.7021
2022-08-20 15:19:57 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 5.5299
2022-08-20 15:20:31 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 5.4778
2022-08-20 15:21:04 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 5.4725
2022-08-20 15:21:39 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 5.4682
2022-08-20 15:22:13 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 5.4069
2022-08-20 15:22:47 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 5.3173
2022-08-20 15:23:21 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 5.3813
2022-08-20 15:23:55 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 5.3398
2022-08-20 15:24:29 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 5.4171
2022-08-20 15:25:03 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 5.0815
2022-08-20 15:25:38 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 5.1513
2022-08-20 15:26:12 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 5.0278
2022-08-20 15:26:47 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 5.0421
2022-08-20 15:27:21 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 5.2564
2022-08-20 15:27:55 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.8937
2022-08-20 15:28:29 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 5.0978
2022-08-20 15:29:04 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.9531
2022-08-20 15:29:37 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.7951
2022-08-20 15:29:39 - train: epoch 001, train_loss: 5.9570
2022-08-20 15:30:57 - eval: epoch: 001, acc1: 11.418%, acc5: 28.458%, test_loss: 4.7790, per_image_load_time: 0.733ms, per_image_inference_time: 0.617ms
2022-08-20 15:30:57 - until epoch: 001, best_acc1: 11.418%
2022-08-20 15:30:57 - epoch 002 lr: 0.099606
2022-08-20 15:31:38 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.7620
2022-08-20 15:32:11 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 4.6474
2022-08-20 15:32:44 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.8503
2022-08-20 15:33:18 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.7245
2022-08-20 15:33:51 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.6468
2022-08-20 15:34:25 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 4.5105
2022-08-20 15:34:58 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.7541
2022-08-20 15:35:32 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 4.3899
2022-08-20 15:36:05 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 4.4023
2022-08-20 15:36:39 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.5208
2022-08-20 15:37:13 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.4878
2022-08-20 15:37:47 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 4.4802
2022-08-20 15:38:21 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 4.3953
2022-08-20 15:38:55 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 4.5651
2022-08-20 15:39:29 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 4.3563
2022-08-20 15:40:03 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 4.2084
2022-08-20 15:40:37 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 4.2264
2022-08-20 15:41:11 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 4.5095
2022-08-20 15:41:45 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 4.0139
2022-08-20 15:42:19 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.9213
2022-08-20 15:42:53 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 4.2216
2022-08-20 15:43:27 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 4.0102
2022-08-20 15:44:02 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 4.2642
2022-08-20 15:44:36 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 4.1326
2022-08-20 15:45:10 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 4.0926
2022-08-20 15:45:43 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 4.0139
2022-08-20 15:46:17 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 4.1162
2022-08-20 15:46:51 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.9351
2022-08-20 15:47:25 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 4.0603
2022-08-20 15:47:59 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.8219
2022-08-20 15:48:33 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.9251
2022-08-20 15:49:07 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.8363
2022-08-20 15:49:41 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.9249
2022-08-20 15:50:15 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.9061
2022-08-20 15:50:49 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.6709
2022-08-20 15:51:23 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 4.0253
2022-08-20 15:51:57 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.9138
2022-08-20 15:52:31 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.6672
2022-08-20 15:53:05 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.7859
2022-08-20 15:53:39 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.7241
2022-08-20 15:54:13 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.8728
2022-08-20 15:54:47 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.7134
2022-08-20 15:55:21 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.6440
2022-08-20 15:55:55 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.5796
2022-08-20 15:56:29 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.4696
2022-08-20 15:57:03 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.5372
2022-08-20 15:57:36 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.5967
2022-08-20 15:58:11 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.5833
2022-08-20 15:58:45 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.5938
2022-08-20 15:59:18 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.5937
2022-08-20 15:59:20 - train: epoch 002, train_loss: 4.1098
2022-08-20 16:00:37 - eval: epoch: 002, acc1: 27.272%, acc5: 52.486%, test_loss: 3.5151, per_image_load_time: 0.651ms, per_image_inference_time: 0.602ms
2022-08-20 16:00:37 - until epoch: 002, best_acc1: 27.272%
2022-08-20 16:00:37 - epoch 003 lr: 0.098429
2022-08-20 16:01:17 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.6127
2022-08-20 16:01:51 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.7523
2022-08-20 16:02:24 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.5201
2022-08-20 16:02:58 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.4806
2022-08-20 16:03:31 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.7011
2022-08-20 16:04:05 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.2887
2022-08-20 16:04:38 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.4427
2022-08-20 16:05:12 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.6027
2022-08-20 16:05:46 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.3989
2022-08-20 16:06:20 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.3023
2022-08-20 16:06:54 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.3392
2022-08-20 16:07:28 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.4182
2022-08-20 16:08:02 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.3366
2022-08-20 16:08:35 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.1070
2022-08-20 16:09:09 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.4273
2022-08-20 16:09:43 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 3.4295
2022-08-20 16:10:17 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.2958
2022-08-20 16:10:51 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.2444
2022-08-20 16:11:25 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.2859
2022-08-20 16:11:59 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.3149
2022-08-20 16:12:33 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.4463
2022-08-20 16:13:06 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.5919
2022-08-20 16:13:40 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 3.2311
2022-08-20 16:14:14 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.0311
2022-08-20 16:14:48 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.1554
2022-08-20 16:15:22 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.3429
2022-08-20 16:15:56 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.5071
2022-08-20 16:16:30 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 2.9934
2022-08-20 16:17:04 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 3.1396
2022-08-20 16:17:39 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.0282
2022-08-20 16:18:13 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.5338
2022-08-20 16:18:48 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.2187
2022-08-20 16:19:22 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.3826
2022-08-20 16:19:56 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.4556
2022-08-20 16:20:30 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.9675
2022-08-20 16:21:05 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 3.2261
2022-08-20 16:21:39 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.2623
2022-08-20 16:22:13 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.2635
2022-08-20 16:22:48 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.1288
2022-08-20 16:23:22 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.7299
2022-08-20 16:23:56 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 3.1561
2022-08-20 16:24:30 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.1045
2022-08-20 16:25:04 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 3.0129
2022-08-20 16:25:38 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 3.0895
2022-08-20 16:26:12 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 3.0699
2022-08-20 16:26:47 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 2.8627
2022-08-20 16:27:21 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 3.1328
2022-08-20 16:27:55 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.1671
2022-08-20 16:28:29 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.1119
2022-08-20 16:29:02 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 3.1514
2022-08-20 16:29:04 - train: epoch 003, train_loss: 3.2633
2022-08-20 16:30:20 - eval: epoch: 003, acc1: 36.014%, acc5: 62.198%, test_loss: 2.9760, per_image_load_time: 1.473ms, per_image_inference_time: 0.613ms
2022-08-20 16:30:21 - until epoch: 003, best_acc1: 36.014%
2022-08-20 16:30:21 - epoch 004 lr: 0.096488
2022-08-20 16:31:01 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 3.0661
2022-08-20 16:31:35 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.7691
2022-08-20 16:32:09 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 3.0263
2022-08-20 16:32:42 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.9213
2022-08-20 16:33:15 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.9610
2022-08-20 16:33:49 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.0938
2022-08-20 16:34:23 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.9473
2022-08-20 16:34:57 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.9634
2022-08-20 16:35:31 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.8145
2022-08-20 16:36:05 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 2.9580
2022-08-20 16:36:39 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.1309
2022-08-20 16:37:12 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.8576
2022-08-20 16:37:46 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.9625
2022-08-20 16:38:20 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.9644
2022-08-20 16:38:54 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 3.1007
2022-08-20 16:39:29 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.9838
2022-08-20 16:40:03 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 3.0297
2022-08-20 16:40:37 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 2.9504
2022-08-20 16:41:11 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.9108
2022-08-20 16:41:45 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.7647
2022-08-20 16:42:19 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 3.0176
2022-08-20 16:42:53 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.8245
2022-08-20 16:43:27 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.5187
2022-08-20 16:44:01 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.5863
2022-08-20 16:44:35 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.8218
2022-08-20 16:45:10 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.8252
2022-08-20 16:45:43 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.7118
2022-08-20 16:46:17 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8210
2022-08-20 16:46:51 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8857
2022-08-20 16:47:26 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.7534
2022-08-20 16:48:00 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.6651
2022-08-20 16:48:34 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.8011
2022-08-20 16:49:08 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.8966
2022-08-20 16:49:41 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.8562
2022-08-20 16:50:15 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.7993
2022-08-20 16:50:49 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.7989
2022-08-20 16:51:24 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.8173
2022-08-20 16:51:58 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.7905
2022-08-20 16:52:32 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.5741
2022-08-20 16:53:07 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.4993
2022-08-20 16:53:41 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.7346
2022-08-20 16:54:15 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.7266
2022-08-20 16:54:49 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.5421
2022-08-20 16:55:23 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.7238
2022-08-20 16:55:57 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.3470
2022-08-20 16:56:32 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.6389
2022-08-20 16:57:06 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.8164
2022-08-20 16:57:39 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.5237
2022-08-20 16:58:13 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.9404
2022-08-20 16:58:46 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.7000
2022-08-20 16:58:48 - train: epoch 004, train_loss: 2.8671
2022-08-20 17:00:05 - eval: epoch: 004, acc1: 43.106%, acc5: 69.562%, test_loss: 2.5481, per_image_load_time: 0.616ms, per_image_inference_time: 0.606ms
2022-08-20 17:00:05 - until epoch: 004, best_acc1: 43.106%
2022-08-20 17:00:05 - epoch 005 lr: 0.093815
2022-08-20 17:00:45 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.8689
2022-08-20 17:01:19 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.7772
2022-08-20 17:01:53 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 3.1001
2022-08-20 17:02:26 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.6011
2022-08-20 17:02:59 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.4447
2022-08-20 17:03:33 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.7318
2022-08-20 17:04:06 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.6847
2022-08-20 17:04:40 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.7296
2022-08-20 17:05:14 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.5745
2022-08-20 17:05:47 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.6480
2022-08-20 17:06:21 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.6434
2022-08-20 17:06:55 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.5344
2022-08-20 17:07:29 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.5365
2022-08-20 17:08:03 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.7016
2022-08-20 17:08:37 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.5105
2022-08-20 17:09:11 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.5000
2022-08-20 17:09:45 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.6487
2022-08-20 17:10:19 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.5324
2022-08-20 17:10:53 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.6836
2022-08-20 17:11:27 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.7662
2022-08-20 17:12:01 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.3888
2022-08-20 17:12:36 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.4444
2022-08-20 17:13:10 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.4505
2022-08-20 17:13:44 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.6591
2022-08-20 17:14:18 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.6767
2022-08-20 17:14:52 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.7794
2022-08-20 17:15:26 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.6921
2022-08-20 17:16:00 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.4383
2022-08-20 17:16:34 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.4342
2022-08-20 17:17:08 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.5486
2022-08-20 17:17:42 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.8500
2022-08-20 17:18:16 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.4729
2022-08-20 17:18:50 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.4807
2022-08-20 17:19:24 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.4163
2022-08-20 17:19:58 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.6764
2022-08-20 17:20:32 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.5780
2022-08-20 17:21:06 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.4521
2022-08-20 17:21:40 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.6792
2022-08-20 17:22:14 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.6864
2022-08-20 17:22:48 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.4630
2022-08-20 17:23:22 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.6547
2022-08-20 17:23:56 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.7186
2022-08-20 17:24:30 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.7279
2022-08-20 17:25:04 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.5670
2022-08-20 17:25:38 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.5049
2022-08-20 17:26:12 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.4962
2022-08-20 17:26:47 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.2893
2022-08-20 17:27:21 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.3961
2022-08-20 17:27:55 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.7989
2022-08-20 17:28:28 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.5977
2022-08-20 17:28:30 - train: epoch 005, train_loss: 2.6308
2022-08-20 17:29:48 - eval: epoch: 005, acc1: 47.534%, acc5: 73.716%, test_loss: 2.3100, per_image_load_time: 0.714ms, per_image_inference_time: 0.613ms
2022-08-20 17:29:48 - until epoch: 005, best_acc1: 47.534%
2022-08-20 17:29:48 - epoch 006 lr: 0.090450
2022-08-20 17:30:29 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.4421
2022-08-20 17:31:02 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.5595
2022-08-20 17:31:36 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.5701
2022-08-20 17:32:10 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.4623
2022-08-20 17:32:43 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.4583
2022-08-20 17:33:18 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.6123
2022-08-20 17:33:52 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.3621
2022-08-20 17:34:26 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.5256
2022-08-20 17:35:00 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3659
2022-08-20 17:35:33 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.3996
2022-08-20 17:36:07 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.5874
2022-08-20 17:36:41 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.5028
2022-08-20 17:37:15 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5432
2022-08-20 17:37:49 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.4701
2022-08-20 17:38:23 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.6878
2022-08-20 17:38:57 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.3191
2022-08-20 17:39:31 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.6731
2022-08-20 17:40:06 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.4209
2022-08-20 17:40:40 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.4271
2022-08-20 17:41:14 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.6524
2022-08-20 17:41:48 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.4615
2022-08-20 17:42:22 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.2249
2022-08-20 17:42:56 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.4440
2022-08-20 17:43:30 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.3930
2022-08-20 17:44:04 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4765
2022-08-20 17:44:38 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.2138
2022-08-20 17:45:13 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.4813
2022-08-20 17:45:47 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.2615
2022-08-20 17:46:21 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.6094
2022-08-20 17:46:55 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.4583
2022-08-20 17:47:29 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.3025
2022-08-20 17:48:03 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.4286
2022-08-20 17:48:37 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.3743
2022-08-20 17:49:12 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.5672
2022-08-20 17:49:46 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.4683
2022-08-20 17:50:21 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.4691
2022-08-20 17:50:55 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.6508
2022-08-20 17:51:29 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.1598
2022-08-20 17:52:03 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.3950
2022-08-20 17:52:37 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.4329
2022-08-20 17:53:11 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.4921
2022-08-20 17:53:46 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.2845
2022-08-20 17:54:20 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.2757
2022-08-20 17:54:54 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.3090
2022-08-20 17:55:28 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.5542
2022-08-20 17:56:02 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.3313
2022-08-20 17:56:36 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.5133
2022-08-20 17:57:10 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.5328
2022-08-20 17:57:44 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.4702
2022-08-20 17:58:18 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.2802
2022-08-20 17:58:20 - train: epoch 006, train_loss: 2.4784
2022-08-20 17:59:36 - eval: epoch: 006, acc1: 50.016%, acc5: 75.638%, test_loss: 2.1773, per_image_load_time: 0.599ms, per_image_inference_time: 0.604ms
2022-08-20 17:59:36 - until epoch: 006, best_acc1: 50.016%
2022-08-20 17:59:36 - epoch 007 lr: 0.086448
2022-08-20 18:00:17 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.1845
2022-08-20 18:00:50 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.5686
2022-08-20 18:01:24 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.6281
2022-08-20 18:01:58 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.4858
2022-08-20 18:02:33 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.3532
2022-08-20 18:03:07 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.3057
2022-08-20 18:03:41 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.3351
2022-08-20 18:04:15 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.3745
2022-08-20 18:04:49 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.6688
2022-08-20 18:05:23 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.5400
2022-08-20 18:05:57 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.3885
2022-08-20 18:06:31 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.4952
2022-08-20 18:07:05 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.1115
2022-08-20 18:07:38 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.3806
2022-08-20 18:08:12 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.5380
2022-08-20 18:08:46 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.4618
2022-08-20 18:09:20 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.3610
2022-08-20 18:09:54 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.5219
2022-08-20 18:10:29 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.4416
2022-08-20 18:11:03 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 1.9637
2022-08-20 18:11:37 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.5636
2022-08-20 18:12:11 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.2469
2022-08-20 18:12:45 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.6196
2022-08-20 18:13:19 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.6279
2022-08-20 18:13:53 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.4383
2022-08-20 18:14:27 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.2132
2022-08-20 18:15:01 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.2062
2022-08-20 18:15:35 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.2736
2022-08-20 18:16:09 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.3537
2022-08-20 18:16:43 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.3574
2022-08-20 18:17:18 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.3829
2022-08-20 18:17:52 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.2885
2022-08-20 18:18:26 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.5926
2022-08-20 18:19:00 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.2979
2022-08-20 18:19:35 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.6045
2022-08-20 18:20:09 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.2588
2022-08-20 18:20:42 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.3023
2022-08-20 18:21:16 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.4559
2022-08-20 18:21:50 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.4135
2022-08-20 18:22:24 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.4158
2022-08-20 18:22:58 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.2386
2022-08-20 18:23:32 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.2096
2022-08-20 18:24:06 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.4449
2022-08-20 18:24:40 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.1935
2022-08-20 18:25:14 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.3683
2022-08-20 18:25:49 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.5268
2022-08-20 18:26:23 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.3864
2022-08-20 18:26:58 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.3606
2022-08-20 18:27:32 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.3899
2022-08-20 18:28:05 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.3663
2022-08-20 18:28:07 - train: epoch 007, train_loss: 2.3682
2022-08-20 18:29:24 - eval: epoch: 007, acc1: 51.666%, acc5: 77.174%, test_loss: 2.0943, per_image_load_time: 0.688ms, per_image_inference_time: 0.605ms
2022-08-20 18:29:24 - until epoch: 007, best_acc1: 51.666%
2022-08-20 18:29:24 - epoch 008 lr: 0.081870
2022-08-20 18:30:05 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.1936
2022-08-20 18:30:38 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.3407
2022-08-20 18:31:12 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.3681
2022-08-20 18:31:46 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.2428
2022-08-20 18:32:19 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.2567
2022-08-20 18:32:53 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.3895
2022-08-20 18:33:27 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.4517
2022-08-20 18:34:01 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.2073
2022-08-20 18:34:35 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.0657
2022-08-20 18:35:09 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.3366
2022-08-20 18:35:43 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.2374
2022-08-20 18:36:17 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.1846
2022-08-20 18:36:51 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.3870
2022-08-20 18:37:25 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.1223
2022-08-20 18:37:59 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.3611
2022-08-20 18:38:34 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.3387
2022-08-20 18:39:08 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.3199
2022-08-20 18:39:42 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.3064
2022-08-20 18:40:16 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.0657
2022-08-20 18:40:50 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.3290
2022-08-20 18:41:24 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.2285
2022-08-20 18:41:58 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.1260
2022-08-20 18:42:32 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.1694
2022-08-20 18:43:06 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.1236
2022-08-20 18:43:40 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.2888
2022-08-20 18:44:14 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.2922
2022-08-20 18:44:48 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.5116
2022-08-20 18:45:22 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.4267
2022-08-20 18:45:56 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.2216
2022-08-20 18:46:30 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.3769
2022-08-20 18:47:05 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.2652
2022-08-20 18:47:39 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.3202
2022-08-20 18:48:13 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.2587
2022-08-20 18:48:47 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.2738
2022-08-20 18:49:21 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.3040
2022-08-20 18:49:55 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.3406
2022-08-20 18:50:29 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.1515
2022-08-20 18:51:03 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.1692
2022-08-20 18:51:37 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.4314
2022-08-20 18:52:12 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.4875
2022-08-20 18:52:46 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.1217
2022-08-20 18:53:20 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.1525
2022-08-20 18:53:54 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 1.9505
2022-08-20 18:54:28 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.1608
2022-08-20 18:55:02 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.2008
2022-08-20 18:55:35 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.3724
2022-08-20 18:56:10 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.2424
2022-08-20 18:56:43 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.2284
2022-08-20 18:57:18 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.3663
2022-08-20 18:57:51 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.2654
2022-08-20 18:57:52 - train: epoch 008, train_loss: 2.2809
2022-08-20 18:59:09 - eval: epoch: 008, acc1: 53.806%, acc5: 79.142%, test_loss: 1.9783, per_image_load_time: 1.304ms, per_image_inference_time: 0.598ms
2022-08-20 18:59:10 - until epoch: 008, best_acc1: 53.806%
2022-08-20 18:59:10 - epoch 009 lr: 0.076790
2022-08-20 18:59:50 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 1.9719
2022-08-20 19:00:24 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.1213
2022-08-20 19:00:57 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.1064
2022-08-20 19:01:31 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.3432
2022-08-20 19:02:05 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.2032
2022-08-20 19:02:39 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.0433
2022-08-20 19:03:13 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.1225
2022-08-20 19:03:47 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.1932
2022-08-20 19:04:21 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.0776
2022-08-20 19:04:54 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1048
2022-08-20 19:05:28 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.4541
2022-08-20 19:06:02 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.3474
2022-08-20 19:06:35 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.4068
2022-08-20 19:07:09 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 1.9461
2022-08-20 19:07:43 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.2471
2022-08-20 19:08:17 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2147
2022-08-20 19:08:51 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.2428
2022-08-20 19:09:25 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.2849
2022-08-20 19:09:59 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 2.0603
2022-08-20 19:10:33 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.0881
2022-08-20 19:11:07 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.3769
2022-08-20 19:11:41 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.2892
2022-08-20 19:12:15 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.1580
2022-08-20 19:12:49 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.3149
2022-08-20 19:13:24 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.2850
2022-08-20 19:13:58 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.2246
2022-08-20 19:14:32 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.1279
2022-08-20 19:15:06 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2730
2022-08-20 19:15:40 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.1242
2022-08-20 19:16:14 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1765
2022-08-20 19:16:49 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.3956
2022-08-20 19:17:22 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.3289
2022-08-20 19:17:56 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.2401
2022-08-20 19:18:30 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.1563
2022-08-20 19:19:04 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.0884
2022-08-20 19:19:38 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.0670
2022-08-20 19:20:12 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.1960
2022-08-20 19:20:46 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.4334
2022-08-20 19:21:20 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 2.0068
2022-08-20 19:21:54 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.5923
2022-08-20 19:22:29 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.1806
2022-08-20 19:23:03 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 1.8584
2022-08-20 19:23:37 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 1.9658
2022-08-20 19:24:11 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.1814
2022-08-20 19:24:45 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.3122
2022-08-20 19:25:20 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.3144
2022-08-20 19:25:54 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.3602
2022-08-20 19:26:28 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.2129
2022-08-20 19:27:02 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.3684
2022-08-20 19:27:35 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.1990
2022-08-20 19:27:37 - train: epoch 009, train_loss: 2.2039
2022-08-20 19:28:54 - eval: epoch: 009, acc1: 55.334%, acc5: 80.476%, test_loss: 1.9011, per_image_load_time: 0.654ms, per_image_inference_time: 0.601ms
2022-08-20 19:28:54 - until epoch: 009, best_acc1: 55.334%
2022-08-20 19:28:54 - epoch 010 lr: 0.071288
2022-08-20 19:29:35 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 1.9975
2022-08-20 19:30:09 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.0836
2022-08-20 19:30:42 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.1982
2022-08-20 19:31:16 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.3363
2022-08-20 19:31:50 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.1388
2022-08-20 19:32:24 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.0821
2022-08-20 19:32:58 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.2370
2022-08-20 19:33:32 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.3161
2022-08-20 19:34:06 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 1.9268
2022-08-20 19:34:40 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.0307
2022-08-20 19:35:14 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 1.9577
2022-08-20 19:35:49 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.1162
2022-08-20 19:36:23 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 2.0665
2022-08-20 19:36:57 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.1233
2022-08-20 19:37:31 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.8846
2022-08-20 19:38:05 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.0833
2022-08-20 19:38:39 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.0755
2022-08-20 19:39:14 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 1.9801
2022-08-20 19:39:48 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.3948
2022-08-20 19:40:22 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.0954
2022-08-20 19:40:56 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 1.9863
2022-08-20 19:41:30 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.2957
2022-08-20 19:42:04 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.3509
2022-08-20 19:42:38 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.4233
2022-08-20 19:43:12 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.1658
2022-08-20 19:43:46 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.2755
2022-08-20 19:44:20 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 1.8333
2022-08-20 19:44:55 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.0942
2022-08-20 19:45:29 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.3904
2022-08-20 19:46:03 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1854
2022-08-20 19:46:37 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.2478
2022-08-20 19:47:11 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.0772
2022-08-20 19:47:45 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.1779
2022-08-20 19:48:19 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.2120
2022-08-20 19:48:54 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.5815
2022-08-20 19:49:28 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.0997
2022-08-20 19:50:02 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.2933
2022-08-20 19:50:37 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.1775
2022-08-20 19:51:11 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 2.0942
2022-08-20 19:51:45 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 1.9961
2022-08-20 19:52:19 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.0005
2022-08-20 19:52:53 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 1.9930
2022-08-20 19:53:27 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 2.0297
2022-08-20 19:54:02 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.0301
2022-08-20 19:54:36 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 1.9776
2022-08-20 19:55:10 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.0632
2022-08-20 19:55:44 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.1016
2022-08-20 19:56:18 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 1.9359
2022-08-20 19:56:53 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.3075
2022-08-20 19:57:26 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.0659
2022-08-20 19:57:27 - train: epoch 010, train_loss: 2.1367
2022-08-20 19:58:44 - eval: epoch: 010, acc1: 55.966%, acc5: 80.382%, test_loss: 1.8760, per_image_load_time: 0.932ms, per_image_inference_time: 0.604ms
2022-08-20 19:58:44 - until epoch: 010, best_acc1: 55.966%
2022-08-20 19:58:44 - epoch 011 lr: 0.065450
2022-08-20 19:59:25 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.0227
2022-08-20 19:59:58 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.1758
2022-08-20 20:00:32 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.2520
2022-08-20 20:01:06 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.1837
2022-08-20 20:01:40 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 1.8473
2022-08-20 20:02:14 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.3099
2022-08-20 20:02:48 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.0281
2022-08-20 20:03:22 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.3312
2022-08-20 20:03:56 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.1060
2022-08-20 20:04:30 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.0220
2022-08-20 20:05:04 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2207
2022-08-20 20:05:38 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2444
2022-08-20 20:06:11 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.3516
2022-08-20 20:06:45 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.2041
2022-08-20 20:07:19 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 1.9711
2022-08-20 20:07:53 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.2364
2022-08-20 20:08:27 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.0651
2022-08-20 20:09:01 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 2.0194
2022-08-20 20:09:36 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.0711
2022-08-20 20:10:10 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.1308
2022-08-20 20:10:44 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.0848
2022-08-20 20:11:18 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 2.1005
2022-08-20 20:11:52 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.2287
2022-08-20 20:12:26 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 1.9849
2022-08-20 20:13:00 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.0911
2022-08-20 20:13:35 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.1374
2022-08-20 20:14:08 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.1020
2022-08-20 20:14:43 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 2.0044
2022-08-20 20:15:17 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.0406
2022-08-20 20:15:51 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.4253
2022-08-20 20:16:25 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 1.8395
2022-08-20 20:16:59 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 1.8866
2022-08-20 20:17:33 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.2248
2022-08-20 20:18:07 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 1.9443
2022-08-20 20:18:41 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.0987
2022-08-20 20:19:16 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 2.1693
2022-08-20 20:19:49 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.1638
2022-08-20 20:20:23 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.9207
2022-08-20 20:20:58 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.0842
2022-08-20 20:21:32 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 1.8752
2022-08-20 20:22:06 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.8734
2022-08-20 20:22:41 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 1.8241
2022-08-20 20:23:15 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.0998
2022-08-20 20:23:50 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.1395
2022-08-20 20:24:24 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.0945
2022-08-20 20:24:59 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 1.8719
2022-08-20 20:25:33 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.8600
2022-08-20 20:26:07 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 1.8452
2022-08-20 20:26:41 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.1195
2022-08-20 20:27:14 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 1.8927
2022-08-20 20:27:16 - train: epoch 011, train_loss: 2.0700
2022-08-20 20:28:33 - eval: epoch: 011, acc1: 56.436%, acc5: 80.864%, test_loss: 1.8465, per_image_load_time: 0.766ms, per_image_inference_time: 0.599ms
2022-08-20 20:28:33 - until epoch: 011, best_acc1: 56.436%
2022-08-20 20:28:33 - epoch 012 lr: 0.059368
2022-08-20 20:29:13 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.8061
2022-08-20 20:29:47 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.7501
2022-08-20 20:30:20 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 2.0802
2022-08-20 20:30:53 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 2.1280
2022-08-20 20:31:27 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 2.0792
2022-08-20 20:32:01 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 1.9318
2022-08-20 20:32:35 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.7033
2022-08-20 20:33:09 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.0429
2022-08-20 20:33:43 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 1.9362
2022-08-20 20:34:16 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.8679
2022-08-20 20:34:50 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.1192
2022-08-20 20:35:24 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.7194
2022-08-20 20:35:58 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.9529
2022-08-20 20:36:31 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.1281
2022-08-20 20:37:05 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 1.8687
2022-08-20 20:37:39 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 1.8373
2022-08-20 20:38:13 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 2.0234
2022-08-20 20:38:47 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.2202
2022-08-20 20:39:21 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1313
2022-08-20 20:39:56 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.0785
2022-08-20 20:40:30 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.0720
2022-08-20 20:41:04 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.1156
2022-08-20 20:41:38 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 1.9181
2022-08-20 20:42:12 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.1259
2022-08-20 20:42:46 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.6664
2022-08-20 20:43:20 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.8945
2022-08-20 20:43:54 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.0065
2022-08-20 20:44:28 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 1.9521
2022-08-20 20:45:02 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.0272
2022-08-20 20:45:36 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.9346
2022-08-20 20:46:10 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 2.0227
2022-08-20 20:46:44 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 1.9723
2022-08-20 20:47:17 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.2297
2022-08-20 20:47:51 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 2.1255
2022-08-20 20:48:25 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 1.9079
2022-08-20 20:48:59 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.9368
2022-08-20 20:49:33 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 2.0789
2022-08-20 20:50:06 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 1.9665
2022-08-20 20:50:41 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.7925
2022-08-20 20:51:15 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.1612
2022-08-20 20:51:48 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 2.0025
2022-08-20 20:52:23 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 2.1165
2022-08-20 20:52:56 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 1.9241
2022-08-20 20:53:30 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.7830
2022-08-20 20:54:04 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 1.9470
2022-08-20 20:54:38 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.0606
2022-08-20 20:55:12 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 2.0029
2022-08-20 20:55:46 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.0805
2022-08-20 20:56:21 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.9217
2022-08-20 20:56:54 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.6837
2022-08-20 20:56:55 - train: epoch 012, train_loss: 2.0098
2022-08-20 20:58:13 - eval: epoch: 012, acc1: 59.918%, acc5: 83.176%, test_loss: 1.6947, per_image_load_time: 1.364ms, per_image_inference_time: 0.585ms
2022-08-20 20:58:13 - until epoch: 012, best_acc1: 59.918%
2022-08-20 20:58:13 - epoch 013 lr: 0.053138
2022-08-20 20:58:53 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.7434
2022-08-20 20:59:27 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 2.0967
2022-08-20 21:00:01 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8943
2022-08-20 21:00:35 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.7826
2022-08-20 21:01:08 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 1.9401
2022-08-20 21:01:42 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 1.9956
2022-08-20 21:02:16 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.8776
2022-08-20 21:02:50 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.0714
2022-08-20 21:03:24 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 2.0572
2022-08-20 21:03:58 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.8925
2022-08-20 21:04:32 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 1.9522
2022-08-20 21:05:06 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 1.9424
2022-08-20 21:05:40 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.8487
2022-08-20 21:06:14 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 2.0152
2022-08-20 21:06:48 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 2.1099
2022-08-20 21:07:22 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.8674
2022-08-20 21:07:56 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 1.9286
2022-08-20 21:08:30 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.7908
2022-08-20 21:09:04 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.2006
2022-08-20 21:09:38 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 1.9802
2022-08-20 21:10:12 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.1950
2022-08-20 21:10:46 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 1.9007
2022-08-20 21:11:21 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 1.9604
2022-08-20 21:11:55 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 1.9696
2022-08-20 21:12:29 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 1.7516
2022-08-20 21:13:03 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.9527
2022-08-20 21:13:37 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 2.0002
2022-08-20 21:14:11 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 2.0747
2022-08-20 21:14:45 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.1285
2022-08-20 21:15:19 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 2.0516
2022-08-20 21:15:53 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.9305
2022-08-20 21:16:28 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 2.0271
2022-08-20 21:17:02 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.8542
2022-08-20 21:17:36 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.8467
2022-08-20 21:18:10 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.9415
2022-08-20 21:18:44 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 1.8977
2022-08-20 21:19:18 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.6703
2022-08-20 21:19:52 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.1511
2022-08-20 21:20:26 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.2576
2022-08-20 21:21:01 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 1.9584
2022-08-20 21:21:35 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.9081
2022-08-20 21:22:09 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 1.7838
2022-08-20 21:22:43 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.9364
2022-08-20 21:23:17 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 1.7243
2022-08-20 21:23:51 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.9090
2022-08-20 21:24:26 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.1234
2022-08-20 21:25:00 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 1.8911
2022-08-20 21:25:34 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 1.9487
2022-08-20 21:26:08 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.0078
2022-08-20 21:26:42 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.2833
2022-08-20 21:26:43 - train: epoch 013, train_loss: 1.9491
2022-08-20 21:27:59 - eval: epoch: 013, acc1: 60.918%, acc5: 83.976%, test_loss: 1.6367, per_image_load_time: 0.693ms, per_image_inference_time: 0.612ms
2022-08-20 21:28:00 - until epoch: 013, best_acc1: 60.918%
2022-08-20 21:28:00 - epoch 014 lr: 0.046859
2022-08-20 21:28:40 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.7952
2022-08-20 21:29:13 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 2.0067
2022-08-20 21:29:47 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.8317
2022-08-20 21:30:21 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 1.8213
2022-08-20 21:30:55 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.9681
2022-08-20 21:31:29 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.9665
2022-08-20 21:32:03 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.6448
2022-08-20 21:32:38 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.7262
2022-08-20 21:33:12 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.8762
2022-08-20 21:33:45 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 2.1088
2022-08-20 21:34:19 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 1.8305
2022-08-20 21:34:53 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 1.8766
2022-08-20 21:35:27 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.0005
2022-08-20 21:36:01 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 1.7989
2022-08-20 21:36:35 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 1.9508
2022-08-20 21:37:09 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.9082
2022-08-20 21:37:43 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 1.9690
2022-08-20 21:38:17 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 1.8944
2022-08-20 21:38:51 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.7942
2022-08-20 21:39:25 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.9136
2022-08-20 21:39:59 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 1.9229
2022-08-20 21:40:33 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 1.8351
2022-08-20 21:41:07 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 1.8354
2022-08-20 21:41:41 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.9666
2022-08-20 21:42:15 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.9616
2022-08-20 21:42:49 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8034
2022-08-20 21:43:23 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.9373
2022-08-20 21:43:57 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 1.9676
2022-08-20 21:44:31 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 1.8854
2022-08-20 21:45:05 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.9394
2022-08-20 21:45:39 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 2.2063
2022-08-20 21:46:13 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 1.9336
2022-08-20 21:46:47 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.7551
2022-08-20 21:47:21 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 1.7792
2022-08-20 21:47:55 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 1.9388
2022-08-20 21:48:29 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.8541
2022-08-20 21:49:03 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.8889
2022-08-20 21:49:37 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 2.0658
2022-08-20 21:50:11 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.9785
2022-08-20 21:50:44 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.9531
2022-08-20 21:51:19 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 1.7676
2022-08-20 21:51:53 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 1.9676
2022-08-20 21:52:26 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.8119
2022-08-20 21:53:00 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.7843
2022-08-20 21:53:34 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 1.8907
2022-08-20 21:54:08 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.7485
2022-08-20 21:54:42 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.7581
2022-08-20 21:55:17 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.7848
2022-08-20 21:55:51 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.8532
2022-08-20 21:56:24 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.9917
2022-08-20 21:56:25 - train: epoch 014, train_loss: 1.8844
2022-08-20 21:57:43 - eval: epoch: 014, acc1: 61.390%, acc5: 84.406%, test_loss: 1.6185, per_image_load_time: 0.628ms, per_image_inference_time: 0.586ms
2022-08-20 21:57:43 - until epoch: 014, best_acc1: 61.390%
2022-08-20 21:57:43 - epoch 015 lr: 0.040630
2022-08-20 21:58:23 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.6310
2022-08-20 21:58:57 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 1.8384
2022-08-20 21:59:30 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.0527
2022-08-20 22:00:04 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 2.0479
2022-08-20 22:00:38 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.7235
2022-08-20 22:01:11 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 2.0056
2022-08-20 22:01:45 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 1.9176
2022-08-20 22:02:19 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.9706
2022-08-20 22:02:53 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.9826
2022-08-20 22:03:26 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.7712
2022-08-20 22:04:00 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7059
2022-08-20 22:04:34 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.8106
2022-08-20 22:05:08 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 2.0448
2022-08-20 22:05:42 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.7141
2022-08-20 22:06:16 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.7690
2022-08-20 22:06:51 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.8771
2022-08-20 22:07:25 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.7619
2022-08-20 22:07:59 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.8075
2022-08-20 22:08:33 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 1.8646
2022-08-20 22:09:07 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.7243
2022-08-20 22:09:41 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.7372
2022-08-20 22:10:14 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 1.8503
2022-08-20 22:10:48 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.6798
2022-08-20 22:11:22 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 1.7377
2022-08-20 22:11:57 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 2.0566
2022-08-20 22:12:31 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.5997
2022-08-20 22:13:05 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 1.8925
2022-08-20 22:13:39 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.8793
2022-08-20 22:14:13 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.9108
2022-08-20 22:14:47 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.6542
2022-08-20 22:15:21 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.6313
2022-08-20 22:15:55 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 1.8510
2022-08-20 22:16:29 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.6951
2022-08-20 22:17:03 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.6865
2022-08-20 22:17:37 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.0365
2022-08-20 22:18:11 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.6564
2022-08-20 22:18:45 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.7040
2022-08-20 22:19:19 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.7560
2022-08-20 22:19:53 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8829
2022-08-20 22:20:27 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.8785
2022-08-20 22:21:01 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.6220
2022-08-20 22:21:35 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6104
2022-08-20 22:22:09 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.6529
2022-08-20 22:22:43 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.7903
2022-08-20 22:23:18 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.7842
2022-08-20 22:23:52 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 2.0006
2022-08-20 22:24:26 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.8197
2022-08-20 22:25:00 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.7286
2022-08-20 22:25:34 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 1.6850
2022-08-20 22:26:07 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 1.9308
2022-08-20 22:26:09 - train: epoch 015, train_loss: 1.8226
2022-08-20 22:27:26 - eval: epoch: 015, acc1: 62.994%, acc5: 85.434%, test_loss: 1.5365, per_image_load_time: 0.720ms, per_image_inference_time: 0.591ms
2022-08-20 22:27:26 - until epoch: 015, best_acc1: 62.994%
2022-08-20 22:27:26 - epoch 016 lr: 0.034548
2022-08-20 22:28:07 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 1.6675
2022-08-20 22:28:40 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.8869
2022-08-20 22:29:14 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.6939
2022-08-20 22:29:47 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.0048
2022-08-20 22:30:21 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.6226
2022-08-20 22:30:55 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.6613
2022-08-20 22:31:28 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.5613
2022-08-20 22:32:02 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.7752
2022-08-20 22:32:36 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.8403
2022-08-20 22:33:10 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.5929
2022-08-20 22:33:43 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.6719
2022-08-20 22:34:17 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.6944
2022-08-20 22:34:51 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.8377
2022-08-20 22:35:25 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.5879
2022-08-20 22:35:59 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.9128
2022-08-20 22:36:33 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.7978
2022-08-20 22:37:06 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.6141
2022-08-20 22:37:40 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 2.0102
2022-08-20 22:38:14 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.6478
2022-08-20 22:38:48 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.7068
2022-08-20 22:39:22 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.9244
2022-08-20 22:39:56 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.8502
2022-08-20 22:40:30 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.7883
2022-08-20 22:41:04 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.8624
2022-08-20 22:41:38 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.8434
2022-08-20 22:42:12 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 1.8801
2022-08-20 22:42:46 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.7207
2022-08-20 22:43:20 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.7259
2022-08-20 22:43:54 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.9021
2022-08-20 22:44:28 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 1.8284
2022-08-20 22:45:02 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.7285
2022-08-20 22:45:36 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 2.0176
2022-08-20 22:46:10 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9296
2022-08-20 22:46:44 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.7370
2022-08-20 22:47:18 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.7146
2022-08-20 22:47:52 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.5689
2022-08-20 22:48:26 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.7728
2022-08-20 22:49:00 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 2.0708
2022-08-20 22:49:34 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.7478
2022-08-20 22:50:08 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.8335
2022-08-20 22:50:42 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.9804
2022-08-20 22:51:16 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.6791
2022-08-20 22:51:50 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.8103
2022-08-20 22:52:25 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.5815
2022-08-20 22:52:59 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.9101
2022-08-20 22:53:33 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.7154
2022-08-20 22:54:07 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.8931
2022-08-20 22:54:41 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.6666
2022-08-20 22:55:15 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.6196
2022-08-20 22:55:48 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.7747
2022-08-20 22:55:50 - train: epoch 016, train_loss: 1.7566
2022-08-20 22:57:07 - eval: epoch: 016, acc1: 64.046%, acc5: 86.180%, test_loss: 1.4815, per_image_load_time: 0.738ms, per_image_inference_time: 0.615ms
2022-08-20 22:57:07 - until epoch: 016, best_acc1: 64.046%
2022-08-20 22:57:07 - epoch 017 lr: 0.028710
2022-08-20 22:57:47 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.8784
2022-08-20 22:58:21 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.6471
2022-08-20 22:58:55 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 1.9714
2022-08-20 22:59:29 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.4989
2022-08-20 23:00:03 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.8291
2022-08-20 23:00:37 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 1.8971
2022-08-20 23:01:10 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.7198
2022-08-20 23:01:44 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.5890
2022-08-20 23:02:18 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.5387
2022-08-20 23:02:52 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.7399
2022-08-20 23:03:26 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.8570
2022-08-20 23:04:00 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.5674
2022-08-20 23:04:34 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.6942
2022-08-20 23:05:08 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.5757
2022-08-20 23:05:42 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.5514
2022-08-20 23:06:16 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.6350
2022-08-20 23:06:50 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.4616
2022-08-20 23:07:24 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.8607
2022-08-20 23:07:58 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.5810
2022-08-20 23:08:32 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.5159
2022-08-20 23:09:06 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.6639
2022-08-20 23:09:40 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.4212
2022-08-20 23:10:14 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.6257
2022-08-20 23:10:48 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.6962
2022-08-20 23:11:22 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.8117
2022-08-20 23:11:56 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.6957
2022-08-20 23:12:30 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.6737
2022-08-20 23:13:04 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.6800
2022-08-20 23:13:38 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.8150
2022-08-20 23:14:12 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.5563
2022-08-20 23:14:46 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.8338
2022-08-20 23:15:21 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.5515
2022-08-20 23:15:55 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 1.7957
2022-08-20 23:16:29 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.6531
2022-08-20 23:17:04 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.7196
2022-08-20 23:17:37 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.6768
2022-08-20 23:18:12 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.5083
2022-08-20 23:18:46 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.7453
2022-08-20 23:19:20 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.5776
2022-08-20 23:19:54 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.6920
2022-08-20 23:20:28 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.8501
2022-08-20 23:21:03 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.7278
2022-08-20 23:21:37 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.5477
2022-08-20 23:22:11 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.6995
2022-08-20 23:22:45 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.9283
2022-08-20 23:23:19 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.6455
2022-08-20 23:23:53 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.8284
2022-08-20 23:24:27 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.7487
2022-08-20 23:25:02 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.6395
2022-08-20 23:25:35 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.4279
2022-08-20 23:25:36 - train: epoch 017, train_loss: 1.6853
2022-08-20 23:26:53 - eval: epoch: 017, acc1: 65.678%, acc5: 87.102%, test_loss: 1.4102, per_image_load_time: 0.556ms, per_image_inference_time: 0.587ms
2022-08-20 23:26:54 - until epoch: 017, best_acc1: 65.678%
2022-08-20 23:26:54 - epoch 018 lr: 0.023208
2022-08-20 23:27:34 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.5842
2022-08-20 23:28:07 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.8245
2022-08-20 23:28:41 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.6181
2022-08-20 23:29:14 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.8497
2022-08-20 23:29:48 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.5496
2022-08-20 23:30:22 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.8568
2022-08-20 23:30:55 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.5291
2022-08-20 23:31:29 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.8278
2022-08-20 23:32:02 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.5879
2022-08-20 23:32:36 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.6355
2022-08-20 23:33:10 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.7568
2022-08-20 23:33:44 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.5668
2022-08-20 23:34:18 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.8918
2022-08-20 23:34:52 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.7849
2022-08-20 23:35:26 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.8027
2022-08-20 23:36:00 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.3382
2022-08-20 23:36:35 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7847
2022-08-20 23:37:09 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6757
2022-08-20 23:37:43 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.6821
2022-08-20 23:38:17 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.6682
2022-08-20 23:38:51 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.7310
2022-08-20 23:39:25 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.7758
2022-08-20 23:39:59 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.7739
2022-08-20 23:40:33 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.6496
2022-08-20 23:41:07 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.4121
2022-08-20 23:41:40 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.5899
2022-08-20 23:42:14 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.7871
2022-08-20 23:42:48 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5690
2022-08-20 23:43:23 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.6853
2022-08-20 23:43:57 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.5859
2022-08-20 23:44:31 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.9292
2022-08-20 23:45:05 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.6334
2022-08-20 23:45:39 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.4144
2022-08-20 23:46:13 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.6491
2022-08-20 23:46:47 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.6540
2022-08-20 23:47:21 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.7569
2022-08-20 23:47:56 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 1.8025
2022-08-20 23:48:30 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.7051
2022-08-20 23:49:04 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.6874
2022-08-20 23:49:38 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.7285
2022-08-20 23:50:12 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7281
2022-08-20 23:50:46 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.6629
2022-08-20 23:51:20 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.5622
2022-08-20 23:51:55 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.5607
2022-08-20 23:52:29 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.5274
2022-08-20 23:53:03 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.6620
2022-08-20 23:53:38 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.8277
2022-08-20 23:54:12 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.7265
2022-08-20 23:54:46 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.5766
2022-08-20 23:55:19 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.8355
2022-08-20 23:55:21 - train: epoch 018, train_loss: 1.6123
2022-08-20 23:56:37 - eval: epoch: 018, acc1: 66.922%, acc5: 87.698%, test_loss: 1.3603, per_image_load_time: 0.743ms, per_image_inference_time: 0.625ms
2022-08-20 23:56:38 - until epoch: 018, best_acc1: 66.922%
2022-08-20 23:56:38 - epoch 019 lr: 0.018128
2022-08-20 23:57:18 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4202
2022-08-20 23:57:52 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.7606
2022-08-20 23:58:25 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.5984
2022-08-20 23:58:59 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.5309
2022-08-20 23:59:32 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.6310
2022-08-21 00:00:06 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.5381
2022-08-21 00:00:39 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.4631
2022-08-21 00:01:13 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.7633
2022-08-21 00:01:47 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.4787
2022-08-21 00:02:21 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.6513
2022-08-21 00:02:55 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5367
2022-08-21 00:03:29 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.4198
2022-08-21 00:04:03 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.6397
2022-08-21 00:04:37 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.5139
2022-08-21 00:05:11 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.8121
2022-08-21 00:05:45 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.5518
2022-08-21 00:06:19 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.6271
2022-08-21 00:06:53 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.6392
2022-08-21 00:07:27 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 2.0433
2022-08-21 00:08:01 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.4652
2022-08-21 00:08:35 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.4533
2022-08-21 00:09:09 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.5549
2022-08-21 00:09:43 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.5585
2022-08-21 00:10:17 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.6962
2022-08-21 00:10:51 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.6353
2022-08-21 00:11:25 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.5289
2022-08-21 00:12:00 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.2953
2022-08-21 00:12:34 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.5140
2022-08-21 00:13:08 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.7122
2022-08-21 00:13:43 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.7050
2022-08-21 00:14:17 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.5952
2022-08-21 00:14:51 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.3131
2022-08-21 00:15:25 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.6951
2022-08-21 00:16:00 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.5074
2022-08-21 00:16:34 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.5857
2022-08-21 00:17:08 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.3653
2022-08-21 00:17:42 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.5495
2022-08-21 00:18:16 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.6782
2022-08-21 00:18:50 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.5989
2022-08-21 00:19:24 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.4223
2022-08-21 00:19:58 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.7450
2022-08-21 00:20:32 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.4252
2022-08-21 00:21:07 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.3510
2022-08-21 00:21:41 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.6061
2022-08-21 00:22:15 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.8141
2022-08-21 00:22:49 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.5434
2022-08-21 00:23:23 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.4134
2022-08-21 00:23:58 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.5093
2022-08-21 00:24:32 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.4709
2022-08-21 00:25:05 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.5000
2022-08-21 00:25:07 - train: epoch 019, train_loss: 1.5392
2022-08-21 00:26:24 - eval: epoch: 019, acc1: 68.138%, acc5: 88.474%, test_loss: 1.3057, per_image_load_time: 0.635ms, per_image_inference_time: 0.609ms
2022-08-21 00:26:24 - until epoch: 019, best_acc1: 68.138%
2022-08-21 00:26:24 - epoch 020 lr: 0.013551
2022-08-21 00:27:04 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.4265
2022-08-21 00:27:37 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.2661
2022-08-21 00:28:11 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.3614
2022-08-21 00:28:45 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.2732
2022-08-21 00:29:19 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.5382
2022-08-21 00:29:53 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.4536
2022-08-21 00:30:27 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.0769
2022-08-21 00:31:00 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.4949
2022-08-21 00:31:34 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.5406
2022-08-21 00:32:08 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.4688
2022-08-21 00:32:42 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.4076
2022-08-21 00:33:16 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.3772
2022-08-21 00:33:50 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.4658
2022-08-21 00:34:24 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.5766
2022-08-21 00:34:57 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.4316
2022-08-21 00:35:31 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.7937
2022-08-21 00:36:05 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.5211
2022-08-21 00:36:39 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.6370
2022-08-21 00:37:13 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.4619
2022-08-21 00:37:47 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.4338
2022-08-21 00:38:21 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.6897
2022-08-21 00:38:54 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.5065
2022-08-21 00:39:28 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.3763
2022-08-21 00:40:02 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.6958
2022-08-21 00:40:36 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.4623
2022-08-21 00:41:10 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.4501
2022-08-21 00:41:44 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.4789
2022-08-21 00:42:18 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.5027
2022-08-21 00:42:52 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.6045
2022-08-21 00:43:26 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.5756
2022-08-21 00:44:00 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.5176
2022-08-21 00:44:34 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.5792
2022-08-21 00:45:08 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.2799
2022-08-21 00:45:42 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.4139
2022-08-21 00:46:16 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.4487
2022-08-21 00:46:50 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.4425
2022-08-21 00:47:24 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.4589
2022-08-21 00:47:58 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.4578
2022-08-21 00:48:33 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.5917
2022-08-21 00:49:06 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.3248
2022-08-21 00:49:40 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.4749
2022-08-21 00:50:15 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.5128
2022-08-21 00:50:48 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.4426
2022-08-21 00:51:22 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.5232
2022-08-21 00:51:56 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.4414
2022-08-21 00:52:30 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.5555
2022-08-21 00:53:04 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.2307
2022-08-21 00:53:38 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4375
2022-08-21 00:54:12 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.4690
2022-08-21 00:54:46 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.4240
2022-08-21 00:54:47 - train: epoch 020, train_loss: 1.4607
2022-08-21 00:56:04 - eval: epoch: 020, acc1: 69.698%, acc5: 89.440%, test_loss: 1.2333, per_image_load_time: 0.698ms, per_image_inference_time: 0.589ms
2022-08-21 00:56:04 - until epoch: 020, best_acc1: 69.698%
2022-08-21 00:56:04 - epoch 021 lr: 0.009548
2022-08-21 00:56:45 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.4422
2022-08-21 00:57:18 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.3611
2022-08-21 00:57:51 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.2049
2022-08-21 00:58:25 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.4799
2022-08-21 00:58:59 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.2357
2022-08-21 00:59:32 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.3930
2022-08-21 01:00:06 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.2064
2022-08-21 01:00:40 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.4660
2022-08-21 01:01:14 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.3883
2022-08-21 01:01:48 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.3463
2022-08-21 01:02:22 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.2160
2022-08-21 01:02:55 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.4217
2022-08-21 01:03:29 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.2390
2022-08-21 01:04:03 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.2628
2022-08-21 01:04:37 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.1978
2022-08-21 01:05:11 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3328
2022-08-21 01:05:45 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.4175
2022-08-21 01:06:19 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.2680
2022-08-21 01:06:53 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.6766
2022-08-21 01:07:27 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.5355
2022-08-21 01:08:01 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3645
2022-08-21 01:08:35 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.3763
2022-08-21 01:09:09 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.4856
2022-08-21 01:09:43 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.4271
2022-08-21 01:10:17 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.5471
2022-08-21 01:10:52 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.4415
2022-08-21 01:11:26 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.4112
2022-08-21 01:12:00 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.3994
2022-08-21 01:12:34 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.2601
2022-08-21 01:13:08 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.4171
2022-08-21 01:13:42 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.4956
2022-08-21 01:14:16 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.2336
2022-08-21 01:14:50 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.7001
2022-08-21 01:15:23 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.4837
2022-08-21 01:15:57 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.3297
2022-08-21 01:16:31 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.3073
2022-08-21 01:17:05 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.2307
2022-08-21 01:17:39 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.4855
2022-08-21 01:18:14 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.2080
2022-08-21 01:18:48 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.5267
2022-08-21 01:19:22 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.3195
2022-08-21 01:19:56 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.5291
2022-08-21 01:20:30 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3481
2022-08-21 01:21:04 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.4793
2022-08-21 01:21:38 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.2926
2022-08-21 01:22:12 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.4014
2022-08-21 01:22:46 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.4710
2022-08-21 01:23:21 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.4461
2022-08-21 01:23:55 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.1391
2022-08-21 01:24:28 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.2911
2022-08-21 01:24:30 - train: epoch 021, train_loss: 1.3873
2022-08-21 01:25:48 - eval: epoch: 021, acc1: 70.656%, acc5: 89.916%, test_loss: 1.1829, per_image_load_time: 2.138ms, per_image_inference_time: 0.652ms
2022-08-21 01:25:48 - until epoch: 021, best_acc1: 70.656%
2022-08-21 01:25:48 - epoch 022 lr: 0.006184
2022-08-21 01:26:29 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.0625
2022-08-21 01:27:02 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.2929
2022-08-21 01:27:36 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.2371
2022-08-21 01:28:09 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.3338
2022-08-21 01:28:43 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.4590
2022-08-21 01:29:16 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.3146
2022-08-21 01:29:50 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.3533
2022-08-21 01:30:24 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.4164
2022-08-21 01:30:58 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.4607
2022-08-21 01:31:32 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.4150
2022-08-21 01:32:06 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.3192
2022-08-21 01:32:40 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.0922
2022-08-21 01:33:14 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.1719
2022-08-21 01:33:48 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.1731
2022-08-21 01:34:22 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.4136
2022-08-21 01:34:56 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.1006
2022-08-21 01:35:30 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.3618
2022-08-21 01:36:04 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.3767
2022-08-21 01:36:38 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.2563
2022-08-21 01:37:12 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.3952
2022-08-21 01:37:46 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.3072
2022-08-21 01:38:20 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.2751
2022-08-21 01:38:54 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.3156
2022-08-21 01:39:28 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.5282
2022-08-21 01:40:02 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.3355
2022-08-21 01:40:36 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.1553
2022-08-21 01:41:10 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.2546
2022-08-21 01:41:44 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.2988
2022-08-21 01:42:18 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.1824
2022-08-21 01:42:52 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.3906
2022-08-21 01:43:26 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.4339
2022-08-21 01:44:00 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4489
2022-08-21 01:44:34 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.3348
2022-08-21 01:45:08 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.2404
2022-08-21 01:45:42 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.3066
2022-08-21 01:46:16 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.4975
2022-08-21 01:46:50 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.3060
2022-08-21 01:47:24 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.4173
2022-08-21 01:47:59 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.2741
2022-08-21 01:48:33 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.3322
2022-08-21 01:49:07 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.2390
2022-08-21 01:49:41 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.2459
2022-08-21 01:50:15 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4278
2022-08-21 01:50:49 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.1145
2022-08-21 01:51:23 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.2312
2022-08-21 01:51:56 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.5368
2022-08-21 01:52:30 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.2673
2022-08-21 01:53:04 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.0420
2022-08-21 01:53:38 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.2528
2022-08-21 01:54:12 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.2124
2022-08-21 01:54:13 - train: epoch 022, train_loss: 1.3195
2022-08-21 01:55:31 - eval: epoch: 022, acc1: 71.718%, acc5: 90.452%, test_loss: 1.1398, per_image_load_time: 0.986ms, per_image_inference_time: 0.638ms
2022-08-21 01:55:31 - until epoch: 022, best_acc1: 71.718%
2022-08-21 01:55:31 - epoch 023 lr: 0.003511
2022-08-21 01:56:13 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.1445
2022-08-21 01:56:45 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.1687
2022-08-21 01:57:18 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.3527
2022-08-21 01:57:51 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.1996
2022-08-21 01:58:24 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.2449
2022-08-21 01:58:58 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.2790
2022-08-21 01:59:31 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.1563
2022-08-21 02:00:05 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3304
2022-08-21 02:00:38 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.3401
2022-08-21 02:01:12 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.2473
2022-08-21 02:01:46 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.3424
2022-08-21 02:02:19 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.1654
2022-08-21 02:02:53 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.1568
2022-08-21 02:03:27 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.1518
2022-08-21 02:04:00 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.1445
2022-08-21 02:04:34 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.2036
2022-08-21 02:05:08 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.4249
2022-08-21 02:05:41 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.0406
2022-08-21 02:06:15 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.3534
2022-08-21 02:06:49 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.0678
2022-08-21 02:07:23 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.5024
2022-08-21 02:07:57 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.2892
2022-08-21 02:08:31 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 0.9756
2022-08-21 02:09:05 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.3531
2022-08-21 02:09:39 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.1470
2022-08-21 02:10:13 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.2521
2022-08-21 02:10:47 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.1563
2022-08-21 02:11:20 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.2062
2022-08-21 02:11:54 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.1296
2022-08-21 02:12:28 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.2748
2022-08-21 02:13:02 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.4014
2022-08-21 02:13:36 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.3159
2022-08-21 02:14:09 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.2255
2022-08-21 02:14:43 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.2983
2022-08-21 02:15:17 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.3926
2022-08-21 02:15:51 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.3629
2022-08-21 02:16:25 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.1033
2022-08-21 02:16:58 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.3334
2022-08-21 02:17:32 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.4230
2022-08-21 02:18:06 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2571
2022-08-21 02:18:40 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.3161
2022-08-21 02:19:13 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.0260
2022-08-21 02:19:47 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.2198
2022-08-21 02:20:21 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.4064
2022-08-21 02:20:55 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.1626
2022-08-21 02:21:29 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.2969
2022-08-21 02:22:03 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.0102
2022-08-21 02:22:37 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.0554
2022-08-21 02:23:11 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.1421
2022-08-21 02:23:44 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.3574
2022-08-21 02:23:45 - train: epoch 023, train_loss: 1.2652
2022-08-21 02:25:03 - eval: epoch: 023, acc1: 72.510%, acc5: 90.768%, test_loss: 1.1118, per_image_load_time: 2.347ms, per_image_inference_time: 0.612ms
2022-08-21 02:25:03 - until epoch: 023, best_acc1: 72.510%
2022-08-21 02:25:03 - epoch 024 lr: 0.001571
2022-08-21 02:25:44 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.2642
2022-08-21 02:26:18 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.1820
2022-08-21 02:26:51 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.3128
2022-08-21 02:27:24 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.2003
2022-08-21 02:27:58 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.3640
2022-08-21 02:28:31 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.3007
2022-08-21 02:29:05 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.1744
2022-08-21 02:29:38 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.1950
2022-08-21 02:30:12 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.1762
2022-08-21 02:30:45 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.2097
2022-08-21 02:31:19 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.0738
2022-08-21 02:31:52 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.3949
2022-08-21 02:32:26 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.4922
2022-08-21 02:33:00 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.0885
2022-08-21 02:33:33 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.2681
2022-08-21 02:34:07 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.1827
2022-08-21 02:34:41 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 0.9494
2022-08-21 02:35:15 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.5479
2022-08-21 02:35:49 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 0.9655
2022-08-21 02:36:23 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.1687
2022-08-21 02:36:57 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.5026
2022-08-21 02:37:31 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.2185
2022-08-21 02:38:05 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.1823
2022-08-21 02:38:39 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.3036
2022-08-21 02:39:13 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.2520
2022-08-21 02:39:47 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.5594
2022-08-21 02:40:21 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.3675
2022-08-21 02:40:55 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.3462
2022-08-21 02:41:29 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.2932
2022-08-21 02:42:03 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.0324
2022-08-21 02:42:37 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.1282
2022-08-21 02:43:10 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.1602
2022-08-21 02:43:44 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.0986
2022-08-21 02:44:18 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.0378
2022-08-21 02:44:52 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.1770
2022-08-21 02:45:26 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.3033
2022-08-21 02:46:00 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.2488
2022-08-21 02:46:34 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.2421
2022-08-21 02:47:08 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.3762
2022-08-21 02:47:42 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.0670
2022-08-21 02:48:15 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.2047
2022-08-21 02:48:50 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.0183
2022-08-21 02:49:23 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.0897
2022-08-21 02:49:57 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.2641
2022-08-21 02:50:32 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.1153
2022-08-21 02:51:06 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.2561
2022-08-21 02:51:40 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.2799
2022-08-21 02:52:13 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.0315
2022-08-21 02:52:47 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.2424
2022-08-21 02:53:20 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.1361
2022-08-21 02:53:22 - train: epoch 024, train_loss: 1.2212
2022-08-21 02:54:40 - eval: epoch: 024, acc1: 72.812%, acc5: 91.024%, test_loss: 1.0935, per_image_load_time: 2.419ms, per_image_inference_time: 0.632ms
2022-08-21 02:54:40 - until epoch: 024, best_acc1: 72.812%
2022-08-21 02:54:40 - epoch 025 lr: 0.000394
2022-08-21 02:55:22 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.0866
2022-08-21 02:55:55 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.1540
2022-08-21 02:56:28 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.1822
2022-08-21 02:57:02 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.1891
2022-08-21 02:57:35 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 1.0090
2022-08-21 02:58:08 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3947
2022-08-21 02:58:42 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.0470
2022-08-21 02:59:15 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.2201
2022-08-21 02:59:49 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0205
2022-08-21 03:00:23 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.2282
2022-08-21 03:00:57 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.2476
2022-08-21 03:01:30 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.2216
2022-08-21 03:02:04 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.3165
2022-08-21 03:02:37 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.2683
2022-08-21 03:03:11 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.2818
2022-08-21 03:03:45 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1171
2022-08-21 03:04:18 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.2866
2022-08-21 03:04:52 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 0.9754
2022-08-21 03:05:26 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.2616
2022-08-21 03:06:00 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.2058
2022-08-21 03:06:34 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.0293
2022-08-21 03:07:08 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 0.8855
2022-08-21 03:07:42 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.3412
2022-08-21 03:08:16 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.1078
2022-08-21 03:08:50 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.1778
2022-08-21 03:09:24 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.1971
2022-08-21 03:09:58 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.1690
2022-08-21 03:10:32 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.2972
2022-08-21 03:11:06 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.3413
2022-08-21 03:11:40 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.2732
2022-08-21 03:12:13 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.1943
2022-08-21 03:12:47 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.2423
2022-08-21 03:13:21 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.0460
2022-08-21 03:13:55 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.2440
2022-08-21 03:14:29 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.0367
2022-08-21 03:15:03 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.2674
2022-08-21 03:15:37 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.1641
2022-08-21 03:16:11 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.2997
2022-08-21 03:16:45 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.3454
2022-08-21 03:17:20 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.1988
2022-08-21 03:17:54 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.5020
2022-08-21 03:18:27 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.2479
2022-08-21 03:19:01 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.0574
2022-08-21 03:19:35 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 0.9699
2022-08-21 03:20:09 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.1102
2022-08-21 03:20:43 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.1098
2022-08-21 03:21:17 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.0737
2022-08-21 03:21:51 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.0460
2022-08-21 03:22:25 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.1007
2022-08-21 03:22:58 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.2363
2022-08-21 03:23:00 - train: epoch 025, train_loss: 1.2040
2022-08-21 03:24:17 - eval: epoch: 025, acc1: 72.850%, acc5: 91.080%, test_loss: 1.0908, per_image_load_time: 1.963ms, per_image_inference_time: 0.611ms
2022-08-21 03:24:18 - until epoch: 025, best_acc1: 72.850%
2022-08-21 03:24:18 - train done. train time: 12.384 hours, best_acc1: 72.850%
