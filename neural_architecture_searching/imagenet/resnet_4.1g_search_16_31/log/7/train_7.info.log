2022-08-18 12:57:56 - net_idx: 7
2022-08-18 12:57:56 - net_config: {'stem_width': 64, 'depth': 13, 'w_0': 48, 'w_a': 21.446058932628446, 'w_m': 1.6975436606023493}
2022-08-18 12:57:56 - num_classes: 1000
2022-08-18 12:57:56 - input_image_size: 224
2022-08-18 12:57:56 - scale: 1.1428571428571428
2022-08-18 12:57:56 - seed: 0
2022-08-18 12:57:56 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-18 12:57:56 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-18 12:57:56 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c2100>
2022-08-18 12:57:56 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4f201c23d0>
2022-08-18 12:57:56 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2400>
2022-08-18 12:57:56 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4f201c2460>
2022-08-18 12:57:56 - batch_size: 256
2022-08-18 12:57:56 - num_workers: 16
2022-08-18 12:57:56 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-18 12:57:56 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-18 12:57:56 - epochs: 25
2022-08-18 12:57:56 - print_interval: 100
2022-08-18 12:57:56 - accumulation_steps: 1
2022-08-18 12:57:56 - sync_bn: False
2022-08-18 12:57:56 - apex: True
2022-08-18 12:57:56 - use_ema_model: False
2022-08-18 12:57:56 - ema_model_decay: 0.9999
2022-08-18 12:57:56 - log_dir: ./log
2022-08-18 12:57:56 - checkpoint_dir: ./checkpoints
2022-08-18 12:57:56 - gpus_type: NVIDIA RTX A5000
2022-08-18 12:57:56 - gpus_num: 2
2022-08-18 12:57:56 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f4efca864b0>
2022-08-18 12:57:56 - ema_model: None
2022-08-18 12:57:56 - --------------------parameters--------------------
2022-08-18 12:57:56 - name: conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-18 12:57:56 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-18 12:57:56 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-18 12:57:56 - name: fc.weight, grad: True
2022-08-18 12:57:56 - name: fc.bias, grad: True
2022-08-18 12:57:56 - --------------------buffers--------------------
2022-08-18 12:57:56 - name: conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-18 12:57:56 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-18 12:57:56 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-18 12:57:56 - -----------no weight decay layers--------------
2022-08-18 12:57:56 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-18 12:57:56 - -------------weight decay layers---------------
2022-08-18 12:57:56 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-18 12:57:56 - epoch 001 lr: 0.100000
2022-08-18 12:58:38 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9068
2022-08-18 12:59:13 - train: epoch 0001, iter [00200, 05004], lr: 0.099999, loss: 6.8954
2022-08-18 12:59:47 - train: epoch 0001, iter [00300, 05004], lr: 0.099999, loss: 6.8376
2022-08-18 13:00:21 - train: epoch 0001, iter [00400, 05004], lr: 0.099997, loss: 6.8082
2022-08-18 13:00:56 - train: epoch 0001, iter [00500, 05004], lr: 0.099996, loss: 6.7137
2022-08-18 13:01:31 - train: epoch 0001, iter [00600, 05004], lr: 0.099994, loss: 6.5441
2022-08-18 13:02:05 - train: epoch 0001, iter [00700, 05004], lr: 0.099992, loss: 6.6481
2022-08-18 13:02:40 - train: epoch 0001, iter [00800, 05004], lr: 0.099990, loss: 6.4618
2022-08-18 13:03:15 - train: epoch 0001, iter [00900, 05004], lr: 0.099987, loss: 6.3958
2022-08-18 13:03:49 - train: epoch 0001, iter [01000, 05004], lr: 0.099984, loss: 6.3715
2022-08-18 13:04:24 - train: epoch 0001, iter [01100, 05004], lr: 0.099981, loss: 6.3059
2022-08-18 13:04:58 - train: epoch 0001, iter [01200, 05004], lr: 0.099977, loss: 6.2039
2022-08-18 13:05:33 - train: epoch 0001, iter [01300, 05004], lr: 0.099973, loss: 6.2048
2022-08-18 13:06:08 - train: epoch 0001, iter [01400, 05004], lr: 0.099969, loss: 6.0667
2022-08-18 13:06:43 - train: epoch 0001, iter [01500, 05004], lr: 0.099965, loss: 5.8734
2022-08-18 13:07:17 - train: epoch 0001, iter [01600, 05004], lr: 0.099960, loss: 5.9635
2022-08-18 13:07:52 - train: epoch 0001, iter [01700, 05004], lr: 0.099954, loss: 5.7251
2022-08-18 13:08:27 - train: epoch 0001, iter [01800, 05004], lr: 0.099949, loss: 5.6567
2022-08-18 13:09:01 - train: epoch 0001, iter [01900, 05004], lr: 0.099943, loss: 5.6323
2022-08-18 13:09:36 - train: epoch 0001, iter [02000, 05004], lr: 0.099937, loss: 5.5014
2022-08-18 13:10:10 - train: epoch 0001, iter [02100, 05004], lr: 0.099930, loss: 5.4377
2022-08-18 13:10:45 - train: epoch 0001, iter [02200, 05004], lr: 0.099924, loss: 5.5060
2022-08-18 13:11:20 - train: epoch 0001, iter [02300, 05004], lr: 0.099917, loss: 5.2359
2022-08-18 13:11:55 - train: epoch 0001, iter [02400, 05004], lr: 0.099909, loss: 5.2637
2022-08-18 13:12:30 - train: epoch 0001, iter [02500, 05004], lr: 0.099901, loss: 5.2745
2022-08-18 13:13:04 - train: epoch 0001, iter [02600, 05004], lr: 0.099893, loss: 5.4009
2022-08-18 13:13:39 - train: epoch 0001, iter [02700, 05004], lr: 0.099885, loss: 5.1414
2022-08-18 13:14:14 - train: epoch 0001, iter [02800, 05004], lr: 0.099876, loss: 5.1575
2022-08-18 13:14:49 - train: epoch 0001, iter [02900, 05004], lr: 0.099867, loss: 4.9839
2022-08-18 13:15:24 - train: epoch 0001, iter [03000, 05004], lr: 0.099858, loss: 5.1086
2022-08-18 13:15:59 - train: epoch 0001, iter [03100, 05004], lr: 0.099849, loss: 5.1668
2022-08-18 13:16:34 - train: epoch 0001, iter [03200, 05004], lr: 0.099839, loss: 4.9689
2022-08-18 13:17:10 - train: epoch 0001, iter [03300, 05004], lr: 0.099828, loss: 4.9185
2022-08-18 13:17:44 - train: epoch 0001, iter [03400, 05004], lr: 0.099818, loss: 4.9112
2022-08-18 13:18:19 - train: epoch 0001, iter [03500, 05004], lr: 0.099807, loss: 4.6759
2022-08-18 13:18:54 - train: epoch 0001, iter [03600, 05004], lr: 0.099796, loss: 4.5867
2022-08-18 13:19:29 - train: epoch 0001, iter [03700, 05004], lr: 0.099784, loss: 4.9220
2022-08-18 13:20:04 - train: epoch 0001, iter [03800, 05004], lr: 0.099773, loss: 4.5217
2022-08-18 13:20:38 - train: epoch 0001, iter [03900, 05004], lr: 0.099760, loss: 4.5749
2022-08-18 13:21:13 - train: epoch 0001, iter [04000, 05004], lr: 0.099748, loss: 4.6316
2022-08-18 13:21:48 - train: epoch 0001, iter [04100, 05004], lr: 0.099735, loss: 4.7434
2022-08-18 13:22:23 - train: epoch 0001, iter [04200, 05004], lr: 0.099722, loss: 4.3892
2022-08-18 13:22:58 - train: epoch 0001, iter [04300, 05004], lr: 0.099709, loss: 4.5720
2022-08-18 13:23:33 - train: epoch 0001, iter [04400, 05004], lr: 0.099695, loss: 4.2747
2022-08-18 13:24:09 - train: epoch 0001, iter [04500, 05004], lr: 0.099681, loss: 4.3665
2022-08-18 13:24:44 - train: epoch 0001, iter [04600, 05004], lr: 0.099667, loss: 4.5330
2022-08-18 13:25:19 - train: epoch 0001, iter [04700, 05004], lr: 0.099652, loss: 4.3585
2022-08-18 13:25:54 - train: epoch 0001, iter [04800, 05004], lr: 0.099637, loss: 4.4204
2022-08-18 13:26:29 - train: epoch 0001, iter [04900, 05004], lr: 0.099622, loss: 4.3938
2022-08-18 13:27:03 - train: epoch 0001, iter [05000, 05004], lr: 0.099606, loss: 4.1916
2022-08-18 13:27:04 - train: epoch 001, train_loss: 5.3984
2022-08-18 13:28:22 - eval: epoch: 001, acc1: 17.896%, acc5: 39.170%, test_loss: 4.2515, per_image_load_time: 1.661ms, per_image_inference_time: 0.570ms
2022-08-18 13:28:22 - until epoch: 001, best_acc1: 17.896%
2022-08-18 13:28:22 - epoch 002 lr: 0.099606
2022-08-18 13:29:02 - train: epoch 0002, iter [00100, 05004], lr: 0.099590, loss: 4.0161
2022-08-18 13:29:38 - train: epoch 0002, iter [00200, 05004], lr: 0.099574, loss: 3.9800
2022-08-18 13:30:12 - train: epoch 0002, iter [00300, 05004], lr: 0.099557, loss: 4.2106
2022-08-18 13:30:46 - train: epoch 0002, iter [00400, 05004], lr: 0.099540, loss: 4.3110
2022-08-18 13:31:20 - train: epoch 0002, iter [00500, 05004], lr: 0.099523, loss: 4.0115
2022-08-18 13:31:55 - train: epoch 0002, iter [00600, 05004], lr: 0.099506, loss: 3.8187
2022-08-18 13:32:29 - train: epoch 0002, iter [00700, 05004], lr: 0.099488, loss: 4.1664
2022-08-18 13:33:04 - train: epoch 0002, iter [00800, 05004], lr: 0.099470, loss: 3.9379
2022-08-18 13:33:38 - train: epoch 0002, iter [00900, 05004], lr: 0.099451, loss: 3.9231
2022-08-18 13:34:13 - train: epoch 0002, iter [01000, 05004], lr: 0.099433, loss: 4.0564
2022-08-18 13:34:48 - train: epoch 0002, iter [01100, 05004], lr: 0.099414, loss: 4.0465
2022-08-18 13:35:22 - train: epoch 0002, iter [01200, 05004], lr: 0.099394, loss: 3.9994
2022-08-18 13:35:57 - train: epoch 0002, iter [01300, 05004], lr: 0.099375, loss: 3.8804
2022-08-18 13:36:31 - train: epoch 0002, iter [01400, 05004], lr: 0.099355, loss: 3.8853
2022-08-18 13:37:06 - train: epoch 0002, iter [01500, 05004], lr: 0.099335, loss: 3.9717
2022-08-18 13:37:41 - train: epoch 0002, iter [01600, 05004], lr: 0.099314, loss: 3.8435
2022-08-18 13:38:15 - train: epoch 0002, iter [01700, 05004], lr: 0.099293, loss: 3.9200
2022-08-18 13:38:50 - train: epoch 0002, iter [01800, 05004], lr: 0.099272, loss: 3.8760
2022-08-18 13:39:25 - train: epoch 0002, iter [01900, 05004], lr: 0.099250, loss: 3.5897
2022-08-18 13:40:00 - train: epoch 0002, iter [02000, 05004], lr: 0.099229, loss: 3.4580
2022-08-18 13:40:34 - train: epoch 0002, iter [02100, 05004], lr: 0.099206, loss: 3.7503
2022-08-18 13:41:09 - train: epoch 0002, iter [02200, 05004], lr: 0.099184, loss: 3.5932
2022-08-18 13:41:44 - train: epoch 0002, iter [02300, 05004], lr: 0.099161, loss: 3.9187
2022-08-18 13:42:19 - train: epoch 0002, iter [02400, 05004], lr: 0.099138, loss: 3.5809
2022-08-18 13:42:53 - train: epoch 0002, iter [02500, 05004], lr: 0.099115, loss: 3.4946
2022-08-18 13:43:28 - train: epoch 0002, iter [02600, 05004], lr: 0.099091, loss: 3.7720
2022-08-18 13:44:02 - train: epoch 0002, iter [02700, 05004], lr: 0.099067, loss: 3.7958
2022-08-18 13:44:37 - train: epoch 0002, iter [02800, 05004], lr: 0.099043, loss: 3.7142
2022-08-18 13:45:12 - train: epoch 0002, iter [02900, 05004], lr: 0.099018, loss: 3.6476
2022-08-18 13:45:47 - train: epoch 0002, iter [03000, 05004], lr: 0.098993, loss: 3.3795
2022-08-18 13:46:21 - train: epoch 0002, iter [03100, 05004], lr: 0.098968, loss: 3.5348
2022-08-18 13:46:56 - train: epoch 0002, iter [03200, 05004], lr: 0.098943, loss: 3.6158
2022-08-18 13:47:31 - train: epoch 0002, iter [03300, 05004], lr: 0.098917, loss: 3.7123
2022-08-18 13:48:05 - train: epoch 0002, iter [03400, 05004], lr: 0.098891, loss: 3.3617
2022-08-18 13:48:40 - train: epoch 0002, iter [03500, 05004], lr: 0.098864, loss: 3.3359
2022-08-18 13:49:14 - train: epoch 0002, iter [03600, 05004], lr: 0.098837, loss: 3.6108
2022-08-18 13:49:49 - train: epoch 0002, iter [03700, 05004], lr: 0.098810, loss: 3.5036
2022-08-18 13:50:24 - train: epoch 0002, iter [03800, 05004], lr: 0.098783, loss: 3.2483
2022-08-18 13:50:59 - train: epoch 0002, iter [03900, 05004], lr: 0.098755, loss: 3.6017
2022-08-18 13:51:33 - train: epoch 0002, iter [04000, 05004], lr: 0.098727, loss: 3.3698
2022-08-18 13:52:08 - train: epoch 0002, iter [04100, 05004], lr: 0.098699, loss: 3.5694
2022-08-18 13:52:43 - train: epoch 0002, iter [04200, 05004], lr: 0.098670, loss: 3.3379
2022-08-18 13:53:18 - train: epoch 0002, iter [04300, 05004], lr: 0.098641, loss: 3.3112
2022-08-18 13:53:53 - train: epoch 0002, iter [04400, 05004], lr: 0.098612, loss: 3.3542
2022-08-18 13:54:27 - train: epoch 0002, iter [04500, 05004], lr: 0.098583, loss: 3.1372
2022-08-18 13:55:02 - train: epoch 0002, iter [04600, 05004], lr: 0.098553, loss: 3.1336
2022-08-18 13:55:37 - train: epoch 0002, iter [04700, 05004], lr: 0.098523, loss: 3.3238
2022-08-18 13:56:12 - train: epoch 0002, iter [04800, 05004], lr: 0.098492, loss: 3.3699
2022-08-18 13:56:47 - train: epoch 0002, iter [04900, 05004], lr: 0.098461, loss: 3.2076
2022-08-18 13:57:21 - train: epoch 0002, iter [05000, 05004], lr: 0.098430, loss: 3.2499
2022-08-18 13:57:22 - train: epoch 002, train_loss: 3.6717
2022-08-18 13:58:39 - eval: epoch: 002, acc1: 31.970%, acc5: 58.400%, test_loss: 3.1995, per_image_load_time: 2.379ms, per_image_inference_time: 0.564ms
2022-08-18 13:58:39 - until epoch: 002, best_acc1: 31.970%
2022-08-18 13:58:39 - epoch 003 lr: 0.098429
2022-08-18 13:59:21 - train: epoch 0003, iter [00100, 05004], lr: 0.098398, loss: 3.3867
2022-08-18 13:59:55 - train: epoch 0003, iter [00200, 05004], lr: 0.098366, loss: 3.1719
2022-08-18 14:00:29 - train: epoch 0003, iter [00300, 05004], lr: 0.098334, loss: 3.3468
2022-08-18 14:01:03 - train: epoch 0003, iter [00400, 05004], lr: 0.098302, loss: 3.3314
2022-08-18 14:01:37 - train: epoch 0003, iter [00500, 05004], lr: 0.098269, loss: 3.1581
2022-08-18 14:02:11 - train: epoch 0003, iter [00600, 05004], lr: 0.098236, loss: 3.1572
2022-08-18 14:02:46 - train: epoch 0003, iter [00700, 05004], lr: 0.098203, loss: 3.1866
2022-08-18 14:03:20 - train: epoch 0003, iter [00800, 05004], lr: 0.098170, loss: 3.2878
2022-08-18 14:03:54 - train: epoch 0003, iter [00900, 05004], lr: 0.098136, loss: 3.2712
2022-08-18 14:04:29 - train: epoch 0003, iter [01000, 05004], lr: 0.098102, loss: 3.2185
2022-08-18 14:05:03 - train: epoch 0003, iter [01100, 05004], lr: 0.098067, loss: 3.0587
2022-08-18 14:05:37 - train: epoch 0003, iter [01200, 05004], lr: 0.098033, loss: 3.0759
2022-08-18 14:06:12 - train: epoch 0003, iter [01300, 05004], lr: 0.097997, loss: 3.1216
2022-08-18 14:06:47 - train: epoch 0003, iter [01400, 05004], lr: 0.097962, loss: 3.1069
2022-08-18 14:07:21 - train: epoch 0003, iter [01500, 05004], lr: 0.097927, loss: 3.1822
2022-08-18 14:07:56 - train: epoch 0003, iter [01600, 05004], lr: 0.097891, loss: 2.9731
2022-08-18 14:08:30 - train: epoch 0003, iter [01700, 05004], lr: 0.097854, loss: 3.0029
2022-08-18 14:09:05 - train: epoch 0003, iter [01800, 05004], lr: 0.097818, loss: 3.1004
2022-08-18 14:09:39 - train: epoch 0003, iter [01900, 05004], lr: 0.097781, loss: 3.2370
2022-08-18 14:10:14 - train: epoch 0003, iter [02000, 05004], lr: 0.097744, loss: 3.0129
2022-08-18 14:10:49 - train: epoch 0003, iter [02100, 05004], lr: 0.097706, loss: 3.0376
2022-08-18 14:11:23 - train: epoch 0003, iter [02200, 05004], lr: 0.097669, loss: 3.4676
2022-08-18 14:11:58 - train: epoch 0003, iter [02300, 05004], lr: 0.097631, loss: 2.9199
2022-08-18 14:12:32 - train: epoch 0003, iter [02400, 05004], lr: 0.097592, loss: 3.0629
2022-08-18 14:13:07 - train: epoch 0003, iter [02500, 05004], lr: 0.097554, loss: 3.0530
2022-08-18 14:13:41 - train: epoch 0003, iter [02600, 05004], lr: 0.097515, loss: 3.0913
2022-08-18 14:14:16 - train: epoch 0003, iter [02700, 05004], lr: 0.097475, loss: 3.3791
2022-08-18 14:14:50 - train: epoch 0003, iter [02800, 05004], lr: 0.097436, loss: 2.9978
2022-08-18 14:15:24 - train: epoch 0003, iter [02900, 05004], lr: 0.097396, loss: 3.0358
2022-08-18 14:15:59 - train: epoch 0003, iter [03000, 05004], lr: 0.097356, loss: 3.2180
2022-08-18 14:16:33 - train: epoch 0003, iter [03100, 05004], lr: 0.097315, loss: 3.3930
2022-08-18 14:17:07 - train: epoch 0003, iter [03200, 05004], lr: 0.097275, loss: 3.0266
2022-08-18 14:17:42 - train: epoch 0003, iter [03300, 05004], lr: 0.097234, loss: 3.0313
2022-08-18 14:18:16 - train: epoch 0003, iter [03400, 05004], lr: 0.097192, loss: 3.3005
2022-08-18 14:18:51 - train: epoch 0003, iter [03500, 05004], lr: 0.097151, loss: 2.8403
2022-08-18 14:19:25 - train: epoch 0003, iter [03600, 05004], lr: 0.097109, loss: 3.0273
2022-08-18 14:20:00 - train: epoch 0003, iter [03700, 05004], lr: 0.097067, loss: 3.1038
2022-08-18 14:20:35 - train: epoch 0003, iter [03800, 05004], lr: 0.097024, loss: 3.1794
2022-08-18 14:21:09 - train: epoch 0003, iter [03900, 05004], lr: 0.096981, loss: 3.3776
2022-08-18 14:21:43 - train: epoch 0003, iter [04000, 05004], lr: 0.096938, loss: 2.5740
2022-08-18 14:22:17 - train: epoch 0003, iter [04100, 05004], lr: 0.096895, loss: 3.2832
2022-08-18 14:22:52 - train: epoch 0003, iter [04200, 05004], lr: 0.096851, loss: 3.0873
2022-08-18 14:23:26 - train: epoch 0003, iter [04300, 05004], lr: 0.096807, loss: 2.9101
2022-08-18 14:24:00 - train: epoch 0003, iter [04400, 05004], lr: 0.096763, loss: 3.0244
2022-08-18 14:24:35 - train: epoch 0003, iter [04500, 05004], lr: 0.096718, loss: 2.9874
2022-08-18 14:25:09 - train: epoch 0003, iter [04600, 05004], lr: 0.096673, loss: 3.0854
2022-08-18 14:25:44 - train: epoch 0003, iter [04700, 05004], lr: 0.096628, loss: 2.9666
2022-08-18 14:26:18 - train: epoch 0003, iter [04800, 05004], lr: 0.096583, loss: 3.1836
2022-08-18 14:26:52 - train: epoch 0003, iter [04900, 05004], lr: 0.096537, loss: 3.0708
2022-08-18 14:27:25 - train: epoch 0003, iter [05000, 05004], lr: 0.096491, loss: 3.0262
2022-08-18 14:27:26 - train: epoch 003, train_loss: 3.0965
2022-08-18 14:28:44 - eval: epoch: 003, acc1: 37.094%, acc5: 63.354%, test_loss: 2.8941, per_image_load_time: 0.847ms, per_image_inference_time: 0.551ms
2022-08-18 14:28:44 - until epoch: 003, best_acc1: 37.094%
2022-08-18 14:28:44 - epoch 004 lr: 0.096488
2022-08-18 14:29:25 - train: epoch 0004, iter [00100, 05004], lr: 0.096442, loss: 2.9919
2022-08-18 14:29:59 - train: epoch 0004, iter [00200, 05004], lr: 0.096396, loss: 2.7481
2022-08-18 14:30:33 - train: epoch 0004, iter [00300, 05004], lr: 0.096349, loss: 2.8725
2022-08-18 14:31:07 - train: epoch 0004, iter [00400, 05004], lr: 0.096302, loss: 2.6713
2022-08-18 14:31:41 - train: epoch 0004, iter [00500, 05004], lr: 0.096254, loss: 2.7624
2022-08-18 14:32:15 - train: epoch 0004, iter [00600, 05004], lr: 0.096206, loss: 3.0230
2022-08-18 14:32:49 - train: epoch 0004, iter [00700, 05004], lr: 0.096158, loss: 2.9536
2022-08-18 14:33:23 - train: epoch 0004, iter [00800, 05004], lr: 0.096110, loss: 2.9084
2022-08-18 14:33:57 - train: epoch 0004, iter [00900, 05004], lr: 0.096061, loss: 2.5117
2022-08-18 14:34:31 - train: epoch 0004, iter [01000, 05004], lr: 0.096012, loss: 3.0876
2022-08-18 14:35:05 - train: epoch 0004, iter [01100, 05004], lr: 0.095963, loss: 3.2125
2022-08-18 14:35:38 - train: epoch 0004, iter [01200, 05004], lr: 0.095913, loss: 2.6865
2022-08-18 14:36:12 - train: epoch 0004, iter [01300, 05004], lr: 0.095863, loss: 2.8003
2022-08-18 14:36:46 - train: epoch 0004, iter [01400, 05004], lr: 0.095813, loss: 2.9278
2022-08-18 14:37:20 - train: epoch 0004, iter [01500, 05004], lr: 0.095763, loss: 2.9016
2022-08-18 14:37:54 - train: epoch 0004, iter [01600, 05004], lr: 0.095712, loss: 2.8571
2022-08-18 14:38:28 - train: epoch 0004, iter [01700, 05004], lr: 0.095661, loss: 2.8087
2022-08-18 14:39:02 - train: epoch 0004, iter [01800, 05004], lr: 0.095610, loss: 3.0162
2022-08-18 14:39:36 - train: epoch 0004, iter [01900, 05004], lr: 0.095558, loss: 2.9289
2022-08-18 14:40:10 - train: epoch 0004, iter [02000, 05004], lr: 0.095506, loss: 2.7248
2022-08-18 14:40:44 - train: epoch 0004, iter [02100, 05004], lr: 0.095454, loss: 2.8903
2022-08-18 14:41:18 - train: epoch 0004, iter [02200, 05004], lr: 0.095402, loss: 2.7951
2022-08-18 14:41:52 - train: epoch 0004, iter [02300, 05004], lr: 0.095349, loss: 2.5064
2022-08-18 14:42:26 - train: epoch 0004, iter [02400, 05004], lr: 0.095296, loss: 2.7157
2022-08-18 14:43:00 - train: epoch 0004, iter [02500, 05004], lr: 0.095242, loss: 2.9181
2022-08-18 14:43:34 - train: epoch 0004, iter [02600, 05004], lr: 0.095189, loss: 2.5893
2022-08-18 14:44:08 - train: epoch 0004, iter [02700, 05004], lr: 0.095135, loss: 2.8327
2022-08-18 14:44:42 - train: epoch 0004, iter [02800, 05004], lr: 0.095081, loss: 2.8258
2022-08-18 14:45:16 - train: epoch 0004, iter [02900, 05004], lr: 0.095026, loss: 2.8811
2022-08-18 14:45:50 - train: epoch 0004, iter [03000, 05004], lr: 0.094972, loss: 2.9547
2022-08-18 14:46:24 - train: epoch 0004, iter [03100, 05004], lr: 0.094917, loss: 2.6279
2022-08-18 14:46:58 - train: epoch 0004, iter [03200, 05004], lr: 0.094861, loss: 2.9266
2022-08-18 14:47:33 - train: epoch 0004, iter [03300, 05004], lr: 0.094806, loss: 2.6437
2022-08-18 14:48:07 - train: epoch 0004, iter [03400, 05004], lr: 0.094750, loss: 2.8667
2022-08-18 14:48:41 - train: epoch 0004, iter [03500, 05004], lr: 0.094694, loss: 2.8374
2022-08-18 14:49:15 - train: epoch 0004, iter [03600, 05004], lr: 0.094637, loss: 2.7830
2022-08-18 14:49:49 - train: epoch 0004, iter [03700, 05004], lr: 0.094580, loss: 2.7548
2022-08-18 14:50:24 - train: epoch 0004, iter [03800, 05004], lr: 0.094524, loss: 2.5826
2022-08-18 14:50:58 - train: epoch 0004, iter [03900, 05004], lr: 0.094466, loss: 2.6862
2022-08-18 14:51:32 - train: epoch 0004, iter [04000, 05004], lr: 0.094409, loss: 2.6651
2022-08-18 14:52:06 - train: epoch 0004, iter [04100, 05004], lr: 0.094351, loss: 2.7182
2022-08-18 14:52:41 - train: epoch 0004, iter [04200, 05004], lr: 0.094293, loss: 2.6443
2022-08-18 14:53:15 - train: epoch 0004, iter [04300, 05004], lr: 0.094234, loss: 2.5853
2022-08-18 14:53:49 - train: epoch 0004, iter [04400, 05004], lr: 0.094176, loss: 2.7297
2022-08-18 14:54:23 - train: epoch 0004, iter [04500, 05004], lr: 0.094117, loss: 2.3596
2022-08-18 14:54:57 - train: epoch 0004, iter [04600, 05004], lr: 0.094057, loss: 2.8161
2022-08-18 14:55:31 - train: epoch 0004, iter [04700, 05004], lr: 0.093998, loss: 2.6709
2022-08-18 14:56:04 - train: epoch 0004, iter [04800, 05004], lr: 0.093938, loss: 2.4828
2022-08-18 14:56:38 - train: epoch 0004, iter [04900, 05004], lr: 0.093878, loss: 2.6123
2022-08-18 14:57:12 - train: epoch 0004, iter [05000, 05004], lr: 0.093818, loss: 2.7612
2022-08-18 14:57:13 - train: epoch 004, train_loss: 2.8132
2022-08-18 14:58:30 - eval: epoch: 004, acc1: 43.578%, acc5: 70.178%, test_loss: 2.5218, per_image_load_time: 1.684ms, per_image_inference_time: 0.588ms
2022-08-18 14:58:31 - until epoch: 004, best_acc1: 43.578%
2022-08-18 14:58:31 - epoch 005 lr: 0.093815
2022-08-18 14:59:12 - train: epoch 0005, iter [00100, 05004], lr: 0.093755, loss: 2.8420
2022-08-18 14:59:46 - train: epoch 0005, iter [00200, 05004], lr: 0.093694, loss: 2.7543
2022-08-18 15:00:20 - train: epoch 0005, iter [00300, 05004], lr: 0.093633, loss: 2.7119
2022-08-18 15:00:54 - train: epoch 0005, iter [00400, 05004], lr: 0.093571, loss: 2.5049
2022-08-18 15:01:28 - train: epoch 0005, iter [00500, 05004], lr: 0.093509, loss: 2.6543
2022-08-18 15:02:03 - train: epoch 0005, iter [00600, 05004], lr: 0.093447, loss: 2.9126
2022-08-18 15:02:37 - train: epoch 0005, iter [00700, 05004], lr: 0.093385, loss: 2.7349
2022-08-18 15:03:11 - train: epoch 0005, iter [00800, 05004], lr: 0.093323, loss: 2.6861
2022-08-18 15:03:45 - train: epoch 0005, iter [00900, 05004], lr: 0.093260, loss: 2.5857
2022-08-18 15:04:18 - train: epoch 0005, iter [01000, 05004], lr: 0.093197, loss: 2.5612
2022-08-18 15:04:52 - train: epoch 0005, iter [01100, 05004], lr: 0.093133, loss: 2.6067
2022-08-18 15:05:26 - train: epoch 0005, iter [01200, 05004], lr: 0.093070, loss: 2.6920
2022-08-18 15:06:00 - train: epoch 0005, iter [01300, 05004], lr: 0.093006, loss: 2.6389
2022-08-18 15:06:34 - train: epoch 0005, iter [01400, 05004], lr: 0.092942, loss: 2.7029
2022-08-18 15:07:08 - train: epoch 0005, iter [01500, 05004], lr: 0.092877, loss: 2.5102
2022-08-18 15:07:42 - train: epoch 0005, iter [01600, 05004], lr: 0.092812, loss: 2.4699
2022-08-18 15:08:16 - train: epoch 0005, iter [01700, 05004], lr: 0.092747, loss: 2.5913
2022-08-18 15:08:50 - train: epoch 0005, iter [01800, 05004], lr: 0.092682, loss: 2.7901
2022-08-18 15:09:24 - train: epoch 0005, iter [01900, 05004], lr: 0.092617, loss: 2.5333
2022-08-18 15:09:58 - train: epoch 0005, iter [02000, 05004], lr: 0.092551, loss: 2.6500
2022-08-18 15:10:33 - train: epoch 0005, iter [02100, 05004], lr: 0.092485, loss: 2.4703
2022-08-18 15:11:07 - train: epoch 0005, iter [02200, 05004], lr: 0.092418, loss: 2.5344
2022-08-18 15:11:41 - train: epoch 0005, iter [02300, 05004], lr: 0.092352, loss: 2.5245
2022-08-18 15:12:16 - train: epoch 0005, iter [02400, 05004], lr: 0.092285, loss: 2.5915
2022-08-18 15:12:50 - train: epoch 0005, iter [02500, 05004], lr: 0.092218, loss: 2.6764
2022-08-18 15:13:24 - train: epoch 0005, iter [02600, 05004], lr: 0.092150, loss: 2.7722
2022-08-18 15:13:57 - train: epoch 0005, iter [02700, 05004], lr: 0.092083, loss: 2.7275
2022-08-18 15:14:31 - train: epoch 0005, iter [02800, 05004], lr: 0.092015, loss: 2.6033
2022-08-18 15:15:05 - train: epoch 0005, iter [02900, 05004], lr: 0.091947, loss: 2.5177
2022-08-18 15:15:39 - train: epoch 0005, iter [03000, 05004], lr: 0.091878, loss: 2.7366
2022-08-18 15:16:13 - train: epoch 0005, iter [03100, 05004], lr: 0.091809, loss: 2.6305
2022-08-18 15:16:47 - train: epoch 0005, iter [03200, 05004], lr: 0.091740, loss: 2.4167
2022-08-18 15:17:22 - train: epoch 0005, iter [03300, 05004], lr: 0.091671, loss: 2.4532
2022-08-18 15:17:56 - train: epoch 0005, iter [03400, 05004], lr: 0.091602, loss: 2.3941
2022-08-18 15:18:30 - train: epoch 0005, iter [03500, 05004], lr: 0.091532, loss: 2.5574
2022-08-18 15:19:04 - train: epoch 0005, iter [03600, 05004], lr: 0.091462, loss: 2.5607
2022-08-18 15:19:38 - train: epoch 0005, iter [03700, 05004], lr: 0.091391, loss: 2.5757
2022-08-18 15:20:12 - train: epoch 0005, iter [03800, 05004], lr: 0.091321, loss: 2.4839
2022-08-18 15:20:46 - train: epoch 0005, iter [03900, 05004], lr: 0.091250, loss: 2.8761
2022-08-18 15:21:20 - train: epoch 0005, iter [04000, 05004], lr: 0.091179, loss: 2.5036
2022-08-18 15:21:54 - train: epoch 0005, iter [04100, 05004], lr: 0.091108, loss: 2.5125
2022-08-18 15:22:29 - train: epoch 0005, iter [04200, 05004], lr: 0.091036, loss: 2.5947
2022-08-18 15:23:02 - train: epoch 0005, iter [04300, 05004], lr: 0.090964, loss: 2.6937
2022-08-18 15:23:36 - train: epoch 0005, iter [04400, 05004], lr: 0.090892, loss: 2.5472
2022-08-18 15:24:10 - train: epoch 0005, iter [04500, 05004], lr: 0.090820, loss: 2.8137
2022-08-18 15:24:45 - train: epoch 0005, iter [04600, 05004], lr: 0.090747, loss: 2.4609
2022-08-18 15:25:19 - train: epoch 0005, iter [04700, 05004], lr: 0.090674, loss: 2.4355
2022-08-18 15:25:53 - train: epoch 0005, iter [04800, 05004], lr: 0.090601, loss: 2.4670
2022-08-18 15:26:28 - train: epoch 0005, iter [04900, 05004], lr: 0.090527, loss: 2.7653
2022-08-18 15:27:01 - train: epoch 0005, iter [05000, 05004], lr: 0.090454, loss: 2.4436
2022-08-18 15:27:03 - train: epoch 005, train_loss: 2.6386
2022-08-18 15:28:19 - eval: epoch: 005, acc1: 45.290%, acc5: 72.072%, test_loss: 2.4027, per_image_load_time: 1.022ms, per_image_inference_time: 0.596ms
2022-08-18 15:28:19 - until epoch: 005, best_acc1: 45.290%
2022-08-18 15:28:19 - epoch 006 lr: 0.090450
2022-08-18 15:29:00 - train: epoch 0006, iter [00100, 05004], lr: 0.090377, loss: 2.4348
2022-08-18 15:29:34 - train: epoch 0006, iter [00200, 05004], lr: 0.090303, loss: 2.5534
2022-08-18 15:30:08 - train: epoch 0006, iter [00300, 05004], lr: 0.090228, loss: 2.6573
2022-08-18 15:30:42 - train: epoch 0006, iter [00400, 05004], lr: 0.090154, loss: 2.4898
2022-08-18 15:31:16 - train: epoch 0006, iter [00500, 05004], lr: 0.090079, loss: 2.3922
2022-08-18 15:31:50 - train: epoch 0006, iter [00600, 05004], lr: 0.090003, loss: 2.6337
2022-08-18 15:32:25 - train: epoch 0006, iter [00700, 05004], lr: 0.089928, loss: 2.6848
2022-08-18 15:32:58 - train: epoch 0006, iter [00800, 05004], lr: 0.089852, loss: 2.6758
2022-08-18 15:33:32 - train: epoch 0006, iter [00900, 05004], lr: 0.089776, loss: 2.3127
2022-08-18 15:34:06 - train: epoch 0006, iter [01000, 05004], lr: 0.089700, loss: 2.3563
2022-08-18 15:34:40 - train: epoch 0006, iter [01100, 05004], lr: 0.089624, loss: 2.5856
2022-08-18 15:35:15 - train: epoch 0006, iter [01200, 05004], lr: 0.089547, loss: 2.6022
2022-08-18 15:35:48 - train: epoch 0006, iter [01300, 05004], lr: 0.089470, loss: 2.5880
2022-08-18 15:36:23 - train: epoch 0006, iter [01400, 05004], lr: 0.089393, loss: 2.4321
2022-08-18 15:36:57 - train: epoch 0006, iter [01500, 05004], lr: 0.089315, loss: 2.6243
2022-08-18 15:37:31 - train: epoch 0006, iter [01600, 05004], lr: 0.089238, loss: 2.5001
2022-08-18 15:38:05 - train: epoch 0006, iter [01700, 05004], lr: 0.089160, loss: 2.5117
2022-08-18 15:38:40 - train: epoch 0006, iter [01800, 05004], lr: 0.089082, loss: 2.5438
2022-08-18 15:39:14 - train: epoch 0006, iter [01900, 05004], lr: 0.089003, loss: 2.4903
2022-08-18 15:39:48 - train: epoch 0006, iter [02000, 05004], lr: 0.088924, loss: 2.6591
2022-08-18 15:40:22 - train: epoch 0006, iter [02100, 05004], lr: 0.088845, loss: 2.4737
2022-08-18 15:40:56 - train: epoch 0006, iter [02200, 05004], lr: 0.088766, loss: 2.2514
2022-08-18 15:41:30 - train: epoch 0006, iter [02300, 05004], lr: 0.088687, loss: 2.4255
2022-08-18 15:42:04 - train: epoch 0006, iter [02400, 05004], lr: 0.088607, loss: 2.5026
2022-08-18 15:42:37 - train: epoch 0006, iter [02500, 05004], lr: 0.088527, loss: 2.4365
2022-08-18 15:43:11 - train: epoch 0006, iter [02600, 05004], lr: 0.088447, loss: 2.2643
2022-08-18 15:43:46 - train: epoch 0006, iter [02700, 05004], lr: 0.088367, loss: 2.5690
2022-08-18 15:44:20 - train: epoch 0006, iter [02800, 05004], lr: 0.088286, loss: 2.1451
2022-08-18 15:44:54 - train: epoch 0006, iter [02900, 05004], lr: 0.088205, loss: 2.8108
2022-08-18 15:45:28 - train: epoch 0006, iter [03000, 05004], lr: 0.088124, loss: 2.4991
2022-08-18 15:46:02 - train: epoch 0006, iter [03100, 05004], lr: 0.088043, loss: 2.4861
2022-08-18 15:46:36 - train: epoch 0006, iter [03200, 05004], lr: 0.087961, loss: 2.4106
2022-08-18 15:47:11 - train: epoch 0006, iter [03300, 05004], lr: 0.087879, loss: 2.3619
2022-08-18 15:47:45 - train: epoch 0006, iter [03400, 05004], lr: 0.087797, loss: 2.7180
2022-08-18 15:48:19 - train: epoch 0006, iter [03500, 05004], lr: 0.087715, loss: 2.2855
2022-08-18 15:48:53 - train: epoch 0006, iter [03600, 05004], lr: 0.087632, loss: 2.6491
2022-08-18 15:49:27 - train: epoch 0006, iter [03700, 05004], lr: 0.087550, loss: 2.6564
2022-08-18 15:50:02 - train: epoch 0006, iter [03800, 05004], lr: 0.087467, loss: 2.3647
2022-08-18 15:50:36 - train: epoch 0006, iter [03900, 05004], lr: 0.087383, loss: 2.4119
2022-08-18 15:51:10 - train: epoch 0006, iter [04000, 05004], lr: 0.087300, loss: 2.6565
2022-08-18 15:51:44 - train: epoch 0006, iter [04100, 05004], lr: 0.087216, loss: 2.5991
2022-08-18 15:52:17 - train: epoch 0006, iter [04200, 05004], lr: 0.087132, loss: 2.4064
2022-08-18 15:52:51 - train: epoch 0006, iter [04300, 05004], lr: 0.087048, loss: 2.6890
2022-08-18 15:53:26 - train: epoch 0006, iter [04400, 05004], lr: 0.086963, loss: 2.4664
2022-08-18 15:54:00 - train: epoch 0006, iter [04500, 05004], lr: 0.086879, loss: 2.3251
2022-08-18 15:54:34 - train: epoch 0006, iter [04600, 05004], lr: 0.086794, loss: 2.5223
2022-08-18 15:55:09 - train: epoch 0006, iter [04700, 05004], lr: 0.086709, loss: 2.6036
2022-08-18 15:55:43 - train: epoch 0006, iter [04800, 05004], lr: 0.086623, loss: 2.4916
2022-08-18 15:56:17 - train: epoch 0006, iter [04900, 05004], lr: 0.086538, loss: 2.3535
2022-08-18 15:56:50 - train: epoch 0006, iter [05000, 05004], lr: 0.086452, loss: 2.5564
2022-08-18 15:56:52 - train: epoch 006, train_loss: 2.5116
2022-08-18 15:58:08 - eval: epoch: 006, acc1: 48.554%, acc5: 74.590%, test_loss: 2.2620, per_image_load_time: 1.613ms, per_image_inference_time: 0.574ms
2022-08-18 15:58:08 - until epoch: 006, best_acc1: 48.554%
2022-08-18 15:58:08 - epoch 007 lr: 0.086448
2022-08-18 15:58:49 - train: epoch 0007, iter [00100, 05004], lr: 0.086362, loss: 2.4113
2022-08-18 15:59:24 - train: epoch 0007, iter [00200, 05004], lr: 0.086276, loss: 2.5826
2022-08-18 15:59:58 - train: epoch 0007, iter [00300, 05004], lr: 0.086190, loss: 2.7580
2022-08-18 16:00:32 - train: epoch 0007, iter [00400, 05004], lr: 0.086103, loss: 2.4251
2022-08-18 16:01:06 - train: epoch 0007, iter [00500, 05004], lr: 0.086016, loss: 2.3008
2022-08-18 16:01:40 - train: epoch 0007, iter [00600, 05004], lr: 0.085929, loss: 2.3643
2022-08-18 16:02:14 - train: epoch 0007, iter [00700, 05004], lr: 0.085841, loss: 2.1671
2022-08-18 16:02:48 - train: epoch 0007, iter [00800, 05004], lr: 0.085753, loss: 2.5509
2022-08-18 16:03:22 - train: epoch 0007, iter [00900, 05004], lr: 0.085666, loss: 2.4348
2022-08-18 16:03:56 - train: epoch 0007, iter [01000, 05004], lr: 0.085577, loss: 2.4492
2022-08-18 16:04:31 - train: epoch 0007, iter [01100, 05004], lr: 0.085489, loss: 2.4859
2022-08-18 16:05:05 - train: epoch 0007, iter [01200, 05004], lr: 0.085401, loss: 2.4022
2022-08-18 16:05:39 - train: epoch 0007, iter [01300, 05004], lr: 0.085312, loss: 2.1067
2022-08-18 16:06:13 - train: epoch 0007, iter [01400, 05004], lr: 0.085223, loss: 2.4625
2022-08-18 16:06:47 - train: epoch 0007, iter [01500, 05004], lr: 0.085134, loss: 2.5162
2022-08-18 16:07:21 - train: epoch 0007, iter [01600, 05004], lr: 0.085044, loss: 2.6096
2022-08-18 16:07:55 - train: epoch 0007, iter [01700, 05004], lr: 0.084954, loss: 2.4902
2022-08-18 16:08:29 - train: epoch 0007, iter [01800, 05004], lr: 0.084865, loss: 2.3662
2022-08-18 16:09:04 - train: epoch 0007, iter [01900, 05004], lr: 0.084774, loss: 2.5637
2022-08-18 16:09:38 - train: epoch 0007, iter [02000, 05004], lr: 0.084684, loss: 2.2104
2022-08-18 16:10:12 - train: epoch 0007, iter [02100, 05004], lr: 0.084594, loss: 2.7540
2022-08-18 16:10:45 - train: epoch 0007, iter [02200, 05004], lr: 0.084503, loss: 2.1470
2022-08-18 16:11:19 - train: epoch 0007, iter [02300, 05004], lr: 0.084412, loss: 2.3557
2022-08-18 16:11:53 - train: epoch 0007, iter [02400, 05004], lr: 0.084321, loss: 2.6531
2022-08-18 16:12:27 - train: epoch 0007, iter [02500, 05004], lr: 0.084229, loss: 2.5092
2022-08-18 16:13:00 - train: epoch 0007, iter [02600, 05004], lr: 0.084138, loss: 2.3290
2022-08-18 16:13:33 - train: epoch 0007, iter [02700, 05004], lr: 0.084046, loss: 2.3330
2022-08-18 16:14:06 - train: epoch 0007, iter [02800, 05004], lr: 0.083954, loss: 2.2803
2022-08-18 16:14:39 - train: epoch 0007, iter [02900, 05004], lr: 0.083861, loss: 2.1785
2022-08-18 16:15:12 - train: epoch 0007, iter [03000, 05004], lr: 0.083769, loss: 2.5557
2022-08-18 16:15:45 - train: epoch 0007, iter [03100, 05004], lr: 0.083676, loss: 2.3289
2022-08-18 16:16:19 - train: epoch 0007, iter [03200, 05004], lr: 0.083583, loss: 2.3116
2022-08-18 16:16:52 - train: epoch 0007, iter [03300, 05004], lr: 0.083490, loss: 2.4460
2022-08-18 16:17:26 - train: epoch 0007, iter [03400, 05004], lr: 0.083397, loss: 2.3763
2022-08-18 16:18:00 - train: epoch 0007, iter [03500, 05004], lr: 0.083303, loss: 2.5073
2022-08-18 16:18:33 - train: epoch 0007, iter [03600, 05004], lr: 0.083209, loss: 2.4011
2022-08-18 16:19:06 - train: epoch 0007, iter [03700, 05004], lr: 0.083115, loss: 2.3716
2022-08-18 16:19:40 - train: epoch 0007, iter [03800, 05004], lr: 0.083021, loss: 2.4508
2022-08-18 16:20:14 - train: epoch 0007, iter [03900, 05004], lr: 0.082927, loss: 2.3131
2022-08-18 16:20:48 - train: epoch 0007, iter [04000, 05004], lr: 0.082832, loss: 2.6374
2022-08-18 16:21:21 - train: epoch 0007, iter [04100, 05004], lr: 0.082738, loss: 2.2610
2022-08-18 16:21:55 - train: epoch 0007, iter [04200, 05004], lr: 0.082643, loss: 2.3884
2022-08-18 16:22:29 - train: epoch 0007, iter [04300, 05004], lr: 0.082547, loss: 2.4729
2022-08-18 16:23:03 - train: epoch 0007, iter [04400, 05004], lr: 0.082452, loss: 2.4189
2022-08-18 16:23:36 - train: epoch 0007, iter [04500, 05004], lr: 0.082356, loss: 2.4694
2022-08-18 16:24:10 - train: epoch 0007, iter [04600, 05004], lr: 0.082260, loss: 2.4857
2022-08-18 16:24:43 - train: epoch 0007, iter [04700, 05004], lr: 0.082164, loss: 2.2511
2022-08-18 16:25:17 - train: epoch 0007, iter [04800, 05004], lr: 0.082068, loss: 2.6278
2022-08-18 16:25:51 - train: epoch 0007, iter [04900, 05004], lr: 0.081972, loss: 2.4941
2022-08-18 16:26:24 - train: epoch 0007, iter [05000, 05004], lr: 0.081875, loss: 2.3419
2022-08-18 16:26:25 - train: epoch 007, train_loss: 2.4102
2022-08-18 16:27:42 - eval: epoch: 007, acc1: 51.132%, acc5: 76.986%, test_loss: 2.1072, per_image_load_time: 2.071ms, per_image_inference_time: 0.567ms
2022-08-18 16:27:42 - until epoch: 007, best_acc1: 51.132%
2022-08-18 16:27:42 - epoch 008 lr: 0.081870
2022-08-18 16:28:24 - train: epoch 0008, iter [00100, 05004], lr: 0.081774, loss: 2.3403
2022-08-18 16:28:58 - train: epoch 0008, iter [00200, 05004], lr: 0.081677, loss: 2.4828
2022-08-18 16:29:32 - train: epoch 0008, iter [00300, 05004], lr: 0.081580, loss: 2.5593
2022-08-18 16:30:06 - train: epoch 0008, iter [00400, 05004], lr: 0.081483, loss: 2.1358
2022-08-18 16:30:40 - train: epoch 0008, iter [00500, 05004], lr: 0.081385, loss: 2.1824
2022-08-18 16:31:14 - train: epoch 0008, iter [00600, 05004], lr: 0.081287, loss: 2.5247
2022-08-18 16:31:48 - train: epoch 0008, iter [00700, 05004], lr: 0.081189, loss: 2.4758
2022-08-18 16:32:23 - train: epoch 0008, iter [00800, 05004], lr: 0.081091, loss: 2.2090
2022-08-18 16:32:57 - train: epoch 0008, iter [00900, 05004], lr: 0.080992, loss: 2.1435
2022-08-18 16:33:31 - train: epoch 0008, iter [01000, 05004], lr: 0.080894, loss: 2.3392
2022-08-18 16:34:05 - train: epoch 0008, iter [01100, 05004], lr: 0.080795, loss: 2.3372
2022-08-18 16:34:40 - train: epoch 0008, iter [01200, 05004], lr: 0.080696, loss: 2.2265
2022-08-18 16:35:14 - train: epoch 0008, iter [01300, 05004], lr: 0.080597, loss: 2.6311
2022-08-18 16:35:49 - train: epoch 0008, iter [01400, 05004], lr: 0.080497, loss: 2.1748
2022-08-18 16:36:23 - train: epoch 0008, iter [01500, 05004], lr: 0.080398, loss: 2.4275
2022-08-18 16:36:58 - train: epoch 0008, iter [01600, 05004], lr: 0.080298, loss: 2.3908
2022-08-18 16:37:32 - train: epoch 0008, iter [01700, 05004], lr: 0.080198, loss: 2.2087
2022-08-18 16:38:07 - train: epoch 0008, iter [01800, 05004], lr: 0.080098, loss: 2.2870
2022-08-18 16:38:41 - train: epoch 0008, iter [01900, 05004], lr: 0.079997, loss: 2.2868
2022-08-18 16:39:15 - train: epoch 0008, iter [02000, 05004], lr: 0.079897, loss: 2.2264
2022-08-18 16:39:49 - train: epoch 0008, iter [02100, 05004], lr: 0.079796, loss: 2.1688
2022-08-18 16:40:24 - train: epoch 0008, iter [02200, 05004], lr: 0.079695, loss: 2.3151
2022-08-18 16:40:58 - train: epoch 0008, iter [02300, 05004], lr: 0.079594, loss: 2.3133
2022-08-18 16:41:33 - train: epoch 0008, iter [02400, 05004], lr: 0.079493, loss: 2.3034
2022-08-18 16:42:07 - train: epoch 0008, iter [02500, 05004], lr: 0.079391, loss: 2.4590
2022-08-18 16:42:42 - train: epoch 0008, iter [02600, 05004], lr: 0.079290, loss: 2.1737
2022-08-18 16:43:16 - train: epoch 0008, iter [02700, 05004], lr: 0.079188, loss: 2.4361
2022-08-18 16:43:50 - train: epoch 0008, iter [02800, 05004], lr: 0.079086, loss: 2.5911
2022-08-18 16:44:25 - train: epoch 0008, iter [02900, 05004], lr: 0.078984, loss: 2.1536
2022-08-18 16:44:59 - train: epoch 0008, iter [03000, 05004], lr: 0.078881, loss: 2.4056
2022-08-18 16:45:34 - train: epoch 0008, iter [03100, 05004], lr: 0.078779, loss: 2.2990
2022-08-18 16:46:09 - train: epoch 0008, iter [03200, 05004], lr: 0.078676, loss: 2.4747
2022-08-18 16:46:43 - train: epoch 0008, iter [03300, 05004], lr: 0.078573, loss: 2.4152
2022-08-18 16:47:17 - train: epoch 0008, iter [03400, 05004], lr: 0.078470, loss: 2.4553
2022-08-18 16:47:51 - train: epoch 0008, iter [03500, 05004], lr: 0.078366, loss: 2.4240
2022-08-18 16:48:26 - train: epoch 0008, iter [03600, 05004], lr: 0.078263, loss: 2.4844
2022-08-18 16:49:00 - train: epoch 0008, iter [03700, 05004], lr: 0.078159, loss: 2.2116
2022-08-18 16:49:34 - train: epoch 0008, iter [03800, 05004], lr: 0.078055, loss: 2.1168
2022-08-18 16:50:09 - train: epoch 0008, iter [03900, 05004], lr: 0.077951, loss: 2.4323
2022-08-18 16:50:43 - train: epoch 0008, iter [04000, 05004], lr: 0.077847, loss: 2.6598
2022-08-18 16:51:17 - train: epoch 0008, iter [04100, 05004], lr: 0.077743, loss: 2.2207
2022-08-18 16:51:52 - train: epoch 0008, iter [04200, 05004], lr: 0.077638, loss: 2.4109
2022-08-18 16:52:26 - train: epoch 0008, iter [04300, 05004], lr: 0.077533, loss: 1.9860
2022-08-18 16:53:01 - train: epoch 0008, iter [04400, 05004], lr: 0.077429, loss: 2.2500
2022-08-18 16:53:35 - train: epoch 0008, iter [04500, 05004], lr: 0.077324, loss: 2.1465
2022-08-18 16:54:09 - train: epoch 0008, iter [04600, 05004], lr: 0.077218, loss: 2.4068
2022-08-18 16:54:44 - train: epoch 0008, iter [04700, 05004], lr: 0.077113, loss: 2.4160
2022-08-18 16:55:18 - train: epoch 0008, iter [04800, 05004], lr: 0.077007, loss: 2.3887
2022-08-18 16:55:52 - train: epoch 0008, iter [04900, 05004], lr: 0.076902, loss: 2.2978
2022-08-18 16:56:26 - train: epoch 0008, iter [05000, 05004], lr: 0.076796, loss: 2.2994
2022-08-18 16:56:27 - train: epoch 008, train_loss: 2.3264
2022-08-18 16:57:45 - eval: epoch: 008, acc1: 53.234%, acc5: 78.524%, test_loss: 2.0172, per_image_load_time: 2.455ms, per_image_inference_time: 0.554ms
2022-08-18 16:57:45 - until epoch: 008, best_acc1: 53.234%
2022-08-18 16:57:45 - epoch 009 lr: 0.076790
2022-08-18 16:58:26 - train: epoch 0009, iter [00100, 05004], lr: 0.076685, loss: 1.9877
2022-08-18 16:59:01 - train: epoch 0009, iter [00200, 05004], lr: 0.076579, loss: 2.2406
2022-08-18 16:59:35 - train: epoch 0009, iter [00300, 05004], lr: 0.076473, loss: 2.0900
2022-08-18 17:00:09 - train: epoch 0009, iter [00400, 05004], lr: 0.076366, loss: 2.4396
2022-08-18 17:00:43 - train: epoch 0009, iter [00500, 05004], lr: 0.076259, loss: 2.2670
2022-08-18 17:01:18 - train: epoch 0009, iter [00600, 05004], lr: 0.076152, loss: 2.2801
2022-08-18 17:01:52 - train: epoch 0009, iter [00700, 05004], lr: 0.076045, loss: 2.0851
2022-08-18 17:02:27 - train: epoch 0009, iter [00800, 05004], lr: 0.075938, loss: 2.2244
2022-08-18 17:03:01 - train: epoch 0009, iter [00900, 05004], lr: 0.075830, loss: 2.1928
2022-08-18 17:03:36 - train: epoch 0009, iter [01000, 05004], lr: 0.075723, loss: 2.1799
2022-08-18 17:04:11 - train: epoch 0009, iter [01100, 05004], lr: 0.075615, loss: 2.5573
2022-08-18 17:04:45 - train: epoch 0009, iter [01200, 05004], lr: 0.075507, loss: 2.3814
2022-08-18 17:05:19 - train: epoch 0009, iter [01300, 05004], lr: 0.075399, loss: 2.4271
2022-08-18 17:05:54 - train: epoch 0009, iter [01400, 05004], lr: 0.075291, loss: 2.1252
2022-08-18 17:06:29 - train: epoch 0009, iter [01500, 05004], lr: 0.075182, loss: 2.2152
2022-08-18 17:07:03 - train: epoch 0009, iter [01600, 05004], lr: 0.075074, loss: 2.2916
2022-08-18 17:07:37 - train: epoch 0009, iter [01700, 05004], lr: 0.074965, loss: 2.3990
2022-08-18 17:08:11 - train: epoch 0009, iter [01800, 05004], lr: 0.074856, loss: 2.1689
2022-08-18 17:08:46 - train: epoch 0009, iter [01900, 05004], lr: 0.074747, loss: 1.9117
2022-08-18 17:09:21 - train: epoch 0009, iter [02000, 05004], lr: 0.074638, loss: 2.1666
2022-08-18 17:09:55 - train: epoch 0009, iter [02100, 05004], lr: 0.074529, loss: 2.1854
2022-08-18 17:10:29 - train: epoch 0009, iter [02200, 05004], lr: 0.074419, loss: 2.3751
2022-08-18 17:11:04 - train: epoch 0009, iter [02300, 05004], lr: 0.074310, loss: 2.0590
2022-08-18 17:11:39 - train: epoch 0009, iter [02400, 05004], lr: 0.074200, loss: 2.4448
2022-08-18 17:12:13 - train: epoch 0009, iter [02500, 05004], lr: 0.074090, loss: 2.1491
2022-08-18 17:12:47 - train: epoch 0009, iter [02600, 05004], lr: 0.073980, loss: 2.5012
2022-08-18 17:13:22 - train: epoch 0009, iter [02700, 05004], lr: 0.073870, loss: 2.1859
2022-08-18 17:13:56 - train: epoch 0009, iter [02800, 05004], lr: 0.073759, loss: 2.2698
2022-08-18 17:14:30 - train: epoch 0009, iter [02900, 05004], lr: 0.073649, loss: 2.1466
2022-08-18 17:15:05 - train: epoch 0009, iter [03000, 05004], lr: 0.073538, loss: 2.1992
2022-08-18 17:15:39 - train: epoch 0009, iter [03100, 05004], lr: 0.073427, loss: 2.2915
2022-08-18 17:16:14 - train: epoch 0009, iter [03200, 05004], lr: 0.073316, loss: 2.2566
2022-08-18 17:16:49 - train: epoch 0009, iter [03300, 05004], lr: 0.073205, loss: 2.2509
2022-08-18 17:17:23 - train: epoch 0009, iter [03400, 05004], lr: 0.073094, loss: 2.4603
2022-08-18 17:17:57 - train: epoch 0009, iter [03500, 05004], lr: 0.072982, loss: 2.2660
2022-08-18 17:18:32 - train: epoch 0009, iter [03600, 05004], lr: 0.072871, loss: 2.0800
2022-08-18 17:19:07 - train: epoch 0009, iter [03700, 05004], lr: 0.072759, loss: 2.2627
2022-08-18 17:19:41 - train: epoch 0009, iter [03800, 05004], lr: 0.072647, loss: 2.3141
2022-08-18 17:20:15 - train: epoch 0009, iter [03900, 05004], lr: 0.072535, loss: 1.9719
2022-08-18 17:20:50 - train: epoch 0009, iter [04000, 05004], lr: 0.072423, loss: 2.3275
2022-08-18 17:21:23 - train: epoch 0009, iter [04100, 05004], lr: 0.072310, loss: 2.3690
2022-08-18 17:21:57 - train: epoch 0009, iter [04200, 05004], lr: 0.072198, loss: 2.1625
2022-08-18 17:22:31 - train: epoch 0009, iter [04300, 05004], lr: 0.072085, loss: 2.0621
2022-08-18 17:23:04 - train: epoch 0009, iter [04400, 05004], lr: 0.071973, loss: 2.2035
2022-08-18 17:23:38 - train: epoch 0009, iter [04500, 05004], lr: 0.071860, loss: 2.4841
2022-08-18 17:24:11 - train: epoch 0009, iter [04600, 05004], lr: 0.071747, loss: 2.4102
2022-08-18 17:24:44 - train: epoch 0009, iter [04700, 05004], lr: 0.071634, loss: 2.4508
2022-08-18 17:25:18 - train: epoch 0009, iter [04800, 05004], lr: 0.071520, loss: 2.3138
2022-08-18 17:25:51 - train: epoch 0009, iter [04900, 05004], lr: 0.071407, loss: 2.1976
2022-08-18 17:26:24 - train: epoch 0009, iter [05000, 05004], lr: 0.071294, loss: 2.3197
2022-08-18 17:26:26 - train: epoch 009, train_loss: 2.2548
2022-08-18 17:27:40 - eval: epoch: 009, acc1: 54.280%, acc5: 79.288%, test_loss: 1.9653, per_image_load_time: 2.255ms, per_image_inference_time: 0.568ms
2022-08-18 17:27:40 - until epoch: 009, best_acc1: 54.280%
2022-08-18 17:27:40 - epoch 010 lr: 0.071288
2022-08-18 17:28:20 - train: epoch 0010, iter [00100, 05004], lr: 0.071175, loss: 2.0624
2022-08-18 17:28:53 - train: epoch 0010, iter [00200, 05004], lr: 0.071061, loss: 2.2773
2022-08-18 17:29:26 - train: epoch 0010, iter [00300, 05004], lr: 0.070948, loss: 2.2101
2022-08-18 17:30:00 - train: epoch 0010, iter [00400, 05004], lr: 0.070833, loss: 2.3786
2022-08-18 17:30:33 - train: epoch 0010, iter [00500, 05004], lr: 0.070719, loss: 2.0891
2022-08-18 17:31:07 - train: epoch 0010, iter [00600, 05004], lr: 0.070605, loss: 2.3227
2022-08-18 17:31:41 - train: epoch 0010, iter [00700, 05004], lr: 0.070490, loss: 2.1950
2022-08-18 17:32:15 - train: epoch 0010, iter [00800, 05004], lr: 0.070376, loss: 2.1434
2022-08-18 17:32:49 - train: epoch 0010, iter [00900, 05004], lr: 0.070261, loss: 2.0233
2022-08-18 17:33:23 - train: epoch 0010, iter [01000, 05004], lr: 0.070146, loss: 2.2532
2022-08-18 17:33:57 - train: epoch 0010, iter [01100, 05004], lr: 0.070031, loss: 2.1436
2022-08-18 17:34:31 - train: epoch 0010, iter [01200, 05004], lr: 0.069916, loss: 2.2258
2022-08-18 17:35:04 - train: epoch 0010, iter [01300, 05004], lr: 0.069801, loss: 1.9953
2022-08-18 17:35:38 - train: epoch 0010, iter [01400, 05004], lr: 0.069686, loss: 2.1601
2022-08-18 17:36:11 - train: epoch 0010, iter [01500, 05004], lr: 0.069570, loss: 1.8789
2022-08-18 17:36:44 - train: epoch 0010, iter [01600, 05004], lr: 0.069454, loss: 2.1141
2022-08-18 17:37:17 - train: epoch 0010, iter [01700, 05004], lr: 0.069339, loss: 2.2462
2022-08-18 17:37:51 - train: epoch 0010, iter [01800, 05004], lr: 0.069223, loss: 1.9490
2022-08-18 17:38:24 - train: epoch 0010, iter [01900, 05004], lr: 0.069107, loss: 2.1257
2022-08-18 17:38:57 - train: epoch 0010, iter [02000, 05004], lr: 0.068991, loss: 2.3307
2022-08-18 17:39:30 - train: epoch 0010, iter [02100, 05004], lr: 0.068875, loss: 1.8998
2022-08-18 17:40:03 - train: epoch 0010, iter [02200, 05004], lr: 0.068758, loss: 2.1106
2022-08-18 17:40:36 - train: epoch 0010, iter [02300, 05004], lr: 0.068642, loss: 2.4560
2022-08-18 17:41:09 - train: epoch 0010, iter [02400, 05004], lr: 0.068525, loss: 2.3915
2022-08-18 17:41:43 - train: epoch 0010, iter [02500, 05004], lr: 0.068409, loss: 2.2577
2022-08-18 17:42:16 - train: epoch 0010, iter [02600, 05004], lr: 0.068292, loss: 2.3500
2022-08-18 17:42:49 - train: epoch 0010, iter [02700, 05004], lr: 0.068175, loss: 2.0865
2022-08-18 17:43:22 - train: epoch 0010, iter [02800, 05004], lr: 0.068058, loss: 2.2462
2022-08-18 17:43:56 - train: epoch 0010, iter [02900, 05004], lr: 0.067941, loss: 2.2733
2022-08-18 17:44:29 - train: epoch 0010, iter [03000, 05004], lr: 0.067823, loss: 2.1758
2022-08-18 17:45:03 - train: epoch 0010, iter [03100, 05004], lr: 0.067706, loss: 2.3595
2022-08-18 17:45:36 - train: epoch 0010, iter [03200, 05004], lr: 0.067589, loss: 2.0799
2022-08-18 17:46:10 - train: epoch 0010, iter [03300, 05004], lr: 0.067471, loss: 2.1202
2022-08-18 17:46:42 - train: epoch 0010, iter [03400, 05004], lr: 0.067353, loss: 2.2630
2022-08-18 17:47:16 - train: epoch 0010, iter [03500, 05004], lr: 0.067235, loss: 2.4630
2022-08-18 17:47:49 - train: epoch 0010, iter [03600, 05004], lr: 0.067118, loss: 2.3665
2022-08-18 17:48:23 - train: epoch 0010, iter [03700, 05004], lr: 0.066999, loss: 2.3566
2022-08-18 17:48:57 - train: epoch 0010, iter [03800, 05004], lr: 0.066881, loss: 2.2562
2022-08-18 17:49:30 - train: epoch 0010, iter [03900, 05004], lr: 0.066763, loss: 1.8671
2022-08-18 17:50:03 - train: epoch 0010, iter [04000, 05004], lr: 0.066645, loss: 2.0374
2022-08-18 17:50:37 - train: epoch 0010, iter [04100, 05004], lr: 0.066526, loss: 2.0070
2022-08-18 17:51:10 - train: epoch 0010, iter [04200, 05004], lr: 0.066408, loss: 2.1898
2022-08-18 17:51:43 - train: epoch 0010, iter [04300, 05004], lr: 0.066289, loss: 1.9019
2022-08-18 17:52:17 - train: epoch 0010, iter [04400, 05004], lr: 0.066170, loss: 2.1092
2022-08-18 17:52:51 - train: epoch 0010, iter [04500, 05004], lr: 0.066051, loss: 2.1060
2022-08-18 17:53:24 - train: epoch 0010, iter [04600, 05004], lr: 0.065932, loss: 2.1912
2022-08-18 17:53:58 - train: epoch 0010, iter [04700, 05004], lr: 0.065813, loss: 2.2085
2022-08-18 17:54:31 - train: epoch 0010, iter [04800, 05004], lr: 0.065694, loss: 2.1242
2022-08-18 17:55:05 - train: epoch 0010, iter [04900, 05004], lr: 0.065575, loss: 2.4044
2022-08-18 17:55:37 - train: epoch 0010, iter [05000, 05004], lr: 0.065456, loss: 2.2626
2022-08-18 17:55:39 - train: epoch 010, train_loss: 2.1947
2022-08-18 17:56:54 - eval: epoch: 010, acc1: 55.124%, acc5: 79.978%, test_loss: 1.9093, per_image_load_time: 2.290ms, per_image_inference_time: 0.591ms
2022-08-18 17:56:54 - until epoch: 010, best_acc1: 55.124%
2022-08-18 17:56:54 - epoch 011 lr: 0.065450
2022-08-18 17:57:34 - train: epoch 0011, iter [00100, 05004], lr: 0.065331, loss: 2.1952
2022-08-18 17:58:08 - train: epoch 0011, iter [00200, 05004], lr: 0.065212, loss: 2.2521
2022-08-18 17:58:41 - train: epoch 0011, iter [00300, 05004], lr: 0.065092, loss: 2.0594
2022-08-18 17:59:15 - train: epoch 0011, iter [00400, 05004], lr: 0.064972, loss: 2.2395
2022-08-18 17:59:48 - train: epoch 0011, iter [00500, 05004], lr: 0.064853, loss: 2.0042
2022-08-18 18:00:21 - train: epoch 0011, iter [00600, 05004], lr: 0.064733, loss: 2.0690
2022-08-18 18:00:55 - train: epoch 0011, iter [00700, 05004], lr: 0.064613, loss: 2.3978
2022-08-18 18:01:29 - train: epoch 0011, iter [00800, 05004], lr: 0.064492, loss: 2.2581
2022-08-18 18:02:02 - train: epoch 0011, iter [00900, 05004], lr: 0.064372, loss: 2.2355
2022-08-18 18:02:37 - train: epoch 0011, iter [01000, 05004], lr: 0.064252, loss: 2.0731
2022-08-18 18:03:11 - train: epoch 0011, iter [01100, 05004], lr: 0.064132, loss: 2.2162
2022-08-18 18:03:45 - train: epoch 0011, iter [01200, 05004], lr: 0.064011, loss: 2.2348
2022-08-18 18:04:20 - train: epoch 0011, iter [01300, 05004], lr: 0.063890, loss: 2.3078
2022-08-18 18:04:54 - train: epoch 0011, iter [01400, 05004], lr: 0.063770, loss: 2.0885
2022-08-18 18:05:27 - train: epoch 0011, iter [01500, 05004], lr: 0.063649, loss: 1.9558
2022-08-18 18:06:01 - train: epoch 0011, iter [01600, 05004], lr: 0.063528, loss: 2.2157
2022-08-18 18:06:35 - train: epoch 0011, iter [01700, 05004], lr: 0.063407, loss: 2.2385
2022-08-18 18:07:09 - train: epoch 0011, iter [01800, 05004], lr: 0.063286, loss: 1.8989
2022-08-18 18:07:42 - train: epoch 0011, iter [01900, 05004], lr: 0.063165, loss: 2.1868
2022-08-18 18:08:16 - train: epoch 0011, iter [02000, 05004], lr: 0.063044, loss: 2.3560
2022-08-18 18:08:49 - train: epoch 0011, iter [02100, 05004], lr: 0.062923, loss: 2.1902
2022-08-18 18:09:22 - train: epoch 0011, iter [02200, 05004], lr: 0.062801, loss: 1.9420
2022-08-18 18:09:56 - train: epoch 0011, iter [02300, 05004], lr: 0.062680, loss: 2.3473
2022-08-18 18:10:30 - train: epoch 0011, iter [02400, 05004], lr: 0.062559, loss: 2.1175
2022-08-18 18:11:03 - train: epoch 0011, iter [02500, 05004], lr: 0.062437, loss: 2.1046
2022-08-18 18:11:37 - train: epoch 0011, iter [02600, 05004], lr: 0.062315, loss: 2.2221
2022-08-18 18:12:10 - train: epoch 0011, iter [02700, 05004], lr: 0.062194, loss: 2.1746
2022-08-18 18:12:44 - train: epoch 0011, iter [02800, 05004], lr: 0.062072, loss: 1.9400
2022-08-18 18:13:18 - train: epoch 0011, iter [02900, 05004], lr: 0.061950, loss: 2.1399
2022-08-18 18:13:51 - train: epoch 0011, iter [03000, 05004], lr: 0.061828, loss: 2.2144
2022-08-18 18:14:24 - train: epoch 0011, iter [03100, 05004], lr: 0.061706, loss: 2.1834
2022-08-18 18:14:57 - train: epoch 0011, iter [03200, 05004], lr: 0.061584, loss: 2.0995
2022-08-18 18:15:31 - train: epoch 0011, iter [03300, 05004], lr: 0.061462, loss: 2.1492
2022-08-18 18:16:05 - train: epoch 0011, iter [03400, 05004], lr: 0.061339, loss: 2.0789
2022-08-18 18:16:38 - train: epoch 0011, iter [03500, 05004], lr: 0.061217, loss: 2.0995
2022-08-18 18:17:11 - train: epoch 0011, iter [03600, 05004], lr: 0.061095, loss: 1.9419
2022-08-18 18:17:44 - train: epoch 0011, iter [03700, 05004], lr: 0.060972, loss: 2.2686
2022-08-18 18:18:18 - train: epoch 0011, iter [03800, 05004], lr: 0.060850, loss: 1.9255
2022-08-18 18:18:51 - train: epoch 0011, iter [03900, 05004], lr: 0.060727, loss: 2.2338
2022-08-18 18:19:24 - train: epoch 0011, iter [04000, 05004], lr: 0.060604, loss: 2.0438
2022-08-18 18:19:58 - train: epoch 0011, iter [04100, 05004], lr: 0.060482, loss: 1.9291
2022-08-18 18:20:31 - train: epoch 0011, iter [04200, 05004], lr: 0.060359, loss: 2.1469
2022-08-18 18:21:04 - train: epoch 0011, iter [04300, 05004], lr: 0.060236, loss: 2.1682
2022-08-18 18:21:38 - train: epoch 0011, iter [04400, 05004], lr: 0.060113, loss: 2.2150
2022-08-18 18:22:11 - train: epoch 0011, iter [04500, 05004], lr: 0.059990, loss: 2.0459
2022-08-18 18:22:45 - train: epoch 0011, iter [04600, 05004], lr: 0.059867, loss: 2.0303
2022-08-18 18:23:18 - train: epoch 0011, iter [04700, 05004], lr: 0.059744, loss: 1.9538
2022-08-18 18:23:51 - train: epoch 0011, iter [04800, 05004], lr: 0.059621, loss: 2.0099
2022-08-18 18:24:24 - train: epoch 0011, iter [04900, 05004], lr: 0.059497, loss: 2.1150
2022-08-18 18:24:56 - train: epoch 0011, iter [05000, 05004], lr: 0.059374, loss: 2.1315
2022-08-18 18:24:58 - train: epoch 011, train_loss: 2.1321
2022-08-18 18:26:14 - eval: epoch: 011, acc1: 56.762%, acc5: 81.366%, test_loss: 1.8243, per_image_load_time: 2.289ms, per_image_inference_time: 0.620ms
2022-08-18 18:26:14 - until epoch: 011, best_acc1: 56.762%
2022-08-18 18:26:14 - epoch 012 lr: 0.059368
2022-08-18 18:26:54 - train: epoch 0012, iter [00100, 05004], lr: 0.059246, loss: 1.9333
2022-08-18 18:27:27 - train: epoch 0012, iter [00200, 05004], lr: 0.059122, loss: 1.9521
2022-08-18 18:28:00 - train: epoch 0012, iter [00300, 05004], lr: 0.058999, loss: 2.0634
2022-08-18 18:28:34 - train: epoch 0012, iter [00400, 05004], lr: 0.058875, loss: 1.9805
2022-08-18 18:29:07 - train: epoch 0012, iter [00500, 05004], lr: 0.058752, loss: 1.9976
2022-08-18 18:29:40 - train: epoch 0012, iter [00600, 05004], lr: 0.058628, loss: 2.0124
2022-08-18 18:30:14 - train: epoch 0012, iter [00700, 05004], lr: 0.058504, loss: 1.8379
2022-08-18 18:30:47 - train: epoch 0012, iter [00800, 05004], lr: 0.058381, loss: 2.2899
2022-08-18 18:31:20 - train: epoch 0012, iter [00900, 05004], lr: 0.058257, loss: 2.0830
2022-08-18 18:31:53 - train: epoch 0012, iter [01000, 05004], lr: 0.058133, loss: 1.9628
2022-08-18 18:32:26 - train: epoch 0012, iter [01100, 05004], lr: 0.058009, loss: 2.2938
2022-08-18 18:32:59 - train: epoch 0012, iter [01200, 05004], lr: 0.057885, loss: 1.9146
2022-08-18 18:33:32 - train: epoch 0012, iter [01300, 05004], lr: 0.057761, loss: 1.9611
2022-08-18 18:34:05 - train: epoch 0012, iter [01400, 05004], lr: 0.057637, loss: 2.3834
2022-08-18 18:34:39 - train: epoch 0012, iter [01500, 05004], lr: 0.057513, loss: 1.9569
2022-08-18 18:35:12 - train: epoch 0012, iter [01600, 05004], lr: 0.057389, loss: 2.0108
2022-08-18 18:35:46 - train: epoch 0012, iter [01700, 05004], lr: 0.057264, loss: 1.9680
2022-08-18 18:36:19 - train: epoch 0012, iter [01800, 05004], lr: 0.057140, loss: 2.1491
2022-08-18 18:36:52 - train: epoch 0012, iter [01900, 05004], lr: 0.057016, loss: 2.1049
2022-08-18 18:37:26 - train: epoch 0012, iter [02000, 05004], lr: 0.056892, loss: 2.2849
2022-08-18 18:37:59 - train: epoch 0012, iter [02100, 05004], lr: 0.056767, loss: 2.1107
2022-08-18 18:38:32 - train: epoch 0012, iter [02200, 05004], lr: 0.056643, loss: 2.1922
2022-08-18 18:39:06 - train: epoch 0012, iter [02300, 05004], lr: 0.056518, loss: 2.1299
2022-08-18 18:39:40 - train: epoch 0012, iter [02400, 05004], lr: 0.056394, loss: 2.1413
2022-08-18 18:40:13 - train: epoch 0012, iter [02500, 05004], lr: 0.056269, loss: 1.7982
2022-08-18 18:40:48 - train: epoch 0012, iter [02600, 05004], lr: 0.056145, loss: 1.7394
2022-08-18 18:41:22 - train: epoch 0012, iter [02700, 05004], lr: 0.056020, loss: 2.1752
2022-08-18 18:41:56 - train: epoch 0012, iter [02800, 05004], lr: 0.055895, loss: 2.0801
2022-08-18 18:42:31 - train: epoch 0012, iter [02900, 05004], lr: 0.055771, loss: 2.1465
2022-08-18 18:43:05 - train: epoch 0012, iter [03000, 05004], lr: 0.055646, loss: 1.8453
2022-08-18 18:43:40 - train: epoch 0012, iter [03100, 05004], lr: 0.055521, loss: 1.9213
2022-08-18 18:44:14 - train: epoch 0012, iter [03200, 05004], lr: 0.055396, loss: 2.1071
2022-08-18 18:44:48 - train: epoch 0012, iter [03300, 05004], lr: 0.055271, loss: 2.0700
2022-08-18 18:45:22 - train: epoch 0012, iter [03400, 05004], lr: 0.055146, loss: 1.8662
2022-08-18 18:45:56 - train: epoch 0012, iter [03500, 05004], lr: 0.055022, loss: 2.2629
2022-08-18 18:46:31 - train: epoch 0012, iter [03600, 05004], lr: 0.054897, loss: 1.8093
2022-08-18 18:47:05 - train: epoch 0012, iter [03700, 05004], lr: 0.054772, loss: 1.9775
2022-08-18 18:47:40 - train: epoch 0012, iter [03800, 05004], lr: 0.054647, loss: 2.2293
2022-08-18 18:48:14 - train: epoch 0012, iter [03900, 05004], lr: 0.054522, loss: 1.8795
2022-08-18 18:48:49 - train: epoch 0012, iter [04000, 05004], lr: 0.054397, loss: 2.0839
2022-08-18 18:49:24 - train: epoch 0012, iter [04100, 05004], lr: 0.054271, loss: 1.9370
2022-08-18 18:49:58 - train: epoch 0012, iter [04200, 05004], lr: 0.054146, loss: 1.9758
2022-08-18 18:50:33 - train: epoch 0012, iter [04300, 05004], lr: 0.054021, loss: 2.0480
2022-08-18 18:51:07 - train: epoch 0012, iter [04400, 05004], lr: 0.053896, loss: 1.8856
2022-08-18 18:51:42 - train: epoch 0012, iter [04500, 05004], lr: 0.053771, loss: 2.1146
2022-08-18 18:52:17 - train: epoch 0012, iter [04600, 05004], lr: 0.053646, loss: 2.1158
2022-08-18 18:52:52 - train: epoch 0012, iter [04700, 05004], lr: 0.053520, loss: 1.9773
2022-08-18 18:53:26 - train: epoch 0012, iter [04800, 05004], lr: 0.053395, loss: 2.0345
2022-08-18 18:54:01 - train: epoch 0012, iter [04900, 05004], lr: 0.053270, loss: 1.9119
2022-08-18 18:54:34 - train: epoch 0012, iter [05000, 05004], lr: 0.053145, loss: 1.7327
2022-08-18 18:54:35 - train: epoch 012, train_loss: 2.0729
2022-08-18 18:55:54 - eval: epoch: 012, acc1: 57.796%, acc5: 82.110%, test_loss: 1.7770, per_image_load_time: 2.442ms, per_image_inference_time: 0.576ms
2022-08-18 18:55:54 - until epoch: 012, best_acc1: 57.796%
2022-08-18 18:55:54 - epoch 013 lr: 0.053138
2022-08-18 18:56:35 - train: epoch 0013, iter [00100, 05004], lr: 0.053014, loss: 1.8302
2022-08-18 18:57:10 - train: epoch 0013, iter [00200, 05004], lr: 0.052889, loss: 1.8496
2022-08-18 18:57:46 - train: epoch 0013, iter [00300, 05004], lr: 0.052763, loss: 1.8694
2022-08-18 18:58:20 - train: epoch 0013, iter [00400, 05004], lr: 0.052638, loss: 1.8839
2022-08-18 18:58:55 - train: epoch 0013, iter [00500, 05004], lr: 0.052513, loss: 2.1064
2022-08-18 18:59:29 - train: epoch 0013, iter [00600, 05004], lr: 0.052387, loss: 1.9068
2022-08-18 19:00:03 - train: epoch 0013, iter [00700, 05004], lr: 0.052262, loss: 1.8724
2022-08-18 19:00:38 - train: epoch 0013, iter [00800, 05004], lr: 0.052136, loss: 2.1734
2022-08-18 19:01:13 - train: epoch 0013, iter [00900, 05004], lr: 0.052011, loss: 1.9975
2022-08-18 19:01:47 - train: epoch 0013, iter [01000, 05004], lr: 0.051886, loss: 1.9592
2022-08-18 19:02:22 - train: epoch 0013, iter [01100, 05004], lr: 0.051760, loss: 2.0727
2022-08-18 19:02:57 - train: epoch 0013, iter [01200, 05004], lr: 0.051635, loss: 2.2045
2022-08-18 19:03:32 - train: epoch 0013, iter [01300, 05004], lr: 0.051509, loss: 1.9655
2022-08-18 19:04:07 - train: epoch 0013, iter [01400, 05004], lr: 0.051384, loss: 1.9174
2022-08-18 19:04:42 - train: epoch 0013, iter [01500, 05004], lr: 0.051258, loss: 1.9563
2022-08-18 19:05:16 - train: epoch 0013, iter [01600, 05004], lr: 0.051132, loss: 1.9296
2022-08-18 19:05:51 - train: epoch 0013, iter [01700, 05004], lr: 0.051007, loss: 2.0428
2022-08-18 19:06:26 - train: epoch 0013, iter [01800, 05004], lr: 0.050881, loss: 1.8569
2022-08-18 19:07:01 - train: epoch 0013, iter [01900, 05004], lr: 0.050756, loss: 2.1460
2022-08-18 19:07:36 - train: epoch 0013, iter [02000, 05004], lr: 0.050630, loss: 2.2160
2022-08-18 19:08:11 - train: epoch 0013, iter [02100, 05004], lr: 0.050505, loss: 2.1883
2022-08-18 19:08:46 - train: epoch 0013, iter [02200, 05004], lr: 0.050379, loss: 2.0361
2022-08-18 19:09:21 - train: epoch 0013, iter [02300, 05004], lr: 0.050254, loss: 1.9969
2022-08-18 19:09:56 - train: epoch 0013, iter [02400, 05004], lr: 0.050128, loss: 2.0399
2022-08-18 19:10:31 - train: epoch 0013, iter [02500, 05004], lr: 0.050003, loss: 2.0084
2022-08-18 19:11:06 - train: epoch 0013, iter [02600, 05004], lr: 0.049877, loss: 1.6935
2022-08-18 19:11:40 - train: epoch 0013, iter [02700, 05004], lr: 0.049751, loss: 1.8033
2022-08-18 19:12:15 - train: epoch 0013, iter [02800, 05004], lr: 0.049626, loss: 1.9960
2022-08-18 19:12:50 - train: epoch 0013, iter [02900, 05004], lr: 0.049500, loss: 2.0083
2022-08-18 19:13:24 - train: epoch 0013, iter [03000, 05004], lr: 0.049375, loss: 1.7957
2022-08-18 19:13:59 - train: epoch 0013, iter [03100, 05004], lr: 0.049249, loss: 1.8173
2022-08-18 19:14:34 - train: epoch 0013, iter [03200, 05004], lr: 0.049124, loss: 1.9992
2022-08-18 19:15:09 - train: epoch 0013, iter [03300, 05004], lr: 0.048998, loss: 1.7205
2022-08-18 19:15:44 - train: epoch 0013, iter [03400, 05004], lr: 0.048873, loss: 1.9955
2022-08-18 19:16:19 - train: epoch 0013, iter [03500, 05004], lr: 0.048747, loss: 1.9391
2022-08-18 19:16:53 - train: epoch 0013, iter [03600, 05004], lr: 0.048621, loss: 2.1009
2022-08-18 19:17:28 - train: epoch 0013, iter [03700, 05004], lr: 0.048496, loss: 1.8356
2022-08-18 19:18:03 - train: epoch 0013, iter [03800, 05004], lr: 0.048370, loss: 2.2156
2022-08-18 19:18:38 - train: epoch 0013, iter [03900, 05004], lr: 0.048245, loss: 2.0920
2022-08-18 19:19:13 - train: epoch 0013, iter [04000, 05004], lr: 0.048120, loss: 2.0583
2022-08-18 19:19:48 - train: epoch 0013, iter [04100, 05004], lr: 0.047994, loss: 1.8413
2022-08-18 19:20:23 - train: epoch 0013, iter [04200, 05004], lr: 0.047869, loss: 2.2021
2022-08-18 19:20:58 - train: epoch 0013, iter [04300, 05004], lr: 0.047743, loss: 1.9031
2022-08-18 19:21:33 - train: epoch 0013, iter [04400, 05004], lr: 0.047618, loss: 2.0406
2022-08-18 19:22:08 - train: epoch 0013, iter [04500, 05004], lr: 0.047492, loss: 1.8864
2022-08-18 19:22:43 - train: epoch 0013, iter [04600, 05004], lr: 0.047367, loss: 2.1155
2022-08-18 19:23:18 - train: epoch 0013, iter [04700, 05004], lr: 0.047242, loss: 2.0447
2022-08-18 19:23:53 - train: epoch 0013, iter [04800, 05004], lr: 0.047116, loss: 2.0490
2022-08-18 19:24:28 - train: epoch 0013, iter [04900, 05004], lr: 0.046991, loss: 2.1244
2022-08-18 19:25:02 - train: epoch 0013, iter [05000, 05004], lr: 0.046865, loss: 2.1762
2022-08-18 19:25:03 - train: epoch 013, train_loss: 2.0179
2022-08-18 19:26:22 - eval: epoch: 013, acc1: 59.266%, acc5: 83.222%, test_loss: 1.7114, per_image_load_time: 2.478ms, per_image_inference_time: 0.563ms
2022-08-18 19:26:22 - until epoch: 013, best_acc1: 59.266%
2022-08-18 19:26:22 - epoch 014 lr: 0.046859
2022-08-18 19:27:04 - train: epoch 0014, iter [00100, 05004], lr: 0.046735, loss: 1.8021
2022-08-18 19:27:38 - train: epoch 0014, iter [00200, 05004], lr: 0.046610, loss: 1.8621
2022-08-18 19:28:13 - train: epoch 0014, iter [00300, 05004], lr: 0.046485, loss: 1.9877
2022-08-18 19:28:48 - train: epoch 0014, iter [00400, 05004], lr: 0.046359, loss: 2.0028
2022-08-18 19:29:23 - train: epoch 0014, iter [00500, 05004], lr: 0.046234, loss: 1.7854
2022-08-18 19:29:58 - train: epoch 0014, iter [00600, 05004], lr: 0.046109, loss: 1.9308
2022-08-18 19:30:33 - train: epoch 0014, iter [00700, 05004], lr: 0.045984, loss: 1.9270
2022-08-18 19:31:08 - train: epoch 0014, iter [00800, 05004], lr: 0.045859, loss: 1.9025
2022-08-18 19:31:42 - train: epoch 0014, iter [00900, 05004], lr: 0.045734, loss: 1.8561
2022-08-18 19:32:17 - train: epoch 0014, iter [01000, 05004], lr: 0.045608, loss: 1.8977
2022-08-18 19:32:51 - train: epoch 0014, iter [01100, 05004], lr: 0.045483, loss: 2.0365
2022-08-18 19:33:26 - train: epoch 0014, iter [01200, 05004], lr: 0.045358, loss: 2.0406
2022-08-18 19:34:01 - train: epoch 0014, iter [01300, 05004], lr: 0.045233, loss: 2.1441
2022-08-18 19:34:36 - train: epoch 0014, iter [01400, 05004], lr: 0.045108, loss: 2.0592
2022-08-18 19:35:11 - train: epoch 0014, iter [01500, 05004], lr: 0.044983, loss: 2.1027
2022-08-18 19:35:46 - train: epoch 0014, iter [01600, 05004], lr: 0.044858, loss: 1.8697
2022-08-18 19:36:21 - train: epoch 0014, iter [01700, 05004], lr: 0.044734, loss: 2.0172
2022-08-18 19:36:56 - train: epoch 0014, iter [01800, 05004], lr: 0.044609, loss: 2.1016
2022-08-18 19:37:31 - train: epoch 0014, iter [01900, 05004], lr: 0.044484, loss: 1.8590
2022-08-18 19:38:06 - train: epoch 0014, iter [02000, 05004], lr: 0.044359, loss: 1.8982
2022-08-18 19:38:41 - train: epoch 0014, iter [02100, 05004], lr: 0.044234, loss: 2.0588
2022-08-18 19:39:16 - train: epoch 0014, iter [02200, 05004], lr: 0.044110, loss: 2.0607
2022-08-18 19:39:51 - train: epoch 0014, iter [02300, 05004], lr: 0.043985, loss: 2.0258
2022-08-18 19:40:26 - train: epoch 0014, iter [02400, 05004], lr: 0.043860, loss: 1.8530
2022-08-18 19:41:00 - train: epoch 0014, iter [02500, 05004], lr: 0.043736, loss: 1.6919
2022-08-18 19:41:35 - train: epoch 0014, iter [02600, 05004], lr: 0.043611, loss: 1.8766
2022-08-18 19:42:10 - train: epoch 0014, iter [02700, 05004], lr: 0.043487, loss: 1.7416
2022-08-18 19:42:45 - train: epoch 0014, iter [02800, 05004], lr: 0.043362, loss: 2.2689
2022-08-18 19:43:20 - train: epoch 0014, iter [02900, 05004], lr: 0.043238, loss: 2.0270
2022-08-18 19:43:55 - train: epoch 0014, iter [03000, 05004], lr: 0.043113, loss: 1.9541
2022-08-18 19:44:31 - train: epoch 0014, iter [03100, 05004], lr: 0.042989, loss: 1.9555
2022-08-18 19:45:05 - train: epoch 0014, iter [03200, 05004], lr: 0.042865, loss: 2.1445
2022-08-18 19:45:40 - train: epoch 0014, iter [03300, 05004], lr: 0.042741, loss: 1.9090
2022-08-18 19:46:14 - train: epoch 0014, iter [03400, 05004], lr: 0.042616, loss: 2.0403
2022-08-18 19:46:48 - train: epoch 0014, iter [03500, 05004], lr: 0.042492, loss: 2.0210
2022-08-18 19:47:22 - train: epoch 0014, iter [03600, 05004], lr: 0.042368, loss: 1.8658
2022-08-18 19:47:57 - train: epoch 0014, iter [03700, 05004], lr: 0.042244, loss: 1.8760
2022-08-18 19:48:31 - train: epoch 0014, iter [03800, 05004], lr: 0.042120, loss: 1.9467
2022-08-18 19:49:05 - train: epoch 0014, iter [03900, 05004], lr: 0.041996, loss: 1.8562
2022-08-18 19:49:40 - train: epoch 0014, iter [04000, 05004], lr: 0.041872, loss: 1.9072
2022-08-18 19:50:14 - train: epoch 0014, iter [04100, 05004], lr: 0.041748, loss: 2.0762
2022-08-18 19:50:49 - train: epoch 0014, iter [04200, 05004], lr: 0.041624, loss: 2.0212
2022-08-18 19:51:23 - train: epoch 0014, iter [04300, 05004], lr: 0.041501, loss: 1.6749
2022-08-18 19:51:58 - train: epoch 0014, iter [04400, 05004], lr: 0.041377, loss: 1.8730
2022-08-18 19:52:32 - train: epoch 0014, iter [04500, 05004], lr: 0.041253, loss: 2.0054
2022-08-18 19:53:07 - train: epoch 0014, iter [04600, 05004], lr: 0.041130, loss: 1.8858
2022-08-18 19:53:41 - train: epoch 0014, iter [04700, 05004], lr: 0.041006, loss: 1.9226
2022-08-18 19:54:16 - train: epoch 0014, iter [04800, 05004], lr: 0.040883, loss: 1.9579
2022-08-18 19:54:50 - train: epoch 0014, iter [04900, 05004], lr: 0.040759, loss: 1.7590
2022-08-18 19:55:24 - train: epoch 0014, iter [05000, 05004], lr: 0.040636, loss: 1.9842
2022-08-18 19:55:25 - train: epoch 014, train_loss: 1.9563
2022-08-18 19:56:41 - eval: epoch: 014, acc1: 60.250%, acc5: 83.828%, test_loss: 1.6603, per_image_load_time: 2.259ms, per_image_inference_time: 0.592ms
2022-08-18 19:56:41 - until epoch: 014, best_acc1: 60.250%
2022-08-18 19:56:41 - epoch 015 lr: 0.040630
2022-08-18 19:57:21 - train: epoch 0015, iter [00100, 05004], lr: 0.040508, loss: 1.4787
2022-08-18 19:57:54 - train: epoch 0015, iter [00200, 05004], lr: 0.040384, loss: 2.0743
2022-08-18 19:58:27 - train: epoch 0015, iter [00300, 05004], lr: 0.040261, loss: 2.0105
2022-08-18 19:59:01 - train: epoch 0015, iter [00400, 05004], lr: 0.040138, loss: 2.1495
2022-08-18 19:59:35 - train: epoch 0015, iter [00500, 05004], lr: 0.040015, loss: 1.8287
2022-08-18 20:00:09 - train: epoch 0015, iter [00600, 05004], lr: 0.039892, loss: 1.9439
2022-08-18 20:00:42 - train: epoch 0015, iter [00700, 05004], lr: 0.039769, loss: 2.0426
2022-08-18 20:01:16 - train: epoch 0015, iter [00800, 05004], lr: 0.039646, loss: 1.8119
2022-08-18 20:01:49 - train: epoch 0015, iter [00900, 05004], lr: 0.039523, loss: 1.9156
2022-08-18 20:02:23 - train: epoch 0015, iter [01000, 05004], lr: 0.039401, loss: 1.8100
2022-08-18 20:02:56 - train: epoch 0015, iter [01100, 05004], lr: 0.039278, loss: 1.7553
2022-08-18 20:03:30 - train: epoch 0015, iter [01200, 05004], lr: 0.039155, loss: 1.8909
2022-08-18 20:04:04 - train: epoch 0015, iter [01300, 05004], lr: 0.039033, loss: 2.1017
2022-08-18 20:04:38 - train: epoch 0015, iter [01400, 05004], lr: 0.038910, loss: 1.7057
2022-08-18 20:05:12 - train: epoch 0015, iter [01500, 05004], lr: 0.038788, loss: 1.9855
2022-08-18 20:05:46 - train: epoch 0015, iter [01600, 05004], lr: 0.038666, loss: 1.9125
2022-08-18 20:06:20 - train: epoch 0015, iter [01700, 05004], lr: 0.038543, loss: 1.9627
2022-08-18 20:06:54 - train: epoch 0015, iter [01800, 05004], lr: 0.038421, loss: 1.9610
2022-08-18 20:07:28 - train: epoch 0015, iter [01900, 05004], lr: 0.038299, loss: 2.0120
2022-08-18 20:08:02 - train: epoch 0015, iter [02000, 05004], lr: 0.038177, loss: 1.6439
2022-08-18 20:08:37 - train: epoch 0015, iter [02100, 05004], lr: 0.038055, loss: 1.6251
2022-08-18 20:09:11 - train: epoch 0015, iter [02200, 05004], lr: 0.037933, loss: 2.0572
2022-08-18 20:09:45 - train: epoch 0015, iter [02300, 05004], lr: 0.037811, loss: 1.9570
2022-08-18 20:10:20 - train: epoch 0015, iter [02400, 05004], lr: 0.037690, loss: 2.0126
2022-08-18 20:10:54 - train: epoch 0015, iter [02500, 05004], lr: 0.037568, loss: 2.1001
2022-08-18 20:11:28 - train: epoch 0015, iter [02600, 05004], lr: 0.037446, loss: 1.7506
2022-08-18 20:12:03 - train: epoch 0015, iter [02700, 05004], lr: 0.037325, loss: 2.0190
2022-08-18 20:12:37 - train: epoch 0015, iter [02800, 05004], lr: 0.037203, loss: 1.9993
2022-08-18 20:13:11 - train: epoch 0015, iter [02900, 05004], lr: 0.037082, loss: 1.9849
2022-08-18 20:13:45 - train: epoch 0015, iter [03000, 05004], lr: 0.036961, loss: 1.9642
2022-08-18 20:14:19 - train: epoch 0015, iter [03100, 05004], lr: 0.036840, loss: 1.8698
2022-08-18 20:14:53 - train: epoch 0015, iter [03200, 05004], lr: 0.036719, loss: 2.0177
2022-08-18 20:15:28 - train: epoch 0015, iter [03300, 05004], lr: 0.036598, loss: 1.7213
2022-08-18 20:16:02 - train: epoch 0015, iter [03400, 05004], lr: 0.036477, loss: 1.7823
2022-08-18 20:16:37 - train: epoch 0015, iter [03500, 05004], lr: 0.036356, loss: 2.1998
2022-08-18 20:17:11 - train: epoch 0015, iter [03600, 05004], lr: 0.036235, loss: 1.8061
2022-08-18 20:17:46 - train: epoch 0015, iter [03700, 05004], lr: 0.036114, loss: 1.6000
2022-08-18 20:18:21 - train: epoch 0015, iter [03800, 05004], lr: 0.035994, loss: 1.6306
2022-08-18 20:18:56 - train: epoch 0015, iter [03900, 05004], lr: 0.035873, loss: 1.8373
2022-08-18 20:19:30 - train: epoch 0015, iter [04000, 05004], lr: 0.035753, loss: 1.7419
2022-08-18 20:20:05 - train: epoch 0015, iter [04100, 05004], lr: 0.035633, loss: 1.8283
2022-08-18 20:20:40 - train: epoch 0015, iter [04200, 05004], lr: 0.035512, loss: 1.6978
2022-08-18 20:21:15 - train: epoch 0015, iter [04300, 05004], lr: 0.035392, loss: 1.7233
2022-08-18 20:21:50 - train: epoch 0015, iter [04400, 05004], lr: 0.035272, loss: 1.7859
2022-08-18 20:22:25 - train: epoch 0015, iter [04500, 05004], lr: 0.035152, loss: 1.8876
2022-08-18 20:23:00 - train: epoch 0015, iter [04600, 05004], lr: 0.035032, loss: 2.0256
2022-08-18 20:23:35 - train: epoch 0015, iter [04700, 05004], lr: 0.034913, loss: 1.7798
2022-08-18 20:24:11 - train: epoch 0015, iter [04800, 05004], lr: 0.034793, loss: 1.8707
2022-08-18 20:24:46 - train: epoch 0015, iter [04900, 05004], lr: 0.034673, loss: 2.0294
2022-08-18 20:25:20 - train: epoch 0015, iter [05000, 05004], lr: 0.034554, loss: 2.0929
2022-08-18 20:25:21 - train: epoch 015, train_loss: 1.8915
2022-08-18 20:26:39 - eval: epoch: 015, acc1: 61.382%, acc5: 84.200%, test_loss: 1.6197, per_image_load_time: 2.455ms, per_image_inference_time: 0.576ms
2022-08-18 20:26:39 - until epoch: 015, best_acc1: 61.382%
2022-08-18 20:26:39 - epoch 016 lr: 0.034548
2022-08-18 20:27:21 - train: epoch 0016, iter [00100, 05004], lr: 0.034430, loss: 2.0010
2022-08-18 20:27:55 - train: epoch 0016, iter [00200, 05004], lr: 0.034311, loss: 1.7393
2022-08-18 20:28:30 - train: epoch 0016, iter [00300, 05004], lr: 0.034191, loss: 1.8435
2022-08-18 20:29:05 - train: epoch 0016, iter [00400, 05004], lr: 0.034072, loss: 2.0361
2022-08-18 20:29:40 - train: epoch 0016, iter [00500, 05004], lr: 0.033953, loss: 1.8748
2022-08-18 20:30:15 - train: epoch 0016, iter [00600, 05004], lr: 0.033834, loss: 1.6624
2022-08-18 20:30:50 - train: epoch 0016, iter [00700, 05004], lr: 0.033716, loss: 1.7678
2022-08-18 20:31:25 - train: epoch 0016, iter [00800, 05004], lr: 0.033597, loss: 1.8823
2022-08-18 20:31:59 - train: epoch 0016, iter [00900, 05004], lr: 0.033478, loss: 1.8700
2022-08-18 20:32:34 - train: epoch 0016, iter [01000, 05004], lr: 0.033360, loss: 1.7730
2022-08-18 20:33:09 - train: epoch 0016, iter [01100, 05004], lr: 0.033242, loss: 1.6131
2022-08-18 20:33:44 - train: epoch 0016, iter [01200, 05004], lr: 0.033123, loss: 1.8392
2022-08-18 20:34:19 - train: epoch 0016, iter [01300, 05004], lr: 0.033005, loss: 1.9061
2022-08-18 20:34:54 - train: epoch 0016, iter [01400, 05004], lr: 0.032887, loss: 1.5748
2022-08-18 20:35:28 - train: epoch 0016, iter [01500, 05004], lr: 0.032769, loss: 1.9650
2022-08-18 20:36:04 - train: epoch 0016, iter [01600, 05004], lr: 0.032651, loss: 1.9170
2022-08-18 20:36:39 - train: epoch 0016, iter [01700, 05004], lr: 0.032534, loss: 1.9004
2022-08-18 20:37:13 - train: epoch 0016, iter [01800, 05004], lr: 0.032416, loss: 2.0118
2022-08-18 20:37:48 - train: epoch 0016, iter [01900, 05004], lr: 0.032299, loss: 1.9021
2022-08-18 20:38:23 - train: epoch 0016, iter [02000, 05004], lr: 0.032181, loss: 1.6857
2022-08-18 20:38:58 - train: epoch 0016, iter [02100, 05004], lr: 0.032064, loss: 1.9363
2022-08-18 20:39:33 - train: epoch 0016, iter [02200, 05004], lr: 0.031947, loss: 1.9993
2022-08-18 20:40:08 - train: epoch 0016, iter [02300, 05004], lr: 0.031830, loss: 1.7990
2022-08-18 20:40:42 - train: epoch 0016, iter [02400, 05004], lr: 0.031713, loss: 1.9056
2022-08-18 20:41:17 - train: epoch 0016, iter [02500, 05004], lr: 0.031596, loss: 1.8629
2022-08-18 20:41:52 - train: epoch 0016, iter [02600, 05004], lr: 0.031479, loss: 2.0594
2022-08-18 20:42:27 - train: epoch 0016, iter [02700, 05004], lr: 0.031363, loss: 1.7379
2022-08-18 20:43:02 - train: epoch 0016, iter [02800, 05004], lr: 0.031246, loss: 1.7752
2022-08-18 20:43:37 - train: epoch 0016, iter [02900, 05004], lr: 0.031130, loss: 1.8651
2022-08-18 20:44:12 - train: epoch 0016, iter [03000, 05004], lr: 0.031014, loss: 2.0384
2022-08-18 20:44:47 - train: epoch 0016, iter [03100, 05004], lr: 0.030898, loss: 1.8602
2022-08-18 20:45:22 - train: epoch 0016, iter [03200, 05004], lr: 0.030782, loss: 1.9161
2022-08-18 20:45:57 - train: epoch 0016, iter [03300, 05004], lr: 0.030666, loss: 1.9216
2022-08-18 20:46:32 - train: epoch 0016, iter [03400, 05004], lr: 0.030550, loss: 1.8042
2022-08-18 20:47:07 - train: epoch 0016, iter [03500, 05004], lr: 0.030435, loss: 1.6021
2022-08-18 20:47:42 - train: epoch 0016, iter [03600, 05004], lr: 0.030319, loss: 1.5912
2022-08-18 20:48:17 - train: epoch 0016, iter [03700, 05004], lr: 0.030204, loss: 1.9709
2022-08-18 20:48:52 - train: epoch 0016, iter [03800, 05004], lr: 0.030088, loss: 1.9808
2022-08-18 20:49:27 - train: epoch 0016, iter [03900, 05004], lr: 0.029973, loss: 1.7867
2022-08-18 20:50:02 - train: epoch 0016, iter [04000, 05004], lr: 0.029858, loss: 1.8851
2022-08-18 20:50:37 - train: epoch 0016, iter [04100, 05004], lr: 0.029743, loss: 1.7148
2022-08-18 20:51:12 - train: epoch 0016, iter [04200, 05004], lr: 0.029629, loss: 1.8765
2022-08-18 20:51:47 - train: epoch 0016, iter [04300, 05004], lr: 0.029514, loss: 1.7304
2022-08-18 20:52:22 - train: epoch 0016, iter [04400, 05004], lr: 0.029400, loss: 1.6457
2022-08-18 20:52:57 - train: epoch 0016, iter [04500, 05004], lr: 0.029285, loss: 1.9338
2022-08-18 20:53:32 - train: epoch 0016, iter [04600, 05004], lr: 0.029171, loss: 1.5682
2022-08-18 20:54:07 - train: epoch 0016, iter [04700, 05004], lr: 0.029057, loss: 1.9059
2022-08-18 20:54:42 - train: epoch 0016, iter [04800, 05004], lr: 0.028943, loss: 1.7887
2022-08-18 20:55:17 - train: epoch 0016, iter [04900, 05004], lr: 0.028829, loss: 1.7406
2022-08-18 20:55:51 - train: epoch 0016, iter [05000, 05004], lr: 0.028716, loss: 1.9105
2022-08-18 20:55:53 - train: epoch 016, train_loss: 1.8292
2022-08-18 20:57:10 - eval: epoch: 016, acc1: 63.112%, acc5: 85.268%, test_loss: 1.5375, per_image_load_time: 2.380ms, per_image_inference_time: 0.569ms
2022-08-18 20:57:10 - until epoch: 016, best_acc1: 63.112%
2022-08-18 20:57:10 - epoch 017 lr: 0.028710
2022-08-18 20:57:52 - train: epoch 0017, iter [00100, 05004], lr: 0.028597, loss: 1.8914
2022-08-18 20:58:27 - train: epoch 0017, iter [00200, 05004], lr: 0.028484, loss: 1.6171
2022-08-18 20:59:01 - train: epoch 0017, iter [00300, 05004], lr: 0.028371, loss: 2.0250
2022-08-18 20:59:36 - train: epoch 0017, iter [00400, 05004], lr: 0.028258, loss: 1.4608
2022-08-18 21:00:12 - train: epoch 0017, iter [00500, 05004], lr: 0.028145, loss: 1.7550
2022-08-18 21:00:46 - train: epoch 0017, iter [00600, 05004], lr: 0.028032, loss: 1.9880
2022-08-18 21:01:21 - train: epoch 0017, iter [00700, 05004], lr: 0.027919, loss: 1.6024
2022-08-18 21:01:56 - train: epoch 0017, iter [00800, 05004], lr: 0.027806, loss: 1.6370
2022-08-18 21:02:31 - train: epoch 0017, iter [00900, 05004], lr: 0.027694, loss: 1.6746
2022-08-18 21:03:06 - train: epoch 0017, iter [01000, 05004], lr: 0.027582, loss: 1.6678
2022-08-18 21:03:42 - train: epoch 0017, iter [01100, 05004], lr: 0.027470, loss: 1.8609
2022-08-18 21:04:17 - train: epoch 0017, iter [01200, 05004], lr: 0.027358, loss: 1.6002
2022-08-18 21:04:52 - train: epoch 0017, iter [01300, 05004], lr: 0.027246, loss: 1.8020
2022-08-18 21:05:27 - train: epoch 0017, iter [01400, 05004], lr: 0.027134, loss: 1.7852
2022-08-18 21:06:02 - train: epoch 0017, iter [01500, 05004], lr: 0.027022, loss: 1.5982
2022-08-18 21:06:38 - train: epoch 0017, iter [01600, 05004], lr: 0.026911, loss: 1.6941
2022-08-18 21:07:13 - train: epoch 0017, iter [01700, 05004], lr: 0.026800, loss: 1.7092
2022-08-18 21:07:49 - train: epoch 0017, iter [01800, 05004], lr: 0.026688, loss: 1.6715
2022-08-18 21:08:24 - train: epoch 0017, iter [01900, 05004], lr: 0.026577, loss: 1.8488
2022-08-18 21:08:59 - train: epoch 0017, iter [02000, 05004], lr: 0.026467, loss: 1.6654
2022-08-18 21:09:34 - train: epoch 0017, iter [02100, 05004], lr: 0.026356, loss: 1.7893
2022-08-18 21:10:09 - train: epoch 0017, iter [02200, 05004], lr: 0.026245, loss: 1.6906
2022-08-18 21:10:44 - train: epoch 0017, iter [02300, 05004], lr: 0.026135, loss: 1.8358
2022-08-18 21:11:19 - train: epoch 0017, iter [02400, 05004], lr: 0.026025, loss: 1.7960
2022-08-18 21:11:54 - train: epoch 0017, iter [02500, 05004], lr: 0.025915, loss: 1.8587
2022-08-18 21:12:29 - train: epoch 0017, iter [02600, 05004], lr: 0.025805, loss: 1.8333
2022-08-18 21:13:04 - train: epoch 0017, iter [02700, 05004], lr: 0.025695, loss: 1.7399
2022-08-18 21:13:39 - train: epoch 0017, iter [02800, 05004], lr: 0.025585, loss: 1.9103
2022-08-18 21:14:14 - train: epoch 0017, iter [02900, 05004], lr: 0.025476, loss: 1.9241
2022-08-18 21:14:49 - train: epoch 0017, iter [03000, 05004], lr: 0.025366, loss: 1.6991
2022-08-18 21:15:24 - train: epoch 0017, iter [03100, 05004], lr: 0.025257, loss: 1.9213
2022-08-18 21:15:59 - train: epoch 0017, iter [03200, 05004], lr: 0.025148, loss: 1.5883
2022-08-18 21:16:34 - train: epoch 0017, iter [03300, 05004], lr: 0.025039, loss: 2.1093
2022-08-18 21:17:09 - train: epoch 0017, iter [03400, 05004], lr: 0.024930, loss: 1.7138
2022-08-18 21:17:45 - train: epoch 0017, iter [03500, 05004], lr: 0.024822, loss: 1.7356
2022-08-18 21:18:20 - train: epoch 0017, iter [03600, 05004], lr: 0.024713, loss: 1.7784
2022-08-18 21:18:55 - train: epoch 0017, iter [03700, 05004], lr: 0.024605, loss: 1.7105
2022-08-18 21:19:30 - train: epoch 0017, iter [03800, 05004], lr: 0.024497, loss: 1.8815
2022-08-18 21:20:05 - train: epoch 0017, iter [03900, 05004], lr: 0.024389, loss: 1.7436
2022-08-18 21:20:40 - train: epoch 0017, iter [04000, 05004], lr: 0.024281, loss: 1.7325
2022-08-18 21:21:16 - train: epoch 0017, iter [04100, 05004], lr: 0.024174, loss: 1.7960
2022-08-18 21:21:51 - train: epoch 0017, iter [04200, 05004], lr: 0.024066, loss: 1.8785
2022-08-18 21:22:26 - train: epoch 0017, iter [04300, 05004], lr: 0.023959, loss: 1.7474
2022-08-18 21:23:01 - train: epoch 0017, iter [04400, 05004], lr: 0.023852, loss: 1.8787
2022-08-18 21:23:36 - train: epoch 0017, iter [04500, 05004], lr: 0.023745, loss: 1.8726
2022-08-18 21:24:12 - train: epoch 0017, iter [04600, 05004], lr: 0.023638, loss: 1.6567
2022-08-18 21:24:47 - train: epoch 0017, iter [04700, 05004], lr: 0.023532, loss: 1.8665
2022-08-18 21:25:22 - train: epoch 0017, iter [04800, 05004], lr: 0.023425, loss: 1.7754
2022-08-18 21:25:57 - train: epoch 0017, iter [04900, 05004], lr: 0.023319, loss: 1.7783
2022-08-18 21:26:31 - train: epoch 0017, iter [05000, 05004], lr: 0.023213, loss: 1.4591
2022-08-18 21:26:33 - train: epoch 017, train_loss: 1.7626
2022-08-18 21:27:48 - eval: epoch: 017, acc1: 64.208%, acc5: 86.094%, test_loss: 1.4829, per_image_load_time: 2.201ms, per_image_inference_time: 0.628ms
2022-08-18 21:27:49 - until epoch: 017, best_acc1: 64.208%
2022-08-18 21:27:49 - epoch 018 lr: 0.023208
2022-08-18 21:28:29 - train: epoch 0018, iter [00100, 05004], lr: 0.023103, loss: 1.6078
2022-08-18 21:29:02 - train: epoch 0018, iter [00200, 05004], lr: 0.022997, loss: 1.9777
2022-08-18 21:29:36 - train: epoch 0018, iter [00300, 05004], lr: 0.022891, loss: 1.7682
2022-08-18 21:30:10 - train: epoch 0018, iter [00400, 05004], lr: 0.022786, loss: 1.9012
2022-08-18 21:30:45 - train: epoch 0018, iter [00500, 05004], lr: 0.022681, loss: 1.8589
2022-08-18 21:31:19 - train: epoch 0018, iter [00600, 05004], lr: 0.022576, loss: 1.6930
2022-08-18 21:31:53 - train: epoch 0018, iter [00700, 05004], lr: 0.022471, loss: 1.6193
2022-08-18 21:32:27 - train: epoch 0018, iter [00800, 05004], lr: 0.022366, loss: 1.7119
2022-08-18 21:33:02 - train: epoch 0018, iter [00900, 05004], lr: 0.022261, loss: 1.6663
2022-08-18 21:33:36 - train: epoch 0018, iter [01000, 05004], lr: 0.022157, loss: 1.5736
2022-08-18 21:34:11 - train: epoch 0018, iter [01100, 05004], lr: 0.022053, loss: 1.8416
2022-08-18 21:34:45 - train: epoch 0018, iter [01200, 05004], lr: 0.021949, loss: 1.6739
2022-08-18 21:35:20 - train: epoch 0018, iter [01300, 05004], lr: 0.021845, loss: 1.8319
2022-08-18 21:35:55 - train: epoch 0018, iter [01400, 05004], lr: 0.021741, loss: 1.7819
2022-08-18 21:36:29 - train: epoch 0018, iter [01500, 05004], lr: 0.021638, loss: 1.6711
2022-08-18 21:37:04 - train: epoch 0018, iter [01600, 05004], lr: 0.021534, loss: 1.5308
2022-08-18 21:37:38 - train: epoch 0018, iter [01700, 05004], lr: 0.021431, loss: 1.7250
2022-08-18 21:38:13 - train: epoch 0018, iter [01800, 05004], lr: 0.021328, loss: 1.6845
2022-08-18 21:38:48 - train: epoch 0018, iter [01900, 05004], lr: 0.021226, loss: 1.6807
2022-08-18 21:39:22 - train: epoch 0018, iter [02000, 05004], lr: 0.021123, loss: 1.7839
2022-08-18 21:39:57 - train: epoch 0018, iter [02100, 05004], lr: 0.021021, loss: 1.7882
2022-08-18 21:40:32 - train: epoch 0018, iter [02200, 05004], lr: 0.020918, loss: 1.7782
2022-08-18 21:41:06 - train: epoch 0018, iter [02300, 05004], lr: 0.020816, loss: 1.7806
2022-08-18 21:41:41 - train: epoch 0018, iter [02400, 05004], lr: 0.020714, loss: 1.6357
2022-08-18 21:42:15 - train: epoch 0018, iter [02500, 05004], lr: 0.020613, loss: 1.5728
2022-08-18 21:42:49 - train: epoch 0018, iter [02600, 05004], lr: 0.020511, loss: 1.5907
2022-08-18 21:43:23 - train: epoch 0018, iter [02700, 05004], lr: 0.020410, loss: 1.8038
2022-08-18 21:43:56 - train: epoch 0018, iter [02800, 05004], lr: 0.020309, loss: 1.5820
2022-08-18 21:44:30 - train: epoch 0018, iter [02900, 05004], lr: 0.020208, loss: 1.6578
2022-08-18 21:45:05 - train: epoch 0018, iter [03000, 05004], lr: 0.020107, loss: 1.7673
2022-08-18 21:45:38 - train: epoch 0018, iter [03100, 05004], lr: 0.020007, loss: 1.9620
2022-08-18 21:46:13 - train: epoch 0018, iter [03200, 05004], lr: 0.019906, loss: 1.5576
2022-08-18 21:46:48 - train: epoch 0018, iter [03300, 05004], lr: 0.019806, loss: 1.4513
2022-08-18 21:47:22 - train: epoch 0018, iter [03400, 05004], lr: 0.019706, loss: 1.7240
2022-08-18 21:47:57 - train: epoch 0018, iter [03500, 05004], lr: 0.019606, loss: 1.7485
2022-08-18 21:48:32 - train: epoch 0018, iter [03600, 05004], lr: 0.019507, loss: 1.6581
2022-08-18 21:49:07 - train: epoch 0018, iter [03700, 05004], lr: 0.019407, loss: 2.0213
2022-08-18 21:49:42 - train: epoch 0018, iter [03800, 05004], lr: 0.019308, loss: 1.9738
2022-08-18 21:50:16 - train: epoch 0018, iter [03900, 05004], lr: 0.019209, loss: 1.8789
2022-08-18 21:50:51 - train: epoch 0018, iter [04000, 05004], lr: 0.019110, loss: 1.6665
2022-08-18 21:51:26 - train: epoch 0018, iter [04100, 05004], lr: 0.019012, loss: 1.7456
2022-08-18 21:52:01 - train: epoch 0018, iter [04200, 05004], lr: 0.018913, loss: 1.5295
2022-08-18 21:52:36 - train: epoch 0018, iter [04300, 05004], lr: 0.018815, loss: 1.6901
2022-08-18 21:53:12 - train: epoch 0018, iter [04400, 05004], lr: 0.018717, loss: 1.4837
2022-08-18 21:53:47 - train: epoch 0018, iter [04500, 05004], lr: 0.018619, loss: 1.5832
2022-08-18 21:54:22 - train: epoch 0018, iter [04600, 05004], lr: 0.018521, loss: 1.6628
2022-08-18 21:54:58 - train: epoch 0018, iter [04700, 05004], lr: 0.018424, loss: 1.8593
2022-08-18 21:55:33 - train: epoch 0018, iter [04800, 05004], lr: 0.018327, loss: 1.6899
2022-08-18 21:56:08 - train: epoch 0018, iter [04900, 05004], lr: 0.018230, loss: 1.5709
2022-08-18 21:56:42 - train: epoch 0018, iter [05000, 05004], lr: 0.018133, loss: 1.7619
2022-08-18 21:56:43 - train: epoch 018, train_loss: 1.6907
2022-08-18 21:58:01 - eval: epoch: 018, acc1: 65.620%, acc5: 87.056%, test_loss: 1.4208, per_image_load_time: 2.436ms, per_image_inference_time: 0.561ms
2022-08-18 21:58:01 - until epoch: 018, best_acc1: 65.620%
2022-08-18 21:58:01 - epoch 019 lr: 0.018128
2022-08-18 21:58:43 - train: epoch 0019, iter [00100, 05004], lr: 0.018032, loss: 1.4588
2022-08-18 21:59:18 - train: epoch 0019, iter [00200, 05004], lr: 0.017936, loss: 1.6472
2022-08-18 21:59:53 - train: epoch 0019, iter [00300, 05004], lr: 0.017839, loss: 1.6564
2022-08-18 22:00:28 - train: epoch 0019, iter [00400, 05004], lr: 0.017743, loss: 1.6777
2022-08-18 22:01:03 - train: epoch 0019, iter [00500, 05004], lr: 0.017648, loss: 1.5708
2022-08-18 22:01:38 - train: epoch 0019, iter [00600, 05004], lr: 0.017552, loss: 1.4664
2022-08-18 22:02:13 - train: epoch 0019, iter [00700, 05004], lr: 0.017457, loss: 1.5236
2022-08-18 22:02:48 - train: epoch 0019, iter [00800, 05004], lr: 0.017361, loss: 1.7157
2022-08-18 22:03:22 - train: epoch 0019, iter [00900, 05004], lr: 0.017266, loss: 1.6852
2022-08-18 22:03:57 - train: epoch 0019, iter [01000, 05004], lr: 0.017171, loss: 1.6255
2022-08-18 22:04:32 - train: epoch 0019, iter [01100, 05004], lr: 0.017077, loss: 1.5072
2022-08-18 22:05:07 - train: epoch 0019, iter [01200, 05004], lr: 0.016982, loss: 1.7424
2022-08-18 22:05:42 - train: epoch 0019, iter [01300, 05004], lr: 0.016888, loss: 1.6755
2022-08-18 22:06:16 - train: epoch 0019, iter [01400, 05004], lr: 0.016794, loss: 1.3695
2022-08-18 22:06:51 - train: epoch 0019, iter [01500, 05004], lr: 0.016701, loss: 1.9440
2022-08-18 22:07:26 - train: epoch 0019, iter [01600, 05004], lr: 0.016607, loss: 1.5482
2022-08-18 22:08:01 - train: epoch 0019, iter [01700, 05004], lr: 0.016514, loss: 1.7378
2022-08-18 22:08:37 - train: epoch 0019, iter [01800, 05004], lr: 0.016420, loss: 1.4471
2022-08-18 22:09:11 - train: epoch 0019, iter [01900, 05004], lr: 0.016328, loss: 1.8505
2022-08-18 22:09:47 - train: epoch 0019, iter [02000, 05004], lr: 0.016235, loss: 1.4579
2022-08-18 22:10:21 - train: epoch 0019, iter [02100, 05004], lr: 0.016142, loss: 1.5106
2022-08-18 22:10:57 - train: epoch 0019, iter [02200, 05004], lr: 0.016050, loss: 1.7888
2022-08-18 22:11:32 - train: epoch 0019, iter [02300, 05004], lr: 0.015958, loss: 1.6114
2022-08-18 22:12:06 - train: epoch 0019, iter [02400, 05004], lr: 0.015866, loss: 1.8089
2022-08-18 22:12:42 - train: epoch 0019, iter [02500, 05004], lr: 0.015774, loss: 1.6199
2022-08-18 22:13:17 - train: epoch 0019, iter [02600, 05004], lr: 0.015683, loss: 1.7110
2022-08-18 22:13:52 - train: epoch 0019, iter [02700, 05004], lr: 0.015592, loss: 1.4803
2022-08-18 22:14:26 - train: epoch 0019, iter [02800, 05004], lr: 0.015501, loss: 1.6576
2022-08-18 22:15:02 - train: epoch 0019, iter [02900, 05004], lr: 0.015410, loss: 1.9382
2022-08-18 22:15:37 - train: epoch 0019, iter [03000, 05004], lr: 0.015320, loss: 1.7404
2022-08-18 22:16:12 - train: epoch 0019, iter [03100, 05004], lr: 0.015229, loss: 1.6661
2022-08-18 22:16:47 - train: epoch 0019, iter [03200, 05004], lr: 0.015139, loss: 1.4150
2022-08-18 22:17:22 - train: epoch 0019, iter [03300, 05004], lr: 0.015049, loss: 1.7854
2022-08-18 22:17:57 - train: epoch 0019, iter [03400, 05004], lr: 0.014959, loss: 1.4522
2022-08-18 22:18:32 - train: epoch 0019, iter [03500, 05004], lr: 0.014870, loss: 1.6452
2022-08-18 22:19:07 - train: epoch 0019, iter [03600, 05004], lr: 0.014781, loss: 1.4650
2022-08-18 22:19:43 - train: epoch 0019, iter [03700, 05004], lr: 0.014692, loss: 1.6317
2022-08-18 22:20:18 - train: epoch 0019, iter [03800, 05004], lr: 0.014603, loss: 1.8445
2022-08-18 22:20:53 - train: epoch 0019, iter [03900, 05004], lr: 0.014514, loss: 1.3621
2022-08-18 22:21:28 - train: epoch 0019, iter [04000, 05004], lr: 0.014426, loss: 1.6662
2022-08-18 22:22:03 - train: epoch 0019, iter [04100, 05004], lr: 0.014338, loss: 1.6445
2022-08-18 22:22:38 - train: epoch 0019, iter [04200, 05004], lr: 0.014250, loss: 1.5962
2022-08-18 22:23:13 - train: epoch 0019, iter [04300, 05004], lr: 0.014162, loss: 1.4554
2022-08-18 22:23:48 - train: epoch 0019, iter [04400, 05004], lr: 0.014075, loss: 1.6361
2022-08-18 22:24:23 - train: epoch 0019, iter [04500, 05004], lr: 0.013988, loss: 1.8369
2022-08-18 22:24:58 - train: epoch 0019, iter [04600, 05004], lr: 0.013901, loss: 1.5590
2022-08-18 22:25:33 - train: epoch 0019, iter [04700, 05004], lr: 0.013814, loss: 1.5706
2022-08-18 22:26:08 - train: epoch 0019, iter [04800, 05004], lr: 0.013727, loss: 1.5021
2022-08-18 22:26:43 - train: epoch 0019, iter [04900, 05004], lr: 0.013641, loss: 1.5195
2022-08-18 22:27:17 - train: epoch 0019, iter [05000, 05004], lr: 0.013555, loss: 1.5142
2022-08-18 22:27:19 - train: epoch 019, train_loss: 1.6179
2022-08-18 22:28:37 - eval: epoch: 019, acc1: 66.772%, acc5: 87.758%, test_loss: 1.3687, per_image_load_time: 2.444ms, per_image_inference_time: 0.570ms
2022-08-18 22:28:37 - until epoch: 019, best_acc1: 66.772%
2022-08-18 22:28:37 - epoch 020 lr: 0.013551
2022-08-18 22:29:18 - train: epoch 0020, iter [00100, 05004], lr: 0.013466, loss: 1.5577
2022-08-18 22:29:53 - train: epoch 0020, iter [00200, 05004], lr: 0.013380, loss: 1.3616
2022-08-18 22:30:28 - train: epoch 0020, iter [00300, 05004], lr: 0.013295, loss: 1.5193
2022-08-18 22:31:03 - train: epoch 0020, iter [00400, 05004], lr: 0.013210, loss: 1.5893
2022-08-18 22:31:38 - train: epoch 0020, iter [00500, 05004], lr: 0.013125, loss: 1.3930
2022-08-18 22:32:13 - train: epoch 0020, iter [00600, 05004], lr: 0.013040, loss: 1.6435
2022-08-18 22:32:48 - train: epoch 0020, iter [00700, 05004], lr: 0.012956, loss: 1.3331
2022-08-18 22:33:23 - train: epoch 0020, iter [00800, 05004], lr: 0.012871, loss: 1.5820
2022-08-18 22:33:58 - train: epoch 0020, iter [00900, 05004], lr: 0.012787, loss: 1.5286
2022-08-18 22:34:33 - train: epoch 0020, iter [01000, 05004], lr: 0.012704, loss: 1.4934
2022-08-18 22:35:07 - train: epoch 0020, iter [01100, 05004], lr: 0.012620, loss: 1.4428
2022-08-18 22:35:40 - train: epoch 0020, iter [01200, 05004], lr: 0.012537, loss: 1.3199
2022-08-18 22:36:14 - train: epoch 0020, iter [01300, 05004], lr: 0.012454, loss: 1.5231
2022-08-18 22:36:49 - train: epoch 0020, iter [01400, 05004], lr: 0.012371, loss: 1.5803
2022-08-18 22:37:23 - train: epoch 0020, iter [01500, 05004], lr: 0.012288, loss: 1.5640
2022-08-18 22:37:58 - train: epoch 0020, iter [01600, 05004], lr: 0.012206, loss: 1.6941
2022-08-18 22:38:32 - train: epoch 0020, iter [01700, 05004], lr: 0.012124, loss: 1.5239
2022-08-18 22:39:07 - train: epoch 0020, iter [01800, 05004], lr: 0.012042, loss: 1.5641
2022-08-18 22:39:41 - train: epoch 0020, iter [01900, 05004], lr: 0.011961, loss: 1.5849
2022-08-18 22:40:15 - train: epoch 0020, iter [02000, 05004], lr: 0.011879, loss: 1.5395
2022-08-18 22:40:50 - train: epoch 0020, iter [02100, 05004], lr: 0.011798, loss: 1.6473
2022-08-18 22:41:25 - train: epoch 0020, iter [02200, 05004], lr: 0.011717, loss: 1.4910
2022-08-18 22:41:59 - train: epoch 0020, iter [02300, 05004], lr: 0.011637, loss: 1.5184
2022-08-18 22:42:34 - train: epoch 0020, iter [02400, 05004], lr: 0.011556, loss: 1.7943
2022-08-18 22:43:09 - train: epoch 0020, iter [02500, 05004], lr: 0.011476, loss: 1.5769
2022-08-18 22:43:44 - train: epoch 0020, iter [02600, 05004], lr: 0.011396, loss: 1.3401
2022-08-18 22:44:18 - train: epoch 0020, iter [02700, 05004], lr: 0.011316, loss: 1.3640
2022-08-18 22:44:53 - train: epoch 0020, iter [02800, 05004], lr: 0.011237, loss: 1.7839
2022-08-18 22:45:28 - train: epoch 0020, iter [02900, 05004], lr: 0.011158, loss: 1.6492
2022-08-18 22:46:03 - train: epoch 0020, iter [03000, 05004], lr: 0.011079, loss: 1.7304
2022-08-18 22:46:37 - train: epoch 0020, iter [03100, 05004], lr: 0.011000, loss: 1.6952
2022-08-18 22:47:12 - train: epoch 0020, iter [03200, 05004], lr: 0.010922, loss: 1.6980
2022-08-18 22:47:46 - train: epoch 0020, iter [03300, 05004], lr: 0.010843, loss: 1.2717
2022-08-18 22:48:21 - train: epoch 0020, iter [03400, 05004], lr: 0.010765, loss: 1.5356
2022-08-18 22:48:55 - train: epoch 0020, iter [03500, 05004], lr: 0.010688, loss: 1.3574
2022-08-18 22:49:30 - train: epoch 0020, iter [03600, 05004], lr: 0.010610, loss: 1.5586
2022-08-18 22:50:04 - train: epoch 0020, iter [03700, 05004], lr: 0.010533, loss: 1.4875
2022-08-18 22:50:38 - train: epoch 0020, iter [03800, 05004], lr: 0.010456, loss: 1.7469
2022-08-18 22:51:12 - train: epoch 0020, iter [03900, 05004], lr: 0.010379, loss: 1.6092
2022-08-18 22:51:46 - train: epoch 0020, iter [04000, 05004], lr: 0.010303, loss: 1.4154
2022-08-18 22:52:20 - train: epoch 0020, iter [04100, 05004], lr: 0.010227, loss: 1.4460
2022-08-18 22:52:55 - train: epoch 0020, iter [04200, 05004], lr: 0.010151, loss: 1.5162
2022-08-18 22:53:29 - train: epoch 0020, iter [04300, 05004], lr: 0.010075, loss: 1.6407
2022-08-18 22:54:03 - train: epoch 0020, iter [04400, 05004], lr: 0.010000, loss: 1.4978
2022-08-18 22:54:37 - train: epoch 0020, iter [04500, 05004], lr: 0.009924, loss: 1.4838
2022-08-18 22:55:12 - train: epoch 0020, iter [04600, 05004], lr: 0.009849, loss: 1.4888
2022-08-18 22:55:46 - train: epoch 0020, iter [04700, 05004], lr: 0.009775, loss: 1.4484
2022-08-18 22:56:21 - train: epoch 0020, iter [04800, 05004], lr: 0.009700, loss: 1.4003
2022-08-18 22:56:56 - train: epoch 0020, iter [04900, 05004], lr: 0.009626, loss: 1.4481
2022-08-18 22:57:29 - train: epoch 0020, iter [05000, 05004], lr: 0.009552, loss: 1.4060
2022-08-18 22:57:31 - train: epoch 020, train_loss: 1.5462
2022-08-18 22:58:47 - eval: epoch: 020, acc1: 68.328%, acc5: 88.784%, test_loss: 1.2947, per_image_load_time: 1.892ms, per_image_inference_time: 0.592ms
2022-08-18 22:58:48 - until epoch: 020, best_acc1: 68.328%
2022-08-18 22:58:48 - epoch 021 lr: 0.009548
2022-08-18 22:59:29 - train: epoch 0021, iter [00100, 05004], lr: 0.009475, loss: 1.5406
2022-08-18 23:00:03 - train: epoch 0021, iter [00200, 05004], lr: 0.009402, loss: 1.5485
2022-08-18 23:00:37 - train: epoch 0021, iter [00300, 05004], lr: 0.009329, loss: 1.4328
2022-08-18 23:01:11 - train: epoch 0021, iter [00400, 05004], lr: 0.009256, loss: 1.6501
2022-08-18 23:01:46 - train: epoch 0021, iter [00500, 05004], lr: 0.009183, loss: 1.3455
2022-08-18 23:02:20 - train: epoch 0021, iter [00600, 05004], lr: 0.009111, loss: 1.2925
2022-08-18 23:02:55 - train: epoch 0021, iter [00700, 05004], lr: 0.009039, loss: 1.4797
2022-08-18 23:03:30 - train: epoch 0021, iter [00800, 05004], lr: 0.008967, loss: 1.5637
2022-08-18 23:04:04 - train: epoch 0021, iter [00900, 05004], lr: 0.008895, loss: 1.4875
2022-08-18 23:04:39 - train: epoch 0021, iter [01000, 05004], lr: 0.008824, loss: 1.2999
2022-08-18 23:05:13 - train: epoch 0021, iter [01100, 05004], lr: 0.008753, loss: 1.2292
2022-08-18 23:05:47 - train: epoch 0021, iter [01200, 05004], lr: 0.008682, loss: 1.4101
2022-08-18 23:06:21 - train: epoch 0021, iter [01300, 05004], lr: 0.008611, loss: 1.4540
2022-08-18 23:06:56 - train: epoch 0021, iter [01400, 05004], lr: 0.008541, loss: 1.3027
2022-08-18 23:07:30 - train: epoch 0021, iter [01500, 05004], lr: 0.008471, loss: 1.3804
2022-08-18 23:08:03 - train: epoch 0021, iter [01600, 05004], lr: 0.008401, loss: 1.3821
2022-08-18 23:08:37 - train: epoch 0021, iter [01700, 05004], lr: 0.008332, loss: 1.5785
2022-08-18 23:09:11 - train: epoch 0021, iter [01800, 05004], lr: 0.008262, loss: 1.4978
2022-08-18 23:09:45 - train: epoch 0021, iter [01900, 05004], lr: 0.008193, loss: 1.6630
2022-08-18 23:10:20 - train: epoch 0021, iter [02000, 05004], lr: 0.008125, loss: 1.5336
2022-08-18 23:10:54 - train: epoch 0021, iter [02100, 05004], lr: 0.008056, loss: 1.3836
2022-08-18 23:11:29 - train: epoch 0021, iter [02200, 05004], lr: 0.007988, loss: 1.4920
2022-08-18 23:12:03 - train: epoch 0021, iter [02300, 05004], lr: 0.007920, loss: 1.5499
2022-08-18 23:12:38 - train: epoch 0021, iter [02400, 05004], lr: 0.007852, loss: 1.2777
2022-08-18 23:13:11 - train: epoch 0021, iter [02500, 05004], lr: 0.007785, loss: 1.4200
2022-08-18 23:13:45 - train: epoch 0021, iter [02600, 05004], lr: 0.007718, loss: 1.6287
2022-08-18 23:14:19 - train: epoch 0021, iter [02700, 05004], lr: 0.007651, loss: 1.4576
2022-08-18 23:14:53 - train: epoch 0021, iter [02800, 05004], lr: 0.007584, loss: 1.4633
2022-08-18 23:15:27 - train: epoch 0021, iter [02900, 05004], lr: 0.007518, loss: 1.3879
2022-08-18 23:16:01 - train: epoch 0021, iter [03000, 05004], lr: 0.007452, loss: 1.4817
2022-08-18 23:16:35 - train: epoch 0021, iter [03100, 05004], lr: 0.007386, loss: 1.4112
2022-08-18 23:17:10 - train: epoch 0021, iter [03200, 05004], lr: 0.007320, loss: 1.4009
2022-08-18 23:17:45 - train: epoch 0021, iter [03300, 05004], lr: 0.007255, loss: 1.5938
2022-08-18 23:18:19 - train: epoch 0021, iter [03400, 05004], lr: 0.007190, loss: 1.6272
2022-08-18 23:18:54 - train: epoch 0021, iter [03500, 05004], lr: 0.007125, loss: 1.5030
2022-08-18 23:19:28 - train: epoch 0021, iter [03600, 05004], lr: 0.007061, loss: 1.3378
2022-08-18 23:20:03 - train: epoch 0021, iter [03700, 05004], lr: 0.006997, loss: 1.3232
2022-08-18 23:20:38 - train: epoch 0021, iter [03800, 05004], lr: 0.006933, loss: 1.5724
2022-08-18 23:21:13 - train: epoch 0021, iter [03900, 05004], lr: 0.006869, loss: 1.3376
2022-08-18 23:21:47 - train: epoch 0021, iter [04000, 05004], lr: 0.006806, loss: 1.6741
2022-08-18 23:22:22 - train: epoch 0021, iter [04100, 05004], lr: 0.006743, loss: 1.4482
2022-08-18 23:22:57 - train: epoch 0021, iter [04200, 05004], lr: 0.006680, loss: 1.3069
2022-08-18 23:23:31 - train: epoch 0021, iter [04300, 05004], lr: 0.006617, loss: 1.3742
2022-08-18 23:24:06 - train: epoch 0021, iter [04400, 05004], lr: 0.006555, loss: 1.5161
2022-08-18 23:24:40 - train: epoch 0021, iter [04500, 05004], lr: 0.006493, loss: 1.4211
2022-08-18 23:25:15 - train: epoch 0021, iter [04600, 05004], lr: 0.006431, loss: 1.5271
2022-08-18 23:25:50 - train: epoch 0021, iter [04700, 05004], lr: 0.006370, loss: 1.6091
2022-08-18 23:26:24 - train: epoch 0021, iter [04800, 05004], lr: 0.006309, loss: 1.6176
2022-08-18 23:26:59 - train: epoch 0021, iter [04900, 05004], lr: 0.006248, loss: 1.2307
2022-08-18 23:27:32 - train: epoch 0021, iter [05000, 05004], lr: 0.006187, loss: 1.3924
2022-08-18 23:27:34 - train: epoch 021, train_loss: 1.4725
2022-08-18 23:28:51 - eval: epoch: 021, acc1: 69.510%, acc5: 89.240%, test_loss: 1.2418, per_image_load_time: 2.425ms, per_image_inference_time: 0.567ms
2022-08-18 23:28:51 - until epoch: 021, best_acc1: 69.510%
2022-08-18 23:28:51 - epoch 022 lr: 0.006184
2022-08-18 23:29:32 - train: epoch 0022, iter [00100, 05004], lr: 0.006124, loss: 1.2492
2022-08-18 23:30:06 - train: epoch 0022, iter [00200, 05004], lr: 0.006064, loss: 1.3835
2022-08-18 23:30:40 - train: epoch 0022, iter [00300, 05004], lr: 0.006004, loss: 1.2943
2022-08-18 23:31:14 - train: epoch 0022, iter [00400, 05004], lr: 0.005945, loss: 1.4480
2022-08-18 23:31:48 - train: epoch 0022, iter [00500, 05004], lr: 0.005886, loss: 1.3616
2022-08-18 23:32:23 - train: epoch 0022, iter [00600, 05004], lr: 0.005827, loss: 1.3817
2022-08-18 23:32:57 - train: epoch 0022, iter [00700, 05004], lr: 0.005768, loss: 1.4395
2022-08-18 23:33:31 - train: epoch 0022, iter [00800, 05004], lr: 0.005710, loss: 1.6704
2022-08-18 23:34:06 - train: epoch 0022, iter [00900, 05004], lr: 0.005651, loss: 1.6460
2022-08-18 23:34:41 - train: epoch 0022, iter [01000, 05004], lr: 0.005594, loss: 1.4873
2022-08-18 23:35:15 - train: epoch 0022, iter [01100, 05004], lr: 0.005536, loss: 1.4304
2022-08-18 23:35:50 - train: epoch 0022, iter [01200, 05004], lr: 0.005479, loss: 1.1802
2022-08-18 23:36:24 - train: epoch 0022, iter [01300, 05004], lr: 0.005422, loss: 1.5120
2022-08-18 23:36:59 - train: epoch 0022, iter [01400, 05004], lr: 0.005365, loss: 1.3472
2022-08-18 23:37:34 - train: epoch 0022, iter [01500, 05004], lr: 0.005309, loss: 1.4501
2022-08-18 23:38:09 - train: epoch 0022, iter [01600, 05004], lr: 0.005252, loss: 1.3477
2022-08-18 23:38:44 - train: epoch 0022, iter [01700, 05004], lr: 0.005197, loss: 1.6085
2022-08-18 23:39:18 - train: epoch 0022, iter [01800, 05004], lr: 0.005141, loss: 1.5321
2022-08-18 23:39:53 - train: epoch 0022, iter [01900, 05004], lr: 0.005086, loss: 1.3191
2022-08-18 23:40:27 - train: epoch 0022, iter [02000, 05004], lr: 0.005031, loss: 1.4861
2022-08-18 23:41:02 - train: epoch 0022, iter [02100, 05004], lr: 0.004976, loss: 1.3372
2022-08-18 23:41:36 - train: epoch 0022, iter [02200, 05004], lr: 0.004921, loss: 1.1475
2022-08-18 23:42:11 - train: epoch 0022, iter [02300, 05004], lr: 0.004867, loss: 1.5053
2022-08-18 23:42:46 - train: epoch 0022, iter [02400, 05004], lr: 0.004813, loss: 1.6010
2022-08-18 23:43:21 - train: epoch 0022, iter [02500, 05004], lr: 0.004760, loss: 1.4630
2022-08-18 23:43:55 - train: epoch 0022, iter [02600, 05004], lr: 0.004706, loss: 1.3121
2022-08-18 23:44:30 - train: epoch 0022, iter [02700, 05004], lr: 0.004653, loss: 1.3205
2022-08-18 23:45:05 - train: epoch 0022, iter [02800, 05004], lr: 0.004601, loss: 1.5891
2022-08-18 23:45:40 - train: epoch 0022, iter [02900, 05004], lr: 0.004548, loss: 1.1894
2022-08-18 23:46:14 - train: epoch 0022, iter [03000, 05004], lr: 0.004496, loss: 1.4802
2022-08-18 23:46:49 - train: epoch 0022, iter [03100, 05004], lr: 0.004444, loss: 1.5559
2022-08-18 23:47:23 - train: epoch 0022, iter [03200, 05004], lr: 0.004392, loss: 1.4128
2022-08-18 23:47:57 - train: epoch 0022, iter [03300, 05004], lr: 0.004341, loss: 1.3451
2022-08-18 23:48:32 - train: epoch 0022, iter [03400, 05004], lr: 0.004290, loss: 1.3855
2022-08-18 23:49:06 - train: epoch 0022, iter [03500, 05004], lr: 0.004239, loss: 1.3842
2022-08-18 23:49:41 - train: epoch 0022, iter [03600, 05004], lr: 0.004189, loss: 1.3553
2022-08-18 23:50:16 - train: epoch 0022, iter [03700, 05004], lr: 0.004139, loss: 1.4785
2022-08-18 23:50:51 - train: epoch 0022, iter [03800, 05004], lr: 0.004089, loss: 1.6532
2022-08-18 23:51:26 - train: epoch 0022, iter [03900, 05004], lr: 0.004039, loss: 1.2216
2022-08-18 23:52:00 - train: epoch 0022, iter [04000, 05004], lr: 0.003990, loss: 1.5508
2022-08-18 23:52:35 - train: epoch 0022, iter [04100, 05004], lr: 0.003941, loss: 1.2319
2022-08-18 23:53:10 - train: epoch 0022, iter [04200, 05004], lr: 0.003892, loss: 1.3701
2022-08-18 23:53:44 - train: epoch 0022, iter [04300, 05004], lr: 0.003844, loss: 1.4801
2022-08-18 23:54:19 - train: epoch 0022, iter [04400, 05004], lr: 0.003796, loss: 1.3267
2022-08-18 23:54:54 - train: epoch 0022, iter [04500, 05004], lr: 0.003748, loss: 1.3819
2022-08-18 23:55:28 - train: epoch 0022, iter [04600, 05004], lr: 0.003700, loss: 1.5423
2022-08-18 23:56:03 - train: epoch 0022, iter [04700, 05004], lr: 0.003653, loss: 1.3392
2022-08-18 23:56:38 - train: epoch 0022, iter [04800, 05004], lr: 0.003606, loss: 1.2289
2022-08-18 23:57:12 - train: epoch 0022, iter [04900, 05004], lr: 0.003559, loss: 1.4285
2022-08-18 23:57:46 - train: epoch 0022, iter [05000, 05004], lr: 0.003513, loss: 1.3912
2022-08-18 23:57:48 - train: epoch 022, train_loss: 1.4042
2022-08-18 23:59:05 - eval: epoch: 022, acc1: 70.560%, acc5: 89.726%, test_loss: 1.2044, per_image_load_time: 2.172ms, per_image_inference_time: 0.579ms
2022-08-18 23:59:05 - until epoch: 022, best_acc1: 70.560%
2022-08-18 23:59:05 - epoch 023 lr: 0.003511
2022-08-18 23:59:46 - train: epoch 0023, iter [00100, 05004], lr: 0.003465, loss: 1.2449
2022-08-19 00:00:20 - train: epoch 0023, iter [00200, 05004], lr: 0.003419, loss: 1.2710
2022-08-19 00:00:55 - train: epoch 0023, iter [00300, 05004], lr: 0.003374, loss: 1.2925
2022-08-19 00:01:29 - train: epoch 0023, iter [00400, 05004], lr: 0.003329, loss: 1.5266
2022-08-19 00:02:03 - train: epoch 0023, iter [00500, 05004], lr: 0.003284, loss: 1.4302
2022-08-19 00:02:38 - train: epoch 0023, iter [00600, 05004], lr: 0.003239, loss: 1.2652
2022-08-19 00:03:12 - train: epoch 0023, iter [00700, 05004], lr: 0.003195, loss: 1.3557
2022-08-19 00:03:47 - train: epoch 0023, iter [00800, 05004], lr: 0.003151, loss: 1.3858
2022-08-19 00:04:21 - train: epoch 0023, iter [00900, 05004], lr: 0.003107, loss: 1.4927
2022-08-19 00:04:56 - train: epoch 0023, iter [01000, 05004], lr: 0.003064, loss: 1.1615
2022-08-19 00:05:31 - train: epoch 0023, iter [01100, 05004], lr: 0.003021, loss: 1.3899
2022-08-19 00:06:05 - train: epoch 0023, iter [01200, 05004], lr: 0.002978, loss: 1.2449
2022-08-19 00:06:40 - train: epoch 0023, iter [01300, 05004], lr: 0.002935, loss: 1.4063
2022-08-19 00:07:15 - train: epoch 0023, iter [01400, 05004], lr: 0.002893, loss: 1.4633
2022-08-19 00:07:50 - train: epoch 0023, iter [01500, 05004], lr: 0.002851, loss: 1.1513
2022-08-19 00:08:25 - train: epoch 0023, iter [01600, 05004], lr: 0.002809, loss: 1.2196
2022-08-19 00:09:00 - train: epoch 0023, iter [01700, 05004], lr: 0.002768, loss: 1.4740
2022-08-19 00:09:34 - train: epoch 0023, iter [01800, 05004], lr: 0.002727, loss: 1.3735
2022-08-19 00:10:09 - train: epoch 0023, iter [01900, 05004], lr: 0.002686, loss: 1.3062
2022-08-19 00:10:44 - train: epoch 0023, iter [02000, 05004], lr: 0.002646, loss: 1.2319
2022-08-19 00:11:19 - train: epoch 0023, iter [02100, 05004], lr: 0.002606, loss: 1.3794
2022-08-19 00:11:53 - train: epoch 0023, iter [02200, 05004], lr: 0.002566, loss: 1.3092
2022-08-19 00:12:28 - train: epoch 0023, iter [02300, 05004], lr: 0.002526, loss: 1.3112
2022-08-19 00:13:03 - train: epoch 0023, iter [02400, 05004], lr: 0.002487, loss: 1.2697
2022-08-19 00:13:38 - train: epoch 0023, iter [02500, 05004], lr: 0.002448, loss: 1.3182
2022-08-19 00:14:12 - train: epoch 0023, iter [02600, 05004], lr: 0.002409, loss: 1.5514
2022-08-19 00:14:47 - train: epoch 0023, iter [02700, 05004], lr: 0.002371, loss: 1.4069
2022-08-19 00:15:22 - train: epoch 0023, iter [02800, 05004], lr: 0.002333, loss: 1.3424
2022-08-19 00:15:56 - train: epoch 0023, iter [02900, 05004], lr: 0.002295, loss: 1.3739
2022-08-19 00:16:31 - train: epoch 0023, iter [03000, 05004], lr: 0.002258, loss: 1.3271
2022-08-19 00:17:06 - train: epoch 0023, iter [03100, 05004], lr: 0.002221, loss: 1.4725
2022-08-19 00:17:41 - train: epoch 0023, iter [03200, 05004], lr: 0.002184, loss: 1.4222
2022-08-19 00:18:15 - train: epoch 0023, iter [03300, 05004], lr: 0.002147, loss: 1.4034
2022-08-19 00:18:50 - train: epoch 0023, iter [03400, 05004], lr: 0.002111, loss: 1.4597
2022-08-19 00:19:25 - train: epoch 0023, iter [03500, 05004], lr: 0.002075, loss: 1.4127
2022-08-19 00:19:59 - train: epoch 0023, iter [03600, 05004], lr: 0.002039, loss: 1.2922
2022-08-19 00:20:34 - train: epoch 0023, iter [03700, 05004], lr: 0.002004, loss: 1.1044
2022-08-19 00:21:09 - train: epoch 0023, iter [03800, 05004], lr: 0.001969, loss: 1.4232
2022-08-19 00:21:44 - train: epoch 0023, iter [03900, 05004], lr: 0.001934, loss: 1.5300
2022-08-19 00:22:18 - train: epoch 0023, iter [04000, 05004], lr: 0.001900, loss: 1.2179
2022-08-19 00:22:53 - train: epoch 0023, iter [04100, 05004], lr: 0.001866, loss: 1.3672
2022-08-19 00:23:28 - train: epoch 0023, iter [04200, 05004], lr: 0.001832, loss: 1.2037
2022-08-19 00:24:03 - train: epoch 0023, iter [04300, 05004], lr: 0.001798, loss: 1.3742
2022-08-19 00:24:37 - train: epoch 0023, iter [04400, 05004], lr: 0.001765, loss: 1.3380
2022-08-19 00:25:12 - train: epoch 0023, iter [04500, 05004], lr: 0.001732, loss: 1.2262
2022-08-19 00:25:46 - train: epoch 0023, iter [04600, 05004], lr: 0.001699, loss: 1.4139
2022-08-19 00:26:21 - train: epoch 0023, iter [04700, 05004], lr: 0.001667, loss: 1.1421
2022-08-19 00:26:56 - train: epoch 0023, iter [04800, 05004], lr: 0.001635, loss: 1.1367
2022-08-19 00:27:31 - train: epoch 0023, iter [04900, 05004], lr: 0.001603, loss: 1.2351
2022-08-19 00:28:04 - train: epoch 0023, iter [05000, 05004], lr: 0.001572, loss: 1.3392
2022-08-19 00:28:06 - train: epoch 023, train_loss: 1.3492
2022-08-19 00:29:23 - eval: epoch: 023, acc1: 71.272%, acc5: 90.188%, test_loss: 1.1692, per_image_load_time: 2.320ms, per_image_inference_time: 0.572ms
2022-08-19 00:29:23 - until epoch: 023, best_acc1: 71.272%
2022-08-19 00:29:23 - epoch 024 lr: 0.001571
2022-08-19 00:30:04 - train: epoch 0024, iter [00100, 05004], lr: 0.001540, loss: 1.3049
2022-08-19 00:30:38 - train: epoch 0024, iter [00200, 05004], lr: 0.001509, loss: 1.4266
2022-08-19 00:31:12 - train: epoch 0024, iter [00300, 05004], lr: 0.001479, loss: 1.1921
2022-08-19 00:31:47 - train: epoch 0024, iter [00400, 05004], lr: 0.001448, loss: 1.3584
2022-08-19 00:32:22 - train: epoch 0024, iter [00500, 05004], lr: 0.001419, loss: 1.3640
2022-08-19 00:32:57 - train: epoch 0024, iter [00600, 05004], lr: 0.001389, loss: 1.3359
2022-08-19 00:33:31 - train: epoch 0024, iter [00700, 05004], lr: 0.001360, loss: 1.3911
2022-08-19 00:34:06 - train: epoch 0024, iter [00800, 05004], lr: 0.001331, loss: 1.3258
2022-08-19 00:34:40 - train: epoch 0024, iter [00900, 05004], lr: 0.001302, loss: 1.4349
2022-08-19 00:35:15 - train: epoch 0024, iter [01000, 05004], lr: 0.001274, loss: 1.3205
2022-08-19 00:35:49 - train: epoch 0024, iter [01100, 05004], lr: 0.001246, loss: 1.0675
2022-08-19 00:36:24 - train: epoch 0024, iter [01200, 05004], lr: 0.001218, loss: 1.2968
2022-08-19 00:36:58 - train: epoch 0024, iter [01300, 05004], lr: 0.001191, loss: 1.6126
2022-08-19 00:37:33 - train: epoch 0024, iter [01400, 05004], lr: 0.001164, loss: 1.1854
2022-08-19 00:38:08 - train: epoch 0024, iter [01500, 05004], lr: 0.001137, loss: 1.5778
2022-08-19 00:38:42 - train: epoch 0024, iter [01600, 05004], lr: 0.001110, loss: 1.2602
2022-08-19 00:39:17 - train: epoch 0024, iter [01700, 05004], lr: 0.001084, loss: 1.1593
2022-08-19 00:39:51 - train: epoch 0024, iter [01800, 05004], lr: 0.001058, loss: 1.4914
2022-08-19 00:40:26 - train: epoch 0024, iter [01900, 05004], lr: 0.001033, loss: 1.1580
2022-08-19 00:41:01 - train: epoch 0024, iter [02000, 05004], lr: 0.001008, loss: 1.3597
2022-08-19 00:41:35 - train: epoch 0024, iter [02100, 05004], lr: 0.000983, loss: 1.4075
2022-08-19 00:42:10 - train: epoch 0024, iter [02200, 05004], lr: 0.000958, loss: 1.3219
2022-08-19 00:42:45 - train: epoch 0024, iter [02300, 05004], lr: 0.000934, loss: 1.4091
2022-08-19 00:43:20 - train: epoch 0024, iter [02400, 05004], lr: 0.000910, loss: 1.3131
2022-08-19 00:43:55 - train: epoch 0024, iter [02500, 05004], lr: 0.000886, loss: 1.3522
2022-08-19 00:44:29 - train: epoch 0024, iter [02600, 05004], lr: 0.000863, loss: 1.5890
2022-08-19 00:45:04 - train: epoch 0024, iter [02700, 05004], lr: 0.000840, loss: 1.4578
2022-08-19 00:45:38 - train: epoch 0024, iter [02800, 05004], lr: 0.000817, loss: 1.2848
2022-08-19 00:46:13 - train: epoch 0024, iter [02900, 05004], lr: 0.000794, loss: 1.4119
2022-08-19 00:46:48 - train: epoch 0024, iter [03000, 05004], lr: 0.000772, loss: 1.1764
2022-08-19 00:47:22 - train: epoch 0024, iter [03100, 05004], lr: 0.000750, loss: 1.2612
2022-08-19 00:47:57 - train: epoch 0024, iter [03200, 05004], lr: 0.000729, loss: 1.2544
2022-08-19 00:48:31 - train: epoch 0024, iter [03300, 05004], lr: 0.000708, loss: 1.0549
2022-08-19 00:49:06 - train: epoch 0024, iter [03400, 05004], lr: 0.000687, loss: 1.4061
2022-08-19 00:49:41 - train: epoch 0024, iter [03500, 05004], lr: 0.000666, loss: 1.2168
2022-08-19 00:50:15 - train: epoch 0024, iter [03600, 05004], lr: 0.000646, loss: 1.3229
2022-08-19 00:50:50 - train: epoch 0024, iter [03700, 05004], lr: 0.000626, loss: 1.3463
2022-08-19 00:51:25 - train: epoch 0024, iter [03800, 05004], lr: 0.000606, loss: 1.2801
2022-08-19 00:51:59 - train: epoch 0024, iter [03900, 05004], lr: 0.000587, loss: 1.4113
2022-08-19 00:52:34 - train: epoch 0024, iter [04000, 05004], lr: 0.000568, loss: 1.2567
2022-08-19 00:53:09 - train: epoch 0024, iter [04100, 05004], lr: 0.000549, loss: 1.2392
2022-08-19 00:53:43 - train: epoch 0024, iter [04200, 05004], lr: 0.000531, loss: 1.2994
2022-08-19 00:54:18 - train: epoch 0024, iter [04300, 05004], lr: 0.000513, loss: 1.1941
2022-08-19 00:54:53 - train: epoch 0024, iter [04400, 05004], lr: 0.000495, loss: 1.4153
2022-08-19 00:55:28 - train: epoch 0024, iter [04500, 05004], lr: 0.000478, loss: 1.2909
2022-08-19 00:56:02 - train: epoch 0024, iter [04600, 05004], lr: 0.000460, loss: 1.2731
2022-08-19 00:56:37 - train: epoch 0024, iter [04700, 05004], lr: 0.000444, loss: 1.3056
2022-08-19 00:57:12 - train: epoch 0024, iter [04800, 05004], lr: 0.000427, loss: 1.1575
2022-08-19 00:57:46 - train: epoch 0024, iter [04900, 05004], lr: 0.000411, loss: 1.3694
2022-08-19 00:58:20 - train: epoch 0024, iter [05000, 05004], lr: 0.000395, loss: 1.1890
2022-08-19 00:58:21 - train: epoch 024, train_loss: 1.3136
2022-08-19 00:59:38 - eval: epoch: 024, acc1: 71.576%, acc5: 90.386%, test_loss: 1.1537, per_image_load_time: 2.302ms, per_image_inference_time: 0.594ms
2022-08-19 00:59:38 - until epoch: 024, best_acc1: 71.576%
2022-08-19 00:59:38 - epoch 025 lr: 0.000394
2022-08-19 01:00:19 - train: epoch 0025, iter [00100, 05004], lr: 0.000379, loss: 1.1635
2022-08-19 01:00:54 - train: epoch 0025, iter [00200, 05004], lr: 0.000363, loss: 1.2003
2022-08-19 01:01:28 - train: epoch 0025, iter [00300, 05004], lr: 0.000348, loss: 1.3366
2022-08-19 01:02:02 - train: epoch 0025, iter [00400, 05004], lr: 0.000334, loss: 1.3777
2022-08-19 01:02:37 - train: epoch 0025, iter [00500, 05004], lr: 0.000319, loss: 0.9784
2022-08-19 01:03:11 - train: epoch 0025, iter [00600, 05004], lr: 0.000305, loss: 1.3033
2022-08-19 01:03:46 - train: epoch 0025, iter [00700, 05004], lr: 0.000292, loss: 1.2339
2022-08-19 01:04:20 - train: epoch 0025, iter [00800, 05004], lr: 0.000278, loss: 1.3713
2022-08-19 01:04:55 - train: epoch 0025, iter [00900, 05004], lr: 0.000265, loss: 1.0666
2022-08-19 01:05:29 - train: epoch 0025, iter [01000, 05004], lr: 0.000253, loss: 1.3096
2022-08-19 01:06:04 - train: epoch 0025, iter [01100, 05004], lr: 0.000240, loss: 1.2223
2022-08-19 01:06:39 - train: epoch 0025, iter [01200, 05004], lr: 0.000228, loss: 1.2804
2022-08-19 01:07:13 - train: epoch 0025, iter [01300, 05004], lr: 0.000216, loss: 1.4218
2022-08-19 01:07:48 - train: epoch 0025, iter [01400, 05004], lr: 0.000205, loss: 1.4754
2022-08-19 01:08:22 - train: epoch 0025, iter [01500, 05004], lr: 0.000193, loss: 1.3452
2022-08-19 01:08:57 - train: epoch 0025, iter [01600, 05004], lr: 0.000183, loss: 1.1895
2022-08-19 01:09:31 - train: epoch 0025, iter [01700, 05004], lr: 0.000172, loss: 1.2047
2022-08-19 01:10:06 - train: epoch 0025, iter [01800, 05004], lr: 0.000162, loss: 1.0638
2022-08-19 01:10:41 - train: epoch 0025, iter [01900, 05004], lr: 0.000152, loss: 1.0752
2022-08-19 01:11:16 - train: epoch 0025, iter [02000, 05004], lr: 0.000142, loss: 1.3174
2022-08-19 01:11:51 - train: epoch 0025, iter [02100, 05004], lr: 0.000133, loss: 1.0533
2022-08-19 01:12:25 - train: epoch 0025, iter [02200, 05004], lr: 0.000124, loss: 1.0893
2022-08-19 01:13:00 - train: epoch 0025, iter [02300, 05004], lr: 0.000115, loss: 1.2794
2022-08-19 01:13:35 - train: epoch 0025, iter [02400, 05004], lr: 0.000107, loss: 1.2684
2022-08-19 01:14:10 - train: epoch 0025, iter [02500, 05004], lr: 0.000099, loss: 1.1993
2022-08-19 01:14:45 - train: epoch 0025, iter [02600, 05004], lr: 0.000091, loss: 1.2870
2022-08-19 01:15:19 - train: epoch 0025, iter [02700, 05004], lr: 0.000084, loss: 1.3878
2022-08-19 01:15:54 - train: epoch 0025, iter [02800, 05004], lr: 0.000077, loss: 1.1682
2022-08-19 01:16:29 - train: epoch 0025, iter [02900, 05004], lr: 0.000070, loss: 1.5394
2022-08-19 01:17:03 - train: epoch 0025, iter [03000, 05004], lr: 0.000063, loss: 1.4136
2022-08-19 01:17:38 - train: epoch 0025, iter [03100, 05004], lr: 0.000057, loss: 1.3544
2022-08-19 01:18:12 - train: epoch 0025, iter [03200, 05004], lr: 0.000051, loss: 1.1745
2022-08-19 01:18:47 - train: epoch 0025, iter [03300, 05004], lr: 0.000046, loss: 1.2439
2022-08-19 01:19:22 - train: epoch 0025, iter [03400, 05004], lr: 0.000041, loss: 1.4313
2022-08-19 01:19:56 - train: epoch 0025, iter [03500, 05004], lr: 0.000036, loss: 1.2610
2022-08-19 01:20:31 - train: epoch 0025, iter [03600, 05004], lr: 0.000031, loss: 1.2944
2022-08-19 01:21:06 - train: epoch 0025, iter [03700, 05004], lr: 0.000027, loss: 1.2926
2022-08-19 01:21:40 - train: epoch 0025, iter [03800, 05004], lr: 0.000023, loss: 1.3228
2022-08-19 01:22:15 - train: epoch 0025, iter [03900, 05004], lr: 0.000019, loss: 1.3873
2022-08-19 01:22:50 - train: epoch 0025, iter [04000, 05004], lr: 0.000016, loss: 1.3798
2022-08-19 01:23:24 - train: epoch 0025, iter [04100, 05004], lr: 0.000013, loss: 1.3669
2022-08-19 01:23:59 - train: epoch 0025, iter [04200, 05004], lr: 0.000010, loss: 1.4341
2022-08-19 01:24:33 - train: epoch 0025, iter [04300, 05004], lr: 0.000008, loss: 1.1090
2022-08-19 01:25:08 - train: epoch 0025, iter [04400, 05004], lr: 0.000006, loss: 1.0967
2022-08-19 01:25:43 - train: epoch 0025, iter [04500, 05004], lr: 0.000004, loss: 1.4910
2022-08-19 01:26:18 - train: epoch 0025, iter [04600, 05004], lr: 0.000003, loss: 1.1987
2022-08-19 01:26:52 - train: epoch 0025, iter [04700, 05004], lr: 0.000001, loss: 1.3279
2022-08-19 01:27:27 - train: epoch 0025, iter [04800, 05004], lr: 0.000001, loss: 1.1658
2022-08-19 01:28:02 - train: epoch 0025, iter [04900, 05004], lr: 0.000000, loss: 1.2803
2022-08-19 01:28:35 - train: epoch 0025, iter [05000, 05004], lr: 0.000000, loss: 1.4225
2022-08-19 01:28:37 - train: epoch 025, train_loss: 1.2925
2022-08-19 01:29:54 - eval: epoch: 025, acc1: 71.664%, acc5: 90.380%, test_loss: 1.1515, per_image_load_time: 2.173ms, per_image_inference_time: 0.601ms
2022-08-19 01:29:54 - until epoch: 025, best_acc1: 71.664%
2022-08-19 01:29:54 - train done. train time: 12.531 hours, best_acc1: 71.664%
