2022-10-01 22:59:20 - network: poolformer_s36_patch32_224_mae_pretrain_model
2022-10-01 22:59:20 - input_image_size: 224
2022-10-01 22:59:20 - scale: 1.1428571428571428
2022-10-01 22:59:20 - trained_model_path: 
2022-10-01 22:59:20 - train_criterion: MSELoss()
2022-10-01 22:59:20 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f72ddaeb7f0>
2022-10-01 22:59:20 - train_collater: <simpleAICV.masked_image_modeling.common.MAESelfSupervisedPretrainCollater object at 0x7f72ddaeb820>
2022-10-01 22:59:20 - seed: 0
2022-10-01 22:59:20 - batch_size: 256
2022-10-01 22:59:20 - num_workers: 20
2022-10-01 22:59:20 - accumulation_steps: 4
2022-10-01 22:59:20 - optimizer: ('AdamW', {'lr': 0.0006, 'global_weight_decay': False, 'weight_decay': 0.05, 'no_weight_decay_layer_name_list': [], 'beta1': 0.9, 'beta2': 0.95})
2022-10-01 22:59:20 - scheduler: ('CosineLR', {'warm_up_epochs': 10})
2022-10-01 22:59:20 - epochs: 100
2022-10-01 22:59:20 - print_interval: 10
2022-10-01 22:59:20 - sync_bn: False
2022-10-01 22:59:20 - apex: True
2022-10-01 22:59:20 - use_ema_model: False
2022-10-01 22:59:20 - ema_model_decay: 0.9999
2022-10-01 22:59:20 - gpus_type: NVIDIA RTX A5000
2022-10-01 22:59:20 - gpus_num: 2
2022-10-01 22:59:20 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f72cfbc5b70>
2022-10-01 22:59:20 - --------------------parameters--------------------
2022-10-01 22:59:20 - name: encoder.position_encoding, grad: True
2022-10-01 22:59:20 - name: encoder.patch_embedding1.layer.0.weight, grad: True
2022-10-01 22:59:20 - name: encoder.patch_embedding1.layer.0.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.0.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.0.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.0.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.0.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.0.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.0.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.0.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.0.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.0.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.0.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.1.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.1.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.1.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.1.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.1.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.1.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.1.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.1.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.1.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.1.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.2.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.2.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.2.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.2.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.2.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.2.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.2.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.2.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.2.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.2.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.3.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.3.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.3.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.3.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.3.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.3.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.3.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.3.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.3.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.3.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.4.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.4.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.4.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.4.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.4.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.4.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.4.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.4.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.4.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.4.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.5.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.5.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.5.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.5.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.5.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.5.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.5.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.5.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.5.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2.5.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.patch_embedding2.layer.0.weight, grad: True
2022-10-01 22:59:20 - name: encoder.patch_embedding2.layer.0.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.0.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.0.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.0.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.0.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.0.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.0.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.0.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.0.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.0.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.0.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.1.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.1.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.1.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.1.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.1.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.1.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.1.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.1.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.1.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.1.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.2.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.2.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.2.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.2.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.2.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.2.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.2.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.2.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.2.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.2.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.3.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.3.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.3.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.3.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.3.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.3.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.3.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.3.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.3.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.3.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.4.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.4.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.4.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.4.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.4.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.4.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.4.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.4.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.4.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.4.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.5.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.5.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.5.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.5.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.5.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.5.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.5.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.5.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.5.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3.5.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.patch_embedding3.layer.0.weight, grad: True
2022-10-01 22:59:20 - name: encoder.patch_embedding3.layer.0.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.0.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.0.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.0.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.0.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.0.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.0.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.0.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.0.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.0.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.0.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.1.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.1.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.1.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.1.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.1.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.1.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.1.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.1.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.1.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.1.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.2.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.2.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.2.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.2.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.2.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.2.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.2.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.2.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.2.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.2.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.3.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.3.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.3.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.3.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.3.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.3.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.3.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.3.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.3.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.3.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.4.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.4.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.4.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.4.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.4.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.4.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.4.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.4.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.4.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.4.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.5.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.5.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.5.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.5.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.5.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.5.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.5.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.5.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.5.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.5.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.6.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.6.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.6.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.6.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.6.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.6.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.6.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.6.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.6.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.6.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.7.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.7.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.7.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.7.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.7.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.7.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.7.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.7.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.7.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.7.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.8.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.8.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.8.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.8.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.8.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.8.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.8.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.8.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.8.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.8.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.9.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.9.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.9.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.9.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.9.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.9.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.9.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.9.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.9.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.9.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.10.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.10.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.10.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.10.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.10.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.10.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.10.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.10.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.10.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.10.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.11.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.11.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.11.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.11.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.11.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.11.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.11.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.11.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.11.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.11.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.12.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.12.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.12.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.12.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.12.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.12.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.12.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.12.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.12.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.12.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.13.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.13.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.13.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.13.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.13.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.13.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.13.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.13.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.13.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.13.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.14.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.14.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.14.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.14.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.14.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.14.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.14.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.14.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.14.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.14.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.15.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.15.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.15.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.15.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.15.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.15.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.15.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.15.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.15.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.15.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.16.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.16.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.16.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.16.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.16.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.16.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.16.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.16.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.16.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.16.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.17.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.17.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.17.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.17.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.17.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.17.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.17.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.17.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.17.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4.17.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.patch_embedding4.layer.0.weight, grad: True
2022-10-01 22:59:20 - name: encoder.patch_embedding4.layer.0.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.0.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.0.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.0.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.0.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.0.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.0.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.0.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.0.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.0.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.0.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.1.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.1.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.1.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.1.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.1.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.1.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.1.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.1.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.1.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.1.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.2.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.2.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.2.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.2.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.2.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.2.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.2.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.2.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.2.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.2.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.3.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.3.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.3.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.3.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.3.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.3.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.3.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.3.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.3.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.3.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.4.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.4.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.4.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.4.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.4.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.4.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.4.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.4.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.4.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.4.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.5.layer_scale_1, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.5.layer_scale_2, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.5.norm1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.5.norm1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.5.norm2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.5.norm2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.5.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.5.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.5.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage5.5.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage2_decode_conv.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage2_decode_conv.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage3_decode_conv.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage3_decode_conv.bias, grad: True
2022-10-01 22:59:20 - name: encoder.stage4_decode_conv.weight, grad: True
2022-10-01 22:59:20 - name: encoder.stage4_decode_conv.bias, grad: True
2022-10-01 22:59:20 - name: encoder.final_embedding.weight, grad: True
2022-10-01 22:59:20 - name: encoder.final_embedding.bias, grad: True
2022-10-01 22:59:20 - name: encoder.norm.weight, grad: True
2022-10-01 22:59:20 - name: encoder.norm.bias, grad: True
2022-10-01 22:59:20 - name: decoder.mask_token, grad: True
2022-10-01 22:59:20 - name: decoder.position_encoding, grad: False
2022-10-01 22:59:20 - name: decoder.blocks.0.norm1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.0.norm1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.0.attention.qkv_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.0.attention.qkv_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.0.attention.out_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.0.attention.out_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.0.norm2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.0.norm2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.0.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.0.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.0.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.0.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.1.norm1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.1.norm1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.1.attention.qkv_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.1.attention.qkv_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.1.attention.out_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.1.attention.out_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.1.norm2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.1.norm2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.1.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.1.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.1.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.1.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.2.norm1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.2.norm1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.2.attention.qkv_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.2.attention.qkv_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.2.attention.out_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.2.attention.out_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.2.norm2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.2.norm2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.2.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.2.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.2.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.2.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.3.norm1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.3.norm1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.3.attention.qkv_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.3.attention.qkv_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.3.attention.out_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.3.attention.out_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.3.norm2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.3.norm2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.3.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.3.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.3.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.3.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.4.norm1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.4.norm1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.4.attention.qkv_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.4.attention.qkv_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.4.attention.out_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.4.attention.out_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.4.norm2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.4.norm2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.4.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.4.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.4.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.4.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.5.norm1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.5.norm1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.5.attention.qkv_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.5.attention.qkv_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.5.attention.out_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.5.attention.out_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.5.norm2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.5.norm2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.5.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.5.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.5.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.5.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.6.norm1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.6.norm1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.6.attention.qkv_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.6.attention.qkv_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.6.attention.out_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.6.attention.out_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.6.norm2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.6.norm2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.6.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.6.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.6.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.6.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.7.norm1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.7.norm1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.7.attention.qkv_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.7.attention.qkv_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.7.attention.out_linear.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.7.attention.out_linear.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.7.norm2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.7.norm2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.7.feed_forward.fc1.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.7.feed_forward.fc1.bias, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.7.feed_forward.fc2.weight, grad: True
2022-10-01 22:59:20 - name: decoder.blocks.7.feed_forward.fc2.bias, grad: True
2022-10-01 22:59:20 - name: decoder.norm.weight, grad: True
2022-10-01 22:59:20 - name: decoder.norm.bias, grad: True
2022-10-01 22:59:20 - name: decoder.fc.weight, grad: True
2022-10-01 22:59:20 - name: decoder.fc.bias, grad: True
2022-10-01 22:59:20 - name: encoder_to_decoder.weight, grad: True
2022-10-01 22:59:20 - name: encoder_to_decoder.bias, grad: True
2022-10-01 22:59:20 - --------------------buffers--------------------
2022-10-01 22:59:20 - -----------no weight decay layers--------------
2022-10-01 22:59:20 - name: encoder.patch_embedding1.layer.0.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.0.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.0.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.0.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.0.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.0.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.0.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.1.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.1.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.1.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.1.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.1.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.1.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.2.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.2.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.2.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.2.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.2.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.2.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.3.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.3.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.3.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.3.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.3.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.3.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.4.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.4.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.4.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.4.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.4.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.4.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.5.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.5.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.5.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.5.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.5.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.5.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.patch_embedding2.layer.0.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.0.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.0.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.0.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.0.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.0.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.0.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.1.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.1.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.1.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.1.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.1.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.1.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.2.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.2.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.2.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.2.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.2.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.2.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.3.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.3.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.3.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.3.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.3.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.3.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.4.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.4.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.4.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.4.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.4.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.4.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.5.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.5.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.5.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.5.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.5.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.5.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.patch_embedding3.layer.0.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.0.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.0.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.0.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.0.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.0.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.0.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.1.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.1.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.1.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.1.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.1.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.1.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.2.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.2.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.2.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.2.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.2.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.2.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.3.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.3.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.3.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.3.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.3.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.3.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.4.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.4.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.4.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.4.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.4.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.4.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.5.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.5.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.5.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.5.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.5.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.5.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.6.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.6.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.6.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.6.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.6.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.6.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.7.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.7.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.7.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.7.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.7.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.7.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.8.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.8.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.8.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.8.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.8.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.8.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.9.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.9.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.9.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.9.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.9.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.9.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.10.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.10.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.10.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.10.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.10.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.10.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.11.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.11.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.11.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.11.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.11.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.11.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.12.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.12.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.12.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.12.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.12.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.12.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.13.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.13.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.13.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.13.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.13.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.13.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.14.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.14.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.14.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.14.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.14.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.14.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.15.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.15.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.15.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.15.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.15.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.15.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.16.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.16.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.16.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.16.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.16.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.16.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.17.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.17.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.17.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.17.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.17.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.17.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.patch_embedding4.layer.0.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.0.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.0.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.0.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.0.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.0.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.0.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.1.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.1.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.1.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.1.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.1.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.1.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.2.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.2.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.2.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.2.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.2.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.2.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.3.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.3.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.3.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.3.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.3.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.3.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.4.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.4.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.4.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.4.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.4.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.4.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.5.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.5.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.5.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.5.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.5.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.5.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2_decode_conv.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3_decode_conv.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4_decode_conv.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.final_embedding.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.norm.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.norm.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.0.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.0.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.0.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.0.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.0.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.0.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.0.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.0.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.1.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.1.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.1.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.1.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.1.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.1.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.1.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.1.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.2.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.2.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.2.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.2.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.2.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.2.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.2.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.2.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.3.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.3.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.3.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.3.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.3.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.3.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.3.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.3.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.4.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.4.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.4.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.4.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.4.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.4.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.4.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.4.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.5.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.5.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.5.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.5.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.5.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.5.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.5.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.5.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.6.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.6.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.6.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.6.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.6.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.6.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.6.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.6.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.7.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.7.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.7.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.7.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.7.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.7.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.7.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.7.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.norm.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.norm.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder_to_decoder.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-01 22:59:20 - -------------weight decay layers---------------
2022-10-01 22:59:20 - name: encoder.position_encoding, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.patch_embedding1.layer.0.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.0.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.0.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.0.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.0.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.1.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.1.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.1.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.1.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.2.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.2.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.2.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.2.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.3.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.3.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.3.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.3.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.4.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.4.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.4.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.4.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.5.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.5.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.5.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2.5.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.patch_embedding2.layer.0.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.0.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.0.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.0.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.0.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.1.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.1.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.1.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.1.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.2.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.2.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.2.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.2.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.3.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.3.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.3.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.3.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.4.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.4.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.4.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.4.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.5.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.5.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.5.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3.5.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.patch_embedding3.layer.0.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.0.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.0.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.0.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.0.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.1.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.1.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.1.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.1.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.2.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.2.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.2.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.2.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.3.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.3.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.3.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.3.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.4.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.4.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.4.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.4.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.5.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.5.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.5.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.5.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.6.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.6.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.6.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.6.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.7.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.7.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.7.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.7.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.8.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.8.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.8.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.8.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.9.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.9.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.9.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.9.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.10.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.10.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.10.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.10.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.11.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.11.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.11.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.11.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.12.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.12.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.12.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.12.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.13.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.13.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.13.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.13.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.14.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.14.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.14.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.14.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.15.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.15.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.15.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.15.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.16.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.16.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.16.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.16.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.17.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.17.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.17.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4.17.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.patch_embedding4.layer.0.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.0.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.0.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.0.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.0.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.1.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.1.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.1.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.1.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.2.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.2.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.2.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.2.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.3.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.3.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.3.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.3.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.4.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.4.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.4.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.4.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.5.layer_scale_1, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.5.layer_scale_2, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.5.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage5.5.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage2_decode_conv.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage3_decode_conv.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.stage4_decode_conv.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder.final_embedding.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.mask_token, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.0.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.0.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.0.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.0.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.1.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.1.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.1.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.1.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.2.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.2.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.2.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.2.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.3.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.3.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.3.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.3.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.4.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.4.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.4.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.4.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.5.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.5.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.5.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.5.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.6.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.6.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.6.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.6.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.7.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.7.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.7.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.blocks.7.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: decoder.fc.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - name: encoder_to_decoder.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-01 22:59:20 - epoch 001 lr: 0.000600
2022-10-01 23:00:00 - train: epoch 0001, iter [00010, 01251], lr: 0.000060, loss: 1.0836
2022-10-01 23:00:28 - train: epoch 0001, iter [00020, 01251], lr: 0.000061, loss: 1.0205
2022-10-01 23:00:56 - train: epoch 0001, iter [00030, 01251], lr: 0.000061, loss: 0.9975
2022-10-01 23:01:24 - train: epoch 0001, iter [00040, 01251], lr: 0.000062, loss: 0.9977
2022-10-01 23:01:53 - train: epoch 0001, iter [00050, 01251], lr: 0.000062, loss: 0.9959
2022-10-01 23:02:21 - train: epoch 0001, iter [00060, 01251], lr: 0.000063, loss: 0.9868
2022-10-01 23:02:49 - train: epoch 0001, iter [00070, 01251], lr: 0.000063, loss: 0.9573
2022-10-01 23:03:17 - train: epoch 0001, iter [00080, 01251], lr: 0.000064, loss: 0.9367
2022-10-01 23:03:45 - train: epoch 0001, iter [00090, 01251], lr: 0.000064, loss: 0.8864
2022-10-01 23:04:13 - train: epoch 0001, iter [00100, 01251], lr: 0.000065, loss: 0.8995
2022-10-01 23:04:41 - train: epoch 0001, iter [00110, 01251], lr: 0.000065, loss: 0.8718
2022-10-01 23:05:09 - train: epoch 0001, iter [00120, 01251], lr: 0.000066, loss: 0.8828
2022-10-01 23:05:37 - train: epoch 0001, iter [00130, 01251], lr: 0.000066, loss: 0.8681
2022-10-01 23:06:06 - train: epoch 0001, iter [00140, 01251], lr: 0.000067, loss: 0.8813
2022-10-01 23:06:34 - train: epoch 0001, iter [00150, 01251], lr: 0.000067, loss: 0.8594
2022-10-01 23:07:02 - train: epoch 0001, iter [00160, 01251], lr: 0.000068, loss: 0.8409
2022-10-01 23:07:30 - train: epoch 0001, iter [00170, 01251], lr: 0.000068, loss: 0.8016
2022-10-01 23:07:58 - train: epoch 0001, iter [00180, 01251], lr: 0.000069, loss: 0.8125
2022-10-01 23:08:27 - train: epoch 0001, iter [00190, 01251], lr: 0.000069, loss: 0.8099
2022-10-01 23:08:55 - train: epoch 0001, iter [00200, 01251], lr: 0.000070, loss: 0.8117
2022-10-01 23:09:23 - train: epoch 0001, iter [00210, 01251], lr: 0.000070, loss: 0.8080
2022-10-01 23:09:51 - train: epoch 0001, iter [00220, 01251], lr: 0.000071, loss: 0.8124
2022-10-01 23:10:19 - train: epoch 0001, iter [00230, 01251], lr: 0.000071, loss: 0.8138
2022-10-01 23:10:47 - train: epoch 0001, iter [00240, 01251], lr: 0.000072, loss: 0.8034
2022-10-01 23:11:15 - train: epoch 0001, iter [00250, 01251], lr: 0.000072, loss: 0.7996
2022-10-01 23:11:44 - train: epoch 0001, iter [00260, 01251], lr: 0.000072, loss: 0.7908
2022-10-01 23:12:12 - train: epoch 0001, iter [00270, 01251], lr: 0.000073, loss: 0.7971
2022-10-01 23:12:41 - train: epoch 0001, iter [00280, 01251], lr: 0.000073, loss: 0.7884
2022-10-01 23:13:09 - train: epoch 0001, iter [00290, 01251], lr: 0.000074, loss: 0.7943
2022-10-01 23:13:37 - train: epoch 0001, iter [00300, 01251], lr: 0.000074, loss: 0.7936
2022-10-01 23:14:05 - train: epoch 0001, iter [00310, 01251], lr: 0.000075, loss: 0.7627
2022-10-01 23:14:33 - train: epoch 0001, iter [00320, 01251], lr: 0.000075, loss: 0.7907
2022-10-01 23:15:01 - train: epoch 0001, iter [00330, 01251], lr: 0.000076, loss: 0.7889
2022-10-01 23:15:29 - train: epoch 0001, iter [00340, 01251], lr: 0.000076, loss: 0.7673
2022-10-01 23:15:58 - train: epoch 0001, iter [00350, 01251], lr: 0.000077, loss: 0.7676
2022-10-01 23:16:26 - train: epoch 0001, iter [00360, 01251], lr: 0.000077, loss: 0.7524
2022-10-01 23:16:54 - train: epoch 0001, iter [00370, 01251], lr: 0.000078, loss: 0.7591
2022-10-01 23:17:22 - train: epoch 0001, iter [00380, 01251], lr: 0.000078, loss: 0.7866
2022-10-01 23:17:50 - train: epoch 0001, iter [00390, 01251], lr: 0.000079, loss: 0.7642
2022-10-01 23:18:18 - train: epoch 0001, iter [00400, 01251], lr: 0.000079, loss: 0.7833
2022-10-01 23:18:47 - train: epoch 0001, iter [00410, 01251], lr: 0.000080, loss: 0.7684
2022-10-01 23:19:15 - train: epoch 0001, iter [00420, 01251], lr: 0.000080, loss: 0.7526
2022-10-01 23:19:43 - train: epoch 0001, iter [00430, 01251], lr: 0.000081, loss: 0.7435
2022-10-01 23:20:11 - train: epoch 0001, iter [00440, 01251], lr: 0.000081, loss: 0.7297
2022-10-01 23:20:40 - train: epoch 0001, iter [00450, 01251], lr: 0.000082, loss: 0.7511
2022-10-01 23:21:08 - train: epoch 0001, iter [00460, 01251], lr: 0.000082, loss: 0.7352
2022-10-01 23:21:36 - train: epoch 0001, iter [00470, 01251], lr: 0.000083, loss: 0.7254
2022-10-01 23:22:04 - train: epoch 0001, iter [00480, 01251], lr: 0.000083, loss: 0.7364
2022-10-01 23:22:33 - train: epoch 0001, iter [00490, 01251], lr: 0.000084, loss: 0.7446
2022-10-01 23:23:01 - train: epoch 0001, iter [00500, 01251], lr: 0.000084, loss: 0.7358
2022-10-01 23:23:29 - train: epoch 0001, iter [00510, 01251], lr: 0.000084, loss: 0.7031
2022-10-01 23:23:57 - train: epoch 0001, iter [00520, 01251], lr: 0.000085, loss: 0.6877
2022-10-01 23:24:25 - train: epoch 0001, iter [00530, 01251], lr: 0.000085, loss: 0.7013
2022-10-01 23:24:53 - train: epoch 0001, iter [00540, 01251], lr: 0.000086, loss: 0.6751
2022-10-01 23:25:22 - train: epoch 0001, iter [00550, 01251], lr: 0.000086, loss: 0.6947
2022-10-01 23:25:50 - train: epoch 0001, iter [00560, 01251], lr: 0.000087, loss: 0.6842
2022-10-01 23:26:18 - train: epoch 0001, iter [00570, 01251], lr: 0.000087, loss: 0.6832
2022-10-01 23:26:46 - train: epoch 0001, iter [00580, 01251], lr: 0.000088, loss: 0.6707
2022-10-01 23:27:14 - train: epoch 0001, iter [00590, 01251], lr: 0.000088, loss: 0.6734
2022-10-01 23:27:42 - train: epoch 0001, iter [00600, 01251], lr: 0.000089, loss: 0.6785
2022-10-01 23:28:10 - train: epoch 0001, iter [00610, 01251], lr: 0.000089, loss: 0.6353
2022-10-01 23:28:39 - train: epoch 0001, iter [00620, 01251], lr: 0.000090, loss: 0.6626
2022-10-01 23:29:07 - train: epoch 0001, iter [00630, 01251], lr: 0.000090, loss: 0.6614
2022-10-01 23:29:35 - train: epoch 0001, iter [00640, 01251], lr: 0.000091, loss: 0.6608
2022-10-01 23:30:03 - train: epoch 0001, iter [00650, 01251], lr: 0.000091, loss: 0.6628
2022-10-01 23:30:31 - train: epoch 0001, iter [00660, 01251], lr: 0.000092, loss: 0.6367
2022-10-01 23:30:59 - train: epoch 0001, iter [00670, 01251], lr: 0.000092, loss: 0.6282
2022-10-01 23:31:28 - train: epoch 0001, iter [00680, 01251], lr: 0.000093, loss: 0.6513
2022-10-01 23:31:56 - train: epoch 0001, iter [00690, 01251], lr: 0.000093, loss: 0.6329
2022-10-01 23:32:24 - train: epoch 0001, iter [00700, 01251], lr: 0.000094, loss: 0.6398
2022-10-01 23:32:52 - train: epoch 0001, iter [00710, 01251], lr: 0.000094, loss: 0.6117
2022-10-01 23:33:20 - train: epoch 0001, iter [00720, 01251], lr: 0.000095, loss: 0.6268
2022-10-01 23:33:49 - train: epoch 0001, iter [00730, 01251], lr: 0.000095, loss: 0.6225
2022-10-01 23:34:17 - train: epoch 0001, iter [00740, 01251], lr: 0.000095, loss: 0.6268
2022-10-01 23:34:45 - train: epoch 0001, iter [00750, 01251], lr: 0.000096, loss: 0.6145
2022-10-01 23:35:13 - train: epoch 0001, iter [00760, 01251], lr: 0.000096, loss: 0.6051
2022-10-01 23:35:41 - train: epoch 0001, iter [00770, 01251], lr: 0.000097, loss: 0.6073
2022-10-01 23:36:09 - train: epoch 0001, iter [00780, 01251], lr: 0.000097, loss: 0.5944
2022-10-01 23:36:38 - train: epoch 0001, iter [00790, 01251], lr: 0.000098, loss: 0.5751
2022-10-01 23:37:06 - train: epoch 0001, iter [00800, 01251], lr: 0.000098, loss: 0.5765
2022-10-01 23:37:34 - train: epoch 0001, iter [00810, 01251], lr: 0.000099, loss: 0.6011
2022-10-01 23:38:02 - train: epoch 0001, iter [00820, 01251], lr: 0.000099, loss: 0.5848
2022-10-01 23:38:30 - train: epoch 0001, iter [00830, 01251], lr: 0.000100, loss: 0.5617
2022-10-01 23:38:58 - train: epoch 0001, iter [00840, 01251], lr: 0.000100, loss: 0.5741
2022-10-01 23:39:26 - train: epoch 0001, iter [00850, 01251], lr: 0.000101, loss: 0.5684
2022-10-01 23:39:55 - train: epoch 0001, iter [00860, 01251], lr: 0.000101, loss: 0.5457
2022-10-01 23:40:23 - train: epoch 0001, iter [00870, 01251], lr: 0.000102, loss: 0.5641
2022-10-01 23:40:51 - train: epoch 0001, iter [00880, 01251], lr: 0.000102, loss: 0.5596
2022-10-01 23:41:19 - train: epoch 0001, iter [00890, 01251], lr: 0.000103, loss: 0.5463
2022-10-01 23:41:47 - train: epoch 0001, iter [00900, 01251], lr: 0.000103, loss: 0.5871
2022-10-01 23:42:15 - train: epoch 0001, iter [00910, 01251], lr: 0.000104, loss: 0.5470
2022-10-01 23:42:43 - train: epoch 0001, iter [00920, 01251], lr: 0.000104, loss: 0.5444
2022-10-01 23:43:11 - train: epoch 0001, iter [00930, 01251], lr: 0.000105, loss: 0.5711
2022-10-01 23:43:39 - train: epoch 0001, iter [00940, 01251], lr: 0.000105, loss: 0.5686
2022-10-01 23:44:08 - train: epoch 0001, iter [00950, 01251], lr: 0.000106, loss: 0.5387
2022-10-01 23:44:36 - train: epoch 0001, iter [00960, 01251], lr: 0.000106, loss: 0.5471
2022-10-01 23:45:04 - train: epoch 0001, iter [00970, 01251], lr: 0.000107, loss: 0.5447
2022-10-01 23:45:32 - train: epoch 0001, iter [00980, 01251], lr: 0.000107, loss: 0.5380
2022-10-01 23:46:00 - train: epoch 0001, iter [00990, 01251], lr: 0.000107, loss: 0.5466
2022-10-01 23:46:29 - train: epoch 0001, iter [01000, 01251], lr: 0.000108, loss: 0.5333
2022-10-01 23:46:57 - train: epoch 0001, iter [01010, 01251], lr: 0.000108, loss: 0.5326
2022-10-01 23:47:25 - train: epoch 0001, iter [01020, 01251], lr: 0.000109, loss: 0.5335
2022-10-01 23:47:53 - train: epoch 0001, iter [01030, 01251], lr: 0.000109, loss: 0.5412
2022-10-01 23:48:22 - train: epoch 0001, iter [01040, 01251], lr: 0.000110, loss: 0.5333
2022-10-01 23:48:50 - train: epoch 0001, iter [01050, 01251], lr: 0.000110, loss: 0.5261
2022-10-01 23:49:18 - train: epoch 0001, iter [01060, 01251], lr: 0.000111, loss: 0.5215
2022-10-01 23:49:46 - train: epoch 0001, iter [01070, 01251], lr: 0.000111, loss: 0.5151
2022-10-01 23:50:15 - train: epoch 0001, iter [01080, 01251], lr: 0.000112, loss: 0.5219
2022-10-01 23:50:43 - train: epoch 0001, iter [01090, 01251], lr: 0.000112, loss: 0.4981
2022-10-01 23:51:11 - train: epoch 0001, iter [01100, 01251], lr: 0.000113, loss: 0.5266
2022-10-01 23:51:39 - train: epoch 0001, iter [01110, 01251], lr: 0.000113, loss: 0.5093
2022-10-01 23:52:07 - train: epoch 0001, iter [01120, 01251], lr: 0.000114, loss: 0.5164
2022-10-01 23:52:36 - train: epoch 0001, iter [01130, 01251], lr: 0.000114, loss: 0.5084
2022-10-01 23:53:04 - train: epoch 0001, iter [01140, 01251], lr: 0.000115, loss: 0.5208
2022-10-01 23:53:32 - train: epoch 0001, iter [01150, 01251], lr: 0.000115, loss: 0.5024
2022-10-01 23:54:00 - train: epoch 0001, iter [01160, 01251], lr: 0.000116, loss: 0.5230
2022-10-01 23:54:28 - train: epoch 0001, iter [01170, 01251], lr: 0.000116, loss: 0.4980
2022-10-01 23:54:57 - train: epoch 0001, iter [01180, 01251], lr: 0.000117, loss: 0.4951
2022-10-01 23:55:25 - train: epoch 0001, iter [01190, 01251], lr: 0.000117, loss: 0.5111
2022-10-01 23:55:53 - train: epoch 0001, iter [01200, 01251], lr: 0.000118, loss: 0.4992
2022-10-01 23:56:21 - train: epoch 0001, iter [01210, 01251], lr: 0.000118, loss: 0.5084
2022-10-01 23:56:49 - train: epoch 0001, iter [01220, 01251], lr: 0.000119, loss: 0.4919
2022-10-01 23:57:17 - train: epoch 0001, iter [01230, 01251], lr: 0.000119, loss: 0.5110
2022-10-01 23:57:46 - train: epoch 0001, iter [01240, 01251], lr: 0.000119, loss: 0.4737
2022-10-01 23:58:13 - train: epoch 0001, iter [01250, 01251], lr: 0.000120, loss: 0.4755
2022-10-01 23:58:18 - train: epoch 001, train_loss: 0.6795
2022-10-01 23:58:19 - until epoch: 001, best_loss: 0.6795
2022-10-01 23:58:19 - epoch 002 lr: 0.000120
2022-10-01 23:58:55 - train: epoch 0002, iter [00010, 01251], lr: 0.000120, loss: 0.4847
2022-10-01 23:59:23 - train: epoch 0002, iter [00020, 01251], lr: 0.000121, loss: 0.4797
2022-10-01 23:59:51 - train: epoch 0002, iter [00030, 01251], lr: 0.000121, loss: 0.4796
2022-10-02 00:00:19 - train: epoch 0002, iter [00040, 01251], lr: 0.000122, loss: 0.4919
2022-10-02 00:00:47 - train: epoch 0002, iter [00050, 01251], lr: 0.000122, loss: 0.4630
2022-10-02 00:01:15 - train: epoch 0002, iter [00060, 01251], lr: 0.000123, loss: 0.4902
2022-10-02 00:01:43 - train: epoch 0002, iter [00070, 01251], lr: 0.000123, loss: 0.4676
2022-10-02 00:02:11 - train: epoch 0002, iter [00080, 01251], lr: 0.000124, loss: 0.4723
2022-10-02 00:02:39 - train: epoch 0002, iter [00090, 01251], lr: 0.000124, loss: 0.4803
2022-10-02 00:03:08 - train: epoch 0002, iter [00100, 01251], lr: 0.000125, loss: 0.4643
2022-10-02 00:03:36 - train: epoch 0002, iter [00110, 01251], lr: 0.000125, loss: 0.4822
2022-10-02 00:04:04 - train: epoch 0002, iter [00120, 01251], lr: 0.000126, loss: 0.4597
2022-10-02 00:04:32 - train: epoch 0002, iter [00130, 01251], lr: 0.000126, loss: 0.4823
2022-10-02 00:05:00 - train: epoch 0002, iter [00140, 01251], lr: 0.000127, loss: 0.4730
2022-10-02 00:05:28 - train: epoch 0002, iter [00150, 01251], lr: 0.000127, loss: 0.4498
2022-10-02 00:05:56 - train: epoch 0002, iter [00160, 01251], lr: 0.000128, loss: 0.4400
2022-10-02 00:06:24 - train: epoch 0002, iter [00170, 01251], lr: 0.000128, loss: 0.4474
2022-10-02 00:06:52 - train: epoch 0002, iter [00180, 01251], lr: 0.000129, loss: 0.4545
2022-10-02 00:07:20 - train: epoch 0002, iter [00190, 01251], lr: 0.000129, loss: 0.4447
2022-10-02 00:07:48 - train: epoch 0002, iter [00200, 01251], lr: 0.000130, loss: 0.4614
2022-10-02 00:08:16 - train: epoch 0002, iter [00210, 01251], lr: 0.000130, loss: 0.4477
2022-10-02 00:08:44 - train: epoch 0002, iter [00220, 01251], lr: 0.000131, loss: 0.4758
2022-10-02 00:09:13 - train: epoch 0002, iter [00230, 01251], lr: 0.000131, loss: 0.4497
2022-10-02 00:09:41 - train: epoch 0002, iter [00240, 01251], lr: 0.000132, loss: 0.4486
2022-10-02 00:10:09 - train: epoch 0002, iter [00250, 01251], lr: 0.000132, loss: 0.4406
2022-10-02 00:10:37 - train: epoch 0002, iter [00260, 01251], lr: 0.000132, loss: 0.4362
2022-10-02 00:11:06 - train: epoch 0002, iter [00270, 01251], lr: 0.000133, loss: 0.4408
2022-10-02 00:11:34 - train: epoch 0002, iter [00280, 01251], lr: 0.000133, loss: 0.4454
2022-10-02 00:12:02 - train: epoch 0002, iter [00290, 01251], lr: 0.000134, loss: 0.4516
2022-10-02 00:12:30 - train: epoch 0002, iter [00300, 01251], lr: 0.000134, loss: 0.4402
2022-10-02 00:12:58 - train: epoch 0002, iter [00310, 01251], lr: 0.000135, loss: 0.4543
2022-10-02 00:13:26 - train: epoch 0002, iter [00320, 01251], lr: 0.000135, loss: 0.4280
2022-10-02 00:13:54 - train: epoch 0002, iter [00330, 01251], lr: 0.000136, loss: 0.4444
2022-10-02 00:14:22 - train: epoch 0002, iter [00340, 01251], lr: 0.000136, loss: 0.4197
2022-10-02 00:14:50 - train: epoch 0002, iter [00350, 01251], lr: 0.000137, loss: 0.4226
2022-10-02 00:15:18 - train: epoch 0002, iter [00360, 01251], lr: 0.000137, loss: 0.4412
2022-10-02 00:15:46 - train: epoch 0002, iter [00370, 01251], lr: 0.000138, loss: 0.4150
2022-10-02 00:16:14 - train: epoch 0002, iter [00380, 01251], lr: 0.000138, loss: 0.4457
2022-10-02 00:16:42 - train: epoch 0002, iter [00390, 01251], lr: 0.000139, loss: 0.4305
2022-10-02 00:17:11 - train: epoch 0002, iter [00400, 01251], lr: 0.000139, loss: 0.4274
2022-10-02 00:17:39 - train: epoch 0002, iter [00410, 01251], lr: 0.000140, loss: 0.4342
2022-10-02 00:18:07 - train: epoch 0002, iter [00420, 01251], lr: 0.000140, loss: 0.4325
2022-10-02 00:18:35 - train: epoch 0002, iter [00430, 01251], lr: 0.000141, loss: 0.4235
2022-10-02 00:19:03 - train: epoch 0002, iter [00440, 01251], lr: 0.000141, loss: 0.4134
2022-10-02 00:19:32 - train: epoch 0002, iter [00450, 01251], lr: 0.000142, loss: 0.4080
2022-10-02 00:20:00 - train: epoch 0002, iter [00460, 01251], lr: 0.000142, loss: 0.4185
2022-10-02 00:20:28 - train: epoch 0002, iter [00470, 01251], lr: 0.000143, loss: 0.4143
2022-10-02 00:20:56 - train: epoch 0002, iter [00480, 01251], lr: 0.000143, loss: 0.4222
2022-10-02 00:21:24 - train: epoch 0002, iter [00490, 01251], lr: 0.000144, loss: 0.4074
2022-10-02 00:21:52 - train: epoch 0002, iter [00500, 01251], lr: 0.000144, loss: 0.4099
2022-10-02 00:22:20 - train: epoch 0002, iter [00510, 01251], lr: 0.000144, loss: 0.4423
2022-10-02 00:22:48 - train: epoch 0002, iter [00520, 01251], lr: 0.000145, loss: 0.4162
2022-10-02 00:23:17 - train: epoch 0002, iter [00530, 01251], lr: 0.000145, loss: 0.4168
2022-10-02 00:23:45 - train: epoch 0002, iter [00540, 01251], lr: 0.000146, loss: 0.4169
2022-10-02 00:24:13 - train: epoch 0002, iter [00550, 01251], lr: 0.000146, loss: 0.4067
2022-10-02 00:24:42 - train: epoch 0002, iter [00560, 01251], lr: 0.000147, loss: 0.4106
2022-10-02 00:25:10 - train: epoch 0002, iter [00570, 01251], lr: 0.000147, loss: 0.4108
2022-10-02 00:25:38 - train: epoch 0002, iter [00580, 01251], lr: 0.000148, loss: 0.4021
2022-10-02 00:26:06 - train: epoch 0002, iter [00590, 01251], lr: 0.000148, loss: 0.4044
2022-10-02 00:26:34 - train: epoch 0002, iter [00600, 01251], lr: 0.000149, loss: 0.4226
2022-10-02 00:27:02 - train: epoch 0002, iter [00610, 01251], lr: 0.000149, loss: 0.3868
2022-10-02 00:27:31 - train: epoch 0002, iter [00620, 01251], lr: 0.000150, loss: 0.4133
2022-10-02 00:27:59 - train: epoch 0002, iter [00630, 01251], lr: 0.000150, loss: 0.4057
2022-10-02 00:28:27 - train: epoch 0002, iter [00640, 01251], lr: 0.000151, loss: 0.4064
2022-10-02 00:28:55 - train: epoch 0002, iter [00650, 01251], lr: 0.000151, loss: 0.3931
2022-10-02 00:29:23 - train: epoch 0002, iter [00660, 01251], lr: 0.000152, loss: 0.3915
2022-10-02 00:29:51 - train: epoch 0002, iter [00670, 01251], lr: 0.000152, loss: 0.4212
2022-10-02 00:30:20 - train: epoch 0002, iter [00680, 01251], lr: 0.000153, loss: 0.4006
2022-10-02 00:30:48 - train: epoch 0002, iter [00690, 01251], lr: 0.000153, loss: 0.4077
2022-10-02 00:31:16 - train: epoch 0002, iter [00700, 01251], lr: 0.000154, loss: 0.3978
2022-10-02 00:31:44 - train: epoch 0002, iter [00710, 01251], lr: 0.000154, loss: 0.3859
2022-10-02 00:32:13 - train: epoch 0002, iter [00720, 01251], lr: 0.000155, loss: 0.3957
2022-10-02 00:32:41 - train: epoch 0002, iter [00730, 01251], lr: 0.000155, loss: 0.3893
2022-10-02 00:33:09 - train: epoch 0002, iter [00740, 01251], lr: 0.000155, loss: 0.3916
2022-10-02 00:33:37 - train: epoch 0002, iter [00750, 01251], lr: 0.000156, loss: 0.4157
2022-10-02 00:34:06 - train: epoch 0002, iter [00760, 01251], lr: 0.000156, loss: 0.3964
2022-10-02 00:34:34 - train: epoch 0002, iter [00770, 01251], lr: 0.000157, loss: 0.3992
2022-10-02 00:35:02 - train: epoch 0002, iter [00780, 01251], lr: 0.000157, loss: 0.3876
2022-10-02 00:35:31 - train: epoch 0002, iter [00790, 01251], lr: 0.000158, loss: 0.3900
2022-10-02 00:35:59 - train: epoch 0002, iter [00800, 01251], lr: 0.000158, loss: 0.3773
2022-10-02 00:36:27 - train: epoch 0002, iter [00810, 01251], lr: 0.000159, loss: 0.3897
2022-10-02 00:36:55 - train: epoch 0002, iter [00820, 01251], lr: 0.000159, loss: 0.3814
2022-10-02 00:37:23 - train: epoch 0002, iter [00830, 01251], lr: 0.000160, loss: 0.3908
2022-10-02 00:37:51 - train: epoch 0002, iter [00840, 01251], lr: 0.000160, loss: 0.3943
2022-10-02 00:38:20 - train: epoch 0002, iter [00850, 01251], lr: 0.000161, loss: 0.3818
2022-10-02 00:38:48 - train: epoch 0002, iter [00860, 01251], lr: 0.000161, loss: 0.3876
2022-10-02 00:39:16 - train: epoch 0002, iter [00870, 01251], lr: 0.000162, loss: 0.3771
2022-10-02 00:39:44 - train: epoch 0002, iter [00880, 01251], lr: 0.000162, loss: 0.3830
2022-10-02 00:40:13 - train: epoch 0002, iter [00890, 01251], lr: 0.000163, loss: 0.3884
2022-10-02 00:40:41 - train: epoch 0002, iter [00900, 01251], lr: 0.000163, loss: 0.3909
2022-10-02 00:41:09 - train: epoch 0002, iter [00910, 01251], lr: 0.000164, loss: 0.3682
2022-10-02 00:41:38 - train: epoch 0002, iter [00920, 01251], lr: 0.000164, loss: 0.3839
2022-10-02 00:42:06 - train: epoch 0002, iter [00930, 01251], lr: 0.000165, loss: 0.3763
2022-10-02 00:42:34 - train: epoch 0002, iter [00940, 01251], lr: 0.000165, loss: 0.3819
2022-10-02 00:43:03 - train: epoch 0002, iter [00950, 01251], lr: 0.000166, loss: 0.3704
2022-10-02 00:43:31 - train: epoch 0002, iter [00960, 01251], lr: 0.000166, loss: 0.3774
2022-10-02 00:43:59 - train: epoch 0002, iter [00970, 01251], lr: 0.000167, loss: 0.3694
2022-10-02 00:44:27 - train: epoch 0002, iter [00980, 01251], lr: 0.000167, loss: 0.3587
2022-10-02 00:44:56 - train: epoch 0002, iter [00990, 01251], lr: 0.000167, loss: 0.3651
2022-10-02 00:45:24 - train: epoch 0002, iter [01000, 01251], lr: 0.000168, loss: 0.3652
2022-10-02 00:45:52 - train: epoch 0002, iter [01010, 01251], lr: 0.000168, loss: 0.3743
2022-10-02 00:46:20 - train: epoch 0002, iter [01020, 01251], lr: 0.000169, loss: 0.3663
2022-10-02 00:46:48 - train: epoch 0002, iter [01030, 01251], lr: 0.000169, loss: 0.3620
2022-10-02 00:47:17 - train: epoch 0002, iter [01040, 01251], lr: 0.000170, loss: 0.3712
2022-10-02 00:47:45 - train: epoch 0002, iter [01050, 01251], lr: 0.000170, loss: 0.3715
2022-10-02 00:48:13 - train: epoch 0002, iter [01060, 01251], lr: 0.000171, loss: 0.3622
2022-10-02 00:48:41 - train: epoch 0002, iter [01070, 01251], lr: 0.000171, loss: 0.3616
2022-10-02 00:49:09 - train: epoch 0002, iter [01080, 01251], lr: 0.000172, loss: 0.3730
2022-10-02 00:49:37 - train: epoch 0002, iter [01090, 01251], lr: 0.000172, loss: 0.3751
2022-10-02 00:50:05 - train: epoch 0002, iter [01100, 01251], lr: 0.000173, loss: 0.3738
2022-10-02 00:50:33 - train: epoch 0002, iter [01110, 01251], lr: 0.000173, loss: 0.3730
2022-10-02 00:51:02 - train: epoch 0002, iter [01120, 01251], lr: 0.000174, loss: 0.3644
2022-10-02 00:51:30 - train: epoch 0002, iter [01130, 01251], lr: 0.000174, loss: 0.3720
2022-10-02 00:51:58 - train: epoch 0002, iter [01140, 01251], lr: 0.000175, loss: 0.3502
2022-10-02 00:52:26 - train: epoch 0002, iter [01150, 01251], lr: 0.000175, loss: 0.3515
2022-10-02 00:52:55 - train: epoch 0002, iter [01160, 01251], lr: 0.000176, loss: 0.3627
2022-10-02 00:53:23 - train: epoch 0002, iter [01170, 01251], lr: 0.000176, loss: 0.3657
2022-10-02 00:53:51 - train: epoch 0002, iter [01180, 01251], lr: 0.000177, loss: 0.3555
2022-10-02 00:54:19 - train: epoch 0002, iter [01190, 01251], lr: 0.000177, loss: 0.3545
2022-10-02 00:54:47 - train: epoch 0002, iter [01200, 01251], lr: 0.000178, loss: 0.3450
2022-10-02 00:55:16 - train: epoch 0002, iter [01210, 01251], lr: 0.000178, loss: 0.3500
2022-10-02 00:55:44 - train: epoch 0002, iter [01220, 01251], lr: 0.000179, loss: 0.3537
2022-10-02 00:56:12 - train: epoch 0002, iter [01230, 01251], lr: 0.000179, loss: 0.3555
2022-10-02 00:56:40 - train: epoch 0002, iter [01240, 01251], lr: 0.000179, loss: 0.3552
2022-10-02 00:57:08 - train: epoch 0002, iter [01250, 01251], lr: 0.000180, loss: 0.3671
2022-10-02 00:57:12 - train: epoch 002, train_loss: 0.4082
2022-10-02 00:57:14 - until epoch: 002, best_loss: 0.4082
2022-10-02 00:57:14 - epoch 003 lr: 0.000180
2022-10-02 00:57:50 - train: epoch 0003, iter [00010, 01251], lr: 0.000180, loss: 0.3457
2022-10-02 00:58:18 - train: epoch 0003, iter [00020, 01251], lr: 0.000181, loss: 0.3442
2022-10-02 00:58:46 - train: epoch 0003, iter [00030, 01251], lr: 0.000181, loss: 0.3405
2022-10-02 00:59:15 - train: epoch 0003, iter [00040, 01251], lr: 0.000182, loss: 0.3481
2022-10-02 00:59:43 - train: epoch 0003, iter [00050, 01251], lr: 0.000182, loss: 0.3330
2022-10-02 01:00:12 - train: epoch 0003, iter [00060, 01251], lr: 0.000183, loss: 0.3502
2022-10-02 01:00:40 - train: epoch 0003, iter [00070, 01251], lr: 0.000183, loss: 0.3440
2022-10-02 01:01:08 - train: epoch 0003, iter [00080, 01251], lr: 0.000184, loss: 0.3335
2022-10-02 01:01:37 - train: epoch 0003, iter [00090, 01251], lr: 0.000184, loss: 0.3478
2022-10-02 01:02:05 - train: epoch 0003, iter [00100, 01251], lr: 0.000185, loss: 0.3420
2022-10-02 01:02:33 - train: epoch 0003, iter [00110, 01251], lr: 0.000185, loss: 0.3399
2022-10-02 01:03:02 - train: epoch 0003, iter [00120, 01251], lr: 0.000186, loss: 0.3546
2022-10-02 01:03:30 - train: epoch 0003, iter [00130, 01251], lr: 0.000186, loss: 0.3328
2022-10-02 01:03:58 - train: epoch 0003, iter [00140, 01251], lr: 0.000187, loss: 0.3422
2022-10-02 01:04:27 - train: epoch 0003, iter [00150, 01251], lr: 0.000187, loss: 0.3487
2022-10-02 01:04:55 - train: epoch 0003, iter [00160, 01251], lr: 0.000188, loss: 0.3409
2022-10-02 01:05:23 - train: epoch 0003, iter [00170, 01251], lr: 0.000188, loss: 0.3528
2022-10-02 01:05:52 - train: epoch 0003, iter [00180, 01251], lr: 0.000189, loss: 0.3410
2022-10-02 01:06:20 - train: epoch 0003, iter [00190, 01251], lr: 0.000189, loss: 0.3474
2022-10-02 01:06:48 - train: epoch 0003, iter [00200, 01251], lr: 0.000190, loss: 0.3442
2022-10-02 01:07:16 - train: epoch 0003, iter [00210, 01251], lr: 0.000190, loss: 0.3302
2022-10-02 01:07:45 - train: epoch 0003, iter [00220, 01251], lr: 0.000191, loss: 0.3523
2022-10-02 01:08:13 - train: epoch 0003, iter [00230, 01251], lr: 0.000191, loss: 0.3359
2022-10-02 01:08:41 - train: epoch 0003, iter [00240, 01251], lr: 0.000192, loss: 0.3353
2022-10-02 01:09:10 - train: epoch 0003, iter [00250, 01251], lr: 0.000192, loss: 0.3292
2022-10-02 01:09:38 - train: epoch 0003, iter [00260, 01251], lr: 0.000192, loss: 0.3363
2022-10-02 01:10:06 - train: epoch 0003, iter [00270, 01251], lr: 0.000193, loss: 0.3290
2022-10-02 01:10:34 - train: epoch 0003, iter [00280, 01251], lr: 0.000193, loss: 0.3560
2022-10-02 01:11:02 - train: epoch 0003, iter [00290, 01251], lr: 0.000194, loss: 0.3426
2022-10-02 01:11:30 - train: epoch 0003, iter [00300, 01251], lr: 0.000194, loss: 0.3466
2022-10-02 01:11:58 - train: epoch 0003, iter [00310, 01251], lr: 0.000195, loss: 0.3336
2022-10-02 01:12:27 - train: epoch 0003, iter [00320, 01251], lr: 0.000195, loss: 0.3531
2022-10-02 01:12:55 - train: epoch 0003, iter [00330, 01251], lr: 0.000196, loss: 0.3249
2022-10-02 01:13:23 - train: epoch 0003, iter [00340, 01251], lr: 0.000196, loss: 0.3290
2022-10-02 01:13:51 - train: epoch 0003, iter [00350, 01251], lr: 0.000197, loss: 0.3218
2022-10-02 01:14:19 - train: epoch 0003, iter [00360, 01251], lr: 0.000197, loss: 0.3308
2022-10-02 01:14:48 - train: epoch 0003, iter [00370, 01251], lr: 0.000198, loss: 0.3310
2022-10-02 01:15:16 - train: epoch 0003, iter [00380, 01251], lr: 0.000198, loss: 0.3410
2022-10-02 01:15:44 - train: epoch 0003, iter [00390, 01251], lr: 0.000199, loss: 0.3381
2022-10-02 01:16:12 - train: epoch 0003, iter [00400, 01251], lr: 0.000199, loss: 0.3328
2022-10-02 01:16:40 - train: epoch 0003, iter [00410, 01251], lr: 0.000200, loss: 0.3281
2022-10-02 01:17:09 - train: epoch 0003, iter [00420, 01251], lr: 0.000200, loss: 0.3272
2022-10-02 01:17:37 - train: epoch 0003, iter [00430, 01251], lr: 0.000201, loss: 0.3310
2022-10-02 01:18:05 - train: epoch 0003, iter [00440, 01251], lr: 0.000201, loss: 0.3276
2022-10-02 01:18:33 - train: epoch 0003, iter [00450, 01251], lr: 0.000202, loss: 0.3405
2022-10-02 01:19:02 - train: epoch 0003, iter [00460, 01251], lr: 0.000202, loss: 0.3218
2022-10-02 01:19:30 - train: epoch 0003, iter [00470, 01251], lr: 0.000203, loss: 0.3254
2022-10-02 01:19:58 - train: epoch 0003, iter [00480, 01251], lr: 0.000203, loss: 0.3218
2022-10-02 01:20:26 - train: epoch 0003, iter [00490, 01251], lr: 0.000204, loss: 0.3280
2022-10-02 01:20:54 - train: epoch 0003, iter [00500, 01251], lr: 0.000204, loss: 0.3285
2022-10-02 01:21:23 - train: epoch 0003, iter [00510, 01251], lr: 0.000204, loss: 0.3367
2022-10-02 01:21:51 - train: epoch 0003, iter [00520, 01251], lr: 0.000205, loss: 0.3379
2022-10-02 01:22:19 - train: epoch 0003, iter [00530, 01251], lr: 0.000205, loss: 0.3301
2022-10-02 01:22:47 - train: epoch 0003, iter [00540, 01251], lr: 0.000206, loss: 0.3269
2022-10-02 01:23:15 - train: epoch 0003, iter [00550, 01251], lr: 0.000206, loss: 0.3091
2022-10-02 01:23:43 - train: epoch 0003, iter [00560, 01251], lr: 0.000207, loss: 0.3360
2022-10-02 01:24:10 - train: epoch 0003, iter [00570, 01251], lr: 0.000207, loss: 0.3000
2022-10-02 01:24:38 - train: epoch 0003, iter [00580, 01251], lr: 0.000208, loss: 0.3253
2022-10-02 01:25:07 - train: epoch 0003, iter [00590, 01251], lr: 0.000208, loss: 0.3292
2022-10-02 01:25:34 - train: epoch 0003, iter [00600, 01251], lr: 0.000209, loss: 0.3341
2022-10-02 01:26:02 - train: epoch 0003, iter [00610, 01251], lr: 0.000209, loss: 0.3203
2022-10-02 01:26:30 - train: epoch 0003, iter [00620, 01251], lr: 0.000210, loss: 0.3136
2022-10-02 01:26:58 - train: epoch 0003, iter [00630, 01251], lr: 0.000210, loss: 0.3001
2022-10-02 01:27:26 - train: epoch 0003, iter [00640, 01251], lr: 0.000211, loss: 0.3232
2022-10-02 01:27:54 - train: epoch 0003, iter [00650, 01251], lr: 0.000211, loss: 0.3006
2022-10-02 01:28:22 - train: epoch 0003, iter [00660, 01251], lr: 0.000212, loss: 0.3006
2022-10-02 01:28:50 - train: epoch 0003, iter [00670, 01251], lr: 0.000212, loss: 0.3141
2022-10-02 01:29:18 - train: epoch 0003, iter [00680, 01251], lr: 0.000213, loss: 0.3342
2022-10-02 01:29:46 - train: epoch 0003, iter [00690, 01251], lr: 0.000213, loss: 0.3045
2022-10-02 01:30:14 - train: epoch 0003, iter [00700, 01251], lr: 0.000214, loss: 0.3215
2022-10-02 01:30:42 - train: epoch 0003, iter [00710, 01251], lr: 0.000214, loss: 0.3111
2022-10-02 01:31:11 - train: epoch 0003, iter [00720, 01251], lr: 0.000215, loss: 0.3155
2022-10-02 01:31:39 - train: epoch 0003, iter [00730, 01251], lr: 0.000215, loss: 0.2925
2022-10-02 01:32:07 - train: epoch 0003, iter [00740, 01251], lr: 0.000215, loss: 0.3014
2022-10-02 01:32:35 - train: epoch 0003, iter [00750, 01251], lr: 0.000216, loss: 0.2997
2022-10-02 01:33:03 - train: epoch 0003, iter [00760, 01251], lr: 0.000216, loss: 0.3097
2022-10-02 01:33:31 - train: epoch 0003, iter [00770, 01251], lr: 0.000217, loss: 0.2883
2022-10-02 01:33:59 - train: epoch 0003, iter [00780, 01251], lr: 0.000217, loss: 0.3143
2022-10-02 01:34:27 - train: epoch 0003, iter [00790, 01251], lr: 0.000218, loss: 0.3061
2022-10-02 01:34:56 - train: epoch 0003, iter [00800, 01251], lr: 0.000218, loss: 0.3100
2022-10-02 01:35:24 - train: epoch 0003, iter [00810, 01251], lr: 0.000219, loss: 0.2986
2022-10-02 01:35:51 - train: epoch 0003, iter [00820, 01251], lr: 0.000219, loss: 0.3125
2022-10-02 01:36:19 - train: epoch 0003, iter [00830, 01251], lr: 0.000220, loss: 0.3099
2022-10-02 01:36:47 - train: epoch 0003, iter [00840, 01251], lr: 0.000220, loss: 0.3131
2022-10-02 01:37:15 - train: epoch 0003, iter [00850, 01251], lr: 0.000221, loss: 0.2948
2022-10-02 01:37:43 - train: epoch 0003, iter [00860, 01251], lr: 0.000221, loss: 0.3244
2022-10-02 01:38:11 - train: epoch 0003, iter [00870, 01251], lr: 0.000222, loss: 0.3144
2022-10-02 01:38:39 - train: epoch 0003, iter [00880, 01251], lr: 0.000222, loss: 0.3097
2022-10-02 01:39:07 - train: epoch 0003, iter [00890, 01251], lr: 0.000223, loss: 0.3043
2022-10-02 01:39:36 - train: epoch 0003, iter [00900, 01251], lr: 0.000223, loss: 0.3062
2022-10-02 01:40:04 - train: epoch 0003, iter [00910, 01251], lr: 0.000224, loss: 0.2988
2022-10-02 01:40:32 - train: epoch 0003, iter [00920, 01251], lr: 0.000224, loss: 0.2983
2022-10-02 01:41:00 - train: epoch 0003, iter [00930, 01251], lr: 0.000225, loss: 0.3031
2022-10-02 01:41:27 - train: epoch 0003, iter [00940, 01251], lr: 0.000225, loss: 0.3088
2022-10-02 01:41:55 - train: epoch 0003, iter [00950, 01251], lr: 0.000226, loss: 0.2922
2022-10-02 01:42:24 - train: epoch 0003, iter [00960, 01251], lr: 0.000226, loss: 0.3191
2022-10-02 01:42:52 - train: epoch 0003, iter [00970, 01251], lr: 0.000227, loss: 0.3039
2022-10-02 01:43:20 - train: epoch 0003, iter [00980, 01251], lr: 0.000227, loss: 0.2958
2022-10-02 01:43:48 - train: epoch 0003, iter [00990, 01251], lr: 0.000227, loss: 0.2957
2022-10-02 01:44:15 - train: epoch 0003, iter [01000, 01251], lr: 0.000228, loss: 0.3097
2022-10-02 01:44:44 - train: epoch 0003, iter [01010, 01251], lr: 0.000228, loss: 0.2921
2022-10-02 01:45:12 - train: epoch 0003, iter [01020, 01251], lr: 0.000229, loss: 0.3016
2022-10-02 01:45:40 - train: epoch 0003, iter [01030, 01251], lr: 0.000229, loss: 0.3100
2022-10-02 01:46:08 - train: epoch 0003, iter [01040, 01251], lr: 0.000230, loss: 0.3096
2022-10-02 01:46:36 - train: epoch 0003, iter [01050, 01251], lr: 0.000230, loss: 0.2976
2022-10-02 01:47:04 - train: epoch 0003, iter [01060, 01251], lr: 0.000231, loss: 0.2888
2022-10-02 01:47:33 - train: epoch 0003, iter [01070, 01251], lr: 0.000231, loss: 0.2875
2022-10-02 01:48:01 - train: epoch 0003, iter [01080, 01251], lr: 0.000232, loss: 0.2954
2022-10-02 01:48:29 - train: epoch 0003, iter [01090, 01251], lr: 0.000232, loss: 0.2983
2022-10-02 01:48:57 - train: epoch 0003, iter [01100, 01251], lr: 0.000233, loss: 0.2963
2022-10-02 01:49:25 - train: epoch 0003, iter [01110, 01251], lr: 0.000233, loss: 0.2882
2022-10-02 01:49:54 - train: epoch 0003, iter [01120, 01251], lr: 0.000234, loss: 0.2962
2022-10-02 01:50:22 - train: epoch 0003, iter [01130, 01251], lr: 0.000234, loss: 0.2942
2022-10-02 01:50:50 - train: epoch 0003, iter [01140, 01251], lr: 0.000235, loss: 0.2750
2022-10-02 01:51:18 - train: epoch 0003, iter [01150, 01251], lr: 0.000235, loss: 0.2951
2022-10-02 01:51:46 - train: epoch 0003, iter [01160, 01251], lr: 0.000236, loss: 0.2870
2022-10-02 01:52:14 - train: epoch 0003, iter [01170, 01251], lr: 0.000236, loss: 0.2805
2022-10-02 01:52:42 - train: epoch 0003, iter [01180, 01251], lr: 0.000237, loss: 0.2756
2022-10-02 01:53:10 - train: epoch 0003, iter [01190, 01251], lr: 0.000237, loss: 0.2795
2022-10-02 01:53:39 - train: epoch 0003, iter [01200, 01251], lr: 0.000238, loss: 0.2923
2022-10-02 01:54:07 - train: epoch 0003, iter [01210, 01251], lr: 0.000238, loss: 0.2875
2022-10-02 01:54:35 - train: epoch 0003, iter [01220, 01251], lr: 0.000239, loss: 0.2791
2022-10-02 01:55:03 - train: epoch 0003, iter [01230, 01251], lr: 0.000239, loss: 0.2860
2022-10-02 01:55:31 - train: epoch 0003, iter [01240, 01251], lr: 0.000239, loss: 0.2921
2022-10-02 01:55:59 - train: epoch 0003, iter [01250, 01251], lr: 0.000240, loss: 0.2991
2022-10-02 01:56:03 - train: epoch 003, train_loss: 0.3184
2022-10-02 01:56:05 - until epoch: 003, best_loss: 0.3184
2022-10-02 01:56:05 - epoch 004 lr: 0.000240
2022-10-02 01:56:41 - train: epoch 0004, iter [00010, 01251], lr: 0.000240, loss: 0.3002
2022-10-02 01:57:09 - train: epoch 0004, iter [00020, 01251], lr: 0.000241, loss: 0.2791
2022-10-02 01:57:37 - train: epoch 0004, iter [00030, 01251], lr: 0.000241, loss: 0.2780
2022-10-02 01:58:05 - train: epoch 0004, iter [00040, 01251], lr: 0.000242, loss: 0.2975
2022-10-02 01:58:34 - train: epoch 0004, iter [00050, 01251], lr: 0.000242, loss: 0.2945
2022-10-02 01:59:02 - train: epoch 0004, iter [00060, 01251], lr: 0.000243, loss: 0.2976
2022-10-02 01:59:30 - train: epoch 0004, iter [00070, 01251], lr: 0.000243, loss: 0.2749
2022-10-02 01:59:58 - train: epoch 0004, iter [00080, 01251], lr: 0.000244, loss: 0.2891
2022-10-02 02:00:27 - train: epoch 0004, iter [00090, 01251], lr: 0.000244, loss: 0.2904
2022-10-02 02:00:55 - train: epoch 0004, iter [00100, 01251], lr: 0.000245, loss: 0.2943
2022-10-02 02:01:23 - train: epoch 0004, iter [00110, 01251], lr: 0.000245, loss: 0.2853
2022-10-02 02:01:51 - train: epoch 0004, iter [00120, 01251], lr: 0.000246, loss: 0.2813
2022-10-02 02:02:19 - train: epoch 0004, iter [00130, 01251], lr: 0.000246, loss: 0.2877
2022-10-02 02:02:47 - train: epoch 0004, iter [00140, 01251], lr: 0.000247, loss: 0.2889
2022-10-02 02:03:16 - train: epoch 0004, iter [00150, 01251], lr: 0.000247, loss: 0.2744
2022-10-02 02:03:44 - train: epoch 0004, iter [00160, 01251], lr: 0.000248, loss: 0.2719
2022-10-02 02:04:12 - train: epoch 0004, iter [00170, 01251], lr: 0.000248, loss: 0.2580
2022-10-02 02:04:40 - train: epoch 0004, iter [00180, 01251], lr: 0.000249, loss: 0.2733
2022-10-02 02:05:09 - train: epoch 0004, iter [00190, 01251], lr: 0.000249, loss: 0.2811
2022-10-02 02:05:37 - train: epoch 0004, iter [00200, 01251], lr: 0.000250, loss: 0.2774
2022-10-02 02:06:05 - train: epoch 0004, iter [00210, 01251], lr: 0.000250, loss: 0.2740
2022-10-02 02:06:33 - train: epoch 0004, iter [00220, 01251], lr: 0.000251, loss: 0.2837
2022-10-02 02:07:02 - train: epoch 0004, iter [00230, 01251], lr: 0.000251, loss: 0.2718
2022-10-02 02:07:30 - train: epoch 0004, iter [00240, 01251], lr: 0.000252, loss: 0.2712
2022-10-02 02:07:58 - train: epoch 0004, iter [00250, 01251], lr: 0.000252, loss: 0.2748
2022-10-02 02:08:26 - train: epoch 0004, iter [00260, 01251], lr: 0.000252, loss: 0.2771
2022-10-02 02:08:54 - train: epoch 0004, iter [00270, 01251], lr: 0.000253, loss: 0.2747
2022-10-02 02:09:23 - train: epoch 0004, iter [00280, 01251], lr: 0.000253, loss: 0.2787
2022-10-02 02:09:51 - train: epoch 0004, iter [00290, 01251], lr: 0.000254, loss: 0.2579
2022-10-02 02:10:19 - train: epoch 0004, iter [00300, 01251], lr: 0.000254, loss: 0.2791
2022-10-02 02:10:47 - train: epoch 0004, iter [00310, 01251], lr: 0.000255, loss: 0.2716
2022-10-02 02:11:15 - train: epoch 0004, iter [00320, 01251], lr: 0.000255, loss: 0.2765
2022-10-02 02:11:44 - train: epoch 0004, iter [00330, 01251], lr: 0.000256, loss: 0.2740
2022-10-02 02:12:12 - train: epoch 0004, iter [00340, 01251], lr: 0.000256, loss: 0.2825
2022-10-02 02:12:40 - train: epoch 0004, iter [00350, 01251], lr: 0.000257, loss: 0.2725
2022-10-02 02:13:09 - train: epoch 0004, iter [00360, 01251], lr: 0.000257, loss: 0.2769
2022-10-02 02:13:37 - train: epoch 0004, iter [00370, 01251], lr: 0.000258, loss: 0.2632
2022-10-02 02:14:05 - train: epoch 0004, iter [00380, 01251], lr: 0.000258, loss: 0.2777
2022-10-02 02:14:34 - train: epoch 0004, iter [00390, 01251], lr: 0.000259, loss: 0.2823
2022-10-02 02:15:02 - train: epoch 0004, iter [00400, 01251], lr: 0.000259, loss: 0.2710
2022-10-02 02:15:30 - train: epoch 0004, iter [00410, 01251], lr: 0.000260, loss: 0.2746
2022-10-02 02:15:58 - train: epoch 0004, iter [00420, 01251], lr: 0.000260, loss: 0.2852
2022-10-02 02:16:27 - train: epoch 0004, iter [00430, 01251], lr: 0.000261, loss: 0.2644
2022-10-02 02:16:55 - train: epoch 0004, iter [00440, 01251], lr: 0.000261, loss: 0.2637
2022-10-02 02:17:23 - train: epoch 0004, iter [00450, 01251], lr: 0.000262, loss: 0.2786
2022-10-02 02:17:51 - train: epoch 0004, iter [00460, 01251], lr: 0.000262, loss: 0.2610
2022-10-02 02:18:20 - train: epoch 0004, iter [00470, 01251], lr: 0.000263, loss: 0.2711
2022-10-02 02:18:48 - train: epoch 0004, iter [00480, 01251], lr: 0.000263, loss: 0.2587
2022-10-02 02:19:16 - train: epoch 0004, iter [00490, 01251], lr: 0.000264, loss: 0.2555
2022-10-02 02:19:44 - train: epoch 0004, iter [00500, 01251], lr: 0.000264, loss: 0.2792
2022-10-02 02:20:12 - train: epoch 0004, iter [00510, 01251], lr: 0.000264, loss: 0.2584
2022-10-02 02:20:41 - train: epoch 0004, iter [00520, 01251], lr: 0.000265, loss: 0.2695
2022-10-02 02:21:09 - train: epoch 0004, iter [00530, 01251], lr: 0.000265, loss: 0.2710
2022-10-02 02:21:37 - train: epoch 0004, iter [00540, 01251], lr: 0.000266, loss: 0.2541
2022-10-02 02:22:05 - train: epoch 0004, iter [00550, 01251], lr: 0.000266, loss: 0.2615
2022-10-02 02:22:33 - train: epoch 0004, iter [00560, 01251], lr: 0.000267, loss: 0.2575
2022-10-02 02:23:01 - train: epoch 0004, iter [00570, 01251], lr: 0.000267, loss: 0.2607
2022-10-02 02:23:30 - train: epoch 0004, iter [00580, 01251], lr: 0.000268, loss: 0.2763
2022-10-02 02:23:58 - train: epoch 0004, iter [00590, 01251], lr: 0.000268, loss: 0.2563
2022-10-02 02:24:26 - train: epoch 0004, iter [00600, 01251], lr: 0.000269, loss: 0.2778
2022-10-02 02:24:54 - train: epoch 0004, iter [00610, 01251], lr: 0.000269, loss: 0.2709
2022-10-02 02:25:22 - train: epoch 0004, iter [00620, 01251], lr: 0.000270, loss: 0.2555
2022-10-02 02:25:50 - train: epoch 0004, iter [00630, 01251], lr: 0.000270, loss: 0.2656
2022-10-02 02:26:18 - train: epoch 0004, iter [00640, 01251], lr: 0.000271, loss: 0.2690
2022-10-02 02:26:47 - train: epoch 0004, iter [00650, 01251], lr: 0.000271, loss: 0.2361
2022-10-02 02:27:15 - train: epoch 0004, iter [00660, 01251], lr: 0.000272, loss: 0.2666
2022-10-02 02:27:43 - train: epoch 0004, iter [00670, 01251], lr: 0.000272, loss: 0.2888
2022-10-02 02:28:11 - train: epoch 0004, iter [00680, 01251], lr: 0.000273, loss: 0.2599
2022-10-02 02:28:39 - train: epoch 0004, iter [00690, 01251], lr: 0.000273, loss: 0.2592
2022-10-02 02:29:07 - train: epoch 0004, iter [00700, 01251], lr: 0.000274, loss: 0.2595
2022-10-02 02:29:35 - train: epoch 0004, iter [00710, 01251], lr: 0.000274, loss: 0.2720
2022-10-02 02:30:03 - train: epoch 0004, iter [00720, 01251], lr: 0.000275, loss: 0.2544
2022-10-02 02:30:31 - train: epoch 0004, iter [00730, 01251], lr: 0.000275, loss: 0.2621
2022-10-02 02:31:00 - train: epoch 0004, iter [00740, 01251], lr: 0.000275, loss: 0.2713
2022-10-02 02:31:28 - train: epoch 0004, iter [00750, 01251], lr: 0.000276, loss: 0.2607
2022-10-02 02:31:56 - train: epoch 0004, iter [00760, 01251], lr: 0.000276, loss: 0.2624
2022-10-02 02:32:24 - train: epoch 0004, iter [00770, 01251], lr: 0.000277, loss: 0.2694
2022-10-02 02:32:52 - train: epoch 0004, iter [00780, 01251], lr: 0.000277, loss: 0.2653
2022-10-02 02:33:21 - train: epoch 0004, iter [00790, 01251], lr: 0.000278, loss: 0.2696
2022-10-02 02:33:49 - train: epoch 0004, iter [00800, 01251], lr: 0.000278, loss: 0.2525
2022-10-02 02:34:17 - train: epoch 0004, iter [00810, 01251], lr: 0.000279, loss: 0.2571
2022-10-02 02:34:45 - train: epoch 0004, iter [00820, 01251], lr: 0.000279, loss: 0.2603
2022-10-02 02:35:13 - train: epoch 0004, iter [00830, 01251], lr: 0.000280, loss: 0.2397
2022-10-02 02:35:41 - train: epoch 0004, iter [00840, 01251], lr: 0.000280, loss: 0.2569
2022-10-02 02:36:09 - train: epoch 0004, iter [00850, 01251], lr: 0.000281, loss: 0.2596
2022-10-02 02:36:37 - train: epoch 0004, iter [00860, 01251], lr: 0.000281, loss: 0.2639
2022-10-02 02:37:05 - train: epoch 0004, iter [00870, 01251], lr: 0.000282, loss: 0.2689
2022-10-02 02:37:33 - train: epoch 0004, iter [00880, 01251], lr: 0.000282, loss: 0.2631
2022-10-02 02:38:02 - train: epoch 0004, iter [00890, 01251], lr: 0.000283, loss: 0.2499
2022-10-02 02:38:30 - train: epoch 0004, iter [00900, 01251], lr: 0.000283, loss: 0.2568
2022-10-02 02:38:58 - train: epoch 0004, iter [00910, 01251], lr: 0.000284, loss: 0.2628
2022-10-02 02:39:25 - train: epoch 0004, iter [00920, 01251], lr: 0.000284, loss: 0.2534
2022-10-02 02:39:54 - train: epoch 0004, iter [00930, 01251], lr: 0.000285, loss: 0.2476
2022-10-02 02:40:22 - train: epoch 0004, iter [00940, 01251], lr: 0.000285, loss: 0.2581
2022-10-02 02:40:50 - train: epoch 0004, iter [00950, 01251], lr: 0.000286, loss: 0.2485
2022-10-02 02:41:18 - train: epoch 0004, iter [00960, 01251], lr: 0.000286, loss: 0.2568
2022-10-02 02:41:46 - train: epoch 0004, iter [00970, 01251], lr: 0.000287, loss: 0.2463
2022-10-02 02:42:15 - train: epoch 0004, iter [00980, 01251], lr: 0.000287, loss: 0.2560
2022-10-02 02:42:43 - train: epoch 0004, iter [00990, 01251], lr: 0.000287, loss: 0.2338
2022-10-02 02:43:11 - train: epoch 0004, iter [01000, 01251], lr: 0.000288, loss: 0.2466
2022-10-02 02:43:39 - train: epoch 0004, iter [01010, 01251], lr: 0.000288, loss: 0.2517
2022-10-02 02:44:07 - train: epoch 0004, iter [01020, 01251], lr: 0.000289, loss: 0.2448
2022-10-02 02:44:35 - train: epoch 0004, iter [01030, 01251], lr: 0.000289, loss: 0.2378
2022-10-02 02:45:03 - train: epoch 0004, iter [01040, 01251], lr: 0.000290, loss: 0.2504
2022-10-02 02:45:32 - train: epoch 0004, iter [01050, 01251], lr: 0.000290, loss: 0.2535
2022-10-02 02:46:00 - train: epoch 0004, iter [01060, 01251], lr: 0.000291, loss: 0.2384
2022-10-02 02:46:28 - train: epoch 0004, iter [01070, 01251], lr: 0.000291, loss: 0.2422
2022-10-02 02:46:56 - train: epoch 0004, iter [01080, 01251], lr: 0.000292, loss: 0.2566
2022-10-02 02:47:24 - train: epoch 0004, iter [01090, 01251], lr: 0.000292, loss: 0.2465
2022-10-02 02:47:52 - train: epoch 0004, iter [01100, 01251], lr: 0.000293, loss: 0.2548
2022-10-02 02:48:20 - train: epoch 0004, iter [01110, 01251], lr: 0.000293, loss: 0.2400
2022-10-02 02:48:48 - train: epoch 0004, iter [01120, 01251], lr: 0.000294, loss: 0.2523
2022-10-02 02:49:16 - train: epoch 0004, iter [01130, 01251], lr: 0.000294, loss: 0.2608
2022-10-02 02:49:44 - train: epoch 0004, iter [01140, 01251], lr: 0.000295, loss: 0.2389
2022-10-02 02:50:13 - train: epoch 0004, iter [01150, 01251], lr: 0.000295, loss: 0.2471
2022-10-02 02:50:41 - train: epoch 0004, iter [01160, 01251], lr: 0.000296, loss: 0.2541
2022-10-02 02:51:09 - train: epoch 0004, iter [01170, 01251], lr: 0.000296, loss: 0.2414
2022-10-02 02:51:37 - train: epoch 0004, iter [01180, 01251], lr: 0.000297, loss: 0.2538
2022-10-02 02:52:05 - train: epoch 0004, iter [01190, 01251], lr: 0.000297, loss: 0.2584
2022-10-02 02:52:34 - train: epoch 0004, iter [01200, 01251], lr: 0.000298, loss: 0.2407
2022-10-02 02:53:02 - train: epoch 0004, iter [01210, 01251], lr: 0.000298, loss: 0.2523
2022-10-02 02:53:30 - train: epoch 0004, iter [01220, 01251], lr: 0.000299, loss: 0.2366
2022-10-02 02:53:59 - train: epoch 0004, iter [01230, 01251], lr: 0.000299, loss: 0.2515
2022-10-02 02:54:27 - train: epoch 0004, iter [01240, 01251], lr: 0.000299, loss: 0.2517
2022-10-02 02:54:55 - train: epoch 0004, iter [01250, 01251], lr: 0.000300, loss: 0.2495
2022-10-02 02:54:59 - train: epoch 004, train_loss: 0.2649
2022-10-02 02:55:01 - until epoch: 004, best_loss: 0.2649
2022-10-02 02:55:01 - epoch 005 lr: 0.000300
2022-10-02 02:55:36 - train: epoch 0005, iter [00010, 01251], lr: 0.000300, loss: 0.2449
2022-10-02 02:56:04 - train: epoch 0005, iter [00020, 01251], lr: 0.000301, loss: 0.2594
2022-10-02 02:56:32 - train: epoch 0005, iter [00030, 01251], lr: 0.000301, loss: 0.2365
2022-10-02 02:57:01 - train: epoch 0005, iter [00040, 01251], lr: 0.000302, loss: 0.2449
2022-10-02 02:57:29 - train: epoch 0005, iter [00050, 01251], lr: 0.000302, loss: 0.2518
2022-10-02 02:57:57 - train: epoch 0005, iter [00060, 01251], lr: 0.000303, loss: 0.2462
2022-10-02 02:58:26 - train: epoch 0005, iter [00070, 01251], lr: 0.000303, loss: 0.2399
2022-10-02 02:58:54 - train: epoch 0005, iter [00080, 01251], lr: 0.000304, loss: 0.2382
2022-10-02 02:59:22 - train: epoch 0005, iter [00090, 01251], lr: 0.000304, loss: 0.2393
2022-10-02 02:59:50 - train: epoch 0005, iter [00100, 01251], lr: 0.000305, loss: 0.2496
2022-10-02 03:00:18 - train: epoch 0005, iter [00110, 01251], lr: 0.000305, loss: 0.2478
2022-10-02 03:00:46 - train: epoch 0005, iter [00120, 01251], lr: 0.000306, loss: 0.2350
2022-10-02 03:01:15 - train: epoch 0005, iter [00130, 01251], lr: 0.000306, loss: 0.2527
2022-10-02 03:01:43 - train: epoch 0005, iter [00140, 01251], lr: 0.000307, loss: 0.2511
2022-10-02 03:02:11 - train: epoch 0005, iter [00150, 01251], lr: 0.000307, loss: 0.2530
2022-10-02 03:02:39 - train: epoch 0005, iter [00160, 01251], lr: 0.000308, loss: 0.2459
2022-10-02 03:03:08 - train: epoch 0005, iter [00170, 01251], lr: 0.000308, loss: 0.2218
2022-10-02 03:03:36 - train: epoch 0005, iter [00180, 01251], lr: 0.000309, loss: 0.2452
2022-10-02 03:04:04 - train: epoch 0005, iter [00190, 01251], lr: 0.000309, loss: 0.2463
2022-10-02 03:04:32 - train: epoch 0005, iter [00200, 01251], lr: 0.000310, loss: 0.2350
2022-10-02 03:05:00 - train: epoch 0005, iter [00210, 01251], lr: 0.000310, loss: 0.2465
2022-10-02 03:05:28 - train: epoch 0005, iter [00220, 01251], lr: 0.000311, loss: 0.2381
2022-10-02 03:05:56 - train: epoch 0005, iter [00230, 01251], lr: 0.000311, loss: 0.2412
2022-10-02 03:06:25 - train: epoch 0005, iter [00240, 01251], lr: 0.000312, loss: 0.2361
2022-10-02 03:06:53 - train: epoch 0005, iter [00250, 01251], lr: 0.000312, loss: 0.2377
2022-10-02 03:07:21 - train: epoch 0005, iter [00260, 01251], lr: 0.000312, loss: 0.2221
2022-10-02 03:07:50 - train: epoch 0005, iter [00270, 01251], lr: 0.000313, loss: 0.2415
2022-10-02 03:08:18 - train: epoch 0005, iter [00280, 01251], lr: 0.000313, loss: 0.2439
2022-10-02 03:08:46 - train: epoch 0005, iter [00290, 01251], lr: 0.000314, loss: 0.2367
2022-10-02 03:09:14 - train: epoch 0005, iter [00300, 01251], lr: 0.000314, loss: 0.2281
2022-10-02 03:09:42 - train: epoch 0005, iter [00310, 01251], lr: 0.000315, loss: 0.2414
2022-10-02 03:10:10 - train: epoch 0005, iter [00320, 01251], lr: 0.000315, loss: 0.2437
2022-10-02 03:10:38 - train: epoch 0005, iter [00330, 01251], lr: 0.000316, loss: 0.2452
2022-10-02 03:11:06 - train: epoch 0005, iter [00340, 01251], lr: 0.000316, loss: 0.2342
2022-10-02 03:11:35 - train: epoch 0005, iter [00350, 01251], lr: 0.000317, loss: 0.2227
2022-10-02 03:12:03 - train: epoch 0005, iter [00360, 01251], lr: 0.000317, loss: 0.2403
2022-10-02 03:12:31 - train: epoch 0005, iter [00370, 01251], lr: 0.000318, loss: 0.2373
2022-10-02 03:12:59 - train: epoch 0005, iter [00380, 01251], lr: 0.000318, loss: 0.2394
2022-10-02 03:13:28 - train: epoch 0005, iter [00390, 01251], lr: 0.000319, loss: 0.2411
2022-10-02 03:13:56 - train: epoch 0005, iter [00400, 01251], lr: 0.000319, loss: 0.2508
2022-10-02 03:14:24 - train: epoch 0005, iter [00410, 01251], lr: 0.000320, loss: 0.2387
2022-10-02 03:14:52 - train: epoch 0005, iter [00420, 01251], lr: 0.000320, loss: 0.2280
2022-10-02 03:15:20 - train: epoch 0005, iter [00430, 01251], lr: 0.000321, loss: 0.2162
2022-10-02 03:15:48 - train: epoch 0005, iter [00440, 01251], lr: 0.000321, loss: 0.2415
2022-10-02 03:16:16 - train: epoch 0005, iter [00450, 01251], lr: 0.000322, loss: 0.2296
2022-10-02 03:16:45 - train: epoch 0005, iter [00460, 01251], lr: 0.000322, loss: 0.2290
2022-10-02 03:17:13 - train: epoch 0005, iter [00470, 01251], lr: 0.000323, loss: 0.2313
2022-10-02 03:17:41 - train: epoch 0005, iter [00480, 01251], lr: 0.000323, loss: 0.2355
2022-10-02 03:18:09 - train: epoch 0005, iter [00490, 01251], lr: 0.000324, loss: 0.2273
2022-10-02 03:18:37 - train: epoch 0005, iter [00500, 01251], lr: 0.000324, loss: 0.2385
2022-10-02 03:19:06 - train: epoch 0005, iter [00510, 01251], lr: 0.000324, loss: 0.2528
2022-10-02 03:19:34 - train: epoch 0005, iter [00520, 01251], lr: 0.000325, loss: 0.2310
2022-10-02 03:20:02 - train: epoch 0005, iter [00530, 01251], lr: 0.000325, loss: 0.2330
2022-10-02 03:20:30 - train: epoch 0005, iter [00540, 01251], lr: 0.000326, loss: 0.2208
2022-10-02 03:20:58 - train: epoch 0005, iter [00550, 01251], lr: 0.000326, loss: 0.2366
2022-10-02 03:21:26 - train: epoch 0005, iter [00560, 01251], lr: 0.000327, loss: 0.2411
2022-10-02 03:21:54 - train: epoch 0005, iter [00570, 01251], lr: 0.000327, loss: 0.2404
2022-10-02 03:22:23 - train: epoch 0005, iter [00580, 01251], lr: 0.000328, loss: 0.2328
2022-10-02 03:22:51 - train: epoch 0005, iter [00590, 01251], lr: 0.000328, loss: 0.2255
2022-10-02 03:23:19 - train: epoch 0005, iter [00600, 01251], lr: 0.000329, loss: 0.2405
2022-10-02 03:23:47 - train: epoch 0005, iter [00610, 01251], lr: 0.000329, loss: 0.2309
2022-10-02 03:24:15 - train: epoch 0005, iter [00620, 01251], lr: 0.000330, loss: 0.2086
2022-10-02 03:24:44 - train: epoch 0005, iter [00630, 01251], lr: 0.000330, loss: 0.2203
2022-10-02 03:25:12 - train: epoch 0005, iter [00640, 01251], lr: 0.000331, loss: 0.2286
2022-10-02 03:25:40 - train: epoch 0005, iter [00650, 01251], lr: 0.000331, loss: 0.2333
2022-10-02 03:26:08 - train: epoch 0005, iter [00660, 01251], lr: 0.000332, loss: 0.2370
2022-10-02 03:26:36 - train: epoch 0005, iter [00670, 01251], lr: 0.000332, loss: 0.2306
2022-10-02 03:27:04 - train: epoch 0005, iter [00680, 01251], lr: 0.000333, loss: 0.2330
2022-10-02 03:27:32 - train: epoch 0005, iter [00690, 01251], lr: 0.000333, loss: 0.2211
2022-10-02 03:28:01 - train: epoch 0005, iter [00700, 01251], lr: 0.000334, loss: 0.2218
2022-10-02 03:28:29 - train: epoch 0005, iter [00710, 01251], lr: 0.000334, loss: 0.2305
2022-10-02 03:28:57 - train: epoch 0005, iter [00720, 01251], lr: 0.000335, loss: 0.2365
2022-10-02 03:29:25 - train: epoch 0005, iter [00730, 01251], lr: 0.000335, loss: 0.2139
2022-10-02 03:29:54 - train: epoch 0005, iter [00740, 01251], lr: 0.000335, loss: 0.2286
2022-10-02 03:30:22 - train: epoch 0005, iter [00750, 01251], lr: 0.000336, loss: 0.2241
2022-10-02 03:30:50 - train: epoch 0005, iter [00760, 01251], lr: 0.000336, loss: 0.2211
2022-10-02 03:31:18 - train: epoch 0005, iter [00770, 01251], lr: 0.000337, loss: 0.2314
2022-10-02 03:31:46 - train: epoch 0005, iter [00780, 01251], lr: 0.000337, loss: 0.2165
2022-10-02 03:32:14 - train: epoch 0005, iter [00790, 01251], lr: 0.000338, loss: 0.2198
2022-10-02 03:32:42 - train: epoch 0005, iter [00800, 01251], lr: 0.000338, loss: 0.2164
2022-10-02 03:33:11 - train: epoch 0005, iter [00810, 01251], lr: 0.000339, loss: 0.2120
2022-10-02 03:33:39 - train: epoch 0005, iter [00820, 01251], lr: 0.000339, loss: 0.2115
2022-10-02 03:34:07 - train: epoch 0005, iter [00830, 01251], lr: 0.000340, loss: 0.2113
2022-10-02 03:34:35 - train: epoch 0005, iter [00840, 01251], lr: 0.000340, loss: 0.2168
2022-10-02 03:35:03 - train: epoch 0005, iter [00850, 01251], lr: 0.000341, loss: 0.2234
2022-10-02 03:35:31 - train: epoch 0005, iter [00860, 01251], lr: 0.000341, loss: 0.2130
2022-10-02 03:35:59 - train: epoch 0005, iter [00870, 01251], lr: 0.000342, loss: 0.2410
2022-10-02 03:36:28 - train: epoch 0005, iter [00880, 01251], lr: 0.000342, loss: 0.2288
2022-10-02 03:36:56 - train: epoch 0005, iter [00890, 01251], lr: 0.000343, loss: 0.2199
2022-10-02 03:37:24 - train: epoch 0005, iter [00900, 01251], lr: 0.000343, loss: 0.2216
2022-10-02 03:37:53 - train: epoch 0005, iter [00910, 01251], lr: 0.000344, loss: 0.2211
2022-10-02 03:38:21 - train: epoch 0005, iter [00920, 01251], lr: 0.000344, loss: 0.2298
2022-10-02 03:38:49 - train: epoch 0005, iter [00930, 01251], lr: 0.000345, loss: 0.2283
2022-10-02 03:39:18 - train: epoch 0005, iter [00940, 01251], lr: 0.000345, loss: 0.2326
2022-10-02 03:39:46 - train: epoch 0005, iter [00950, 01251], lr: 0.000346, loss: 0.2259
2022-10-02 03:40:14 - train: epoch 0005, iter [00960, 01251], lr: 0.000346, loss: 0.2154
2022-10-02 03:40:42 - train: epoch 0005, iter [00970, 01251], lr: 0.000347, loss: 0.2112
2022-10-02 03:41:10 - train: epoch 0005, iter [00980, 01251], lr: 0.000347, loss: 0.2346
2022-10-02 03:41:38 - train: epoch 0005, iter [00990, 01251], lr: 0.000347, loss: 0.2346
2022-10-02 03:42:07 - train: epoch 0005, iter [01000, 01251], lr: 0.000348, loss: 0.2025
2022-10-02 03:42:35 - train: epoch 0005, iter [01010, 01251], lr: 0.000348, loss: 0.2285
2022-10-02 03:43:03 - train: epoch 0005, iter [01020, 01251], lr: 0.000349, loss: 0.2214
2022-10-02 03:43:31 - train: epoch 0005, iter [01030, 01251], lr: 0.000349, loss: 0.2091
2022-10-02 03:43:59 - train: epoch 0005, iter [01040, 01251], lr: 0.000350, loss: 0.2247
2022-10-02 03:44:27 - train: epoch 0005, iter [01050, 01251], lr: 0.000350, loss: 0.2152
2022-10-02 03:44:55 - train: epoch 0005, iter [01060, 01251], lr: 0.000351, loss: 0.2185
2022-10-02 03:45:23 - train: epoch 0005, iter [01070, 01251], lr: 0.000351, loss: 0.2086
2022-10-02 03:45:52 - train: epoch 0005, iter [01080, 01251], lr: 0.000352, loss: 0.2167
2022-10-02 03:46:20 - train: epoch 0005, iter [01090, 01251], lr: 0.000352, loss: 0.2180
2022-10-02 03:46:48 - train: epoch 0005, iter [01100, 01251], lr: 0.000353, loss: 0.2132
2022-10-02 03:47:16 - train: epoch 0005, iter [01110, 01251], lr: 0.000353, loss: 0.2207
2022-10-02 03:47:44 - train: epoch 0005, iter [01120, 01251], lr: 0.000354, loss: 0.2162
2022-10-02 03:48:12 - train: epoch 0005, iter [01130, 01251], lr: 0.000354, loss: 0.2090
2022-10-02 03:48:40 - train: epoch 0005, iter [01140, 01251], lr: 0.000355, loss: 0.2204
2022-10-02 03:49:08 - train: epoch 0005, iter [01150, 01251], lr: 0.000355, loss: 0.2188
2022-10-02 03:49:37 - train: epoch 0005, iter [01160, 01251], lr: 0.000356, loss: 0.2175
2022-10-02 03:50:05 - train: epoch 0005, iter [01170, 01251], lr: 0.000356, loss: 0.2226
2022-10-02 03:50:33 - train: epoch 0005, iter [01180, 01251], lr: 0.000357, loss: 0.2062
2022-10-02 03:51:01 - train: epoch 0005, iter [01190, 01251], lr: 0.000357, loss: 0.2231
2022-10-02 03:51:29 - train: epoch 0005, iter [01200, 01251], lr: 0.000358, loss: 0.2160
2022-10-02 03:51:57 - train: epoch 0005, iter [01210, 01251], lr: 0.000358, loss: 0.2203
2022-10-02 03:52:25 - train: epoch 0005, iter [01220, 01251], lr: 0.000359, loss: 0.2299
2022-10-02 03:52:53 - train: epoch 0005, iter [01230, 01251], lr: 0.000359, loss: 0.2277
2022-10-02 03:53:21 - train: epoch 0005, iter [01240, 01251], lr: 0.000359, loss: 0.2110
2022-10-02 03:53:49 - train: epoch 0005, iter [01250, 01251], lr: 0.000360, loss: 0.2255
2022-10-02 03:53:53 - train: epoch 005, train_loss: 0.2292
2022-10-02 03:53:55 - until epoch: 005, best_loss: 0.2292
2022-10-02 03:53:55 - epoch 006 lr: 0.000360
2022-10-02 03:54:30 - train: epoch 0006, iter [00010, 01251], lr: 0.000360, loss: 0.2249
2022-10-02 03:54:58 - train: epoch 0006, iter [00020, 01251], lr: 0.000361, loss: 0.2065
2022-10-02 03:55:27 - train: epoch 0006, iter [00030, 01251], lr: 0.000361, loss: 0.2090
2022-10-02 03:55:55 - train: epoch 0006, iter [00040, 01251], lr: 0.000362, loss: 0.2045
2022-10-02 03:56:24 - train: epoch 0006, iter [00050, 01251], lr: 0.000362, loss: 0.2181
2022-10-02 03:56:52 - train: epoch 0006, iter [00060, 01251], lr: 0.000363, loss: 0.2163
2022-10-02 03:57:20 - train: epoch 0006, iter [00070, 01251], lr: 0.000363, loss: 0.2183
2022-10-02 03:57:48 - train: epoch 0006, iter [00080, 01251], lr: 0.000364, loss: 0.2049
2022-10-02 03:58:16 - train: epoch 0006, iter [00090, 01251], lr: 0.000364, loss: 0.2019
2022-10-02 03:58:45 - train: epoch 0006, iter [00100, 01251], lr: 0.000365, loss: 0.2175
2022-10-02 03:59:12 - train: epoch 0006, iter [00110, 01251], lr: 0.000365, loss: 0.2123
2022-10-02 03:59:40 - train: epoch 0006, iter [00120, 01251], lr: 0.000366, loss: 0.2059
2022-10-02 04:00:09 - train: epoch 0006, iter [00130, 01251], lr: 0.000366, loss: 0.1966
2022-10-02 04:00:37 - train: epoch 0006, iter [00140, 01251], lr: 0.000367, loss: 0.2132
2022-10-02 04:01:05 - train: epoch 0006, iter [00150, 01251], lr: 0.000367, loss: 0.2163
2022-10-02 04:01:33 - train: epoch 0006, iter [00160, 01251], lr: 0.000368, loss: 0.2027
2022-10-02 04:02:01 - train: epoch 0006, iter [00170, 01251], lr: 0.000368, loss: 0.2201
2022-10-02 04:02:29 - train: epoch 0006, iter [00180, 01251], lr: 0.000369, loss: 0.2247
2022-10-02 04:02:57 - train: epoch 0006, iter [00190, 01251], lr: 0.000369, loss: 0.2096
2022-10-02 04:03:25 - train: epoch 0006, iter [00200, 01251], lr: 0.000370, loss: 0.2049
2022-10-02 04:03:53 - train: epoch 0006, iter [00210, 01251], lr: 0.000370, loss: 0.2238
2022-10-02 04:04:22 - train: epoch 0006, iter [00220, 01251], lr: 0.000371, loss: 0.2102
2022-10-02 04:04:50 - train: epoch 0006, iter [00230, 01251], lr: 0.000371, loss: 0.2216
2022-10-02 04:05:18 - train: epoch 0006, iter [00240, 01251], lr: 0.000372, loss: 0.2172
2022-10-02 04:05:46 - train: epoch 0006, iter [00250, 01251], lr: 0.000372, loss: 0.2071
2022-10-02 04:06:15 - train: epoch 0006, iter [00260, 01251], lr: 0.000372, loss: 0.2134
2022-10-02 04:06:43 - train: epoch 0006, iter [00270, 01251], lr: 0.000373, loss: 0.2051
2022-10-02 04:07:11 - train: epoch 0006, iter [00280, 01251], lr: 0.000373, loss: 0.2122
2022-10-02 04:07:39 - train: epoch 0006, iter [00290, 01251], lr: 0.000374, loss: 0.2059
2022-10-02 04:08:07 - train: epoch 0006, iter [00300, 01251], lr: 0.000374, loss: 0.2005
2022-10-02 04:08:35 - train: epoch 0006, iter [00310, 01251], lr: 0.000375, loss: 0.2239
2022-10-02 04:09:04 - train: epoch 0006, iter [00320, 01251], lr: 0.000375, loss: 0.2068
2022-10-02 04:09:32 - train: epoch 0006, iter [00330, 01251], lr: 0.000376, loss: 0.2052
2022-10-02 04:10:00 - train: epoch 0006, iter [00340, 01251], lr: 0.000376, loss: 0.2175
2022-10-02 04:10:28 - train: epoch 0006, iter [00350, 01251], lr: 0.000377, loss: 0.2161
2022-10-02 04:10:57 - train: epoch 0006, iter [00360, 01251], lr: 0.000377, loss: 0.2045
2022-10-02 04:11:25 - train: epoch 0006, iter [00370, 01251], lr: 0.000378, loss: 0.2213
2022-10-02 04:11:53 - train: epoch 0006, iter [00380, 01251], lr: 0.000378, loss: 0.2119
2022-10-02 04:12:21 - train: epoch 0006, iter [00390, 01251], lr: 0.000379, loss: 0.2006
2022-10-02 04:12:49 - train: epoch 0006, iter [00400, 01251], lr: 0.000379, loss: 0.2065
2022-10-02 04:13:17 - train: epoch 0006, iter [00410, 01251], lr: 0.000380, loss: 0.2140
2022-10-02 04:13:46 - train: epoch 0006, iter [00420, 01251], lr: 0.000380, loss: 0.2065
2022-10-02 04:14:14 - train: epoch 0006, iter [00430, 01251], lr: 0.000381, loss: 0.2125
2022-10-02 04:14:42 - train: epoch 0006, iter [00440, 01251], lr: 0.000381, loss: 0.2224
2022-10-02 04:15:10 - train: epoch 0006, iter [00450, 01251], lr: 0.000382, loss: 0.2074
2022-10-02 04:15:38 - train: epoch 0006, iter [00460, 01251], lr: 0.000382, loss: 0.2180
2022-10-02 04:16:06 - train: epoch 0006, iter [00470, 01251], lr: 0.000383, loss: 0.2053
2022-10-02 04:16:35 - train: epoch 0006, iter [00480, 01251], lr: 0.000383, loss: 0.1991
2022-10-02 04:17:03 - train: epoch 0006, iter [00490, 01251], lr: 0.000384, loss: 0.2097
2022-10-02 04:17:31 - train: epoch 0006, iter [00500, 01251], lr: 0.000384, loss: 0.2230
2022-10-02 04:17:59 - train: epoch 0006, iter [00510, 01251], lr: 0.000384, loss: 0.2190
2022-10-02 04:18:27 - train: epoch 0006, iter [00520, 01251], lr: 0.000385, loss: 0.2158
2022-10-02 04:18:56 - train: epoch 0006, iter [00530, 01251], lr: 0.000385, loss: 0.2190
2022-10-02 04:19:24 - train: epoch 0006, iter [00540, 01251], lr: 0.000386, loss: 0.2074
2022-10-02 04:19:52 - train: epoch 0006, iter [00550, 01251], lr: 0.000386, loss: 0.1956
2022-10-02 04:20:20 - train: epoch 0006, iter [00560, 01251], lr: 0.000387, loss: 0.2029
2022-10-02 04:20:48 - train: epoch 0006, iter [00570, 01251], lr: 0.000387, loss: 0.1884
2022-10-02 04:21:16 - train: epoch 0006, iter [00580, 01251], lr: 0.000388, loss: 0.2008
2022-10-02 04:21:44 - train: epoch 0006, iter [00590, 01251], lr: 0.000388, loss: 0.2037
2022-10-02 04:22:13 - train: epoch 0006, iter [00600, 01251], lr: 0.000389, loss: 0.2153
2022-10-02 04:22:41 - train: epoch 0006, iter [00610, 01251], lr: 0.000389, loss: 0.2080
2022-10-02 04:23:09 - train: epoch 0006, iter [00620, 01251], lr: 0.000390, loss: 0.1969
2022-10-02 04:23:37 - train: epoch 0006, iter [00630, 01251], lr: 0.000390, loss: 0.1833
2022-10-02 04:24:06 - train: epoch 0006, iter [00640, 01251], lr: 0.000391, loss: 0.2004
2022-10-02 04:24:34 - train: epoch 0006, iter [00650, 01251], lr: 0.000391, loss: 0.2139
2022-10-02 04:25:02 - train: epoch 0006, iter [00660, 01251], lr: 0.000392, loss: 0.2035
2022-10-02 04:25:30 - train: epoch 0006, iter [00670, 01251], lr: 0.000392, loss: 0.2068
2022-10-02 04:25:59 - train: epoch 0006, iter [00680, 01251], lr: 0.000393, loss: 0.2081
2022-10-02 04:26:27 - train: epoch 0006, iter [00690, 01251], lr: 0.000393, loss: 0.2102
2022-10-02 04:26:55 - train: epoch 0006, iter [00700, 01251], lr: 0.000394, loss: 0.2041
2022-10-02 04:27:23 - train: epoch 0006, iter [00710, 01251], lr: 0.000394, loss: 0.2120
2022-10-02 04:27:51 - train: epoch 0006, iter [00720, 01251], lr: 0.000395, loss: 0.2147
2022-10-02 04:28:19 - train: epoch 0006, iter [00730, 01251], lr: 0.000395, loss: 0.2051
2022-10-02 04:28:47 - train: epoch 0006, iter [00740, 01251], lr: 0.000395, loss: 0.1924
2022-10-02 04:29:16 - train: epoch 0006, iter [00750, 01251], lr: 0.000396, loss: 0.1966
2022-10-02 04:29:44 - train: epoch 0006, iter [00760, 01251], lr: 0.000396, loss: 0.1921
2022-10-02 04:30:12 - train: epoch 0006, iter [00770, 01251], lr: 0.000397, loss: 0.1964
2022-10-02 04:30:40 - train: epoch 0006, iter [00780, 01251], lr: 0.000397, loss: 0.2113
2022-10-02 04:31:09 - train: epoch 0006, iter [00790, 01251], lr: 0.000398, loss: 0.2101
2022-10-02 04:31:37 - train: epoch 0006, iter [00800, 01251], lr: 0.000398, loss: 0.1917
2022-10-02 04:32:05 - train: epoch 0006, iter [00810, 01251], lr: 0.000399, loss: 0.2046
2022-10-02 04:32:33 - train: epoch 0006, iter [00820, 01251], lr: 0.000399, loss: 0.1973
2022-10-02 04:33:01 - train: epoch 0006, iter [00830, 01251], lr: 0.000400, loss: 0.1976
2022-10-02 04:33:29 - train: epoch 0006, iter [00840, 01251], lr: 0.000400, loss: 0.1868
2022-10-02 04:33:58 - train: epoch 0006, iter [00850, 01251], lr: 0.000401, loss: 0.2101
2022-10-02 04:34:26 - train: epoch 0006, iter [00860, 01251], lr: 0.000401, loss: 0.1979
2022-10-02 04:34:54 - train: epoch 0006, iter [00870, 01251], lr: 0.000402, loss: 0.1934
2022-10-02 04:35:22 - train: epoch 0006, iter [00880, 01251], lr: 0.000402, loss: 0.1987
2022-10-02 04:35:50 - train: epoch 0006, iter [00890, 01251], lr: 0.000403, loss: 0.2043
2022-10-02 04:36:19 - train: epoch 0006, iter [00900, 01251], lr: 0.000403, loss: 0.1943
2022-10-02 04:36:47 - train: epoch 0006, iter [00910, 01251], lr: 0.000404, loss: 0.1917
2022-10-02 04:37:15 - train: epoch 0006, iter [00920, 01251], lr: 0.000404, loss: 0.2008
2022-10-02 04:37:43 - train: epoch 0006, iter [00930, 01251], lr: 0.000405, loss: 0.1967
2022-10-02 04:38:11 - train: epoch 0006, iter [00940, 01251], lr: 0.000405, loss: 0.1875
2022-10-02 04:38:39 - train: epoch 0006, iter [00950, 01251], lr: 0.000406, loss: 0.1973
2022-10-02 04:39:07 - train: epoch 0006, iter [00960, 01251], lr: 0.000406, loss: 0.1993
2022-10-02 04:39:35 - train: epoch 0006, iter [00970, 01251], lr: 0.000407, loss: 0.1886
2022-10-02 04:40:03 - train: epoch 0006, iter [00980, 01251], lr: 0.000407, loss: 0.1879
2022-10-02 04:40:31 - train: epoch 0006, iter [00990, 01251], lr: 0.000407, loss: 0.2033
2022-10-02 04:40:59 - train: epoch 0006, iter [01000, 01251], lr: 0.000408, loss: 0.1899
2022-10-02 04:41:27 - train: epoch 0006, iter [01010, 01251], lr: 0.000408, loss: 0.2050
2022-10-02 04:41:55 - train: epoch 0006, iter [01020, 01251], lr: 0.000409, loss: 0.1819
2022-10-02 04:42:23 - train: epoch 0006, iter [01030, 01251], lr: 0.000409, loss: 0.1910
2022-10-02 04:42:52 - train: epoch 0006, iter [01040, 01251], lr: 0.000410, loss: 0.1992
2022-10-02 04:43:20 - train: epoch 0006, iter [01050, 01251], lr: 0.000410, loss: 0.1935
2022-10-02 04:43:48 - train: epoch 0006, iter [01060, 01251], lr: 0.000411, loss: 0.1997
2022-10-02 04:44:16 - train: epoch 0006, iter [01070, 01251], lr: 0.000411, loss: 0.2139
2022-10-02 04:44:44 - train: epoch 0006, iter [01080, 01251], lr: 0.000412, loss: 0.2016
2022-10-02 04:45:12 - train: epoch 0006, iter [01090, 01251], lr: 0.000412, loss: 0.2014
2022-10-02 04:45:40 - train: epoch 0006, iter [01100, 01251], lr: 0.000413, loss: 0.1926
2022-10-02 04:46:08 - train: epoch 0006, iter [01110, 01251], lr: 0.000413, loss: 0.1977
2022-10-02 04:46:36 - train: epoch 0006, iter [01120, 01251], lr: 0.000414, loss: 0.1882
2022-10-02 04:47:04 - train: epoch 0006, iter [01130, 01251], lr: 0.000414, loss: 0.2070
2022-10-02 04:47:32 - train: epoch 0006, iter [01140, 01251], lr: 0.000415, loss: 0.2002
2022-10-02 04:48:00 - train: epoch 0006, iter [01150, 01251], lr: 0.000415, loss: 0.1974
2022-10-02 04:48:28 - train: epoch 0006, iter [01160, 01251], lr: 0.000416, loss: 0.1973
2022-10-02 04:48:56 - train: epoch 0006, iter [01170, 01251], lr: 0.000416, loss: 0.1928
2022-10-02 04:49:24 - train: epoch 0006, iter [01180, 01251], lr: 0.000417, loss: 0.1954
2022-10-02 04:49:52 - train: epoch 0006, iter [01190, 01251], lr: 0.000417, loss: 0.2074
2022-10-02 04:50:20 - train: epoch 0006, iter [01200, 01251], lr: 0.000418, loss: 0.1946
2022-10-02 04:50:48 - train: epoch 0006, iter [01210, 01251], lr: 0.000418, loss: 0.1881
2022-10-02 04:51:16 - train: epoch 0006, iter [01220, 01251], lr: 0.000419, loss: 0.2158
2022-10-02 04:51:44 - train: epoch 0006, iter [01230, 01251], lr: 0.000419, loss: 0.1916
2022-10-02 04:52:12 - train: epoch 0006, iter [01240, 01251], lr: 0.000419, loss: 0.1903
2022-10-02 04:52:40 - train: epoch 0006, iter [01250, 01251], lr: 0.000420, loss: 0.2034
2022-10-02 04:52:44 - train: epoch 006, train_loss: 0.2038
2022-10-02 04:52:46 - until epoch: 006, best_loss: 0.2038
2022-10-02 04:52:46 - epoch 007 lr: 0.000420
2022-10-02 04:53:21 - train: epoch 0007, iter [00010, 01251], lr: 0.000420, loss: 0.1932
2022-10-02 04:53:49 - train: epoch 0007, iter [00020, 01251], lr: 0.000421, loss: 0.1880
2022-10-02 04:54:17 - train: epoch 0007, iter [00030, 01251], lr: 0.000421, loss: 0.1926
2022-10-02 04:54:45 - train: epoch 0007, iter [00040, 01251], lr: 0.000422, loss: 0.1850
2022-10-02 04:55:14 - train: epoch 0007, iter [00050, 01251], lr: 0.000422, loss: 0.1872
2022-10-02 04:55:42 - train: epoch 0007, iter [00060, 01251], lr: 0.000423, loss: 0.1918
2022-10-02 04:56:10 - train: epoch 0007, iter [00070, 01251], lr: 0.000423, loss: 0.1961
2022-10-02 04:56:38 - train: epoch 0007, iter [00080, 01251], lr: 0.000424, loss: 0.1853
2022-10-02 04:57:06 - train: epoch 0007, iter [00090, 01251], lr: 0.000424, loss: 0.1912
2022-10-02 04:57:34 - train: epoch 0007, iter [00100, 01251], lr: 0.000425, loss: 0.2018
2022-10-02 04:58:03 - train: epoch 0007, iter [00110, 01251], lr: 0.000425, loss: 0.1966
2022-10-02 04:58:31 - train: epoch 0007, iter [00120, 01251], lr: 0.000426, loss: 0.1830
2022-10-02 04:58:59 - train: epoch 0007, iter [00130, 01251], lr: 0.000426, loss: 0.1902
2022-10-02 04:59:27 - train: epoch 0007, iter [00140, 01251], lr: 0.000427, loss: 0.1881
2022-10-02 04:59:55 - train: epoch 0007, iter [00150, 01251], lr: 0.000427, loss: 0.1955
2022-10-02 05:00:23 - train: epoch 0007, iter [00160, 01251], lr: 0.000428, loss: 0.1861
2022-10-02 05:00:51 - train: epoch 0007, iter [00170, 01251], lr: 0.000428, loss: 0.1964
2022-10-02 05:01:19 - train: epoch 0007, iter [00180, 01251], lr: 0.000429, loss: 0.1938
2022-10-02 05:01:47 - train: epoch 0007, iter [00190, 01251], lr: 0.000429, loss: 0.1869
2022-10-02 05:02:15 - train: epoch 0007, iter [00200, 01251], lr: 0.000430, loss: 0.1832
2022-10-02 05:02:43 - train: epoch 0007, iter [00210, 01251], lr: 0.000430, loss: 0.1835
2022-10-02 05:03:12 - train: epoch 0007, iter [00220, 01251], lr: 0.000431, loss: 0.2015
2022-10-02 05:03:40 - train: epoch 0007, iter [00230, 01251], lr: 0.000431, loss: 0.1930
2022-10-02 05:04:08 - train: epoch 0007, iter [00240, 01251], lr: 0.000432, loss: 0.1830
2022-10-02 05:04:36 - train: epoch 0007, iter [00250, 01251], lr: 0.000432, loss: 0.1886
2022-10-02 05:05:04 - train: epoch 0007, iter [00260, 01251], lr: 0.000432, loss: 0.2058
2022-10-02 05:05:32 - train: epoch 0007, iter [00270, 01251], lr: 0.000433, loss: 0.1898
2022-10-02 05:06:00 - train: epoch 0007, iter [00280, 01251], lr: 0.000433, loss: 0.1902
2022-10-02 05:06:28 - train: epoch 0007, iter [00290, 01251], lr: 0.000434, loss: 0.1779
2022-10-02 05:06:57 - train: epoch 0007, iter [00300, 01251], lr: 0.000434, loss: 0.1828
2022-10-02 05:07:25 - train: epoch 0007, iter [00310, 01251], lr: 0.000435, loss: 0.1933
2022-10-02 05:07:53 - train: epoch 0007, iter [00320, 01251], lr: 0.000435, loss: 0.1884
2022-10-02 05:08:21 - train: epoch 0007, iter [00330, 01251], lr: 0.000436, loss: 0.1969
2022-10-02 05:08:49 - train: epoch 0007, iter [00340, 01251], lr: 0.000436, loss: 0.1800
2022-10-02 05:09:18 - train: epoch 0007, iter [00350, 01251], lr: 0.000437, loss: 0.1837
2022-10-02 05:09:46 - train: epoch 0007, iter [00360, 01251], lr: 0.000437, loss: 0.1886
2022-10-02 05:10:14 - train: epoch 0007, iter [00370, 01251], lr: 0.000438, loss: 0.1809
2022-10-02 05:10:42 - train: epoch 0007, iter [00380, 01251], lr: 0.000438, loss: 0.1962
2022-10-02 05:11:10 - train: epoch 0007, iter [00390, 01251], lr: 0.000439, loss: 0.1832
2022-10-02 05:11:39 - train: epoch 0007, iter [00400, 01251], lr: 0.000439, loss: 0.1966
2022-10-02 05:12:07 - train: epoch 0007, iter [00410, 01251], lr: 0.000440, loss: 0.1786
2022-10-02 05:12:35 - train: epoch 0007, iter [00420, 01251], lr: 0.000440, loss: 0.1881
2022-10-02 05:13:03 - train: epoch 0007, iter [00430, 01251], lr: 0.000441, loss: 0.1749
2022-10-02 05:13:31 - train: epoch 0007, iter [00440, 01251], lr: 0.000441, loss: 0.1826
2022-10-02 05:13:59 - train: epoch 0007, iter [00450, 01251], lr: 0.000442, loss: 0.1876
2022-10-02 05:14:27 - train: epoch 0007, iter [00460, 01251], lr: 0.000442, loss: 0.1764
2022-10-02 05:14:55 - train: epoch 0007, iter [00470, 01251], lr: 0.000443, loss: 0.1921
2022-10-02 05:15:24 - train: epoch 0007, iter [00480, 01251], lr: 0.000443, loss: 0.1815
2022-10-02 05:15:52 - train: epoch 0007, iter [00490, 01251], lr: 0.000444, loss: 0.1934
2022-10-02 05:16:20 - train: epoch 0007, iter [00500, 01251], lr: 0.000444, loss: 0.1836
2022-10-02 05:16:48 - train: epoch 0007, iter [00510, 01251], lr: 0.000444, loss: 0.1866
2022-10-02 05:17:16 - train: epoch 0007, iter [00520, 01251], lr: 0.000445, loss: 0.1849
2022-10-02 05:17:44 - train: epoch 0007, iter [00530, 01251], lr: 0.000445, loss: 0.1899
2022-10-02 05:18:12 - train: epoch 0007, iter [00540, 01251], lr: 0.000446, loss: 0.1858
2022-10-02 05:18:40 - train: epoch 0007, iter [00550, 01251], lr: 0.000446, loss: 0.1832
2022-10-02 05:19:08 - train: epoch 0007, iter [00560, 01251], lr: 0.000447, loss: 0.1776
2022-10-02 05:19:37 - train: epoch 0007, iter [00570, 01251], lr: 0.000447, loss: 0.2013
2022-10-02 05:20:05 - train: epoch 0007, iter [00580, 01251], lr: 0.000448, loss: 0.1943
2022-10-02 05:20:33 - train: epoch 0007, iter [00590, 01251], lr: 0.000448, loss: 0.1950
2022-10-02 05:21:01 - train: epoch 0007, iter [00600, 01251], lr: 0.000449, loss: 0.1941
2022-10-02 05:21:29 - train: epoch 0007, iter [00610, 01251], lr: 0.000449, loss: 0.1691
2022-10-02 05:21:58 - train: epoch 0007, iter [00620, 01251], lr: 0.000450, loss: 0.1811
2022-10-02 05:22:26 - train: epoch 0007, iter [00630, 01251], lr: 0.000450, loss: 0.1841
2022-10-02 05:22:54 - train: epoch 0007, iter [00640, 01251], lr: 0.000451, loss: 0.1745
2022-10-02 05:23:22 - train: epoch 0007, iter [00650, 01251], lr: 0.000451, loss: 0.1885
2022-10-02 05:23:50 - train: epoch 0007, iter [00660, 01251], lr: 0.000452, loss: 0.1874
2022-10-02 05:24:18 - train: epoch 0007, iter [00670, 01251], lr: 0.000452, loss: 0.1784
2022-10-02 05:24:46 - train: epoch 0007, iter [00680, 01251], lr: 0.000453, loss: 0.1810
2022-10-02 05:25:14 - train: epoch 0007, iter [00690, 01251], lr: 0.000453, loss: 0.1723
2022-10-02 05:25:42 - train: epoch 0007, iter [00700, 01251], lr: 0.000454, loss: 0.1877
2022-10-02 05:26:10 - train: epoch 0007, iter [00710, 01251], lr: 0.000454, loss: 0.1826
2022-10-02 05:26:39 - train: epoch 0007, iter [00720, 01251], lr: 0.000455, loss: 0.1808
2022-10-02 05:27:07 - train: epoch 0007, iter [00730, 01251], lr: 0.000455, loss: 0.2002
2022-10-02 05:27:35 - train: epoch 0007, iter [00740, 01251], lr: 0.000455, loss: 0.1985
2022-10-02 05:28:03 - train: epoch 0007, iter [00750, 01251], lr: 0.000456, loss: 0.1789
2022-10-02 05:28:31 - train: epoch 0007, iter [00760, 01251], lr: 0.000456, loss: 0.1803
2022-10-02 05:28:59 - train: epoch 0007, iter [00770, 01251], lr: 0.000457, loss: 0.1790
2022-10-02 05:29:28 - train: epoch 0007, iter [00780, 01251], lr: 0.000457, loss: 0.1824
2022-10-02 05:29:56 - train: epoch 0007, iter [00790, 01251], lr: 0.000458, loss: 0.1821
2022-10-02 05:30:24 - train: epoch 0007, iter [00800, 01251], lr: 0.000458, loss: 0.1768
2022-10-02 05:30:52 - train: epoch 0007, iter [00810, 01251], lr: 0.000459, loss: 0.1823
2022-10-02 05:31:20 - train: epoch 0007, iter [00820, 01251], lr: 0.000459, loss: 0.1791
2022-10-02 05:31:48 - train: epoch 0007, iter [00830, 01251], lr: 0.000460, loss: 0.1814
2022-10-02 05:32:16 - train: epoch 0007, iter [00840, 01251], lr: 0.000460, loss: 0.1852
2022-10-02 05:32:44 - train: epoch 0007, iter [00850, 01251], lr: 0.000461, loss: 0.1793
2022-10-02 05:33:12 - train: epoch 0007, iter [00860, 01251], lr: 0.000461, loss: 0.1919
2022-10-02 05:33:41 - train: epoch 0007, iter [00870, 01251], lr: 0.000462, loss: 0.1698
2022-10-02 05:34:09 - train: epoch 0007, iter [00880, 01251], lr: 0.000462, loss: 0.1780
2022-10-02 05:34:37 - train: epoch 0007, iter [00890, 01251], lr: 0.000463, loss: 0.1919
2022-10-02 05:35:05 - train: epoch 0007, iter [00900, 01251], lr: 0.000463, loss: 0.1786
2022-10-02 05:35:33 - train: epoch 0007, iter [00910, 01251], lr: 0.000464, loss: 0.1711
2022-10-02 05:36:01 - train: epoch 0007, iter [00920, 01251], lr: 0.000464, loss: 0.1876
2022-10-02 05:36:29 - train: epoch 0007, iter [00930, 01251], lr: 0.000465, loss: 0.1826
2022-10-02 05:36:57 - train: epoch 0007, iter [00940, 01251], lr: 0.000465, loss: 0.1768
2022-10-02 05:37:26 - train: epoch 0007, iter [00950, 01251], lr: 0.000466, loss: 0.1847
2022-10-02 05:37:54 - train: epoch 0007, iter [00960, 01251], lr: 0.000466, loss: 0.1773
2022-10-02 05:38:22 - train: epoch 0007, iter [00970, 01251], lr: 0.000467, loss: 0.1837
2022-10-02 05:38:50 - train: epoch 0007, iter [00980, 01251], lr: 0.000467, loss: 0.1685
2022-10-02 05:39:19 - train: epoch 0007, iter [00990, 01251], lr: 0.000467, loss: 0.1802
2022-10-02 05:39:47 - train: epoch 0007, iter [01000, 01251], lr: 0.000468, loss: 0.1818
2022-10-02 05:40:15 - train: epoch 0007, iter [01010, 01251], lr: 0.000468, loss: 0.1803
2022-10-02 05:40:43 - train: epoch 0007, iter [01020, 01251], lr: 0.000469, loss: 0.1903
2022-10-02 05:41:12 - train: epoch 0007, iter [01030, 01251], lr: 0.000469, loss: 0.1778
2022-10-02 05:41:40 - train: epoch 0007, iter [01040, 01251], lr: 0.000470, loss: 0.1867
2022-10-02 05:42:08 - train: epoch 0007, iter [01050, 01251], lr: 0.000470, loss: 0.1858
2022-10-02 05:42:36 - train: epoch 0007, iter [01060, 01251], lr: 0.000471, loss: 0.1869
2022-10-02 05:43:04 - train: epoch 0007, iter [01070, 01251], lr: 0.000471, loss: 0.1787
2022-10-02 05:43:32 - train: epoch 0007, iter [01080, 01251], lr: 0.000472, loss: 0.1749
2022-10-02 05:44:01 - train: epoch 0007, iter [01090, 01251], lr: 0.000472, loss: 0.1731
2022-10-02 05:44:29 - train: epoch 0007, iter [01100, 01251], lr: 0.000473, loss: 0.1802
2022-10-02 05:44:57 - train: epoch 0007, iter [01110, 01251], lr: 0.000473, loss: 0.1763
2022-10-02 05:45:25 - train: epoch 0007, iter [01120, 01251], lr: 0.000474, loss: 0.1799
2022-10-02 05:45:53 - train: epoch 0007, iter [01130, 01251], lr: 0.000474, loss: 0.1815
2022-10-02 05:46:22 - train: epoch 0007, iter [01140, 01251], lr: 0.000475, loss: 0.1870
2022-10-02 05:46:50 - train: epoch 0007, iter [01150, 01251], lr: 0.000475, loss: 0.1747
2022-10-02 05:47:18 - train: epoch 0007, iter [01160, 01251], lr: 0.000476, loss: 0.1790
2022-10-02 05:47:47 - train: epoch 0007, iter [01170, 01251], lr: 0.000476, loss: 0.1776
2022-10-02 05:48:15 - train: epoch 0007, iter [01180, 01251], lr: 0.000477, loss: 0.1817
2022-10-02 05:48:43 - train: epoch 0007, iter [01190, 01251], lr: 0.000477, loss: 0.1671
2022-10-02 05:49:11 - train: epoch 0007, iter [01200, 01251], lr: 0.000478, loss: 0.1799
2022-10-02 05:49:39 - train: epoch 0007, iter [01210, 01251], lr: 0.000478, loss: 0.1703
2022-10-02 05:50:08 - train: epoch 0007, iter [01220, 01251], lr: 0.000479, loss: 0.1666
2022-10-02 05:50:36 - train: epoch 0007, iter [01230, 01251], lr: 0.000479, loss: 0.1801
2022-10-02 05:51:04 - train: epoch 0007, iter [01240, 01251], lr: 0.000479, loss: 0.1769
2022-10-02 05:51:32 - train: epoch 0007, iter [01250, 01251], lr: 0.000480, loss: 0.1798
2022-10-02 05:51:36 - train: epoch 007, train_loss: 0.1850
2022-10-02 05:51:38 - until epoch: 007, best_loss: 0.1850
2022-10-02 05:51:38 - epoch 008 lr: 0.000480
2022-10-02 05:52:13 - train: epoch 0008, iter [00010, 01251], lr: 0.000480, loss: 0.1708
2022-10-02 05:52:40 - train: epoch 0008, iter [00020, 01251], lr: 0.000481, loss: 0.1742
2022-10-02 05:53:08 - train: epoch 0008, iter [00030, 01251], lr: 0.000481, loss: 0.1784
2022-10-02 05:53:36 - train: epoch 0008, iter [00040, 01251], lr: 0.000482, loss: 0.1718
2022-10-02 05:54:05 - train: epoch 0008, iter [00050, 01251], lr: 0.000482, loss: 0.1635
2022-10-02 05:54:33 - train: epoch 0008, iter [00060, 01251], lr: 0.000483, loss: 0.1790
2022-10-02 05:55:01 - train: epoch 0008, iter [00070, 01251], lr: 0.000483, loss: 0.1709
2022-10-02 05:55:29 - train: epoch 0008, iter [00080, 01251], lr: 0.000484, loss: 0.1695
2022-10-02 05:55:57 - train: epoch 0008, iter [00090, 01251], lr: 0.000484, loss: 0.1855
2022-10-02 05:56:25 - train: epoch 0008, iter [00100, 01251], lr: 0.000485, loss: 0.1777
2022-10-02 05:56:53 - train: epoch 0008, iter [00110, 01251], lr: 0.000485, loss: 0.1719
2022-10-02 05:57:22 - train: epoch 0008, iter [00120, 01251], lr: 0.000486, loss: 0.1865
2022-10-02 05:57:50 - train: epoch 0008, iter [00130, 01251], lr: 0.000486, loss: 0.1786
2022-10-02 05:58:18 - train: epoch 0008, iter [00140, 01251], lr: 0.000487, loss: 0.1699
2022-10-02 05:58:46 - train: epoch 0008, iter [00150, 01251], lr: 0.000487, loss: 0.1746
2022-10-02 05:59:14 - train: epoch 0008, iter [00160, 01251], lr: 0.000488, loss: 0.1835
2022-10-02 05:59:42 - train: epoch 0008, iter [00170, 01251], lr: 0.000488, loss: 0.1671
2022-10-02 06:00:10 - train: epoch 0008, iter [00180, 01251], lr: 0.000489, loss: 0.1645
2022-10-02 06:00:38 - train: epoch 0008, iter [00190, 01251], lr: 0.000489, loss: 0.1701
2022-10-02 06:01:06 - train: epoch 0008, iter [00200, 01251], lr: 0.000490, loss: 0.1768
2022-10-02 06:01:34 - train: epoch 0008, iter [00210, 01251], lr: 0.000490, loss: 0.1652
2022-10-02 06:02:02 - train: epoch 0008, iter [00220, 01251], lr: 0.000491, loss: 0.1730
2022-10-02 06:02:30 - train: epoch 0008, iter [00230, 01251], lr: 0.000491, loss: 0.1760
2022-10-02 06:02:58 - train: epoch 0008, iter [00240, 01251], lr: 0.000492, loss: 0.1633
2022-10-02 06:03:26 - train: epoch 0008, iter [00250, 01251], lr: 0.000492, loss: 0.1628
2022-10-02 06:03:54 - train: epoch 0008, iter [00260, 01251], lr: 0.000492, loss: 0.1809
2022-10-02 06:04:23 - train: epoch 0008, iter [00270, 01251], lr: 0.000493, loss: 0.1727
2022-10-02 06:04:51 - train: epoch 0008, iter [00280, 01251], lr: 0.000493, loss: 0.1669
2022-10-02 06:05:19 - train: epoch 0008, iter [00290, 01251], lr: 0.000494, loss: 0.1641
2022-10-02 06:05:47 - train: epoch 0008, iter [00300, 01251], lr: 0.000494, loss: 0.1665
2022-10-02 06:06:15 - train: epoch 0008, iter [00310, 01251], lr: 0.000495, loss: 0.1723
2022-10-02 06:06:43 - train: epoch 0008, iter [00320, 01251], lr: 0.000495, loss: 0.1683
2022-10-02 06:07:11 - train: epoch 0008, iter [00330, 01251], lr: 0.000496, loss: 0.1705
2022-10-02 06:07:40 - train: epoch 0008, iter [00340, 01251], lr: 0.000496, loss: 0.1754
2022-10-02 06:08:08 - train: epoch 0008, iter [00350, 01251], lr: 0.000497, loss: 0.1729
2022-10-02 06:08:36 - train: epoch 0008, iter [00360, 01251], lr: 0.000497, loss: 0.1735
2022-10-02 06:09:04 - train: epoch 0008, iter [00370, 01251], lr: 0.000498, loss: 0.1760
2022-10-02 06:09:32 - train: epoch 0008, iter [00380, 01251], lr: 0.000498, loss: 0.1864
2022-10-02 06:10:00 - train: epoch 0008, iter [00390, 01251], lr: 0.000499, loss: 0.1779
2022-10-02 06:10:28 - train: epoch 0008, iter [00400, 01251], lr: 0.000499, loss: 0.1642
2022-10-02 06:10:56 - train: epoch 0008, iter [00410, 01251], lr: 0.000500, loss: 0.1744
2022-10-02 06:11:24 - train: epoch 0008, iter [00420, 01251], lr: 0.000500, loss: 0.1750
2022-10-02 06:11:52 - train: epoch 0008, iter [00430, 01251], lr: 0.000501, loss: 0.1852
2022-10-02 06:12:20 - train: epoch 0008, iter [00440, 01251], lr: 0.000501, loss: 0.1629
2022-10-02 06:12:49 - train: epoch 0008, iter [00450, 01251], lr: 0.000502, loss: 0.1757
2022-10-02 06:13:17 - train: epoch 0008, iter [00460, 01251], lr: 0.000502, loss: 0.1818
2022-10-02 06:13:45 - train: epoch 0008, iter [00470, 01251], lr: 0.000503, loss: 0.1626
2022-10-02 06:14:13 - train: epoch 0008, iter [00480, 01251], lr: 0.000503, loss: 0.1833
2022-10-02 06:14:41 - train: epoch 0008, iter [00490, 01251], lr: 0.000504, loss: 0.1784
2022-10-02 06:15:10 - train: epoch 0008, iter [00500, 01251], lr: 0.000504, loss: 0.1691
2022-10-02 06:15:38 - train: epoch 0008, iter [00510, 01251], lr: 0.000504, loss: 0.1814
2022-10-02 06:16:06 - train: epoch 0008, iter [00520, 01251], lr: 0.000505, loss: 0.1644
2022-10-02 06:16:34 - train: epoch 0008, iter [00530, 01251], lr: 0.000505, loss: 0.1693
2022-10-02 06:17:02 - train: epoch 0008, iter [00540, 01251], lr: 0.000506, loss: 0.1728
2022-10-02 06:17:30 - train: epoch 0008, iter [00550, 01251], lr: 0.000506, loss: 0.1661
2022-10-02 06:17:58 - train: epoch 0008, iter [00560, 01251], lr: 0.000507, loss: 0.1707
2022-10-02 06:18:26 - train: epoch 0008, iter [00570, 01251], lr: 0.000507, loss: 0.1664
2022-10-02 06:18:54 - train: epoch 0008, iter [00580, 01251], lr: 0.000508, loss: 0.1706
2022-10-02 06:19:22 - train: epoch 0008, iter [00590, 01251], lr: 0.000508, loss: 0.1629
2022-10-02 06:19:50 - train: epoch 0008, iter [00600, 01251], lr: 0.000509, loss: 0.1652
2022-10-02 06:20:18 - train: epoch 0008, iter [00610, 01251], lr: 0.000509, loss: 0.1595
2022-10-02 06:20:46 - train: epoch 0008, iter [00620, 01251], lr: 0.000510, loss: 0.1660
2022-10-02 06:21:14 - train: epoch 0008, iter [00630, 01251], lr: 0.000510, loss: 0.1750
2022-10-02 06:21:42 - train: epoch 0008, iter [00640, 01251], lr: 0.000511, loss: 0.1616
2022-10-02 06:22:10 - train: epoch 0008, iter [00650, 01251], lr: 0.000511, loss: 0.1651
2022-10-02 06:22:39 - train: epoch 0008, iter [00660, 01251], lr: 0.000512, loss: 0.1649
2022-10-02 06:23:07 - train: epoch 0008, iter [00670, 01251], lr: 0.000512, loss: 0.1733
2022-10-02 06:23:35 - train: epoch 0008, iter [00680, 01251], lr: 0.000513, loss: 0.1681
2022-10-02 06:24:03 - train: epoch 0008, iter [00690, 01251], lr: 0.000513, loss: 0.1807
2022-10-02 06:24:31 - train: epoch 0008, iter [00700, 01251], lr: 0.000514, loss: 0.1681
2022-10-02 06:24:59 - train: epoch 0008, iter [00710, 01251], lr: 0.000514, loss: 0.1572
2022-10-02 06:25:27 - train: epoch 0008, iter [00720, 01251], lr: 0.000515, loss: 0.1705
2022-10-02 06:25:55 - train: epoch 0008, iter [00730, 01251], lr: 0.000515, loss: 0.1725
2022-10-02 06:26:23 - train: epoch 0008, iter [00740, 01251], lr: 0.000515, loss: 0.1690
2022-10-02 06:26:52 - train: epoch 0008, iter [00750, 01251], lr: 0.000516, loss: 0.1694
2022-10-02 06:27:20 - train: epoch 0008, iter [00760, 01251], lr: 0.000516, loss: 0.1605
2022-10-02 06:27:48 - train: epoch 0008, iter [00770, 01251], lr: 0.000517, loss: 0.1727
2022-10-02 06:28:16 - train: epoch 0008, iter [00780, 01251], lr: 0.000517, loss: 0.1710
2022-10-02 06:28:44 - train: epoch 0008, iter [00790, 01251], lr: 0.000518, loss: 0.1784
2022-10-02 06:29:12 - train: epoch 0008, iter [00800, 01251], lr: 0.000518, loss: 0.1716
2022-10-02 06:29:40 - train: epoch 0008, iter [00810, 01251], lr: 0.000519, loss: 0.1738
2022-10-02 06:30:08 - train: epoch 0008, iter [00820, 01251], lr: 0.000519, loss: 0.1689
2022-10-02 06:30:36 - train: epoch 0008, iter [00830, 01251], lr: 0.000520, loss: 0.1636
2022-10-02 06:31:04 - train: epoch 0008, iter [00840, 01251], lr: 0.000520, loss: 0.1712
2022-10-02 06:31:33 - train: epoch 0008, iter [00850, 01251], lr: 0.000521, loss: 0.1545
2022-10-02 06:32:01 - train: epoch 0008, iter [00860, 01251], lr: 0.000521, loss: 0.1616
2022-10-02 06:32:29 - train: epoch 0008, iter [00870, 01251], lr: 0.000522, loss: 0.1770
2022-10-02 06:32:57 - train: epoch 0008, iter [00880, 01251], lr: 0.000522, loss: 0.1622
2022-10-02 06:33:25 - train: epoch 0008, iter [00890, 01251], lr: 0.000523, loss: 0.1706
2022-10-02 06:33:53 - train: epoch 0008, iter [00900, 01251], lr: 0.000523, loss: 0.1621
2022-10-02 06:34:21 - train: epoch 0008, iter [00910, 01251], lr: 0.000524, loss: 0.1686
2022-10-02 06:34:49 - train: epoch 0008, iter [00920, 01251], lr: 0.000524, loss: 0.1632
2022-10-02 06:35:18 - train: epoch 0008, iter [00930, 01251], lr: 0.000525, loss: 0.1672
2022-10-02 06:35:46 - train: epoch 0008, iter [00940, 01251], lr: 0.000525, loss: 0.1622
2022-10-02 06:36:14 - train: epoch 0008, iter [00950, 01251], lr: 0.000526, loss: 0.1628
2022-10-02 06:36:42 - train: epoch 0008, iter [00960, 01251], lr: 0.000526, loss: 0.1620
2022-10-02 06:37:10 - train: epoch 0008, iter [00970, 01251], lr: 0.000527, loss: 0.1678
2022-10-02 06:37:38 - train: epoch 0008, iter [00980, 01251], lr: 0.000527, loss: 0.1666
2022-10-02 06:38:06 - train: epoch 0008, iter [00990, 01251], lr: 0.000527, loss: 0.1595
2022-10-02 06:38:34 - train: epoch 0008, iter [01000, 01251], lr: 0.000528, loss: 0.1582
2022-10-02 06:39:02 - train: epoch 0008, iter [01010, 01251], lr: 0.000528, loss: 0.1675
2022-10-02 06:39:30 - train: epoch 0008, iter [01020, 01251], lr: 0.000529, loss: 0.1681
2022-10-02 06:39:59 - train: epoch 0008, iter [01030, 01251], lr: 0.000529, loss: 0.1687
2022-10-02 06:40:27 - train: epoch 0008, iter [01040, 01251], lr: 0.000530, loss: 0.1817
2022-10-02 06:40:55 - train: epoch 0008, iter [01050, 01251], lr: 0.000530, loss: 0.1619
2022-10-02 06:41:23 - train: epoch 0008, iter [01060, 01251], lr: 0.000531, loss: 0.1597
2022-10-02 06:41:51 - train: epoch 0008, iter [01070, 01251], lr: 0.000531, loss: 0.1660
2022-10-02 06:42:19 - train: epoch 0008, iter [01080, 01251], lr: 0.000532, loss: 0.1617
2022-10-02 06:42:47 - train: epoch 0008, iter [01090, 01251], lr: 0.000532, loss: 0.1650
2022-10-02 06:43:15 - train: epoch 0008, iter [01100, 01251], lr: 0.000533, loss: 0.1756
2022-10-02 06:43:43 - train: epoch 0008, iter [01110, 01251], lr: 0.000533, loss: 0.1656
2022-10-02 06:44:12 - train: epoch 0008, iter [01120, 01251], lr: 0.000534, loss: 0.1702
2022-10-02 06:44:40 - train: epoch 0008, iter [01130, 01251], lr: 0.000534, loss: 0.1678
2022-10-02 06:45:08 - train: epoch 0008, iter [01140, 01251], lr: 0.000535, loss: 0.1744
2022-10-02 06:45:36 - train: epoch 0008, iter [01150, 01251], lr: 0.000535, loss: 0.1689
2022-10-02 06:46:04 - train: epoch 0008, iter [01160, 01251], lr: 0.000536, loss: 0.1721
2022-10-02 06:46:32 - train: epoch 0008, iter [01170, 01251], lr: 0.000536, loss: 0.1790
2022-10-02 06:47:00 - train: epoch 0008, iter [01180, 01251], lr: 0.000537, loss: 0.1740
2022-10-02 06:47:28 - train: epoch 0008, iter [01190, 01251], lr: 0.000537, loss: 0.1587
2022-10-02 06:47:56 - train: epoch 0008, iter [01200, 01251], lr: 0.000538, loss: 0.1611
2022-10-02 06:48:24 - train: epoch 0008, iter [01210, 01251], lr: 0.000538, loss: 0.1626
2022-10-02 06:48:52 - train: epoch 0008, iter [01220, 01251], lr: 0.000539, loss: 0.1661
2022-10-02 06:49:20 - train: epoch 0008, iter [01230, 01251], lr: 0.000539, loss: 0.1565
2022-10-02 06:49:48 - train: epoch 0008, iter [01240, 01251], lr: 0.000539, loss: 0.1654
2022-10-02 06:50:16 - train: epoch 0008, iter [01250, 01251], lr: 0.000540, loss: 0.1553
2022-10-02 06:50:20 - train: epoch 008, train_loss: 0.1701
2022-10-02 06:50:22 - until epoch: 008, best_loss: 0.1701
2022-10-02 06:50:22 - epoch 009 lr: 0.000540
2022-10-02 06:50:58 - train: epoch 0009, iter [00010, 01251], lr: 0.000540, loss: 0.1579
2022-10-02 06:51:26 - train: epoch 0009, iter [00020, 01251], lr: 0.000541, loss: 0.1649
2022-10-02 06:51:54 - train: epoch 0009, iter [00030, 01251], lr: 0.000541, loss: 0.1517
2022-10-02 06:52:23 - train: epoch 0009, iter [00040, 01251], lr: 0.000542, loss: 0.1527
2022-10-02 06:52:51 - train: epoch 0009, iter [00050, 01251], lr: 0.000542, loss: 0.1570
2022-10-02 06:53:19 - train: epoch 0009, iter [00060, 01251], lr: 0.000543, loss: 0.1662
2022-10-02 06:53:47 - train: epoch 0009, iter [00070, 01251], lr: 0.000543, loss: 0.1607
2022-10-02 06:54:15 - train: epoch 0009, iter [00080, 01251], lr: 0.000544, loss: 0.1563
2022-10-02 06:54:43 - train: epoch 0009, iter [00090, 01251], lr: 0.000544, loss: 0.1537
2022-10-02 06:55:11 - train: epoch 0009, iter [00100, 01251], lr: 0.000545, loss: 0.1563
2022-10-02 06:55:39 - train: epoch 0009, iter [00110, 01251], lr: 0.000545, loss: 0.1663
2022-10-02 06:56:07 - train: epoch 0009, iter [00120, 01251], lr: 0.000546, loss: 0.1477
2022-10-02 06:56:35 - train: epoch 0009, iter [00130, 01251], lr: 0.000546, loss: 0.1699
2022-10-02 06:57:03 - train: epoch 0009, iter [00140, 01251], lr: 0.000547, loss: 0.1638
2022-10-02 06:57:31 - train: epoch 0009, iter [00150, 01251], lr: 0.000547, loss: 0.1612
2022-10-02 06:57:59 - train: epoch 0009, iter [00160, 01251], lr: 0.000548, loss: 0.1557
2022-10-02 06:58:27 - train: epoch 0009, iter [00170, 01251], lr: 0.000548, loss: 0.1579
2022-10-02 06:58:55 - train: epoch 0009, iter [00180, 01251], lr: 0.000549, loss: 0.1523
2022-10-02 06:59:23 - train: epoch 0009, iter [00190, 01251], lr: 0.000549, loss: 0.1512
2022-10-02 06:59:52 - train: epoch 0009, iter [00200, 01251], lr: 0.000550, loss: 0.1573
2022-10-02 07:00:20 - train: epoch 0009, iter [00210, 01251], lr: 0.000550, loss: 0.1694
2022-10-02 07:00:48 - train: epoch 0009, iter [00220, 01251], lr: 0.000551, loss: 0.1593
2022-10-02 07:01:16 - train: epoch 0009, iter [00230, 01251], lr: 0.000551, loss: 0.1542
2022-10-02 07:01:44 - train: epoch 0009, iter [00240, 01251], lr: 0.000552, loss: 0.1605
2022-10-02 07:02:12 - train: epoch 0009, iter [00250, 01251], lr: 0.000552, loss: 0.1608
2022-10-02 07:02:40 - train: epoch 0009, iter [00260, 01251], lr: 0.000552, loss: 0.1545
2022-10-02 07:03:08 - train: epoch 0009, iter [00270, 01251], lr: 0.000553, loss: 0.1581
2022-10-02 07:03:36 - train: epoch 0009, iter [00280, 01251], lr: 0.000553, loss: 0.1675
2022-10-02 07:04:03 - train: epoch 0009, iter [00290, 01251], lr: 0.000554, loss: 0.1596
2022-10-02 07:04:31 - train: epoch 0009, iter [00300, 01251], lr: 0.000554, loss: 0.1547
2022-10-02 07:04:59 - train: epoch 0009, iter [00310, 01251], lr: 0.000555, loss: 0.1601
2022-10-02 07:05:28 - train: epoch 0009, iter [00320, 01251], lr: 0.000555, loss: 0.1564
2022-10-02 07:05:56 - train: epoch 0009, iter [00330, 01251], lr: 0.000556, loss: 0.1560
2022-10-02 07:06:24 - train: epoch 0009, iter [00340, 01251], lr: 0.000556, loss: 0.1612
2022-10-02 07:06:52 - train: epoch 0009, iter [00350, 01251], lr: 0.000557, loss: 0.1585
2022-10-02 07:07:20 - train: epoch 0009, iter [00360, 01251], lr: 0.000557, loss: 0.1460
2022-10-02 07:07:48 - train: epoch 0009, iter [00370, 01251], lr: 0.000558, loss: 0.1647
2022-10-02 07:08:16 - train: epoch 0009, iter [00380, 01251], lr: 0.000558, loss: 0.1624
2022-10-02 07:08:44 - train: epoch 0009, iter [00390, 01251], lr: 0.000559, loss: 0.1443
2022-10-02 07:09:13 - train: epoch 0009, iter [00400, 01251], lr: 0.000559, loss: 0.1565
2022-10-02 07:09:41 - train: epoch 0009, iter [00410, 01251], lr: 0.000560, loss: 0.1523
2022-10-02 07:10:09 - train: epoch 0009, iter [00420, 01251], lr: 0.000560, loss: 0.1600
2022-10-02 07:10:37 - train: epoch 0009, iter [00430, 01251], lr: 0.000561, loss: 0.1620
2022-10-02 07:11:05 - train: epoch 0009, iter [00440, 01251], lr: 0.000561, loss: 0.1609
2022-10-02 07:11:33 - train: epoch 0009, iter [00450, 01251], lr: 0.000562, loss: 0.1648
2022-10-02 07:12:01 - train: epoch 0009, iter [00460, 01251], lr: 0.000562, loss: 0.1690
2022-10-02 07:12:29 - train: epoch 0009, iter [00470, 01251], lr: 0.000563, loss: 0.1587
2022-10-02 07:12:57 - train: epoch 0009, iter [00480, 01251], lr: 0.000563, loss: 0.1667
2022-10-02 07:13:25 - train: epoch 0009, iter [00490, 01251], lr: 0.000564, loss: 0.1574
2022-10-02 07:13:53 - train: epoch 0009, iter [00500, 01251], lr: 0.000564, loss: 0.1632
2022-10-02 07:14:21 - train: epoch 0009, iter [00510, 01251], lr: 0.000564, loss: 0.1632
2022-10-02 07:14:50 - train: epoch 0009, iter [00520, 01251], lr: 0.000565, loss: 0.1550
2022-10-02 07:15:18 - train: epoch 0009, iter [00530, 01251], lr: 0.000565, loss: 0.1582
2022-10-02 07:15:46 - train: epoch 0009, iter [00540, 01251], lr: 0.000566, loss: 0.1576
2022-10-02 07:16:14 - train: epoch 0009, iter [00550, 01251], lr: 0.000566, loss: 0.1555
2022-10-02 07:16:42 - train: epoch 0009, iter [00560, 01251], lr: 0.000567, loss: 0.1637
2022-10-02 07:17:10 - train: epoch 0009, iter [00570, 01251], lr: 0.000567, loss: 0.1503
2022-10-02 07:17:38 - train: epoch 0009, iter [00580, 01251], lr: 0.000568, loss: 0.1633
2022-10-02 07:18:07 - train: epoch 0009, iter [00590, 01251], lr: 0.000568, loss: 0.1561
2022-10-02 07:18:35 - train: epoch 0009, iter [00600, 01251], lr: 0.000569, loss: 0.1570
2022-10-02 07:19:03 - train: epoch 0009, iter [00610, 01251], lr: 0.000569, loss: 0.1590
2022-10-02 07:19:31 - train: epoch 0009, iter [00620, 01251], lr: 0.000570, loss: 0.1594
2022-10-02 07:19:59 - train: epoch 0009, iter [00630, 01251], lr: 0.000570, loss: 0.1643
2022-10-02 07:20:28 - train: epoch 0009, iter [00640, 01251], lr: 0.000571, loss: 0.1637
2022-10-02 07:20:56 - train: epoch 0009, iter [00650, 01251], lr: 0.000571, loss: 0.1484
2022-10-02 07:21:24 - train: epoch 0009, iter [00660, 01251], lr: 0.000572, loss: 0.1603
2022-10-02 07:21:52 - train: epoch 0009, iter [00670, 01251], lr: 0.000572, loss: 0.1607
2022-10-02 07:22:20 - train: epoch 0009, iter [00680, 01251], lr: 0.000573, loss: 0.1568
2022-10-02 07:22:48 - train: epoch 0009, iter [00690, 01251], lr: 0.000573, loss: 0.1485
2022-10-02 07:23:16 - train: epoch 0009, iter [00700, 01251], lr: 0.000574, loss: 0.1691
2022-10-02 07:23:44 - train: epoch 0009, iter [00710, 01251], lr: 0.000574, loss: 0.1480
2022-10-02 07:24:12 - train: epoch 0009, iter [00720, 01251], lr: 0.000575, loss: 0.1546
2022-10-02 07:24:40 - train: epoch 0009, iter [00730, 01251], lr: 0.000575, loss: 0.1420
2022-10-02 07:25:08 - train: epoch 0009, iter [00740, 01251], lr: 0.000575, loss: 0.1651
2022-10-02 07:25:36 - train: epoch 0009, iter [00750, 01251], lr: 0.000576, loss: 0.1583
2022-10-02 07:26:04 - train: epoch 0009, iter [00760, 01251], lr: 0.000576, loss: 0.1561
2022-10-02 07:26:32 - train: epoch 0009, iter [00770, 01251], lr: 0.000577, loss: 0.1586
2022-10-02 07:27:00 - train: epoch 0009, iter [00780, 01251], lr: 0.000577, loss: 0.1593
2022-10-02 07:27:28 - train: epoch 0009, iter [00790, 01251], lr: 0.000578, loss: 0.1630
2022-10-02 07:27:56 - train: epoch 0009, iter [00800, 01251], lr: 0.000578, loss: 0.1618
2022-10-02 07:28:24 - train: epoch 0009, iter [00810, 01251], lr: 0.000579, loss: 0.1485
2022-10-02 07:28:52 - train: epoch 0009, iter [00820, 01251], lr: 0.000579, loss: 0.1499
2022-10-02 07:29:20 - train: epoch 0009, iter [00830, 01251], lr: 0.000580, loss: 0.1639
2022-10-02 07:29:48 - train: epoch 0009, iter [00840, 01251], lr: 0.000580, loss: 0.1509
2022-10-02 07:30:17 - train: epoch 0009, iter [00850, 01251], lr: 0.000581, loss: 0.1530
2022-10-02 07:30:45 - train: epoch 0009, iter [00860, 01251], lr: 0.000581, loss: 0.1569
2022-10-02 07:31:13 - train: epoch 0009, iter [00870, 01251], lr: 0.000582, loss: 0.1620
2022-10-02 07:31:41 - train: epoch 0009, iter [00880, 01251], lr: 0.000582, loss: 0.1730
2022-10-02 07:32:09 - train: epoch 0009, iter [00890, 01251], lr: 0.000583, loss: 0.1639
2022-10-02 07:32:37 - train: epoch 0009, iter [00900, 01251], lr: 0.000583, loss: 0.1687
2022-10-02 07:33:05 - train: epoch 0009, iter [00910, 01251], lr: 0.000584, loss: 0.1488
2022-10-02 07:33:33 - train: epoch 0009, iter [00920, 01251], lr: 0.000584, loss: 0.1437
2022-10-02 07:34:01 - train: epoch 0009, iter [00930, 01251], lr: 0.000585, loss: 0.1573
2022-10-02 07:34:29 - train: epoch 0009, iter [00940, 01251], lr: 0.000585, loss: 0.1577
2022-10-02 07:34:58 - train: epoch 0009, iter [00950, 01251], lr: 0.000586, loss: 0.1464
2022-10-02 07:35:26 - train: epoch 0009, iter [00960, 01251], lr: 0.000586, loss: 0.1498
2022-10-02 07:35:54 - train: epoch 0009, iter [00970, 01251], lr: 0.000587, loss: 0.1557
2022-10-02 07:36:22 - train: epoch 0009, iter [00980, 01251], lr: 0.000587, loss: 0.1680
2022-10-02 07:36:50 - train: epoch 0009, iter [00990, 01251], lr: 0.000587, loss: 0.1472
2022-10-02 07:37:18 - train: epoch 0009, iter [01000, 01251], lr: 0.000588, loss: 0.1656
2022-10-02 07:37:46 - train: epoch 0009, iter [01010, 01251], lr: 0.000588, loss: 0.1622
2022-10-02 07:38:15 - train: epoch 0009, iter [01020, 01251], lr: 0.000589, loss: 0.1542
2022-10-02 07:38:43 - train: epoch 0009, iter [01030, 01251], lr: 0.000589, loss: 0.1486
2022-10-02 07:39:11 - train: epoch 0009, iter [01040, 01251], lr: 0.000590, loss: 0.1586
2022-10-02 07:39:39 - train: epoch 0009, iter [01050, 01251], lr: 0.000590, loss: 0.1603
2022-10-02 07:40:07 - train: epoch 0009, iter [01060, 01251], lr: 0.000591, loss: 0.1492
2022-10-02 07:40:35 - train: epoch 0009, iter [01070, 01251], lr: 0.000591, loss: 0.1516
2022-10-02 07:41:03 - train: epoch 0009, iter [01080, 01251], lr: 0.000592, loss: 0.1546
2022-10-02 07:41:31 - train: epoch 0009, iter [01090, 01251], lr: 0.000592, loss: 0.1564
2022-10-02 07:41:59 - train: epoch 0009, iter [01100, 01251], lr: 0.000593, loss: 0.1533
2022-10-02 07:42:27 - train: epoch 0009, iter [01110, 01251], lr: 0.000593, loss: 0.1464
2022-10-02 07:42:56 - train: epoch 0009, iter [01120, 01251], lr: 0.000594, loss: 0.1671
2022-10-02 07:43:24 - train: epoch 0009, iter [01130, 01251], lr: 0.000594, loss: 0.1542
2022-10-02 07:43:52 - train: epoch 0009, iter [01140, 01251], lr: 0.000595, loss: 0.1472
2022-10-02 07:44:20 - train: epoch 0009, iter [01150, 01251], lr: 0.000595, loss: 0.1469
2022-10-02 07:44:48 - train: epoch 0009, iter [01160, 01251], lr: 0.000596, loss: 0.1570
2022-10-02 07:45:16 - train: epoch 0009, iter [01170, 01251], lr: 0.000596, loss: 0.1694
2022-10-02 07:45:44 - train: epoch 0009, iter [01180, 01251], lr: 0.000597, loss: 0.1537
2022-10-02 07:46:12 - train: epoch 0009, iter [01190, 01251], lr: 0.000597, loss: 0.1547
2022-10-02 07:46:40 - train: epoch 0009, iter [01200, 01251], lr: 0.000598, loss: 0.1586
2022-10-02 07:47:08 - train: epoch 0009, iter [01210, 01251], lr: 0.000598, loss: 0.1569
2022-10-02 07:47:36 - train: epoch 0009, iter [01220, 01251], lr: 0.000599, loss: 0.1585
2022-10-02 07:48:04 - train: epoch 0009, iter [01230, 01251], lr: 0.000599, loss: 0.1615
2022-10-02 07:48:31 - train: epoch 0009, iter [01240, 01251], lr: 0.000599, loss: 0.1574
2022-10-02 07:48:59 - train: epoch 0009, iter [01250, 01251], lr: 0.000600, loss: 0.1483
2022-10-02 07:49:04 - train: epoch 009, train_loss: 0.1578
2022-10-02 07:49:05 - until epoch: 009, best_loss: 0.1578
2022-10-02 07:49:05 - epoch 010 lr: 0.000600
2022-10-02 07:49:40 - train: epoch 0010, iter [00010, 01251], lr: 0.000600, loss: 0.1567
2022-10-02 07:50:08 - train: epoch 0010, iter [00020, 01251], lr: 0.000601, loss: 0.1571
2022-10-02 07:50:36 - train: epoch 0010, iter [00030, 01251], lr: 0.000601, loss: 0.1567
2022-10-02 07:51:04 - train: epoch 0010, iter [00040, 01251], lr: 0.000602, loss: 0.1505
2022-10-02 07:51:32 - train: epoch 0010, iter [00050, 01251], lr: 0.000602, loss: 0.1462
2022-10-02 07:52:01 - train: epoch 0010, iter [00060, 01251], lr: 0.000603, loss: 0.1543
2022-10-02 07:52:29 - train: epoch 0010, iter [00070, 01251], lr: 0.000603, loss: 0.1612
2022-10-02 07:52:57 - train: epoch 0010, iter [00080, 01251], lr: 0.000604, loss: 0.1562
2022-10-02 07:53:25 - train: epoch 0010, iter [00090, 01251], lr: 0.000604, loss: 0.1634
2022-10-02 07:53:53 - train: epoch 0010, iter [00100, 01251], lr: 0.000605, loss: 0.1543
2022-10-02 07:54:21 - train: epoch 0010, iter [00110, 01251], lr: 0.000605, loss: 0.1537
2022-10-02 07:54:50 - train: epoch 0010, iter [00120, 01251], lr: 0.000606, loss: 0.1404
2022-10-02 07:55:18 - train: epoch 0010, iter [00130, 01251], lr: 0.000606, loss: 0.1525
2022-10-02 07:55:46 - train: epoch 0010, iter [00140, 01251], lr: 0.000607, loss: 0.1455
2022-10-02 07:56:15 - train: epoch 0010, iter [00150, 01251], lr: 0.000607, loss: 0.1584
2022-10-02 07:56:43 - train: epoch 0010, iter [00160, 01251], lr: 0.000608, loss: 0.1523
2022-10-02 07:57:11 - train: epoch 0010, iter [00170, 01251], lr: 0.000608, loss: 0.1532
2022-10-02 07:57:39 - train: epoch 0010, iter [00180, 01251], lr: 0.000609, loss: 0.1491
2022-10-02 07:58:07 - train: epoch 0010, iter [00190, 01251], lr: 0.000609, loss: 0.1481
2022-10-02 07:58:35 - train: epoch 0010, iter [00200, 01251], lr: 0.000610, loss: 0.1514
2022-10-02 07:59:03 - train: epoch 0010, iter [00210, 01251], lr: 0.000610, loss: 0.1518
2022-10-02 07:59:31 - train: epoch 0010, iter [00220, 01251], lr: 0.000611, loss: 0.1540
2022-10-02 07:59:59 - train: epoch 0010, iter [00230, 01251], lr: 0.000611, loss: 0.1548
2022-10-02 08:00:27 - train: epoch 0010, iter [00240, 01251], lr: 0.000612, loss: 0.1422
2022-10-02 08:00:55 - train: epoch 0010, iter [00250, 01251], lr: 0.000612, loss: 0.1502
2022-10-02 08:01:23 - train: epoch 0010, iter [00260, 01251], lr: 0.000612, loss: 0.1450
2022-10-02 08:01:51 - train: epoch 0010, iter [00270, 01251], lr: 0.000613, loss: 0.1557
2022-10-02 08:02:20 - train: epoch 0010, iter [00280, 01251], lr: 0.000613, loss: 0.1543
2022-10-02 08:02:48 - train: epoch 0010, iter [00290, 01251], lr: 0.000614, loss: 0.1489
2022-10-02 08:03:16 - train: epoch 0010, iter [00300, 01251], lr: 0.000614, loss: 0.1488
2022-10-02 08:03:44 - train: epoch 0010, iter [00310, 01251], lr: 0.000615, loss: 0.1548
2022-10-02 08:04:12 - train: epoch 0010, iter [00320, 01251], lr: 0.000615, loss: 0.1533
2022-10-02 08:04:40 - train: epoch 0010, iter [00330, 01251], lr: 0.000616, loss: 0.1664
2022-10-02 08:05:08 - train: epoch 0010, iter [00340, 01251], lr: 0.000616, loss: 0.1504
2022-10-02 08:05:36 - train: epoch 0010, iter [00350, 01251], lr: 0.000617, loss: 0.1456
2022-10-02 08:06:04 - train: epoch 0010, iter [00360, 01251], lr: 0.000617, loss: 0.1582
2022-10-02 08:06:32 - train: epoch 0010, iter [00370, 01251], lr: 0.000618, loss: 0.1507
2022-10-02 08:07:00 - train: epoch 0010, iter [00380, 01251], lr: 0.000618, loss: 0.1521
2022-10-02 08:07:28 - train: epoch 0010, iter [00390, 01251], lr: 0.000619, loss: 0.1465
2022-10-02 08:07:56 - train: epoch 0010, iter [00400, 01251], lr: 0.000619, loss: 0.1530
2022-10-02 08:08:25 - train: epoch 0010, iter [00410, 01251], lr: 0.000620, loss: 0.1542
2022-10-02 08:08:53 - train: epoch 0010, iter [00420, 01251], lr: 0.000620, loss: 0.1460
2022-10-02 08:09:21 - train: epoch 0010, iter [00430, 01251], lr: 0.000621, loss: 0.1357
2022-10-02 08:09:49 - train: epoch 0010, iter [00440, 01251], lr: 0.000621, loss: 0.1553
2022-10-02 08:10:17 - train: epoch 0010, iter [00450, 01251], lr: 0.000622, loss: 0.1443
2022-10-02 08:10:45 - train: epoch 0010, iter [00460, 01251], lr: 0.000622, loss: 0.1501
2022-10-02 08:11:13 - train: epoch 0010, iter [00470, 01251], lr: 0.000623, loss: 0.1522
2022-10-02 08:11:41 - train: epoch 0010, iter [00480, 01251], lr: 0.000623, loss: 0.1523
2022-10-02 08:12:08 - train: epoch 0010, iter [00490, 01251], lr: 0.000624, loss: 0.1529
2022-10-02 08:12:36 - train: epoch 0010, iter [00500, 01251], lr: 0.000624, loss: 0.1440
2022-10-02 08:13:04 - train: epoch 0010, iter [00510, 01251], lr: 0.000624, loss: 0.1530
2022-10-02 08:13:32 - train: epoch 0010, iter [00520, 01251], lr: 0.000625, loss: 0.1445
2022-10-02 08:14:00 - train: epoch 0010, iter [00530, 01251], lr: 0.000625, loss: 0.1529
2022-10-02 08:14:28 - train: epoch 0010, iter [00540, 01251], lr: 0.000626, loss: 0.1549
2022-10-02 08:14:55 - train: epoch 0010, iter [00550, 01251], lr: 0.000626, loss: 0.1532
2022-10-02 08:15:23 - train: epoch 0010, iter [00560, 01251], lr: 0.000627, loss: 0.1411
2022-10-02 08:15:52 - train: epoch 0010, iter [00570, 01251], lr: 0.000627, loss: 0.1508
2022-10-02 08:16:20 - train: epoch 0010, iter [00580, 01251], lr: 0.000628, loss: 0.1364
2022-10-02 08:16:48 - train: epoch 0010, iter [00590, 01251], lr: 0.000628, loss: 0.1450
2022-10-02 08:17:16 - train: epoch 0010, iter [00600, 01251], lr: 0.000629, loss: 0.1502
2022-10-02 08:17:44 - train: epoch 0010, iter [00610, 01251], lr: 0.000629, loss: 0.1583
2022-10-02 08:18:12 - train: epoch 0010, iter [00620, 01251], lr: 0.000630, loss: 0.1572
2022-10-02 08:18:40 - train: epoch 0010, iter [00630, 01251], lr: 0.000630, loss: 0.1414
2022-10-02 08:19:07 - train: epoch 0010, iter [00640, 01251], lr: 0.000631, loss: 0.1522
2022-10-02 08:19:35 - train: epoch 0010, iter [00650, 01251], lr: 0.000631, loss: 0.1393
2022-10-02 08:20:03 - train: epoch 0010, iter [00660, 01251], lr: 0.000632, loss: 0.1434
2022-10-02 08:20:31 - train: epoch 0010, iter [00670, 01251], lr: 0.000632, loss: 0.1512
2022-10-02 08:20:59 - train: epoch 0010, iter [00680, 01251], lr: 0.000633, loss: 0.1479
2022-10-02 08:21:27 - train: epoch 0010, iter [00690, 01251], lr: 0.000633, loss: 0.1413
2022-10-02 08:21:55 - train: epoch 0010, iter [00700, 01251], lr: 0.000634, loss: 0.1415
2022-10-02 08:22:24 - train: epoch 0010, iter [00710, 01251], lr: 0.000634, loss: 0.1480
2022-10-02 08:22:52 - train: epoch 0010, iter [00720, 01251], lr: 0.000635, loss: 0.1429
2022-10-02 08:23:20 - train: epoch 0010, iter [00730, 01251], lr: 0.000635, loss: 0.1492
2022-10-02 08:23:48 - train: epoch 0010, iter [00740, 01251], lr: 0.000635, loss: 0.1544
2022-10-02 08:24:16 - train: epoch 0010, iter [00750, 01251], lr: 0.000636, loss: 0.1475
2022-10-02 08:24:44 - train: epoch 0010, iter [00760, 01251], lr: 0.000636, loss: 0.1358
2022-10-02 08:25:13 - train: epoch 0010, iter [00770, 01251], lr: 0.000637, loss: 0.1501
2022-10-02 08:25:41 - train: epoch 0010, iter [00780, 01251], lr: 0.000637, loss: 0.1596
2022-10-02 08:26:09 - train: epoch 0010, iter [00790, 01251], lr: 0.000638, loss: 0.1465
2022-10-02 08:26:37 - train: epoch 0010, iter [00800, 01251], lr: 0.000638, loss: 0.1420
2022-10-02 08:27:05 - train: epoch 0010, iter [00810, 01251], lr: 0.000639, loss: 0.1437
2022-10-02 08:27:34 - train: epoch 0010, iter [00820, 01251], lr: 0.000639, loss: 0.1547
2022-10-02 08:28:02 - train: epoch 0010, iter [00830, 01251], lr: 0.000640, loss: 0.1430
2022-10-02 08:28:30 - train: epoch 0010, iter [00840, 01251], lr: 0.000640, loss: 0.1432
2022-10-02 08:28:57 - train: epoch 0010, iter [00850, 01251], lr: 0.000641, loss: 0.1453
2022-10-02 08:29:25 - train: epoch 0010, iter [00860, 01251], lr: 0.000641, loss: 0.1450
2022-10-02 08:29:53 - train: epoch 0010, iter [00870, 01251], lr: 0.000642, loss: 0.1468
2022-10-02 08:30:21 - train: epoch 0010, iter [00880, 01251], lr: 0.000642, loss: 0.1524
2022-10-02 08:30:49 - train: epoch 0010, iter [00890, 01251], lr: 0.000643, loss: 0.1452
2022-10-02 08:31:17 - train: epoch 0010, iter [00900, 01251], lr: 0.000643, loss: 0.1454
2022-10-02 08:31:45 - train: epoch 0010, iter [00910, 01251], lr: 0.000644, loss: 0.1405
2022-10-02 08:32:13 - train: epoch 0010, iter [00920, 01251], lr: 0.000644, loss: 0.1504
2022-10-02 08:32:41 - train: epoch 0010, iter [00930, 01251], lr: 0.000645, loss: 0.1460
2022-10-02 08:33:09 - train: epoch 0010, iter [00940, 01251], lr: 0.000645, loss: 0.1491
2022-10-02 08:33:37 - train: epoch 0010, iter [00950, 01251], lr: 0.000646, loss: 0.1444
2022-10-02 08:34:05 - train: epoch 0010, iter [00960, 01251], lr: 0.000646, loss: 0.1411
2022-10-02 08:34:33 - train: epoch 0010, iter [00970, 01251], lr: 0.000647, loss: 0.1411
2022-10-02 08:35:01 - train: epoch 0010, iter [00980, 01251], lr: 0.000647, loss: 0.1584
2022-10-02 08:35:29 - train: epoch 0010, iter [00990, 01251], lr: 0.000647, loss: 0.1467
2022-10-02 08:35:58 - train: epoch 0010, iter [01000, 01251], lr: 0.000648, loss: 0.1499
2022-10-02 08:36:26 - train: epoch 0010, iter [01010, 01251], lr: 0.000648, loss: 0.1355
2022-10-02 08:36:54 - train: epoch 0010, iter [01020, 01251], lr: 0.000649, loss: 0.1394
2022-10-02 08:37:21 - train: epoch 0010, iter [01030, 01251], lr: 0.000649, loss: 0.1524
2022-10-02 08:37:50 - train: epoch 0010, iter [01040, 01251], lr: 0.000650, loss: 0.1397
2022-10-02 08:38:18 - train: epoch 0010, iter [01050, 01251], lr: 0.000650, loss: 0.1610
2022-10-02 08:38:46 - train: epoch 0010, iter [01060, 01251], lr: 0.000651, loss: 0.1365
2022-10-02 08:39:14 - train: epoch 0010, iter [01070, 01251], lr: 0.000651, loss: 0.1669
2022-10-02 08:39:42 - train: epoch 0010, iter [01080, 01251], lr: 0.000652, loss: 0.1617
2022-10-02 08:40:10 - train: epoch 0010, iter [01090, 01251], lr: 0.000652, loss: 0.1483
2022-10-02 08:40:38 - train: epoch 0010, iter [01100, 01251], lr: 0.000653, loss: 0.1486
2022-10-02 08:41:06 - train: epoch 0010, iter [01110, 01251], lr: 0.000653, loss: 0.1526
2022-10-02 08:41:34 - train: epoch 0010, iter [01120, 01251], lr: 0.000654, loss: 0.1393
2022-10-02 08:42:03 - train: epoch 0010, iter [01130, 01251], lr: 0.000654, loss: 0.1435
2022-10-02 08:42:31 - train: epoch 0010, iter [01140, 01251], lr: 0.000655, loss: 0.1475
2022-10-02 08:42:59 - train: epoch 0010, iter [01150, 01251], lr: 0.000655, loss: 0.1456
2022-10-02 08:43:27 - train: epoch 0010, iter [01160, 01251], lr: 0.000656, loss: 0.1454
2022-10-02 08:43:55 - train: epoch 0010, iter [01170, 01251], lr: 0.000656, loss: 0.1447
2022-10-02 08:44:23 - train: epoch 0010, iter [01180, 01251], lr: 0.000657, loss: 0.1401
2022-10-02 08:44:51 - train: epoch 0010, iter [01190, 01251], lr: 0.000657, loss: 0.1361
2022-10-02 08:45:19 - train: epoch 0010, iter [01200, 01251], lr: 0.000658, loss: 0.1445
2022-10-02 08:45:47 - train: epoch 0010, iter [01210, 01251], lr: 0.000658, loss: 0.1512
2022-10-02 08:46:16 - train: epoch 0010, iter [01220, 01251], lr: 0.000659, loss: 0.1471
2022-10-02 08:46:44 - train: epoch 0010, iter [01230, 01251], lr: 0.000659, loss: 0.1292
2022-10-02 08:47:12 - train: epoch 0010, iter [01240, 01251], lr: 0.000659, loss: 0.1579
2022-10-02 08:47:39 - train: epoch 0010, iter [01250, 01251], lr: 0.000660, loss: 0.1311
2022-10-02 08:47:44 - train: epoch 010, train_loss: 0.1485
2022-10-02 08:47:45 - until epoch: 010, best_loss: 0.1485
2022-10-02 08:47:45 - epoch 011 lr: 0.000600
2022-10-02 08:48:20 - train: epoch 0011, iter [00010, 01251], lr: 0.000600, loss: 0.1590
2022-10-02 08:48:49 - train: epoch 0011, iter [00020, 01251], lr: 0.000600, loss: 0.1397
2022-10-02 08:49:17 - train: epoch 0011, iter [00030, 01251], lr: 0.000600, loss: 0.1491
2022-10-02 08:49:45 - train: epoch 0011, iter [00040, 01251], lr: 0.000600, loss: 0.1428
2022-10-02 08:50:13 - train: epoch 0011, iter [00050, 01251], lr: 0.000600, loss: 0.1367
2022-10-02 08:50:41 - train: epoch 0011, iter [00060, 01251], lr: 0.000600, loss: 0.1432
2022-10-02 08:51:09 - train: epoch 0011, iter [00070, 01251], lr: 0.000600, loss: 0.1486
2022-10-02 08:51:37 - train: epoch 0011, iter [00080, 01251], lr: 0.000600, loss: 0.1449
2022-10-02 08:52:05 - train: epoch 0011, iter [00090, 01251], lr: 0.000600, loss: 0.1525
2022-10-02 08:52:33 - train: epoch 0011, iter [00100, 01251], lr: 0.000600, loss: 0.1305
2022-10-02 08:53:02 - train: epoch 0011, iter [00110, 01251], lr: 0.000600, loss: 0.1338
2022-10-02 08:53:30 - train: epoch 0011, iter [00120, 01251], lr: 0.000600, loss: 0.1325
2022-10-02 08:53:58 - train: epoch 0011, iter [00130, 01251], lr: 0.000600, loss: 0.1462
2022-10-02 08:54:26 - train: epoch 0011, iter [00140, 01251], lr: 0.000600, loss: 0.1374
2022-10-02 08:54:55 - train: epoch 0011, iter [00150, 01251], lr: 0.000600, loss: 0.1475
2022-10-02 08:55:23 - train: epoch 0011, iter [00160, 01251], lr: 0.000600, loss: 0.1279
2022-10-02 08:55:51 - train: epoch 0011, iter [00170, 01251], lr: 0.000600, loss: 0.1422
2022-10-02 08:56:19 - train: epoch 0011, iter [00180, 01251], lr: 0.000600, loss: 0.1357
2022-10-02 08:56:47 - train: epoch 0011, iter [00190, 01251], lr: 0.000600, loss: 0.1289
2022-10-02 08:57:16 - train: epoch 0011, iter [00200, 01251], lr: 0.000600, loss: 0.1427
2022-10-02 08:57:44 - train: epoch 0011, iter [00210, 01251], lr: 0.000600, loss: 0.1428
2022-10-02 08:58:12 - train: epoch 0011, iter [00220, 01251], lr: 0.000600, loss: 0.1376
2022-10-02 08:58:40 - train: epoch 0011, iter [00230, 01251], lr: 0.000600, loss: 0.1401
2022-10-02 08:59:08 - train: epoch 0011, iter [00240, 01251], lr: 0.000600, loss: 0.1359
2022-10-02 08:59:37 - train: epoch 0011, iter [00250, 01251], lr: 0.000600, loss: 0.1512
2022-10-02 09:00:05 - train: epoch 0011, iter [00260, 01251], lr: 0.000600, loss: 0.1450
2022-10-02 09:00:33 - train: epoch 0011, iter [00270, 01251], lr: 0.000600, loss: 0.1342
2022-10-02 09:01:01 - train: epoch 0011, iter [00280, 01251], lr: 0.000600, loss: 0.1381
2022-10-02 09:01:29 - train: epoch 0011, iter [00290, 01251], lr: 0.000600, loss: 0.1330
2022-10-02 09:01:57 - train: epoch 0011, iter [00300, 01251], lr: 0.000600, loss: 0.1341
2022-10-02 09:02:26 - train: epoch 0011, iter [00310, 01251], lr: 0.000600, loss: 0.1333
2022-10-02 09:02:54 - train: epoch 0011, iter [00320, 01251], lr: 0.000600, loss: 0.1290
2022-10-02 09:03:22 - train: epoch 0011, iter [00330, 01251], lr: 0.000600, loss: 0.1405
2022-10-02 09:03:50 - train: epoch 0011, iter [00340, 01251], lr: 0.000600, loss: 0.1376
2022-10-02 09:04:18 - train: epoch 0011, iter [00350, 01251], lr: 0.000600, loss: 0.1458
2022-10-02 09:04:46 - train: epoch 0011, iter [00360, 01251], lr: 0.000600, loss: 0.1300
2022-10-02 09:05:15 - train: epoch 0011, iter [00370, 01251], lr: 0.000600, loss: 0.1554
2022-10-02 09:05:43 - train: epoch 0011, iter [00380, 01251], lr: 0.000600, loss: 0.1427
2022-10-02 09:06:11 - train: epoch 0011, iter [00390, 01251], lr: 0.000600, loss: 0.1418
2022-10-02 09:06:39 - train: epoch 0011, iter [00400, 01251], lr: 0.000600, loss: 0.1399
2022-10-02 09:07:07 - train: epoch 0011, iter [00410, 01251], lr: 0.000600, loss: 0.1353
2022-10-02 09:07:36 - train: epoch 0011, iter [00420, 01251], lr: 0.000600, loss: 0.1444
2022-10-02 09:08:04 - train: epoch 0011, iter [00430, 01251], lr: 0.000600, loss: 0.1352
2022-10-02 09:08:32 - train: epoch 0011, iter [00440, 01251], lr: 0.000600, loss: 0.1315
2022-10-02 09:09:01 - train: epoch 0011, iter [00450, 01251], lr: 0.000600, loss: 0.1476
2022-10-02 09:09:29 - train: epoch 0011, iter [00460, 01251], lr: 0.000600, loss: 0.1357
2022-10-02 09:09:57 - train: epoch 0011, iter [00470, 01251], lr: 0.000600, loss: 0.1277
2022-10-02 09:10:25 - train: epoch 0011, iter [00480, 01251], lr: 0.000600, loss: 0.1433
2022-10-02 09:10:54 - train: epoch 0011, iter [00490, 01251], lr: 0.000600, loss: 0.1375
2022-10-02 09:11:22 - train: epoch 0011, iter [00500, 01251], lr: 0.000600, loss: 0.1365
2022-10-02 09:11:50 - train: epoch 0011, iter [00510, 01251], lr: 0.000600, loss: 0.1522
2022-10-02 09:12:19 - train: epoch 0011, iter [00520, 01251], lr: 0.000600, loss: 0.1330
2022-10-02 09:12:47 - train: epoch 0011, iter [00530, 01251], lr: 0.000600, loss: 0.1335
2022-10-02 09:13:15 - train: epoch 0011, iter [00540, 01251], lr: 0.000600, loss: 0.1337
2022-10-02 09:13:43 - train: epoch 0011, iter [00550, 01251], lr: 0.000600, loss: 0.1382
2022-10-02 09:14:12 - train: epoch 0011, iter [00560, 01251], lr: 0.000600, loss: 0.1477
2022-10-02 09:14:40 - train: epoch 0011, iter [00570, 01251], lr: 0.000600, loss: 0.1473
2022-10-02 09:15:08 - train: epoch 0011, iter [00580, 01251], lr: 0.000600, loss: 0.1310
2022-10-02 09:15:36 - train: epoch 0011, iter [00590, 01251], lr: 0.000600, loss: 0.1429
2022-10-02 09:16:04 - train: epoch 0011, iter [00600, 01251], lr: 0.000600, loss: 0.1330
2022-10-02 09:16:32 - train: epoch 0011, iter [00610, 01251], lr: 0.000600, loss: 0.1384
2022-10-02 09:17:00 - train: epoch 0011, iter [00620, 01251], lr: 0.000600, loss: 0.1451
2022-10-02 09:17:28 - train: epoch 0011, iter [00630, 01251], lr: 0.000600, loss: 0.1366
2022-10-02 09:17:56 - train: epoch 0011, iter [00640, 01251], lr: 0.000600, loss: 0.1392
2022-10-02 09:18:24 - train: epoch 0011, iter [00650, 01251], lr: 0.000600, loss: 0.1490
2022-10-02 09:18:52 - train: epoch 0011, iter [00660, 01251], lr: 0.000600, loss: 0.1415
2022-10-02 09:19:20 - train: epoch 0011, iter [00670, 01251], lr: 0.000600, loss: 0.1397
2022-10-02 09:19:48 - train: epoch 0011, iter [00680, 01251], lr: 0.000600, loss: 0.1365
2022-10-02 09:20:16 - train: epoch 0011, iter [00690, 01251], lr: 0.000600, loss: 0.1382
2022-10-02 09:20:44 - train: epoch 0011, iter [00700, 01251], lr: 0.000600, loss: 0.1335
2022-10-02 09:21:12 - train: epoch 0011, iter [00710, 01251], lr: 0.000600, loss: 0.1329
2022-10-02 09:21:40 - train: epoch 0011, iter [00720, 01251], lr: 0.000600, loss: 0.1390
2022-10-02 09:22:08 - train: epoch 0011, iter [00730, 01251], lr: 0.000600, loss: 0.1341
2022-10-02 09:22:36 - train: epoch 0011, iter [00740, 01251], lr: 0.000600, loss: 0.1291
2022-10-02 09:23:04 - train: epoch 0011, iter [00750, 01251], lr: 0.000600, loss: 0.1449
2022-10-02 09:23:33 - train: epoch 0011, iter [00760, 01251], lr: 0.000600, loss: 0.1276
2022-10-02 09:24:01 - train: epoch 0011, iter [00770, 01251], lr: 0.000600, loss: 0.1459
2022-10-02 09:24:29 - train: epoch 0011, iter [00780, 01251], lr: 0.000600, loss: 0.1352
2022-10-02 09:24:57 - train: epoch 0011, iter [00790, 01251], lr: 0.000600, loss: 0.1377
2022-10-02 09:25:25 - train: epoch 0011, iter [00800, 01251], lr: 0.000600, loss: 0.1462
2022-10-02 09:25:53 - train: epoch 0011, iter [00810, 01251], lr: 0.000600, loss: 0.1450
2022-10-02 09:26:21 - train: epoch 0011, iter [00820, 01251], lr: 0.000600, loss: 0.1415
2022-10-02 09:26:49 - train: epoch 0011, iter [00830, 01251], lr: 0.000600, loss: 0.1391
2022-10-02 09:27:17 - train: epoch 0011, iter [00840, 01251], lr: 0.000600, loss: 0.1369
2022-10-02 09:27:45 - train: epoch 0011, iter [00850, 01251], lr: 0.000600, loss: 0.1413
2022-10-02 09:28:14 - train: epoch 0011, iter [00860, 01251], lr: 0.000600, loss: 0.1361
2022-10-02 09:28:42 - train: epoch 0011, iter [00870, 01251], lr: 0.000600, loss: 0.1373
2022-10-02 09:29:10 - train: epoch 0011, iter [00880, 01251], lr: 0.000600, loss: 0.1373
2022-10-02 09:29:38 - train: epoch 0011, iter [00890, 01251], lr: 0.000600, loss: 0.1390
2022-10-02 09:30:07 - train: epoch 0011, iter [00900, 01251], lr: 0.000600, loss: 0.1306
2022-10-02 09:30:35 - train: epoch 0011, iter [00910, 01251], lr: 0.000600, loss: 0.1369
2022-10-02 09:31:03 - train: epoch 0011, iter [00920, 01251], lr: 0.000600, loss: 0.1483
2022-10-02 09:31:31 - train: epoch 0011, iter [00930, 01251], lr: 0.000600, loss: 0.1333
2022-10-02 09:31:59 - train: epoch 0011, iter [00940, 01251], lr: 0.000600, loss: 0.1489
2022-10-02 09:32:28 - train: epoch 0011, iter [00950, 01251], lr: 0.000600, loss: 0.1296
2022-10-02 09:32:56 - train: epoch 0011, iter [00960, 01251], lr: 0.000600, loss: 0.1375
2022-10-02 09:33:24 - train: epoch 0011, iter [00970, 01251], lr: 0.000600, loss: 0.1308
2022-10-02 09:33:52 - train: epoch 0011, iter [00980, 01251], lr: 0.000600, loss: 0.1371
2022-10-02 09:34:20 - train: epoch 0011, iter [00990, 01251], lr: 0.000600, loss: 0.1398
2022-10-02 09:34:48 - train: epoch 0011, iter [01000, 01251], lr: 0.000600, loss: 0.1367
2022-10-02 09:35:16 - train: epoch 0011, iter [01010, 01251], lr: 0.000600, loss: 0.1284
2022-10-02 09:35:44 - train: epoch 0011, iter [01020, 01251], lr: 0.000600, loss: 0.1387
2022-10-02 09:36:13 - train: epoch 0011, iter [01030, 01251], lr: 0.000600, loss: 0.1380
2022-10-02 09:36:41 - train: epoch 0011, iter [01040, 01251], lr: 0.000600, loss: 0.1340
2022-10-02 09:37:09 - train: epoch 0011, iter [01050, 01251], lr: 0.000600, loss: 0.1342
2022-10-02 09:37:37 - train: epoch 0011, iter [01060, 01251], lr: 0.000600, loss: 0.1347
2022-10-02 09:38:05 - train: epoch 0011, iter [01070, 01251], lr: 0.000600, loss: 0.1371
2022-10-02 09:38:33 - train: epoch 0011, iter [01080, 01251], lr: 0.000600, loss: 0.1328
2022-10-02 09:39:02 - train: epoch 0011, iter [01090, 01251], lr: 0.000600, loss: 0.1298
2022-10-02 09:39:30 - train: epoch 0011, iter [01100, 01251], lr: 0.000600, loss: 0.1372
2022-10-02 09:39:58 - train: epoch 0011, iter [01110, 01251], lr: 0.000600, loss: 0.1227
2022-10-02 09:40:26 - train: epoch 0011, iter [01120, 01251], lr: 0.000600, loss: 0.1378
2022-10-02 09:40:55 - train: epoch 0011, iter [01130, 01251], lr: 0.000600, loss: 0.1305
2022-10-02 09:41:23 - train: epoch 0011, iter [01140, 01251], lr: 0.000600, loss: 0.1337
2022-10-02 09:41:51 - train: epoch 0011, iter [01150, 01251], lr: 0.000600, loss: 0.1331
2022-10-02 09:42:19 - train: epoch 0011, iter [01160, 01251], lr: 0.000600, loss: 0.1344
2022-10-02 09:42:47 - train: epoch 0011, iter [01170, 01251], lr: 0.000600, loss: 0.1280
2022-10-02 09:43:15 - train: epoch 0011, iter [01180, 01251], lr: 0.000600, loss: 0.1371
2022-10-02 09:43:43 - train: epoch 0011, iter [01190, 01251], lr: 0.000600, loss: 0.1384
2022-10-02 09:44:11 - train: epoch 0011, iter [01200, 01251], lr: 0.000600, loss: 0.1335
2022-10-02 09:44:40 - train: epoch 0011, iter [01210, 01251], lr: 0.000600, loss: 0.1441
2022-10-02 09:45:08 - train: epoch 0011, iter [01220, 01251], lr: 0.000600, loss: 0.1328
2022-10-02 09:45:36 - train: epoch 0011, iter [01230, 01251], lr: 0.000600, loss: 0.1506
2022-10-02 09:46:04 - train: epoch 0011, iter [01240, 01251], lr: 0.000600, loss: 0.1369
2022-10-02 09:46:32 - train: epoch 0011, iter [01250, 01251], lr: 0.000600, loss: 0.1374
2022-10-02 09:46:36 - train: epoch 011, train_loss: 0.1386
2022-10-02 09:46:38 - until epoch: 011, best_loss: 0.1386
2022-10-02 09:46:38 - epoch 012 lr: 0.000600
2022-10-02 09:47:13 - train: epoch 0012, iter [00010, 01251], lr: 0.000600, loss: 0.1318
2022-10-02 09:47:41 - train: epoch 0012, iter [00020, 01251], lr: 0.000600, loss: 0.1335
2022-10-02 09:48:09 - train: epoch 0012, iter [00030, 01251], lr: 0.000600, loss: 0.1439
2022-10-02 09:48:37 - train: epoch 0012, iter [00040, 01251], lr: 0.000600, loss: 0.1298
2022-10-02 09:49:05 - train: epoch 0012, iter [00050, 01251], lr: 0.000600, loss: 0.1384
2022-10-02 09:49:33 - train: epoch 0012, iter [00060, 01251], lr: 0.000600, loss: 0.1393
2022-10-02 09:50:02 - train: epoch 0012, iter [00070, 01251], lr: 0.000600, loss: 0.1322
2022-10-02 09:50:30 - train: epoch 0012, iter [00080, 01251], lr: 0.000600, loss: 0.1295
2022-10-02 09:50:58 - train: epoch 0012, iter [00090, 01251], lr: 0.000600, loss: 0.1314
2022-10-02 09:51:27 - train: epoch 0012, iter [00100, 01251], lr: 0.000600, loss: 0.1272
2022-10-02 09:51:55 - train: epoch 0012, iter [00110, 01251], lr: 0.000600, loss: 0.1399
2022-10-02 09:52:23 - train: epoch 0012, iter [00120, 01251], lr: 0.000600, loss: 0.1521
2022-10-02 09:52:51 - train: epoch 0012, iter [00130, 01251], lr: 0.000600, loss: 0.1280
2022-10-02 09:53:20 - train: epoch 0012, iter [00140, 01251], lr: 0.000600, loss: 0.1239
2022-10-02 09:53:48 - train: epoch 0012, iter [00150, 01251], lr: 0.000600, loss: 0.1334
2022-10-02 09:54:16 - train: epoch 0012, iter [00160, 01251], lr: 0.000600, loss: 0.1440
2022-10-02 09:54:44 - train: epoch 0012, iter [00170, 01251], lr: 0.000600, loss: 0.1266
2022-10-02 09:55:12 - train: epoch 0012, iter [00180, 01251], lr: 0.000600, loss: 0.1456
2022-10-02 09:55:40 - train: epoch 0012, iter [00190, 01251], lr: 0.000600, loss: 0.1370
2022-10-02 09:56:08 - train: epoch 0012, iter [00200, 01251], lr: 0.000600, loss: 0.1305
2022-10-02 09:56:37 - train: epoch 0012, iter [00210, 01251], lr: 0.000600, loss: 0.1422
2022-10-02 09:57:05 - train: epoch 0012, iter [00220, 01251], lr: 0.000600, loss: 0.1394
2022-10-02 09:57:33 - train: epoch 0012, iter [00230, 01251], lr: 0.000600, loss: 0.1400
2022-10-02 09:58:01 - train: epoch 0012, iter [00240, 01251], lr: 0.000600, loss: 0.1484
2022-10-02 09:58:29 - train: epoch 0012, iter [00250, 01251], lr: 0.000600, loss: 0.1314
2022-10-02 09:58:57 - train: epoch 0012, iter [00260, 01251], lr: 0.000600, loss: 0.1275
2022-10-02 09:59:25 - train: epoch 0012, iter [00270, 01251], lr: 0.000600, loss: 0.1275
2022-10-02 09:59:54 - train: epoch 0012, iter [00280, 01251], lr: 0.000600, loss: 0.1285
2022-10-02 10:00:22 - train: epoch 0012, iter [00290, 01251], lr: 0.000600, loss: 0.1330
2022-10-02 10:00:50 - train: epoch 0012, iter [00300, 01251], lr: 0.000600, loss: 0.1350
2022-10-02 10:01:18 - train: epoch 0012, iter [00310, 01251], lr: 0.000600, loss: 0.1336
2022-10-02 10:01:46 - train: epoch 0012, iter [00320, 01251], lr: 0.000600, loss: 0.1330
2022-10-02 10:02:14 - train: epoch 0012, iter [00330, 01251], lr: 0.000600, loss: 0.1267
2022-10-02 10:02:43 - train: epoch 0012, iter [00340, 01251], lr: 0.000600, loss: 0.1410
2022-10-02 10:03:11 - train: epoch 0012, iter [00350, 01251], lr: 0.000600, loss: 0.1355
2022-10-02 10:03:39 - train: epoch 0012, iter [00360, 01251], lr: 0.000600, loss: 0.1298
2022-10-02 10:04:07 - train: epoch 0012, iter [00370, 01251], lr: 0.000600, loss: 0.1316
2022-10-02 10:04:36 - train: epoch 0012, iter [00380, 01251], lr: 0.000600, loss: 0.1335
2022-10-02 10:05:04 - train: epoch 0012, iter [00390, 01251], lr: 0.000600, loss: 0.1324
2022-10-02 10:05:32 - train: epoch 0012, iter [00400, 01251], lr: 0.000600, loss: 0.1293
2022-10-02 10:06:00 - train: epoch 0012, iter [00410, 01251], lr: 0.000600, loss: 0.1296
2022-10-02 10:06:29 - train: epoch 0012, iter [00420, 01251], lr: 0.000600, loss: 0.1366
2022-10-02 10:06:57 - train: epoch 0012, iter [00430, 01251], lr: 0.000600, loss: 0.1233
2022-10-02 10:07:25 - train: epoch 0012, iter [00440, 01251], lr: 0.000600, loss: 0.1363
2022-10-02 10:07:53 - train: epoch 0012, iter [00450, 01251], lr: 0.000600, loss: 0.1256
2022-10-02 10:08:21 - train: epoch 0012, iter [00460, 01251], lr: 0.000600, loss: 0.1276
2022-10-02 10:08:50 - train: epoch 0012, iter [00470, 01251], lr: 0.000600, loss: 0.1281
2022-10-02 10:09:18 - train: epoch 0012, iter [00480, 01251], lr: 0.000600, loss: 0.1288
2022-10-02 10:09:46 - train: epoch 0012, iter [00490, 01251], lr: 0.000600, loss: 0.1228
2022-10-02 10:10:14 - train: epoch 0012, iter [00500, 01251], lr: 0.000600, loss: 0.1287
2022-10-02 10:10:42 - train: epoch 0012, iter [00510, 01251], lr: 0.000600, loss: 0.1291
2022-10-02 10:11:11 - train: epoch 0012, iter [00520, 01251], lr: 0.000600, loss: 0.1334
2022-10-02 10:11:39 - train: epoch 0012, iter [00530, 01251], lr: 0.000600, loss: 0.1252
2022-10-02 10:12:07 - train: epoch 0012, iter [00540, 01251], lr: 0.000600, loss: 0.1352
2022-10-02 10:12:35 - train: epoch 0012, iter [00550, 01251], lr: 0.000600, loss: 0.1451
2022-10-02 10:13:03 - train: epoch 0012, iter [00560, 01251], lr: 0.000600, loss: 0.1414
2022-10-02 10:13:31 - train: epoch 0012, iter [00570, 01251], lr: 0.000600, loss: 0.1395
2022-10-02 10:13:59 - train: epoch 0012, iter [00580, 01251], lr: 0.000600, loss: 0.1397
2022-10-02 10:14:27 - train: epoch 0012, iter [00590, 01251], lr: 0.000600, loss: 0.1446
2022-10-02 10:14:55 - train: epoch 0012, iter [00600, 01251], lr: 0.000600, loss: 0.1359
2022-10-02 10:15:23 - train: epoch 0012, iter [00610, 01251], lr: 0.000600, loss: 0.1245
2022-10-02 10:15:51 - train: epoch 0012, iter [00620, 01251], lr: 0.000600, loss: 0.1377
2022-10-02 10:16:19 - train: epoch 0012, iter [00630, 01251], lr: 0.000600, loss: 0.1233
2022-10-02 10:16:48 - train: epoch 0012, iter [00640, 01251], lr: 0.000600, loss: 0.1227
2022-10-02 10:17:16 - train: epoch 0012, iter [00650, 01251], lr: 0.000600, loss: 0.1403
2022-10-02 10:17:44 - train: epoch 0012, iter [00660, 01251], lr: 0.000600, loss: 0.1278
2022-10-02 10:18:12 - train: epoch 0012, iter [00670, 01251], lr: 0.000600, loss: 0.1264
2022-10-02 10:18:40 - train: epoch 0012, iter [00680, 01251], lr: 0.000600, loss: 0.1405
2022-10-02 10:19:09 - train: epoch 0012, iter [00690, 01251], lr: 0.000600, loss: 0.1382
2022-10-02 10:19:37 - train: epoch 0012, iter [00700, 01251], lr: 0.000600, loss: 0.1191
2022-10-02 10:20:05 - train: epoch 0012, iter [00710, 01251], lr: 0.000600, loss: 0.1274
2022-10-02 10:20:33 - train: epoch 0012, iter [00720, 01251], lr: 0.000600, loss: 0.1284
2022-10-02 10:21:01 - train: epoch 0012, iter [00730, 01251], lr: 0.000600, loss: 0.1361
2022-10-02 10:21:29 - train: epoch 0012, iter [00740, 01251], lr: 0.000600, loss: 0.1262
2022-10-02 10:21:57 - train: epoch 0012, iter [00750, 01251], lr: 0.000600, loss: 0.1226
2022-10-02 10:22:25 - train: epoch 0012, iter [00760, 01251], lr: 0.000600, loss: 0.1439
2022-10-02 10:22:54 - train: epoch 0012, iter [00770, 01251], lr: 0.000600, loss: 0.1228
2022-10-02 10:23:22 - train: epoch 0012, iter [00780, 01251], lr: 0.000600, loss: 0.1263
2022-10-02 10:23:50 - train: epoch 0012, iter [00790, 01251], lr: 0.000600, loss: 0.1334
2022-10-02 10:24:18 - train: epoch 0012, iter [00800, 01251], lr: 0.000600, loss: 0.1377
2022-10-02 10:24:46 - train: epoch 0012, iter [00810, 01251], lr: 0.000600, loss: 0.1313
2022-10-02 10:25:14 - train: epoch 0012, iter [00820, 01251], lr: 0.000599, loss: 0.1322
2022-10-02 10:25:42 - train: epoch 0012, iter [00830, 01251], lr: 0.000599, loss: 0.1346
2022-10-02 10:26:10 - train: epoch 0012, iter [00840, 01251], lr: 0.000599, loss: 0.1324
2022-10-02 10:26:38 - train: epoch 0012, iter [00850, 01251], lr: 0.000599, loss: 0.1152
2022-10-02 10:27:06 - train: epoch 0012, iter [00860, 01251], lr: 0.000599, loss: 0.1389
2022-10-02 10:27:34 - train: epoch 0012, iter [00870, 01251], lr: 0.000599, loss: 0.1273
2022-10-02 10:28:03 - train: epoch 0012, iter [00880, 01251], lr: 0.000599, loss: 0.1221
2022-10-02 10:28:31 - train: epoch 0012, iter [00890, 01251], lr: 0.000599, loss: 0.1402
2022-10-02 10:28:59 - train: epoch 0012, iter [00900, 01251], lr: 0.000599, loss: 0.1232
2022-10-02 10:29:27 - train: epoch 0012, iter [00910, 01251], lr: 0.000599, loss: 0.1308
2022-10-02 10:29:55 - train: epoch 0012, iter [00920, 01251], lr: 0.000599, loss: 0.1240
2022-10-02 10:30:24 - train: epoch 0012, iter [00930, 01251], lr: 0.000599, loss: 0.1257
2022-10-02 10:30:52 - train: epoch 0012, iter [00940, 01251], lr: 0.000599, loss: 0.1389
2022-10-02 10:31:20 - train: epoch 0012, iter [00950, 01251], lr: 0.000599, loss: 0.1334
2022-10-02 10:31:48 - train: epoch 0012, iter [00960, 01251], lr: 0.000599, loss: 0.1265
2022-10-02 10:32:16 - train: epoch 0012, iter [00970, 01251], lr: 0.000599, loss: 0.1318
2022-10-02 10:32:44 - train: epoch 0012, iter [00980, 01251], lr: 0.000599, loss: 0.1351
2022-10-02 10:33:12 - train: epoch 0012, iter [00990, 01251], lr: 0.000599, loss: 0.1371
2022-10-02 10:33:41 - train: epoch 0012, iter [01000, 01251], lr: 0.000599, loss: 0.1290
2022-10-02 10:34:08 - train: epoch 0012, iter [01010, 01251], lr: 0.000599, loss: 0.1359
2022-10-02 10:34:37 - train: epoch 0012, iter [01020, 01251], lr: 0.000599, loss: 0.1171
2022-10-02 10:35:05 - train: epoch 0012, iter [01030, 01251], lr: 0.000599, loss: 0.1350
2022-10-02 10:35:33 - train: epoch 0012, iter [01040, 01251], lr: 0.000599, loss: 0.1169
2022-10-02 10:36:01 - train: epoch 0012, iter [01050, 01251], lr: 0.000599, loss: 0.1277
2022-10-02 10:36:29 - train: epoch 0012, iter [01060, 01251], lr: 0.000599, loss: 0.1333
2022-10-02 10:36:57 - train: epoch 0012, iter [01070, 01251], lr: 0.000599, loss: 0.1370
2022-10-02 10:37:26 - train: epoch 0012, iter [01080, 01251], lr: 0.000599, loss: 0.1244
2022-10-02 10:37:54 - train: epoch 0012, iter [01090, 01251], lr: 0.000599, loss: 0.1270
2022-10-02 10:38:22 - train: epoch 0012, iter [01100, 01251], lr: 0.000599, loss: 0.1275
2022-10-02 10:38:50 - train: epoch 0012, iter [01110, 01251], lr: 0.000599, loss: 0.1304
2022-10-02 10:39:19 - train: epoch 0012, iter [01120, 01251], lr: 0.000599, loss: 0.1296
2022-10-02 10:39:47 - train: epoch 0012, iter [01130, 01251], lr: 0.000599, loss: 0.1297
2022-10-02 10:40:15 - train: epoch 0012, iter [01140, 01251], lr: 0.000599, loss: 0.1300
2022-10-02 10:40:43 - train: epoch 0012, iter [01150, 01251], lr: 0.000599, loss: 0.1195
2022-10-02 10:41:11 - train: epoch 0012, iter [01160, 01251], lr: 0.000599, loss: 0.1271
2022-10-02 10:41:39 - train: epoch 0012, iter [01170, 01251], lr: 0.000599, loss: 0.1315
2022-10-02 10:42:07 - train: epoch 0012, iter [01180, 01251], lr: 0.000599, loss: 0.1220
2022-10-02 10:42:36 - train: epoch 0012, iter [01190, 01251], lr: 0.000599, loss: 0.1287
2022-10-02 10:43:04 - train: epoch 0012, iter [01200, 01251], lr: 0.000599, loss: 0.1353
2022-10-02 10:43:32 - train: epoch 0012, iter [01210, 01251], lr: 0.000599, loss: 0.1307
2022-10-02 10:44:00 - train: epoch 0012, iter [01220, 01251], lr: 0.000599, loss: 0.1216
2022-10-02 10:44:28 - train: epoch 0012, iter [01230, 01251], lr: 0.000599, loss: 0.1310
2022-10-02 10:44:56 - train: epoch 0012, iter [01240, 01251], lr: 0.000599, loss: 0.1242
2022-10-02 10:45:24 - train: epoch 0012, iter [01250, 01251], lr: 0.000599, loss: 0.1271
2022-10-02 10:45:29 - train: epoch 012, train_loss: 0.1322
2022-10-02 10:45:30 - until epoch: 012, best_loss: 0.1322
2022-10-02 10:45:30 - epoch 013 lr: 0.000599
2022-10-02 10:46:05 - train: epoch 0013, iter [00010, 01251], lr: 0.000599, loss: 0.1254
2022-10-02 10:46:33 - train: epoch 0013, iter [00020, 01251], lr: 0.000599, loss: 0.1319
2022-10-02 10:47:01 - train: epoch 0013, iter [00030, 01251], lr: 0.000599, loss: 0.1368
2022-10-02 10:47:30 - train: epoch 0013, iter [00040, 01251], lr: 0.000599, loss: 0.1211
2022-10-02 10:47:57 - train: epoch 0013, iter [00050, 01251], lr: 0.000599, loss: 0.1385
2022-10-02 10:48:25 - train: epoch 0013, iter [00060, 01251], lr: 0.000599, loss: 0.1312
2022-10-02 10:48:53 - train: epoch 0013, iter [00070, 01251], lr: 0.000599, loss: 0.1216
2022-10-02 10:49:21 - train: epoch 0013, iter [00080, 01251], lr: 0.000599, loss: 0.1288
2022-10-02 10:49:49 - train: epoch 0013, iter [00090, 01251], lr: 0.000599, loss: 0.1399
2022-10-02 10:50:17 - train: epoch 0013, iter [00100, 01251], lr: 0.000599, loss: 0.1325
2022-10-02 10:50:45 - train: epoch 0013, iter [00110, 01251], lr: 0.000599, loss: 0.1305
2022-10-02 10:51:13 - train: epoch 0013, iter [00120, 01251], lr: 0.000599, loss: 0.1260
2022-10-02 10:51:41 - train: epoch 0013, iter [00130, 01251], lr: 0.000599, loss: 0.1235
2022-10-02 10:52:09 - train: epoch 0013, iter [00140, 01251], lr: 0.000599, loss: 0.1288
2022-10-02 10:52:37 - train: epoch 0013, iter [00150, 01251], lr: 0.000599, loss: 0.1314
2022-10-02 10:53:05 - train: epoch 0013, iter [00160, 01251], lr: 0.000599, loss: 0.1238
2022-10-02 10:53:33 - train: epoch 0013, iter [00170, 01251], lr: 0.000599, loss: 0.1259
2022-10-02 10:54:01 - train: epoch 0013, iter [00180, 01251], lr: 0.000599, loss: 0.1291
2022-10-02 10:54:29 - train: epoch 0013, iter [00190, 01251], lr: 0.000599, loss: 0.1379
2022-10-02 10:54:57 - train: epoch 0013, iter [00200, 01251], lr: 0.000599, loss: 0.1231
2022-10-02 10:55:25 - train: epoch 0013, iter [00210, 01251], lr: 0.000599, loss: 0.1172
2022-10-02 10:55:53 - train: epoch 0013, iter [00220, 01251], lr: 0.000599, loss: 0.1247
2022-10-02 10:56:21 - train: epoch 0013, iter [00230, 01251], lr: 0.000599, loss: 0.1242
2022-10-02 10:56:49 - train: epoch 0013, iter [00240, 01251], lr: 0.000599, loss: 0.1258
2022-10-02 10:57:17 - train: epoch 0013, iter [00250, 01251], lr: 0.000599, loss: 0.1248
2022-10-02 10:57:44 - train: epoch 0013, iter [00260, 01251], lr: 0.000599, loss: 0.1204
2022-10-02 10:58:12 - train: epoch 0013, iter [00270, 01251], lr: 0.000599, loss: 0.1271
2022-10-02 10:58:40 - train: epoch 0013, iter [00280, 01251], lr: 0.000599, loss: 0.1305
2022-10-02 10:59:08 - train: epoch 0013, iter [00290, 01251], lr: 0.000599, loss: 0.1360
2022-10-02 10:59:36 - train: epoch 0013, iter [00300, 01251], lr: 0.000599, loss: 0.1279
2022-10-02 11:00:04 - train: epoch 0013, iter [00310, 01251], lr: 0.000599, loss: 0.1318
2022-10-02 11:00:32 - train: epoch 0013, iter [00320, 01251], lr: 0.000599, loss: 0.1321
2022-10-02 11:01:00 - train: epoch 0013, iter [00330, 01251], lr: 0.000599, loss: 0.1257
2022-10-02 11:01:28 - train: epoch 0013, iter [00340, 01251], lr: 0.000599, loss: 0.1311
2022-10-02 11:01:56 - train: epoch 0013, iter [00350, 01251], lr: 0.000599, loss: 0.1263
2022-10-02 11:02:23 - train: epoch 0013, iter [00360, 01251], lr: 0.000599, loss: 0.1297
2022-10-02 11:02:51 - train: epoch 0013, iter [00370, 01251], lr: 0.000599, loss: 0.1228
2022-10-02 11:03:19 - train: epoch 0013, iter [00380, 01251], lr: 0.000599, loss: 0.1165
2022-10-02 11:03:47 - train: epoch 0013, iter [00390, 01251], lr: 0.000599, loss: 0.1235
2022-10-02 11:04:14 - train: epoch 0013, iter [00400, 01251], lr: 0.000599, loss: 0.1206
2022-10-02 11:04:42 - train: epoch 0013, iter [00410, 01251], lr: 0.000599, loss: 0.1305
2022-10-02 11:05:10 - train: epoch 0013, iter [00420, 01251], lr: 0.000599, loss: 0.1300
2022-10-02 11:05:38 - train: epoch 0013, iter [00430, 01251], lr: 0.000599, loss: 0.1219
2022-10-02 11:06:06 - train: epoch 0013, iter [00440, 01251], lr: 0.000599, loss: 0.1297
2022-10-02 11:06:33 - train: epoch 0013, iter [00450, 01251], lr: 0.000599, loss: 0.1291
2022-10-02 11:07:01 - train: epoch 0013, iter [00460, 01251], lr: 0.000599, loss: 0.1293
2022-10-02 11:07:29 - train: epoch 0013, iter [00470, 01251], lr: 0.000599, loss: 0.1221
2022-10-02 11:07:57 - train: epoch 0013, iter [00480, 01251], lr: 0.000599, loss: 0.1222
2022-10-02 11:08:25 - train: epoch 0013, iter [00490, 01251], lr: 0.000599, loss: 0.1336
2022-10-02 11:08:53 - train: epoch 0013, iter [00500, 01251], lr: 0.000599, loss: 0.1266
2022-10-02 11:09:20 - train: epoch 0013, iter [00510, 01251], lr: 0.000599, loss: 0.1324
2022-10-02 11:09:48 - train: epoch 0013, iter [00520, 01251], lr: 0.000599, loss: 0.1242
2022-10-02 11:10:16 - train: epoch 0013, iter [00530, 01251], lr: 0.000599, loss: 0.1245
2022-10-02 11:10:44 - train: epoch 0013, iter [00540, 01251], lr: 0.000599, loss: 0.1199
2022-10-02 11:11:12 - train: epoch 0013, iter [00550, 01251], lr: 0.000599, loss: 0.1234
2022-10-02 11:11:40 - train: epoch 0013, iter [00560, 01251], lr: 0.000599, loss: 0.1194
2022-10-02 11:12:07 - train: epoch 0013, iter [00570, 01251], lr: 0.000599, loss: 0.1391
2022-10-02 11:12:35 - train: epoch 0013, iter [00580, 01251], lr: 0.000599, loss: 0.1212
2022-10-02 11:13:03 - train: epoch 0013, iter [00590, 01251], lr: 0.000599, loss: 0.1362
2022-10-02 11:13:31 - train: epoch 0013, iter [00600, 01251], lr: 0.000599, loss: 0.1305
2022-10-02 11:13:58 - train: epoch 0013, iter [00610, 01251], lr: 0.000599, loss: 0.1185
2022-10-02 11:14:26 - train: epoch 0013, iter [00620, 01251], lr: 0.000599, loss: 0.1275
2022-10-02 11:14:54 - train: epoch 0013, iter [00630, 01251], lr: 0.000599, loss: 0.1290
2022-10-02 11:15:22 - train: epoch 0013, iter [00640, 01251], lr: 0.000599, loss: 0.1246
2022-10-02 11:15:50 - train: epoch 0013, iter [00650, 01251], lr: 0.000599, loss: 0.1280
2022-10-02 11:16:17 - train: epoch 0013, iter [00660, 01251], lr: 0.000599, loss: 0.1264
2022-10-02 11:16:45 - train: epoch 0013, iter [00670, 01251], lr: 0.000599, loss: 0.1276
2022-10-02 11:17:13 - train: epoch 0013, iter [00680, 01251], lr: 0.000599, loss: 0.1320
2022-10-02 11:17:41 - train: epoch 0013, iter [00690, 01251], lr: 0.000599, loss: 0.1282
2022-10-02 11:18:09 - train: epoch 0013, iter [00700, 01251], lr: 0.000599, loss: 0.1248
2022-10-02 11:18:37 - train: epoch 0013, iter [00710, 01251], lr: 0.000599, loss: 0.1225
2022-10-02 11:19:05 - train: epoch 0013, iter [00720, 01251], lr: 0.000599, loss: 0.1299
2022-10-02 11:19:32 - train: epoch 0013, iter [00730, 01251], lr: 0.000599, loss: 0.1299
2022-10-02 11:20:00 - train: epoch 0013, iter [00740, 01251], lr: 0.000599, loss: 0.1343
2022-10-02 11:20:28 - train: epoch 0013, iter [00750, 01251], lr: 0.000599, loss: 0.1139
2022-10-02 11:20:56 - train: epoch 0013, iter [00760, 01251], lr: 0.000599, loss: 0.1281
2022-10-02 11:21:24 - train: epoch 0013, iter [00770, 01251], lr: 0.000599, loss: 0.1217
2022-10-02 11:21:52 - train: epoch 0013, iter [00780, 01251], lr: 0.000599, loss: 0.1240
2022-10-02 11:22:20 - train: epoch 0013, iter [00790, 01251], lr: 0.000599, loss: 0.1287
2022-10-02 11:22:48 - train: epoch 0013, iter [00800, 01251], lr: 0.000599, loss: 0.1330
2022-10-02 11:23:16 - train: epoch 0013, iter [00810, 01251], lr: 0.000599, loss: 0.1231
2022-10-02 11:23:44 - train: epoch 0013, iter [00820, 01251], lr: 0.000599, loss: 0.1357
2022-10-02 11:24:12 - train: epoch 0013, iter [00830, 01251], lr: 0.000599, loss: 0.1269
2022-10-02 11:24:39 - train: epoch 0013, iter [00840, 01251], lr: 0.000599, loss: 0.1276
2022-10-02 11:25:07 - train: epoch 0013, iter [00850, 01251], lr: 0.000599, loss: 0.1265
2022-10-02 11:25:35 - train: epoch 0013, iter [00860, 01251], lr: 0.000599, loss: 0.1195
2022-10-02 11:26:03 - train: epoch 0013, iter [00870, 01251], lr: 0.000599, loss: 0.1247
2022-10-02 11:26:31 - train: epoch 0013, iter [00880, 01251], lr: 0.000599, loss: 0.1192
2022-10-02 11:26:59 - train: epoch 0013, iter [00890, 01251], lr: 0.000599, loss: 0.1296
2022-10-02 11:27:27 - train: epoch 0013, iter [00900, 01251], lr: 0.000599, loss: 0.1346
2022-10-02 11:27:55 - train: epoch 0013, iter [00910, 01251], lr: 0.000599, loss: 0.1292
2022-10-02 11:28:23 - train: epoch 0013, iter [00920, 01251], lr: 0.000599, loss: 0.1247
2022-10-02 11:28:51 - train: epoch 0013, iter [00930, 01251], lr: 0.000599, loss: 0.1218
2022-10-02 11:29:19 - train: epoch 0013, iter [00940, 01251], lr: 0.000599, loss: 0.1209
2022-10-02 11:29:47 - train: epoch 0013, iter [00950, 01251], lr: 0.000599, loss: 0.1227
2022-10-02 11:30:15 - train: epoch 0013, iter [00960, 01251], lr: 0.000599, loss: 0.1148
2022-10-02 11:30:43 - train: epoch 0013, iter [00970, 01251], lr: 0.000599, loss: 0.1254
2022-10-02 11:31:11 - train: epoch 0013, iter [00980, 01251], lr: 0.000599, loss: 0.1341
2022-10-02 11:31:39 - train: epoch 0013, iter [00990, 01251], lr: 0.000599, loss: 0.1298
2022-10-02 11:32:07 - train: epoch 0013, iter [01000, 01251], lr: 0.000599, loss: 0.1268
2022-10-02 11:32:35 - train: epoch 0013, iter [01010, 01251], lr: 0.000599, loss: 0.1176
2022-10-02 11:33:03 - train: epoch 0013, iter [01020, 01251], lr: 0.000599, loss: 0.1162
2022-10-02 11:33:31 - train: epoch 0013, iter [01030, 01251], lr: 0.000599, loss: 0.1372
2022-10-02 11:33:59 - train: epoch 0013, iter [01040, 01251], lr: 0.000599, loss: 0.1260
2022-10-02 11:34:27 - train: epoch 0013, iter [01050, 01251], lr: 0.000599, loss: 0.1161
2022-10-02 11:34:55 - train: epoch 0013, iter [01060, 01251], lr: 0.000599, loss: 0.1161
2022-10-02 11:35:23 - train: epoch 0013, iter [01070, 01251], lr: 0.000599, loss: 0.1217
2022-10-02 11:35:51 - train: epoch 0013, iter [01080, 01251], lr: 0.000599, loss: 0.1204
2022-10-02 11:36:18 - train: epoch 0013, iter [01090, 01251], lr: 0.000598, loss: 0.1183
2022-10-02 11:36:47 - train: epoch 0013, iter [01100, 01251], lr: 0.000598, loss: 0.1326
2022-10-02 11:37:15 - train: epoch 0013, iter [01110, 01251], lr: 0.000598, loss: 0.1295
2022-10-02 11:37:43 - train: epoch 0013, iter [01120, 01251], lr: 0.000598, loss: 0.1231
2022-10-02 11:38:11 - train: epoch 0013, iter [01130, 01251], lr: 0.000598, loss: 0.1266
2022-10-02 11:38:39 - train: epoch 0013, iter [01140, 01251], lr: 0.000598, loss: 0.1232
2022-10-02 11:39:07 - train: epoch 0013, iter [01150, 01251], lr: 0.000598, loss: 0.1252
2022-10-02 11:39:35 - train: epoch 0013, iter [01160, 01251], lr: 0.000598, loss: 0.1235
2022-10-02 11:40:03 - train: epoch 0013, iter [01170, 01251], lr: 0.000598, loss: 0.1314
2022-10-02 11:40:31 - train: epoch 0013, iter [01180, 01251], lr: 0.000598, loss: 0.1231
2022-10-02 11:40:59 - train: epoch 0013, iter [01190, 01251], lr: 0.000598, loss: 0.1168
2022-10-02 11:41:27 - train: epoch 0013, iter [01200, 01251], lr: 0.000598, loss: 0.1217
2022-10-02 11:41:55 - train: epoch 0013, iter [01210, 01251], lr: 0.000598, loss: 0.1151
2022-10-02 11:42:23 - train: epoch 0013, iter [01220, 01251], lr: 0.000598, loss: 0.1242
2022-10-02 11:42:51 - train: epoch 0013, iter [01230, 01251], lr: 0.000598, loss: 0.1177
2022-10-02 11:43:19 - train: epoch 0013, iter [01240, 01251], lr: 0.000598, loss: 0.1266
2022-10-02 11:43:46 - train: epoch 0013, iter [01250, 01251], lr: 0.000598, loss: 0.1236
2022-10-02 11:43:51 - train: epoch 013, train_loss: 0.1264
2022-10-02 11:43:53 - until epoch: 013, best_loss: 0.1264
2022-10-02 11:43:53 - epoch 014 lr: 0.000598
2022-10-02 11:44:28 - train: epoch 0014, iter [00010, 01251], lr: 0.000598, loss: 0.1189
2022-10-02 11:44:56 - train: epoch 0014, iter [00020, 01251], lr: 0.000598, loss: 0.1264
2022-10-02 11:45:24 - train: epoch 0014, iter [00030, 01251], lr: 0.000598, loss: 0.1281
2022-10-02 11:45:52 - train: epoch 0014, iter [00040, 01251], lr: 0.000598, loss: 0.1264
2022-10-02 11:46:20 - train: epoch 0014, iter [00050, 01251], lr: 0.000598, loss: 0.1273
2022-10-02 11:46:48 - train: epoch 0014, iter [00060, 01251], lr: 0.000598, loss: 0.1203
2022-10-02 11:47:16 - train: epoch 0014, iter [00070, 01251], lr: 0.000598, loss: 0.1293
2022-10-02 11:47:43 - train: epoch 0014, iter [00080, 01251], lr: 0.000598, loss: 0.1188
2022-10-02 11:48:11 - train: epoch 0014, iter [00090, 01251], lr: 0.000598, loss: 0.1244
2022-10-02 11:48:39 - train: epoch 0014, iter [00100, 01251], lr: 0.000598, loss: 0.1248
2022-10-02 11:49:07 - train: epoch 0014, iter [00110, 01251], lr: 0.000598, loss: 0.1176
2022-10-02 11:49:35 - train: epoch 0014, iter [00120, 01251], lr: 0.000598, loss: 0.1276
2022-10-02 11:50:03 - train: epoch 0014, iter [00130, 01251], lr: 0.000598, loss: 0.1276
2022-10-02 11:50:31 - train: epoch 0014, iter [00140, 01251], lr: 0.000598, loss: 0.1211
2022-10-02 11:50:58 - train: epoch 0014, iter [00150, 01251], lr: 0.000598, loss: 0.1241
2022-10-02 11:51:26 - train: epoch 0014, iter [00160, 01251], lr: 0.000598, loss: 0.1193
2022-10-02 11:51:54 - train: epoch 0014, iter [00170, 01251], lr: 0.000598, loss: 0.1308
2022-10-02 11:52:22 - train: epoch 0014, iter [00180, 01251], lr: 0.000598, loss: 0.1323
2022-10-02 11:52:50 - train: epoch 0014, iter [00190, 01251], lr: 0.000598, loss: 0.1209
2022-10-02 11:53:18 - train: epoch 0014, iter [00200, 01251], lr: 0.000598, loss: 0.1248
2022-10-02 11:53:46 - train: epoch 0014, iter [00210, 01251], lr: 0.000598, loss: 0.1210
2022-10-02 11:54:14 - train: epoch 0014, iter [00220, 01251], lr: 0.000598, loss: 0.1307
2022-10-02 11:54:41 - train: epoch 0014, iter [00230, 01251], lr: 0.000598, loss: 0.1223
2022-10-02 11:55:09 - train: epoch 0014, iter [00240, 01251], lr: 0.000598, loss: 0.1320
2022-10-02 11:55:37 - train: epoch 0014, iter [00250, 01251], lr: 0.000598, loss: 0.1285
2022-10-02 11:56:05 - train: epoch 0014, iter [00260, 01251], lr: 0.000598, loss: 0.1278
2022-10-02 11:56:33 - train: epoch 0014, iter [00270, 01251], lr: 0.000598, loss: 0.1206
2022-10-02 11:57:01 - train: epoch 0014, iter [00280, 01251], lr: 0.000598, loss: 0.1254
2022-10-02 11:57:29 - train: epoch 0014, iter [00290, 01251], lr: 0.000598, loss: 0.1308
2022-10-02 11:57:57 - train: epoch 0014, iter [00300, 01251], lr: 0.000598, loss: 0.1190
2022-10-02 11:58:25 - train: epoch 0014, iter [00310, 01251], lr: 0.000598, loss: 0.1250
2022-10-02 11:58:53 - train: epoch 0014, iter [00320, 01251], lr: 0.000598, loss: 0.1230
2022-10-02 11:59:21 - train: epoch 0014, iter [00330, 01251], lr: 0.000598, loss: 0.1188
2022-10-02 11:59:49 - train: epoch 0014, iter [00340, 01251], lr: 0.000598, loss: 0.1272
2022-10-02 12:00:17 - train: epoch 0014, iter [00350, 01251], lr: 0.000598, loss: 0.1209
2022-10-02 12:00:44 - train: epoch 0014, iter [00360, 01251], lr: 0.000598, loss: 0.1376
2022-10-02 12:01:12 - train: epoch 0014, iter [00370, 01251], lr: 0.000598, loss: 0.1210
2022-10-02 12:01:40 - train: epoch 0014, iter [00380, 01251], lr: 0.000598, loss: 0.1256
2022-10-02 12:02:08 - train: epoch 0014, iter [00390, 01251], lr: 0.000598, loss: 0.1307
2022-10-02 12:02:36 - train: epoch 0014, iter [00400, 01251], lr: 0.000598, loss: 0.1184
2022-10-02 12:03:04 - train: epoch 0014, iter [00410, 01251], lr: 0.000598, loss: 0.1215
2022-10-02 12:03:32 - train: epoch 0014, iter [00420, 01251], lr: 0.000598, loss: 0.1278
2022-10-02 12:04:00 - train: epoch 0014, iter [00430, 01251], lr: 0.000598, loss: 0.1139
2022-10-02 12:04:28 - train: epoch 0014, iter [00440, 01251], lr: 0.000598, loss: 0.1267
2022-10-02 12:04:56 - train: epoch 0014, iter [00450, 01251], lr: 0.000598, loss: 0.1168
2022-10-02 12:05:24 - train: epoch 0014, iter [00460, 01251], lr: 0.000598, loss: 0.1232
2022-10-02 12:05:52 - train: epoch 0014, iter [00470, 01251], lr: 0.000598, loss: 0.1337
2022-10-02 12:06:20 - train: epoch 0014, iter [00480, 01251], lr: 0.000598, loss: 0.1203
2022-10-02 12:06:48 - train: epoch 0014, iter [00490, 01251], lr: 0.000598, loss: 0.1204
2022-10-02 12:07:16 - train: epoch 0014, iter [00500, 01251], lr: 0.000598, loss: 0.1229
2022-10-02 12:07:44 - train: epoch 0014, iter [00510, 01251], lr: 0.000598, loss: 0.1200
2022-10-02 12:08:12 - train: epoch 0014, iter [00520, 01251], lr: 0.000598, loss: 0.1234
2022-10-02 12:08:40 - train: epoch 0014, iter [00530, 01251], lr: 0.000598, loss: 0.1278
2022-10-02 12:09:07 - train: epoch 0014, iter [00540, 01251], lr: 0.000598, loss: 0.1222
2022-10-02 12:09:35 - train: epoch 0014, iter [00550, 01251], lr: 0.000598, loss: 0.1296
2022-10-02 12:10:03 - train: epoch 0014, iter [00560, 01251], lr: 0.000598, loss: 0.1158
2022-10-02 12:10:31 - train: epoch 0014, iter [00570, 01251], lr: 0.000598, loss: 0.1155
2022-10-02 12:10:59 - train: epoch 0014, iter [00580, 01251], lr: 0.000598, loss: 0.1184
2022-10-02 12:11:27 - train: epoch 0014, iter [00590, 01251], lr: 0.000598, loss: 0.1247
2022-10-02 12:11:55 - train: epoch 0014, iter [00600, 01251], lr: 0.000598, loss: 0.1220
2022-10-02 12:12:23 - train: epoch 0014, iter [00610, 01251], lr: 0.000598, loss: 0.1169
2022-10-02 12:12:51 - train: epoch 0014, iter [00620, 01251], lr: 0.000598, loss: 0.1261
2022-10-02 12:13:19 - train: epoch 0014, iter [00630, 01251], lr: 0.000598, loss: 0.1252
2022-10-02 12:13:47 - train: epoch 0014, iter [00640, 01251], lr: 0.000598, loss: 0.1305
2022-10-02 12:14:15 - train: epoch 0014, iter [00650, 01251], lr: 0.000598, loss: 0.1195
2022-10-02 12:14:43 - train: epoch 0014, iter [00660, 01251], lr: 0.000598, loss: 0.1210
2022-10-02 12:15:11 - train: epoch 0014, iter [00670, 01251], lr: 0.000598, loss: 0.1225
2022-10-02 12:15:39 - train: epoch 0014, iter [00680, 01251], lr: 0.000598, loss: 0.1231
2022-10-02 12:16:07 - train: epoch 0014, iter [00690, 01251], lr: 0.000598, loss: 0.1181
2022-10-02 12:16:35 - train: epoch 0014, iter [00700, 01251], lr: 0.000598, loss: 0.1156
2022-10-02 12:17:03 - train: epoch 0014, iter [00710, 01251], lr: 0.000598, loss: 0.1232
2022-10-02 12:17:31 - train: epoch 0014, iter [00720, 01251], lr: 0.000598, loss: 0.1201
2022-10-02 12:17:59 - train: epoch 0014, iter [00730, 01251], lr: 0.000598, loss: 0.1193
2022-10-02 12:18:27 - train: epoch 0014, iter [00740, 01251], lr: 0.000598, loss: 0.1198
2022-10-02 12:18:55 - train: epoch 0014, iter [00750, 01251], lr: 0.000598, loss: 0.1259
2022-10-02 12:19:23 - train: epoch 0014, iter [00760, 01251], lr: 0.000598, loss: 0.1140
2022-10-02 12:19:51 - train: epoch 0014, iter [00770, 01251], lr: 0.000598, loss: 0.1256
2022-10-02 12:20:19 - train: epoch 0014, iter [00780, 01251], lr: 0.000598, loss: 0.1385
2022-10-02 12:20:47 - train: epoch 0014, iter [00790, 01251], lr: 0.000598, loss: 0.1448
2022-10-02 12:21:15 - train: epoch 0014, iter [00800, 01251], lr: 0.000598, loss: 0.1254
2022-10-02 12:21:43 - train: epoch 0014, iter [00810, 01251], lr: 0.000598, loss: 0.1272
2022-10-02 12:22:11 - train: epoch 0014, iter [00820, 01251], lr: 0.000598, loss: 0.1297
2022-10-02 12:22:39 - train: epoch 0014, iter [00830, 01251], lr: 0.000598, loss: 0.1201
2022-10-02 12:23:07 - train: epoch 0014, iter [00840, 01251], lr: 0.000598, loss: 0.1226
2022-10-02 12:23:35 - train: epoch 0014, iter [00850, 01251], lr: 0.000598, loss: 0.1248
2022-10-02 12:24:02 - train: epoch 0014, iter [00860, 01251], lr: 0.000598, loss: 0.1352
2022-10-02 12:24:30 - train: epoch 0014, iter [00870, 01251], lr: 0.000598, loss: 0.1043
2022-10-02 12:24:58 - train: epoch 0014, iter [00880, 01251], lr: 0.000597, loss: 0.1203
2022-10-02 12:25:26 - train: epoch 0014, iter [00890, 01251], lr: 0.000597, loss: 0.1288
2022-10-02 12:25:54 - train: epoch 0014, iter [00900, 01251], lr: 0.000597, loss: 0.1289
2022-10-02 12:26:22 - train: epoch 0014, iter [00910, 01251], lr: 0.000597, loss: 0.1221
2022-10-02 12:26:50 - train: epoch 0014, iter [00920, 01251], lr: 0.000597, loss: 0.1240
2022-10-02 12:27:18 - train: epoch 0014, iter [00930, 01251], lr: 0.000597, loss: 0.1205
2022-10-02 12:27:46 - train: epoch 0014, iter [00940, 01251], lr: 0.000597, loss: 0.1215
2022-10-02 12:28:14 - train: epoch 0014, iter [00950, 01251], lr: 0.000597, loss: 0.1221
2022-10-02 12:28:41 - train: epoch 0014, iter [00960, 01251], lr: 0.000597, loss: 0.1233
2022-10-02 12:29:09 - train: epoch 0014, iter [00970, 01251], lr: 0.000597, loss: 0.1264
2022-10-02 12:29:37 - train: epoch 0014, iter [00980, 01251], lr: 0.000597, loss: 0.1235
2022-10-02 12:30:05 - train: epoch 0014, iter [00990, 01251], lr: 0.000597, loss: 0.1235
2022-10-02 12:30:33 - train: epoch 0014, iter [01000, 01251], lr: 0.000597, loss: 0.1234
2022-10-02 12:31:01 - train: epoch 0014, iter [01010, 01251], lr: 0.000597, loss: 0.1205
2022-10-02 12:31:29 - train: epoch 0014, iter [01020, 01251], lr: 0.000597, loss: 0.1206
2022-10-02 12:31:57 - train: epoch 0014, iter [01030, 01251], lr: 0.000597, loss: 0.1171
2022-10-02 12:32:25 - train: epoch 0014, iter [01040, 01251], lr: 0.000597, loss: 0.1249
2022-10-02 12:32:53 - train: epoch 0014, iter [01050, 01251], lr: 0.000597, loss: 0.1218
2022-10-02 12:33:21 - train: epoch 0014, iter [01060, 01251], lr: 0.000597, loss: 0.1203
2022-10-02 12:33:49 - train: epoch 0014, iter [01070, 01251], lr: 0.000597, loss: 0.1187
2022-10-02 12:34:17 - train: epoch 0014, iter [01080, 01251], lr: 0.000597, loss: 0.1353
2022-10-02 12:34:45 - train: epoch 0014, iter [01090, 01251], lr: 0.000597, loss: 0.1183
2022-10-02 12:35:13 - train: epoch 0014, iter [01100, 01251], lr: 0.000597, loss: 0.1160
2022-10-02 12:35:41 - train: epoch 0014, iter [01110, 01251], lr: 0.000597, loss: 0.1301
2022-10-02 12:36:09 - train: epoch 0014, iter [01120, 01251], lr: 0.000597, loss: 0.1096
2022-10-02 12:36:37 - train: epoch 0014, iter [01130, 01251], lr: 0.000597, loss: 0.1233
2022-10-02 12:37:05 - train: epoch 0014, iter [01140, 01251], lr: 0.000597, loss: 0.1162
2022-10-02 12:37:33 - train: epoch 0014, iter [01150, 01251], lr: 0.000597, loss: 0.1218
2022-10-02 12:38:01 - train: epoch 0014, iter [01160, 01251], lr: 0.000597, loss: 0.1226
2022-10-02 12:38:29 - train: epoch 0014, iter [01170, 01251], lr: 0.000597, loss: 0.1215
2022-10-02 12:38:57 - train: epoch 0014, iter [01180, 01251], lr: 0.000597, loss: 0.1177
2022-10-02 12:39:25 - train: epoch 0014, iter [01190, 01251], lr: 0.000597, loss: 0.1277
2022-10-02 12:39:53 - train: epoch 0014, iter [01200, 01251], lr: 0.000597, loss: 0.1217
2022-10-02 12:40:21 - train: epoch 0014, iter [01210, 01251], lr: 0.000597, loss: 0.1153
2022-10-02 12:40:49 - train: epoch 0014, iter [01220, 01251], lr: 0.000597, loss: 0.1235
2022-10-02 12:41:17 - train: epoch 0014, iter [01230, 01251], lr: 0.000597, loss: 0.1205
2022-10-02 12:41:45 - train: epoch 0014, iter [01240, 01251], lr: 0.000597, loss: 0.1144
2022-10-02 12:42:13 - train: epoch 0014, iter [01250, 01251], lr: 0.000597, loss: 0.1241
2022-10-02 12:42:17 - train: epoch 014, train_loss: 0.1226
2022-10-02 12:42:19 - until epoch: 014, best_loss: 0.1226
2022-10-02 12:42:19 - epoch 015 lr: 0.000597
2022-10-02 12:42:54 - train: epoch 0015, iter [00010, 01251], lr: 0.000597, loss: 0.1118
2022-10-02 12:43:22 - train: epoch 0015, iter [00020, 01251], lr: 0.000597, loss: 0.1190
2022-10-02 12:43:50 - train: epoch 0015, iter [00030, 01251], lr: 0.000597, loss: 0.1159
2022-10-02 12:44:17 - train: epoch 0015, iter [00040, 01251], lr: 0.000597, loss: 0.1241
2022-10-02 12:44:45 - train: epoch 0015, iter [00050, 01251], lr: 0.000597, loss: 0.1196
2022-10-02 12:45:13 - train: epoch 0015, iter [00060, 01251], lr: 0.000597, loss: 0.1214
2022-10-02 12:45:41 - train: epoch 0015, iter [00070, 01251], lr: 0.000597, loss: 0.1276
2022-10-02 12:46:09 - train: epoch 0015, iter [00080, 01251], lr: 0.000597, loss: 0.1164
2022-10-02 12:46:37 - train: epoch 0015, iter [00090, 01251], lr: 0.000597, loss: 0.1156
2022-10-02 12:47:05 - train: epoch 0015, iter [00100, 01251], lr: 0.000597, loss: 0.1202
2022-10-02 12:47:33 - train: epoch 0015, iter [00110, 01251], lr: 0.000597, loss: 0.1259
2022-10-02 12:48:01 - train: epoch 0015, iter [00120, 01251], lr: 0.000597, loss: 0.1262
2022-10-02 12:48:28 - train: epoch 0015, iter [00130, 01251], lr: 0.000597, loss: 0.1207
2022-10-02 12:48:56 - train: epoch 0015, iter [00140, 01251], lr: 0.000597, loss: 0.1228
2022-10-02 12:49:24 - train: epoch 0015, iter [00150, 01251], lr: 0.000597, loss: 0.1189
2022-10-02 12:49:52 - train: epoch 0015, iter [00160, 01251], lr: 0.000597, loss: 0.1128
2022-10-02 12:50:20 - train: epoch 0015, iter [00170, 01251], lr: 0.000597, loss: 0.1273
2022-10-02 12:50:48 - train: epoch 0015, iter [00180, 01251], lr: 0.000597, loss: 0.1284
2022-10-02 12:51:16 - train: epoch 0015, iter [00190, 01251], lr: 0.000597, loss: 0.1153
2022-10-02 12:51:44 - train: epoch 0015, iter [00200, 01251], lr: 0.000597, loss: 0.1220
2022-10-02 12:52:12 - train: epoch 0015, iter [00210, 01251], lr: 0.000597, loss: 0.1200
2022-10-02 12:52:40 - train: epoch 0015, iter [00220, 01251], lr: 0.000597, loss: 0.1185
2022-10-02 12:53:08 - train: epoch 0015, iter [00230, 01251], lr: 0.000597, loss: 0.1113
2022-10-02 12:53:36 - train: epoch 0015, iter [00240, 01251], lr: 0.000597, loss: 0.1238
2022-10-02 12:54:04 - train: epoch 0015, iter [00250, 01251], lr: 0.000597, loss: 0.1196
2022-10-02 12:54:32 - train: epoch 0015, iter [00260, 01251], lr: 0.000597, loss: 0.1166
2022-10-02 12:55:00 - train: epoch 0015, iter [00270, 01251], lr: 0.000597, loss: 0.1177
2022-10-02 12:55:28 - train: epoch 0015, iter [00280, 01251], lr: 0.000597, loss: 0.1224
2022-10-02 12:55:56 - train: epoch 0015, iter [00290, 01251], lr: 0.000597, loss: 0.1164
2022-10-02 12:56:24 - train: epoch 0015, iter [00300, 01251], lr: 0.000597, loss: 0.1239
2022-10-02 12:56:52 - train: epoch 0015, iter [00310, 01251], lr: 0.000597, loss: 0.1188
2022-10-02 12:57:20 - train: epoch 0015, iter [00320, 01251], lr: 0.000597, loss: 0.1155
2022-10-02 12:57:48 - train: epoch 0015, iter [00330, 01251], lr: 0.000597, loss: 0.1214
2022-10-02 12:58:16 - train: epoch 0015, iter [00340, 01251], lr: 0.000597, loss: 0.1217
2022-10-02 12:58:44 - train: epoch 0015, iter [00350, 01251], lr: 0.000597, loss: 0.1157
2022-10-02 12:59:12 - train: epoch 0015, iter [00360, 01251], lr: 0.000597, loss: 0.1254
2022-10-02 12:59:40 - train: epoch 0015, iter [00370, 01251], lr: 0.000597, loss: 0.1289
2022-10-02 13:00:08 - train: epoch 0015, iter [00380, 01251], lr: 0.000597, loss: 0.1219
2022-10-02 13:00:36 - train: epoch 0015, iter [00390, 01251], lr: 0.000597, loss: 0.1183
2022-10-02 13:01:04 - train: epoch 0015, iter [00400, 01251], lr: 0.000597, loss: 0.1253
2022-10-02 13:01:32 - train: epoch 0015, iter [00410, 01251], lr: 0.000597, loss: 0.1153
2022-10-02 13:01:59 - train: epoch 0015, iter [00420, 01251], lr: 0.000597, loss: 0.1237
2022-10-02 13:02:27 - train: epoch 0015, iter [00430, 01251], lr: 0.000597, loss: 0.1201
2022-10-02 13:02:56 - train: epoch 0015, iter [00440, 01251], lr: 0.000597, loss: 0.1258
2022-10-02 13:03:23 - train: epoch 0015, iter [00450, 01251], lr: 0.000597, loss: 0.1191
2022-10-02 13:03:51 - train: epoch 0015, iter [00460, 01251], lr: 0.000597, loss: 0.1150
2022-10-02 13:04:19 - train: epoch 0015, iter [00470, 01251], lr: 0.000597, loss: 0.1243
2022-10-02 13:04:47 - train: epoch 0015, iter [00480, 01251], lr: 0.000596, loss: 0.1072
2022-10-02 13:05:15 - train: epoch 0015, iter [00490, 01251], lr: 0.000596, loss: 0.1206
2022-10-02 13:05:43 - train: epoch 0015, iter [00500, 01251], lr: 0.000596, loss: 0.1163
2022-10-02 13:06:11 - train: epoch 0015, iter [00510, 01251], lr: 0.000596, loss: 0.1160
2022-10-02 13:06:39 - train: epoch 0015, iter [00520, 01251], lr: 0.000596, loss: 0.1203
2022-10-02 13:07:07 - train: epoch 0015, iter [00530, 01251], lr: 0.000596, loss: 0.1258
2022-10-02 13:07:35 - train: epoch 0015, iter [00540, 01251], lr: 0.000596, loss: 0.1139
2022-10-02 13:08:03 - train: epoch 0015, iter [00550, 01251], lr: 0.000596, loss: 0.1114
2022-10-02 13:08:31 - train: epoch 0015, iter [00560, 01251], lr: 0.000596, loss: 0.1202
2022-10-02 13:08:59 - train: epoch 0015, iter [00570, 01251], lr: 0.000596, loss: 0.1157
2022-10-02 13:09:27 - train: epoch 0015, iter [00580, 01251], lr: 0.000596, loss: 0.1241
2022-10-02 13:09:55 - train: epoch 0015, iter [00590, 01251], lr: 0.000596, loss: 0.1117
2022-10-02 13:10:23 - train: epoch 0015, iter [00600, 01251], lr: 0.000596, loss: 0.1121
2022-10-02 13:10:51 - train: epoch 0015, iter [00610, 01251], lr: 0.000596, loss: 0.1235
2022-10-02 13:11:19 - train: epoch 0015, iter [00620, 01251], lr: 0.000596, loss: 0.1267
2022-10-02 13:11:46 - train: epoch 0015, iter [00630, 01251], lr: 0.000596, loss: 0.1119
2022-10-02 13:12:15 - train: epoch 0015, iter [00640, 01251], lr: 0.000596, loss: 0.1189
2022-10-02 13:12:42 - train: epoch 0015, iter [00650, 01251], lr: 0.000596, loss: 0.1145
2022-10-02 13:13:10 - train: epoch 0015, iter [00660, 01251], lr: 0.000596, loss: 0.1197
2022-10-02 13:13:38 - train: epoch 0015, iter [00670, 01251], lr: 0.000596, loss: 0.1130
2022-10-02 13:14:06 - train: epoch 0015, iter [00680, 01251], lr: 0.000596, loss: 0.1208
2022-10-02 13:14:34 - train: epoch 0015, iter [00690, 01251], lr: 0.000596, loss: 0.1198
2022-10-02 13:15:02 - train: epoch 0015, iter [00700, 01251], lr: 0.000596, loss: 0.1177
2022-10-02 13:15:30 - train: epoch 0015, iter [00710, 01251], lr: 0.000596, loss: 0.1226
2022-10-02 13:15:58 - train: epoch 0015, iter [00720, 01251], lr: 0.000596, loss: 0.1113
2022-10-02 13:16:26 - train: epoch 0015, iter [00730, 01251], lr: 0.000596, loss: 0.1210
2022-10-02 13:16:54 - train: epoch 0015, iter [00740, 01251], lr: 0.000596, loss: 0.1132
2022-10-02 13:17:22 - train: epoch 0015, iter [00750, 01251], lr: 0.000596, loss: 0.1249
2022-10-02 13:17:50 - train: epoch 0015, iter [00760, 01251], lr: 0.000596, loss: 0.1223
2022-10-02 13:18:18 - train: epoch 0015, iter [00770, 01251], lr: 0.000596, loss: 0.1175
2022-10-02 13:18:46 - train: epoch 0015, iter [00780, 01251], lr: 0.000596, loss: 0.1162
2022-10-02 13:19:14 - train: epoch 0015, iter [00790, 01251], lr: 0.000596, loss: 0.1218
2022-10-02 13:19:42 - train: epoch 0015, iter [00800, 01251], lr: 0.000596, loss: 0.1169
2022-10-02 13:20:10 - train: epoch 0015, iter [00810, 01251], lr: 0.000596, loss: 0.1192
2022-10-02 13:20:38 - train: epoch 0015, iter [00820, 01251], lr: 0.000596, loss: 0.1188
2022-10-02 13:21:06 - train: epoch 0015, iter [00830, 01251], lr: 0.000596, loss: 0.1171
2022-10-02 13:21:34 - train: epoch 0015, iter [00840, 01251], lr: 0.000596, loss: 0.1172
2022-10-02 13:22:02 - train: epoch 0015, iter [00850, 01251], lr: 0.000596, loss: 0.1185
2022-10-02 13:22:30 - train: epoch 0015, iter [00860, 01251], lr: 0.000596, loss: 0.1130
2022-10-02 13:22:58 - train: epoch 0015, iter [00870, 01251], lr: 0.000596, loss: 0.1203
2022-10-02 13:23:26 - train: epoch 0015, iter [00880, 01251], lr: 0.000596, loss: 0.1185
2022-10-02 13:23:54 - train: epoch 0015, iter [00890, 01251], lr: 0.000596, loss: 0.1100
2022-10-02 13:24:22 - train: epoch 0015, iter [00900, 01251], lr: 0.000596, loss: 0.1179
2022-10-02 13:24:50 - train: epoch 0015, iter [00910, 01251], lr: 0.000596, loss: 0.1149
2022-10-02 13:25:19 - train: epoch 0015, iter [00920, 01251], lr: 0.000596, loss: 0.1021
2022-10-02 13:25:47 - train: epoch 0015, iter [00930, 01251], lr: 0.000596, loss: 0.1078
2022-10-02 13:26:15 - train: epoch 0015, iter [00940, 01251], lr: 0.000596, loss: 0.1211
2022-10-02 13:26:43 - train: epoch 0015, iter [00950, 01251], lr: 0.000596, loss: 0.1185
2022-10-02 13:27:11 - train: epoch 0015, iter [00960, 01251], lr: 0.000596, loss: 0.1239
2022-10-02 13:27:39 - train: epoch 0015, iter [00970, 01251], lr: 0.000596, loss: 0.1097
2022-10-02 13:28:06 - train: epoch 0015, iter [00980, 01251], lr: 0.000596, loss: 0.1122
2022-10-02 13:28:35 - train: epoch 0015, iter [00990, 01251], lr: 0.000596, loss: 0.1144
2022-10-02 13:29:02 - train: epoch 0015, iter [01000, 01251], lr: 0.000596, loss: 0.1163
2022-10-02 13:29:30 - train: epoch 0015, iter [01010, 01251], lr: 0.000596, loss: 0.1092
2022-10-02 13:29:58 - train: epoch 0015, iter [01020, 01251], lr: 0.000596, loss: 0.1151
2022-10-02 13:30:26 - train: epoch 0015, iter [01030, 01251], lr: 0.000596, loss: 0.1232
2022-10-02 13:30:54 - train: epoch 0015, iter [01040, 01251], lr: 0.000596, loss: 0.1168
2022-10-02 13:31:22 - train: epoch 0015, iter [01050, 01251], lr: 0.000596, loss: 0.1277
2022-10-02 13:31:50 - train: epoch 0015, iter [01060, 01251], lr: 0.000596, loss: 0.1242
2022-10-02 13:32:18 - train: epoch 0015, iter [01070, 01251], lr: 0.000596, loss: 0.1201
2022-10-02 13:32:46 - train: epoch 0015, iter [01080, 01251], lr: 0.000596, loss: 0.1224
2022-10-02 13:33:14 - train: epoch 0015, iter [01090, 01251], lr: 0.000596, loss: 0.1136
2022-10-02 13:33:42 - train: epoch 0015, iter [01100, 01251], lr: 0.000596, loss: 0.1241
2022-10-02 13:34:10 - train: epoch 0015, iter [01110, 01251], lr: 0.000596, loss: 0.1230
2022-10-02 13:34:38 - train: epoch 0015, iter [01120, 01251], lr: 0.000596, loss: 0.1165
2022-10-02 13:35:06 - train: epoch 0015, iter [01130, 01251], lr: 0.000596, loss: 0.1155
2022-10-02 13:35:33 - train: epoch 0015, iter [01140, 01251], lr: 0.000596, loss: 0.1174
2022-10-02 13:36:01 - train: epoch 0015, iter [01150, 01251], lr: 0.000596, loss: 0.1178
2022-10-02 13:36:30 - train: epoch 0015, iter [01160, 01251], lr: 0.000596, loss: 0.1164
2022-10-02 13:36:58 - train: epoch 0015, iter [01170, 01251], lr: 0.000596, loss: 0.1255
2022-10-02 13:37:26 - train: epoch 0015, iter [01180, 01251], lr: 0.000596, loss: 0.1138
2022-10-02 13:37:54 - train: epoch 0015, iter [01190, 01251], lr: 0.000596, loss: 0.1310
2022-10-02 13:38:22 - train: epoch 0015, iter [01200, 01251], lr: 0.000596, loss: 0.1174
2022-10-02 13:38:50 - train: epoch 0015, iter [01210, 01251], lr: 0.000596, loss: 0.1208
2022-10-02 13:39:18 - train: epoch 0015, iter [01220, 01251], lr: 0.000595, loss: 0.1140
2022-10-02 13:39:46 - train: epoch 0015, iter [01230, 01251], lr: 0.000595, loss: 0.1162
2022-10-02 13:40:14 - train: epoch 0015, iter [01240, 01251], lr: 0.000595, loss: 0.1200
2022-10-02 13:40:41 - train: epoch 0015, iter [01250, 01251], lr: 0.000595, loss: 0.1238
2022-10-02 13:40:46 - train: epoch 015, train_loss: 0.1184
2022-10-02 13:40:48 - until epoch: 015, best_loss: 0.1184
2022-10-02 13:40:48 - epoch 016 lr: 0.000595
2022-10-02 13:41:23 - train: epoch 0016, iter [00010, 01251], lr: 0.000595, loss: 0.1258
2022-10-02 13:41:51 - train: epoch 0016, iter [00020, 01251], lr: 0.000595, loss: 0.1216
2022-10-02 13:42:19 - train: epoch 0016, iter [00030, 01251], lr: 0.000595, loss: 0.1283
2022-10-02 13:42:47 - train: epoch 0016, iter [00040, 01251], lr: 0.000595, loss: 0.1146
2022-10-02 13:43:15 - train: epoch 0016, iter [00050, 01251], lr: 0.000595, loss: 0.1176
2022-10-02 13:43:43 - train: epoch 0016, iter [00060, 01251], lr: 0.000595, loss: 0.1196
2022-10-02 13:44:11 - train: epoch 0016, iter [00070, 01251], lr: 0.000595, loss: 0.1139
2022-10-02 13:44:39 - train: epoch 0016, iter [00080, 01251], lr: 0.000595, loss: 0.1143
2022-10-02 13:45:07 - train: epoch 0016, iter [00090, 01251], lr: 0.000595, loss: 0.1181
2022-10-02 13:45:35 - train: epoch 0016, iter [00100, 01251], lr: 0.000595, loss: 0.1110
2022-10-02 13:46:03 - train: epoch 0016, iter [00110, 01251], lr: 0.000595, loss: 0.1171
2022-10-02 13:46:31 - train: epoch 0016, iter [00120, 01251], lr: 0.000595, loss: 0.1182
2022-10-02 13:46:58 - train: epoch 0016, iter [00130, 01251], lr: 0.000595, loss: 0.1133
2022-10-02 13:47:26 - train: epoch 0016, iter [00140, 01251], lr: 0.000595, loss: 0.1171
2022-10-02 13:47:54 - train: epoch 0016, iter [00150, 01251], lr: 0.000595, loss: 0.1175
2022-10-02 13:48:22 - train: epoch 0016, iter [00160, 01251], lr: 0.000595, loss: 0.1319
2022-10-02 13:48:50 - train: epoch 0016, iter [00170, 01251], lr: 0.000595, loss: 0.1205
2022-10-02 13:49:18 - train: epoch 0016, iter [00180, 01251], lr: 0.000595, loss: 0.1167
2022-10-02 13:49:46 - train: epoch 0016, iter [00190, 01251], lr: 0.000595, loss: 0.1175
2022-10-02 13:50:14 - train: epoch 0016, iter [00200, 01251], lr: 0.000595, loss: 0.1181
2022-10-02 13:50:42 - train: epoch 0016, iter [00210, 01251], lr: 0.000595, loss: 0.1168
2022-10-02 13:51:10 - train: epoch 0016, iter [00220, 01251], lr: 0.000595, loss: 0.1207
2022-10-02 13:51:38 - train: epoch 0016, iter [00230, 01251], lr: 0.000595, loss: 0.1124
2022-10-02 13:52:06 - train: epoch 0016, iter [00240, 01251], lr: 0.000595, loss: 0.1199
2022-10-02 13:52:34 - train: epoch 0016, iter [00250, 01251], lr: 0.000595, loss: 0.1138
2022-10-02 13:53:02 - train: epoch 0016, iter [00260, 01251], lr: 0.000595, loss: 0.1129
2022-10-02 13:53:29 - train: epoch 0016, iter [00270, 01251], lr: 0.000595, loss: 0.1221
2022-10-02 13:53:57 - train: epoch 0016, iter [00280, 01251], lr: 0.000595, loss: 0.1121
2022-10-02 13:54:25 - train: epoch 0016, iter [00290, 01251], lr: 0.000595, loss: 0.1181
2022-10-02 13:54:53 - train: epoch 0016, iter [00300, 01251], lr: 0.000595, loss: 0.1150
2022-10-02 13:55:21 - train: epoch 0016, iter [00310, 01251], lr: 0.000595, loss: 0.1270
2022-10-02 13:55:49 - train: epoch 0016, iter [00320, 01251], lr: 0.000595, loss: 0.1168
2022-10-02 13:56:17 - train: epoch 0016, iter [00330, 01251], lr: 0.000595, loss: 0.1197
2022-10-02 13:56:45 - train: epoch 0016, iter [00340, 01251], lr: 0.000595, loss: 0.1147
2022-10-02 13:57:13 - train: epoch 0016, iter [00350, 01251], lr: 0.000595, loss: 0.1143
2022-10-02 13:57:41 - train: epoch 0016, iter [00360, 01251], lr: 0.000595, loss: 0.1232
2022-10-02 13:58:09 - train: epoch 0016, iter [00370, 01251], lr: 0.000595, loss: 0.1163
2022-10-02 13:58:37 - train: epoch 0016, iter [00380, 01251], lr: 0.000595, loss: 0.1157
2022-10-02 13:59:05 - train: epoch 0016, iter [00390, 01251], lr: 0.000595, loss: 0.1272
2022-10-02 13:59:33 - train: epoch 0016, iter [00400, 01251], lr: 0.000595, loss: 0.1089
2022-10-02 14:00:01 - train: epoch 0016, iter [00410, 01251], lr: 0.000595, loss: 0.1207
2022-10-02 14:00:29 - train: epoch 0016, iter [00420, 01251], lr: 0.000595, loss: 0.1098
2022-10-02 14:00:57 - train: epoch 0016, iter [00430, 01251], lr: 0.000595, loss: 0.1153
2022-10-02 14:01:25 - train: epoch 0016, iter [00440, 01251], lr: 0.000595, loss: 0.1185
2022-10-02 14:01:53 - train: epoch 0016, iter [00450, 01251], lr: 0.000595, loss: 0.1107
2022-10-02 14:02:21 - train: epoch 0016, iter [00460, 01251], lr: 0.000595, loss: 0.1061
2022-10-02 14:02:48 - train: epoch 0016, iter [00470, 01251], lr: 0.000595, loss: 0.1119
2022-10-02 14:03:16 - train: epoch 0016, iter [00480, 01251], lr: 0.000595, loss: 0.1255
2022-10-02 14:03:44 - train: epoch 0016, iter [00490, 01251], lr: 0.000595, loss: 0.1102
2022-10-02 14:04:12 - train: epoch 0016, iter [00500, 01251], lr: 0.000595, loss: 0.1121
2022-10-02 14:04:41 - train: epoch 0016, iter [00510, 01251], lr: 0.000595, loss: 0.1229
2022-10-02 14:05:09 - train: epoch 0016, iter [00520, 01251], lr: 0.000595, loss: 0.1203
2022-10-02 14:05:37 - train: epoch 0016, iter [00530, 01251], lr: 0.000595, loss: 0.1029
2022-10-02 14:06:05 - train: epoch 0016, iter [00540, 01251], lr: 0.000595, loss: 0.1205
2022-10-02 14:06:33 - train: epoch 0016, iter [00550, 01251], lr: 0.000595, loss: 0.1134
2022-10-02 14:07:01 - train: epoch 0016, iter [00560, 01251], lr: 0.000595, loss: 0.1016
2022-10-02 14:07:29 - train: epoch 0016, iter [00570, 01251], lr: 0.000595, loss: 0.1142
2022-10-02 14:07:58 - train: epoch 0016, iter [00580, 01251], lr: 0.000595, loss: 0.1117
2022-10-02 14:08:26 - train: epoch 0016, iter [00590, 01251], lr: 0.000595, loss: 0.1213
2022-10-02 14:08:54 - train: epoch 0016, iter [00600, 01251], lr: 0.000595, loss: 0.1125
2022-10-02 14:09:22 - train: epoch 0016, iter [00610, 01251], lr: 0.000595, loss: 0.1140
2022-10-02 14:09:50 - train: epoch 0016, iter [00620, 01251], lr: 0.000594, loss: 0.1129
2022-10-02 14:10:18 - train: epoch 0016, iter [00630, 01251], lr: 0.000594, loss: 0.1172
2022-10-02 14:10:47 - train: epoch 0016, iter [00640, 01251], lr: 0.000594, loss: 0.1146
2022-10-02 14:11:15 - train: epoch 0016, iter [00650, 01251], lr: 0.000594, loss: 0.1140
2022-10-02 14:11:42 - train: epoch 0016, iter [00660, 01251], lr: 0.000594, loss: 0.1093
2022-10-02 14:12:10 - train: epoch 0016, iter [00670, 01251], lr: 0.000594, loss: 0.1097
2022-10-02 14:12:38 - train: epoch 0016, iter [00680, 01251], lr: 0.000594, loss: 0.1224
2022-10-02 14:13:06 - train: epoch 0016, iter [00690, 01251], lr: 0.000594, loss: 0.1169
2022-10-02 14:13:34 - train: epoch 0016, iter [00700, 01251], lr: 0.000594, loss: 0.1078
2022-10-02 14:14:02 - train: epoch 0016, iter [00710, 01251], lr: 0.000594, loss: 0.1161
2022-10-02 14:14:30 - train: epoch 0016, iter [00720, 01251], lr: 0.000594, loss: 0.1192
2022-10-02 14:14:58 - train: epoch 0016, iter [00730, 01251], lr: 0.000594, loss: 0.1109
2022-10-02 14:15:26 - train: epoch 0016, iter [00740, 01251], lr: 0.000594, loss: 0.1154
2022-10-02 14:15:54 - train: epoch 0016, iter [00750, 01251], lr: 0.000594, loss: 0.1165
2022-10-02 14:16:23 - train: epoch 0016, iter [00760, 01251], lr: 0.000594, loss: 0.1124
2022-10-02 14:16:51 - train: epoch 0016, iter [00770, 01251], lr: 0.000594, loss: 0.1172
2022-10-02 14:17:19 - train: epoch 0016, iter [00780, 01251], lr: 0.000594, loss: 0.1176
2022-10-02 14:17:47 - train: epoch 0016, iter [00790, 01251], lr: 0.000594, loss: 0.1076
2022-10-02 14:18:15 - train: epoch 0016, iter [00800, 01251], lr: 0.000594, loss: 0.1164
2022-10-02 14:18:43 - train: epoch 0016, iter [00810, 01251], lr: 0.000594, loss: 0.1136
2022-10-02 14:19:11 - train: epoch 0016, iter [00820, 01251], lr: 0.000594, loss: 0.1153
2022-10-02 14:19:39 - train: epoch 0016, iter [00830, 01251], lr: 0.000594, loss: 0.1172
2022-10-02 14:20:07 - train: epoch 0016, iter [00840, 01251], lr: 0.000594, loss: 0.1170
2022-10-02 14:20:35 - train: epoch 0016, iter [00850, 01251], lr: 0.000594, loss: 0.1136
2022-10-02 14:21:03 - train: epoch 0016, iter [00860, 01251], lr: 0.000594, loss: 0.1273
2022-10-02 14:21:31 - train: epoch 0016, iter [00870, 01251], lr: 0.000594, loss: 0.1137
2022-10-02 14:21:59 - train: epoch 0016, iter [00880, 01251], lr: 0.000594, loss: 0.1143
2022-10-02 14:22:27 - train: epoch 0016, iter [00890, 01251], lr: 0.000594, loss: 0.1160
2022-10-02 14:22:55 - train: epoch 0016, iter [00900, 01251], lr: 0.000594, loss: 0.1100
2022-10-02 14:23:23 - train: epoch 0016, iter [00910, 01251], lr: 0.000594, loss: 0.1107
2022-10-02 14:23:51 - train: epoch 0016, iter [00920, 01251], lr: 0.000594, loss: 0.1155
2022-10-02 14:24:19 - train: epoch 0016, iter [00930, 01251], lr: 0.000594, loss: 0.1041
2022-10-02 14:24:47 - train: epoch 0016, iter [00940, 01251], lr: 0.000594, loss: 0.1044
2022-10-02 14:25:15 - train: epoch 0016, iter [00950, 01251], lr: 0.000594, loss: 0.1135
2022-10-02 14:25:43 - train: epoch 0016, iter [00960, 01251], lr: 0.000594, loss: 0.1275
2022-10-02 14:26:11 - train: epoch 0016, iter [00970, 01251], lr: 0.000594, loss: 0.1200
2022-10-02 14:26:39 - train: epoch 0016, iter [00980, 01251], lr: 0.000594, loss: 0.1108
2022-10-02 14:27:07 - train: epoch 0016, iter [00990, 01251], lr: 0.000594, loss: 0.1030
2022-10-02 14:27:36 - train: epoch 0016, iter [01000, 01251], lr: 0.000594, loss: 0.1207
2022-10-02 14:28:04 - train: epoch 0016, iter [01010, 01251], lr: 0.000594, loss: 0.1193
2022-10-02 14:28:32 - train: epoch 0016, iter [01020, 01251], lr: 0.000594, loss: 0.1131
2022-10-02 14:29:00 - train: epoch 0016, iter [01030, 01251], lr: 0.000594, loss: 0.1142
2022-10-02 14:29:28 - train: epoch 0016, iter [01040, 01251], lr: 0.000594, loss: 0.1159
2022-10-02 14:29:56 - train: epoch 0016, iter [01050, 01251], lr: 0.000594, loss: 0.1137
2022-10-02 14:30:24 - train: epoch 0016, iter [01060, 01251], lr: 0.000594, loss: 0.1173
2022-10-02 14:30:51 - train: epoch 0016, iter [01070, 01251], lr: 0.000594, loss: 0.1173
2022-10-02 14:31:19 - train: epoch 0016, iter [01080, 01251], lr: 0.000594, loss: 0.1123
2022-10-02 14:31:48 - train: epoch 0016, iter [01090, 01251], lr: 0.000594, loss: 0.1160
2022-10-02 14:32:16 - train: epoch 0016, iter [01100, 01251], lr: 0.000594, loss: 0.1135
2022-10-02 14:32:44 - train: epoch 0016, iter [01110, 01251], lr: 0.000594, loss: 0.1143
2022-10-02 14:33:12 - train: epoch 0016, iter [01120, 01251], lr: 0.000594, loss: 0.1081
2022-10-02 14:33:40 - train: epoch 0016, iter [01130, 01251], lr: 0.000594, loss: 0.1165
2022-10-02 14:34:08 - train: epoch 0016, iter [01140, 01251], lr: 0.000594, loss: 0.1073
2022-10-02 14:34:36 - train: epoch 0016, iter [01150, 01251], lr: 0.000594, loss: 0.1107
2022-10-02 14:35:04 - train: epoch 0016, iter [01160, 01251], lr: 0.000594, loss: 0.1155
2022-10-02 14:35:32 - train: epoch 0016, iter [01170, 01251], lr: 0.000594, loss: 0.1148
2022-10-02 14:36:00 - train: epoch 0016, iter [01180, 01251], lr: 0.000594, loss: 0.1103
2022-10-02 14:36:28 - train: epoch 0016, iter [01190, 01251], lr: 0.000594, loss: 0.1092
2022-10-02 14:36:56 - train: epoch 0016, iter [01200, 01251], lr: 0.000594, loss: 0.1138
2022-10-02 14:37:24 - train: epoch 0016, iter [01210, 01251], lr: 0.000594, loss: 0.1081
2022-10-02 14:37:52 - train: epoch 0016, iter [01220, 01251], lr: 0.000593, loss: 0.1202
2022-10-02 14:38:20 - train: epoch 0016, iter [01230, 01251], lr: 0.000593, loss: 0.1166
2022-10-02 14:38:48 - train: epoch 0016, iter [01240, 01251], lr: 0.000593, loss: 0.1230
2022-10-02 14:39:16 - train: epoch 0016, iter [01250, 01251], lr: 0.000593, loss: 0.1190
2022-10-02 14:39:20 - train: epoch 016, train_loss: 0.1153
2022-10-02 14:39:22 - until epoch: 016, best_loss: 0.1153
2022-10-02 14:39:22 - epoch 017 lr: 0.000593
2022-10-02 14:39:57 - train: epoch 0017, iter [00010, 01251], lr: 0.000593, loss: 0.1101
2022-10-02 14:40:25 - train: epoch 0017, iter [00020, 01251], lr: 0.000593, loss: 0.1112
2022-10-02 14:40:53 - train: epoch 0017, iter [00030, 01251], lr: 0.000593, loss: 0.1179
2022-10-02 14:41:21 - train: epoch 0017, iter [00040, 01251], lr: 0.000593, loss: 0.1083
2022-10-02 14:41:49 - train: epoch 0017, iter [00050, 01251], lr: 0.000593, loss: 0.1115
2022-10-02 14:42:17 - train: epoch 0017, iter [00060, 01251], lr: 0.000593, loss: 0.1162
2022-10-02 14:42:45 - train: epoch 0017, iter [00070, 01251], lr: 0.000593, loss: 0.1091
2022-10-02 14:43:13 - train: epoch 0017, iter [00080, 01251], lr: 0.000593, loss: 0.1111
2022-10-02 14:43:41 - train: epoch 0017, iter [00090, 01251], lr: 0.000593, loss: 0.1142
2022-10-02 14:44:09 - train: epoch 0017, iter [00100, 01251], lr: 0.000593, loss: 0.1112
2022-10-02 14:44:37 - train: epoch 0017, iter [00110, 01251], lr: 0.000593, loss: 0.1102
2022-10-02 14:45:05 - train: epoch 0017, iter [00120, 01251], lr: 0.000593, loss: 0.1166
2022-10-02 14:45:33 - train: epoch 0017, iter [00130, 01251], lr: 0.000593, loss: 0.1085
2022-10-02 14:46:01 - train: epoch 0017, iter [00140, 01251], lr: 0.000593, loss: 0.1147
2022-10-02 14:46:29 - train: epoch 0017, iter [00150, 01251], lr: 0.000593, loss: 0.1110
2022-10-02 14:46:57 - train: epoch 0017, iter [00160, 01251], lr: 0.000593, loss: 0.1072
2022-10-02 14:47:25 - train: epoch 0017, iter [00170, 01251], lr: 0.000593, loss: 0.1050
2022-10-02 14:47:52 - train: epoch 0017, iter [00180, 01251], lr: 0.000593, loss: 0.1154
2022-10-02 14:48:20 - train: epoch 0017, iter [00190, 01251], lr: 0.000593, loss: 0.1141
2022-10-02 14:48:48 - train: epoch 0017, iter [00200, 01251], lr: 0.000593, loss: 0.1122
2022-10-02 14:49:16 - train: epoch 0017, iter [00210, 01251], lr: 0.000593, loss: 0.1097
2022-10-02 14:49:44 - train: epoch 0017, iter [00220, 01251], lr: 0.000593, loss: 0.1206
2022-10-02 14:50:12 - train: epoch 0017, iter [00230, 01251], lr: 0.000593, loss: 0.1101
2022-10-02 14:50:40 - train: epoch 0017, iter [00240, 01251], lr: 0.000593, loss: 0.1050
2022-10-02 14:51:08 - train: epoch 0017, iter [00250, 01251], lr: 0.000593, loss: 0.1162
2022-10-02 14:51:35 - train: epoch 0017, iter [00260, 01251], lr: 0.000593, loss: 0.1077
2022-10-02 14:52:03 - train: epoch 0017, iter [00270, 01251], lr: 0.000593, loss: 0.1075
2022-10-02 14:52:31 - train: epoch 0017, iter [00280, 01251], lr: 0.000593, loss: 0.1166
2022-10-02 14:52:59 - train: epoch 0017, iter [00290, 01251], lr: 0.000593, loss: 0.1104
2022-10-02 14:53:27 - train: epoch 0017, iter [00300, 01251], lr: 0.000593, loss: 0.1062
2022-10-02 14:53:55 - train: epoch 0017, iter [00310, 01251], lr: 0.000593, loss: 0.1136
2022-10-02 14:54:23 - train: epoch 0017, iter [00320, 01251], lr: 0.000593, loss: 0.1130
2022-10-02 14:54:51 - train: epoch 0017, iter [00330, 01251], lr: 0.000593, loss: 0.1164
2022-10-02 14:55:19 - train: epoch 0017, iter [00340, 01251], lr: 0.000593, loss: 0.1070
2022-10-02 14:55:47 - train: epoch 0017, iter [00350, 01251], lr: 0.000593, loss: 0.1151
2022-10-02 14:56:15 - train: epoch 0017, iter [00360, 01251], lr: 0.000593, loss: 0.1153
2022-10-02 14:56:42 - train: epoch 0017, iter [00370, 01251], lr: 0.000593, loss: 0.1075
2022-10-02 14:57:10 - train: epoch 0017, iter [00380, 01251], lr: 0.000593, loss: 0.1207
2022-10-02 14:57:38 - train: epoch 0017, iter [00390, 01251], lr: 0.000593, loss: 0.1208
2022-10-02 14:58:06 - train: epoch 0017, iter [00400, 01251], lr: 0.000593, loss: 0.1123
2022-10-02 14:58:34 - train: epoch 0017, iter [00410, 01251], lr: 0.000593, loss: 0.1083
2022-10-02 14:59:02 - train: epoch 0017, iter [00420, 01251], lr: 0.000593, loss: 0.1178
2022-10-02 14:59:30 - train: epoch 0017, iter [00430, 01251], lr: 0.000593, loss: 0.1160
2022-10-02 14:59:58 - train: epoch 0017, iter [00440, 01251], lr: 0.000593, loss: 0.1154
2022-10-02 15:00:26 - train: epoch 0017, iter [00450, 01251], lr: 0.000593, loss: 0.1180
2022-10-02 15:00:54 - train: epoch 0017, iter [00460, 01251], lr: 0.000593, loss: 0.1061
2022-10-02 15:01:21 - train: epoch 0017, iter [00470, 01251], lr: 0.000593, loss: 0.1117
2022-10-02 15:01:49 - train: epoch 0017, iter [00480, 01251], lr: 0.000593, loss: 0.1132
2022-10-02 15:02:17 - train: epoch 0017, iter [00490, 01251], lr: 0.000593, loss: 0.1079
2022-10-02 15:02:45 - train: epoch 0017, iter [00500, 01251], lr: 0.000593, loss: 0.1185
2022-10-02 15:03:13 - train: epoch 0017, iter [00510, 01251], lr: 0.000593, loss: 0.1085
2022-10-02 15:03:41 - train: epoch 0017, iter [00520, 01251], lr: 0.000593, loss: 0.1089
2022-10-02 15:04:09 - train: epoch 0017, iter [00530, 01251], lr: 0.000592, loss: 0.1029
2022-10-02 15:04:36 - train: epoch 0017, iter [00540, 01251], lr: 0.000592, loss: 0.1078
2022-10-02 15:05:04 - train: epoch 0017, iter [00550, 01251], lr: 0.000592, loss: 0.1174
2022-10-02 15:05:32 - train: epoch 0017, iter [00560, 01251], lr: 0.000592, loss: 0.1150
2022-10-02 15:06:00 - train: epoch 0017, iter [00570, 01251], lr: 0.000592, loss: 0.1132
2022-10-02 15:06:28 - train: epoch 0017, iter [00580, 01251], lr: 0.000592, loss: 0.1125
2022-10-02 15:06:56 - train: epoch 0017, iter [00590, 01251], lr: 0.000592, loss: 0.1133
2022-10-02 15:07:24 - train: epoch 0017, iter [00600, 01251], lr: 0.000592, loss: 0.1238
2022-10-02 15:07:52 - train: epoch 0017, iter [00610, 01251], lr: 0.000592, loss: 0.1190
2022-10-02 15:08:19 - train: epoch 0017, iter [00620, 01251], lr: 0.000592, loss: 0.1142
2022-10-02 15:08:47 - train: epoch 0017, iter [00630, 01251], lr: 0.000592, loss: 0.1174
2022-10-02 15:09:15 - train: epoch 0017, iter [00640, 01251], lr: 0.000592, loss: 0.1075
2022-10-02 15:09:43 - train: epoch 0017, iter [00650, 01251], lr: 0.000592, loss: 0.1165
2022-10-02 15:10:11 - train: epoch 0017, iter [00660, 01251], lr: 0.000592, loss: 0.1086
2022-10-02 15:10:39 - train: epoch 0017, iter [00670, 01251], lr: 0.000592, loss: 0.1088
2022-10-02 15:11:06 - train: epoch 0017, iter [00680, 01251], lr: 0.000592, loss: 0.1174
2022-10-02 15:11:34 - train: epoch 0017, iter [00690, 01251], lr: 0.000592, loss: 0.1154
2022-10-02 15:12:02 - train: epoch 0017, iter [00700, 01251], lr: 0.000592, loss: 0.1183
2022-10-02 15:12:30 - train: epoch 0017, iter [00710, 01251], lr: 0.000592, loss: 0.1128
2022-10-02 15:12:58 - train: epoch 0017, iter [00720, 01251], lr: 0.000592, loss: 0.1127
2022-10-02 15:13:26 - train: epoch 0017, iter [00730, 01251], lr: 0.000592, loss: 0.1159
2022-10-02 15:13:54 - train: epoch 0017, iter [00740, 01251], lr: 0.000592, loss: 0.1053
2022-10-02 15:14:22 - train: epoch 0017, iter [00750, 01251], lr: 0.000592, loss: 0.1070
2022-10-02 15:14:50 - train: epoch 0017, iter [00760, 01251], lr: 0.000592, loss: 0.1117
2022-10-02 15:15:18 - train: epoch 0017, iter [00770, 01251], lr: 0.000592, loss: 0.1124
2022-10-02 15:15:46 - train: epoch 0017, iter [00780, 01251], lr: 0.000592, loss: 0.1136
2022-10-02 15:16:14 - train: epoch 0017, iter [00790, 01251], lr: 0.000592, loss: 0.1132
2022-10-02 15:16:42 - train: epoch 0017, iter [00800, 01251], lr: 0.000592, loss: 0.1193
2022-10-02 15:17:10 - train: epoch 0017, iter [00810, 01251], lr: 0.000592, loss: 0.1113
2022-10-02 15:17:38 - train: epoch 0017, iter [00820, 01251], lr: 0.000592, loss: 0.1188
2022-10-02 15:18:06 - train: epoch 0017, iter [00830, 01251], lr: 0.000592, loss: 0.1154
2022-10-02 15:18:34 - train: epoch 0017, iter [00840, 01251], lr: 0.000592, loss: 0.0965
2022-10-02 15:19:02 - train: epoch 0017, iter [00850, 01251], lr: 0.000592, loss: 0.1067
2022-10-02 15:19:30 - train: epoch 0017, iter [00860, 01251], lr: 0.000592, loss: 0.1091
2022-10-02 15:19:58 - train: epoch 0017, iter [00870, 01251], lr: 0.000592, loss: 0.1110
2022-10-02 15:20:26 - train: epoch 0017, iter [00880, 01251], lr: 0.000592, loss: 0.1059
2022-10-02 15:20:54 - train: epoch 0017, iter [00890, 01251], lr: 0.000592, loss: 0.1064
2022-10-02 15:21:22 - train: epoch 0017, iter [00900, 01251], lr: 0.000592, loss: 0.1130
2022-10-02 15:21:50 - train: epoch 0017, iter [00910, 01251], lr: 0.000592, loss: 0.1155
2022-10-02 15:22:19 - train: epoch 0017, iter [00920, 01251], lr: 0.000592, loss: 0.1132
2022-10-02 15:22:47 - train: epoch 0017, iter [00930, 01251], lr: 0.000592, loss: 0.1176
2022-10-02 15:23:14 - train: epoch 0017, iter [00940, 01251], lr: 0.000592, loss: 0.1021
2022-10-02 15:23:42 - train: epoch 0017, iter [00950, 01251], lr: 0.000592, loss: 0.1134
2022-10-02 15:24:11 - train: epoch 0017, iter [00960, 01251], lr: 0.000592, loss: 0.1136
2022-10-02 15:24:39 - train: epoch 0017, iter [00970, 01251], lr: 0.000592, loss: 0.1026
2022-10-02 15:25:07 - train: epoch 0017, iter [00980, 01251], lr: 0.000592, loss: 0.1134
2022-10-02 15:25:35 - train: epoch 0017, iter [00990, 01251], lr: 0.000592, loss: 0.1062
2022-10-02 15:26:03 - train: epoch 0017, iter [01000, 01251], lr: 0.000592, loss: 0.1130
2022-10-02 15:26:31 - train: epoch 0017, iter [01010, 01251], lr: 0.000592, loss: 0.1109
2022-10-02 15:26:59 - train: epoch 0017, iter [01020, 01251], lr: 0.000592, loss: 0.1183
2022-10-02 15:27:28 - train: epoch 0017, iter [01030, 01251], lr: 0.000592, loss: 0.1192
2022-10-02 15:27:56 - train: epoch 0017, iter [01040, 01251], lr: 0.000592, loss: 0.1118
2022-10-02 15:28:24 - train: epoch 0017, iter [01050, 01251], lr: 0.000591, loss: 0.1098
2022-10-02 15:28:53 - train: epoch 0017, iter [01060, 01251], lr: 0.000591, loss: 0.1142
2022-10-02 15:29:21 - train: epoch 0017, iter [01070, 01251], lr: 0.000591, loss: 0.1041
2022-10-02 15:29:49 - train: epoch 0017, iter [01080, 01251], lr: 0.000591, loss: 0.1098
2022-10-02 15:30:17 - train: epoch 0017, iter [01090, 01251], lr: 0.000591, loss: 0.1076
2022-10-02 15:30:45 - train: epoch 0017, iter [01100, 01251], lr: 0.000591, loss: 0.1080
2022-10-02 15:31:13 - train: epoch 0017, iter [01110, 01251], lr: 0.000591, loss: 0.1114
2022-10-02 15:31:42 - train: epoch 0017, iter [01120, 01251], lr: 0.000591, loss: 0.1110
2022-10-02 15:32:10 - train: epoch 0017, iter [01130, 01251], lr: 0.000591, loss: 0.1101
2022-10-02 15:32:38 - train: epoch 0017, iter [01140, 01251], lr: 0.000591, loss: 0.1169
2022-10-02 15:33:07 - train: epoch 0017, iter [01150, 01251], lr: 0.000591, loss: 0.1141
2022-10-02 15:33:35 - train: epoch 0017, iter [01160, 01251], lr: 0.000591, loss: 0.1133
2022-10-02 15:34:03 - train: epoch 0017, iter [01170, 01251], lr: 0.000591, loss: 0.1128
2022-10-02 15:34:31 - train: epoch 0017, iter [01180, 01251], lr: 0.000591, loss: 0.1177
2022-10-02 15:35:00 - train: epoch 0017, iter [01190, 01251], lr: 0.000591, loss: 0.1057
2022-10-02 15:35:28 - train: epoch 0017, iter [01200, 01251], lr: 0.000591, loss: 0.1148
2022-10-02 15:35:56 - train: epoch 0017, iter [01210, 01251], lr: 0.000591, loss: 0.1131
2022-10-02 15:36:25 - train: epoch 0017, iter [01220, 01251], lr: 0.000591, loss: 0.1065
2022-10-02 15:36:53 - train: epoch 0017, iter [01230, 01251], lr: 0.000591, loss: 0.1165
2022-10-02 15:37:21 - train: epoch 0017, iter [01240, 01251], lr: 0.000591, loss: 0.1075
2022-10-02 15:37:49 - train: epoch 0017, iter [01250, 01251], lr: 0.000591, loss: 0.1104
2022-10-02 15:37:53 - train: epoch 017, train_loss: 0.1130
2022-10-02 15:37:55 - until epoch: 017, best_loss: 0.1130
2022-10-02 15:37:55 - epoch 018 lr: 0.000591
2022-10-02 15:38:30 - train: epoch 0018, iter [00010, 01251], lr: 0.000591, loss: 0.1109
2022-10-02 15:38:58 - train: epoch 0018, iter [00020, 01251], lr: 0.000591, loss: 0.1164
2022-10-02 15:39:26 - train: epoch 0018, iter [00030, 01251], lr: 0.000591, loss: 0.1050
2022-10-02 15:39:54 - train: epoch 0018, iter [00040, 01251], lr: 0.000591, loss: 0.1048
2022-10-02 15:40:22 - train: epoch 0018, iter [00050, 01251], lr: 0.000591, loss: 0.1091
2022-10-02 15:40:51 - train: epoch 0018, iter [00060, 01251], lr: 0.000591, loss: 0.1150
2022-10-02 15:41:19 - train: epoch 0018, iter [00070, 01251], lr: 0.000591, loss: 0.1111
2022-10-02 15:41:47 - train: epoch 0018, iter [00080, 01251], lr: 0.000591, loss: 0.1134
2022-10-02 15:42:15 - train: epoch 0018, iter [00090, 01251], lr: 0.000591, loss: 0.1098
2022-10-02 15:42:43 - train: epoch 0018, iter [00100, 01251], lr: 0.000591, loss: 0.1081
2022-10-02 15:43:11 - train: epoch 0018, iter [00110, 01251], lr: 0.000591, loss: 0.1152
2022-10-02 15:43:40 - train: epoch 0018, iter [00120, 01251], lr: 0.000591, loss: 0.1031
2022-10-02 15:44:08 - train: epoch 0018, iter [00130, 01251], lr: 0.000591, loss: 0.1080
2022-10-02 15:44:36 - train: epoch 0018, iter [00140, 01251], lr: 0.000591, loss: 0.1132
2022-10-02 15:45:05 - train: epoch 0018, iter [00150, 01251], lr: 0.000591, loss: 0.1113
2022-10-02 15:45:33 - train: epoch 0018, iter [00160, 01251], lr: 0.000591, loss: 0.1145
2022-10-02 15:46:01 - train: epoch 0018, iter [00170, 01251], lr: 0.000591, loss: 0.1118
2022-10-02 15:46:29 - train: epoch 0018, iter [00180, 01251], lr: 0.000591, loss: 0.1058
2022-10-02 15:46:58 - train: epoch 0018, iter [00190, 01251], lr: 0.000591, loss: 0.1187
2022-10-02 15:47:26 - train: epoch 0018, iter [00200, 01251], lr: 0.000591, loss: 0.0984
2022-10-02 15:47:54 - train: epoch 0018, iter [00210, 01251], lr: 0.000591, loss: 0.1074
2022-10-02 15:48:23 - train: epoch 0018, iter [00220, 01251], lr: 0.000591, loss: 0.1135
2022-10-02 15:48:51 - train: epoch 0018, iter [00230, 01251], lr: 0.000591, loss: 0.1136
2022-10-02 15:49:19 - train: epoch 0018, iter [00240, 01251], lr: 0.000591, loss: 0.1116
2022-10-02 15:49:47 - train: epoch 0018, iter [00250, 01251], lr: 0.000591, loss: 0.1090
2022-10-02 15:50:16 - train: epoch 0018, iter [00260, 01251], lr: 0.000591, loss: 0.1141
2022-10-02 15:50:44 - train: epoch 0018, iter [00270, 01251], lr: 0.000591, loss: 0.1175
2022-10-02 15:51:12 - train: epoch 0018, iter [00280, 01251], lr: 0.000591, loss: 0.1091
2022-10-02 15:51:41 - train: epoch 0018, iter [00290, 01251], lr: 0.000590, loss: 0.1044
2022-10-02 15:52:09 - train: epoch 0018, iter [00300, 01251], lr: 0.000590, loss: 0.1067
2022-10-02 15:52:37 - train: epoch 0018, iter [00310, 01251], lr: 0.000590, loss: 0.1105
2022-10-02 15:53:05 - train: epoch 0018, iter [00320, 01251], lr: 0.000590, loss: 0.1155
2022-10-02 15:53:34 - train: epoch 0018, iter [00330, 01251], lr: 0.000590, loss: 0.1111
2022-10-02 15:54:02 - train: epoch 0018, iter [00340, 01251], lr: 0.000590, loss: 0.1076
2022-10-02 15:54:30 - train: epoch 0018, iter [00350, 01251], lr: 0.000590, loss: 0.1149
2022-10-02 15:54:58 - train: epoch 0018, iter [00360, 01251], lr: 0.000590, loss: 0.1120
2022-10-02 15:55:27 - train: epoch 0018, iter [00370, 01251], lr: 0.000590, loss: 0.1131
2022-10-02 15:55:55 - train: epoch 0018, iter [00380, 01251], lr: 0.000590, loss: 0.1097
2022-10-02 15:56:23 - train: epoch 0018, iter [00390, 01251], lr: 0.000590, loss: 0.1086
2022-10-02 15:56:51 - train: epoch 0018, iter [00400, 01251], lr: 0.000590, loss: 0.1080
2022-10-02 15:57:20 - train: epoch 0018, iter [00410, 01251], lr: 0.000590, loss: 0.1178
2022-10-02 15:57:48 - train: epoch 0018, iter [00420, 01251], lr: 0.000590, loss: 0.1082
2022-10-02 15:58:17 - train: epoch 0018, iter [00430, 01251], lr: 0.000590, loss: 0.1125
2022-10-02 15:58:45 - train: epoch 0018, iter [00440, 01251], lr: 0.000590, loss: 0.1080
2022-10-02 15:59:13 - train: epoch 0018, iter [00450, 01251], lr: 0.000590, loss: 0.1063
2022-10-02 15:59:41 - train: epoch 0018, iter [00460, 01251], lr: 0.000590, loss: 0.1001
2022-10-02 16:00:10 - train: epoch 0018, iter [00470, 01251], lr: 0.000590, loss: 0.1126
2022-10-02 16:00:38 - train: epoch 0018, iter [00480, 01251], lr: 0.000590, loss: 0.1127
2022-10-02 16:01:06 - train: epoch 0018, iter [00490, 01251], lr: 0.000590, loss: 0.1094
2022-10-02 16:01:34 - train: epoch 0018, iter [00500, 01251], lr: 0.000590, loss: 0.1069
2022-10-02 16:02:02 - train: epoch 0018, iter [00510, 01251], lr: 0.000590, loss: 0.1134
2022-10-02 16:02:31 - train: epoch 0018, iter [00520, 01251], lr: 0.000590, loss: 0.1106
2022-10-02 16:02:59 - train: epoch 0018, iter [00530, 01251], lr: 0.000590, loss: 0.1185
2022-10-02 16:03:27 - train: epoch 0018, iter [00540, 01251], lr: 0.000590, loss: 0.1064
2022-10-02 16:03:55 - train: epoch 0018, iter [00550, 01251], lr: 0.000590, loss: 0.1121
2022-10-02 16:04:24 - train: epoch 0018, iter [00560, 01251], lr: 0.000590, loss: 0.1139
2022-10-02 16:04:52 - train: epoch 0018, iter [00570, 01251], lr: 0.000590, loss: 0.1017
2022-10-02 16:05:20 - train: epoch 0018, iter [00580, 01251], lr: 0.000590, loss: 0.1140
2022-10-02 16:05:48 - train: epoch 0018, iter [00590, 01251], lr: 0.000590, loss: 0.1008
2022-10-02 16:06:17 - train: epoch 0018, iter [00600, 01251], lr: 0.000590, loss: 0.0987
2022-10-02 16:06:45 - train: epoch 0018, iter [00610, 01251], lr: 0.000590, loss: 0.1076
2022-10-02 16:07:13 - train: epoch 0018, iter [00620, 01251], lr: 0.000590, loss: 0.1065
2022-10-02 16:07:42 - train: epoch 0018, iter [00630, 01251], lr: 0.000590, loss: 0.1089
2022-10-02 16:08:10 - train: epoch 0018, iter [00640, 01251], lr: 0.000590, loss: 0.1085
2022-10-02 16:08:38 - train: epoch 0018, iter [00650, 01251], lr: 0.000590, loss: 0.1148
2022-10-02 16:09:07 - train: epoch 0018, iter [00660, 01251], lr: 0.000590, loss: 0.1127
2022-10-02 16:09:35 - train: epoch 0018, iter [00670, 01251], lr: 0.000590, loss: 0.1152
2022-10-02 16:10:03 - train: epoch 0018, iter [00680, 01251], lr: 0.000590, loss: 0.1085
2022-10-02 16:10:32 - train: epoch 0018, iter [00690, 01251], lr: 0.000590, loss: 0.1178
2022-10-02 16:11:00 - train: epoch 0018, iter [00700, 01251], lr: 0.000590, loss: 0.1118
2022-10-02 16:11:28 - train: epoch 0018, iter [00710, 01251], lr: 0.000590, loss: 0.1142
2022-10-02 16:11:56 - train: epoch 0018, iter [00720, 01251], lr: 0.000590, loss: 0.1065
2022-10-02 16:12:25 - train: epoch 0018, iter [00730, 01251], lr: 0.000590, loss: 0.1102
2022-10-02 16:12:53 - train: epoch 0018, iter [00740, 01251], lr: 0.000590, loss: 0.1101
2022-10-02 16:13:22 - train: epoch 0018, iter [00750, 01251], lr: 0.000590, loss: 0.1128
2022-10-02 16:13:50 - train: epoch 0018, iter [00760, 01251], lr: 0.000589, loss: 0.1081
2022-10-02 16:14:18 - train: epoch 0018, iter [00770, 01251], lr: 0.000589, loss: 0.0989
2022-10-02 16:14:47 - train: epoch 0018, iter [00780, 01251], lr: 0.000589, loss: 0.1146
2022-10-02 16:15:15 - train: epoch 0018, iter [00790, 01251], lr: 0.000589, loss: 0.1022
2022-10-02 16:15:43 - train: epoch 0018, iter [00800, 01251], lr: 0.000589, loss: 0.1025
2022-10-02 16:16:12 - train: epoch 0018, iter [00810, 01251], lr: 0.000589, loss: 0.1100
2022-10-02 16:16:40 - train: epoch 0018, iter [00820, 01251], lr: 0.000589, loss: 0.1128
2022-10-02 16:17:08 - train: epoch 0018, iter [00830, 01251], lr: 0.000589, loss: 0.1140
2022-10-02 16:17:37 - train: epoch 0018, iter [00840, 01251], lr: 0.000589, loss: 0.1128
2022-10-02 16:18:05 - train: epoch 0018, iter [00850, 01251], lr: 0.000589, loss: 0.1072
2022-10-02 16:18:33 - train: epoch 0018, iter [00860, 01251], lr: 0.000589, loss: 0.1152
2022-10-02 16:19:01 - train: epoch 0018, iter [00870, 01251], lr: 0.000589, loss: 0.1107
2022-10-02 16:19:29 - train: epoch 0018, iter [00880, 01251], lr: 0.000589, loss: 0.1061
2022-10-02 16:19:57 - train: epoch 0018, iter [00890, 01251], lr: 0.000589, loss: 0.1115
2022-10-02 16:20:26 - train: epoch 0018, iter [00900, 01251], lr: 0.000589, loss: 0.1067
2022-10-02 16:20:54 - train: epoch 0018, iter [00910, 01251], lr: 0.000589, loss: 0.1107
2022-10-02 16:21:22 - train: epoch 0018, iter [00920, 01251], lr: 0.000589, loss: 0.1199
2022-10-02 16:21:50 - train: epoch 0018, iter [00930, 01251], lr: 0.000589, loss: 0.1106
2022-10-02 16:22:18 - train: epoch 0018, iter [00940, 01251], lr: 0.000589, loss: 0.1072
2022-10-02 16:22:47 - train: epoch 0018, iter [00950, 01251], lr: 0.000589, loss: 0.1155
2022-10-02 16:23:15 - train: epoch 0018, iter [00960, 01251], lr: 0.000589, loss: 0.1142
2022-10-02 16:23:43 - train: epoch 0018, iter [00970, 01251], lr: 0.000589, loss: 0.1113
2022-10-02 16:24:11 - train: epoch 0018, iter [00980, 01251], lr: 0.000589, loss: 0.1226
2022-10-02 16:24:40 - train: epoch 0018, iter [00990, 01251], lr: 0.000589, loss: 0.1081
2022-10-02 16:25:08 - train: epoch 0018, iter [01000, 01251], lr: 0.000589, loss: 0.1087
2022-10-02 16:25:36 - train: epoch 0018, iter [01010, 01251], lr: 0.000589, loss: 0.1056
2022-10-02 16:26:04 - train: epoch 0018, iter [01020, 01251], lr: 0.000589, loss: 0.1028
2022-10-02 16:26:32 - train: epoch 0018, iter [01030, 01251], lr: 0.000589, loss: 0.1019
2022-10-02 16:27:01 - train: epoch 0018, iter [01040, 01251], lr: 0.000589, loss: 0.1096
2022-10-02 16:27:29 - train: epoch 0018, iter [01050, 01251], lr: 0.000589, loss: 0.1116
2022-10-02 16:27:57 - train: epoch 0018, iter [01060, 01251], lr: 0.000589, loss: 0.1082
2022-10-02 16:28:25 - train: epoch 0018, iter [01070, 01251], lr: 0.000589, loss: 0.1077
2022-10-02 16:28:53 - train: epoch 0018, iter [01080, 01251], lr: 0.000589, loss: 0.1161
2022-10-02 16:29:21 - train: epoch 0018, iter [01090, 01251], lr: 0.000589, loss: 0.1020
2022-10-02 16:29:49 - train: epoch 0018, iter [01100, 01251], lr: 0.000589, loss: 0.1144
2022-10-02 16:30:18 - train: epoch 0018, iter [01110, 01251], lr: 0.000589, loss: 0.1171
2022-10-02 16:30:46 - train: epoch 0018, iter [01120, 01251], lr: 0.000589, loss: 0.1115
2022-10-02 16:31:14 - train: epoch 0018, iter [01130, 01251], lr: 0.000589, loss: 0.1045
2022-10-02 16:31:42 - train: epoch 0018, iter [01140, 01251], lr: 0.000589, loss: 0.1036
2022-10-02 16:32:10 - train: epoch 0018, iter [01150, 01251], lr: 0.000589, loss: 0.1244
2022-10-02 16:32:38 - train: epoch 0018, iter [01160, 01251], lr: 0.000589, loss: 0.1162
2022-10-02 16:33:06 - train: epoch 0018, iter [01170, 01251], lr: 0.000589, loss: 0.1059
2022-10-02 16:33:34 - train: epoch 0018, iter [01180, 01251], lr: 0.000589, loss: 0.1151
2022-10-02 16:34:03 - train: epoch 0018, iter [01190, 01251], lr: 0.000589, loss: 0.1167
2022-10-02 16:34:31 - train: epoch 0018, iter [01200, 01251], lr: 0.000588, loss: 0.1118
2022-10-02 16:34:59 - train: epoch 0018, iter [01210, 01251], lr: 0.000588, loss: 0.1055
2022-10-02 16:35:27 - train: epoch 0018, iter [01220, 01251], lr: 0.000588, loss: 0.1080
2022-10-02 16:35:56 - train: epoch 0018, iter [01230, 01251], lr: 0.000588, loss: 0.1074
2022-10-02 16:36:24 - train: epoch 0018, iter [01240, 01251], lr: 0.000588, loss: 0.1053
2022-10-02 16:36:52 - train: epoch 0018, iter [01250, 01251], lr: 0.000588, loss: 0.1027
2022-10-02 16:36:56 - train: epoch 018, train_loss: 0.1109
2022-10-02 16:36:58 - until epoch: 018, best_loss: 0.1109
2022-10-02 16:36:58 - epoch 019 lr: 0.000588
2022-10-02 16:37:32 - train: epoch 0019, iter [00010, 01251], lr: 0.000588, loss: 0.1111
2022-10-02 16:38:00 - train: epoch 0019, iter [00020, 01251], lr: 0.000588, loss: 0.1171
2022-10-02 16:38:29 - train: epoch 0019, iter [00030, 01251], lr: 0.000588, loss: 0.1083
2022-10-02 16:38:57 - train: epoch 0019, iter [00040, 01251], lr: 0.000588, loss: 0.1061
2022-10-02 16:39:25 - train: epoch 0019, iter [00050, 01251], lr: 0.000588, loss: 0.1124
2022-10-02 16:39:53 - train: epoch 0019, iter [00060, 01251], lr: 0.000588, loss: 0.1163
2022-10-02 16:40:21 - train: epoch 0019, iter [00070, 01251], lr: 0.000588, loss: 0.1088
2022-10-02 16:40:49 - train: epoch 0019, iter [00080, 01251], lr: 0.000588, loss: 0.1094
2022-10-02 16:41:17 - train: epoch 0019, iter [00090, 01251], lr: 0.000588, loss: 0.1082
2022-10-02 16:41:45 - train: epoch 0019, iter [00100, 01251], lr: 0.000588, loss: 0.1142
2022-10-02 16:42:13 - train: epoch 0019, iter [00110, 01251], lr: 0.000588, loss: 0.1128
2022-10-02 16:42:41 - train: epoch 0019, iter [00120, 01251], lr: 0.000588, loss: 0.1068
2022-10-02 16:43:09 - train: epoch 0019, iter [00130, 01251], lr: 0.000588, loss: 0.1072
2022-10-02 16:43:37 - train: epoch 0019, iter [00140, 01251], lr: 0.000588, loss: 0.1064
2022-10-02 16:44:05 - train: epoch 0019, iter [00150, 01251], lr: 0.000588, loss: 0.1117
2022-10-02 16:44:33 - train: epoch 0019, iter [00160, 01251], lr: 0.000588, loss: 0.1193
2022-10-02 16:45:02 - train: epoch 0019, iter [00170, 01251], lr: 0.000588, loss: 0.1134
2022-10-02 16:45:30 - train: epoch 0019, iter [00180, 01251], lr: 0.000588, loss: 0.1035
2022-10-02 16:45:58 - train: epoch 0019, iter [00190, 01251], lr: 0.000588, loss: 0.1196
2022-10-02 16:46:26 - train: epoch 0019, iter [00200, 01251], lr: 0.000588, loss: 0.1128
2022-10-02 16:46:54 - train: epoch 0019, iter [00210, 01251], lr: 0.000588, loss: 0.1071
2022-10-02 16:47:22 - train: epoch 0019, iter [00220, 01251], lr: 0.000588, loss: 0.1109
2022-10-02 16:47:50 - train: epoch 0019, iter [00230, 01251], lr: 0.000588, loss: 0.1095
2022-10-02 16:48:18 - train: epoch 0019, iter [00240, 01251], lr: 0.000588, loss: 0.1076
2022-10-02 16:48:46 - train: epoch 0019, iter [00250, 01251], lr: 0.000588, loss: 0.1050
2022-10-02 16:49:15 - train: epoch 0019, iter [00260, 01251], lr: 0.000588, loss: 0.1147
2022-10-02 16:49:43 - train: epoch 0019, iter [00270, 01251], lr: 0.000588, loss: 0.1063
2022-10-02 16:50:11 - train: epoch 0019, iter [00280, 01251], lr: 0.000588, loss: 0.1127
2022-10-02 16:50:39 - train: epoch 0019, iter [00290, 01251], lr: 0.000588, loss: 0.1049
2022-10-02 16:51:07 - train: epoch 0019, iter [00300, 01251], lr: 0.000588, loss: 0.1011
2022-10-02 16:51:36 - train: epoch 0019, iter [00310, 01251], lr: 0.000588, loss: 0.1135
2022-10-02 16:52:04 - train: epoch 0019, iter [00320, 01251], lr: 0.000588, loss: 0.1135
2022-10-02 16:52:32 - train: epoch 0019, iter [00330, 01251], lr: 0.000588, loss: 0.1112
2022-10-02 16:53:00 - train: epoch 0019, iter [00340, 01251], lr: 0.000588, loss: 0.1114
2022-10-02 16:53:28 - train: epoch 0019, iter [00350, 01251], lr: 0.000588, loss: 0.1087
2022-10-02 16:53:56 - train: epoch 0019, iter [00360, 01251], lr: 0.000588, loss: 0.1099
2022-10-02 16:54:25 - train: epoch 0019, iter [00370, 01251], lr: 0.000588, loss: 0.1045
2022-10-02 16:54:53 - train: epoch 0019, iter [00380, 01251], lr: 0.000587, loss: 0.1101
2022-10-02 16:55:21 - train: epoch 0019, iter [00390, 01251], lr: 0.000587, loss: 0.1302
2022-10-02 16:55:49 - train: epoch 0019, iter [00400, 01251], lr: 0.000587, loss: 0.1113
2022-10-02 16:56:17 - train: epoch 0019, iter [00410, 01251], lr: 0.000587, loss: 0.1041
2022-10-02 16:56:45 - train: epoch 0019, iter [00420, 01251], lr: 0.000587, loss: 0.1158
2022-10-02 16:57:13 - train: epoch 0019, iter [00430, 01251], lr: 0.000587, loss: 0.1119
2022-10-02 16:57:41 - train: epoch 0019, iter [00440, 01251], lr: 0.000587, loss: 0.1143
2022-10-02 16:58:09 - train: epoch 0019, iter [00450, 01251], lr: 0.000587, loss: 0.1102
2022-10-02 16:58:38 - train: epoch 0019, iter [00460, 01251], lr: 0.000587, loss: 0.1034
2022-10-02 16:59:06 - train: epoch 0019, iter [00470, 01251], lr: 0.000587, loss: 0.1118
2022-10-02 16:59:34 - train: epoch 0019, iter [00480, 01251], lr: 0.000587, loss: 0.1086
2022-10-02 17:00:02 - train: epoch 0019, iter [00490, 01251], lr: 0.000587, loss: 0.1043
2022-10-02 17:00:30 - train: epoch 0019, iter [00500, 01251], lr: 0.000587, loss: 0.0988
2022-10-02 17:00:58 - train: epoch 0019, iter [00510, 01251], lr: 0.000587, loss: 0.1145
2022-10-02 17:01:26 - train: epoch 0019, iter [00520, 01251], lr: 0.000587, loss: 0.1105
2022-10-02 17:01:54 - train: epoch 0019, iter [00530, 01251], lr: 0.000587, loss: 0.1074
2022-10-02 17:02:22 - train: epoch 0019, iter [00540, 01251], lr: 0.000587, loss: 0.1151
2022-10-02 17:02:50 - train: epoch 0019, iter [00550, 01251], lr: 0.000587, loss: 0.1148
2022-10-02 17:03:18 - train: epoch 0019, iter [00560, 01251], lr: 0.000587, loss: 0.1098
2022-10-02 17:03:46 - train: epoch 0019, iter [00570, 01251], lr: 0.000587, loss: 0.1082
2022-10-02 17:04:14 - train: epoch 0019, iter [00580, 01251], lr: 0.000587, loss: 0.1100
2022-10-02 17:04:42 - train: epoch 0019, iter [00590, 01251], lr: 0.000587, loss: 0.1052
2022-10-02 17:05:10 - train: epoch 0019, iter [00600, 01251], lr: 0.000587, loss: 0.1085
2022-10-02 17:05:38 - train: epoch 0019, iter [00610, 01251], lr: 0.000587, loss: 0.1082
2022-10-02 17:06:06 - train: epoch 0019, iter [00620, 01251], lr: 0.000587, loss: 0.1036
2022-10-02 17:06:34 - train: epoch 0019, iter [00630, 01251], lr: 0.000587, loss: 0.1152
2022-10-02 17:07:02 - train: epoch 0019, iter [00640, 01251], lr: 0.000587, loss: 0.1165
2022-10-02 17:07:30 - train: epoch 0019, iter [00650, 01251], lr: 0.000587, loss: 0.1178
2022-10-02 17:07:58 - train: epoch 0019, iter [00660, 01251], lr: 0.000587, loss: 0.1115
2022-10-02 17:08:26 - train: epoch 0019, iter [00670, 01251], lr: 0.000587, loss: 0.1002
2022-10-02 17:08:54 - train: epoch 0019, iter [00680, 01251], lr: 0.000587, loss: 0.1003
2022-10-02 17:09:23 - train: epoch 0019, iter [00690, 01251], lr: 0.000587, loss: 0.1136
2022-10-02 17:09:51 - train: epoch 0019, iter [00700, 01251], lr: 0.000587, loss: 0.1034
2022-10-02 17:10:19 - train: epoch 0019, iter [00710, 01251], lr: 0.000587, loss: 0.1077
2022-10-02 17:10:47 - train: epoch 0019, iter [00720, 01251], lr: 0.000587, loss: 0.1087
2022-10-02 17:11:15 - train: epoch 0019, iter [00730, 01251], lr: 0.000587, loss: 0.1132
2022-10-02 17:11:43 - train: epoch 0019, iter [00740, 01251], lr: 0.000587, loss: 0.1140
2022-10-02 17:12:11 - train: epoch 0019, iter [00750, 01251], lr: 0.000587, loss: 0.1089
2022-10-02 17:12:39 - train: epoch 0019, iter [00760, 01251], lr: 0.000587, loss: 0.1263
2022-10-02 17:13:07 - train: epoch 0019, iter [00770, 01251], lr: 0.000587, loss: 0.1078
2022-10-02 17:13:35 - train: epoch 0019, iter [00780, 01251], lr: 0.000587, loss: 0.1025
2022-10-02 17:14:03 - train: epoch 0019, iter [00790, 01251], lr: 0.000586, loss: 0.1106
2022-10-02 17:14:31 - train: epoch 0019, iter [00800, 01251], lr: 0.000586, loss: 0.1166
2022-10-02 17:14:59 - train: epoch 0019, iter [00810, 01251], lr: 0.000586, loss: 0.1059
2022-10-02 17:15:28 - train: epoch 0019, iter [00820, 01251], lr: 0.000586, loss: 0.1080
2022-10-02 17:15:56 - train: epoch 0019, iter [00830, 01251], lr: 0.000586, loss: 0.1205
2022-10-02 17:16:24 - train: epoch 0019, iter [00840, 01251], lr: 0.000586, loss: 0.1047
2022-10-02 17:16:52 - train: epoch 0019, iter [00850, 01251], lr: 0.000586, loss: 0.1088
2022-10-02 17:17:20 - train: epoch 0019, iter [00860, 01251], lr: 0.000586, loss: 0.1048
2022-10-02 17:17:48 - train: epoch 0019, iter [00870, 01251], lr: 0.000586, loss: 0.1076
2022-10-02 17:18:16 - train: epoch 0019, iter [00880, 01251], lr: 0.000586, loss: 0.1089
2022-10-02 17:18:45 - train: epoch 0019, iter [00890, 01251], lr: 0.000586, loss: 0.1024
2022-10-02 17:19:12 - train: epoch 0019, iter [00900, 01251], lr: 0.000586, loss: 0.1076
2022-10-02 17:19:40 - train: epoch 0019, iter [00910, 01251], lr: 0.000586, loss: 0.1103
2022-10-02 17:20:08 - train: epoch 0019, iter [00920, 01251], lr: 0.000586, loss: 0.1202
2022-10-02 17:20:36 - train: epoch 0019, iter [00930, 01251], lr: 0.000586, loss: 0.1046
2022-10-02 17:21:04 - train: epoch 0019, iter [00940, 01251], lr: 0.000586, loss: 0.0994
2022-10-02 17:21:32 - train: epoch 0019, iter [00950, 01251], lr: 0.000586, loss: 0.1094
2022-10-02 17:22:00 - train: epoch 0019, iter [00960, 01251], lr: 0.000586, loss: 0.1093
2022-10-02 17:22:28 - train: epoch 0019, iter [00970, 01251], lr: 0.000586, loss: 0.1026
2022-10-02 17:22:56 - train: epoch 0019, iter [00980, 01251], lr: 0.000586, loss: 0.1093
2022-10-02 17:23:24 - train: epoch 0019, iter [00990, 01251], lr: 0.000586, loss: 0.1055
2022-10-02 17:23:52 - train: epoch 0019, iter [01000, 01251], lr: 0.000586, loss: 0.1116
2022-10-02 17:24:21 - train: epoch 0019, iter [01010, 01251], lr: 0.000586, loss: 0.1056
2022-10-02 17:24:49 - train: epoch 0019, iter [01020, 01251], lr: 0.000586, loss: 0.1112
2022-10-02 17:25:17 - train: epoch 0019, iter [01030, 01251], lr: 0.000586, loss: 0.1085
2022-10-02 17:25:45 - train: epoch 0019, iter [01040, 01251], lr: 0.000586, loss: 0.1080
2022-10-02 17:26:13 - train: epoch 0019, iter [01050, 01251], lr: 0.000586, loss: 0.1119
2022-10-02 17:26:41 - train: epoch 0019, iter [01060, 01251], lr: 0.000586, loss: 0.1133
2022-10-02 17:27:09 - train: epoch 0019, iter [01070, 01251], lr: 0.000586, loss: 0.1047
2022-10-02 17:27:37 - train: epoch 0019, iter [01080, 01251], lr: 0.000586, loss: 0.1115
2022-10-02 17:28:06 - train: epoch 0019, iter [01090, 01251], lr: 0.000586, loss: 0.1074
2022-10-02 17:28:33 - train: epoch 0019, iter [01100, 01251], lr: 0.000586, loss: 0.1122
2022-10-02 17:29:01 - train: epoch 0019, iter [01110, 01251], lr: 0.000586, loss: 0.1056
2022-10-02 17:29:29 - train: epoch 0019, iter [01120, 01251], lr: 0.000586, loss: 0.1030
2022-10-02 17:29:57 - train: epoch 0019, iter [01130, 01251], lr: 0.000586, loss: 0.1040
2022-10-02 17:30:25 - train: epoch 0019, iter [01140, 01251], lr: 0.000586, loss: 0.1071
2022-10-02 17:30:54 - train: epoch 0019, iter [01150, 01251], lr: 0.000586, loss: 0.1040
2022-10-02 17:31:22 - train: epoch 0019, iter [01160, 01251], lr: 0.000586, loss: 0.1185
2022-10-02 17:31:50 - train: epoch 0019, iter [01170, 01251], lr: 0.000586, loss: 0.1067
2022-10-02 17:32:18 - train: epoch 0019, iter [01180, 01251], lr: 0.000586, loss: 0.1175
2022-10-02 17:32:47 - train: epoch 0019, iter [01190, 01251], lr: 0.000585, loss: 0.1130
2022-10-02 17:33:15 - train: epoch 0019, iter [01200, 01251], lr: 0.000585, loss: 0.1068
2022-10-02 17:33:43 - train: epoch 0019, iter [01210, 01251], lr: 0.000585, loss: 0.1141
2022-10-02 17:34:11 - train: epoch 0019, iter [01220, 01251], lr: 0.000585, loss: 0.1115
2022-10-02 17:34:39 - train: epoch 0019, iter [01230, 01251], lr: 0.000585, loss: 0.1139
2022-10-02 17:35:07 - train: epoch 0019, iter [01240, 01251], lr: 0.000585, loss: 0.1107
2022-10-02 17:35:34 - train: epoch 0019, iter [01250, 01251], lr: 0.000585, loss: 0.1026
2022-10-02 17:35:39 - train: epoch 019, train_loss: 0.1093
2022-10-02 17:35:41 - until epoch: 019, best_loss: 0.1093
2022-10-02 17:35:41 - epoch 020 lr: 0.000585
2022-10-02 17:36:16 - train: epoch 0020, iter [00010, 01251], lr: 0.000585, loss: 0.1159
2022-10-02 17:36:44 - train: epoch 0020, iter [00020, 01251], lr: 0.000585, loss: 0.1153
2022-10-02 17:37:12 - train: epoch 0020, iter [00030, 01251], lr: 0.000585, loss: 0.1075
2022-10-02 17:37:40 - train: epoch 0020, iter [00040, 01251], lr: 0.000585, loss: 0.1056
2022-10-02 17:38:09 - train: epoch 0020, iter [00050, 01251], lr: 0.000585, loss: 0.1058
2022-10-02 17:38:37 - train: epoch 0020, iter [00060, 01251], lr: 0.000585, loss: 0.1012
2022-10-02 17:39:05 - train: epoch 0020, iter [00070, 01251], lr: 0.000585, loss: 0.1108
2022-10-02 17:39:33 - train: epoch 0020, iter [00080, 01251], lr: 0.000585, loss: 0.1100
2022-10-02 17:40:01 - train: epoch 0020, iter [00090, 01251], lr: 0.000585, loss: 0.1070
2022-10-02 17:40:29 - train: epoch 0020, iter [00100, 01251], lr: 0.000585, loss: 0.1072
2022-10-02 17:40:57 - train: epoch 0020, iter [00110, 01251], lr: 0.000585, loss: 0.1165
2022-10-02 17:41:25 - train: epoch 0020, iter [00120, 01251], lr: 0.000585, loss: 0.1100
2022-10-02 17:41:53 - train: epoch 0020, iter [00130, 01251], lr: 0.000585, loss: 0.1122
2022-10-02 17:42:22 - train: epoch 0020, iter [00140, 01251], lr: 0.000585, loss: 0.1035
2022-10-02 17:42:50 - train: epoch 0020, iter [00150, 01251], lr: 0.000585, loss: 0.1095
2022-10-02 17:43:18 - train: epoch 0020, iter [00160, 01251], lr: 0.000585, loss: 0.1036
2022-10-02 17:43:46 - train: epoch 0020, iter [00170, 01251], lr: 0.000585, loss: 0.1062
2022-10-02 17:44:14 - train: epoch 0020, iter [00180, 01251], lr: 0.000585, loss: 0.1055
2022-10-02 17:44:43 - train: epoch 0020, iter [00190, 01251], lr: 0.000585, loss: 0.1029
2022-10-02 17:45:11 - train: epoch 0020, iter [00200, 01251], lr: 0.000585, loss: 0.1008
2022-10-02 17:45:39 - train: epoch 0020, iter [00210, 01251], lr: 0.000585, loss: 0.1070
2022-10-02 17:46:08 - train: epoch 0020, iter [00220, 01251], lr: 0.000585, loss: 0.1080
2022-10-02 17:46:36 - train: epoch 0020, iter [00230, 01251], lr: 0.000585, loss: 0.1122
2022-10-02 17:47:04 - train: epoch 0020, iter [00240, 01251], lr: 0.000585, loss: 0.1079
2022-10-02 17:47:32 - train: epoch 0020, iter [00250, 01251], lr: 0.000585, loss: 0.1026
2022-10-02 17:48:00 - train: epoch 0020, iter [00260, 01251], lr: 0.000585, loss: 0.1172
2022-10-02 17:48:28 - train: epoch 0020, iter [00270, 01251], lr: 0.000585, loss: 0.1080
2022-10-02 17:48:56 - train: epoch 0020, iter [00280, 01251], lr: 0.000585, loss: 0.0998
2022-10-02 17:49:25 - train: epoch 0020, iter [00290, 01251], lr: 0.000585, loss: 0.1042
2022-10-02 17:49:53 - train: epoch 0020, iter [00300, 01251], lr: 0.000585, loss: 0.1129
2022-10-02 17:50:21 - train: epoch 0020, iter [00310, 01251], lr: 0.000585, loss: 0.1111
2022-10-02 17:50:49 - train: epoch 0020, iter [00320, 01251], lr: 0.000584, loss: 0.1113
2022-10-02 17:51:17 - train: epoch 0020, iter [00330, 01251], lr: 0.000584, loss: 0.1010
2022-10-02 17:51:45 - train: epoch 0020, iter [00340, 01251], lr: 0.000584, loss: 0.1049
2022-10-02 17:52:14 - train: epoch 0020, iter [00350, 01251], lr: 0.000584, loss: 0.1014
2022-10-02 17:52:42 - train: epoch 0020, iter [00360, 01251], lr: 0.000584, loss: 0.1150
2022-10-02 17:53:10 - train: epoch 0020, iter [00370, 01251], lr: 0.000584, loss: 0.1039
2022-10-02 17:53:38 - train: epoch 0020, iter [00380, 01251], lr: 0.000584, loss: 0.1112
2022-10-02 17:54:06 - train: epoch 0020, iter [00390, 01251], lr: 0.000584, loss: 0.1051
2022-10-02 17:54:34 - train: epoch 0020, iter [00400, 01251], lr: 0.000584, loss: 0.1065
2022-10-02 17:55:03 - train: epoch 0020, iter [00410, 01251], lr: 0.000584, loss: 0.0999
2022-10-02 17:55:31 - train: epoch 0020, iter [00420, 01251], lr: 0.000584, loss: 0.1058
2022-10-02 17:55:59 - train: epoch 0020, iter [00430, 01251], lr: 0.000584, loss: 0.1028
2022-10-02 17:56:27 - train: epoch 0020, iter [00440, 01251], lr: 0.000584, loss: 0.0965
2022-10-02 17:56:56 - train: epoch 0020, iter [00450, 01251], lr: 0.000584, loss: 0.1094
2022-10-02 17:57:24 - train: epoch 0020, iter [00460, 01251], lr: 0.000584, loss: 0.1021
2022-10-02 17:57:53 - train: epoch 0020, iter [00470, 01251], lr: 0.000584, loss: 0.1050
2022-10-02 17:58:21 - train: epoch 0020, iter [00480, 01251], lr: 0.000584, loss: 0.1067
2022-10-02 17:58:49 - train: epoch 0020, iter [00490, 01251], lr: 0.000584, loss: 0.1083
2022-10-02 17:59:18 - train: epoch 0020, iter [00500, 01251], lr: 0.000584, loss: 0.1017
2022-10-02 17:59:46 - train: epoch 0020, iter [00510, 01251], lr: 0.000584, loss: 0.1101
2022-10-02 18:00:14 - train: epoch 0020, iter [00520, 01251], lr: 0.000584, loss: 0.0978
2022-10-02 18:00:43 - train: epoch 0020, iter [00530, 01251], lr: 0.000584, loss: 0.1115
2022-10-02 18:01:11 - train: epoch 0020, iter [00540, 01251], lr: 0.000584, loss: 0.1071
2022-10-02 18:01:39 - train: epoch 0020, iter [00550, 01251], lr: 0.000584, loss: 0.1076
2022-10-02 18:02:07 - train: epoch 0020, iter [00560, 01251], lr: 0.000584, loss: 0.1027
2022-10-02 18:02:35 - train: epoch 0020, iter [00570, 01251], lr: 0.000584, loss: 0.1009
2022-10-02 18:03:04 - train: epoch 0020, iter [00580, 01251], lr: 0.000584, loss: 0.1051
2022-10-02 18:03:32 - train: epoch 0020, iter [00590, 01251], lr: 0.000584, loss: 0.1030
2022-10-02 18:04:00 - train: epoch 0020, iter [00600, 01251], lr: 0.000584, loss: 0.1098
2022-10-02 18:04:28 - train: epoch 0020, iter [00610, 01251], lr: 0.000584, loss: 0.1041
2022-10-02 18:04:56 - train: epoch 0020, iter [00620, 01251], lr: 0.000584, loss: 0.0994
2022-10-02 18:05:24 - train: epoch 0020, iter [00630, 01251], lr: 0.000584, loss: 0.1002
2022-10-02 18:05:53 - train: epoch 0020, iter [00640, 01251], lr: 0.000584, loss: 0.1086
2022-10-02 18:06:21 - train: epoch 0020, iter [00650, 01251], lr: 0.000584, loss: 0.1069
2022-10-02 18:06:49 - train: epoch 0020, iter [00660, 01251], lr: 0.000584, loss: 0.1164
2022-10-02 18:07:17 - train: epoch 0020, iter [00670, 01251], lr: 0.000584, loss: 0.0997
2022-10-02 18:07:45 - train: epoch 0020, iter [00680, 01251], lr: 0.000584, loss: 0.1175
2022-10-02 18:08:14 - train: epoch 0020, iter [00690, 01251], lr: 0.000583, loss: 0.1109
2022-10-02 18:08:42 - train: epoch 0020, iter [00700, 01251], lr: 0.000583, loss: 0.1022
2022-10-02 18:09:10 - train: epoch 0020, iter [00710, 01251], lr: 0.000583, loss: 0.1144
2022-10-02 18:09:38 - train: epoch 0020, iter [00720, 01251], lr: 0.000583, loss: 0.1077
2022-10-02 18:10:06 - train: epoch 0020, iter [00730, 01251], lr: 0.000583, loss: 0.1031
2022-10-02 18:10:35 - train: epoch 0020, iter [00740, 01251], lr: 0.000583, loss: 0.1039
2022-10-02 18:11:03 - train: epoch 0020, iter [00750, 01251], lr: 0.000583, loss: 0.1114
2022-10-02 18:11:31 - train: epoch 0020, iter [00760, 01251], lr: 0.000583, loss: 0.1054
2022-10-02 18:11:59 - train: epoch 0020, iter [00770, 01251], lr: 0.000583, loss: 0.0976
2022-10-02 18:12:27 - train: epoch 0020, iter [00780, 01251], lr: 0.000583, loss: 0.1066
2022-10-02 18:12:56 - train: epoch 0020, iter [00790, 01251], lr: 0.000583, loss: 0.1066
2022-10-02 18:13:24 - train: epoch 0020, iter [00800, 01251], lr: 0.000583, loss: 0.1044
2022-10-02 18:13:52 - train: epoch 0020, iter [00810, 01251], lr: 0.000583, loss: 0.1093
2022-10-02 18:14:20 - train: epoch 0020, iter [00820, 01251], lr: 0.000583, loss: 0.1068
2022-10-02 18:14:49 - train: epoch 0020, iter [00830, 01251], lr: 0.000583, loss: 0.1026
2022-10-02 18:15:17 - train: epoch 0020, iter [00840, 01251], lr: 0.000583, loss: 0.1038
2022-10-02 18:15:45 - train: epoch 0020, iter [00850, 01251], lr: 0.000583, loss: 0.1040
2022-10-02 18:16:14 - train: epoch 0020, iter [00860, 01251], lr: 0.000583, loss: 0.1072
2022-10-02 18:16:42 - train: epoch 0020, iter [00870, 01251], lr: 0.000583, loss: 0.1029
2022-10-02 18:17:11 - train: epoch 0020, iter [00880, 01251], lr: 0.000583, loss: 0.1083
2022-10-02 18:17:39 - train: epoch 0020, iter [00890, 01251], lr: 0.000583, loss: 0.1068
2022-10-02 18:18:07 - train: epoch 0020, iter [00900, 01251], lr: 0.000583, loss: 0.1133
2022-10-02 18:18:35 - train: epoch 0020, iter [00910, 01251], lr: 0.000583, loss: 0.1060
2022-10-02 18:19:03 - train: epoch 0020, iter [00920, 01251], lr: 0.000583, loss: 0.1100
2022-10-02 18:19:32 - train: epoch 0020, iter [00930, 01251], lr: 0.000583, loss: 0.1038
2022-10-02 18:20:00 - train: epoch 0020, iter [00940, 01251], lr: 0.000583, loss: 0.1097
2022-10-02 18:20:28 - train: epoch 0020, iter [00950, 01251], lr: 0.000583, loss: 0.1059
2022-10-02 18:20:56 - train: epoch 0020, iter [00960, 01251], lr: 0.000583, loss: 0.1062
2022-10-02 18:21:25 - train: epoch 0020, iter [00970, 01251], lr: 0.000583, loss: 0.1038
2022-10-02 18:21:53 - train: epoch 0020, iter [00980, 01251], lr: 0.000583, loss: 0.1048
2022-10-02 18:22:21 - train: epoch 0020, iter [00990, 01251], lr: 0.000583, loss: 0.1056
2022-10-02 18:22:50 - train: epoch 0020, iter [01000, 01251], lr: 0.000583, loss: 0.1159
2022-10-02 18:23:18 - train: epoch 0020, iter [01010, 01251], lr: 0.000583, loss: 0.1221
2022-10-02 18:23:46 - train: epoch 0020, iter [01020, 01251], lr: 0.000583, loss: 0.1065
2022-10-02 18:24:14 - train: epoch 0020, iter [01030, 01251], lr: 0.000583, loss: 0.1001
2022-10-02 18:24:43 - train: epoch 0020, iter [01040, 01251], lr: 0.000583, loss: 0.1023
2022-10-02 18:25:11 - train: epoch 0020, iter [01050, 01251], lr: 0.000582, loss: 0.1067
2022-10-02 18:25:39 - train: epoch 0020, iter [01060, 01251], lr: 0.000582, loss: 0.1113
2022-10-02 18:26:08 - train: epoch 0020, iter [01070, 01251], lr: 0.000582, loss: 0.0990
2022-10-02 18:26:36 - train: epoch 0020, iter [01080, 01251], lr: 0.000582, loss: 0.1061
2022-10-02 18:27:04 - train: epoch 0020, iter [01090, 01251], lr: 0.000582, loss: 0.1123
2022-10-02 18:27:32 - train: epoch 0020, iter [01100, 01251], lr: 0.000582, loss: 0.1079
2022-10-02 18:28:00 - train: epoch 0020, iter [01110, 01251], lr: 0.000582, loss: 0.1121
2022-10-02 18:28:29 - train: epoch 0020, iter [01120, 01251], lr: 0.000582, loss: 0.1082
2022-10-02 18:28:57 - train: epoch 0020, iter [01130, 01251], lr: 0.000582, loss: 0.1100
2022-10-02 18:29:26 - train: epoch 0020, iter [01140, 01251], lr: 0.000582, loss: 0.1052
2022-10-02 18:29:54 - train: epoch 0020, iter [01150, 01251], lr: 0.000582, loss: 0.1112
2022-10-02 18:30:22 - train: epoch 0020, iter [01160, 01251], lr: 0.000582, loss: 0.1091
2022-10-02 18:30:50 - train: epoch 0020, iter [01170, 01251], lr: 0.000582, loss: 0.0990
2022-10-02 18:31:18 - train: epoch 0020, iter [01180, 01251], lr: 0.000582, loss: 0.1099
2022-10-02 18:31:47 - train: epoch 0020, iter [01190, 01251], lr: 0.000582, loss: 0.1039
2022-10-02 18:32:15 - train: epoch 0020, iter [01200, 01251], lr: 0.000582, loss: 0.1028
2022-10-02 18:32:43 - train: epoch 0020, iter [01210, 01251], lr: 0.000582, loss: 0.1140
2022-10-02 18:33:12 - train: epoch 0020, iter [01220, 01251], lr: 0.000582, loss: 0.1019
2022-10-02 18:33:40 - train: epoch 0020, iter [01230, 01251], lr: 0.000582, loss: 0.1004
2022-10-02 18:34:08 - train: epoch 0020, iter [01240, 01251], lr: 0.000582, loss: 0.1068
2022-10-02 18:34:36 - train: epoch 0020, iter [01250, 01251], lr: 0.000582, loss: 0.1196
2022-10-02 18:34:40 - train: epoch 020, train_loss: 0.1077
2022-10-02 18:34:42 - until epoch: 020, best_loss: 0.1077
2022-10-02 18:34:42 - epoch 021 lr: 0.000582
2022-10-02 18:35:17 - train: epoch 0021, iter [00010, 01251], lr: 0.000582, loss: 0.1031
2022-10-02 18:35:45 - train: epoch 0021, iter [00020, 01251], lr: 0.000582, loss: 0.1084
2022-10-02 18:36:13 - train: epoch 0021, iter [00030, 01251], lr: 0.000582, loss: 0.1083
2022-10-02 18:36:41 - train: epoch 0021, iter [00040, 01251], lr: 0.000582, loss: 0.1038
2022-10-02 18:37:09 - train: epoch 0021, iter [00050, 01251], lr: 0.000582, loss: 0.0996
2022-10-02 18:37:37 - train: epoch 0021, iter [00060, 01251], lr: 0.000582, loss: 0.1004
2022-10-02 18:38:05 - train: epoch 0021, iter [00070, 01251], lr: 0.000582, loss: 0.1080
2022-10-02 18:38:34 - train: epoch 0021, iter [00080, 01251], lr: 0.000582, loss: 0.1135
2022-10-02 18:39:02 - train: epoch 0021, iter [00090, 01251], lr: 0.000582, loss: 0.1107
2022-10-02 18:39:30 - train: epoch 0021, iter [00100, 01251], lr: 0.000582, loss: 0.1071
2022-10-02 18:39:58 - train: epoch 0021, iter [00110, 01251], lr: 0.000582, loss: 0.1079
2022-10-02 18:40:26 - train: epoch 0021, iter [00120, 01251], lr: 0.000582, loss: 0.1116
2022-10-02 18:40:54 - train: epoch 0021, iter [00130, 01251], lr: 0.000582, loss: 0.1033
2022-10-02 18:41:22 - train: epoch 0021, iter [00140, 01251], lr: 0.000582, loss: 0.1168
2022-10-02 18:41:50 - train: epoch 0021, iter [00150, 01251], lr: 0.000581, loss: 0.0989
2022-10-02 18:42:18 - train: epoch 0021, iter [00160, 01251], lr: 0.000581, loss: 0.1033
2022-10-02 18:42:46 - train: epoch 0021, iter [00170, 01251], lr: 0.000581, loss: 0.1111
2022-10-02 18:43:15 - train: epoch 0021, iter [00180, 01251], lr: 0.000581, loss: 0.1075
2022-10-02 18:43:43 - train: epoch 0021, iter [00190, 01251], lr: 0.000581, loss: 0.0975
2022-10-02 18:44:11 - train: epoch 0021, iter [00200, 01251], lr: 0.000581, loss: 0.1091
2022-10-02 18:44:39 - train: epoch 0021, iter [00210, 01251], lr: 0.000581, loss: 0.1125
2022-10-02 18:45:07 - train: epoch 0021, iter [00220, 01251], lr: 0.000581, loss: 0.1037
2022-10-02 18:45:35 - train: epoch 0021, iter [00230, 01251], lr: 0.000581, loss: 0.1039
2022-10-02 18:46:04 - train: epoch 0021, iter [00240, 01251], lr: 0.000581, loss: 0.1079
2022-10-02 18:46:32 - train: epoch 0021, iter [00250, 01251], lr: 0.000581, loss: 0.1150
2022-10-02 18:47:00 - train: epoch 0021, iter [00260, 01251], lr: 0.000581, loss: 0.1064
2022-10-02 18:47:28 - train: epoch 0021, iter [00270, 01251], lr: 0.000581, loss: 0.1080
2022-10-02 18:47:56 - train: epoch 0021, iter [00280, 01251], lr: 0.000581, loss: 0.0928
2022-10-02 18:48:24 - train: epoch 0021, iter [00290, 01251], lr: 0.000581, loss: 0.1016
2022-10-02 18:48:53 - train: epoch 0021, iter [00300, 01251], lr: 0.000581, loss: 0.1116
2022-10-02 18:49:21 - train: epoch 0021, iter [00310, 01251], lr: 0.000581, loss: 0.1123
2022-10-02 18:49:49 - train: epoch 0021, iter [00320, 01251], lr: 0.000581, loss: 0.1038
2022-10-02 18:50:17 - train: epoch 0021, iter [00330, 01251], lr: 0.000581, loss: 0.1022
2022-10-02 18:50:45 - train: epoch 0021, iter [00340, 01251], lr: 0.000581, loss: 0.1117
2022-10-02 18:51:13 - train: epoch 0021, iter [00350, 01251], lr: 0.000581, loss: 0.0977
2022-10-02 18:51:41 - train: epoch 0021, iter [00360, 01251], lr: 0.000581, loss: 0.0946
2022-10-02 18:52:09 - train: epoch 0021, iter [00370, 01251], lr: 0.000581, loss: 0.1025
2022-10-02 18:52:37 - train: epoch 0021, iter [00380, 01251], lr: 0.000581, loss: 0.1147
2022-10-02 18:53:06 - train: epoch 0021, iter [00390, 01251], lr: 0.000581, loss: 0.1087
2022-10-02 18:53:34 - train: epoch 0021, iter [00400, 01251], lr: 0.000581, loss: 0.0995
2022-10-02 18:54:02 - train: epoch 0021, iter [00410, 01251], lr: 0.000581, loss: 0.1027
2022-10-02 18:54:30 - train: epoch 0021, iter [00420, 01251], lr: 0.000581, loss: 0.0992
2022-10-02 18:54:58 - train: epoch 0021, iter [00430, 01251], lr: 0.000581, loss: 0.1055
2022-10-02 18:55:27 - train: epoch 0021, iter [00440, 01251], lr: 0.000581, loss: 0.0973
2022-10-02 18:55:55 - train: epoch 0021, iter [00450, 01251], lr: 0.000581, loss: 0.0995
2022-10-02 18:56:23 - train: epoch 0021, iter [00460, 01251], lr: 0.000581, loss: 0.1121
2022-10-02 18:56:51 - train: epoch 0021, iter [00470, 01251], lr: 0.000581, loss: 0.1084
2022-10-02 18:57:19 - train: epoch 0021, iter [00480, 01251], lr: 0.000581, loss: 0.1111
2022-10-02 18:57:48 - train: epoch 0021, iter [00490, 01251], lr: 0.000580, loss: 0.1109
2022-10-02 18:58:16 - train: epoch 0021, iter [00500, 01251], lr: 0.000580, loss: 0.1040
2022-10-02 18:58:44 - train: epoch 0021, iter [00510, 01251], lr: 0.000580, loss: 0.1092
2022-10-02 18:59:12 - train: epoch 0021, iter [00520, 01251], lr: 0.000580, loss: 0.0966
2022-10-02 18:59:41 - train: epoch 0021, iter [00530, 01251], lr: 0.000580, loss: 0.1061
2022-10-02 19:00:09 - train: epoch 0021, iter [00540, 01251], lr: 0.000580, loss: 0.1086
2022-10-02 19:00:37 - train: epoch 0021, iter [00550, 01251], lr: 0.000580, loss: 0.1011
2022-10-02 19:01:05 - train: epoch 0021, iter [00560, 01251], lr: 0.000580, loss: 0.1119
2022-10-02 19:01:34 - train: epoch 0021, iter [00570, 01251], lr: 0.000580, loss: 0.1026
2022-10-02 19:02:02 - train: epoch 0021, iter [00580, 01251], lr: 0.000580, loss: 0.1075
2022-10-02 19:02:30 - train: epoch 0021, iter [00590, 01251], lr: 0.000580, loss: 0.1107
2022-10-02 19:02:59 - train: epoch 0021, iter [00600, 01251], lr: 0.000580, loss: 0.1068
2022-10-02 19:03:27 - train: epoch 0021, iter [00610, 01251], lr: 0.000580, loss: 0.1094
2022-10-02 19:03:55 - train: epoch 0021, iter [00620, 01251], lr: 0.000580, loss: 0.1018
2022-10-02 19:04:23 - train: epoch 0021, iter [00630, 01251], lr: 0.000580, loss: 0.1090
2022-10-02 19:04:52 - train: epoch 0021, iter [00640, 01251], lr: 0.000580, loss: 0.1170
2022-10-02 19:05:20 - train: epoch 0021, iter [00650, 01251], lr: 0.000580, loss: 0.1106
2022-10-02 19:05:48 - train: epoch 0021, iter [00660, 01251], lr: 0.000580, loss: 0.1131
2022-10-02 19:06:16 - train: epoch 0021, iter [00670, 01251], lr: 0.000580, loss: 0.1040
2022-10-02 19:06:45 - train: epoch 0021, iter [00680, 01251], lr: 0.000580, loss: 0.1060
2022-10-02 19:07:13 - train: epoch 0021, iter [00690, 01251], lr: 0.000580, loss: 0.1026
2022-10-02 19:07:41 - train: epoch 0021, iter [00700, 01251], lr: 0.000580, loss: 0.1073
2022-10-02 19:08:09 - train: epoch 0021, iter [00710, 01251], lr: 0.000580, loss: 0.1061
2022-10-02 19:08:37 - train: epoch 0021, iter [00720, 01251], lr: 0.000580, loss: 0.1002
2022-10-02 19:09:06 - train: epoch 0021, iter [00730, 01251], lr: 0.000580, loss: 0.1018
2022-10-02 19:09:34 - train: epoch 0021, iter [00740, 01251], lr: 0.000580, loss: 0.1086
2022-10-02 19:10:02 - train: epoch 0021, iter [00750, 01251], lr: 0.000580, loss: 0.1041
2022-10-02 19:10:31 - train: epoch 0021, iter [00760, 01251], lr: 0.000580, loss: 0.1058
2022-10-02 19:10:59 - train: epoch 0021, iter [00770, 01251], lr: 0.000580, loss: 0.1188
2022-10-02 19:11:27 - train: epoch 0021, iter [00780, 01251], lr: 0.000580, loss: 0.1055
2022-10-02 19:11:55 - train: epoch 0021, iter [00790, 01251], lr: 0.000580, loss: 0.1079
2022-10-02 19:12:24 - train: epoch 0021, iter [00800, 01251], lr: 0.000580, loss: 0.1004
2022-10-02 19:12:52 - train: epoch 0021, iter [00810, 01251], lr: 0.000580, loss: 0.1105
2022-10-02 19:13:20 - train: epoch 0021, iter [00820, 01251], lr: 0.000579, loss: 0.1043
2022-10-02 19:13:48 - train: epoch 0021, iter [00830, 01251], lr: 0.000579, loss: 0.1066
2022-10-02 19:14:16 - train: epoch 0021, iter [00840, 01251], lr: 0.000579, loss: 0.1044
2022-10-02 19:14:44 - train: epoch 0021, iter [00850, 01251], lr: 0.000579, loss: 0.1018
2022-10-02 19:15:13 - train: epoch 0021, iter [00860, 01251], lr: 0.000579, loss: 0.1100
2022-10-02 19:15:41 - train: epoch 0021, iter [00870, 01251], lr: 0.000579, loss: 0.0993
2022-10-02 19:16:09 - train: epoch 0021, iter [00880, 01251], lr: 0.000579, loss: 0.1098
2022-10-02 19:16:37 - train: epoch 0021, iter [00890, 01251], lr: 0.000579, loss: 0.1070
2022-10-02 19:17:05 - train: epoch 0021, iter [00900, 01251], lr: 0.000579, loss: 0.1089
2022-10-02 19:17:34 - train: epoch 0021, iter [00910, 01251], lr: 0.000579, loss: 0.1081
2022-10-02 19:18:02 - train: epoch 0021, iter [00920, 01251], lr: 0.000579, loss: 0.1033
2022-10-02 19:18:30 - train: epoch 0021, iter [00930, 01251], lr: 0.000579, loss: 0.1089
2022-10-02 19:18:58 - train: epoch 0021, iter [00940, 01251], lr: 0.000579, loss: 0.1035
2022-10-02 19:19:26 - train: epoch 0021, iter [00950, 01251], lr: 0.000579, loss: 0.1087
2022-10-02 19:19:54 - train: epoch 0021, iter [00960, 01251], lr: 0.000579, loss: 0.1175
2022-10-02 19:20:23 - train: epoch 0021, iter [00970, 01251], lr: 0.000579, loss: 0.1126
2022-10-02 19:20:51 - train: epoch 0021, iter [00980, 01251], lr: 0.000579, loss: 0.0966
2022-10-02 19:21:19 - train: epoch 0021, iter [00990, 01251], lr: 0.000579, loss: 0.0975
2022-10-02 19:21:48 - train: epoch 0021, iter [01000, 01251], lr: 0.000579, loss: 0.1139
2022-10-02 19:22:16 - train: epoch 0021, iter [01010, 01251], lr: 0.000579, loss: 0.1057
2022-10-02 19:22:44 - train: epoch 0021, iter [01020, 01251], lr: 0.000579, loss: 0.1045
2022-10-02 19:23:12 - train: epoch 0021, iter [01030, 01251], lr: 0.000579, loss: 0.1064
2022-10-02 19:23:40 - train: epoch 0021, iter [01040, 01251], lr: 0.000579, loss: 0.1020
2022-10-02 19:24:08 - train: epoch 0021, iter [01050, 01251], lr: 0.000579, loss: 0.1068
2022-10-02 19:24:36 - train: epoch 0021, iter [01060, 01251], lr: 0.000579, loss: 0.1053
2022-10-02 19:25:05 - train: epoch 0021, iter [01070, 01251], lr: 0.000579, loss: 0.1025
2022-10-02 19:25:33 - train: epoch 0021, iter [01080, 01251], lr: 0.000579, loss: 0.1104
2022-10-02 19:26:01 - train: epoch 0021, iter [01090, 01251], lr: 0.000579, loss: 0.1025
2022-10-02 19:26:29 - train: epoch 0021, iter [01100, 01251], lr: 0.000579, loss: 0.1076
2022-10-02 19:26:57 - train: epoch 0021, iter [01110, 01251], lr: 0.000579, loss: 0.1094
2022-10-02 19:27:25 - train: epoch 0021, iter [01120, 01251], lr: 0.000579, loss: 0.1105
2022-10-02 19:27:54 - train: epoch 0021, iter [01130, 01251], lr: 0.000579, loss: 0.1022
2022-10-02 19:28:22 - train: epoch 0021, iter [01140, 01251], lr: 0.000579, loss: 0.1012
2022-10-02 19:28:50 - train: epoch 0021, iter [01150, 01251], lr: 0.000578, loss: 0.1011
2022-10-02 19:29:18 - train: epoch 0021, iter [01160, 01251], lr: 0.000578, loss: 0.1014
2022-10-02 19:29:46 - train: epoch 0021, iter [01170, 01251], lr: 0.000578, loss: 0.1064
2022-10-02 19:30:15 - train: epoch 0021, iter [01180, 01251], lr: 0.000578, loss: 0.0969
2022-10-02 19:30:43 - train: epoch 0021, iter [01190, 01251], lr: 0.000578, loss: 0.1021
2022-10-02 19:31:11 - train: epoch 0021, iter [01200, 01251], lr: 0.000578, loss: 0.1051
2022-10-02 19:31:39 - train: epoch 0021, iter [01210, 01251], lr: 0.000578, loss: 0.1035
2022-10-02 19:32:08 - train: epoch 0021, iter [01220, 01251], lr: 0.000578, loss: 0.1038
2022-10-02 19:32:36 - train: epoch 0021, iter [01230, 01251], lr: 0.000578, loss: 0.1033
2022-10-02 19:33:04 - train: epoch 0021, iter [01240, 01251], lr: 0.000578, loss: 0.0933
2022-10-02 19:33:32 - train: epoch 0021, iter [01250, 01251], lr: 0.000578, loss: 0.1053
2022-10-02 19:33:37 - train: epoch 021, train_loss: 0.1060
2022-10-02 19:33:38 - until epoch: 021, best_loss: 0.1060
2022-10-02 19:33:38 - epoch 022 lr: 0.000578
2022-10-02 19:34:13 - train: epoch 0022, iter [00010, 01251], lr: 0.000578, loss: 0.1113
2022-10-02 19:34:41 - train: epoch 0022, iter [00020, 01251], lr: 0.000578, loss: 0.1110
2022-10-02 19:35:10 - train: epoch 0022, iter [00030, 01251], lr: 0.000578, loss: 0.1082
2022-10-02 19:35:38 - train: epoch 0022, iter [00040, 01251], lr: 0.000578, loss: 0.1041
2022-10-02 19:36:06 - train: epoch 0022, iter [00050, 01251], lr: 0.000578, loss: 0.1094
2022-10-02 19:36:34 - train: epoch 0022, iter [00060, 01251], lr: 0.000578, loss: 0.1136
2022-10-02 19:37:02 - train: epoch 0022, iter [00070, 01251], lr: 0.000578, loss: 0.1043
2022-10-02 19:37:30 - train: epoch 0022, iter [00080, 01251], lr: 0.000578, loss: 0.1029
2022-10-02 19:37:59 - train: epoch 0022, iter [00090, 01251], lr: 0.000578, loss: 0.1060
2022-10-02 19:38:27 - train: epoch 0022, iter [00100, 01251], lr: 0.000578, loss: 0.1023
2022-10-02 19:38:55 - train: epoch 0022, iter [00110, 01251], lr: 0.000578, loss: 0.1165
2022-10-02 19:39:23 - train: epoch 0022, iter [00120, 01251], lr: 0.000578, loss: 0.1077
2022-10-02 19:39:51 - train: epoch 0022, iter [00130, 01251], lr: 0.000578, loss: 0.1021
2022-10-02 19:40:19 - train: epoch 0022, iter [00140, 01251], lr: 0.000578, loss: 0.1133
2022-10-02 19:40:47 - train: epoch 0022, iter [00150, 01251], lr: 0.000578, loss: 0.1023
2022-10-02 19:41:15 - train: epoch 0022, iter [00160, 01251], lr: 0.000578, loss: 0.1092
2022-10-02 19:41:43 - train: epoch 0022, iter [00170, 01251], lr: 0.000578, loss: 0.1020
2022-10-02 19:42:11 - train: epoch 0022, iter [00180, 01251], lr: 0.000578, loss: 0.1028
2022-10-02 19:42:39 - train: epoch 0022, iter [00190, 01251], lr: 0.000578, loss: 0.1012
2022-10-02 19:43:08 - train: epoch 0022, iter [00200, 01251], lr: 0.000578, loss: 0.1149
2022-10-02 19:43:36 - train: epoch 0022, iter [00210, 01251], lr: 0.000577, loss: 0.1043
2022-10-02 19:44:04 - train: epoch 0022, iter [00220, 01251], lr: 0.000577, loss: 0.1013
2022-10-02 19:44:32 - train: epoch 0022, iter [00230, 01251], lr: 0.000577, loss: 0.1071
2022-10-02 19:45:00 - train: epoch 0022, iter [00240, 01251], lr: 0.000577, loss: 0.1072
2022-10-02 19:45:29 - train: epoch 0022, iter [00250, 01251], lr: 0.000577, loss: 0.1170
2022-10-02 19:45:57 - train: epoch 0022, iter [00260, 01251], lr: 0.000577, loss: 0.1046
2022-10-02 19:46:25 - train: epoch 0022, iter [00270, 01251], lr: 0.000577, loss: 0.1049
2022-10-02 19:46:53 - train: epoch 0022, iter [00280, 01251], lr: 0.000577, loss: 0.1069
2022-10-02 19:47:21 - train: epoch 0022, iter [00290, 01251], lr: 0.000577, loss: 0.1091
2022-10-02 19:47:49 - train: epoch 0022, iter [00300, 01251], lr: 0.000577, loss: 0.1070
2022-10-02 19:48:18 - train: epoch 0022, iter [00310, 01251], lr: 0.000577, loss: 0.1084
2022-10-02 19:48:46 - train: epoch 0022, iter [00320, 01251], lr: 0.000577, loss: 0.1053
2022-10-02 19:49:14 - train: epoch 0022, iter [00330, 01251], lr: 0.000577, loss: 0.1078
2022-10-02 19:49:42 - train: epoch 0022, iter [00340, 01251], lr: 0.000577, loss: 0.0985
2022-10-02 19:50:10 - train: epoch 0022, iter [00350, 01251], lr: 0.000577, loss: 0.1038
2022-10-02 19:50:38 - train: epoch 0022, iter [00360, 01251], lr: 0.000577, loss: 0.1063
2022-10-02 19:51:06 - train: epoch 0022, iter [00370, 01251], lr: 0.000577, loss: 0.1093
2022-10-02 19:51:34 - train: epoch 0022, iter [00380, 01251], lr: 0.000577, loss: 0.1084
2022-10-02 19:52:02 - train: epoch 0022, iter [00390, 01251], lr: 0.000577, loss: 0.1042
2022-10-02 19:52:30 - train: epoch 0022, iter [00400, 01251], lr: 0.000577, loss: 0.0929
2022-10-02 19:52:59 - train: epoch 0022, iter [00410, 01251], lr: 0.000577, loss: 0.1135
2022-10-02 19:53:27 - train: epoch 0022, iter [00420, 01251], lr: 0.000577, loss: 0.1094
2022-10-02 19:53:55 - train: epoch 0022, iter [00430, 01251], lr: 0.000577, loss: 0.1097
2022-10-02 19:54:23 - train: epoch 0022, iter [00440, 01251], lr: 0.000577, loss: 0.1082
2022-10-02 19:54:51 - train: epoch 0022, iter [00450, 01251], lr: 0.000577, loss: 0.1001
2022-10-02 19:55:19 - train: epoch 0022, iter [00460, 01251], lr: 0.000577, loss: 0.1070
2022-10-02 19:55:48 - train: epoch 0022, iter [00470, 01251], lr: 0.000577, loss: 0.1083
2022-10-02 19:56:16 - train: epoch 0022, iter [00480, 01251], lr: 0.000577, loss: 0.1086
2022-10-02 19:56:44 - train: epoch 0022, iter [00490, 01251], lr: 0.000577, loss: 0.1015
2022-10-02 19:57:12 - train: epoch 0022, iter [00500, 01251], lr: 0.000577, loss: 0.1082
2022-10-02 19:57:40 - train: epoch 0022, iter [00510, 01251], lr: 0.000577, loss: 0.1037
2022-10-02 19:58:08 - train: epoch 0022, iter [00520, 01251], lr: 0.000576, loss: 0.1129
2022-10-02 19:58:36 - train: epoch 0022, iter [00530, 01251], lr: 0.000576, loss: 0.1043
2022-10-02 19:59:05 - train: epoch 0022, iter [00540, 01251], lr: 0.000576, loss: 0.0991
2022-10-02 19:59:33 - train: epoch 0022, iter [00550, 01251], lr: 0.000576, loss: 0.1068
2022-10-02 20:00:01 - train: epoch 0022, iter [00560, 01251], lr: 0.000576, loss: 0.1067
2022-10-02 20:00:29 - train: epoch 0022, iter [00570, 01251], lr: 0.000576, loss: 0.0960
2022-10-02 20:00:57 - train: epoch 0022, iter [00580, 01251], lr: 0.000576, loss: 0.0993
2022-10-02 20:01:26 - train: epoch 0022, iter [00590, 01251], lr: 0.000576, loss: 0.1081
2022-10-02 20:01:54 - train: epoch 0022, iter [00600, 01251], lr: 0.000576, loss: 0.0984
2022-10-02 20:02:22 - train: epoch 0022, iter [00610, 01251], lr: 0.000576, loss: 0.1024
2022-10-02 20:02:50 - train: epoch 0022, iter [00620, 01251], lr: 0.000576, loss: 0.1006
2022-10-02 20:03:18 - train: epoch 0022, iter [00630, 01251], lr: 0.000576, loss: 0.1066
2022-10-02 20:03:46 - train: epoch 0022, iter [00640, 01251], lr: 0.000576, loss: 0.1014
2022-10-02 20:04:15 - train: epoch 0022, iter [00650, 01251], lr: 0.000576, loss: 0.1106
2022-10-02 20:04:43 - train: epoch 0022, iter [00660, 01251], lr: 0.000576, loss: 0.1110
2022-10-02 20:05:11 - train: epoch 0022, iter [00670, 01251], lr: 0.000576, loss: 0.1040
2022-10-02 20:05:39 - train: epoch 0022, iter [00680, 01251], lr: 0.000576, loss: 0.1003
2022-10-02 20:06:08 - train: epoch 0022, iter [00690, 01251], lr: 0.000576, loss: 0.1043
2022-10-02 20:06:36 - train: epoch 0022, iter [00700, 01251], lr: 0.000576, loss: 0.1092
2022-10-02 20:07:04 - train: epoch 0022, iter [00710, 01251], lr: 0.000576, loss: 0.1107
2022-10-02 20:07:32 - train: epoch 0022, iter [00720, 01251], lr: 0.000576, loss: 0.1052
2022-10-02 20:08:00 - train: epoch 0022, iter [00730, 01251], lr: 0.000576, loss: 0.1051
2022-10-02 20:08:28 - train: epoch 0022, iter [00740, 01251], lr: 0.000576, loss: 0.0984
2022-10-02 20:08:57 - train: epoch 0022, iter [00750, 01251], lr: 0.000576, loss: 0.1035
2022-10-02 20:09:25 - train: epoch 0022, iter [00760, 01251], lr: 0.000576, loss: 0.1013
2022-10-02 20:09:53 - train: epoch 0022, iter [00770, 01251], lr: 0.000576, loss: 0.0949
2022-10-02 20:10:22 - train: epoch 0022, iter [00780, 01251], lr: 0.000576, loss: 0.1007
2022-10-02 20:10:50 - train: epoch 0022, iter [00790, 01251], lr: 0.000576, loss: 0.1030
2022-10-02 20:11:18 - train: epoch 0022, iter [00800, 01251], lr: 0.000576, loss: 0.1015
2022-10-02 20:11:46 - train: epoch 0022, iter [00810, 01251], lr: 0.000576, loss: 0.1025
2022-10-02 20:12:14 - train: epoch 0022, iter [00820, 01251], lr: 0.000576, loss: 0.0987
2022-10-02 20:12:42 - train: epoch 0022, iter [00830, 01251], lr: 0.000575, loss: 0.1049
2022-10-02 20:13:10 - train: epoch 0022, iter [00840, 01251], lr: 0.000575, loss: 0.1095
2022-10-02 20:13:38 - train: epoch 0022, iter [00850, 01251], lr: 0.000575, loss: 0.1046
2022-10-02 20:14:07 - train: epoch 0022, iter [00860, 01251], lr: 0.000575, loss: 0.1000
2022-10-02 20:14:35 - train: epoch 0022, iter [00870, 01251], lr: 0.000575, loss: 0.1046
2022-10-02 20:15:03 - train: epoch 0022, iter [00880, 01251], lr: 0.000575, loss: 0.1093
2022-10-02 20:15:31 - train: epoch 0022, iter [00890, 01251], lr: 0.000575, loss: 0.1088
2022-10-02 20:15:59 - train: epoch 0022, iter [00900, 01251], lr: 0.000575, loss: 0.1140
2022-10-02 20:16:27 - train: epoch 0022, iter [00910, 01251], lr: 0.000575, loss: 0.1023
2022-10-02 20:16:55 - train: epoch 0022, iter [00920, 01251], lr: 0.000575, loss: 0.1071
2022-10-02 20:17:23 - train: epoch 0022, iter [00930, 01251], lr: 0.000575, loss: 0.1049
2022-10-02 20:17:51 - train: epoch 0022, iter [00940, 01251], lr: 0.000575, loss: 0.1024
2022-10-02 20:18:20 - train: epoch 0022, iter [00950, 01251], lr: 0.000575, loss: 0.1040
2022-10-02 20:18:48 - train: epoch 0022, iter [00960, 01251], lr: 0.000575, loss: 0.1037
2022-10-02 20:19:16 - train: epoch 0022, iter [00970, 01251], lr: 0.000575, loss: 0.1047
2022-10-02 20:19:44 - train: epoch 0022, iter [00980, 01251], lr: 0.000575, loss: 0.1042
2022-10-02 20:20:12 - train: epoch 0022, iter [00990, 01251], lr: 0.000575, loss: 0.1042
2022-10-02 20:20:40 - train: epoch 0022, iter [01000, 01251], lr: 0.000575, loss: 0.1007
2022-10-02 20:21:08 - train: epoch 0022, iter [01010, 01251], lr: 0.000575, loss: 0.1067
2022-10-02 20:21:36 - train: epoch 0022, iter [01020, 01251], lr: 0.000575, loss: 0.1049
2022-10-02 20:22:04 - train: epoch 0022, iter [01030, 01251], lr: 0.000575, loss: 0.1029
2022-10-02 20:22:32 - train: epoch 0022, iter [01040, 01251], lr: 0.000575, loss: 0.1044
2022-10-02 20:23:00 - train: epoch 0022, iter [01050, 01251], lr: 0.000575, loss: 0.0964
2022-10-02 20:23:28 - train: epoch 0022, iter [01060, 01251], lr: 0.000575, loss: 0.1012
2022-10-02 20:23:56 - train: epoch 0022, iter [01070, 01251], lr: 0.000575, loss: 0.1081
2022-10-02 20:24:24 - train: epoch 0022, iter [01080, 01251], lr: 0.000575, loss: 0.0980
2022-10-02 20:24:52 - train: epoch 0022, iter [01090, 01251], lr: 0.000575, loss: 0.1107
2022-10-02 20:25:20 - train: epoch 0022, iter [01100, 01251], lr: 0.000575, loss: 0.0970
2022-10-02 20:25:49 - train: epoch 0022, iter [01110, 01251], lr: 0.000575, loss: 0.1123
2022-10-02 20:26:17 - train: epoch 0022, iter [01120, 01251], lr: 0.000575, loss: 0.1026
2022-10-02 20:26:45 - train: epoch 0022, iter [01130, 01251], lr: 0.000574, loss: 0.1013
2022-10-02 20:27:13 - train: epoch 0022, iter [01140, 01251], lr: 0.000574, loss: 0.1010
2022-10-02 20:27:41 - train: epoch 0022, iter [01150, 01251], lr: 0.000574, loss: 0.1035
2022-10-02 20:28:09 - train: epoch 0022, iter [01160, 01251], lr: 0.000574, loss: 0.1035
2022-10-02 20:28:37 - train: epoch 0022, iter [01170, 01251], lr: 0.000574, loss: 0.1129
2022-10-02 20:29:05 - train: epoch 0022, iter [01180, 01251], lr: 0.000574, loss: 0.1055
2022-10-02 20:29:33 - train: epoch 0022, iter [01190, 01251], lr: 0.000574, loss: 0.1004
2022-10-02 20:30:01 - train: epoch 0022, iter [01200, 01251], lr: 0.000574, loss: 0.1034
2022-10-02 20:30:29 - train: epoch 0022, iter [01210, 01251], lr: 0.000574, loss: 0.1122
2022-10-02 20:30:57 - train: epoch 0022, iter [01220, 01251], lr: 0.000574, loss: 0.1063
2022-10-02 20:31:25 - train: epoch 0022, iter [01230, 01251], lr: 0.000574, loss: 0.1001
2022-10-02 20:31:53 - train: epoch 0022, iter [01240, 01251], lr: 0.000574, loss: 0.1100
2022-10-02 20:32:21 - train: epoch 0022, iter [01250, 01251], lr: 0.000574, loss: 0.1026
2022-10-02 20:32:26 - train: epoch 022, train_loss: 0.1047
2022-10-02 20:32:27 - until epoch: 022, best_loss: 0.1047
2022-10-02 20:32:27 - epoch 023 lr: 0.000574
2022-10-02 20:33:02 - train: epoch 0023, iter [00010, 01251], lr: 0.000574, loss: 0.1016
2022-10-02 20:33:31 - train: epoch 0023, iter [00020, 01251], lr: 0.000574, loss: 0.0966
2022-10-02 20:33:59 - train: epoch 0023, iter [00030, 01251], lr: 0.000574, loss: 0.1052
2022-10-02 20:34:27 - train: epoch 0023, iter [00040, 01251], lr: 0.000574, loss: 0.1083
2022-10-02 20:34:55 - train: epoch 0023, iter [00050, 01251], lr: 0.000574, loss: 0.0940
2022-10-02 20:35:23 - train: epoch 0023, iter [00060, 01251], lr: 0.000574, loss: 0.0954
2022-10-02 20:35:51 - train: epoch 0023, iter [00070, 01251], lr: 0.000574, loss: 0.1082
2022-10-02 20:36:19 - train: epoch 0023, iter [00080, 01251], lr: 0.000574, loss: 0.0986
2022-10-02 20:36:47 - train: epoch 0023, iter [00090, 01251], lr: 0.000574, loss: 0.0992
2022-10-02 20:37:16 - train: epoch 0023, iter [00100, 01251], lr: 0.000574, loss: 0.0944
2022-10-02 20:37:44 - train: epoch 0023, iter [00110, 01251], lr: 0.000574, loss: 0.1057
2022-10-02 20:38:12 - train: epoch 0023, iter [00120, 01251], lr: 0.000574, loss: 0.1061
2022-10-02 20:38:40 - train: epoch 0023, iter [00130, 01251], lr: 0.000574, loss: 0.0996
2022-10-02 20:39:08 - train: epoch 0023, iter [00140, 01251], lr: 0.000574, loss: 0.1149
2022-10-02 20:39:36 - train: epoch 0023, iter [00150, 01251], lr: 0.000574, loss: 0.1037
2022-10-02 20:40:04 - train: epoch 0023, iter [00160, 01251], lr: 0.000574, loss: 0.1013
2022-10-02 20:40:32 - train: epoch 0023, iter [00170, 01251], lr: 0.000573, loss: 0.1131
2022-10-02 20:41:00 - train: epoch 0023, iter [00180, 01251], lr: 0.000573, loss: 0.1120
2022-10-02 20:41:28 - train: epoch 0023, iter [00190, 01251], lr: 0.000573, loss: 0.1060
2022-10-02 20:41:56 - train: epoch 0023, iter [00200, 01251], lr: 0.000573, loss: 0.1066
2022-10-02 20:42:24 - train: epoch 0023, iter [00210, 01251], lr: 0.000573, loss: 0.1119
2022-10-02 20:42:52 - train: epoch 0023, iter [00220, 01251], lr: 0.000573, loss: 0.1054
2022-10-02 20:43:20 - train: epoch 0023, iter [00230, 01251], lr: 0.000573, loss: 0.1105
2022-10-02 20:43:48 - train: epoch 0023, iter [00240, 01251], lr: 0.000573, loss: 0.1051
2022-10-02 20:44:16 - train: epoch 0023, iter [00250, 01251], lr: 0.000573, loss: 0.1050
2022-10-02 20:44:44 - train: epoch 0023, iter [00260, 01251], lr: 0.000573, loss: 0.1081
2022-10-02 20:45:12 - train: epoch 0023, iter [00270, 01251], lr: 0.000573, loss: 0.1127
2022-10-02 20:45:40 - train: epoch 0023, iter [00280, 01251], lr: 0.000573, loss: 0.1007
2022-10-02 20:46:08 - train: epoch 0023, iter [00290, 01251], lr: 0.000573, loss: 0.1144
2022-10-02 20:46:36 - train: epoch 0023, iter [00300, 01251], lr: 0.000573, loss: 0.1006
2022-10-02 20:47:04 - train: epoch 0023, iter [00310, 01251], lr: 0.000573, loss: 0.1006
2022-10-02 20:47:32 - train: epoch 0023, iter [00320, 01251], lr: 0.000573, loss: 0.1030
2022-10-02 20:48:00 - train: epoch 0023, iter [00330, 01251], lr: 0.000573, loss: 0.1019
2022-10-02 20:48:28 - train: epoch 0023, iter [00340, 01251], lr: 0.000573, loss: 0.1024
2022-10-02 20:48:56 - train: epoch 0023, iter [00350, 01251], lr: 0.000573, loss: 0.1019
2022-10-02 20:49:24 - train: epoch 0023, iter [00360, 01251], lr: 0.000573, loss: 0.1087
2022-10-02 20:49:52 - train: epoch 0023, iter [00370, 01251], lr: 0.000573, loss: 0.1050
2022-10-02 20:50:20 - train: epoch 0023, iter [00380, 01251], lr: 0.000573, loss: 0.1032
2022-10-02 20:50:48 - train: epoch 0023, iter [00390, 01251], lr: 0.000573, loss: 0.1039
2022-10-02 20:51:16 - train: epoch 0023, iter [00400, 01251], lr: 0.000573, loss: 0.1091
2022-10-02 20:51:44 - train: epoch 0023, iter [00410, 01251], lr: 0.000573, loss: 0.1069
2022-10-02 20:52:12 - train: epoch 0023, iter [00420, 01251], lr: 0.000573, loss: 0.1019
2022-10-02 20:52:40 - train: epoch 0023, iter [00430, 01251], lr: 0.000573, loss: 0.1056
2022-10-02 20:53:07 - train: epoch 0023, iter [00440, 01251], lr: 0.000573, loss: 0.1044
2022-10-02 20:53:35 - train: epoch 0023, iter [00450, 01251], lr: 0.000573, loss: 0.1014
2022-10-02 20:54:03 - train: epoch 0023, iter [00460, 01251], lr: 0.000572, loss: 0.1016
2022-10-02 20:54:31 - train: epoch 0023, iter [00470, 01251], lr: 0.000572, loss: 0.0964
2022-10-02 20:54:59 - train: epoch 0023, iter [00480, 01251], lr: 0.000572, loss: 0.1025
2022-10-02 20:55:27 - train: epoch 0023, iter [00490, 01251], lr: 0.000572, loss: 0.0918
2022-10-02 20:55:55 - train: epoch 0023, iter [00500, 01251], lr: 0.000572, loss: 0.1033
2022-10-02 20:56:24 - train: epoch 0023, iter [00510, 01251], lr: 0.000572, loss: 0.1105
2022-10-02 20:56:52 - train: epoch 0023, iter [00520, 01251], lr: 0.000572, loss: 0.1027
2022-10-02 20:57:20 - train: epoch 0023, iter [00530, 01251], lr: 0.000572, loss: 0.1058
2022-10-02 20:57:48 - train: epoch 0023, iter [00540, 01251], lr: 0.000572, loss: 0.0960
2022-10-02 20:58:16 - train: epoch 0023, iter [00550, 01251], lr: 0.000572, loss: 0.0986
2022-10-02 20:58:44 - train: epoch 0023, iter [00560, 01251], lr: 0.000572, loss: 0.1064
2022-10-02 20:59:12 - train: epoch 0023, iter [00570, 01251], lr: 0.000572, loss: 0.1055
2022-10-02 20:59:40 - train: epoch 0023, iter [00580, 01251], lr: 0.000572, loss: 0.0963
2022-10-02 21:00:08 - train: epoch 0023, iter [00590, 01251], lr: 0.000572, loss: 0.1004
2022-10-02 21:00:36 - train: epoch 0023, iter [00600, 01251], lr: 0.000572, loss: 0.1032
2022-10-02 21:01:04 - train: epoch 0023, iter [00610, 01251], lr: 0.000572, loss: 0.0982
2022-10-02 21:01:32 - train: epoch 0023, iter [00620, 01251], lr: 0.000572, loss: 0.1090
2022-10-02 21:02:00 - train: epoch 0023, iter [00630, 01251], lr: 0.000572, loss: 0.1180
2022-10-02 21:02:28 - train: epoch 0023, iter [00640, 01251], lr: 0.000572, loss: 0.1086
2022-10-02 21:02:56 - train: epoch 0023, iter [00650, 01251], lr: 0.000572, loss: 0.1055
2022-10-02 21:03:24 - train: epoch 0023, iter [00660, 01251], lr: 0.000572, loss: 0.1078
2022-10-02 21:03:51 - train: epoch 0023, iter [00670, 01251], lr: 0.000572, loss: 0.1076
2022-10-02 21:04:19 - train: epoch 0023, iter [00680, 01251], lr: 0.000572, loss: 0.1053
2022-10-02 21:04:47 - train: epoch 0023, iter [00690, 01251], lr: 0.000572, loss: 0.1066
2022-10-02 21:05:15 - train: epoch 0023, iter [00700, 01251], lr: 0.000572, loss: 0.1026
2022-10-02 21:05:43 - train: epoch 0023, iter [00710, 01251], lr: 0.000572, loss: 0.0974
2022-10-02 21:06:11 - train: epoch 0023, iter [00720, 01251], lr: 0.000572, loss: 0.1002
2022-10-02 21:06:39 - train: epoch 0023, iter [00730, 01251], lr: 0.000572, loss: 0.1012
2022-10-02 21:07:08 - train: epoch 0023, iter [00740, 01251], lr: 0.000571, loss: 0.0978
2022-10-02 21:07:36 - train: epoch 0023, iter [00750, 01251], lr: 0.000571, loss: 0.0996
2022-10-02 21:08:04 - train: epoch 0023, iter [00760, 01251], lr: 0.000571, loss: 0.0993
2022-10-02 21:08:32 - train: epoch 0023, iter [00770, 01251], lr: 0.000571, loss: 0.1014
2022-10-02 21:09:00 - train: epoch 0023, iter [00780, 01251], lr: 0.000571, loss: 0.1053
2022-10-02 21:09:28 - train: epoch 0023, iter [00790, 01251], lr: 0.000571, loss: 0.1084
2022-10-02 21:09:56 - train: epoch 0023, iter [00800, 01251], lr: 0.000571, loss: 0.1044
2022-10-02 21:10:24 - train: epoch 0023, iter [00810, 01251], lr: 0.000571, loss: 0.0965
2022-10-02 21:10:52 - train: epoch 0023, iter [00820, 01251], lr: 0.000571, loss: 0.0932
2022-10-02 21:11:20 - train: epoch 0023, iter [00830, 01251], lr: 0.000571, loss: 0.1023
2022-10-02 21:11:48 - train: epoch 0023, iter [00840, 01251], lr: 0.000571, loss: 0.1037
2022-10-02 21:12:16 - train: epoch 0023, iter [00850, 01251], lr: 0.000571, loss: 0.0955
2022-10-02 21:12:44 - train: epoch 0023, iter [00860, 01251], lr: 0.000571, loss: 0.1133
2022-10-02 21:13:12 - train: epoch 0023, iter [00870, 01251], lr: 0.000571, loss: 0.1073
2022-10-02 21:13:40 - train: epoch 0023, iter [00880, 01251], lr: 0.000571, loss: 0.1129
2022-10-02 21:14:08 - train: epoch 0023, iter [00890, 01251], lr: 0.000571, loss: 0.1035
2022-10-02 21:14:36 - train: epoch 0023, iter [00900, 01251], lr: 0.000571, loss: 0.1136
2022-10-02 21:15:04 - train: epoch 0023, iter [00910, 01251], lr: 0.000571, loss: 0.0993
2022-10-02 21:15:32 - train: epoch 0023, iter [00920, 01251], lr: 0.000571, loss: 0.1035
2022-10-02 21:16:00 - train: epoch 0023, iter [00930, 01251], lr: 0.000571, loss: 0.1027
2022-10-02 21:16:28 - train: epoch 0023, iter [00940, 01251], lr: 0.000571, loss: 0.1044
2022-10-02 21:16:56 - train: epoch 0023, iter [00950, 01251], lr: 0.000571, loss: 0.1006
2022-10-02 21:17:24 - train: epoch 0023, iter [00960, 01251], lr: 0.000571, loss: 0.0956
2022-10-02 21:17:52 - train: epoch 0023, iter [00970, 01251], lr: 0.000571, loss: 0.1113
2022-10-02 21:18:20 - train: epoch 0023, iter [00980, 01251], lr: 0.000571, loss: 0.0962
2022-10-02 21:18:47 - train: epoch 0023, iter [00990, 01251], lr: 0.000571, loss: 0.1040
2022-10-02 21:19:16 - train: epoch 0023, iter [01000, 01251], lr: 0.000571, loss: 0.1053
2022-10-02 21:19:43 - train: epoch 0023, iter [01010, 01251], lr: 0.000571, loss: 0.1027
2022-10-02 21:20:11 - train: epoch 0023, iter [01020, 01251], lr: 0.000570, loss: 0.0997
2022-10-02 21:20:39 - train: epoch 0023, iter [01030, 01251], lr: 0.000570, loss: 0.1019
2022-10-02 21:21:07 - train: epoch 0023, iter [01040, 01251], lr: 0.000570, loss: 0.1074
2022-10-02 21:21:35 - train: epoch 0023, iter [01050, 01251], lr: 0.000570, loss: 0.1012
2022-10-02 21:22:03 - train: epoch 0023, iter [01060, 01251], lr: 0.000570, loss: 0.1006
2022-10-02 21:22:31 - train: epoch 0023, iter [01070, 01251], lr: 0.000570, loss: 0.0974
2022-10-02 21:22:59 - train: epoch 0023, iter [01080, 01251], lr: 0.000570, loss: 0.1104
2022-10-02 21:23:27 - train: epoch 0023, iter [01090, 01251], lr: 0.000570, loss: 0.0999
2022-10-02 21:23:55 - train: epoch 0023, iter [01100, 01251], lr: 0.000570, loss: 0.1103
2022-10-02 21:24:23 - train: epoch 0023, iter [01110, 01251], lr: 0.000570, loss: 0.0996
2022-10-02 21:24:51 - train: epoch 0023, iter [01120, 01251], lr: 0.000570, loss: 0.0957
2022-10-02 21:25:20 - train: epoch 0023, iter [01130, 01251], lr: 0.000570, loss: 0.1132
2022-10-02 21:25:48 - train: epoch 0023, iter [01140, 01251], lr: 0.000570, loss: 0.1061
2022-10-02 21:26:16 - train: epoch 0023, iter [01150, 01251], lr: 0.000570, loss: 0.1129
2022-10-02 21:26:44 - train: epoch 0023, iter [01160, 01251], lr: 0.000570, loss: 0.1034
2022-10-02 21:27:12 - train: epoch 0023, iter [01170, 01251], lr: 0.000570, loss: 0.1042
2022-10-02 21:27:40 - train: epoch 0023, iter [01180, 01251], lr: 0.000570, loss: 0.1069
2022-10-02 21:28:08 - train: epoch 0023, iter [01190, 01251], lr: 0.000570, loss: 0.1030
2022-10-02 21:28:37 - train: epoch 0023, iter [01200, 01251], lr: 0.000570, loss: 0.0966
2022-10-02 21:29:05 - train: epoch 0023, iter [01210, 01251], lr: 0.000570, loss: 0.1035
2022-10-02 21:29:33 - train: epoch 0023, iter [01220, 01251], lr: 0.000570, loss: 0.0951
2022-10-02 21:30:01 - train: epoch 0023, iter [01230, 01251], lr: 0.000570, loss: 0.0984
2022-10-02 21:30:30 - train: epoch 0023, iter [01240, 01251], lr: 0.000570, loss: 0.1014
2022-10-02 21:30:58 - train: epoch 0023, iter [01250, 01251], lr: 0.000570, loss: 0.1060
2022-10-02 21:31:02 - train: epoch 023, train_loss: 0.1038
2022-10-02 21:31:04 - until epoch: 023, best_loss: 0.1038
2022-10-02 21:31:04 - epoch 024 lr: 0.000570
2022-10-02 21:31:39 - train: epoch 0024, iter [00010, 01251], lr: 0.000570, loss: 0.1016
2022-10-02 21:32:07 - train: epoch 0024, iter [00020, 01251], lr: 0.000570, loss: 0.1091
2022-10-02 21:32:36 - train: epoch 0024, iter [00030, 01251], lr: 0.000570, loss: 0.0991
2022-10-02 21:33:04 - train: epoch 0024, iter [00040, 01251], lr: 0.000569, loss: 0.1057
2022-10-02 21:33:32 - train: epoch 0024, iter [00050, 01251], lr: 0.000569, loss: 0.1122
2022-10-02 21:34:01 - train: epoch 0024, iter [00060, 01251], lr: 0.000569, loss: 0.1031
2022-10-02 21:34:29 - train: epoch 0024, iter [00070, 01251], lr: 0.000569, loss: 0.1018
2022-10-02 21:34:57 - train: epoch 0024, iter [00080, 01251], lr: 0.000569, loss: 0.1042
2022-10-02 21:35:25 - train: epoch 0024, iter [00090, 01251], lr: 0.000569, loss: 0.1001
2022-10-02 21:35:54 - train: epoch 0024, iter [00100, 01251], lr: 0.000569, loss: 0.0985
2022-10-02 21:36:22 - train: epoch 0024, iter [00110, 01251], lr: 0.000569, loss: 0.0978
2022-10-02 21:36:50 - train: epoch 0024, iter [00120, 01251], lr: 0.000569, loss: 0.0978
2022-10-02 21:37:18 - train: epoch 0024, iter [00130, 01251], lr: 0.000569, loss: 0.1125
2022-10-02 21:37:46 - train: epoch 0024, iter [00140, 01251], lr: 0.000569, loss: 0.1041
2022-10-02 21:38:14 - train: epoch 0024, iter [00150, 01251], lr: 0.000569, loss: 0.1082
2022-10-02 21:38:42 - train: epoch 0024, iter [00160, 01251], lr: 0.000569, loss: 0.1019
2022-10-02 21:39:11 - train: epoch 0024, iter [00170, 01251], lr: 0.000569, loss: 0.1136
2022-10-02 21:39:39 - train: epoch 0024, iter [00180, 01251], lr: 0.000569, loss: 0.1068
2022-10-02 21:40:07 - train: epoch 0024, iter [00190, 01251], lr: 0.000569, loss: 0.0970
2022-10-02 21:40:35 - train: epoch 0024, iter [00200, 01251], lr: 0.000569, loss: 0.1102
2022-10-02 21:41:04 - train: epoch 0024, iter [00210, 01251], lr: 0.000569, loss: 0.1043
2022-10-02 21:41:32 - train: epoch 0024, iter [00220, 01251], lr: 0.000569, loss: 0.0934
2022-10-02 21:42:00 - train: epoch 0024, iter [00230, 01251], lr: 0.000569, loss: 0.0971
2022-10-02 21:42:28 - train: epoch 0024, iter [00240, 01251], lr: 0.000569, loss: 0.1031
2022-10-02 21:42:56 - train: epoch 0024, iter [00250, 01251], lr: 0.000569, loss: 0.1080
2022-10-02 21:43:25 - train: epoch 0024, iter [00260, 01251], lr: 0.000569, loss: 0.1125
2022-10-02 21:43:53 - train: epoch 0024, iter [00270, 01251], lr: 0.000569, loss: 0.0968
2022-10-02 21:44:21 - train: epoch 0024, iter [00280, 01251], lr: 0.000569, loss: 0.0997
2022-10-02 21:44:49 - train: epoch 0024, iter [00290, 01251], lr: 0.000569, loss: 0.1048
2022-10-02 21:45:18 - train: epoch 0024, iter [00300, 01251], lr: 0.000569, loss: 0.0967
2022-10-02 21:45:46 - train: epoch 0024, iter [00310, 01251], lr: 0.000568, loss: 0.1054
2022-10-02 21:46:14 - train: epoch 0024, iter [00320, 01251], lr: 0.000568, loss: 0.0985
2022-10-02 21:46:42 - train: epoch 0024, iter [00330, 01251], lr: 0.000568, loss: 0.0964
2022-10-02 21:47:10 - train: epoch 0024, iter [00340, 01251], lr: 0.000568, loss: 0.1049
2022-10-02 21:47:39 - train: epoch 0024, iter [00350, 01251], lr: 0.000568, loss: 0.1016
2022-10-02 21:48:07 - train: epoch 0024, iter [00360, 01251], lr: 0.000568, loss: 0.1090
2022-10-02 21:48:35 - train: epoch 0024, iter [00370, 01251], lr: 0.000568, loss: 0.1055
2022-10-02 21:49:04 - train: epoch 0024, iter [00380, 01251], lr: 0.000568, loss: 0.1027
2022-10-02 21:49:32 - train: epoch 0024, iter [00390, 01251], lr: 0.000568, loss: 0.0994
2022-10-02 21:50:00 - train: epoch 0024, iter [00400, 01251], lr: 0.000568, loss: 0.1062
2022-10-02 21:50:29 - train: epoch 0024, iter [00410, 01251], lr: 0.000568, loss: 0.1032
2022-10-02 21:50:57 - train: epoch 0024, iter [00420, 01251], lr: 0.000568, loss: 0.1040
2022-10-02 21:51:25 - train: epoch 0024, iter [00430, 01251], lr: 0.000568, loss: 0.1059
2022-10-02 21:51:54 - train: epoch 0024, iter [00440, 01251], lr: 0.000568, loss: 0.0955
2022-10-02 21:52:22 - train: epoch 0024, iter [00450, 01251], lr: 0.000568, loss: 0.1014
2022-10-02 21:52:50 - train: epoch 0024, iter [00460, 01251], lr: 0.000568, loss: 0.0998
2022-10-02 21:53:18 - train: epoch 0024, iter [00470, 01251], lr: 0.000568, loss: 0.1007
2022-10-02 21:53:46 - train: epoch 0024, iter [00480, 01251], lr: 0.000568, loss: 0.0973
2022-10-02 21:54:15 - train: epoch 0024, iter [00490, 01251], lr: 0.000568, loss: 0.0986
2022-10-02 21:54:43 - train: epoch 0024, iter [00500, 01251], lr: 0.000568, loss: 0.0965
2022-10-02 21:55:11 - train: epoch 0024, iter [00510, 01251], lr: 0.000568, loss: 0.0986
2022-10-02 21:55:40 - train: epoch 0024, iter [00520, 01251], lr: 0.000568, loss: 0.1047
2022-10-02 21:56:08 - train: epoch 0024, iter [00530, 01251], lr: 0.000568, loss: 0.1027
2022-10-02 21:56:36 - train: epoch 0024, iter [00540, 01251], lr: 0.000568, loss: 0.1047
2022-10-02 21:57:04 - train: epoch 0024, iter [00550, 01251], lr: 0.000568, loss: 0.1020
2022-10-02 21:57:32 - train: epoch 0024, iter [00560, 01251], lr: 0.000568, loss: 0.0991
2022-10-02 21:58:01 - train: epoch 0024, iter [00570, 01251], lr: 0.000568, loss: 0.1010
2022-10-02 21:58:29 - train: epoch 0024, iter [00580, 01251], lr: 0.000567, loss: 0.1007
2022-10-02 21:58:57 - train: epoch 0024, iter [00590, 01251], lr: 0.000567, loss: 0.1089
2022-10-02 21:59:25 - train: epoch 0024, iter [00600, 01251], lr: 0.000567, loss: 0.1009
2022-10-02 21:59:54 - train: epoch 0024, iter [00610, 01251], lr: 0.000567, loss: 0.1061
2022-10-02 22:00:22 - train: epoch 0024, iter [00620, 01251], lr: 0.000567, loss: 0.0986
2022-10-02 22:00:50 - train: epoch 0024, iter [00630, 01251], lr: 0.000567, loss: 0.0988
2022-10-02 22:01:18 - train: epoch 0024, iter [00640, 01251], lr: 0.000567, loss: 0.1035
2022-10-02 22:01:46 - train: epoch 0024, iter [00650, 01251], lr: 0.000567, loss: 0.0948
2022-10-02 22:02:15 - train: epoch 0024, iter [00660, 01251], lr: 0.000567, loss: 0.0989
2022-10-02 22:02:43 - train: epoch 0024, iter [00670, 01251], lr: 0.000567, loss: 0.1093
2022-10-02 22:03:11 - train: epoch 0024, iter [00680, 01251], lr: 0.000567, loss: 0.1034
2022-10-02 22:03:40 - train: epoch 0024, iter [00690, 01251], lr: 0.000567, loss: 0.1017
2022-10-02 22:04:08 - train: epoch 0024, iter [00700, 01251], lr: 0.000567, loss: 0.0946
2022-10-02 22:04:36 - train: epoch 0024, iter [00710, 01251], lr: 0.000567, loss: 0.1004
2022-10-02 22:05:05 - train: epoch 0024, iter [00720, 01251], lr: 0.000567, loss: 0.1036
2022-10-02 22:05:33 - train: epoch 0024, iter [00730, 01251], lr: 0.000567, loss: 0.0968
2022-10-02 22:06:02 - train: epoch 0024, iter [00740, 01251], lr: 0.000567, loss: 0.1034
2022-10-02 22:06:30 - train: epoch 0024, iter [00750, 01251], lr: 0.000567, loss: 0.0961
2022-10-02 22:06:58 - train: epoch 0024, iter [00760, 01251], lr: 0.000567, loss: 0.1005
2022-10-02 22:07:26 - train: epoch 0024, iter [00770, 01251], lr: 0.000567, loss: 0.1105
2022-10-02 22:07:55 - train: epoch 0024, iter [00780, 01251], lr: 0.000567, loss: 0.0981
2022-10-02 22:08:23 - train: epoch 0024, iter [00790, 01251], lr: 0.000567, loss: 0.1071
2022-10-02 22:08:51 - train: epoch 0024, iter [00800, 01251], lr: 0.000567, loss: 0.1092
2022-10-02 22:09:20 - train: epoch 0024, iter [00810, 01251], lr: 0.000567, loss: 0.1027
2022-10-02 22:09:48 - train: epoch 0024, iter [00820, 01251], lr: 0.000567, loss: 0.1000
2022-10-02 22:10:16 - train: epoch 0024, iter [00830, 01251], lr: 0.000567, loss: 0.0998
2022-10-02 22:10:44 - train: epoch 0024, iter [00840, 01251], lr: 0.000566, loss: 0.1029
2022-10-02 22:11:13 - train: epoch 0024, iter [00850, 01251], lr: 0.000566, loss: 0.1001
2022-10-02 22:11:41 - train: epoch 0024, iter [00860, 01251], lr: 0.000566, loss: 0.1036
2022-10-02 22:12:09 - train: epoch 0024, iter [00870, 01251], lr: 0.000566, loss: 0.0978
2022-10-02 22:12:37 - train: epoch 0024, iter [00880, 01251], lr: 0.000566, loss: 0.1067
2022-10-02 22:13:05 - train: epoch 0024, iter [00890, 01251], lr: 0.000566, loss: 0.0940
2022-10-02 22:13:34 - train: epoch 0024, iter [00900, 01251], lr: 0.000566, loss: 0.1058
2022-10-02 22:14:02 - train: epoch 0024, iter [00910, 01251], lr: 0.000566, loss: 0.1103
2022-10-02 22:14:30 - train: epoch 0024, iter [00920, 01251], lr: 0.000566, loss: 0.1102
2022-10-02 22:14:58 - train: epoch 0024, iter [00930, 01251], lr: 0.000566, loss: 0.1133
2022-10-02 22:15:26 - train: epoch 0024, iter [00940, 01251], lr: 0.000566, loss: 0.0974
2022-10-02 22:15:55 - train: epoch 0024, iter [00950, 01251], lr: 0.000566, loss: 0.0938
2022-10-02 22:16:23 - train: epoch 0024, iter [00960, 01251], lr: 0.000566, loss: 0.0995
2022-10-02 22:16:51 - train: epoch 0024, iter [00970, 01251], lr: 0.000566, loss: 0.1022
2022-10-02 22:17:19 - train: epoch 0024, iter [00980, 01251], lr: 0.000566, loss: 0.1022
2022-10-02 22:17:48 - train: epoch 0024, iter [00990, 01251], lr: 0.000566, loss: 0.1044
2022-10-02 22:18:16 - train: epoch 0024, iter [01000, 01251], lr: 0.000566, loss: 0.1048
2022-10-02 22:18:44 - train: epoch 0024, iter [01010, 01251], lr: 0.000566, loss: 0.1022
2022-10-02 22:19:12 - train: epoch 0024, iter [01020, 01251], lr: 0.000566, loss: 0.1071
2022-10-02 22:19:41 - train: epoch 0024, iter [01030, 01251], lr: 0.000566, loss: 0.1065
2022-10-02 22:20:09 - train: epoch 0024, iter [01040, 01251], lr: 0.000566, loss: 0.1011
2022-10-02 22:20:38 - train: epoch 0024, iter [01050, 01251], lr: 0.000566, loss: 0.0958
2022-10-02 22:21:06 - train: epoch 0024, iter [01060, 01251], lr: 0.000566, loss: 0.1013
2022-10-02 22:21:34 - train: epoch 0024, iter [01070, 01251], lr: 0.000566, loss: 0.0965
2022-10-02 22:22:03 - train: epoch 0024, iter [01080, 01251], lr: 0.000566, loss: 0.1027
2022-10-02 22:22:31 - train: epoch 0024, iter [01090, 01251], lr: 0.000566, loss: 0.1037
2022-10-02 22:23:00 - train: epoch 0024, iter [01100, 01251], lr: 0.000565, loss: 0.1027
2022-10-02 22:23:28 - train: epoch 0024, iter [01110, 01251], lr: 0.000565, loss: 0.1043
2022-10-02 22:23:57 - train: epoch 0024, iter [01120, 01251], lr: 0.000565, loss: 0.1118
2022-10-02 22:24:25 - train: epoch 0024, iter [01130, 01251], lr: 0.000565, loss: 0.1056
2022-10-02 22:24:53 - train: epoch 0024, iter [01140, 01251], lr: 0.000565, loss: 0.0992
2022-10-02 22:25:22 - train: epoch 0024, iter [01150, 01251], lr: 0.000565, loss: 0.1055
2022-10-02 22:25:50 - train: epoch 0024, iter [01160, 01251], lr: 0.000565, loss: 0.1016
2022-10-02 22:26:18 - train: epoch 0024, iter [01170, 01251], lr: 0.000565, loss: 0.0975
2022-10-02 22:26:46 - train: epoch 0024, iter [01180, 01251], lr: 0.000565, loss: 0.0982
2022-10-02 22:27:15 - train: epoch 0024, iter [01190, 01251], lr: 0.000565, loss: 0.1061
2022-10-02 22:27:43 - train: epoch 0024, iter [01200, 01251], lr: 0.000565, loss: 0.1070
2022-10-02 22:28:11 - train: epoch 0024, iter [01210, 01251], lr: 0.000565, loss: 0.0958
2022-10-02 22:28:39 - train: epoch 0024, iter [01220, 01251], lr: 0.000565, loss: 0.1004
2022-10-02 22:29:08 - train: epoch 0024, iter [01230, 01251], lr: 0.000565, loss: 0.0965
2022-10-02 22:29:36 - train: epoch 0024, iter [01240, 01251], lr: 0.000565, loss: 0.1051
2022-10-02 22:30:04 - train: epoch 0024, iter [01250, 01251], lr: 0.000565, loss: 0.1031
2022-10-02 22:30:08 - train: epoch 024, train_loss: 0.1025
2022-10-02 22:30:10 - until epoch: 024, best_loss: 0.1025
2022-10-02 22:30:10 - epoch 025 lr: 0.000565
2022-10-02 22:30:45 - train: epoch 0025, iter [00010, 01251], lr: 0.000565, loss: 0.0951
2022-10-02 22:31:13 - train: epoch 0025, iter [00020, 01251], lr: 0.000565, loss: 0.0958
2022-10-02 22:31:42 - train: epoch 0025, iter [00030, 01251], lr: 0.000565, loss: 0.1001
2022-10-02 22:32:10 - train: epoch 0025, iter [00040, 01251], lr: 0.000565, loss: 0.1055
2022-10-02 22:32:38 - train: epoch 0025, iter [00050, 01251], lr: 0.000565, loss: 0.1063
2022-10-02 22:33:06 - train: epoch 0025, iter [00060, 01251], lr: 0.000565, loss: 0.1107
2022-10-02 22:33:34 - train: epoch 0025, iter [00070, 01251], lr: 0.000565, loss: 0.0996
2022-10-02 22:34:02 - train: epoch 0025, iter [00080, 01251], lr: 0.000565, loss: 0.1052
2022-10-02 22:34:30 - train: epoch 0025, iter [00090, 01251], lr: 0.000565, loss: 0.0954
2022-10-02 22:34:58 - train: epoch 0025, iter [00100, 01251], lr: 0.000564, loss: 0.1049
2022-10-02 22:35:26 - train: epoch 0025, iter [00110, 01251], lr: 0.000564, loss: 0.1034
2022-10-02 22:35:54 - train: epoch 0025, iter [00120, 01251], lr: 0.000564, loss: 0.0985
2022-10-02 22:36:22 - train: epoch 0025, iter [00130, 01251], lr: 0.000564, loss: 0.1006
2022-10-02 22:36:50 - train: epoch 0025, iter [00140, 01251], lr: 0.000564, loss: 0.1084
2022-10-02 22:37:18 - train: epoch 0025, iter [00150, 01251], lr: 0.000564, loss: 0.0980
2022-10-02 22:37:47 - train: epoch 0025, iter [00160, 01251], lr: 0.000564, loss: 0.0937
2022-10-02 22:38:15 - train: epoch 0025, iter [00170, 01251], lr: 0.000564, loss: 0.1009
2022-10-02 22:38:43 - train: epoch 0025, iter [00180, 01251], lr: 0.000564, loss: 0.0944
2022-10-02 22:39:11 - train: epoch 0025, iter [00190, 01251], lr: 0.000564, loss: 0.1012
2022-10-02 22:39:39 - train: epoch 0025, iter [00200, 01251], lr: 0.000564, loss: 0.1018
2022-10-02 22:40:07 - train: epoch 0025, iter [00210, 01251], lr: 0.000564, loss: 0.1096
2022-10-02 22:40:35 - train: epoch 0025, iter [00220, 01251], lr: 0.000564, loss: 0.0968
2022-10-02 22:41:04 - train: epoch 0025, iter [00230, 01251], lr: 0.000564, loss: 0.1051
2022-10-02 22:41:32 - train: epoch 0025, iter [00240, 01251], lr: 0.000564, loss: 0.0986
2022-10-02 22:42:00 - train: epoch 0025, iter [00250, 01251], lr: 0.000564, loss: 0.1001
2022-10-02 22:42:29 - train: epoch 0025, iter [00260, 01251], lr: 0.000564, loss: 0.1169
2022-10-02 22:42:57 - train: epoch 0025, iter [00270, 01251], lr: 0.000564, loss: 0.0947
2022-10-02 22:43:25 - train: epoch 0025, iter [00280, 01251], lr: 0.000564, loss: 0.1041
2022-10-02 22:43:54 - train: epoch 0025, iter [00290, 01251], lr: 0.000564, loss: 0.1008
2022-10-02 22:44:22 - train: epoch 0025, iter [00300, 01251], lr: 0.000564, loss: 0.1054
2022-10-02 22:44:50 - train: epoch 0025, iter [00310, 01251], lr: 0.000564, loss: 0.0973
2022-10-02 22:45:18 - train: epoch 0025, iter [00320, 01251], lr: 0.000564, loss: 0.1005
2022-10-02 22:45:46 - train: epoch 0025, iter [00330, 01251], lr: 0.000564, loss: 0.1088
2022-10-02 22:46:14 - train: epoch 0025, iter [00340, 01251], lr: 0.000564, loss: 0.0967
2022-10-02 22:46:42 - train: epoch 0025, iter [00350, 01251], lr: 0.000563, loss: 0.1011
2022-10-02 22:47:10 - train: epoch 0025, iter [00360, 01251], lr: 0.000563, loss: 0.1016
2022-10-02 22:47:38 - train: epoch 0025, iter [00370, 01251], lr: 0.000563, loss: 0.1046
2022-10-02 22:48:06 - train: epoch 0025, iter [00380, 01251], lr: 0.000563, loss: 0.1021
2022-10-02 22:48:34 - train: epoch 0025, iter [00390, 01251], lr: 0.000563, loss: 0.0988
2022-10-02 22:49:02 - train: epoch 0025, iter [00400, 01251], lr: 0.000563, loss: 0.0971
2022-10-02 22:49:30 - train: epoch 0025, iter [00410, 01251], lr: 0.000563, loss: 0.1019
2022-10-02 22:49:58 - train: epoch 0025, iter [00420, 01251], lr: 0.000563, loss: 0.1042
2022-10-02 22:50:26 - train: epoch 0025, iter [00430, 01251], lr: 0.000563, loss: 0.1021
2022-10-02 22:50:54 - train: epoch 0025, iter [00440, 01251], lr: 0.000563, loss: 0.0979
2022-10-02 22:51:22 - train: epoch 0025, iter [00450, 01251], lr: 0.000563, loss: 0.0969
2022-10-02 22:51:50 - train: epoch 0025, iter [00460, 01251], lr: 0.000563, loss: 0.0994
2022-10-02 22:52:18 - train: epoch 0025, iter [00470, 01251], lr: 0.000563, loss: 0.1014
2022-10-02 22:52:46 - train: epoch 0025, iter [00480, 01251], lr: 0.000563, loss: 0.1105
2022-10-02 22:53:14 - train: epoch 0025, iter [00490, 01251], lr: 0.000563, loss: 0.1040
2022-10-02 22:53:42 - train: epoch 0025, iter [00500, 01251], lr: 0.000563, loss: 0.1001
2022-10-02 22:54:10 - train: epoch 0025, iter [00510, 01251], lr: 0.000563, loss: 0.0969
2022-10-02 22:54:38 - train: epoch 0025, iter [00520, 01251], lr: 0.000563, loss: 0.1087
2022-10-02 22:55:06 - train: epoch 0025, iter [00530, 01251], lr: 0.000563, loss: 0.1040
2022-10-02 22:55:34 - train: epoch 0025, iter [00540, 01251], lr: 0.000563, loss: 0.1045
2022-10-02 22:56:02 - train: epoch 0025, iter [00550, 01251], lr: 0.000563, loss: 0.1005
2022-10-02 22:56:30 - train: epoch 0025, iter [00560, 01251], lr: 0.000563, loss: 0.1018
2022-10-02 22:56:58 - train: epoch 0025, iter [00570, 01251], lr: 0.000563, loss: 0.1037
2022-10-02 22:57:26 - train: epoch 0025, iter [00580, 01251], lr: 0.000563, loss: 0.1065
2022-10-02 22:57:54 - train: epoch 0025, iter [00590, 01251], lr: 0.000563, loss: 0.1079
2022-10-02 22:58:21 - train: epoch 0025, iter [00600, 01251], lr: 0.000562, loss: 0.1053
2022-10-02 22:58:50 - train: epoch 0025, iter [00610, 01251], lr: 0.000562, loss: 0.0980
2022-10-02 22:59:18 - train: epoch 0025, iter [00620, 01251], lr: 0.000562, loss: 0.1065
2022-10-02 22:59:46 - train: epoch 0025, iter [00630, 01251], lr: 0.000562, loss: 0.0986
2022-10-02 23:00:14 - train: epoch 0025, iter [00640, 01251], lr: 0.000562, loss: 0.1072
2022-10-02 23:00:43 - train: epoch 0025, iter [00650, 01251], lr: 0.000562, loss: 0.1016
2022-10-02 23:01:11 - train: epoch 0025, iter [00660, 01251], lr: 0.000562, loss: 0.1038
2022-10-02 23:01:39 - train: epoch 0025, iter [00670, 01251], lr: 0.000562, loss: 0.1013
2022-10-02 23:02:07 - train: epoch 0025, iter [00680, 01251], lr: 0.000562, loss: 0.1069
2022-10-02 23:02:35 - train: epoch 0025, iter [00690, 01251], lr: 0.000562, loss: 0.0924
2022-10-02 23:03:04 - train: epoch 0025, iter [00700, 01251], lr: 0.000562, loss: 0.0933
2022-10-02 23:03:32 - train: epoch 0025, iter [00710, 01251], lr: 0.000562, loss: 0.0939
2022-10-02 23:04:00 - train: epoch 0025, iter [00720, 01251], lr: 0.000562, loss: 0.1058
2022-10-02 23:04:29 - train: epoch 0025, iter [00730, 01251], lr: 0.000562, loss: 0.1030
2022-10-02 23:04:57 - train: epoch 0025, iter [00740, 01251], lr: 0.000562, loss: 0.0962
2022-10-02 23:05:25 - train: epoch 0025, iter [00750, 01251], lr: 0.000562, loss: 0.1006
2022-10-02 23:05:53 - train: epoch 0025, iter [00760, 01251], lr: 0.000562, loss: 0.1082
2022-10-02 23:06:21 - train: epoch 0025, iter [00770, 01251], lr: 0.000562, loss: 0.1039
2022-10-02 23:06:49 - train: epoch 0025, iter [00780, 01251], lr: 0.000562, loss: 0.1002
2022-10-02 23:07:18 - train: epoch 0025, iter [00790, 01251], lr: 0.000562, loss: 0.1039
2022-10-02 23:07:46 - train: epoch 0025, iter [00800, 01251], lr: 0.000562, loss: 0.1018
2022-10-02 23:08:14 - train: epoch 0025, iter [00810, 01251], lr: 0.000562, loss: 0.1032
2022-10-02 23:08:42 - train: epoch 0025, iter [00820, 01251], lr: 0.000562, loss: 0.1038
2022-10-02 23:09:10 - train: epoch 0025, iter [00830, 01251], lr: 0.000562, loss: 0.1063
2022-10-02 23:09:38 - train: epoch 0025, iter [00840, 01251], lr: 0.000562, loss: 0.1084
2022-10-02 23:10:07 - train: epoch 0025, iter [00850, 01251], lr: 0.000561, loss: 0.0939
2022-10-02 23:10:35 - train: epoch 0025, iter [00860, 01251], lr: 0.000561, loss: 0.1086
2022-10-02 23:11:03 - train: epoch 0025, iter [00870, 01251], lr: 0.000561, loss: 0.1052
2022-10-02 23:11:31 - train: epoch 0025, iter [00880, 01251], lr: 0.000561, loss: 0.1099
2022-10-02 23:11:59 - train: epoch 0025, iter [00890, 01251], lr: 0.000561, loss: 0.1026
2022-10-02 23:12:28 - train: epoch 0025, iter [00900, 01251], lr: 0.000561, loss: 0.1002
2022-10-02 23:12:56 - train: epoch 0025, iter [00910, 01251], lr: 0.000561, loss: 0.1037
2022-10-02 23:13:24 - train: epoch 0025, iter [00920, 01251], lr: 0.000561, loss: 0.1057
2022-10-02 23:13:53 - train: epoch 0025, iter [00930, 01251], lr: 0.000561, loss: 0.0924
2022-10-02 23:14:21 - train: epoch 0025, iter [00940, 01251], lr: 0.000561, loss: 0.1053
2022-10-02 23:14:49 - train: epoch 0025, iter [00950, 01251], lr: 0.000561, loss: 0.1040
2022-10-02 23:15:17 - train: epoch 0025, iter [00960, 01251], lr: 0.000561, loss: 0.1019
2022-10-02 23:15:46 - train: epoch 0025, iter [00970, 01251], lr: 0.000561, loss: 0.0993
2022-10-02 23:16:14 - train: epoch 0025, iter [00980, 01251], lr: 0.000561, loss: 0.1004
2022-10-02 23:16:42 - train: epoch 0025, iter [00990, 01251], lr: 0.000561, loss: 0.1023
2022-10-02 23:17:10 - train: epoch 0025, iter [01000, 01251], lr: 0.000561, loss: 0.1022
2022-10-02 23:17:38 - train: epoch 0025, iter [01010, 01251], lr: 0.000561, loss: 0.0902
2022-10-02 23:18:07 - train: epoch 0025, iter [01020, 01251], lr: 0.000561, loss: 0.1041
2022-10-02 23:18:35 - train: epoch 0025, iter [01030, 01251], lr: 0.000561, loss: 0.1053
2022-10-02 23:19:03 - train: epoch 0025, iter [01040, 01251], lr: 0.000561, loss: 0.1052
2022-10-02 23:19:32 - train: epoch 0025, iter [01050, 01251], lr: 0.000561, loss: 0.0957
2022-10-02 23:20:00 - train: epoch 0025, iter [01060, 01251], lr: 0.000561, loss: 0.0938
2022-10-02 23:20:29 - train: epoch 0025, iter [01070, 01251], lr: 0.000561, loss: 0.1023
2022-10-02 23:20:57 - train: epoch 0025, iter [01080, 01251], lr: 0.000561, loss: 0.0943
2022-10-02 23:21:25 - train: epoch 0025, iter [01090, 01251], lr: 0.000560, loss: 0.0983
2022-10-02 23:21:53 - train: epoch 0025, iter [01100, 01251], lr: 0.000560, loss: 0.1040
2022-10-02 23:22:22 - train: epoch 0025, iter [01110, 01251], lr: 0.000560, loss: 0.1040
2022-10-02 23:22:50 - train: epoch 0025, iter [01120, 01251], lr: 0.000560, loss: 0.0967
2022-10-02 23:23:18 - train: epoch 0025, iter [01130, 01251], lr: 0.000560, loss: 0.1060
2022-10-02 23:23:46 - train: epoch 0025, iter [01140, 01251], lr: 0.000560, loss: 0.0967
2022-10-02 23:24:14 - train: epoch 0025, iter [01150, 01251], lr: 0.000560, loss: 0.1045
2022-10-02 23:24:43 - train: epoch 0025, iter [01160, 01251], lr: 0.000560, loss: 0.0988
2022-10-02 23:25:11 - train: epoch 0025, iter [01170, 01251], lr: 0.000560, loss: 0.0993
2022-10-02 23:25:39 - train: epoch 0025, iter [01180, 01251], lr: 0.000560, loss: 0.0945
2022-10-02 23:26:08 - train: epoch 0025, iter [01190, 01251], lr: 0.000560, loss: 0.0982
2022-10-02 23:26:36 - train: epoch 0025, iter [01200, 01251], lr: 0.000560, loss: 0.0977
2022-10-02 23:27:04 - train: epoch 0025, iter [01210, 01251], lr: 0.000560, loss: 0.0992
2022-10-02 23:27:32 - train: epoch 0025, iter [01220, 01251], lr: 0.000560, loss: 0.1020
2022-10-02 23:28:01 - train: epoch 0025, iter [01230, 01251], lr: 0.000560, loss: 0.0947
2022-10-02 23:28:29 - train: epoch 0025, iter [01240, 01251], lr: 0.000560, loss: 0.0957
2022-10-02 23:28:57 - train: epoch 0025, iter [01250, 01251], lr: 0.000560, loss: 0.1048
2022-10-02 23:29:01 - train: epoch 025, train_loss: 0.1016
2022-10-02 23:29:03 - until epoch: 025, best_loss: 0.1016
2022-10-02 23:29:03 - epoch 026 lr: 0.000560
2022-10-02 23:29:38 - train: epoch 0026, iter [00010, 01251], lr: 0.000560, loss: 0.1026
2022-10-02 23:30:06 - train: epoch 0026, iter [00020, 01251], lr: 0.000560, loss: 0.1060
2022-10-02 23:30:35 - train: epoch 0026, iter [00030, 01251], lr: 0.000560, loss: 0.0995
2022-10-02 23:31:03 - train: epoch 0026, iter [00040, 01251], lr: 0.000560, loss: 0.1033
2022-10-02 23:31:31 - train: epoch 0026, iter [00050, 01251], lr: 0.000560, loss: 0.1060
2022-10-02 23:32:00 - train: epoch 0026, iter [00060, 01251], lr: 0.000560, loss: 0.1047
2022-10-02 23:32:28 - train: epoch 0026, iter [00070, 01251], lr: 0.000560, loss: 0.0999
2022-10-02 23:32:56 - train: epoch 0026, iter [00080, 01251], lr: 0.000559, loss: 0.0988
2022-10-02 23:33:24 - train: epoch 0026, iter [00090, 01251], lr: 0.000559, loss: 0.0989
2022-10-02 23:33:53 - train: epoch 0026, iter [00100, 01251], lr: 0.000559, loss: 0.1023
2022-10-02 23:34:21 - train: epoch 0026, iter [00110, 01251], lr: 0.000559, loss: 0.0950
2022-10-02 23:34:49 - train: epoch 0026, iter [00120, 01251], lr: 0.000559, loss: 0.0989
2022-10-02 23:35:18 - train: epoch 0026, iter [00130, 01251], lr: 0.000559, loss: 0.0980
2022-10-02 23:35:46 - train: epoch 0026, iter [00140, 01251], lr: 0.000559, loss: 0.1042
2022-10-02 23:36:15 - train: epoch 0026, iter [00150, 01251], lr: 0.000559, loss: 0.0985
2022-10-02 23:36:43 - train: epoch 0026, iter [00160, 01251], lr: 0.000559, loss: 0.1043
2022-10-02 23:37:11 - train: epoch 0026, iter [00170, 01251], lr: 0.000559, loss: 0.1062
2022-10-02 23:37:40 - train: epoch 0026, iter [00180, 01251], lr: 0.000559, loss: 0.1009
2022-10-02 23:38:08 - train: epoch 0026, iter [00190, 01251], lr: 0.000559, loss: 0.0946
2022-10-02 23:38:37 - train: epoch 0026, iter [00200, 01251], lr: 0.000559, loss: 0.1057
2022-10-02 23:39:05 - train: epoch 0026, iter [00210, 01251], lr: 0.000559, loss: 0.1074
2022-10-02 23:39:33 - train: epoch 0026, iter [00220, 01251], lr: 0.000559, loss: 0.1087
2022-10-02 23:40:02 - train: epoch 0026, iter [00230, 01251], lr: 0.000559, loss: 0.1028
2022-10-02 23:40:30 - train: epoch 0026, iter [00240, 01251], lr: 0.000559, loss: 0.0953
2022-10-02 23:40:59 - train: epoch 0026, iter [00250, 01251], lr: 0.000559, loss: 0.1048
2022-10-02 23:41:27 - train: epoch 0026, iter [00260, 01251], lr: 0.000559, loss: 0.0956
2022-10-02 23:41:55 - train: epoch 0026, iter [00270, 01251], lr: 0.000559, loss: 0.1005
2022-10-02 23:42:23 - train: epoch 0026, iter [00280, 01251], lr: 0.000559, loss: 0.1038
2022-10-02 23:42:51 - train: epoch 0026, iter [00290, 01251], lr: 0.000559, loss: 0.1097
2022-10-02 23:43:19 - train: epoch 0026, iter [00300, 01251], lr: 0.000559, loss: 0.1061
2022-10-02 23:43:48 - train: epoch 0026, iter [00310, 01251], lr: 0.000559, loss: 0.1006
2022-10-02 23:44:16 - train: epoch 0026, iter [00320, 01251], lr: 0.000558, loss: 0.1050
2022-10-02 23:44:44 - train: epoch 0026, iter [00330, 01251], lr: 0.000558, loss: 0.1091
2022-10-02 23:45:12 - train: epoch 0026, iter [00340, 01251], lr: 0.000558, loss: 0.1072
2022-10-02 23:45:40 - train: epoch 0026, iter [00350, 01251], lr: 0.000558, loss: 0.0954
2022-10-02 23:46:08 - train: epoch 0026, iter [00360, 01251], lr: 0.000558, loss: 0.0981
2022-10-02 23:46:36 - train: epoch 0026, iter [00370, 01251], lr: 0.000558, loss: 0.1014
2022-10-02 23:47:04 - train: epoch 0026, iter [00380, 01251], lr: 0.000558, loss: 0.0986
2022-10-02 23:47:32 - train: epoch 0026, iter [00390, 01251], lr: 0.000558, loss: 0.1008
2022-10-02 23:48:00 - train: epoch 0026, iter [00400, 01251], lr: 0.000558, loss: 0.0982
2022-10-02 23:48:28 - train: epoch 0026, iter [00410, 01251], lr: 0.000558, loss: 0.1014
2022-10-02 23:48:56 - train: epoch 0026, iter [00420, 01251], lr: 0.000558, loss: 0.1006
2022-10-02 23:49:24 - train: epoch 0026, iter [00430, 01251], lr: 0.000558, loss: 0.1006
2022-10-02 23:49:52 - train: epoch 0026, iter [00440, 01251], lr: 0.000558, loss: 0.0995
2022-10-02 23:50:20 - train: epoch 0026, iter [00450, 01251], lr: 0.000558, loss: 0.1040
2022-10-02 23:50:48 - train: epoch 0026, iter [00460, 01251], lr: 0.000558, loss: 0.0979
2022-10-02 23:51:16 - train: epoch 0026, iter [00470, 01251], lr: 0.000558, loss: 0.1038
2022-10-02 23:51:45 - train: epoch 0026, iter [00480, 01251], lr: 0.000558, loss: 0.0988
2022-10-02 23:52:13 - train: epoch 0026, iter [00490, 01251], lr: 0.000558, loss: 0.1091
2022-10-02 23:52:41 - train: epoch 0026, iter [00500, 01251], lr: 0.000558, loss: 0.0971
2022-10-02 23:53:09 - train: epoch 0026, iter [00510, 01251], lr: 0.000558, loss: 0.0902
2022-10-02 23:53:37 - train: epoch 0026, iter [00520, 01251], lr: 0.000558, loss: 0.1041
2022-10-02 23:54:05 - train: epoch 0026, iter [00530, 01251], lr: 0.000558, loss: 0.0980
2022-10-02 23:54:33 - train: epoch 0026, iter [00540, 01251], lr: 0.000558, loss: 0.1089
2022-10-02 23:55:01 - train: epoch 0026, iter [00550, 01251], lr: 0.000557, loss: 0.1037
2022-10-02 23:55:30 - train: epoch 0026, iter [00560, 01251], lr: 0.000557, loss: 0.0949
2022-10-02 23:55:58 - train: epoch 0026, iter [00570, 01251], lr: 0.000557, loss: 0.0985
2022-10-02 23:56:26 - train: epoch 0026, iter [00580, 01251], lr: 0.000557, loss: 0.0987
2022-10-02 23:56:54 - train: epoch 0026, iter [00590, 01251], lr: 0.000557, loss: 0.0955
2022-10-02 23:57:22 - train: epoch 0026, iter [00600, 01251], lr: 0.000557, loss: 0.1052
2022-10-02 23:57:50 - train: epoch 0026, iter [00610, 01251], lr: 0.000557, loss: 0.1031
2022-10-02 23:58:18 - train: epoch 0026, iter [00620, 01251], lr: 0.000557, loss: 0.1002
2022-10-02 23:58:46 - train: epoch 0026, iter [00630, 01251], lr: 0.000557, loss: 0.1087
2022-10-02 23:59:15 - train: epoch 0026, iter [00640, 01251], lr: 0.000557, loss: 0.1023
2022-10-02 23:59:43 - train: epoch 0026, iter [00650, 01251], lr: 0.000557, loss: 0.0930
2022-10-03 00:00:11 - train: epoch 0026, iter [00660, 01251], lr: 0.000557, loss: 0.0959
2022-10-03 00:00:39 - train: epoch 0026, iter [00670, 01251], lr: 0.000557, loss: 0.1034
2022-10-03 00:01:07 - train: epoch 0026, iter [00680, 01251], lr: 0.000557, loss: 0.1069
2022-10-03 00:01:35 - train: epoch 0026, iter [00690, 01251], lr: 0.000557, loss: 0.0924
2022-10-03 00:02:04 - train: epoch 0026, iter [00700, 01251], lr: 0.000557, loss: 0.0956
2022-10-03 00:02:32 - train: epoch 0026, iter [00710, 01251], lr: 0.000557, loss: 0.1065
2022-10-03 00:03:00 - train: epoch 0026, iter [00720, 01251], lr: 0.000557, loss: 0.0984
2022-10-03 00:03:28 - train: epoch 0026, iter [00730, 01251], lr: 0.000557, loss: 0.0980
2022-10-03 00:03:56 - train: epoch 0026, iter [00740, 01251], lr: 0.000557, loss: 0.0996
2022-10-03 00:04:24 - train: epoch 0026, iter [00750, 01251], lr: 0.000557, loss: 0.0979
2022-10-03 00:04:52 - train: epoch 0026, iter [00760, 01251], lr: 0.000557, loss: 0.1027
2022-10-03 00:05:20 - train: epoch 0026, iter [00770, 01251], lr: 0.000557, loss: 0.1024
2022-10-03 00:05:48 - train: epoch 0026, iter [00780, 01251], lr: 0.000556, loss: 0.1021
2022-10-03 00:06:17 - train: epoch 0026, iter [00790, 01251], lr: 0.000556, loss: 0.0992
2022-10-03 00:06:45 - train: epoch 0026, iter [00800, 01251], lr: 0.000556, loss: 0.1021
2022-10-03 00:07:13 - train: epoch 0026, iter [00810, 01251], lr: 0.000556, loss: 0.1031
2022-10-03 00:07:41 - train: epoch 0026, iter [00820, 01251], lr: 0.000556, loss: 0.1053
2022-10-03 00:08:09 - train: epoch 0026, iter [00830, 01251], lr: 0.000556, loss: 0.1029
2022-10-03 00:08:37 - train: epoch 0026, iter [00840, 01251], lr: 0.000556, loss: 0.0967
2022-10-03 00:09:06 - train: epoch 0026, iter [00850, 01251], lr: 0.000556, loss: 0.1018
2022-10-03 00:09:34 - train: epoch 0026, iter [00860, 01251], lr: 0.000556, loss: 0.1003
2022-10-03 00:10:02 - train: epoch 0026, iter [00870, 01251], lr: 0.000556, loss: 0.0960
2022-10-03 00:10:30 - train: epoch 0026, iter [00880, 01251], lr: 0.000556, loss: 0.1026
2022-10-03 00:10:58 - train: epoch 0026, iter [00890, 01251], lr: 0.000556, loss: 0.0988
2022-10-03 00:11:26 - train: epoch 0026, iter [00900, 01251], lr: 0.000556, loss: 0.0997
2022-10-03 00:11:54 - train: epoch 0026, iter [00910, 01251], lr: 0.000556, loss: 0.1075
2022-10-03 00:12:23 - train: epoch 0026, iter [00920, 01251], lr: 0.000556, loss: 0.0985
2022-10-03 00:12:51 - train: epoch 0026, iter [00930, 01251], lr: 0.000556, loss: 0.0962
2022-10-03 00:13:19 - train: epoch 0026, iter [00940, 01251], lr: 0.000556, loss: 0.1005
2022-10-03 00:13:48 - train: epoch 0026, iter [00950, 01251], lr: 0.000556, loss: 0.1026
2022-10-03 00:14:16 - train: epoch 0026, iter [00960, 01251], lr: 0.000556, loss: 0.0960
2022-10-03 00:14:44 - train: epoch 0026, iter [00970, 01251], lr: 0.000556, loss: 0.0926
2022-10-03 00:15:13 - train: epoch 0026, iter [00980, 01251], lr: 0.000556, loss: 0.1012
2022-10-03 00:15:41 - train: epoch 0026, iter [00990, 01251], lr: 0.000556, loss: 0.1059
2022-10-03 00:16:09 - train: epoch 0026, iter [01000, 01251], lr: 0.000556, loss: 0.0922
2022-10-03 00:16:37 - train: epoch 0026, iter [01010, 01251], lr: 0.000555, loss: 0.1054
2022-10-03 00:17:06 - train: epoch 0026, iter [01020, 01251], lr: 0.000555, loss: 0.1013
2022-10-03 00:17:34 - train: epoch 0026, iter [01030, 01251], lr: 0.000555, loss: 0.0945
2022-10-03 00:18:02 - train: epoch 0026, iter [01040, 01251], lr: 0.000555, loss: 0.0998
2022-10-03 00:18:31 - train: epoch 0026, iter [01050, 01251], lr: 0.000555, loss: 0.1005
2022-10-03 00:18:59 - train: epoch 0026, iter [01060, 01251], lr: 0.000555, loss: 0.1025
2022-10-03 00:19:27 - train: epoch 0026, iter [01070, 01251], lr: 0.000555, loss: 0.1036
2022-10-03 00:19:56 - train: epoch 0026, iter [01080, 01251], lr: 0.000555, loss: 0.0943
2022-10-03 00:20:24 - train: epoch 0026, iter [01090, 01251], lr: 0.000555, loss: 0.0999
2022-10-03 00:20:52 - train: epoch 0026, iter [01100, 01251], lr: 0.000555, loss: 0.1041
2022-10-03 00:21:21 - train: epoch 0026, iter [01110, 01251], lr: 0.000555, loss: 0.0935
2022-10-03 00:21:49 - train: epoch 0026, iter [01120, 01251], lr: 0.000555, loss: 0.0937
2022-10-03 00:22:17 - train: epoch 0026, iter [01130, 01251], lr: 0.000555, loss: 0.1018
2022-10-03 00:22:46 - train: epoch 0026, iter [01140, 01251], lr: 0.000555, loss: 0.1031
2022-10-03 00:23:14 - train: epoch 0026, iter [01150, 01251], lr: 0.000555, loss: 0.1014
2022-10-03 00:23:43 - train: epoch 0026, iter [01160, 01251], lr: 0.000555, loss: 0.1040
2022-10-03 00:24:11 - train: epoch 0026, iter [01170, 01251], lr: 0.000555, loss: 0.1093
2022-10-03 00:24:40 - train: epoch 0026, iter [01180, 01251], lr: 0.000555, loss: 0.0978
2022-10-03 00:25:08 - train: epoch 0026, iter [01190, 01251], lr: 0.000555, loss: 0.1008
2022-10-03 00:25:36 - train: epoch 0026, iter [01200, 01251], lr: 0.000555, loss: 0.1042
2022-10-03 00:26:04 - train: epoch 0026, iter [01210, 01251], lr: 0.000555, loss: 0.1028
2022-10-03 00:26:33 - train: epoch 0026, iter [01220, 01251], lr: 0.000555, loss: 0.1066
2022-10-03 00:27:01 - train: epoch 0026, iter [01230, 01251], lr: 0.000555, loss: 0.0953
2022-10-03 00:27:29 - train: epoch 0026, iter [01240, 01251], lr: 0.000554, loss: 0.1044
2022-10-03 00:27:57 - train: epoch 0026, iter [01250, 01251], lr: 0.000554, loss: 0.0930
2022-10-03 00:28:01 - train: epoch 026, train_loss: 0.1009
2022-10-03 00:28:03 - until epoch: 026, best_loss: 0.1009
2022-10-03 00:28:03 - epoch 027 lr: 0.000554
2022-10-03 00:28:39 - train: epoch 0027, iter [00010, 01251], lr: 0.000554, loss: 0.1012
2022-10-03 00:29:07 - train: epoch 0027, iter [00020, 01251], lr: 0.000554, loss: 0.0961
2022-10-03 00:29:35 - train: epoch 0027, iter [00030, 01251], lr: 0.000554, loss: 0.1079
2022-10-03 00:30:04 - train: epoch 0027, iter [00040, 01251], lr: 0.000554, loss: 0.1002
2022-10-03 00:30:32 - train: epoch 0027, iter [00050, 01251], lr: 0.000554, loss: 0.0961
2022-10-03 00:31:01 - train: epoch 0027, iter [00060, 01251], lr: 0.000554, loss: 0.0933
2022-10-03 00:31:29 - train: epoch 0027, iter [00070, 01251], lr: 0.000554, loss: 0.1031
2022-10-03 00:31:57 - train: epoch 0027, iter [00080, 01251], lr: 0.000554, loss: 0.0988
2022-10-03 00:32:26 - train: epoch 0027, iter [00090, 01251], lr: 0.000554, loss: 0.0996
2022-10-03 00:32:54 - train: epoch 0027, iter [00100, 01251], lr: 0.000554, loss: 0.1042
2022-10-03 00:33:22 - train: epoch 0027, iter [00110, 01251], lr: 0.000554, loss: 0.1036
2022-10-03 00:33:50 - train: epoch 0027, iter [00120, 01251], lr: 0.000554, loss: 0.0978
2022-10-03 00:34:19 - train: epoch 0027, iter [00130, 01251], lr: 0.000554, loss: 0.1053
2022-10-03 00:34:47 - train: epoch 0027, iter [00140, 01251], lr: 0.000554, loss: 0.0996
2022-10-03 00:35:15 - train: epoch 0027, iter [00150, 01251], lr: 0.000554, loss: 0.0967
2022-10-03 00:35:43 - train: epoch 0027, iter [00160, 01251], lr: 0.000554, loss: 0.1023
2022-10-03 00:36:11 - train: epoch 0027, iter [00170, 01251], lr: 0.000554, loss: 0.0934
2022-10-03 00:36:39 - train: epoch 0027, iter [00180, 01251], lr: 0.000554, loss: 0.0993
2022-10-03 00:37:08 - train: epoch 0027, iter [00190, 01251], lr: 0.000554, loss: 0.1040
2022-10-03 00:37:36 - train: epoch 0027, iter [00200, 01251], lr: 0.000554, loss: 0.0974
2022-10-03 00:38:04 - train: epoch 0027, iter [00210, 01251], lr: 0.000553, loss: 0.0991
2022-10-03 00:38:33 - train: epoch 0027, iter [00220, 01251], lr: 0.000553, loss: 0.1011
2022-10-03 00:39:01 - train: epoch 0027, iter [00230, 01251], lr: 0.000553, loss: 0.1055
2022-10-03 00:39:29 - train: epoch 0027, iter [00240, 01251], lr: 0.000553, loss: 0.0968
2022-10-03 00:39:57 - train: epoch 0027, iter [00250, 01251], lr: 0.000553, loss: 0.1046
2022-10-03 00:40:25 - train: epoch 0027, iter [00260, 01251], lr: 0.000553, loss: 0.1002
2022-10-03 00:40:53 - train: epoch 0027, iter [00270, 01251], lr: 0.000553, loss: 0.1038
2022-10-03 00:41:22 - train: epoch 0027, iter [00280, 01251], lr: 0.000553, loss: 0.1034
2022-10-03 00:41:50 - train: epoch 0027, iter [00290, 01251], lr: 0.000553, loss: 0.0982
2022-10-03 00:42:18 - train: epoch 0027, iter [00300, 01251], lr: 0.000553, loss: 0.1001
2022-10-03 00:42:46 - train: epoch 0027, iter [00310, 01251], lr: 0.000553, loss: 0.1068
2022-10-03 00:43:14 - train: epoch 0027, iter [00320, 01251], lr: 0.000553, loss: 0.1036
2022-10-03 00:43:43 - train: epoch 0027, iter [00330, 01251], lr: 0.000553, loss: 0.0973
2022-10-03 00:44:11 - train: epoch 0027, iter [00340, 01251], lr: 0.000553, loss: 0.1038
2022-10-03 00:44:39 - train: epoch 0027, iter [00350, 01251], lr: 0.000553, loss: 0.1032
2022-10-03 00:45:07 - train: epoch 0027, iter [00360, 01251], lr: 0.000553, loss: 0.1008
2022-10-03 00:45:35 - train: epoch 0027, iter [00370, 01251], lr: 0.000553, loss: 0.0996
2022-10-03 00:46:04 - train: epoch 0027, iter [00380, 01251], lr: 0.000553, loss: 0.0977
2022-10-03 00:46:32 - train: epoch 0027, iter [00390, 01251], lr: 0.000553, loss: 0.1013
2022-10-03 00:47:00 - train: epoch 0027, iter [00400, 01251], lr: 0.000553, loss: 0.1045
2022-10-03 00:47:28 - train: epoch 0027, iter [00410, 01251], lr: 0.000553, loss: 0.1040
2022-10-03 00:47:57 - train: epoch 0027, iter [00420, 01251], lr: 0.000553, loss: 0.1053
2022-10-03 00:48:25 - train: epoch 0027, iter [00430, 01251], lr: 0.000552, loss: 0.1044
2022-10-03 00:48:53 - train: epoch 0027, iter [00440, 01251], lr: 0.000552, loss: 0.1006
2022-10-03 00:49:22 - train: epoch 0027, iter [00450, 01251], lr: 0.000552, loss: 0.0933
2022-10-03 00:49:50 - train: epoch 0027, iter [00460, 01251], lr: 0.000552, loss: 0.0964
2022-10-03 00:50:18 - train: epoch 0027, iter [00470, 01251], lr: 0.000552, loss: 0.0996
2022-10-03 00:50:47 - train: epoch 0027, iter [00480, 01251], lr: 0.000552, loss: 0.0937
2022-10-03 00:51:15 - train: epoch 0027, iter [00490, 01251], lr: 0.000552, loss: 0.1026
2022-10-03 00:51:44 - train: epoch 0027, iter [00500, 01251], lr: 0.000552, loss: 0.1030
2022-10-03 00:52:12 - train: epoch 0027, iter [00510, 01251], lr: 0.000552, loss: 0.0848
2022-10-03 00:52:40 - train: epoch 0027, iter [00520, 01251], lr: 0.000552, loss: 0.1028
2022-10-03 00:53:08 - train: epoch 0027, iter [00530, 01251], lr: 0.000552, loss: 0.1066
2022-10-03 00:53:36 - train: epoch 0027, iter [00540, 01251], lr: 0.000552, loss: 0.1033
2022-10-03 00:54:05 - train: epoch 0027, iter [00550, 01251], lr: 0.000552, loss: 0.0983
2022-10-03 00:54:33 - train: epoch 0027, iter [00560, 01251], lr: 0.000552, loss: 0.0956
2022-10-03 00:55:01 - train: epoch 0027, iter [00570, 01251], lr: 0.000552, loss: 0.1047
2022-10-03 00:55:29 - train: epoch 0027, iter [00580, 01251], lr: 0.000552, loss: 0.1050
2022-10-03 00:55:58 - train: epoch 0027, iter [00590, 01251], lr: 0.000552, loss: 0.0998
2022-10-03 00:56:26 - train: epoch 0027, iter [00600, 01251], lr: 0.000552, loss: 0.1026
2022-10-03 00:56:54 - train: epoch 0027, iter [00610, 01251], lr: 0.000552, loss: 0.0952
2022-10-03 00:57:23 - train: epoch 0027, iter [00620, 01251], lr: 0.000552, loss: 0.0984
2022-10-03 00:57:51 - train: epoch 0027, iter [00630, 01251], lr: 0.000552, loss: 0.1041
2022-10-03 00:58:19 - train: epoch 0027, iter [00640, 01251], lr: 0.000552, loss: 0.1061
2022-10-03 00:58:47 - train: epoch 0027, iter [00650, 01251], lr: 0.000551, loss: 0.1012
2022-10-03 00:59:15 - train: epoch 0027, iter [00660, 01251], lr: 0.000551, loss: 0.0998
2022-10-03 00:59:43 - train: epoch 0027, iter [00670, 01251], lr: 0.000551, loss: 0.0990
2022-10-03 01:00:12 - train: epoch 0027, iter [00680, 01251], lr: 0.000551, loss: 0.0924
2022-10-03 01:00:40 - train: epoch 0027, iter [00690, 01251], lr: 0.000551, loss: 0.1015
2022-10-03 01:01:09 - train: epoch 0027, iter [00700, 01251], lr: 0.000551, loss: 0.1041
2022-10-03 01:01:37 - train: epoch 0027, iter [00710, 01251], lr: 0.000551, loss: 0.1045
2022-10-03 01:02:05 - train: epoch 0027, iter [00720, 01251], lr: 0.000551, loss: 0.1057
2022-10-03 01:02:33 - train: epoch 0027, iter [00730, 01251], lr: 0.000551, loss: 0.1006
2022-10-03 01:03:02 - train: epoch 0027, iter [00740, 01251], lr: 0.000551, loss: 0.0970
2022-10-03 01:03:30 - train: epoch 0027, iter [00750, 01251], lr: 0.000551, loss: 0.1000
2022-10-03 01:03:58 - train: epoch 0027, iter [00760, 01251], lr: 0.000551, loss: 0.1020
2022-10-03 01:04:26 - train: epoch 0027, iter [00770, 01251], lr: 0.000551, loss: 0.0990
2022-10-03 01:04:54 - train: epoch 0027, iter [00780, 01251], lr: 0.000551, loss: 0.1013
2022-10-03 01:05:23 - train: epoch 0027, iter [00790, 01251], lr: 0.000551, loss: 0.0962
2022-10-03 01:05:51 - train: epoch 0027, iter [00800, 01251], lr: 0.000551, loss: 0.1015
2022-10-03 01:06:19 - train: epoch 0027, iter [00810, 01251], lr: 0.000551, loss: 0.1019
2022-10-03 01:06:48 - train: epoch 0027, iter [00820, 01251], lr: 0.000551, loss: 0.1075
2022-10-03 01:07:16 - train: epoch 0027, iter [00830, 01251], lr: 0.000551, loss: 0.0928
2022-10-03 01:07:44 - train: epoch 0027, iter [00840, 01251], lr: 0.000551, loss: 0.1108
2022-10-03 01:08:12 - train: epoch 0027, iter [00850, 01251], lr: 0.000551, loss: 0.0897
2022-10-03 01:08:41 - train: epoch 0027, iter [00860, 01251], lr: 0.000551, loss: 0.1029
2022-10-03 01:09:09 - train: epoch 0027, iter [00870, 01251], lr: 0.000550, loss: 0.1067
2022-10-03 01:09:37 - train: epoch 0027, iter [00880, 01251], lr: 0.000550, loss: 0.0943
2022-10-03 01:10:05 - train: epoch 0027, iter [00890, 01251], lr: 0.000550, loss: 0.1047
2022-10-03 01:10:33 - train: epoch 0027, iter [00900, 01251], lr: 0.000550, loss: 0.0994
2022-10-03 01:11:01 - train: epoch 0027, iter [00910, 01251], lr: 0.000550, loss: 0.0922
2022-10-03 01:11:29 - train: epoch 0027, iter [00920, 01251], lr: 0.000550, loss: 0.0972
2022-10-03 01:11:58 - train: epoch 0027, iter [00930, 01251], lr: 0.000550, loss: 0.1005
2022-10-03 01:12:26 - train: epoch 0027, iter [00940, 01251], lr: 0.000550, loss: 0.1008
2022-10-03 01:12:54 - train: epoch 0027, iter [00950, 01251], lr: 0.000550, loss: 0.0968
2022-10-03 01:13:22 - train: epoch 0027, iter [00960, 01251], lr: 0.000550, loss: 0.0941
2022-10-03 01:13:50 - train: epoch 0027, iter [00970, 01251], lr: 0.000550, loss: 0.0940
2022-10-03 01:14:18 - train: epoch 0027, iter [00980, 01251], lr: 0.000550, loss: 0.1028
2022-10-03 01:14:46 - train: epoch 0027, iter [00990, 01251], lr: 0.000550, loss: 0.0926
2022-10-03 01:15:15 - train: epoch 0027, iter [01000, 01251], lr: 0.000550, loss: 0.0982
2022-10-03 01:15:43 - train: epoch 0027, iter [01010, 01251], lr: 0.000550, loss: 0.0999
2022-10-03 01:16:11 - train: epoch 0027, iter [01020, 01251], lr: 0.000550, loss: 0.1011
2022-10-03 01:16:39 - train: epoch 0027, iter [01030, 01251], lr: 0.000550, loss: 0.0972
2022-10-03 01:17:07 - train: epoch 0027, iter [01040, 01251], lr: 0.000550, loss: 0.0988
2022-10-03 01:17:35 - train: epoch 0027, iter [01050, 01251], lr: 0.000550, loss: 0.0991
2022-10-03 01:18:04 - train: epoch 0027, iter [01060, 01251], lr: 0.000550, loss: 0.1077
2022-10-03 01:18:32 - train: epoch 0027, iter [01070, 01251], lr: 0.000550, loss: 0.0992
2022-10-03 01:19:00 - train: epoch 0027, iter [01080, 01251], lr: 0.000550, loss: 0.0977
2022-10-03 01:19:29 - train: epoch 0027, iter [01090, 01251], lr: 0.000549, loss: 0.0996
2022-10-03 01:19:57 - train: epoch 0027, iter [01100, 01251], lr: 0.000549, loss: 0.0999
2022-10-03 01:20:25 - train: epoch 0027, iter [01110, 01251], lr: 0.000549, loss: 0.0937
2022-10-03 01:20:53 - train: epoch 0027, iter [01120, 01251], lr: 0.000549, loss: 0.0990
2022-10-03 01:21:22 - train: epoch 0027, iter [01130, 01251], lr: 0.000549, loss: 0.1022
2022-10-03 01:21:50 - train: epoch 0027, iter [01140, 01251], lr: 0.000549, loss: 0.1062
2022-10-03 01:22:18 - train: epoch 0027, iter [01150, 01251], lr: 0.000549, loss: 0.1060
2022-10-03 01:22:46 - train: epoch 0027, iter [01160, 01251], lr: 0.000549, loss: 0.0954
2022-10-03 01:23:14 - train: epoch 0027, iter [01170, 01251], lr: 0.000549, loss: 0.1029
2022-10-03 01:23:43 - train: epoch 0027, iter [01180, 01251], lr: 0.000549, loss: 0.1059
2022-10-03 01:24:11 - train: epoch 0027, iter [01190, 01251], lr: 0.000549, loss: 0.0930
2022-10-03 01:24:39 - train: epoch 0027, iter [01200, 01251], lr: 0.000549, loss: 0.1067
2022-10-03 01:25:07 - train: epoch 0027, iter [01210, 01251], lr: 0.000549, loss: 0.0931
2022-10-03 01:25:36 - train: epoch 0027, iter [01220, 01251], lr: 0.000549, loss: 0.1000
2022-10-03 01:26:04 - train: epoch 0027, iter [01230, 01251], lr: 0.000549, loss: 0.1021
2022-10-03 01:26:33 - train: epoch 0027, iter [01240, 01251], lr: 0.000549, loss: 0.0983
2022-10-03 01:27:00 - train: epoch 0027, iter [01250, 01251], lr: 0.000549, loss: 0.1079
2022-10-03 01:27:05 - train: epoch 027, train_loss: 0.1000
2022-10-03 01:27:07 - until epoch: 027, best_loss: 0.1000
2022-10-03 01:27:07 - epoch 028 lr: 0.000549
2022-10-03 01:27:42 - train: epoch 0028, iter [00010, 01251], lr: 0.000549, loss: 0.0995
2022-10-03 01:28:10 - train: epoch 0028, iter [00020, 01251], lr: 0.000549, loss: 0.0931
2022-10-03 01:28:39 - train: epoch 0028, iter [00030, 01251], lr: 0.000549, loss: 0.0972
2022-10-03 01:29:07 - train: epoch 0028, iter [00040, 01251], lr: 0.000549, loss: 0.1019
2022-10-03 01:29:36 - train: epoch 0028, iter [00050, 01251], lr: 0.000548, loss: 0.0994
2022-10-03 01:30:04 - train: epoch 0028, iter [00060, 01251], lr: 0.000548, loss: 0.0966
2022-10-03 01:30:33 - train: epoch 0028, iter [00070, 01251], lr: 0.000548, loss: 0.0978
2022-10-03 01:31:01 - train: epoch 0028, iter [00080, 01251], lr: 0.000548, loss: 0.1000
2022-10-03 01:31:29 - train: epoch 0028, iter [00090, 01251], lr: 0.000548, loss: 0.0961
2022-10-03 01:31:58 - train: epoch 0028, iter [00100, 01251], lr: 0.000548, loss: 0.0996
2022-10-03 01:32:26 - train: epoch 0028, iter [00110, 01251], lr: 0.000548, loss: 0.0977
2022-10-03 01:32:54 - train: epoch 0028, iter [00120, 01251], lr: 0.000548, loss: 0.0976
2022-10-03 01:33:22 - train: epoch 0028, iter [00130, 01251], lr: 0.000548, loss: 0.0928
2022-10-03 01:33:51 - train: epoch 0028, iter [00140, 01251], lr: 0.000548, loss: 0.1016
2022-10-03 01:34:19 - train: epoch 0028, iter [00150, 01251], lr: 0.000548, loss: 0.0916
2022-10-03 01:34:47 - train: epoch 0028, iter [00160, 01251], lr: 0.000548, loss: 0.1001
2022-10-03 01:35:15 - train: epoch 0028, iter [00170, 01251], lr: 0.000548, loss: 0.0939
2022-10-03 01:35:44 - train: epoch 0028, iter [00180, 01251], lr: 0.000548, loss: 0.0953
2022-10-03 01:36:12 - train: epoch 0028, iter [00190, 01251], lr: 0.000548, loss: 0.1035
2022-10-03 01:36:40 - train: epoch 0028, iter [00200, 01251], lr: 0.000548, loss: 0.1015
2022-10-03 01:37:08 - train: epoch 0028, iter [00210, 01251], lr: 0.000548, loss: 0.0992
2022-10-03 01:37:37 - train: epoch 0028, iter [00220, 01251], lr: 0.000548, loss: 0.0993
2022-10-03 01:38:05 - train: epoch 0028, iter [00230, 01251], lr: 0.000548, loss: 0.1019
2022-10-03 01:38:33 - train: epoch 0028, iter [00240, 01251], lr: 0.000548, loss: 0.1037
2022-10-03 01:39:01 - train: epoch 0028, iter [00250, 01251], lr: 0.000548, loss: 0.1015
2022-10-03 01:39:30 - train: epoch 0028, iter [00260, 01251], lr: 0.000547, loss: 0.1026
2022-10-03 01:39:58 - train: epoch 0028, iter [00270, 01251], lr: 0.000547, loss: 0.0939
2022-10-03 01:40:26 - train: epoch 0028, iter [00280, 01251], lr: 0.000547, loss: 0.0968
2022-10-03 01:40:55 - train: epoch 0028, iter [00290, 01251], lr: 0.000547, loss: 0.1041
2022-10-03 01:41:23 - train: epoch 0028, iter [00300, 01251], lr: 0.000547, loss: 0.1107
2022-10-03 01:41:51 - train: epoch 0028, iter [00310, 01251], lr: 0.000547, loss: 0.0989
2022-10-03 01:42:19 - train: epoch 0028, iter [00320, 01251], lr: 0.000547, loss: 0.1018
2022-10-03 01:42:47 - train: epoch 0028, iter [00330, 01251], lr: 0.000547, loss: 0.0981
2022-10-03 01:43:16 - train: epoch 0028, iter [00340, 01251], lr: 0.000547, loss: 0.0934
2022-10-03 01:43:44 - train: epoch 0028, iter [00350, 01251], lr: 0.000547, loss: 0.0969
2022-10-03 01:44:12 - train: epoch 0028, iter [00360, 01251], lr: 0.000547, loss: 0.0995
2022-10-03 01:44:40 - train: epoch 0028, iter [00370, 01251], lr: 0.000547, loss: 0.1000
2022-10-03 01:45:09 - train: epoch 0028, iter [00380, 01251], lr: 0.000547, loss: 0.1010
2022-10-03 01:45:37 - train: epoch 0028, iter [00390, 01251], lr: 0.000547, loss: 0.0983
2022-10-03 01:46:05 - train: epoch 0028, iter [00400, 01251], lr: 0.000547, loss: 0.0973
2022-10-03 01:46:33 - train: epoch 0028, iter [00410, 01251], lr: 0.000547, loss: 0.0994
2022-10-03 01:47:01 - train: epoch 0028, iter [00420, 01251], lr: 0.000547, loss: 0.1023
2022-10-03 01:47:29 - train: epoch 0028, iter [00430, 01251], lr: 0.000547, loss: 0.1006
2022-10-03 01:47:57 - train: epoch 0028, iter [00440, 01251], lr: 0.000547, loss: 0.0984
2022-10-03 01:48:25 - train: epoch 0028, iter [00450, 01251], lr: 0.000547, loss: 0.1039
2022-10-03 01:48:53 - train: epoch 0028, iter [00460, 01251], lr: 0.000547, loss: 0.1045
2022-10-03 01:49:22 - train: epoch 0028, iter [00470, 01251], lr: 0.000546, loss: 0.1002
2022-10-03 01:49:50 - train: epoch 0028, iter [00480, 01251], lr: 0.000546, loss: 0.1048
2022-10-03 01:50:18 - train: epoch 0028, iter [00490, 01251], lr: 0.000546, loss: 0.1009
2022-10-03 01:50:47 - train: epoch 0028, iter [00500, 01251], lr: 0.000546, loss: 0.0928
2022-10-03 01:51:15 - train: epoch 0028, iter [00510, 01251], lr: 0.000546, loss: 0.1011
2022-10-03 01:51:43 - train: epoch 0028, iter [00520, 01251], lr: 0.000546, loss: 0.1040
2022-10-03 01:52:11 - train: epoch 0028, iter [00530, 01251], lr: 0.000546, loss: 0.1032
2022-10-03 01:52:39 - train: epoch 0028, iter [00540, 01251], lr: 0.000546, loss: 0.1020
2022-10-03 01:53:08 - train: epoch 0028, iter [00550, 01251], lr: 0.000546, loss: 0.1007
2022-10-03 01:53:36 - train: epoch 0028, iter [00560, 01251], lr: 0.000546, loss: 0.0947
2022-10-03 01:54:04 - train: epoch 0028, iter [00570, 01251], lr: 0.000546, loss: 0.1006
2022-10-03 01:54:32 - train: epoch 0028, iter [00580, 01251], lr: 0.000546, loss: 0.1049
2022-10-03 01:55:00 - train: epoch 0028, iter [00590, 01251], lr: 0.000546, loss: 0.1011
2022-10-03 01:55:28 - train: epoch 0028, iter [00600, 01251], lr: 0.000546, loss: 0.0968
2022-10-03 01:55:57 - train: epoch 0028, iter [00610, 01251], lr: 0.000546, loss: 0.1062
2022-10-03 01:56:25 - train: epoch 0028, iter [00620, 01251], lr: 0.000546, loss: 0.0992
2022-10-03 01:56:53 - train: epoch 0028, iter [00630, 01251], lr: 0.000546, loss: 0.1014
2022-10-03 01:57:21 - train: epoch 0028, iter [00640, 01251], lr: 0.000546, loss: 0.0975
2022-10-03 01:57:49 - train: epoch 0028, iter [00650, 01251], lr: 0.000546, loss: 0.1055
2022-10-03 01:58:17 - train: epoch 0028, iter [00660, 01251], lr: 0.000546, loss: 0.1040
2022-10-03 01:58:46 - train: epoch 0028, iter [00670, 01251], lr: 0.000546, loss: 0.0869
2022-10-03 01:59:14 - train: epoch 0028, iter [00680, 01251], lr: 0.000545, loss: 0.0934
2022-10-03 01:59:42 - train: epoch 0028, iter [00690, 01251], lr: 0.000545, loss: 0.1009
2022-10-03 02:00:10 - train: epoch 0028, iter [00700, 01251], lr: 0.000545, loss: 0.1019
2022-10-03 02:00:38 - train: epoch 0028, iter [00710, 01251], lr: 0.000545, loss: 0.1050
2022-10-03 02:01:06 - train: epoch 0028, iter [00720, 01251], lr: 0.000545, loss: 0.0951
2022-10-03 02:01:34 - train: epoch 0028, iter [00730, 01251], lr: 0.000545, loss: 0.0922
2022-10-03 02:02:03 - train: epoch 0028, iter [00740, 01251], lr: 0.000545, loss: 0.1000
2022-10-03 02:02:31 - train: epoch 0028, iter [00750, 01251], lr: 0.000545, loss: 0.0996
2022-10-03 02:02:59 - train: epoch 0028, iter [00760, 01251], lr: 0.000545, loss: 0.1077
2022-10-03 02:03:27 - train: epoch 0028, iter [00770, 01251], lr: 0.000545, loss: 0.1028
2022-10-03 02:03:55 - train: epoch 0028, iter [00780, 01251], lr: 0.000545, loss: 0.1013
2022-10-03 02:04:23 - train: epoch 0028, iter [00790, 01251], lr: 0.000545, loss: 0.0963
2022-10-03 02:04:51 - train: epoch 0028, iter [00800, 01251], lr: 0.000545, loss: 0.0899
2022-10-03 02:05:19 - train: epoch 0028, iter [00810, 01251], lr: 0.000545, loss: 0.1015
2022-10-03 02:05:47 - train: epoch 0028, iter [00820, 01251], lr: 0.000545, loss: 0.0998
2022-10-03 02:06:15 - train: epoch 0028, iter [00830, 01251], lr: 0.000545, loss: 0.1037
2022-10-03 02:06:43 - train: epoch 0028, iter [00840, 01251], lr: 0.000545, loss: 0.0962
2022-10-03 02:07:11 - train: epoch 0028, iter [00850, 01251], lr: 0.000545, loss: 0.0936
2022-10-03 02:07:40 - train: epoch 0028, iter [00860, 01251], lr: 0.000545, loss: 0.0915
2022-10-03 02:08:08 - train: epoch 0028, iter [00870, 01251], lr: 0.000545, loss: 0.1016
2022-10-03 02:08:36 - train: epoch 0028, iter [00880, 01251], lr: 0.000545, loss: 0.1002
2022-10-03 02:09:05 - train: epoch 0028, iter [00890, 01251], lr: 0.000544, loss: 0.0976
2022-10-03 02:09:33 - train: epoch 0028, iter [00900, 01251], lr: 0.000544, loss: 0.1041
2022-10-03 02:10:01 - train: epoch 0028, iter [00910, 01251], lr: 0.000544, loss: 0.1079
2022-10-03 02:10:30 - train: epoch 0028, iter [00920, 01251], lr: 0.000544, loss: 0.1034
2022-10-03 02:10:58 - train: epoch 0028, iter [00930, 01251], lr: 0.000544, loss: 0.0974
2022-10-03 02:11:27 - train: epoch 0028, iter [00940, 01251], lr: 0.000544, loss: 0.1040
2022-10-03 02:11:55 - train: epoch 0028, iter [00950, 01251], lr: 0.000544, loss: 0.0931
2022-10-03 02:12:24 - train: epoch 0028, iter [00960, 01251], lr: 0.000544, loss: 0.1082
2022-10-03 02:12:52 - train: epoch 0028, iter [00970, 01251], lr: 0.000544, loss: 0.0946
2022-10-03 02:13:21 - train: epoch 0028, iter [00980, 01251], lr: 0.000544, loss: 0.0990
2022-10-03 02:13:49 - train: epoch 0028, iter [00990, 01251], lr: 0.000544, loss: 0.1027
2022-10-03 02:14:18 - train: epoch 0028, iter [01000, 01251], lr: 0.000544, loss: 0.0996
2022-10-03 02:14:46 - train: epoch 0028, iter [01010, 01251], lr: 0.000544, loss: 0.0985
2022-10-03 02:15:15 - train: epoch 0028, iter [01020, 01251], lr: 0.000544, loss: 0.0917
2022-10-03 02:15:43 - train: epoch 0028, iter [01030, 01251], lr: 0.000544, loss: 0.1023
2022-10-03 02:16:11 - train: epoch 0028, iter [01040, 01251], lr: 0.000544, loss: 0.0901
2022-10-03 02:16:40 - train: epoch 0028, iter [01050, 01251], lr: 0.000544, loss: 0.0944
2022-10-03 02:17:08 - train: epoch 0028, iter [01060, 01251], lr: 0.000544, loss: 0.1059
2022-10-03 02:17:37 - train: epoch 0028, iter [01070, 01251], lr: 0.000544, loss: 0.1017
2022-10-03 02:18:05 - train: epoch 0028, iter [01080, 01251], lr: 0.000544, loss: 0.0902
2022-10-03 02:18:33 - train: epoch 0028, iter [01090, 01251], lr: 0.000543, loss: 0.0915
2022-10-03 02:19:01 - train: epoch 0028, iter [01100, 01251], lr: 0.000543, loss: 0.0972
2022-10-03 02:19:30 - train: epoch 0028, iter [01110, 01251], lr: 0.000543, loss: 0.1029
2022-10-03 02:19:58 - train: epoch 0028, iter [01120, 01251], lr: 0.000543, loss: 0.1041
2022-10-03 02:20:26 - train: epoch 0028, iter [01130, 01251], lr: 0.000543, loss: 0.1039
2022-10-03 02:20:54 - train: epoch 0028, iter [01140, 01251], lr: 0.000543, loss: 0.1059
2022-10-03 02:21:22 - train: epoch 0028, iter [01150, 01251], lr: 0.000543, loss: 0.0982
2022-10-03 02:21:51 - train: epoch 0028, iter [01160, 01251], lr: 0.000543, loss: 0.0959
2022-10-03 02:22:19 - train: epoch 0028, iter [01170, 01251], lr: 0.000543, loss: 0.0971
2022-10-03 02:22:47 - train: epoch 0028, iter [01180, 01251], lr: 0.000543, loss: 0.0905
2022-10-03 02:23:15 - train: epoch 0028, iter [01190, 01251], lr: 0.000543, loss: 0.1030
2022-10-03 02:23:44 - train: epoch 0028, iter [01200, 01251], lr: 0.000543, loss: 0.0996
2022-10-03 02:24:12 - train: epoch 0028, iter [01210, 01251], lr: 0.000543, loss: 0.0960
2022-10-03 02:24:40 - train: epoch 0028, iter [01220, 01251], lr: 0.000543, loss: 0.1026
2022-10-03 02:25:09 - train: epoch 0028, iter [01230, 01251], lr: 0.000543, loss: 0.1017
2022-10-03 02:25:37 - train: epoch 0028, iter [01240, 01251], lr: 0.000543, loss: 0.1009
2022-10-03 02:26:05 - train: epoch 0028, iter [01250, 01251], lr: 0.000543, loss: 0.1063
2022-10-03 02:26:10 - train: epoch 028, train_loss: 0.0994
2022-10-03 02:26:11 - until epoch: 028, best_loss: 0.0994
2022-10-03 02:26:11 - epoch 029 lr: 0.000543
2022-10-03 02:26:47 - train: epoch 0029, iter [00010, 01251], lr: 0.000543, loss: 0.0969
2022-10-03 02:27:15 - train: epoch 0029, iter [00020, 01251], lr: 0.000543, loss: 0.0952
2022-10-03 02:27:43 - train: epoch 0029, iter [00030, 01251], lr: 0.000543, loss: 0.1081
2022-10-03 02:28:11 - train: epoch 0029, iter [00040, 01251], lr: 0.000543, loss: 0.1044
2022-10-03 02:28:40 - train: epoch 0029, iter [00050, 01251], lr: 0.000542, loss: 0.0946
2022-10-03 02:29:08 - train: epoch 0029, iter [00060, 01251], lr: 0.000542, loss: 0.0996
2022-10-03 02:29:36 - train: epoch 0029, iter [00070, 01251], lr: 0.000542, loss: 0.0931
2022-10-03 02:30:04 - train: epoch 0029, iter [00080, 01251], lr: 0.000542, loss: 0.1059
2022-10-03 02:30:33 - train: epoch 0029, iter [00090, 01251], lr: 0.000542, loss: 0.0966
2022-10-03 02:31:01 - train: epoch 0029, iter [00100, 01251], lr: 0.000542, loss: 0.0974
2022-10-03 02:31:29 - train: epoch 0029, iter [00110, 01251], lr: 0.000542, loss: 0.1005
2022-10-03 02:31:57 - train: epoch 0029, iter [00120, 01251], lr: 0.000542, loss: 0.0991
2022-10-03 02:32:25 - train: epoch 0029, iter [00130, 01251], lr: 0.000542, loss: 0.0998
2022-10-03 02:32:53 - train: epoch 0029, iter [00140, 01251], lr: 0.000542, loss: 0.1029
2022-10-03 02:33:21 - train: epoch 0029, iter [00150, 01251], lr: 0.000542, loss: 0.0964
2022-10-03 02:33:50 - train: epoch 0029, iter [00160, 01251], lr: 0.000542, loss: 0.0972
2022-10-03 02:34:18 - train: epoch 0029, iter [00170, 01251], lr: 0.000542, loss: 0.0985
2022-10-03 02:34:46 - train: epoch 0029, iter [00180, 01251], lr: 0.000542, loss: 0.1002
2022-10-03 02:35:14 - train: epoch 0029, iter [00190, 01251], lr: 0.000542, loss: 0.1005
2022-10-03 02:35:42 - train: epoch 0029, iter [00200, 01251], lr: 0.000542, loss: 0.0983
2022-10-03 02:36:11 - train: epoch 0029, iter [00210, 01251], lr: 0.000542, loss: 0.1027
2022-10-03 02:36:39 - train: epoch 0029, iter [00220, 01251], lr: 0.000542, loss: 0.0920
2022-10-03 02:37:07 - train: epoch 0029, iter [00230, 01251], lr: 0.000542, loss: 0.1045
2022-10-03 02:37:35 - train: epoch 0029, iter [00240, 01251], lr: 0.000542, loss: 0.0960
2022-10-03 02:38:03 - train: epoch 0029, iter [00250, 01251], lr: 0.000541, loss: 0.1004
2022-10-03 02:38:31 - train: epoch 0029, iter [00260, 01251], lr: 0.000541, loss: 0.1020
2022-10-03 02:39:00 - train: epoch 0029, iter [00270, 01251], lr: 0.000541, loss: 0.0967
2022-10-03 02:39:28 - train: epoch 0029, iter [00280, 01251], lr: 0.000541, loss: 0.1002
2022-10-03 02:39:56 - train: epoch 0029, iter [00290, 01251], lr: 0.000541, loss: 0.0985
2022-10-03 02:40:24 - train: epoch 0029, iter [00300, 01251], lr: 0.000541, loss: 0.0945
2022-10-03 02:40:52 - train: epoch 0029, iter [00310, 01251], lr: 0.000541, loss: 0.0963
2022-10-03 02:41:20 - train: epoch 0029, iter [00320, 01251], lr: 0.000541, loss: 0.0977
2022-10-03 02:41:49 - train: epoch 0029, iter [00330, 01251], lr: 0.000541, loss: 0.0937
2022-10-03 02:42:17 - train: epoch 0029, iter [00340, 01251], lr: 0.000541, loss: 0.1003
2022-10-03 02:42:45 - train: epoch 0029, iter [00350, 01251], lr: 0.000541, loss: 0.0982
2022-10-03 02:43:13 - train: epoch 0029, iter [00360, 01251], lr: 0.000541, loss: 0.1005
2022-10-03 02:43:41 - train: epoch 0029, iter [00370, 01251], lr: 0.000541, loss: 0.0933
2022-10-03 02:44:09 - train: epoch 0029, iter [00380, 01251], lr: 0.000541, loss: 0.1072
2022-10-03 02:44:37 - train: epoch 0029, iter [00390, 01251], lr: 0.000541, loss: 0.1002
2022-10-03 02:45:06 - train: epoch 0029, iter [00400, 01251], lr: 0.000541, loss: 0.0976
2022-10-03 02:45:33 - train: epoch 0029, iter [00410, 01251], lr: 0.000541, loss: 0.1013
2022-10-03 02:46:01 - train: epoch 0029, iter [00420, 01251], lr: 0.000541, loss: 0.1014
2022-10-03 02:46:29 - train: epoch 0029, iter [00430, 01251], lr: 0.000541, loss: 0.0998
2022-10-03 02:46:58 - train: epoch 0029, iter [00440, 01251], lr: 0.000541, loss: 0.1012
2022-10-03 02:47:26 - train: epoch 0029, iter [00450, 01251], lr: 0.000540, loss: 0.0973
2022-10-03 02:47:53 - train: epoch 0029, iter [00460, 01251], lr: 0.000540, loss: 0.1026
2022-10-03 02:48:21 - train: epoch 0029, iter [00470, 01251], lr: 0.000540, loss: 0.0979
2022-10-03 02:48:49 - train: epoch 0029, iter [00480, 01251], lr: 0.000540, loss: 0.0992
2022-10-03 02:49:18 - train: epoch 0029, iter [00490, 01251], lr: 0.000540, loss: 0.0946
2022-10-03 02:49:46 - train: epoch 0029, iter [00500, 01251], lr: 0.000540, loss: 0.1015
2022-10-03 02:50:14 - train: epoch 0029, iter [00510, 01251], lr: 0.000540, loss: 0.1077
2022-10-03 02:50:42 - train: epoch 0029, iter [00520, 01251], lr: 0.000540, loss: 0.0996
2022-10-03 02:51:09 - train: epoch 0029, iter [00530, 01251], lr: 0.000540, loss: 0.1053
2022-10-03 02:51:37 - train: epoch 0029, iter [00540, 01251], lr: 0.000540, loss: 0.1102
2022-10-03 02:52:06 - train: epoch 0029, iter [00550, 01251], lr: 0.000540, loss: 0.0974
2022-10-03 02:52:34 - train: epoch 0029, iter [00560, 01251], lr: 0.000540, loss: 0.0960
2022-10-03 02:53:02 - train: epoch 0029, iter [00570, 01251], lr: 0.000540, loss: 0.1022
2022-10-03 02:53:30 - train: epoch 0029, iter [00580, 01251], lr: 0.000540, loss: 0.0937
2022-10-03 02:53:58 - train: epoch 0029, iter [00590, 01251], lr: 0.000540, loss: 0.0941
2022-10-03 02:54:26 - train: epoch 0029, iter [00600, 01251], lr: 0.000540, loss: 0.0991
2022-10-03 02:54:54 - train: epoch 0029, iter [00610, 01251], lr: 0.000540, loss: 0.0910
2022-10-03 02:55:22 - train: epoch 0029, iter [00620, 01251], lr: 0.000540, loss: 0.1020
2022-10-03 02:55:50 - train: epoch 0029, iter [00630, 01251], lr: 0.000540, loss: 0.1006
2022-10-03 02:56:18 - train: epoch 0029, iter [00640, 01251], lr: 0.000540, loss: 0.0958
2022-10-03 02:56:46 - train: epoch 0029, iter [00650, 01251], lr: 0.000539, loss: 0.0996
2022-10-03 02:57:14 - train: epoch 0029, iter [00660, 01251], lr: 0.000539, loss: 0.0925
2022-10-03 02:57:42 - train: epoch 0029, iter [00670, 01251], lr: 0.000539, loss: 0.0946
2022-10-03 02:58:10 - train: epoch 0029, iter [00680, 01251], lr: 0.000539, loss: 0.0985
2022-10-03 02:58:38 - train: epoch 0029, iter [00690, 01251], lr: 0.000539, loss: 0.0991
2022-10-03 02:59:06 - train: epoch 0029, iter [00700, 01251], lr: 0.000539, loss: 0.0949
2022-10-03 02:59:34 - train: epoch 0029, iter [00710, 01251], lr: 0.000539, loss: 0.1036
2022-10-03 03:00:02 - train: epoch 0029, iter [00720, 01251], lr: 0.000539, loss: 0.1028
2022-10-03 03:00:30 - train: epoch 0029, iter [00730, 01251], lr: 0.000539, loss: 0.0928
2022-10-03 03:00:58 - train: epoch 0029, iter [00740, 01251], lr: 0.000539, loss: 0.1043
2022-10-03 03:01:27 - train: epoch 0029, iter [00750, 01251], lr: 0.000539, loss: 0.0931
2022-10-03 03:01:55 - train: epoch 0029, iter [00760, 01251], lr: 0.000539, loss: 0.1047
2022-10-03 03:02:23 - train: epoch 0029, iter [00770, 01251], lr: 0.000539, loss: 0.0967
2022-10-03 03:02:51 - train: epoch 0029, iter [00780, 01251], lr: 0.000539, loss: 0.1073
2022-10-03 03:03:19 - train: epoch 0029, iter [00790, 01251], lr: 0.000539, loss: 0.0990
2022-10-03 03:03:47 - train: epoch 0029, iter [00800, 01251], lr: 0.000539, loss: 0.0950
2022-10-03 03:04:15 - train: epoch 0029, iter [00810, 01251], lr: 0.000539, loss: 0.1024
2022-10-03 03:04:43 - train: epoch 0029, iter [00820, 01251], lr: 0.000539, loss: 0.1008
2022-10-03 03:05:11 - train: epoch 0029, iter [00830, 01251], lr: 0.000539, loss: 0.0983
2022-10-03 03:05:39 - train: epoch 0029, iter [00840, 01251], lr: 0.000539, loss: 0.0970
2022-10-03 03:06:07 - train: epoch 0029, iter [00850, 01251], lr: 0.000538, loss: 0.1061
2022-10-03 03:06:35 - train: epoch 0029, iter [00860, 01251], lr: 0.000538, loss: 0.1033
2022-10-03 03:07:03 - train: epoch 0029, iter [00870, 01251], lr: 0.000538, loss: 0.1027
2022-10-03 03:07:31 - train: epoch 0029, iter [00880, 01251], lr: 0.000538, loss: 0.0961
2022-10-03 03:07:59 - train: epoch 0029, iter [00890, 01251], lr: 0.000538, loss: 0.1017
2022-10-03 03:08:27 - train: epoch 0029, iter [00900, 01251], lr: 0.000538, loss: 0.0982
2022-10-03 03:08:55 - train: epoch 0029, iter [00910, 01251], lr: 0.000538, loss: 0.0982
2022-10-03 03:09:23 - train: epoch 0029, iter [00920, 01251], lr: 0.000538, loss: 0.0965
2022-10-03 03:09:51 - train: epoch 0029, iter [00930, 01251], lr: 0.000538, loss: 0.1077
2022-10-03 03:10:19 - train: epoch 0029, iter [00940, 01251], lr: 0.000538, loss: 0.0909
2022-10-03 03:10:47 - train: epoch 0029, iter [00950, 01251], lr: 0.000538, loss: 0.0973
2022-10-03 03:11:15 - train: epoch 0029, iter [00960, 01251], lr: 0.000538, loss: 0.1000
2022-10-03 03:11:44 - train: epoch 0029, iter [00970, 01251], lr: 0.000538, loss: 0.1018
2022-10-03 03:12:12 - train: epoch 0029, iter [00980, 01251], lr: 0.000538, loss: 0.0932
2022-10-03 03:12:40 - train: epoch 0029, iter [00990, 01251], lr: 0.000538, loss: 0.0990
2022-10-03 03:13:08 - train: epoch 0029, iter [01000, 01251], lr: 0.000538, loss: 0.0998
2022-10-03 03:13:35 - train: epoch 0029, iter [01010, 01251], lr: 0.000538, loss: 0.0934
2022-10-03 03:14:04 - train: epoch 0029, iter [01020, 01251], lr: 0.000538, loss: 0.1058
2022-10-03 03:14:32 - train: epoch 0029, iter [01030, 01251], lr: 0.000538, loss: 0.1056
2022-10-03 03:14:59 - train: epoch 0029, iter [01040, 01251], lr: 0.000537, loss: 0.1026
2022-10-03 03:15:27 - train: epoch 0029, iter [01050, 01251], lr: 0.000537, loss: 0.1044
2022-10-03 03:15:56 - train: epoch 0029, iter [01060, 01251], lr: 0.000537, loss: 0.0976
2022-10-03 03:16:24 - train: epoch 0029, iter [01070, 01251], lr: 0.000537, loss: 0.1041
2022-10-03 03:16:51 - train: epoch 0029, iter [01080, 01251], lr: 0.000537, loss: 0.0995
2022-10-03 03:17:19 - train: epoch 0029, iter [01090, 01251], lr: 0.000537, loss: 0.1047
2022-10-03 03:17:47 - train: epoch 0029, iter [01100, 01251], lr: 0.000537, loss: 0.1062
2022-10-03 03:18:15 - train: epoch 0029, iter [01110, 01251], lr: 0.000537, loss: 0.1008
2022-10-03 03:18:43 - train: epoch 0029, iter [01120, 01251], lr: 0.000537, loss: 0.0992
2022-10-03 03:19:11 - train: epoch 0029, iter [01130, 01251], lr: 0.000537, loss: 0.0918
2022-10-03 03:19:39 - train: epoch 0029, iter [01140, 01251], lr: 0.000537, loss: 0.1001
2022-10-03 03:20:07 - train: epoch 0029, iter [01150, 01251], lr: 0.000537, loss: 0.1042
2022-10-03 03:20:35 - train: epoch 0029, iter [01160, 01251], lr: 0.000537, loss: 0.1059
2022-10-03 03:21:03 - train: epoch 0029, iter [01170, 01251], lr: 0.000537, loss: 0.0894
2022-10-03 03:21:31 - train: epoch 0029, iter [01180, 01251], lr: 0.000537, loss: 0.0980
2022-10-03 03:21:59 - train: epoch 0029, iter [01190, 01251], lr: 0.000537, loss: 0.1048
2022-10-03 03:22:27 - train: epoch 0029, iter [01200, 01251], lr: 0.000537, loss: 0.0981
2022-10-03 03:22:55 - train: epoch 0029, iter [01210, 01251], lr: 0.000537, loss: 0.0961
2022-10-03 03:23:23 - train: epoch 0029, iter [01220, 01251], lr: 0.000537, loss: 0.1010
2022-10-03 03:23:50 - train: epoch 0029, iter [01230, 01251], lr: 0.000537, loss: 0.1007
2022-10-03 03:24:19 - train: epoch 0029, iter [01240, 01251], lr: 0.000536, loss: 0.1004
2022-10-03 03:24:46 - train: epoch 0029, iter [01250, 01251], lr: 0.000536, loss: 0.1020
2022-10-03 03:24:50 - train: epoch 029, train_loss: 0.0988
2022-10-03 03:24:52 - until epoch: 029, best_loss: 0.0988
2022-10-03 03:24:52 - epoch 030 lr: 0.000536
2022-10-03 03:25:27 - train: epoch 0030, iter [00010, 01251], lr: 0.000536, loss: 0.0921
2022-10-03 03:25:55 - train: epoch 0030, iter [00020, 01251], lr: 0.000536, loss: 0.0922
2022-10-03 03:26:23 - train: epoch 0030, iter [00030, 01251], lr: 0.000536, loss: 0.0963
2022-10-03 03:26:51 - train: epoch 0030, iter [00040, 01251], lr: 0.000536, loss: 0.1062
2022-10-03 03:27:20 - train: epoch 0030, iter [00050, 01251], lr: 0.000536, loss: 0.0952
2022-10-03 03:27:48 - train: epoch 0030, iter [00060, 01251], lr: 0.000536, loss: 0.0970
2022-10-03 03:28:16 - train: epoch 0030, iter [00070, 01251], lr: 0.000536, loss: 0.0977
2022-10-03 03:28:45 - train: epoch 0030, iter [00080, 01251], lr: 0.000536, loss: 0.0954
2022-10-03 03:29:13 - train: epoch 0030, iter [00090, 01251], lr: 0.000536, loss: 0.0942
2022-10-03 03:29:41 - train: epoch 0030, iter [00100, 01251], lr: 0.000536, loss: 0.1070
2022-10-03 03:30:09 - train: epoch 0030, iter [00110, 01251], lr: 0.000536, loss: 0.0986
2022-10-03 03:30:37 - train: epoch 0030, iter [00120, 01251], lr: 0.000536, loss: 0.1017
2022-10-03 03:31:06 - train: epoch 0030, iter [00130, 01251], lr: 0.000536, loss: 0.1047
2022-10-03 03:31:34 - train: epoch 0030, iter [00140, 01251], lr: 0.000536, loss: 0.0999
2022-10-03 03:32:02 - train: epoch 0030, iter [00150, 01251], lr: 0.000536, loss: 0.1027
2022-10-03 03:32:30 - train: epoch 0030, iter [00160, 01251], lr: 0.000536, loss: 0.0968
2022-10-03 03:32:58 - train: epoch 0030, iter [00170, 01251], lr: 0.000536, loss: 0.1033
2022-10-03 03:33:26 - train: epoch 0030, iter [00180, 01251], lr: 0.000535, loss: 0.0974
2022-10-03 03:33:54 - train: epoch 0030, iter [00190, 01251], lr: 0.000535, loss: 0.0997
2022-10-03 03:34:22 - train: epoch 0030, iter [00200, 01251], lr: 0.000535, loss: 0.0956
2022-10-03 03:34:51 - train: epoch 0030, iter [00210, 01251], lr: 0.000535, loss: 0.1055
2022-10-03 03:35:19 - train: epoch 0030, iter [00220, 01251], lr: 0.000535, loss: 0.0932
2022-10-03 03:35:47 - train: epoch 0030, iter [00230, 01251], lr: 0.000535, loss: 0.1047
2022-10-03 03:36:15 - train: epoch 0030, iter [00240, 01251], lr: 0.000535, loss: 0.1027
2022-10-03 03:36:43 - train: epoch 0030, iter [00250, 01251], lr: 0.000535, loss: 0.0952
2022-10-03 03:37:11 - train: epoch 0030, iter [00260, 01251], lr: 0.000535, loss: 0.0998
2022-10-03 03:37:39 - train: epoch 0030, iter [00270, 01251], lr: 0.000535, loss: 0.0953
2022-10-03 03:38:07 - train: epoch 0030, iter [00280, 01251], lr: 0.000535, loss: 0.0942
2022-10-03 03:38:35 - train: epoch 0030, iter [00290, 01251], lr: 0.000535, loss: 0.0988
2022-10-03 03:39:04 - train: epoch 0030, iter [00300, 01251], lr: 0.000535, loss: 0.0938
2022-10-03 03:39:32 - train: epoch 0030, iter [00310, 01251], lr: 0.000535, loss: 0.1065
2022-10-03 03:40:00 - train: epoch 0030, iter [00320, 01251], lr: 0.000535, loss: 0.0964
2022-10-03 03:40:28 - train: epoch 0030, iter [00330, 01251], lr: 0.000535, loss: 0.1034
2022-10-03 03:40:56 - train: epoch 0030, iter [00340, 01251], lr: 0.000535, loss: 0.0905
2022-10-03 03:41:24 - train: epoch 0030, iter [00350, 01251], lr: 0.000535, loss: 0.0950
2022-10-03 03:41:53 - train: epoch 0030, iter [00360, 01251], lr: 0.000535, loss: 0.0989
2022-10-03 03:42:21 - train: epoch 0030, iter [00370, 01251], lr: 0.000534, loss: 0.0956
2022-10-03 03:42:49 - train: epoch 0030, iter [00380, 01251], lr: 0.000534, loss: 0.0991
2022-10-03 03:43:17 - train: epoch 0030, iter [00390, 01251], lr: 0.000534, loss: 0.0973
2022-10-03 03:43:45 - train: epoch 0030, iter [00400, 01251], lr: 0.000534, loss: 0.1087
2022-10-03 03:44:14 - train: epoch 0030, iter [00410, 01251], lr: 0.000534, loss: 0.1007
2022-10-03 03:44:42 - train: epoch 0030, iter [00420, 01251], lr: 0.000534, loss: 0.0944
2022-10-03 03:45:10 - train: epoch 0030, iter [00430, 01251], lr: 0.000534, loss: 0.0932
2022-10-03 03:45:39 - train: epoch 0030, iter [00440, 01251], lr: 0.000534, loss: 0.0951
2022-10-03 03:46:07 - train: epoch 0030, iter [00450, 01251], lr: 0.000534, loss: 0.0882
2022-10-03 03:46:35 - train: epoch 0030, iter [00460, 01251], lr: 0.000534, loss: 0.0999
2022-10-03 03:47:03 - train: epoch 0030, iter [00470, 01251], lr: 0.000534, loss: 0.0967
2022-10-03 03:47:31 - train: epoch 0030, iter [00480, 01251], lr: 0.000534, loss: 0.0937
2022-10-03 03:47:59 - train: epoch 0030, iter [00490, 01251], lr: 0.000534, loss: 0.0986
2022-10-03 03:48:27 - train: epoch 0030, iter [00500, 01251], lr: 0.000534, loss: 0.0930
2022-10-03 03:48:55 - train: epoch 0030, iter [00510, 01251], lr: 0.000534, loss: 0.1021
2022-10-03 03:49:24 - train: epoch 0030, iter [00520, 01251], lr: 0.000534, loss: 0.0928
2022-10-03 03:49:52 - train: epoch 0030, iter [00530, 01251], lr: 0.000534, loss: 0.0977
2022-10-03 03:50:20 - train: epoch 0030, iter [00540, 01251], lr: 0.000534, loss: 0.0976
2022-10-03 03:50:48 - train: epoch 0030, iter [00550, 01251], lr: 0.000534, loss: 0.0998
2022-10-03 03:51:16 - train: epoch 0030, iter [00560, 01251], lr: 0.000533, loss: 0.0903
2022-10-03 03:51:44 - train: epoch 0030, iter [00570, 01251], lr: 0.000533, loss: 0.0943
2022-10-03 03:52:12 - train: epoch 0030, iter [00580, 01251], lr: 0.000533, loss: 0.1062
2022-10-03 03:52:40 - train: epoch 0030, iter [00590, 01251], lr: 0.000533, loss: 0.0989
2022-10-03 03:53:09 - train: epoch 0030, iter [00600, 01251], lr: 0.000533, loss: 0.0859
2022-10-03 03:53:37 - train: epoch 0030, iter [00610, 01251], lr: 0.000533, loss: 0.1010
2022-10-03 03:54:05 - train: epoch 0030, iter [00620, 01251], lr: 0.000533, loss: 0.1046
2022-10-03 03:54:33 - train: epoch 0030, iter [00630, 01251], lr: 0.000533, loss: 0.1035
2022-10-03 03:55:01 - train: epoch 0030, iter [00640, 01251], lr: 0.000533, loss: 0.0943
2022-10-03 03:55:29 - train: epoch 0030, iter [00650, 01251], lr: 0.000533, loss: 0.0931
2022-10-03 03:55:57 - train: epoch 0030, iter [00660, 01251], lr: 0.000533, loss: 0.0976
2022-10-03 03:56:25 - train: epoch 0030, iter [00670, 01251], lr: 0.000533, loss: 0.0912
2022-10-03 03:56:54 - train: epoch 0030, iter [00680, 01251], lr: 0.000533, loss: 0.1015
2022-10-03 03:57:22 - train: epoch 0030, iter [00690, 01251], lr: 0.000533, loss: 0.0975
2022-10-03 03:57:50 - train: epoch 0030, iter [00700, 01251], lr: 0.000533, loss: 0.0935
2022-10-03 03:58:18 - train: epoch 0030, iter [00710, 01251], lr: 0.000533, loss: 0.1050
2022-10-03 03:58:46 - train: epoch 0030, iter [00720, 01251], lr: 0.000533, loss: 0.0942
2022-10-03 03:59:15 - train: epoch 0030, iter [00730, 01251], lr: 0.000533, loss: 0.0990
2022-10-03 03:59:43 - train: epoch 0030, iter [00740, 01251], lr: 0.000533, loss: 0.0910
2022-10-03 04:00:11 - train: epoch 0030, iter [00750, 01251], lr: 0.000532, loss: 0.0975
2022-10-03 04:00:39 - train: epoch 0030, iter [00760, 01251], lr: 0.000532, loss: 0.0943
2022-10-03 04:01:08 - train: epoch 0030, iter [00770, 01251], lr: 0.000532, loss: 0.0999
2022-10-03 04:01:36 - train: epoch 0030, iter [00780, 01251], lr: 0.000532, loss: 0.0916
2022-10-03 04:02:04 - train: epoch 0030, iter [00790, 01251], lr: 0.000532, loss: 0.1010
2022-10-03 04:02:32 - train: epoch 0030, iter [00800, 01251], lr: 0.000532, loss: 0.0994
2022-10-03 04:03:00 - train: epoch 0030, iter [00810, 01251], lr: 0.000532, loss: 0.1096
2022-10-03 04:03:28 - train: epoch 0030, iter [00820, 01251], lr: 0.000532, loss: 0.1056
2022-10-03 04:03:56 - train: epoch 0030, iter [00830, 01251], lr: 0.000532, loss: 0.1041
2022-10-03 04:04:24 - train: epoch 0030, iter [00840, 01251], lr: 0.000532, loss: 0.0995
2022-10-03 04:04:52 - train: epoch 0030, iter [00850, 01251], lr: 0.000532, loss: 0.0954
2022-10-03 04:05:20 - train: epoch 0030, iter [00860, 01251], lr: 0.000532, loss: 0.1075
2022-10-03 04:05:48 - train: epoch 0030, iter [00870, 01251], lr: 0.000532, loss: 0.0957
2022-10-03 04:06:16 - train: epoch 0030, iter [00880, 01251], lr: 0.000532, loss: 0.1012
2022-10-03 04:06:44 - train: epoch 0030, iter [00890, 01251], lr: 0.000532, loss: 0.0976
2022-10-03 04:07:12 - train: epoch 0030, iter [00900, 01251], lr: 0.000532, loss: 0.1013
2022-10-03 04:07:41 - train: epoch 0030, iter [00910, 01251], lr: 0.000532, loss: 0.0990
2022-10-03 04:08:09 - train: epoch 0030, iter [00920, 01251], lr: 0.000532, loss: 0.1029
2022-10-03 04:08:36 - train: epoch 0030, iter [00930, 01251], lr: 0.000532, loss: 0.0884
2022-10-03 04:09:05 - train: epoch 0030, iter [00940, 01251], lr: 0.000531, loss: 0.0953
2022-10-03 04:09:33 - train: epoch 0030, iter [00950, 01251], lr: 0.000531, loss: 0.0914
2022-10-03 04:10:01 - train: epoch 0030, iter [00960, 01251], lr: 0.000531, loss: 0.1036
2022-10-03 04:10:29 - train: epoch 0030, iter [00970, 01251], lr: 0.000531, loss: 0.1081
2022-10-03 04:10:57 - train: epoch 0030, iter [00980, 01251], lr: 0.000531, loss: 0.1041
2022-10-03 04:11:25 - train: epoch 0030, iter [00990, 01251], lr: 0.000531, loss: 0.0973
2022-10-03 04:11:53 - train: epoch 0030, iter [01000, 01251], lr: 0.000531, loss: 0.1001
2022-10-03 04:12:21 - train: epoch 0030, iter [01010, 01251], lr: 0.000531, loss: 0.1011
2022-10-03 04:12:49 - train: epoch 0030, iter [01020, 01251], lr: 0.000531, loss: 0.1036
2022-10-03 04:13:17 - train: epoch 0030, iter [01030, 01251], lr: 0.000531, loss: 0.0974
2022-10-03 04:13:46 - train: epoch 0030, iter [01040, 01251], lr: 0.000531, loss: 0.0913
2022-10-03 04:14:14 - train: epoch 0030, iter [01050, 01251], lr: 0.000531, loss: 0.0985
2022-10-03 04:14:42 - train: epoch 0030, iter [01060, 01251], lr: 0.000531, loss: 0.0922
2022-10-03 04:15:10 - train: epoch 0030, iter [01070, 01251], lr: 0.000531, loss: 0.0951
2022-10-03 04:15:38 - train: epoch 0030, iter [01080, 01251], lr: 0.000531, loss: 0.1032
2022-10-03 04:16:07 - train: epoch 0030, iter [01090, 01251], lr: 0.000531, loss: 0.1025
2022-10-03 04:16:35 - train: epoch 0030, iter [01100, 01251], lr: 0.000531, loss: 0.0953
2022-10-03 04:17:03 - train: epoch 0030, iter [01110, 01251], lr: 0.000531, loss: 0.0870
2022-10-03 04:17:31 - train: epoch 0030, iter [01120, 01251], lr: 0.000531, loss: 0.0981
2022-10-03 04:17:59 - train: epoch 0030, iter [01130, 01251], lr: 0.000530, loss: 0.0927
2022-10-03 04:18:27 - train: epoch 0030, iter [01140, 01251], lr: 0.000530, loss: 0.0997
2022-10-03 04:18:55 - train: epoch 0030, iter [01150, 01251], lr: 0.000530, loss: 0.1001
2022-10-03 04:19:24 - train: epoch 0030, iter [01160, 01251], lr: 0.000530, loss: 0.0916
2022-10-03 04:19:52 - train: epoch 0030, iter [01170, 01251], lr: 0.000530, loss: 0.1023
2022-10-03 04:20:20 - train: epoch 0030, iter [01180, 01251], lr: 0.000530, loss: 0.1019
2022-10-03 04:20:48 - train: epoch 0030, iter [01190, 01251], lr: 0.000530, loss: 0.1005
2022-10-03 04:21:17 - train: epoch 0030, iter [01200, 01251], lr: 0.000530, loss: 0.0976
2022-10-03 04:21:44 - train: epoch 0030, iter [01210, 01251], lr: 0.000530, loss: 0.1001
2022-10-03 04:22:12 - train: epoch 0030, iter [01220, 01251], lr: 0.000530, loss: 0.0987
2022-10-03 04:22:41 - train: epoch 0030, iter [01230, 01251], lr: 0.000530, loss: 0.1017
2022-10-03 04:23:09 - train: epoch 0030, iter [01240, 01251], lr: 0.000530, loss: 0.1024
2022-10-03 04:23:37 - train: epoch 0030, iter [01250, 01251], lr: 0.000530, loss: 0.1017
2022-10-03 04:23:41 - train: epoch 030, train_loss: 0.0983
2022-10-03 04:23:43 - until epoch: 030, best_loss: 0.0983
2022-10-03 04:23:43 - epoch 031 lr: 0.000530
2022-10-03 04:24:18 - train: epoch 0031, iter [00010, 01251], lr: 0.000530, loss: 0.0978
2022-10-03 04:24:46 - train: epoch 0031, iter [00020, 01251], lr: 0.000530, loss: 0.0984
2022-10-03 04:25:13 - train: epoch 0031, iter [00030, 01251], lr: 0.000530, loss: 0.0950
2022-10-03 04:25:42 - train: epoch 0031, iter [00040, 01251], lr: 0.000530, loss: 0.1017
2022-10-03 04:26:10 - train: epoch 0031, iter [00050, 01251], lr: 0.000530, loss: 0.0964
2022-10-03 04:26:38 - train: epoch 0031, iter [00060, 01251], lr: 0.000529, loss: 0.0917
2022-10-03 04:27:06 - train: epoch 0031, iter [00070, 01251], lr: 0.000529, loss: 0.0948
2022-10-03 04:27:34 - train: epoch 0031, iter [00080, 01251], lr: 0.000529, loss: 0.0972
2022-10-03 04:28:02 - train: epoch 0031, iter [00090, 01251], lr: 0.000529, loss: 0.0953
2022-10-03 04:28:30 - train: epoch 0031, iter [00100, 01251], lr: 0.000529, loss: 0.1007
2022-10-03 04:28:58 - train: epoch 0031, iter [00110, 01251], lr: 0.000529, loss: 0.0982
2022-10-03 04:29:26 - train: epoch 0031, iter [00120, 01251], lr: 0.000529, loss: 0.0939
2022-10-03 04:29:54 - train: epoch 0031, iter [00130, 01251], lr: 0.000529, loss: 0.1016
2022-10-03 04:30:22 - train: epoch 0031, iter [00140, 01251], lr: 0.000529, loss: 0.0994
2022-10-03 04:30:50 - train: epoch 0031, iter [00150, 01251], lr: 0.000529, loss: 0.0940
2022-10-03 04:31:19 - train: epoch 0031, iter [00160, 01251], lr: 0.000529, loss: 0.1017
2022-10-03 04:31:47 - train: epoch 0031, iter [00170, 01251], lr: 0.000529, loss: 0.0909
2022-10-03 04:32:15 - train: epoch 0031, iter [00180, 01251], lr: 0.000529, loss: 0.1012
2022-10-03 04:32:43 - train: epoch 0031, iter [00190, 01251], lr: 0.000529, loss: 0.1102
2022-10-03 04:33:11 - train: epoch 0031, iter [00200, 01251], lr: 0.000529, loss: 0.0887
2022-10-03 04:33:39 - train: epoch 0031, iter [00210, 01251], lr: 0.000529, loss: 0.0901
2022-10-03 04:34:07 - train: epoch 0031, iter [00220, 01251], lr: 0.000529, loss: 0.0961
2022-10-03 04:34:35 - train: epoch 0031, iter [00230, 01251], lr: 0.000529, loss: 0.1050
2022-10-03 04:35:03 - train: epoch 0031, iter [00240, 01251], lr: 0.000529, loss: 0.1039
2022-10-03 04:35:31 - train: epoch 0031, iter [00250, 01251], lr: 0.000528, loss: 0.0968
2022-10-03 04:36:00 - train: epoch 0031, iter [00260, 01251], lr: 0.000528, loss: 0.0969
2022-10-03 04:36:28 - train: epoch 0031, iter [00270, 01251], lr: 0.000528, loss: 0.1038
2022-10-03 04:36:56 - train: epoch 0031, iter [00280, 01251], lr: 0.000528, loss: 0.0908
2022-10-03 04:37:24 - train: epoch 0031, iter [00290, 01251], lr: 0.000528, loss: 0.0942
2022-10-03 04:37:52 - train: epoch 0031, iter [00300, 01251], lr: 0.000528, loss: 0.0931
2022-10-03 04:38:20 - train: epoch 0031, iter [00310, 01251], lr: 0.000528, loss: 0.1057
2022-10-03 04:38:48 - train: epoch 0031, iter [00320, 01251], lr: 0.000528, loss: 0.1098
2022-10-03 04:39:16 - train: epoch 0031, iter [00330, 01251], lr: 0.000528, loss: 0.0965
2022-10-03 04:39:45 - train: epoch 0031, iter [00340, 01251], lr: 0.000528, loss: 0.0963
2022-10-03 04:40:13 - train: epoch 0031, iter [00350, 01251], lr: 0.000528, loss: 0.0945
2022-10-03 04:40:41 - train: epoch 0031, iter [00360, 01251], lr: 0.000528, loss: 0.1041
2022-10-03 04:41:09 - train: epoch 0031, iter [00370, 01251], lr: 0.000528, loss: 0.0868
2022-10-03 04:41:37 - train: epoch 0031, iter [00380, 01251], lr: 0.000528, loss: 0.0965
2022-10-03 04:42:05 - train: epoch 0031, iter [00390, 01251], lr: 0.000528, loss: 0.0942
2022-10-03 04:42:33 - train: epoch 0031, iter [00400, 01251], lr: 0.000528, loss: 0.0986
2022-10-03 04:43:01 - train: epoch 0031, iter [00410, 01251], lr: 0.000528, loss: 0.0925
2022-10-03 04:43:29 - train: epoch 0031, iter [00420, 01251], lr: 0.000528, loss: 0.1068
2022-10-03 04:43:57 - train: epoch 0031, iter [00430, 01251], lr: 0.000527, loss: 0.1011
2022-10-03 04:44:25 - train: epoch 0031, iter [00440, 01251], lr: 0.000527, loss: 0.0942
2022-10-03 04:44:53 - train: epoch 0031, iter [00450, 01251], lr: 0.000527, loss: 0.0924
2022-10-03 04:45:21 - train: epoch 0031, iter [00460, 01251], lr: 0.000527, loss: 0.0946
2022-10-03 04:45:49 - train: epoch 0031, iter [00470, 01251], lr: 0.000527, loss: 0.0923
2022-10-03 04:46:17 - train: epoch 0031, iter [00480, 01251], lr: 0.000527, loss: 0.1061
2022-10-03 04:46:46 - train: epoch 0031, iter [00490, 01251], lr: 0.000527, loss: 0.0903
2022-10-03 04:47:14 - train: epoch 0031, iter [00500, 01251], lr: 0.000527, loss: 0.0910
2022-10-03 04:47:42 - train: epoch 0031, iter [00510, 01251], lr: 0.000527, loss: 0.0947
2022-10-03 04:48:10 - train: epoch 0031, iter [00520, 01251], lr: 0.000527, loss: 0.1084
2022-10-03 04:48:38 - train: epoch 0031, iter [00530, 01251], lr: 0.000527, loss: 0.0896
2022-10-03 04:49:06 - train: epoch 0031, iter [00540, 01251], lr: 0.000527, loss: 0.0952
2022-10-03 04:49:34 - train: epoch 0031, iter [00550, 01251], lr: 0.000527, loss: 0.0967
2022-10-03 04:50:02 - train: epoch 0031, iter [00560, 01251], lr: 0.000527, loss: 0.0980
2022-10-03 04:50:30 - train: epoch 0031, iter [00570, 01251], lr: 0.000527, loss: 0.0995
2022-10-03 04:50:58 - train: epoch 0031, iter [00580, 01251], lr: 0.000527, loss: 0.1013
2022-10-03 04:51:26 - train: epoch 0031, iter [00590, 01251], lr: 0.000527, loss: 0.0992
2022-10-03 04:51:54 - train: epoch 0031, iter [00600, 01251], lr: 0.000527, loss: 0.0967
2022-10-03 04:52:23 - train: epoch 0031, iter [00610, 01251], lr: 0.000526, loss: 0.0929
2022-10-03 04:52:51 - train: epoch 0031, iter [00620, 01251], lr: 0.000526, loss: 0.1007
2022-10-03 04:53:19 - train: epoch 0031, iter [00630, 01251], lr: 0.000526, loss: 0.0932
2022-10-03 04:53:47 - train: epoch 0031, iter [00640, 01251], lr: 0.000526, loss: 0.1008
2022-10-03 04:54:15 - train: epoch 0031, iter [00650, 01251], lr: 0.000526, loss: 0.0921
2022-10-03 04:54:44 - train: epoch 0031, iter [00660, 01251], lr: 0.000526, loss: 0.0970
2022-10-03 04:55:11 - train: epoch 0031, iter [00670, 01251], lr: 0.000526, loss: 0.1002
2022-10-03 04:55:39 - train: epoch 0031, iter [00680, 01251], lr: 0.000526, loss: 0.0966
2022-10-03 04:56:08 - train: epoch 0031, iter [00690, 01251], lr: 0.000526, loss: 0.0953
2022-10-03 04:56:36 - train: epoch 0031, iter [00700, 01251], lr: 0.000526, loss: 0.0952
2022-10-03 04:57:04 - train: epoch 0031, iter [00710, 01251], lr: 0.000526, loss: 0.0959
2022-10-03 04:57:32 - train: epoch 0031, iter [00720, 01251], lr: 0.000526, loss: 0.1017
2022-10-03 04:58:00 - train: epoch 0031, iter [00730, 01251], lr: 0.000526, loss: 0.1046
2022-10-03 04:58:28 - train: epoch 0031, iter [00740, 01251], lr: 0.000526, loss: 0.0941
2022-10-03 04:58:57 - train: epoch 0031, iter [00750, 01251], lr: 0.000526, loss: 0.0973
2022-10-03 04:59:25 - train: epoch 0031, iter [00760, 01251], lr: 0.000526, loss: 0.0935
2022-10-03 04:59:53 - train: epoch 0031, iter [00770, 01251], lr: 0.000526, loss: 0.0916
2022-10-03 05:00:21 - train: epoch 0031, iter [00780, 01251], lr: 0.000526, loss: 0.0998
2022-10-03 05:00:49 - train: epoch 0031, iter [00790, 01251], lr: 0.000526, loss: 0.0914
2022-10-03 05:01:18 - train: epoch 0031, iter [00800, 01251], lr: 0.000525, loss: 0.0960
2022-10-03 05:01:46 - train: epoch 0031, iter [00810, 01251], lr: 0.000525, loss: 0.1020
2022-10-03 05:02:14 - train: epoch 0031, iter [00820, 01251], lr: 0.000525, loss: 0.0973
2022-10-03 05:02:42 - train: epoch 0031, iter [00830, 01251], lr: 0.000525, loss: 0.1031
2022-10-03 05:03:11 - train: epoch 0031, iter [00840, 01251], lr: 0.000525, loss: 0.0965
2022-10-03 05:03:39 - train: epoch 0031, iter [00850, 01251], lr: 0.000525, loss: 0.0993
2022-10-03 05:04:07 - train: epoch 0031, iter [00860, 01251], lr: 0.000525, loss: 0.0910
2022-10-03 05:04:35 - train: epoch 0031, iter [00870, 01251], lr: 0.000525, loss: 0.0983
2022-10-03 05:05:03 - train: epoch 0031, iter [00880, 01251], lr: 0.000525, loss: 0.1079
2022-10-03 05:05:32 - train: epoch 0031, iter [00890, 01251], lr: 0.000525, loss: 0.1076
2022-10-03 05:06:00 - train: epoch 0031, iter [00900, 01251], lr: 0.000525, loss: 0.1079
2022-10-03 05:06:28 - train: epoch 0031, iter [00910, 01251], lr: 0.000525, loss: 0.1047
2022-10-03 05:06:56 - train: epoch 0031, iter [00920, 01251], lr: 0.000525, loss: 0.1070
2022-10-03 05:07:25 - train: epoch 0031, iter [00930, 01251], lr: 0.000525, loss: 0.0907
2022-10-03 05:07:53 - train: epoch 0031, iter [00940, 01251], lr: 0.000525, loss: 0.0963
2022-10-03 05:08:21 - train: epoch 0031, iter [00950, 01251], lr: 0.000525, loss: 0.0995
2022-10-03 05:08:50 - train: epoch 0031, iter [00960, 01251], lr: 0.000525, loss: 0.0953
2022-10-03 05:09:18 - train: epoch 0031, iter [00970, 01251], lr: 0.000525, loss: 0.0982
2022-10-03 05:09:46 - train: epoch 0031, iter [00980, 01251], lr: 0.000524, loss: 0.1046
2022-10-03 05:10:14 - train: epoch 0031, iter [00990, 01251], lr: 0.000524, loss: 0.0970
2022-10-03 05:10:42 - train: epoch 0031, iter [01000, 01251], lr: 0.000524, loss: 0.0939
2022-10-03 05:11:10 - train: epoch 0031, iter [01010, 01251], lr: 0.000524, loss: 0.0985
2022-10-03 05:11:39 - train: epoch 0031, iter [01020, 01251], lr: 0.000524, loss: 0.1096
2022-10-03 05:12:07 - train: epoch 0031, iter [01030, 01251], lr: 0.000524, loss: 0.1050
2022-10-03 05:12:35 - train: epoch 0031, iter [01040, 01251], lr: 0.000524, loss: 0.0969
2022-10-03 05:13:03 - train: epoch 0031, iter [01050, 01251], lr: 0.000524, loss: 0.0902
2022-10-03 05:13:31 - train: epoch 0031, iter [01060, 01251], lr: 0.000524, loss: 0.0949
2022-10-03 05:13:59 - train: epoch 0031, iter [01070, 01251], lr: 0.000524, loss: 0.0859
2022-10-03 05:14:27 - train: epoch 0031, iter [01080, 01251], lr: 0.000524, loss: 0.0911
2022-10-03 05:14:55 - train: epoch 0031, iter [01090, 01251], lr: 0.000524, loss: 0.1003
2022-10-03 05:15:23 - train: epoch 0031, iter [01100, 01251], lr: 0.000524, loss: 0.1020
2022-10-03 05:15:52 - train: epoch 0031, iter [01110, 01251], lr: 0.000524, loss: 0.0967
2022-10-03 05:16:20 - train: epoch 0031, iter [01120, 01251], lr: 0.000524, loss: 0.0923
2022-10-03 05:16:48 - train: epoch 0031, iter [01130, 01251], lr: 0.000524, loss: 0.0934
2022-10-03 05:17:16 - train: epoch 0031, iter [01140, 01251], lr: 0.000524, loss: 0.0909
2022-10-03 05:17:44 - train: epoch 0031, iter [01150, 01251], lr: 0.000524, loss: 0.0992
2022-10-03 05:18:12 - train: epoch 0031, iter [01160, 01251], lr: 0.000523, loss: 0.0852
2022-10-03 05:18:41 - train: epoch 0031, iter [01170, 01251], lr: 0.000523, loss: 0.0930
2022-10-03 05:19:09 - train: epoch 0031, iter [01180, 01251], lr: 0.000523, loss: 0.0973
2022-10-03 05:19:37 - train: epoch 0031, iter [01190, 01251], lr: 0.000523, loss: 0.0996
2022-10-03 05:20:05 - train: epoch 0031, iter [01200, 01251], lr: 0.000523, loss: 0.0972
2022-10-03 05:20:33 - train: epoch 0031, iter [01210, 01251], lr: 0.000523, loss: 0.0930
2022-10-03 05:21:01 - train: epoch 0031, iter [01220, 01251], lr: 0.000523, loss: 0.0925
2022-10-03 05:21:29 - train: epoch 0031, iter [01230, 01251], lr: 0.000523, loss: 0.0889
2022-10-03 05:21:57 - train: epoch 0031, iter [01240, 01251], lr: 0.000523, loss: 0.1039
2022-10-03 05:22:25 - train: epoch 0031, iter [01250, 01251], lr: 0.000523, loss: 0.0937
2022-10-03 05:22:29 - train: epoch 031, train_loss: 0.0976
2022-10-03 05:22:31 - until epoch: 031, best_loss: 0.0976
2022-10-03 05:22:31 - epoch 032 lr: 0.000523
2022-10-03 05:23:06 - train: epoch 0032, iter [00010, 01251], lr: 0.000523, loss: 0.0998
2022-10-03 05:23:33 - train: epoch 0032, iter [00020, 01251], lr: 0.000523, loss: 0.0959
2022-10-03 05:24:02 - train: epoch 0032, iter [00030, 01251], lr: 0.000523, loss: 0.0922
2022-10-03 05:24:30 - train: epoch 0032, iter [00040, 01251], lr: 0.000523, loss: 0.0910
2022-10-03 05:24:58 - train: epoch 0032, iter [00050, 01251], lr: 0.000523, loss: 0.0995
2022-10-03 05:25:26 - train: epoch 0032, iter [00060, 01251], lr: 0.000523, loss: 0.0929
2022-10-03 05:25:54 - train: epoch 0032, iter [00070, 01251], lr: 0.000523, loss: 0.0959
2022-10-03 05:26:22 - train: epoch 0032, iter [00080, 01251], lr: 0.000522, loss: 0.0966
2022-10-03 05:26:51 - train: epoch 0032, iter [00090, 01251], lr: 0.000522, loss: 0.0919
2022-10-03 05:27:19 - train: epoch 0032, iter [00100, 01251], lr: 0.000522, loss: 0.1019
2022-10-03 05:27:47 - train: epoch 0032, iter [00110, 01251], lr: 0.000522, loss: 0.0936
2022-10-03 05:28:15 - train: epoch 0032, iter [00120, 01251], lr: 0.000522, loss: 0.1041
2022-10-03 05:28:43 - train: epoch 0032, iter [00130, 01251], lr: 0.000522, loss: 0.0901
2022-10-03 05:29:11 - train: epoch 0032, iter [00140, 01251], lr: 0.000522, loss: 0.0952
2022-10-03 05:29:39 - train: epoch 0032, iter [00150, 01251], lr: 0.000522, loss: 0.0972
2022-10-03 05:30:07 - train: epoch 0032, iter [00160, 01251], lr: 0.000522, loss: 0.0952
2022-10-03 05:30:35 - train: epoch 0032, iter [00170, 01251], lr: 0.000522, loss: 0.1063
2022-10-03 05:31:03 - train: epoch 0032, iter [00180, 01251], lr: 0.000522, loss: 0.0967
2022-10-03 05:31:32 - train: epoch 0032, iter [00190, 01251], lr: 0.000522, loss: 0.0955
2022-10-03 05:32:00 - train: epoch 0032, iter [00200, 01251], lr: 0.000522, loss: 0.1045
2022-10-03 05:32:28 - train: epoch 0032, iter [00210, 01251], lr: 0.000522, loss: 0.0971
2022-10-03 05:32:56 - train: epoch 0032, iter [00220, 01251], lr: 0.000522, loss: 0.1010
2022-10-03 05:33:25 - train: epoch 0032, iter [00230, 01251], lr: 0.000522, loss: 0.1002
2022-10-03 05:33:53 - train: epoch 0032, iter [00240, 01251], lr: 0.000522, loss: 0.0961
2022-10-03 05:34:21 - train: epoch 0032, iter [00250, 01251], lr: 0.000522, loss: 0.0922
2022-10-03 05:34:49 - train: epoch 0032, iter [00260, 01251], lr: 0.000521, loss: 0.1004
2022-10-03 05:35:17 - train: epoch 0032, iter [00270, 01251], lr: 0.000521, loss: 0.1019
2022-10-03 05:35:45 - train: epoch 0032, iter [00280, 01251], lr: 0.000521, loss: 0.0939
2022-10-03 05:36:14 - train: epoch 0032, iter [00290, 01251], lr: 0.000521, loss: 0.0886
2022-10-03 05:36:42 - train: epoch 0032, iter [00300, 01251], lr: 0.000521, loss: 0.1036
2022-10-03 05:37:10 - train: epoch 0032, iter [00310, 01251], lr: 0.000521, loss: 0.1026
2022-10-03 05:37:38 - train: epoch 0032, iter [00320, 01251], lr: 0.000521, loss: 0.1019
2022-10-03 05:38:06 - train: epoch 0032, iter [00330, 01251], lr: 0.000521, loss: 0.0961
2022-10-03 05:38:35 - train: epoch 0032, iter [00340, 01251], lr: 0.000521, loss: 0.1046
2022-10-03 05:39:03 - train: epoch 0032, iter [00350, 01251], lr: 0.000521, loss: 0.0983
2022-10-03 05:39:31 - train: epoch 0032, iter [00360, 01251], lr: 0.000521, loss: 0.1016
2022-10-03 05:39:59 - train: epoch 0032, iter [00370, 01251], lr: 0.000521, loss: 0.1069
2022-10-03 05:40:27 - train: epoch 0032, iter [00380, 01251], lr: 0.000521, loss: 0.0922
2022-10-03 05:40:55 - train: epoch 0032, iter [00390, 01251], lr: 0.000521, loss: 0.0973
2022-10-03 05:41:23 - train: epoch 0032, iter [00400, 01251], lr: 0.000521, loss: 0.1000
2022-10-03 05:41:52 - train: epoch 0032, iter [00410, 01251], lr: 0.000521, loss: 0.0999
2022-10-03 05:42:20 - train: epoch 0032, iter [00420, 01251], lr: 0.000521, loss: 0.1014
2022-10-03 05:42:48 - train: epoch 0032, iter [00430, 01251], lr: 0.000521, loss: 0.0914
2022-10-03 05:43:16 - train: epoch 0032, iter [00440, 01251], lr: 0.000520, loss: 0.0966
2022-10-03 05:43:44 - train: epoch 0032, iter [00450, 01251], lr: 0.000520, loss: 0.0939
2022-10-03 05:44:13 - train: epoch 0032, iter [00460, 01251], lr: 0.000520, loss: 0.0987
2022-10-03 05:44:41 - train: epoch 0032, iter [00470, 01251], lr: 0.000520, loss: 0.0955
2022-10-03 05:45:09 - train: epoch 0032, iter [00480, 01251], lr: 0.000520, loss: 0.0976
2022-10-03 05:45:37 - train: epoch 0032, iter [00490, 01251], lr: 0.000520, loss: 0.0927
2022-10-03 05:46:06 - train: epoch 0032, iter [00500, 01251], lr: 0.000520, loss: 0.1022
2022-10-03 05:46:34 - train: epoch 0032, iter [00510, 01251], lr: 0.000520, loss: 0.0925
2022-10-03 05:47:02 - train: epoch 0032, iter [00520, 01251], lr: 0.000520, loss: 0.0932
2022-10-03 05:47:30 - train: epoch 0032, iter [00530, 01251], lr: 0.000520, loss: 0.0908
2022-10-03 05:47:59 - train: epoch 0032, iter [00540, 01251], lr: 0.000520, loss: 0.0924
2022-10-03 05:48:27 - train: epoch 0032, iter [00550, 01251], lr: 0.000520, loss: 0.0919
2022-10-03 05:48:55 - train: epoch 0032, iter [00560, 01251], lr: 0.000520, loss: 0.1046
2022-10-03 05:49:24 - train: epoch 0032, iter [00570, 01251], lr: 0.000520, loss: 0.0906
2022-10-03 05:49:52 - train: epoch 0032, iter [00580, 01251], lr: 0.000520, loss: 0.1000
2022-10-03 05:50:20 - train: epoch 0032, iter [00590, 01251], lr: 0.000520, loss: 0.0971
2022-10-03 05:50:49 - train: epoch 0032, iter [00600, 01251], lr: 0.000520, loss: 0.0914
2022-10-03 05:51:17 - train: epoch 0032, iter [00610, 01251], lr: 0.000519, loss: 0.0991
2022-10-03 05:51:45 - train: epoch 0032, iter [00620, 01251], lr: 0.000519, loss: 0.0997
2022-10-03 05:52:13 - train: epoch 0032, iter [00630, 01251], lr: 0.000519, loss: 0.1002
2022-10-03 05:52:41 - train: epoch 0032, iter [00640, 01251], lr: 0.000519, loss: 0.0904
2022-10-03 05:53:09 - train: epoch 0032, iter [00650, 01251], lr: 0.000519, loss: 0.0992
2022-10-03 05:53:38 - train: epoch 0032, iter [00660, 01251], lr: 0.000519, loss: 0.0990
2022-10-03 05:54:06 - train: epoch 0032, iter [00670, 01251], lr: 0.000519, loss: 0.0981
2022-10-03 05:54:34 - train: epoch 0032, iter [00680, 01251], lr: 0.000519, loss: 0.0916
2022-10-03 05:55:02 - train: epoch 0032, iter [00690, 01251], lr: 0.000519, loss: 0.0921
2022-10-03 05:55:31 - train: epoch 0032, iter [00700, 01251], lr: 0.000519, loss: 0.0945
2022-10-03 05:55:59 - train: epoch 0032, iter [00710, 01251], lr: 0.000519, loss: 0.0969
2022-10-03 05:56:27 - train: epoch 0032, iter [00720, 01251], lr: 0.000519, loss: 0.0961
2022-10-03 05:56:55 - train: epoch 0032, iter [00730, 01251], lr: 0.000519, loss: 0.0961
2022-10-03 05:57:23 - train: epoch 0032, iter [00740, 01251], lr: 0.000519, loss: 0.0966
2022-10-03 05:57:51 - train: epoch 0032, iter [00750, 01251], lr: 0.000519, loss: 0.0920
2022-10-03 05:58:19 - train: epoch 0032, iter [00760, 01251], lr: 0.000519, loss: 0.1004
2022-10-03 05:58:48 - train: epoch 0032, iter [00770, 01251], lr: 0.000519, loss: 0.0970
2022-10-03 05:59:16 - train: epoch 0032, iter [00780, 01251], lr: 0.000519, loss: 0.0904
2022-10-03 05:59:44 - train: epoch 0032, iter [00790, 01251], lr: 0.000518, loss: 0.0950
2022-10-03 06:00:12 - train: epoch 0032, iter [00800, 01251], lr: 0.000518, loss: 0.0892
2022-10-03 06:00:40 - train: epoch 0032, iter [00810, 01251], lr: 0.000518, loss: 0.1038
2022-10-03 06:01:08 - train: epoch 0032, iter [00820, 01251], lr: 0.000518, loss: 0.0945
2022-10-03 06:01:36 - train: epoch 0032, iter [00830, 01251], lr: 0.000518, loss: 0.0946
2022-10-03 06:02:04 - train: epoch 0032, iter [00840, 01251], lr: 0.000518, loss: 0.0884
2022-10-03 06:02:32 - train: epoch 0032, iter [00850, 01251], lr: 0.000518, loss: 0.1076
2022-10-03 06:03:01 - train: epoch 0032, iter [00860, 01251], lr: 0.000518, loss: 0.0934
2022-10-03 06:03:29 - train: epoch 0032, iter [00870, 01251], lr: 0.000518, loss: 0.0964
2022-10-03 06:03:57 - train: epoch 0032, iter [00880, 01251], lr: 0.000518, loss: 0.0980
2022-10-03 06:04:25 - train: epoch 0032, iter [00890, 01251], lr: 0.000518, loss: 0.0952
2022-10-03 06:04:53 - train: epoch 0032, iter [00900, 01251], lr: 0.000518, loss: 0.1002
2022-10-03 06:05:21 - train: epoch 0032, iter [00910, 01251], lr: 0.000518, loss: 0.0919
2022-10-03 06:05:49 - train: epoch 0032, iter [00920, 01251], lr: 0.000518, loss: 0.0973
2022-10-03 06:06:18 - train: epoch 0032, iter [00930, 01251], lr: 0.000518, loss: 0.0876
2022-10-03 06:06:46 - train: epoch 0032, iter [00940, 01251], lr: 0.000518, loss: 0.1000
2022-10-03 06:07:14 - train: epoch 0032, iter [00950, 01251], lr: 0.000518, loss: 0.0881
2022-10-03 06:07:42 - train: epoch 0032, iter [00960, 01251], lr: 0.000517, loss: 0.0908
2022-10-03 06:08:10 - train: epoch 0032, iter [00970, 01251], lr: 0.000517, loss: 0.0927
2022-10-03 06:08:38 - train: epoch 0032, iter [00980, 01251], lr: 0.000517, loss: 0.0993
2022-10-03 06:09:06 - train: epoch 0032, iter [00990, 01251], lr: 0.000517, loss: 0.1018
2022-10-03 06:09:34 - train: epoch 0032, iter [01000, 01251], lr: 0.000517, loss: 0.0937
2022-10-03 06:10:02 - train: epoch 0032, iter [01010, 01251], lr: 0.000517, loss: 0.1031
2022-10-03 06:10:30 - train: epoch 0032, iter [01020, 01251], lr: 0.000517, loss: 0.0941
2022-10-03 06:10:58 - train: epoch 0032, iter [01030, 01251], lr: 0.000517, loss: 0.1033
2022-10-03 06:11:26 - train: epoch 0032, iter [01040, 01251], lr: 0.000517, loss: 0.0932
2022-10-03 06:11:54 - train: epoch 0032, iter [01050, 01251], lr: 0.000517, loss: 0.0936
2022-10-03 06:12:22 - train: epoch 0032, iter [01060, 01251], lr: 0.000517, loss: 0.1058
2022-10-03 06:12:50 - train: epoch 0032, iter [01070, 01251], lr: 0.000517, loss: 0.0909
2022-10-03 06:13:18 - train: epoch 0032, iter [01080, 01251], lr: 0.000517, loss: 0.1061
2022-10-03 06:13:46 - train: epoch 0032, iter [01090, 01251], lr: 0.000517, loss: 0.1007
2022-10-03 06:14:15 - train: epoch 0032, iter [01100, 01251], lr: 0.000517, loss: 0.0966
2022-10-03 06:14:43 - train: epoch 0032, iter [01110, 01251], lr: 0.000517, loss: 0.0967
2022-10-03 06:15:11 - train: epoch 0032, iter [01120, 01251], lr: 0.000517, loss: 0.0996
2022-10-03 06:15:39 - train: epoch 0032, iter [01130, 01251], lr: 0.000517, loss: 0.0969
2022-10-03 06:16:07 - train: epoch 0032, iter [01140, 01251], lr: 0.000516, loss: 0.0931
2022-10-03 06:16:35 - train: epoch 0032, iter [01150, 01251], lr: 0.000516, loss: 0.0994
2022-10-03 06:17:03 - train: epoch 0032, iter [01160, 01251], lr: 0.000516, loss: 0.0987
2022-10-03 06:17:32 - train: epoch 0032, iter [01170, 01251], lr: 0.000516, loss: 0.0985
2022-10-03 06:18:00 - train: epoch 0032, iter [01180, 01251], lr: 0.000516, loss: 0.0948
2022-10-03 06:18:28 - train: epoch 0032, iter [01190, 01251], lr: 0.000516, loss: 0.0979
2022-10-03 06:18:56 - train: epoch 0032, iter [01200, 01251], lr: 0.000516, loss: 0.0982
2022-10-03 06:19:24 - train: epoch 0032, iter [01210, 01251], lr: 0.000516, loss: 0.1031
2022-10-03 06:19:53 - train: epoch 0032, iter [01220, 01251], lr: 0.000516, loss: 0.0994
2022-10-03 06:20:21 - train: epoch 0032, iter [01230, 01251], lr: 0.000516, loss: 0.0928
2022-10-03 06:20:49 - train: epoch 0032, iter [01240, 01251], lr: 0.000516, loss: 0.1028
2022-10-03 06:21:17 - train: epoch 0032, iter [01250, 01251], lr: 0.000516, loss: 0.0978
2022-10-03 06:21:21 - train: epoch 032, train_loss: 0.0969
2022-10-03 06:21:23 - until epoch: 032, best_loss: 0.0969
2022-10-03 06:21:23 - epoch 033 lr: 0.000516
2022-10-03 06:21:57 - train: epoch 0033, iter [00010, 01251], lr: 0.000516, loss: 0.0945
2022-10-03 06:22:25 - train: epoch 0033, iter [00020, 01251], lr: 0.000516, loss: 0.0997
2022-10-03 06:22:54 - train: epoch 0033, iter [00030, 01251], lr: 0.000516, loss: 0.0948
2022-10-03 06:23:22 - train: epoch 0033, iter [00040, 01251], lr: 0.000516, loss: 0.0896
2022-10-03 06:23:50 - train: epoch 0033, iter [00050, 01251], lr: 0.000516, loss: 0.1031
2022-10-03 06:24:18 - train: epoch 0033, iter [00060, 01251], lr: 0.000515, loss: 0.0929
2022-10-03 06:24:47 - train: epoch 0033, iter [00070, 01251], lr: 0.000515, loss: 0.0871
2022-10-03 06:25:15 - train: epoch 0033, iter [00080, 01251], lr: 0.000515, loss: 0.0879
2022-10-03 06:25:43 - train: epoch 0033, iter [00090, 01251], lr: 0.000515, loss: 0.1010
2022-10-03 06:26:11 - train: epoch 0033, iter [00100, 01251], lr: 0.000515, loss: 0.0895
2022-10-03 06:26:39 - train: epoch 0033, iter [00110, 01251], lr: 0.000515, loss: 0.0863
2022-10-03 06:27:08 - train: epoch 0033, iter [00120, 01251], lr: 0.000515, loss: 0.0893
2022-10-03 06:27:36 - train: epoch 0033, iter [00130, 01251], lr: 0.000515, loss: 0.0933
2022-10-03 06:28:04 - train: epoch 0033, iter [00140, 01251], lr: 0.000515, loss: 0.1012
2022-10-03 06:28:32 - train: epoch 0033, iter [00150, 01251], lr: 0.000515, loss: 0.1131
2022-10-03 06:29:01 - train: epoch 0033, iter [00160, 01251], lr: 0.000515, loss: 0.0979
2022-10-03 06:29:29 - train: epoch 0033, iter [00170, 01251], lr: 0.000515, loss: 0.0929
2022-10-03 06:29:57 - train: epoch 0033, iter [00180, 01251], lr: 0.000515, loss: 0.0876
2022-10-03 06:30:25 - train: epoch 0033, iter [00190, 01251], lr: 0.000515, loss: 0.0945
2022-10-03 06:30:54 - train: epoch 0033, iter [00200, 01251], lr: 0.000515, loss: 0.1013
2022-10-03 06:31:22 - train: epoch 0033, iter [00210, 01251], lr: 0.000515, loss: 0.0970
2022-10-03 06:31:50 - train: epoch 0033, iter [00220, 01251], lr: 0.000515, loss: 0.0964
2022-10-03 06:32:18 - train: epoch 0033, iter [00230, 01251], lr: 0.000514, loss: 0.0917
2022-10-03 06:32:46 - train: epoch 0033, iter [00240, 01251], lr: 0.000514, loss: 0.0963
2022-10-03 06:33:15 - train: epoch 0033, iter [00250, 01251], lr: 0.000514, loss: 0.0925
2022-10-03 06:33:43 - train: epoch 0033, iter [00260, 01251], lr: 0.000514, loss: 0.0933
2022-10-03 06:34:11 - train: epoch 0033, iter [00270, 01251], lr: 0.000514, loss: 0.0918
2022-10-03 06:34:39 - train: epoch 0033, iter [00280, 01251], lr: 0.000514, loss: 0.0897
2022-10-03 06:35:07 - train: epoch 0033, iter [00290, 01251], lr: 0.000514, loss: 0.1018
2022-10-03 06:35:35 - train: epoch 0033, iter [00300, 01251], lr: 0.000514, loss: 0.0962
2022-10-03 06:36:04 - train: epoch 0033, iter [00310, 01251], lr: 0.000514, loss: 0.0971
2022-10-03 06:36:32 - train: epoch 0033, iter [00320, 01251], lr: 0.000514, loss: 0.0969
2022-10-03 06:37:00 - train: epoch 0033, iter [00330, 01251], lr: 0.000514, loss: 0.0965
2022-10-03 06:37:28 - train: epoch 0033, iter [00340, 01251], lr: 0.000514, loss: 0.0941
2022-10-03 06:37:57 - train: epoch 0033, iter [00350, 01251], lr: 0.000514, loss: 0.0919
2022-10-03 06:38:25 - train: epoch 0033, iter [00360, 01251], lr: 0.000514, loss: 0.0917
2022-10-03 06:38:53 - train: epoch 0033, iter [00370, 01251], lr: 0.000514, loss: 0.1051
2022-10-03 06:39:22 - train: epoch 0033, iter [00380, 01251], lr: 0.000514, loss: 0.0870
2022-10-03 06:39:50 - train: epoch 0033, iter [00390, 01251], lr: 0.000514, loss: 0.0911
2022-10-03 06:40:18 - train: epoch 0033, iter [00400, 01251], lr: 0.000513, loss: 0.0915
2022-10-03 06:40:46 - train: epoch 0033, iter [00410, 01251], lr: 0.000513, loss: 0.0918
2022-10-03 06:41:15 - train: epoch 0033, iter [00420, 01251], lr: 0.000513, loss: 0.0975
2022-10-03 06:41:43 - train: epoch 0033, iter [00430, 01251], lr: 0.000513, loss: 0.1026
2022-10-03 06:42:11 - train: epoch 0033, iter [00440, 01251], lr: 0.000513, loss: 0.0952
2022-10-03 06:42:39 - train: epoch 0033, iter [00450, 01251], lr: 0.000513, loss: 0.0948
2022-10-03 06:43:07 - train: epoch 0033, iter [00460, 01251], lr: 0.000513, loss: 0.0993
2022-10-03 06:43:35 - train: epoch 0033, iter [00470, 01251], lr: 0.000513, loss: 0.1005
2022-10-03 06:44:03 - train: epoch 0033, iter [00480, 01251], lr: 0.000513, loss: 0.0911
2022-10-03 06:44:32 - train: epoch 0033, iter [00490, 01251], lr: 0.000513, loss: 0.0918
2022-10-03 06:45:00 - train: epoch 0033, iter [00500, 01251], lr: 0.000513, loss: 0.1027
2022-10-03 06:45:28 - train: epoch 0033, iter [00510, 01251], lr: 0.000513, loss: 0.1035
2022-10-03 06:45:56 - train: epoch 0033, iter [00520, 01251], lr: 0.000513, loss: 0.0994
2022-10-03 06:46:24 - train: epoch 0033, iter [00530, 01251], lr: 0.000513, loss: 0.0947
2022-10-03 06:46:52 - train: epoch 0033, iter [00540, 01251], lr: 0.000513, loss: 0.0972
2022-10-03 06:47:20 - train: epoch 0033, iter [00550, 01251], lr: 0.000513, loss: 0.1055
2022-10-03 06:47:48 - train: epoch 0033, iter [00560, 01251], lr: 0.000513, loss: 0.0896
2022-10-03 06:48:17 - train: epoch 0033, iter [00570, 01251], lr: 0.000512, loss: 0.0889
2022-10-03 06:48:45 - train: epoch 0033, iter [00580, 01251], lr: 0.000512, loss: 0.0925
2022-10-03 06:49:13 - train: epoch 0033, iter [00590, 01251], lr: 0.000512, loss: 0.0939
2022-10-03 06:49:41 - train: epoch 0033, iter [00600, 01251], lr: 0.000512, loss: 0.0969
2022-10-03 06:50:09 - train: epoch 0033, iter [00610, 01251], lr: 0.000512, loss: 0.0840
2022-10-03 06:50:37 - train: epoch 0033, iter [00620, 01251], lr: 0.000512, loss: 0.0932
2022-10-03 06:51:05 - train: epoch 0033, iter [00630, 01251], lr: 0.000512, loss: 0.0882
2022-10-03 06:51:33 - train: epoch 0033, iter [00640, 01251], lr: 0.000512, loss: 0.1021
2022-10-03 06:52:02 - train: epoch 0033, iter [00650, 01251], lr: 0.000512, loss: 0.0956
2022-10-03 06:52:30 - train: epoch 0033, iter [00660, 01251], lr: 0.000512, loss: 0.1013
2022-10-03 06:52:58 - train: epoch 0033, iter [00670, 01251], lr: 0.000512, loss: 0.0992
2022-10-03 06:53:26 - train: epoch 0033, iter [00680, 01251], lr: 0.000512, loss: 0.0943
2022-10-03 06:53:55 - train: epoch 0033, iter [00690, 01251], lr: 0.000512, loss: 0.1020
2022-10-03 06:54:23 - train: epoch 0033, iter [00700, 01251], lr: 0.000512, loss: 0.1007
2022-10-03 06:54:51 - train: epoch 0033, iter [00710, 01251], lr: 0.000512, loss: 0.0894
2022-10-03 06:55:19 - train: epoch 0033, iter [00720, 01251], lr: 0.000512, loss: 0.1003
2022-10-03 06:55:47 - train: epoch 0033, iter [00730, 01251], lr: 0.000512, loss: 0.0988
2022-10-03 06:56:15 - train: epoch 0033, iter [00740, 01251], lr: 0.000511, loss: 0.1021
2022-10-03 06:56:43 - train: epoch 0033, iter [00750, 01251], lr: 0.000511, loss: 0.1027
2022-10-03 06:57:12 - train: epoch 0033, iter [00760, 01251], lr: 0.000511, loss: 0.1091
2022-10-03 06:57:40 - train: epoch 0033, iter [00770, 01251], lr: 0.000511, loss: 0.0933
2022-10-03 06:58:08 - train: epoch 0033, iter [00780, 01251], lr: 0.000511, loss: 0.1010
2022-10-03 06:58:36 - train: epoch 0033, iter [00790, 01251], lr: 0.000511, loss: 0.0947
2022-10-03 06:59:04 - train: epoch 0033, iter [00800, 01251], lr: 0.000511, loss: 0.1023
2022-10-03 06:59:32 - train: epoch 0033, iter [00810, 01251], lr: 0.000511, loss: 0.1018
2022-10-03 07:00:00 - train: epoch 0033, iter [00820, 01251], lr: 0.000511, loss: 0.1027
2022-10-03 07:00:29 - train: epoch 0033, iter [00830, 01251], lr: 0.000511, loss: 0.1030
2022-10-03 07:00:57 - train: epoch 0033, iter [00840, 01251], lr: 0.000511, loss: 0.1023
2022-10-03 07:01:25 - train: epoch 0033, iter [00850, 01251], lr: 0.000511, loss: 0.1061
2022-10-03 07:01:53 - train: epoch 0033, iter [00860, 01251], lr: 0.000511, loss: 0.0993
2022-10-03 07:02:21 - train: epoch 0033, iter [00870, 01251], lr: 0.000511, loss: 0.1004
2022-10-03 07:02:48 - train: epoch 0033, iter [00880, 01251], lr: 0.000511, loss: 0.0955
2022-10-03 07:03:17 - train: epoch 0033, iter [00890, 01251], lr: 0.000511, loss: 0.0938
2022-10-03 07:03:44 - train: epoch 0033, iter [00900, 01251], lr: 0.000511, loss: 0.0948
2022-10-03 07:04:13 - train: epoch 0033, iter [00910, 01251], lr: 0.000510, loss: 0.0965
2022-10-03 07:04:40 - train: epoch 0033, iter [00920, 01251], lr: 0.000510, loss: 0.0939
2022-10-03 07:05:08 - train: epoch 0033, iter [00930, 01251], lr: 0.000510, loss: 0.0918
2022-10-03 07:05:37 - train: epoch 0033, iter [00940, 01251], lr: 0.000510, loss: 0.1088
2022-10-03 07:06:04 - train: epoch 0033, iter [00950, 01251], lr: 0.000510, loss: 0.0965
2022-10-03 07:06:33 - train: epoch 0033, iter [00960, 01251], lr: 0.000510, loss: 0.0981
2022-10-03 07:07:01 - train: epoch 0033, iter [00970, 01251], lr: 0.000510, loss: 0.0970
2022-10-03 07:07:29 - train: epoch 0033, iter [00980, 01251], lr: 0.000510, loss: 0.0954
2022-10-03 07:07:57 - train: epoch 0033, iter [00990, 01251], lr: 0.000510, loss: 0.0956
2022-10-03 07:08:25 - train: epoch 0033, iter [01000, 01251], lr: 0.000510, loss: 0.0977
2022-10-03 07:08:53 - train: epoch 0033, iter [01010, 01251], lr: 0.000510, loss: 0.0950
2022-10-03 07:09:21 - train: epoch 0033, iter [01020, 01251], lr: 0.000510, loss: 0.0929
2022-10-03 07:09:50 - train: epoch 0033, iter [01030, 01251], lr: 0.000510, loss: 0.0958
2022-10-03 07:10:18 - train: epoch 0033, iter [01040, 01251], lr: 0.000510, loss: 0.0926
2022-10-03 07:10:46 - train: epoch 0033, iter [01050, 01251], lr: 0.000510, loss: 0.0999
2022-10-03 07:11:14 - train: epoch 0033, iter [01060, 01251], lr: 0.000510, loss: 0.0971
2022-10-03 07:11:42 - train: epoch 0033, iter [01070, 01251], lr: 0.000509, loss: 0.1037
2022-10-03 07:12:10 - train: epoch 0033, iter [01080, 01251], lr: 0.000509, loss: 0.0975
2022-10-03 07:12:39 - train: epoch 0033, iter [01090, 01251], lr: 0.000509, loss: 0.0931
2022-10-03 07:13:07 - train: epoch 0033, iter [01100, 01251], lr: 0.000509, loss: 0.0934
2022-10-03 07:13:35 - train: epoch 0033, iter [01110, 01251], lr: 0.000509, loss: 0.1003
2022-10-03 07:14:03 - train: epoch 0033, iter [01120, 01251], lr: 0.000509, loss: 0.0913
2022-10-03 07:14:31 - train: epoch 0033, iter [01130, 01251], lr: 0.000509, loss: 0.0918
2022-10-03 07:15:00 - train: epoch 0033, iter [01140, 01251], lr: 0.000509, loss: 0.0942
2022-10-03 07:15:28 - train: epoch 0033, iter [01150, 01251], lr: 0.000509, loss: 0.0989
2022-10-03 07:15:56 - train: epoch 0033, iter [01160, 01251], lr: 0.000509, loss: 0.0958
2022-10-03 07:16:24 - train: epoch 0033, iter [01170, 01251], lr: 0.000509, loss: 0.0938
2022-10-03 07:16:52 - train: epoch 0033, iter [01180, 01251], lr: 0.000509, loss: 0.0944
2022-10-03 07:17:20 - train: epoch 0033, iter [01190, 01251], lr: 0.000509, loss: 0.0896
2022-10-03 07:17:48 - train: epoch 0033, iter [01200, 01251], lr: 0.000509, loss: 0.0996
2022-10-03 07:18:17 - train: epoch 0033, iter [01210, 01251], lr: 0.000509, loss: 0.0958
2022-10-03 07:18:45 - train: epoch 0033, iter [01220, 01251], lr: 0.000509, loss: 0.0961
2022-10-03 07:19:13 - train: epoch 0033, iter [01230, 01251], lr: 0.000509, loss: 0.0952
2022-10-03 07:19:41 - train: epoch 0033, iter [01240, 01251], lr: 0.000508, loss: 0.0824
2022-10-03 07:20:09 - train: epoch 0033, iter [01250, 01251], lr: 0.000508, loss: 0.1042
2022-10-03 07:20:13 - train: epoch 033, train_loss: 0.0965
2022-10-03 07:20:15 - until epoch: 033, best_loss: 0.0965
2022-10-03 07:20:15 - epoch 034 lr: 0.000508
2022-10-03 07:20:50 - train: epoch 0034, iter [00010, 01251], lr: 0.000508, loss: 0.1011
2022-10-03 07:21:18 - train: epoch 0034, iter [00020, 01251], lr: 0.000508, loss: 0.0911
2022-10-03 07:21:46 - train: epoch 0034, iter [00030, 01251], lr: 0.000508, loss: 0.0904
2022-10-03 07:22:14 - train: epoch 0034, iter [00040, 01251], lr: 0.000508, loss: 0.0902
2022-10-03 07:22:42 - train: epoch 0034, iter [00050, 01251], lr: 0.000508, loss: 0.0909
2022-10-03 07:23:11 - train: epoch 0034, iter [00060, 01251], lr: 0.000508, loss: 0.0989
2022-10-03 07:23:39 - train: epoch 0034, iter [00070, 01251], lr: 0.000508, loss: 0.0960
2022-10-03 07:24:07 - train: epoch 0034, iter [00080, 01251], lr: 0.000508, loss: 0.1076
2022-10-03 07:24:35 - train: epoch 0034, iter [00090, 01251], lr: 0.000508, loss: 0.0954
2022-10-03 07:25:04 - train: epoch 0034, iter [00100, 01251], lr: 0.000508, loss: 0.0960
2022-10-03 07:25:32 - train: epoch 0034, iter [00110, 01251], lr: 0.000508, loss: 0.1104
2022-10-03 07:26:00 - train: epoch 0034, iter [00120, 01251], lr: 0.000508, loss: 0.0892
2022-10-03 07:26:28 - train: epoch 0034, iter [00130, 01251], lr: 0.000508, loss: 0.0931
2022-10-03 07:26:56 - train: epoch 0034, iter [00140, 01251], lr: 0.000508, loss: 0.0984
2022-10-03 07:27:25 - train: epoch 0034, iter [00150, 01251], lr: 0.000507, loss: 0.0898
2022-10-03 07:27:53 - train: epoch 0034, iter [00160, 01251], lr: 0.000507, loss: 0.0963
2022-10-03 07:28:21 - train: epoch 0034, iter [00170, 01251], lr: 0.000507, loss: 0.0987
2022-10-03 07:28:49 - train: epoch 0034, iter [00180, 01251], lr: 0.000507, loss: 0.0979
2022-10-03 07:29:17 - train: epoch 0034, iter [00190, 01251], lr: 0.000507, loss: 0.1012
2022-10-03 07:29:46 - train: epoch 0034, iter [00200, 01251], lr: 0.000507, loss: 0.0984
2022-10-03 07:30:14 - train: epoch 0034, iter [00210, 01251], lr: 0.000507, loss: 0.0942
2022-10-03 07:30:42 - train: epoch 0034, iter [00220, 01251], lr: 0.000507, loss: 0.0936
2022-10-03 07:31:11 - train: epoch 0034, iter [00230, 01251], lr: 0.000507, loss: 0.0980
2022-10-03 07:31:39 - train: epoch 0034, iter [00240, 01251], lr: 0.000507, loss: 0.1053
2022-10-03 07:32:07 - train: epoch 0034, iter [00250, 01251], lr: 0.000507, loss: 0.0933
2022-10-03 07:32:35 - train: epoch 0034, iter [00260, 01251], lr: 0.000507, loss: 0.0876
2022-10-03 07:33:03 - train: epoch 0034, iter [00270, 01251], lr: 0.000507, loss: 0.0903
2022-10-03 07:33:32 - train: epoch 0034, iter [00280, 01251], lr: 0.000507, loss: 0.0949
2022-10-03 07:34:00 - train: epoch 0034, iter [00290, 01251], lr: 0.000507, loss: 0.0860
2022-10-03 07:34:28 - train: epoch 0034, iter [00300, 01251], lr: 0.000507, loss: 0.0920
2022-10-03 07:34:56 - train: epoch 0034, iter [00310, 01251], lr: 0.000507, loss: 0.0897
2022-10-03 07:35:24 - train: epoch 0034, iter [00320, 01251], lr: 0.000506, loss: 0.0964
2022-10-03 07:35:53 - train: epoch 0034, iter [00330, 01251], lr: 0.000506, loss: 0.0919
2022-10-03 07:36:21 - train: epoch 0034, iter [00340, 01251], lr: 0.000506, loss: 0.1001
2022-10-03 07:36:49 - train: epoch 0034, iter [00350, 01251], lr: 0.000506, loss: 0.0945
2022-10-03 07:37:18 - train: epoch 0034, iter [00360, 01251], lr: 0.000506, loss: 0.0930
2022-10-03 07:37:46 - train: epoch 0034, iter [00370, 01251], lr: 0.000506, loss: 0.0939
2022-10-03 07:38:14 - train: epoch 0034, iter [00380, 01251], lr: 0.000506, loss: 0.1028
2022-10-03 07:38:42 - train: epoch 0034, iter [00390, 01251], lr: 0.000506, loss: 0.0963
2022-10-03 07:39:10 - train: epoch 0034, iter [00400, 01251], lr: 0.000506, loss: 0.0919
2022-10-03 07:39:39 - train: epoch 0034, iter [00410, 01251], lr: 0.000506, loss: 0.0937
2022-10-03 07:40:07 - train: epoch 0034, iter [00420, 01251], lr: 0.000506, loss: 0.0956
2022-10-03 07:40:35 - train: epoch 0034, iter [00430, 01251], lr: 0.000506, loss: 0.0978
2022-10-03 07:41:03 - train: epoch 0034, iter [00440, 01251], lr: 0.000506, loss: 0.0916
2022-10-03 07:41:32 - train: epoch 0034, iter [00450, 01251], lr: 0.000506, loss: 0.0939
2022-10-03 07:42:00 - train: epoch 0034, iter [00460, 01251], lr: 0.000506, loss: 0.0934
2022-10-03 07:42:29 - train: epoch 0034, iter [00470, 01251], lr: 0.000506, loss: 0.0928
2022-10-03 07:42:57 - train: epoch 0034, iter [00480, 01251], lr: 0.000505, loss: 0.0887
2022-10-03 07:43:25 - train: epoch 0034, iter [00490, 01251], lr: 0.000505, loss: 0.0992
2022-10-03 07:43:53 - train: epoch 0034, iter [00500, 01251], lr: 0.000505, loss: 0.0933
2022-10-03 07:44:22 - train: epoch 0034, iter [00510, 01251], lr: 0.000505, loss: 0.0971
2022-10-03 07:44:50 - train: epoch 0034, iter [00520, 01251], lr: 0.000505, loss: 0.0904
2022-10-03 07:45:18 - train: epoch 0034, iter [00530, 01251], lr: 0.000505, loss: 0.0932
2022-10-03 07:45:46 - train: epoch 0034, iter [00540, 01251], lr: 0.000505, loss: 0.0908
2022-10-03 07:46:14 - train: epoch 0034, iter [00550, 01251], lr: 0.000505, loss: 0.0954
2022-10-03 07:46:43 - train: epoch 0034, iter [00560, 01251], lr: 0.000505, loss: 0.0954
2022-10-03 07:47:11 - train: epoch 0034, iter [00570, 01251], lr: 0.000505, loss: 0.1023
2022-10-03 07:47:40 - train: epoch 0034, iter [00580, 01251], lr: 0.000505, loss: 0.0975
2022-10-03 07:48:08 - train: epoch 0034, iter [00590, 01251], lr: 0.000505, loss: 0.0991
2022-10-03 07:48:36 - train: epoch 0034, iter [00600, 01251], lr: 0.000505, loss: 0.0965
2022-10-03 07:49:05 - train: epoch 0034, iter [00610, 01251], lr: 0.000505, loss: 0.0875
2022-10-03 07:49:33 - train: epoch 0034, iter [00620, 01251], lr: 0.000505, loss: 0.0963
2022-10-03 07:50:01 - train: epoch 0034, iter [00630, 01251], lr: 0.000505, loss: 0.0861
2022-10-03 07:50:29 - train: epoch 0034, iter [00640, 01251], lr: 0.000505, loss: 0.0961
2022-10-03 07:50:57 - train: epoch 0034, iter [00650, 01251], lr: 0.000504, loss: 0.0928
2022-10-03 07:51:26 - train: epoch 0034, iter [00660, 01251], lr: 0.000504, loss: 0.0973
2022-10-03 07:51:54 - train: epoch 0034, iter [00670, 01251], lr: 0.000504, loss: 0.1020
2022-10-03 07:52:22 - train: epoch 0034, iter [00680, 01251], lr: 0.000504, loss: 0.1056
2022-10-03 07:52:50 - train: epoch 0034, iter [00690, 01251], lr: 0.000504, loss: 0.0883
2022-10-03 07:53:18 - train: epoch 0034, iter [00700, 01251], lr: 0.000504, loss: 0.1023
2022-10-03 07:53:46 - train: epoch 0034, iter [00710, 01251], lr: 0.000504, loss: 0.0917
2022-10-03 07:54:15 - train: epoch 0034, iter [00720, 01251], lr: 0.000504, loss: 0.0981
2022-10-03 07:54:43 - train: epoch 0034, iter [00730, 01251], lr: 0.000504, loss: 0.0980
2022-10-03 07:55:11 - train: epoch 0034, iter [00740, 01251], lr: 0.000504, loss: 0.0927
2022-10-03 07:55:39 - train: epoch 0034, iter [00750, 01251], lr: 0.000504, loss: 0.0970
2022-10-03 07:56:08 - train: epoch 0034, iter [00760, 01251], lr: 0.000504, loss: 0.1013
2022-10-03 07:56:36 - train: epoch 0034, iter [00770, 01251], lr: 0.000504, loss: 0.0941
2022-10-03 07:57:04 - train: epoch 0034, iter [00780, 01251], lr: 0.000504, loss: 0.0918
2022-10-03 07:57:33 - train: epoch 0034, iter [00790, 01251], lr: 0.000504, loss: 0.0894
2022-10-03 07:58:01 - train: epoch 0034, iter [00800, 01251], lr: 0.000504, loss: 0.0882
2022-10-03 07:58:29 - train: epoch 0034, iter [00810, 01251], lr: 0.000503, loss: 0.0891
2022-10-03 07:58:58 - train: epoch 0034, iter [00820, 01251], lr: 0.000503, loss: 0.1056
2022-10-03 07:59:26 - train: epoch 0034, iter [00830, 01251], lr: 0.000503, loss: 0.0864
2022-10-03 07:59:54 - train: epoch 0034, iter [00840, 01251], lr: 0.000503, loss: 0.0966
2022-10-03 08:00:22 - train: epoch 0034, iter [00850, 01251], lr: 0.000503, loss: 0.1071
2022-10-03 08:00:50 - train: epoch 0034, iter [00860, 01251], lr: 0.000503, loss: 0.1015
2022-10-03 08:01:18 - train: epoch 0034, iter [00870, 01251], lr: 0.000503, loss: 0.0953
2022-10-03 08:01:46 - train: epoch 0034, iter [00880, 01251], lr: 0.000503, loss: 0.0931
2022-10-03 08:02:15 - train: epoch 0034, iter [00890, 01251], lr: 0.000503, loss: 0.0992
2022-10-03 08:02:43 - train: epoch 0034, iter [00900, 01251], lr: 0.000503, loss: 0.1064
2022-10-03 08:03:11 - train: epoch 0034, iter [00910, 01251], lr: 0.000503, loss: 0.0991
2022-10-03 08:03:39 - train: epoch 0034, iter [00920, 01251], lr: 0.000503, loss: 0.0986
2022-10-03 08:04:07 - train: epoch 0034, iter [00930, 01251], lr: 0.000503, loss: 0.1072
2022-10-03 08:04:36 - train: epoch 0034, iter [00940, 01251], lr: 0.000503, loss: 0.0983
2022-10-03 08:05:04 - train: epoch 0034, iter [00950, 01251], lr: 0.000503, loss: 0.0933
2022-10-03 08:05:32 - train: epoch 0034, iter [00960, 01251], lr: 0.000503, loss: 0.0908
2022-10-03 08:06:00 - train: epoch 0034, iter [00970, 01251], lr: 0.000502, loss: 0.0962
2022-10-03 08:06:28 - train: epoch 0034, iter [00980, 01251], lr: 0.000502, loss: 0.0981
2022-10-03 08:06:56 - train: epoch 0034, iter [00990, 01251], lr: 0.000502, loss: 0.0982
2022-10-03 08:07:24 - train: epoch 0034, iter [01000, 01251], lr: 0.000502, loss: 0.0936
2022-10-03 08:07:52 - train: epoch 0034, iter [01010, 01251], lr: 0.000502, loss: 0.0904
2022-10-03 08:08:20 - train: epoch 0034, iter [01020, 01251], lr: 0.000502, loss: 0.0933
2022-10-03 08:08:49 - train: epoch 0034, iter [01030, 01251], lr: 0.000502, loss: 0.0940
2022-10-03 08:09:17 - train: epoch 0034, iter [01040, 01251], lr: 0.000502, loss: 0.1038
2022-10-03 08:09:45 - train: epoch 0034, iter [01050, 01251], lr: 0.000502, loss: 0.0961
2022-10-03 08:10:13 - train: epoch 0034, iter [01060, 01251], lr: 0.000502, loss: 0.0988
2022-10-03 08:10:41 - train: epoch 0034, iter [01070, 01251], lr: 0.000502, loss: 0.0956
2022-10-03 08:11:10 - train: epoch 0034, iter [01080, 01251], lr: 0.000502, loss: 0.0995
2022-10-03 08:11:38 - train: epoch 0034, iter [01090, 01251], lr: 0.000502, loss: 0.1063
2022-10-03 08:12:06 - train: epoch 0034, iter [01100, 01251], lr: 0.000502, loss: 0.0972
2022-10-03 08:12:34 - train: epoch 0034, iter [01110, 01251], lr: 0.000502, loss: 0.0950
2022-10-03 08:13:02 - train: epoch 0034, iter [01120, 01251], lr: 0.000502, loss: 0.0963
2022-10-03 08:13:30 - train: epoch 0034, iter [01130, 01251], lr: 0.000501, loss: 0.0956
2022-10-03 08:13:58 - train: epoch 0034, iter [01140, 01251], lr: 0.000501, loss: 0.0873
2022-10-03 08:14:27 - train: epoch 0034, iter [01150, 01251], lr: 0.000501, loss: 0.1046
2022-10-03 08:14:55 - train: epoch 0034, iter [01160, 01251], lr: 0.000501, loss: 0.0893
2022-10-03 08:15:23 - train: epoch 0034, iter [01170, 01251], lr: 0.000501, loss: 0.0970
2022-10-03 08:15:51 - train: epoch 0034, iter [01180, 01251], lr: 0.000501, loss: 0.0919
2022-10-03 08:16:19 - train: epoch 0034, iter [01190, 01251], lr: 0.000501, loss: 0.0989
2022-10-03 08:16:47 - train: epoch 0034, iter [01200, 01251], lr: 0.000501, loss: 0.0928
2022-10-03 08:17:16 - train: epoch 0034, iter [01210, 01251], lr: 0.000501, loss: 0.1075
2022-10-03 08:17:44 - train: epoch 0034, iter [01220, 01251], lr: 0.000501, loss: 0.0910
2022-10-03 08:18:12 - train: epoch 0034, iter [01230, 01251], lr: 0.000501, loss: 0.0974
2022-10-03 08:18:40 - train: epoch 0034, iter [01240, 01251], lr: 0.000501, loss: 0.0919
2022-10-03 08:19:08 - train: epoch 0034, iter [01250, 01251], lr: 0.000501, loss: 0.0965
2022-10-03 08:19:12 - train: epoch 034, train_loss: 0.0959
2022-10-03 08:19:14 - until epoch: 034, best_loss: 0.0959
2022-10-03 08:19:14 - epoch 035 lr: 0.000501
2022-10-03 08:19:49 - train: epoch 0035, iter [00010, 01251], lr: 0.000501, loss: 0.1022
2022-10-03 08:20:17 - train: epoch 0035, iter [00020, 01251], lr: 0.000501, loss: 0.0913
2022-10-03 08:20:45 - train: epoch 0035, iter [00030, 01251], lr: 0.000501, loss: 0.0898
2022-10-03 08:21:14 - train: epoch 0035, iter [00040, 01251], lr: 0.000500, loss: 0.0967
2022-10-03 08:21:42 - train: epoch 0035, iter [00050, 01251], lr: 0.000500, loss: 0.0851
2022-10-03 08:22:10 - train: epoch 0035, iter [00060, 01251], lr: 0.000500, loss: 0.0986
2022-10-03 08:22:38 - train: epoch 0035, iter [00070, 01251], lr: 0.000500, loss: 0.0916
2022-10-03 08:23:06 - train: epoch 0035, iter [00080, 01251], lr: 0.000500, loss: 0.0941
2022-10-03 08:23:35 - train: epoch 0035, iter [00090, 01251], lr: 0.000500, loss: 0.0983
2022-10-03 08:24:03 - train: epoch 0035, iter [00100, 01251], lr: 0.000500, loss: 0.0931
2022-10-03 08:24:31 - train: epoch 0035, iter [00110, 01251], lr: 0.000500, loss: 0.0911
2022-10-03 08:24:59 - train: epoch 0035, iter [00120, 01251], lr: 0.000500, loss: 0.0912
2022-10-03 08:25:27 - train: epoch 0035, iter [00130, 01251], lr: 0.000500, loss: 0.0944
2022-10-03 08:25:55 - train: epoch 0035, iter [00140, 01251], lr: 0.000500, loss: 0.0976
2022-10-03 08:26:23 - train: epoch 0035, iter [00150, 01251], lr: 0.000500, loss: 0.0960
2022-10-03 08:26:52 - train: epoch 0035, iter [00160, 01251], lr: 0.000500, loss: 0.0909
2022-10-03 08:27:20 - train: epoch 0035, iter [00170, 01251], lr: 0.000500, loss: 0.1013
2022-10-03 08:27:48 - train: epoch 0035, iter [00180, 01251], lr: 0.000500, loss: 0.0996
2022-10-03 08:28:16 - train: epoch 0035, iter [00190, 01251], lr: 0.000500, loss: 0.0957
2022-10-03 08:28:44 - train: epoch 0035, iter [00200, 01251], lr: 0.000499, loss: 0.1026
2022-10-03 08:29:12 - train: epoch 0035, iter [00210, 01251], lr: 0.000499, loss: 0.0901
2022-10-03 08:29:40 - train: epoch 0035, iter [00220, 01251], lr: 0.000499, loss: 0.0909
2022-10-03 08:30:08 - train: epoch 0035, iter [00230, 01251], lr: 0.000499, loss: 0.0869
2022-10-03 08:30:36 - train: epoch 0035, iter [00240, 01251], lr: 0.000499, loss: 0.0880
2022-10-03 08:31:05 - train: epoch 0035, iter [00250, 01251], lr: 0.000499, loss: 0.1014
2022-10-03 08:31:33 - train: epoch 0035, iter [00260, 01251], lr: 0.000499, loss: 0.0918
2022-10-03 08:32:01 - train: epoch 0035, iter [00270, 01251], lr: 0.000499, loss: 0.0938
2022-10-03 08:32:29 - train: epoch 0035, iter [00280, 01251], lr: 0.000499, loss: 0.0936
2022-10-03 08:32:57 - train: epoch 0035, iter [00290, 01251], lr: 0.000499, loss: 0.0964
2022-10-03 08:33:25 - train: epoch 0035, iter [00300, 01251], lr: 0.000499, loss: 0.0938
2022-10-03 08:33:53 - train: epoch 0035, iter [00310, 01251], lr: 0.000499, loss: 0.0943
2022-10-03 08:34:21 - train: epoch 0035, iter [00320, 01251], lr: 0.000499, loss: 0.0876
2022-10-03 08:34:49 - train: epoch 0035, iter [00330, 01251], lr: 0.000499, loss: 0.0950
2022-10-03 08:35:18 - train: epoch 0035, iter [00340, 01251], lr: 0.000499, loss: 0.0856
2022-10-03 08:35:46 - train: epoch 0035, iter [00350, 01251], lr: 0.000499, loss: 0.1006
2022-10-03 08:36:14 - train: epoch 0035, iter [00360, 01251], lr: 0.000498, loss: 0.0999
2022-10-03 08:36:42 - train: epoch 0035, iter [00370, 01251], lr: 0.000498, loss: 0.0938
2022-10-03 08:37:10 - train: epoch 0035, iter [00380, 01251], lr: 0.000498, loss: 0.0979
2022-10-03 08:37:38 - train: epoch 0035, iter [00390, 01251], lr: 0.000498, loss: 0.0921
2022-10-03 08:38:06 - train: epoch 0035, iter [00400, 01251], lr: 0.000498, loss: 0.0982
2022-10-03 08:38:34 - train: epoch 0035, iter [00410, 01251], lr: 0.000498, loss: 0.0934
2022-10-03 08:39:02 - train: epoch 0035, iter [00420, 01251], lr: 0.000498, loss: 0.0939
2022-10-03 08:39:30 - train: epoch 0035, iter [00430, 01251], lr: 0.000498, loss: 0.0943
2022-10-03 08:39:58 - train: epoch 0035, iter [00440, 01251], lr: 0.000498, loss: 0.0893
2022-10-03 08:40:27 - train: epoch 0035, iter [00450, 01251], lr: 0.000498, loss: 0.0922
2022-10-03 08:40:55 - train: epoch 0035, iter [00460, 01251], lr: 0.000498, loss: 0.0959
2022-10-03 08:41:23 - train: epoch 0035, iter [00470, 01251], lr: 0.000498, loss: 0.0984
2022-10-03 08:41:51 - train: epoch 0035, iter [00480, 01251], lr: 0.000498, loss: 0.0951
2022-10-03 08:42:19 - train: epoch 0035, iter [00490, 01251], lr: 0.000498, loss: 0.0913
2022-10-03 08:42:48 - train: epoch 0035, iter [00500, 01251], lr: 0.000498, loss: 0.0971
2022-10-03 08:43:16 - train: epoch 0035, iter [00510, 01251], lr: 0.000498, loss: 0.0912
2022-10-03 08:43:44 - train: epoch 0035, iter [00520, 01251], lr: 0.000497, loss: 0.0922
2022-10-03 08:44:13 - train: epoch 0035, iter [00530, 01251], lr: 0.000497, loss: 0.0993
2022-10-03 08:44:41 - train: epoch 0035, iter [00540, 01251], lr: 0.000497, loss: 0.0956
2022-10-03 08:45:09 - train: epoch 0035, iter [00550, 01251], lr: 0.000497, loss: 0.1053
2022-10-03 08:45:37 - train: epoch 0035, iter [00560, 01251], lr: 0.000497, loss: 0.0958
2022-10-03 08:46:06 - train: epoch 0035, iter [00570, 01251], lr: 0.000497, loss: 0.0931
2022-10-03 08:46:34 - train: epoch 0035, iter [00580, 01251], lr: 0.000497, loss: 0.0946
2022-10-03 08:47:02 - train: epoch 0035, iter [00590, 01251], lr: 0.000497, loss: 0.0951
2022-10-03 08:47:30 - train: epoch 0035, iter [00600, 01251], lr: 0.000497, loss: 0.0967
2022-10-03 08:47:58 - train: epoch 0035, iter [00610, 01251], lr: 0.000497, loss: 0.0941
2022-10-03 08:48:26 - train: epoch 0035, iter [00620, 01251], lr: 0.000497, loss: 0.1030
2022-10-03 08:48:54 - train: epoch 0035, iter [00630, 01251], lr: 0.000497, loss: 0.0958
2022-10-03 08:49:23 - train: epoch 0035, iter [00640, 01251], lr: 0.000497, loss: 0.0951
2022-10-03 08:49:51 - train: epoch 0035, iter [00650, 01251], lr: 0.000497, loss: 0.1074
2022-10-03 08:50:19 - train: epoch 0035, iter [00660, 01251], lr: 0.000497, loss: 0.0986
2022-10-03 08:50:47 - train: epoch 0035, iter [00670, 01251], lr: 0.000497, loss: 0.0987
2022-10-03 08:51:15 - train: epoch 0035, iter [00680, 01251], lr: 0.000496, loss: 0.0921
2022-10-03 08:51:43 - train: epoch 0035, iter [00690, 01251], lr: 0.000496, loss: 0.0965
2022-10-03 08:52:12 - train: epoch 0035, iter [00700, 01251], lr: 0.000496, loss: 0.0972
2022-10-03 08:52:40 - train: epoch 0035, iter [00710, 01251], lr: 0.000496, loss: 0.0992
2022-10-03 08:53:08 - train: epoch 0035, iter [00720, 01251], lr: 0.000496, loss: 0.0952
2022-10-03 08:53:36 - train: epoch 0035, iter [00730, 01251], lr: 0.000496, loss: 0.0932
2022-10-03 08:54:04 - train: epoch 0035, iter [00740, 01251], lr: 0.000496, loss: 0.0974
2022-10-03 08:54:33 - train: epoch 0035, iter [00750, 01251], lr: 0.000496, loss: 0.1001
2022-10-03 08:55:01 - train: epoch 0035, iter [00760, 01251], lr: 0.000496, loss: 0.0987
2022-10-03 08:55:29 - train: epoch 0035, iter [00770, 01251], lr: 0.000496, loss: 0.1040
2022-10-03 08:55:57 - train: epoch 0035, iter [00780, 01251], lr: 0.000496, loss: 0.0960
2022-10-03 08:56:25 - train: epoch 0035, iter [00790, 01251], lr: 0.000496, loss: 0.0955
2022-10-03 08:56:54 - train: epoch 0035, iter [00800, 01251], lr: 0.000496, loss: 0.0885
2022-10-03 08:57:22 - train: epoch 0035, iter [00810, 01251], lr: 0.000496, loss: 0.1015
2022-10-03 08:57:50 - train: epoch 0035, iter [00820, 01251], lr: 0.000496, loss: 0.0898
2022-10-03 08:58:18 - train: epoch 0035, iter [00830, 01251], lr: 0.000496, loss: 0.0938
2022-10-03 08:58:46 - train: epoch 0035, iter [00840, 01251], lr: 0.000495, loss: 0.0938
2022-10-03 08:59:14 - train: epoch 0035, iter [00850, 01251], lr: 0.000495, loss: 0.0928
2022-10-03 08:59:43 - train: epoch 0035, iter [00860, 01251], lr: 0.000495, loss: 0.1023
2022-10-03 09:00:11 - train: epoch 0035, iter [00870, 01251], lr: 0.000495, loss: 0.0936
2022-10-03 09:00:39 - train: epoch 0035, iter [00880, 01251], lr: 0.000495, loss: 0.1008
2022-10-03 09:01:07 - train: epoch 0035, iter [00890, 01251], lr: 0.000495, loss: 0.0937
2022-10-03 09:01:35 - train: epoch 0035, iter [00900, 01251], lr: 0.000495, loss: 0.0940
2022-10-03 09:02:03 - train: epoch 0035, iter [00910, 01251], lr: 0.000495, loss: 0.0909
2022-10-03 09:02:32 - train: epoch 0035, iter [00920, 01251], lr: 0.000495, loss: 0.0919
2022-10-03 09:03:00 - train: epoch 0035, iter [00930, 01251], lr: 0.000495, loss: 0.1046
2022-10-03 09:03:28 - train: epoch 0035, iter [00940, 01251], lr: 0.000495, loss: 0.0992
2022-10-03 09:03:57 - train: epoch 0035, iter [00950, 01251], lr: 0.000495, loss: 0.0928
2022-10-03 09:04:25 - train: epoch 0035, iter [00960, 01251], lr: 0.000495, loss: 0.0891
2022-10-03 09:04:53 - train: epoch 0035, iter [00970, 01251], lr: 0.000495, loss: 0.1021
2022-10-03 09:05:21 - train: epoch 0035, iter [00980, 01251], lr: 0.000495, loss: 0.0951
2022-10-03 09:05:49 - train: epoch 0035, iter [00990, 01251], lr: 0.000495, loss: 0.0908
2022-10-03 09:06:17 - train: epoch 0035, iter [01000, 01251], lr: 0.000494, loss: 0.1035
2022-10-03 09:06:45 - train: epoch 0035, iter [01010, 01251], lr: 0.000494, loss: 0.0951
2022-10-03 09:07:13 - train: epoch 0035, iter [01020, 01251], lr: 0.000494, loss: 0.0994
2022-10-03 09:07:42 - train: epoch 0035, iter [01030, 01251], lr: 0.000494, loss: 0.0984
2022-10-03 09:08:10 - train: epoch 0035, iter [01040, 01251], lr: 0.000494, loss: 0.0971
2022-10-03 09:08:38 - train: epoch 0035, iter [01050, 01251], lr: 0.000494, loss: 0.1006
2022-10-03 09:09:06 - train: epoch 0035, iter [01060, 01251], lr: 0.000494, loss: 0.0873
2022-10-03 09:09:34 - train: epoch 0035, iter [01070, 01251], lr: 0.000494, loss: 0.0883
2022-10-03 09:10:02 - train: epoch 0035, iter [01080, 01251], lr: 0.000494, loss: 0.0947
2022-10-03 09:10:31 - train: epoch 0035, iter [01090, 01251], lr: 0.000494, loss: 0.0945
2022-10-03 09:10:59 - train: epoch 0035, iter [01100, 01251], lr: 0.000494, loss: 0.0927
2022-10-03 09:11:27 - train: epoch 0035, iter [01110, 01251], lr: 0.000494, loss: 0.0875
2022-10-03 09:11:55 - train: epoch 0035, iter [01120, 01251], lr: 0.000494, loss: 0.0961
2022-10-03 09:12:24 - train: epoch 0035, iter [01130, 01251], lr: 0.000494, loss: 0.0970
2022-10-03 09:12:52 - train: epoch 0035, iter [01140, 01251], lr: 0.000494, loss: 0.1031
2022-10-03 09:13:20 - train: epoch 0035, iter [01150, 01251], lr: 0.000493, loss: 0.1071
2022-10-03 09:13:48 - train: epoch 0035, iter [01160, 01251], lr: 0.000493, loss: 0.0976
2022-10-03 09:14:16 - train: epoch 0035, iter [01170, 01251], lr: 0.000493, loss: 0.0923
2022-10-03 09:14:44 - train: epoch 0035, iter [01180, 01251], lr: 0.000493, loss: 0.0942
2022-10-03 09:15:12 - train: epoch 0035, iter [01190, 01251], lr: 0.000493, loss: 0.0919
2022-10-03 09:15:40 - train: epoch 0035, iter [01200, 01251], lr: 0.000493, loss: 0.0944
2022-10-03 09:16:09 - train: epoch 0035, iter [01210, 01251], lr: 0.000493, loss: 0.0928
2022-10-03 09:16:37 - train: epoch 0035, iter [01220, 01251], lr: 0.000493, loss: 0.0860
2022-10-03 09:17:05 - train: epoch 0035, iter [01230, 01251], lr: 0.000493, loss: 0.0971
2022-10-03 09:17:33 - train: epoch 0035, iter [01240, 01251], lr: 0.000493, loss: 0.0942
2022-10-03 09:18:01 - train: epoch 0035, iter [01250, 01251], lr: 0.000493, loss: 0.0973
2022-10-03 09:18:05 - train: epoch 035, train_loss: 0.0955
2022-10-03 09:18:06 - until epoch: 035, best_loss: 0.0955
2022-10-03 09:18:06 - epoch 036 lr: 0.000493
2022-10-03 09:18:42 - train: epoch 0036, iter [00010, 01251], lr: 0.000493, loss: 0.0952
2022-10-03 09:19:10 - train: epoch 0036, iter [00020, 01251], lr: 0.000493, loss: 0.0984
2022-10-03 09:19:38 - train: epoch 0036, iter [00030, 01251], lr: 0.000493, loss: 0.0944
2022-10-03 09:20:06 - train: epoch 0036, iter [00040, 01251], lr: 0.000493, loss: 0.1030
2022-10-03 09:20:34 - train: epoch 0036, iter [00050, 01251], lr: 0.000493, loss: 0.0945
2022-10-03 09:21:03 - train: epoch 0036, iter [00060, 01251], lr: 0.000492, loss: 0.0952
2022-10-03 09:21:31 - train: epoch 0036, iter [00070, 01251], lr: 0.000492, loss: 0.0941
2022-10-03 09:21:59 - train: epoch 0036, iter [00080, 01251], lr: 0.000492, loss: 0.0881
2022-10-03 09:22:27 - train: epoch 0036, iter [00090, 01251], lr: 0.000492, loss: 0.0946
2022-10-03 09:22:55 - train: epoch 0036, iter [00100, 01251], lr: 0.000492, loss: 0.0905
2022-10-03 09:23:23 - train: epoch 0036, iter [00110, 01251], lr: 0.000492, loss: 0.0947
2022-10-03 09:23:51 - train: epoch 0036, iter [00120, 01251], lr: 0.000492, loss: 0.0942
2022-10-03 09:24:20 - train: epoch 0036, iter [00130, 01251], lr: 0.000492, loss: 0.0936
2022-10-03 09:24:48 - train: epoch 0036, iter [00140, 01251], lr: 0.000492, loss: 0.0952
2022-10-03 09:25:16 - train: epoch 0036, iter [00150, 01251], lr: 0.000492, loss: 0.1000
2022-10-03 09:25:44 - train: epoch 0036, iter [00160, 01251], lr: 0.000492, loss: 0.1061
2022-10-03 09:26:12 - train: epoch 0036, iter [00170, 01251], lr: 0.000492, loss: 0.0941
2022-10-03 09:26:40 - train: epoch 0036, iter [00180, 01251], lr: 0.000492, loss: 0.1048
2022-10-03 09:27:08 - train: epoch 0036, iter [00190, 01251], lr: 0.000492, loss: 0.0921
2022-10-03 09:27:37 - train: epoch 0036, iter [00200, 01251], lr: 0.000492, loss: 0.1041
2022-10-03 09:28:05 - train: epoch 0036, iter [00210, 01251], lr: 0.000491, loss: 0.0926
2022-10-03 09:28:33 - train: epoch 0036, iter [00220, 01251], lr: 0.000491, loss: 0.0973
2022-10-03 09:29:01 - train: epoch 0036, iter [00230, 01251], lr: 0.000491, loss: 0.0891
2022-10-03 09:29:30 - train: epoch 0036, iter [00240, 01251], lr: 0.000491, loss: 0.0988
2022-10-03 09:29:58 - train: epoch 0036, iter [00250, 01251], lr: 0.000491, loss: 0.0883
2022-10-03 09:30:26 - train: epoch 0036, iter [00260, 01251], lr: 0.000491, loss: 0.1040
2022-10-03 09:30:54 - train: epoch 0036, iter [00270, 01251], lr: 0.000491, loss: 0.0978
2022-10-03 09:31:22 - train: epoch 0036, iter [00280, 01251], lr: 0.000491, loss: 0.0902
2022-10-03 09:31:50 - train: epoch 0036, iter [00290, 01251], lr: 0.000491, loss: 0.0965
2022-10-03 09:32:18 - train: epoch 0036, iter [00300, 01251], lr: 0.000491, loss: 0.0898
2022-10-03 09:32:46 - train: epoch 0036, iter [00310, 01251], lr: 0.000491, loss: 0.0968
2022-10-03 09:33:14 - train: epoch 0036, iter [00320, 01251], lr: 0.000491, loss: 0.0882
2022-10-03 09:33:43 - train: epoch 0036, iter [00330, 01251], lr: 0.000491, loss: 0.0880
2022-10-03 09:34:11 - train: epoch 0036, iter [00340, 01251], lr: 0.000491, loss: 0.1042
2022-10-03 09:34:39 - train: epoch 0036, iter [00350, 01251], lr: 0.000491, loss: 0.0900
2022-10-03 09:35:07 - train: epoch 0036, iter [00360, 01251], lr: 0.000491, loss: 0.1018
2022-10-03 09:35:35 - train: epoch 0036, iter [00370, 01251], lr: 0.000490, loss: 0.0971
2022-10-03 09:36:04 - train: epoch 0036, iter [00380, 01251], lr: 0.000490, loss: 0.0944
2022-10-03 09:36:32 - train: epoch 0036, iter [00390, 01251], lr: 0.000490, loss: 0.1022
2022-10-03 09:37:00 - train: epoch 0036, iter [00400, 01251], lr: 0.000490, loss: 0.0944
2022-10-03 09:37:28 - train: epoch 0036, iter [00410, 01251], lr: 0.000490, loss: 0.0985
2022-10-03 09:37:57 - train: epoch 0036, iter [00420, 01251], lr: 0.000490, loss: 0.1016
2022-10-03 09:38:25 - train: epoch 0036, iter [00430, 01251], lr: 0.000490, loss: 0.0927
2022-10-03 09:38:53 - train: epoch 0036, iter [00440, 01251], lr: 0.000490, loss: 0.0976
2022-10-03 09:39:21 - train: epoch 0036, iter [00450, 01251], lr: 0.000490, loss: 0.0990
2022-10-03 09:39:49 - train: epoch 0036, iter [00460, 01251], lr: 0.000490, loss: 0.0935
2022-10-03 09:40:18 - train: epoch 0036, iter [00470, 01251], lr: 0.000490, loss: 0.0882
2022-10-03 09:40:46 - train: epoch 0036, iter [00480, 01251], lr: 0.000490, loss: 0.0913
2022-10-03 09:41:14 - train: epoch 0036, iter [00490, 01251], lr: 0.000490, loss: 0.0989
2022-10-03 09:41:42 - train: epoch 0036, iter [00500, 01251], lr: 0.000490, loss: 0.1014
2022-10-03 09:42:10 - train: epoch 0036, iter [00510, 01251], lr: 0.000490, loss: 0.0931
2022-10-03 09:42:38 - train: epoch 0036, iter [00520, 01251], lr: 0.000489, loss: 0.0910
2022-10-03 09:43:06 - train: epoch 0036, iter [00530, 01251], lr: 0.000489, loss: 0.0927
2022-10-03 09:43:35 - train: epoch 0036, iter [00540, 01251], lr: 0.000489, loss: 0.0939
2022-10-03 09:44:03 - train: epoch 0036, iter [00550, 01251], lr: 0.000489, loss: 0.0881
2022-10-03 09:44:31 - train: epoch 0036, iter [00560, 01251], lr: 0.000489, loss: 0.0953
2022-10-03 09:44:59 - train: epoch 0036, iter [00570, 01251], lr: 0.000489, loss: 0.0961
2022-10-03 09:45:28 - train: epoch 0036, iter [00580, 01251], lr: 0.000489, loss: 0.0988
2022-10-03 09:45:56 - train: epoch 0036, iter [00590, 01251], lr: 0.000489, loss: 0.0965
2022-10-03 09:46:24 - train: epoch 0036, iter [00600, 01251], lr: 0.000489, loss: 0.0967
2022-10-03 09:46:52 - train: epoch 0036, iter [00610, 01251], lr: 0.000489, loss: 0.0892
2022-10-03 09:47:20 - train: epoch 0036, iter [00620, 01251], lr: 0.000489, loss: 0.0874
2022-10-03 09:47:48 - train: epoch 0036, iter [00630, 01251], lr: 0.000489, loss: 0.0928
2022-10-03 09:48:17 - train: epoch 0036, iter [00640, 01251], lr: 0.000489, loss: 0.0983
2022-10-03 09:48:45 - train: epoch 0036, iter [00650, 01251], lr: 0.000489, loss: 0.0914
2022-10-03 09:49:13 - train: epoch 0036, iter [00660, 01251], lr: 0.000489, loss: 0.0959
2022-10-03 09:49:41 - train: epoch 0036, iter [00670, 01251], lr: 0.000489, loss: 0.0926
2022-10-03 09:50:09 - train: epoch 0036, iter [00680, 01251], lr: 0.000488, loss: 0.0938
2022-10-03 09:50:37 - train: epoch 0036, iter [00690, 01251], lr: 0.000488, loss: 0.1028
2022-10-03 09:51:05 - train: epoch 0036, iter [00700, 01251], lr: 0.000488, loss: 0.0881
2022-10-03 09:51:33 - train: epoch 0036, iter [00710, 01251], lr: 0.000488, loss: 0.0900
2022-10-03 09:52:01 - train: epoch 0036, iter [00720, 01251], lr: 0.000488, loss: 0.0900
2022-10-03 09:52:29 - train: epoch 0036, iter [00730, 01251], lr: 0.000488, loss: 0.0950
2022-10-03 09:52:58 - train: epoch 0036, iter [00740, 01251], lr: 0.000488, loss: 0.0955
2022-10-03 09:53:26 - train: epoch 0036, iter [00750, 01251], lr: 0.000488, loss: 0.1048
2022-10-03 09:53:54 - train: epoch 0036, iter [00760, 01251], lr: 0.000488, loss: 0.0876
2022-10-03 09:54:22 - train: epoch 0036, iter [00770, 01251], lr: 0.000488, loss: 0.0895
2022-10-03 09:54:50 - train: epoch 0036, iter [00780, 01251], lr: 0.000488, loss: 0.0917
2022-10-03 09:55:19 - train: epoch 0036, iter [00790, 01251], lr: 0.000488, loss: 0.0893
2022-10-03 09:55:47 - train: epoch 0036, iter [00800, 01251], lr: 0.000488, loss: 0.0904
2022-10-03 09:56:15 - train: epoch 0036, iter [00810, 01251], lr: 0.000488, loss: 0.0972
2022-10-03 09:56:43 - train: epoch 0036, iter [00820, 01251], lr: 0.000488, loss: 0.0951
2022-10-03 09:57:11 - train: epoch 0036, iter [00830, 01251], lr: 0.000487, loss: 0.0898
2022-10-03 09:57:39 - train: epoch 0036, iter [00840, 01251], lr: 0.000487, loss: 0.0929
2022-10-03 09:58:07 - train: epoch 0036, iter [00850, 01251], lr: 0.000487, loss: 0.0976
2022-10-03 09:58:36 - train: epoch 0036, iter [00860, 01251], lr: 0.000487, loss: 0.0965
2022-10-03 09:59:04 - train: epoch 0036, iter [00870, 01251], lr: 0.000487, loss: 0.0936
2022-10-03 09:59:32 - train: epoch 0036, iter [00880, 01251], lr: 0.000487, loss: 0.1000
2022-10-03 10:00:01 - train: epoch 0036, iter [00890, 01251], lr: 0.000487, loss: 0.0936
2022-10-03 10:00:29 - train: epoch 0036, iter [00900, 01251], lr: 0.000487, loss: 0.0983
2022-10-03 10:00:57 - train: epoch 0036, iter [00910, 01251], lr: 0.000487, loss: 0.0962
2022-10-03 10:01:26 - train: epoch 0036, iter [00920, 01251], lr: 0.000487, loss: 0.0882
2022-10-03 10:01:54 - train: epoch 0036, iter [00930, 01251], lr: 0.000487, loss: 0.0969
2022-10-03 10:02:22 - train: epoch 0036, iter [00940, 01251], lr: 0.000487, loss: 0.0965
2022-10-03 10:02:50 - train: epoch 0036, iter [00950, 01251], lr: 0.000487, loss: 0.1032
2022-10-03 10:03:19 - train: epoch 0036, iter [00960, 01251], lr: 0.000487, loss: 0.0949
2022-10-03 10:03:47 - train: epoch 0036, iter [00970, 01251], lr: 0.000487, loss: 0.0989
2022-10-03 10:04:15 - train: epoch 0036, iter [00980, 01251], lr: 0.000486, loss: 0.0965
2022-10-03 10:04:43 - train: epoch 0036, iter [00990, 01251], lr: 0.000486, loss: 0.0971
2022-10-03 10:05:11 - train: epoch 0036, iter [01000, 01251], lr: 0.000486, loss: 0.0952
2022-10-03 10:05:40 - train: epoch 0036, iter [01010, 01251], lr: 0.000486, loss: 0.0924
2022-10-03 10:06:08 - train: epoch 0036, iter [01020, 01251], lr: 0.000486, loss: 0.0899
2022-10-03 10:06:36 - train: epoch 0036, iter [01030, 01251], lr: 0.000486, loss: 0.0927
2022-10-03 10:07:05 - train: epoch 0036, iter [01040, 01251], lr: 0.000486, loss: 0.0935
2022-10-03 10:07:33 - train: epoch 0036, iter [01050, 01251], lr: 0.000486, loss: 0.0908
2022-10-03 10:08:01 - train: epoch 0036, iter [01060, 01251], lr: 0.000486, loss: 0.0909
2022-10-03 10:08:29 - train: epoch 0036, iter [01070, 01251], lr: 0.000486, loss: 0.0970
2022-10-03 10:08:58 - train: epoch 0036, iter [01080, 01251], lr: 0.000486, loss: 0.0909
2022-10-03 10:09:26 - train: epoch 0036, iter [01090, 01251], lr: 0.000486, loss: 0.0955
2022-10-03 10:09:54 - train: epoch 0036, iter [01100, 01251], lr: 0.000486, loss: 0.0957
2022-10-03 10:10:22 - train: epoch 0036, iter [01110, 01251], lr: 0.000486, loss: 0.0988
2022-10-03 10:10:50 - train: epoch 0036, iter [01120, 01251], lr: 0.000486, loss: 0.0940
2022-10-03 10:11:18 - train: epoch 0036, iter [01130, 01251], lr: 0.000485, loss: 0.0875
2022-10-03 10:11:46 - train: epoch 0036, iter [01140, 01251], lr: 0.000485, loss: 0.0934
2022-10-03 10:12:15 - train: epoch 0036, iter [01150, 01251], lr: 0.000485, loss: 0.0976
2022-10-03 10:12:43 - train: epoch 0036, iter [01160, 01251], lr: 0.000485, loss: 0.0850
2022-10-03 10:13:11 - train: epoch 0036, iter [01170, 01251], lr: 0.000485, loss: 0.0987
2022-10-03 10:13:40 - train: epoch 0036, iter [01180, 01251], lr: 0.000485, loss: 0.0952
2022-10-03 10:14:08 - train: epoch 0036, iter [01190, 01251], lr: 0.000485, loss: 0.0958
2022-10-03 10:14:36 - train: epoch 0036, iter [01200, 01251], lr: 0.000485, loss: 0.0981
2022-10-03 10:15:05 - train: epoch 0036, iter [01210, 01251], lr: 0.000485, loss: 0.0890
2022-10-03 10:15:33 - train: epoch 0036, iter [01220, 01251], lr: 0.000485, loss: 0.0889
2022-10-03 10:16:01 - train: epoch 0036, iter [01230, 01251], lr: 0.000485, loss: 0.0991
2022-10-03 10:16:29 - train: epoch 0036, iter [01240, 01251], lr: 0.000485, loss: 0.1002
2022-10-03 10:16:57 - train: epoch 0036, iter [01250, 01251], lr: 0.000485, loss: 0.0929
2022-10-03 10:17:01 - train: epoch 036, train_loss: 0.0951
2022-10-03 10:17:03 - until epoch: 036, best_loss: 0.0951
2022-10-03 10:17:03 - epoch 037 lr: 0.000485
2022-10-03 10:17:38 - train: epoch 0037, iter [00010, 01251], lr: 0.000485, loss: 0.1000
2022-10-03 10:18:06 - train: epoch 0037, iter [00020, 01251], lr: 0.000485, loss: 0.0910
2022-10-03 10:18:34 - train: epoch 0037, iter [00030, 01251], lr: 0.000485, loss: 0.0968
2022-10-03 10:19:03 - train: epoch 0037, iter [00040, 01251], lr: 0.000484, loss: 0.0963
2022-10-03 10:19:31 - train: epoch 0037, iter [00050, 01251], lr: 0.000484, loss: 0.0930
2022-10-03 10:19:59 - train: epoch 0037, iter [00060, 01251], lr: 0.000484, loss: 0.0905
2022-10-03 10:20:27 - train: epoch 0037, iter [00070, 01251], lr: 0.000484, loss: 0.0882
2022-10-03 10:20:56 - train: epoch 0037, iter [00080, 01251], lr: 0.000484, loss: 0.1031
2022-10-03 10:21:24 - train: epoch 0037, iter [00090, 01251], lr: 0.000484, loss: 0.0970
2022-10-03 10:21:52 - train: epoch 0037, iter [00100, 01251], lr: 0.000484, loss: 0.0873
2022-10-03 10:22:20 - train: epoch 0037, iter [00110, 01251], lr: 0.000484, loss: 0.0977
2022-10-03 10:22:48 - train: epoch 0037, iter [00120, 01251], lr: 0.000484, loss: 0.1040
2022-10-03 10:23:16 - train: epoch 0037, iter [00130, 01251], lr: 0.000484, loss: 0.0919
2022-10-03 10:23:45 - train: epoch 0037, iter [00140, 01251], lr: 0.000484, loss: 0.0975
2022-10-03 10:24:13 - train: epoch 0037, iter [00150, 01251], lr: 0.000484, loss: 0.0958
2022-10-03 10:24:41 - train: epoch 0037, iter [00160, 01251], lr: 0.000484, loss: 0.0989
2022-10-03 10:25:10 - train: epoch 0037, iter [00170, 01251], lr: 0.000484, loss: 0.1025
2022-10-03 10:25:38 - train: epoch 0037, iter [00180, 01251], lr: 0.000484, loss: 0.0958
2022-10-03 10:26:06 - train: epoch 0037, iter [00190, 01251], lr: 0.000483, loss: 0.0994
2022-10-03 10:26:34 - train: epoch 0037, iter [00200, 01251], lr: 0.000483, loss: 0.0931
2022-10-03 10:27:02 - train: epoch 0037, iter [00210, 01251], lr: 0.000483, loss: 0.0928
2022-10-03 10:27:31 - train: epoch 0037, iter [00220, 01251], lr: 0.000483, loss: 0.0933
2022-10-03 10:27:59 - train: epoch 0037, iter [00230, 01251], lr: 0.000483, loss: 0.0940
2022-10-03 10:28:27 - train: epoch 0037, iter [00240, 01251], lr: 0.000483, loss: 0.0876
2022-10-03 10:28:55 - train: epoch 0037, iter [00250, 01251], lr: 0.000483, loss: 0.0945
2022-10-03 10:29:23 - train: epoch 0037, iter [00260, 01251], lr: 0.000483, loss: 0.0955
2022-10-03 10:29:52 - train: epoch 0037, iter [00270, 01251], lr: 0.000483, loss: 0.0944
2022-10-03 10:30:20 - train: epoch 0037, iter [00280, 01251], lr: 0.000483, loss: 0.1022
2022-10-03 10:30:48 - train: epoch 0037, iter [00290, 01251], lr: 0.000483, loss: 0.0922
2022-10-03 10:31:17 - train: epoch 0037, iter [00300, 01251], lr: 0.000483, loss: 0.0975
2022-10-03 10:31:45 - train: epoch 0037, iter [00310, 01251], lr: 0.000483, loss: 0.0957
2022-10-03 10:32:13 - train: epoch 0037, iter [00320, 01251], lr: 0.000483, loss: 0.1029
2022-10-03 10:32:41 - train: epoch 0037, iter [00330, 01251], lr: 0.000483, loss: 0.0925
2022-10-03 10:33:10 - train: epoch 0037, iter [00340, 01251], lr: 0.000482, loss: 0.0968
2022-10-03 10:33:38 - train: epoch 0037, iter [00350, 01251], lr: 0.000482, loss: 0.0953
2022-10-03 10:34:06 - train: epoch 0037, iter [00360, 01251], lr: 0.000482, loss: 0.0920
2022-10-03 10:34:35 - train: epoch 0037, iter [00370, 01251], lr: 0.000482, loss: 0.0923
2022-10-03 10:35:03 - train: epoch 0037, iter [00380, 01251], lr: 0.000482, loss: 0.0986
2022-10-03 10:35:31 - train: epoch 0037, iter [00390, 01251], lr: 0.000482, loss: 0.0877
2022-10-03 10:36:00 - train: epoch 0037, iter [00400, 01251], lr: 0.000482, loss: 0.0996
2022-10-03 10:36:28 - train: epoch 0037, iter [00410, 01251], lr: 0.000482, loss: 0.0953
2022-10-03 10:36:56 - train: epoch 0037, iter [00420, 01251], lr: 0.000482, loss: 0.0996
2022-10-03 10:37:25 - train: epoch 0037, iter [00430, 01251], lr: 0.000482, loss: 0.0903
2022-10-03 10:37:53 - train: epoch 0037, iter [00440, 01251], lr: 0.000482, loss: 0.0974
2022-10-03 10:38:21 - train: epoch 0037, iter [00450, 01251], lr: 0.000482, loss: 0.0890
2022-10-03 10:38:50 - train: epoch 0037, iter [00460, 01251], lr: 0.000482, loss: 0.0896
2022-10-03 10:39:18 - train: epoch 0037, iter [00470, 01251], lr: 0.000482, loss: 0.0978
2022-10-03 10:39:46 - train: epoch 0037, iter [00480, 01251], lr: 0.000482, loss: 0.0973
2022-10-03 10:40:14 - train: epoch 0037, iter [00490, 01251], lr: 0.000481, loss: 0.0928
2022-10-03 10:40:43 - train: epoch 0037, iter [00500, 01251], lr: 0.000481, loss: 0.0961
2022-10-03 10:41:11 - train: epoch 0037, iter [00510, 01251], lr: 0.000481, loss: 0.1014
2022-10-03 10:41:39 - train: epoch 0037, iter [00520, 01251], lr: 0.000481, loss: 0.0998
2022-10-03 10:42:08 - train: epoch 0037, iter [00530, 01251], lr: 0.000481, loss: 0.0945
2022-10-03 10:42:36 - train: epoch 0037, iter [00540, 01251], lr: 0.000481, loss: 0.0932
2022-10-03 10:43:05 - train: epoch 0037, iter [00550, 01251], lr: 0.000481, loss: 0.0922
2022-10-03 10:43:33 - train: epoch 0037, iter [00560, 01251], lr: 0.000481, loss: 0.0910
2022-10-03 10:44:01 - train: epoch 0037, iter [00570, 01251], lr: 0.000481, loss: 0.0942
2022-10-03 10:44:29 - train: epoch 0037, iter [00580, 01251], lr: 0.000481, loss: 0.0925
2022-10-03 10:44:58 - train: epoch 0037, iter [00590, 01251], lr: 0.000481, loss: 0.0915
2022-10-03 10:45:26 - train: epoch 0037, iter [00600, 01251], lr: 0.000481, loss: 0.0889
2022-10-03 10:45:55 - train: epoch 0037, iter [00610, 01251], lr: 0.000481, loss: 0.0920
2022-10-03 10:46:23 - train: epoch 0037, iter [00620, 01251], lr: 0.000481, loss: 0.0941
2022-10-03 10:46:51 - train: epoch 0037, iter [00630, 01251], lr: 0.000481, loss: 0.0905
2022-10-03 10:47:19 - train: epoch 0037, iter [00640, 01251], lr: 0.000480, loss: 0.0874
2022-10-03 10:47:47 - train: epoch 0037, iter [00650, 01251], lr: 0.000480, loss: 0.0970
2022-10-03 10:48:16 - train: epoch 0037, iter [00660, 01251], lr: 0.000480, loss: 0.0939
2022-10-03 10:48:44 - train: epoch 0037, iter [00670, 01251], lr: 0.000480, loss: 0.0922
2022-10-03 10:49:12 - train: epoch 0037, iter [00680, 01251], lr: 0.000480, loss: 0.0938
2022-10-03 10:49:41 - train: epoch 0037, iter [00690, 01251], lr: 0.000480, loss: 0.0979
2022-10-03 10:50:09 - train: epoch 0037, iter [00700, 01251], lr: 0.000480, loss: 0.0906
2022-10-03 10:50:37 - train: epoch 0037, iter [00710, 01251], lr: 0.000480, loss: 0.0936
2022-10-03 10:51:05 - train: epoch 0037, iter [00720, 01251], lr: 0.000480, loss: 0.0916
2022-10-03 10:51:34 - train: epoch 0037, iter [00730, 01251], lr: 0.000480, loss: 0.0950
2022-10-03 10:52:02 - train: epoch 0037, iter [00740, 01251], lr: 0.000480, loss: 0.0980
2022-10-03 10:52:30 - train: epoch 0037, iter [00750, 01251], lr: 0.000480, loss: 0.0890
2022-10-03 10:52:58 - train: epoch 0037, iter [00760, 01251], lr: 0.000480, loss: 0.0991
2022-10-03 10:53:27 - train: epoch 0037, iter [00770, 01251], lr: 0.000480, loss: 0.0937
2022-10-03 10:53:55 - train: epoch 0037, iter [00780, 01251], lr: 0.000480, loss: 0.0972
2022-10-03 10:54:23 - train: epoch 0037, iter [00790, 01251], lr: 0.000479, loss: 0.0934
2022-10-03 10:54:52 - train: epoch 0037, iter [00800, 01251], lr: 0.000479, loss: 0.0902
2022-10-03 10:55:20 - train: epoch 0037, iter [00810, 01251], lr: 0.000479, loss: 0.1001
2022-10-03 10:55:48 - train: epoch 0037, iter [00820, 01251], lr: 0.000479, loss: 0.0892
2022-10-03 10:56:17 - train: epoch 0037, iter [00830, 01251], lr: 0.000479, loss: 0.1016
2022-10-03 10:56:45 - train: epoch 0037, iter [00840, 01251], lr: 0.000479, loss: 0.0900
2022-10-03 10:57:14 - train: epoch 0037, iter [00850, 01251], lr: 0.000479, loss: 0.0962
2022-10-03 10:57:42 - train: epoch 0037, iter [00860, 01251], lr: 0.000479, loss: 0.0898
2022-10-03 10:58:10 - train: epoch 0037, iter [00870, 01251], lr: 0.000479, loss: 0.0987
2022-10-03 10:58:38 - train: epoch 0037, iter [00880, 01251], lr: 0.000479, loss: 0.0922
2022-10-03 10:59:07 - train: epoch 0037, iter [00890, 01251], lr: 0.000479, loss: 0.0852
2022-10-03 10:59:35 - train: epoch 0037, iter [00900, 01251], lr: 0.000479, loss: 0.0918
2022-10-03 11:00:03 - train: epoch 0037, iter [00910, 01251], lr: 0.000479, loss: 0.0954
2022-10-03 11:00:31 - train: epoch 0037, iter [00920, 01251], lr: 0.000479, loss: 0.0872
2022-10-03 11:01:00 - train: epoch 0037, iter [00930, 01251], lr: 0.000479, loss: 0.0930
2022-10-03 11:01:28 - train: epoch 0037, iter [00940, 01251], lr: 0.000478, loss: 0.0918
2022-10-03 11:01:57 - train: epoch 0037, iter [00950, 01251], lr: 0.000478, loss: 0.0994
2022-10-03 11:02:25 - train: epoch 0037, iter [00960, 01251], lr: 0.000478, loss: 0.0935
2022-10-03 11:02:53 - train: epoch 0037, iter [00970, 01251], lr: 0.000478, loss: 0.0919
2022-10-03 11:03:22 - train: epoch 0037, iter [00980, 01251], lr: 0.000478, loss: 0.0882
2022-10-03 11:03:50 - train: epoch 0037, iter [00990, 01251], lr: 0.000478, loss: 0.0929
2022-10-03 11:04:18 - train: epoch 0037, iter [01000, 01251], lr: 0.000478, loss: 0.0951
2022-10-03 11:04:47 - train: epoch 0037, iter [01010, 01251], lr: 0.000478, loss: 0.0934
2022-10-03 11:05:15 - train: epoch 0037, iter [01020, 01251], lr: 0.000478, loss: 0.0879
2022-10-03 11:05:44 - train: epoch 0037, iter [01030, 01251], lr: 0.000478, loss: 0.0976
2022-10-03 11:06:12 - train: epoch 0037, iter [01040, 01251], lr: 0.000478, loss: 0.0845
2022-10-03 11:06:40 - train: epoch 0037, iter [01050, 01251], lr: 0.000478, loss: 0.0844
2022-10-03 11:07:09 - train: epoch 0037, iter [01060, 01251], lr: 0.000478, loss: 0.0967
2022-10-03 11:07:37 - train: epoch 0037, iter [01070, 01251], lr: 0.000478, loss: 0.0829
2022-10-03 11:08:05 - train: epoch 0037, iter [01080, 01251], lr: 0.000477, loss: 0.0925
2022-10-03 11:08:34 - train: epoch 0037, iter [01090, 01251], lr: 0.000477, loss: 0.0909
2022-10-03 11:09:02 - train: epoch 0037, iter [01100, 01251], lr: 0.000477, loss: 0.0976
2022-10-03 11:09:30 - train: epoch 0037, iter [01110, 01251], lr: 0.000477, loss: 0.0980
2022-10-03 11:09:59 - train: epoch 0037, iter [01120, 01251], lr: 0.000477, loss: 0.0970
2022-10-03 11:10:27 - train: epoch 0037, iter [01130, 01251], lr: 0.000477, loss: 0.0945
2022-10-03 11:10:55 - train: epoch 0037, iter [01140, 01251], lr: 0.000477, loss: 0.0973
2022-10-03 11:11:23 - train: epoch 0037, iter [01150, 01251], lr: 0.000477, loss: 0.0985
2022-10-03 11:11:52 - train: epoch 0037, iter [01160, 01251], lr: 0.000477, loss: 0.0991
2022-10-03 11:12:20 - train: epoch 0037, iter [01170, 01251], lr: 0.000477, loss: 0.0920
2022-10-03 11:12:48 - train: epoch 0037, iter [01180, 01251], lr: 0.000477, loss: 0.0947
2022-10-03 11:13:16 - train: epoch 0037, iter [01190, 01251], lr: 0.000477, loss: 0.0971
2022-10-03 11:13:45 - train: epoch 0037, iter [01200, 01251], lr: 0.000477, loss: 0.0979
2022-10-03 11:14:13 - train: epoch 0037, iter [01210, 01251], lr: 0.000477, loss: 0.0913
2022-10-03 11:14:41 - train: epoch 0037, iter [01220, 01251], lr: 0.000477, loss: 0.1009
2022-10-03 11:15:09 - train: epoch 0037, iter [01230, 01251], lr: 0.000476, loss: 0.0987
2022-10-03 11:15:38 - train: epoch 0037, iter [01240, 01251], lr: 0.000476, loss: 0.0910
2022-10-03 11:16:06 - train: epoch 0037, iter [01250, 01251], lr: 0.000476, loss: 0.0907
2022-10-03 11:16:10 - train: epoch 037, train_loss: 0.0946
2022-10-03 11:16:11 - until epoch: 037, best_loss: 0.0946
2022-10-03 11:16:11 - epoch 038 lr: 0.000476
2022-10-03 11:16:47 - train: epoch 0038, iter [00010, 01251], lr: 0.000476, loss: 0.0938
2022-10-03 11:17:15 - train: epoch 0038, iter [00020, 01251], lr: 0.000476, loss: 0.0977
2022-10-03 11:17:43 - train: epoch 0038, iter [00030, 01251], lr: 0.000476, loss: 0.0914
2022-10-03 11:18:11 - train: epoch 0038, iter [00040, 01251], lr: 0.000476, loss: 0.0936
2022-10-03 11:18:39 - train: epoch 0038, iter [00050, 01251], lr: 0.000476, loss: 0.1011
2022-10-03 11:19:08 - train: epoch 0038, iter [00060, 01251], lr: 0.000476, loss: 0.0916
2022-10-03 11:19:36 - train: epoch 0038, iter [00070, 01251], lr: 0.000476, loss: 0.1009
2022-10-03 11:20:04 - train: epoch 0038, iter [00080, 01251], lr: 0.000476, loss: 0.1009
2022-10-03 11:20:32 - train: epoch 0038, iter [00090, 01251], lr: 0.000476, loss: 0.0960
2022-10-03 11:21:00 - train: epoch 0038, iter [00100, 01251], lr: 0.000476, loss: 0.0977
2022-10-03 11:21:28 - train: epoch 0038, iter [00110, 01251], lr: 0.000476, loss: 0.1000
2022-10-03 11:21:57 - train: epoch 0038, iter [00120, 01251], lr: 0.000476, loss: 0.1017
2022-10-03 11:22:25 - train: epoch 0038, iter [00130, 01251], lr: 0.000475, loss: 0.0978
2022-10-03 11:22:53 - train: epoch 0038, iter [00140, 01251], lr: 0.000475, loss: 0.0900
2022-10-03 11:23:21 - train: epoch 0038, iter [00150, 01251], lr: 0.000475, loss: 0.0849
2022-10-03 11:23:49 - train: epoch 0038, iter [00160, 01251], lr: 0.000475, loss: 0.0881
2022-10-03 11:24:17 - train: epoch 0038, iter [00170, 01251], lr: 0.000475, loss: 0.0963
2022-10-03 11:24:46 - train: epoch 0038, iter [00180, 01251], lr: 0.000475, loss: 0.1008
2022-10-03 11:25:14 - train: epoch 0038, iter [00190, 01251], lr: 0.000475, loss: 0.0906
2022-10-03 11:25:42 - train: epoch 0038, iter [00200, 01251], lr: 0.000475, loss: 0.0866
2022-10-03 11:26:10 - train: epoch 0038, iter [00210, 01251], lr: 0.000475, loss: 0.0945
2022-10-03 11:26:38 - train: epoch 0038, iter [00220, 01251], lr: 0.000475, loss: 0.0968
2022-10-03 11:27:07 - train: epoch 0038, iter [00230, 01251], lr: 0.000475, loss: 0.0973
2022-10-03 11:27:35 - train: epoch 0038, iter [00240, 01251], lr: 0.000475, loss: 0.1010
2022-10-03 11:28:03 - train: epoch 0038, iter [00250, 01251], lr: 0.000475, loss: 0.0961
2022-10-03 11:28:31 - train: epoch 0038, iter [00260, 01251], lr: 0.000475, loss: 0.0923
2022-10-03 11:28:59 - train: epoch 0038, iter [00270, 01251], lr: 0.000475, loss: 0.0927
2022-10-03 11:29:27 - train: epoch 0038, iter [00280, 01251], lr: 0.000474, loss: 0.0921
2022-10-03 11:29:56 - train: epoch 0038, iter [00290, 01251], lr: 0.000474, loss: 0.0880
2022-10-03 11:30:24 - train: epoch 0038, iter [00300, 01251], lr: 0.000474, loss: 0.0984
2022-10-03 11:30:52 - train: epoch 0038, iter [00310, 01251], lr: 0.000474, loss: 0.1032
2022-10-03 11:31:20 - train: epoch 0038, iter [00320, 01251], lr: 0.000474, loss: 0.0891
2022-10-03 11:31:48 - train: epoch 0038, iter [00330, 01251], lr: 0.000474, loss: 0.0938
2022-10-03 11:32:17 - train: epoch 0038, iter [00340, 01251], lr: 0.000474, loss: 0.0994
2022-10-03 11:32:45 - train: epoch 0038, iter [00350, 01251], lr: 0.000474, loss: 0.0944
2022-10-03 11:33:13 - train: epoch 0038, iter [00360, 01251], lr: 0.000474, loss: 0.0974
2022-10-03 11:33:41 - train: epoch 0038, iter [00370, 01251], lr: 0.000474, loss: 0.0917
2022-10-03 11:34:09 - train: epoch 0038, iter [00380, 01251], lr: 0.000474, loss: 0.0930
2022-10-03 11:34:37 - train: epoch 0038, iter [00390, 01251], lr: 0.000474, loss: 0.0845
2022-10-03 11:35:05 - train: epoch 0038, iter [00400, 01251], lr: 0.000474, loss: 0.0849
2022-10-03 11:35:33 - train: epoch 0038, iter [00410, 01251], lr: 0.000474, loss: 0.0946
2022-10-03 11:36:01 - train: epoch 0038, iter [00420, 01251], lr: 0.000473, loss: 0.0949
2022-10-03 11:36:30 - train: epoch 0038, iter [00430, 01251], lr: 0.000473, loss: 0.0911
2022-10-03 11:36:58 - train: epoch 0038, iter [00440, 01251], lr: 0.000473, loss: 0.1006
2022-10-03 11:37:26 - train: epoch 0038, iter [00450, 01251], lr: 0.000473, loss: 0.0994
2022-10-03 11:37:54 - train: epoch 0038, iter [00460, 01251], lr: 0.000473, loss: 0.0937
2022-10-03 11:38:22 - train: epoch 0038, iter [00470, 01251], lr: 0.000473, loss: 0.0967
2022-10-03 11:38:50 - train: epoch 0038, iter [00480, 01251], lr: 0.000473, loss: 0.0922
2022-10-03 11:39:18 - train: epoch 0038, iter [00490, 01251], lr: 0.000473, loss: 0.0945
2022-10-03 11:39:46 - train: epoch 0038, iter [00500, 01251], lr: 0.000473, loss: 0.0892
2022-10-03 11:40:14 - train: epoch 0038, iter [00510, 01251], lr: 0.000473, loss: 0.0914
2022-10-03 11:40:42 - train: epoch 0038, iter [00520, 01251], lr: 0.000473, loss: 0.0945
2022-10-03 11:41:10 - train: epoch 0038, iter [00530, 01251], lr: 0.000473, loss: 0.0992
2022-10-03 11:41:38 - train: epoch 0038, iter [00540, 01251], lr: 0.000473, loss: 0.0981
2022-10-03 11:42:06 - train: epoch 0038, iter [00550, 01251], lr: 0.000473, loss: 0.0872
2022-10-03 11:42:34 - train: epoch 0038, iter [00560, 01251], lr: 0.000473, loss: 0.0974
2022-10-03 11:43:02 - train: epoch 0038, iter [00570, 01251], lr: 0.000472, loss: 0.0963
2022-10-03 11:43:31 - train: epoch 0038, iter [00580, 01251], lr: 0.000472, loss: 0.0964
2022-10-03 11:43:59 - train: epoch 0038, iter [00590, 01251], lr: 0.000472, loss: 0.0897
2022-10-03 11:44:26 - train: epoch 0038, iter [00600, 01251], lr: 0.000472, loss: 0.0958
2022-10-03 11:44:54 - train: epoch 0038, iter [00610, 01251], lr: 0.000472, loss: 0.0909
2022-10-03 11:45:22 - train: epoch 0038, iter [00620, 01251], lr: 0.000472, loss: 0.1001
2022-10-03 11:45:50 - train: epoch 0038, iter [00630, 01251], lr: 0.000472, loss: 0.0952
2022-10-03 11:46:18 - train: epoch 0038, iter [00640, 01251], lr: 0.000472, loss: 0.0957
2022-10-03 11:46:46 - train: epoch 0038, iter [00650, 01251], lr: 0.000472, loss: 0.1002
2022-10-03 11:47:15 - train: epoch 0038, iter [00660, 01251], lr: 0.000472, loss: 0.0961
2022-10-03 11:47:43 - train: epoch 0038, iter [00670, 01251], lr: 0.000472, loss: 0.0918
2022-10-03 11:48:11 - train: epoch 0038, iter [00680, 01251], lr: 0.000472, loss: 0.0925
2022-10-03 11:48:39 - train: epoch 0038, iter [00690, 01251], lr: 0.000472, loss: 0.0944
2022-10-03 11:49:07 - train: epoch 0038, iter [00700, 01251], lr: 0.000472, loss: 0.0921
2022-10-03 11:49:35 - train: epoch 0038, iter [00710, 01251], lr: 0.000471, loss: 0.0968
2022-10-03 11:50:03 - train: epoch 0038, iter [00720, 01251], lr: 0.000471, loss: 0.0933
2022-10-03 11:50:31 - train: epoch 0038, iter [00730, 01251], lr: 0.000471, loss: 0.0952
2022-10-03 11:51:00 - train: epoch 0038, iter [00740, 01251], lr: 0.000471, loss: 0.0942
2022-10-03 11:51:28 - train: epoch 0038, iter [00750, 01251], lr: 0.000471, loss: 0.0933
2022-10-03 11:51:56 - train: epoch 0038, iter [00760, 01251], lr: 0.000471, loss: 0.0990
2022-10-03 11:52:24 - train: epoch 0038, iter [00770, 01251], lr: 0.000471, loss: 0.0918
2022-10-03 11:52:52 - train: epoch 0038, iter [00780, 01251], lr: 0.000471, loss: 0.0954
2022-10-03 11:53:20 - train: epoch 0038, iter [00790, 01251], lr: 0.000471, loss: 0.0947
2022-10-03 11:53:48 - train: epoch 0038, iter [00800, 01251], lr: 0.000471, loss: 0.0928
2022-10-03 11:54:16 - train: epoch 0038, iter [00810, 01251], lr: 0.000471, loss: 0.0905
2022-10-03 11:54:44 - train: epoch 0038, iter [00820, 01251], lr: 0.000471, loss: 0.1031
2022-10-03 11:55:12 - train: epoch 0038, iter [00830, 01251], lr: 0.000471, loss: 0.0915
2022-10-03 11:55:40 - train: epoch 0038, iter [00840, 01251], lr: 0.000471, loss: 0.0862
2022-10-03 11:56:08 - train: epoch 0038, iter [00850, 01251], lr: 0.000471, loss: 0.0974
2022-10-03 11:56:37 - train: epoch 0038, iter [00860, 01251], lr: 0.000470, loss: 0.0956
2022-10-03 11:57:05 - train: epoch 0038, iter [00870, 01251], lr: 0.000470, loss: 0.0895
2022-10-03 11:57:32 - train: epoch 0038, iter [00880, 01251], lr: 0.000470, loss: 0.0942
2022-10-03 11:58:00 - train: epoch 0038, iter [00890, 01251], lr: 0.000470, loss: 0.0933
2022-10-03 11:58:28 - train: epoch 0038, iter [00900, 01251], lr: 0.000470, loss: 0.0915
2022-10-03 11:58:57 - train: epoch 0038, iter [00910, 01251], lr: 0.000470, loss: 0.0950
2022-10-03 11:59:25 - train: epoch 0038, iter [00920, 01251], lr: 0.000470, loss: 0.0915
2022-10-03 11:59:53 - train: epoch 0038, iter [00930, 01251], lr: 0.000470, loss: 0.0965
2022-10-03 12:00:21 - train: epoch 0038, iter [00940, 01251], lr: 0.000470, loss: 0.0968
2022-10-03 12:00:49 - train: epoch 0038, iter [00950, 01251], lr: 0.000470, loss: 0.0978
2022-10-03 12:01:17 - train: epoch 0038, iter [00960, 01251], lr: 0.000470, loss: 0.0985
2022-10-03 12:01:45 - train: epoch 0038, iter [00970, 01251], lr: 0.000470, loss: 0.0944
2022-10-03 12:02:13 - train: epoch 0038, iter [00980, 01251], lr: 0.000470, loss: 0.1004
2022-10-03 12:02:41 - train: epoch 0038, iter [00990, 01251], lr: 0.000470, loss: 0.0994
2022-10-03 12:03:09 - train: epoch 0038, iter [01000, 01251], lr: 0.000469, loss: 0.1005
2022-10-03 12:03:37 - train: epoch 0038, iter [01010, 01251], lr: 0.000469, loss: 0.0971
2022-10-03 12:04:05 - train: epoch 0038, iter [01020, 01251], lr: 0.000469, loss: 0.0985
2022-10-03 12:04:34 - train: epoch 0038, iter [01030, 01251], lr: 0.000469, loss: 0.0929
2022-10-03 12:05:02 - train: epoch 0038, iter [01040, 01251], lr: 0.000469, loss: 0.0905
2022-10-03 12:05:30 - train: epoch 0038, iter [01050, 01251], lr: 0.000469, loss: 0.0859
2022-10-03 12:05:58 - train: epoch 0038, iter [01060, 01251], lr: 0.000469, loss: 0.0913
2022-10-03 12:06:26 - train: epoch 0038, iter [01070, 01251], lr: 0.000469, loss: 0.0979
2022-10-03 12:06:54 - train: epoch 0038, iter [01080, 01251], lr: 0.000469, loss: 0.0891
2022-10-03 12:07:22 - train: epoch 0038, iter [01090, 01251], lr: 0.000469, loss: 0.0867
2022-10-03 12:07:50 - train: epoch 0038, iter [01100, 01251], lr: 0.000469, loss: 0.0965
2022-10-03 12:08:18 - train: epoch 0038, iter [01110, 01251], lr: 0.000469, loss: 0.1019
2022-10-03 12:08:46 - train: epoch 0038, iter [01120, 01251], lr: 0.000469, loss: 0.0976
2022-10-03 12:09:14 - train: epoch 0038, iter [01130, 01251], lr: 0.000469, loss: 0.1050
2022-10-03 12:09:42 - train: epoch 0038, iter [01140, 01251], lr: 0.000469, loss: 0.0881
2022-10-03 12:10:11 - train: epoch 0038, iter [01150, 01251], lr: 0.000468, loss: 0.0833
2022-10-03 12:10:39 - train: epoch 0038, iter [01160, 01251], lr: 0.000468, loss: 0.0904
2022-10-03 12:11:07 - train: epoch 0038, iter [01170, 01251], lr: 0.000468, loss: 0.0919
2022-10-03 12:11:35 - train: epoch 0038, iter [01180, 01251], lr: 0.000468, loss: 0.0974
2022-10-03 12:12:03 - train: epoch 0038, iter [01190, 01251], lr: 0.000468, loss: 0.0925
2022-10-03 12:12:31 - train: epoch 0038, iter [01200, 01251], lr: 0.000468, loss: 0.0904
2022-10-03 12:13:00 - train: epoch 0038, iter [01210, 01251], lr: 0.000468, loss: 0.0934
2022-10-03 12:13:28 - train: epoch 0038, iter [01220, 01251], lr: 0.000468, loss: 0.0832
2022-10-03 12:13:56 - train: epoch 0038, iter [01230, 01251], lr: 0.000468, loss: 0.0948
2022-10-03 12:14:24 - train: epoch 0038, iter [01240, 01251], lr: 0.000468, loss: 0.0933
2022-10-03 12:14:52 - train: epoch 0038, iter [01250, 01251], lr: 0.000468, loss: 0.1003
2022-10-03 12:14:56 - train: epoch 038, train_loss: 0.0940
2022-10-03 12:14:58 - until epoch: 038, best_loss: 0.0940
2022-10-03 12:14:58 - epoch 039 lr: 0.000468
2022-10-03 12:15:34 - train: epoch 0039, iter [00010, 01251], lr: 0.000468, loss: 0.0983
2022-10-03 12:16:02 - train: epoch 0039, iter [00020, 01251], lr: 0.000468, loss: 0.0903
2022-10-03 12:16:30 - train: epoch 0039, iter [00030, 01251], lr: 0.000468, loss: 0.0917
2022-10-03 12:16:59 - train: epoch 0039, iter [00040, 01251], lr: 0.000467, loss: 0.0964
2022-10-03 12:17:27 - train: epoch 0039, iter [00050, 01251], lr: 0.000467, loss: 0.0945
2022-10-03 12:17:55 - train: epoch 0039, iter [00060, 01251], lr: 0.000467, loss: 0.0897
2022-10-03 12:18:23 - train: epoch 0039, iter [00070, 01251], lr: 0.000467, loss: 0.0926
2022-10-03 12:18:52 - train: epoch 0039, iter [00080, 01251], lr: 0.000467, loss: 0.0962
2022-10-03 12:19:20 - train: epoch 0039, iter [00090, 01251], lr: 0.000467, loss: 0.0926
2022-10-03 12:19:48 - train: epoch 0039, iter [00100, 01251], lr: 0.000467, loss: 0.0967
2022-10-03 12:20:16 - train: epoch 0039, iter [00110, 01251], lr: 0.000467, loss: 0.0958
2022-10-03 12:20:44 - train: epoch 0039, iter [00120, 01251], lr: 0.000467, loss: 0.0934
2022-10-03 12:21:13 - train: epoch 0039, iter [00130, 01251], lr: 0.000467, loss: 0.0862
2022-10-03 12:21:41 - train: epoch 0039, iter [00140, 01251], lr: 0.000467, loss: 0.0913
2022-10-03 12:22:09 - train: epoch 0039, iter [00150, 01251], lr: 0.000467, loss: 0.0888
2022-10-03 12:22:37 - train: epoch 0039, iter [00160, 01251], lr: 0.000467, loss: 0.0882
2022-10-03 12:23:05 - train: epoch 0039, iter [00170, 01251], lr: 0.000467, loss: 0.0831
2022-10-03 12:23:33 - train: epoch 0039, iter [00180, 01251], lr: 0.000467, loss: 0.0989
2022-10-03 12:24:02 - train: epoch 0039, iter [00190, 01251], lr: 0.000466, loss: 0.0859
2022-10-03 12:24:30 - train: epoch 0039, iter [00200, 01251], lr: 0.000466, loss: 0.0884
2022-10-03 12:24:58 - train: epoch 0039, iter [00210, 01251], lr: 0.000466, loss: 0.0911
2022-10-03 12:25:26 - train: epoch 0039, iter [00220, 01251], lr: 0.000466, loss: 0.0950
2022-10-03 12:25:55 - train: epoch 0039, iter [00230, 01251], lr: 0.000466, loss: 0.0988
2022-10-03 12:26:23 - train: epoch 0039, iter [00240, 01251], lr: 0.000466, loss: 0.0895
2022-10-03 12:26:51 - train: epoch 0039, iter [00250, 01251], lr: 0.000466, loss: 0.0930
2022-10-03 12:27:20 - train: epoch 0039, iter [00260, 01251], lr: 0.000466, loss: 0.0969
2022-10-03 12:27:48 - train: epoch 0039, iter [00270, 01251], lr: 0.000466, loss: 0.0952
2022-10-03 12:28:16 - train: epoch 0039, iter [00280, 01251], lr: 0.000466, loss: 0.0986
2022-10-03 12:28:45 - train: epoch 0039, iter [00290, 01251], lr: 0.000466, loss: 0.0921
2022-10-03 12:29:13 - train: epoch 0039, iter [00300, 01251], lr: 0.000466, loss: 0.0949
2022-10-03 12:29:41 - train: epoch 0039, iter [00310, 01251], lr: 0.000466, loss: 0.0957
2022-10-03 12:30:10 - train: epoch 0039, iter [00320, 01251], lr: 0.000466, loss: 0.0845
2022-10-03 12:30:38 - train: epoch 0039, iter [00330, 01251], lr: 0.000465, loss: 0.0955
2022-10-03 12:31:06 - train: epoch 0039, iter [00340, 01251], lr: 0.000465, loss: 0.0987
2022-10-03 12:31:35 - train: epoch 0039, iter [00350, 01251], lr: 0.000465, loss: 0.0942
2022-10-03 12:32:03 - train: epoch 0039, iter [00360, 01251], lr: 0.000465, loss: 0.0860
2022-10-03 12:32:31 - train: epoch 0039, iter [00370, 01251], lr: 0.000465, loss: 0.0903
2022-10-03 12:33:00 - train: epoch 0039, iter [00380, 01251], lr: 0.000465, loss: 0.0859
2022-10-03 12:33:28 - train: epoch 0039, iter [00390, 01251], lr: 0.000465, loss: 0.0897
2022-10-03 12:33:56 - train: epoch 0039, iter [00400, 01251], lr: 0.000465, loss: 0.0969
2022-10-03 12:34:25 - train: epoch 0039, iter [00410, 01251], lr: 0.000465, loss: 0.0947
2022-10-03 12:34:53 - train: epoch 0039, iter [00420, 01251], lr: 0.000465, loss: 0.0797
2022-10-03 12:35:21 - train: epoch 0039, iter [00430, 01251], lr: 0.000465, loss: 0.1007
2022-10-03 12:35:49 - train: epoch 0039, iter [00440, 01251], lr: 0.000465, loss: 0.0951
2022-10-03 12:36:17 - train: epoch 0039, iter [00450, 01251], lr: 0.000465, loss: 0.0979
2022-10-03 12:36:46 - train: epoch 0039, iter [00460, 01251], lr: 0.000465, loss: 0.0933
2022-10-03 12:37:14 - train: epoch 0039, iter [00470, 01251], lr: 0.000464, loss: 0.0885
2022-10-03 12:37:42 - train: epoch 0039, iter [00480, 01251], lr: 0.000464, loss: 0.0875
2022-10-03 12:38:10 - train: epoch 0039, iter [00490, 01251], lr: 0.000464, loss: 0.0886
2022-10-03 12:38:38 - train: epoch 0039, iter [00500, 01251], lr: 0.000464, loss: 0.0835
2022-10-03 12:39:06 - train: epoch 0039, iter [00510, 01251], lr: 0.000464, loss: 0.0921
2022-10-03 12:39:35 - train: epoch 0039, iter [00520, 01251], lr: 0.000464, loss: 0.0881
2022-10-03 12:40:03 - train: epoch 0039, iter [00530, 01251], lr: 0.000464, loss: 0.0907
2022-10-03 12:40:31 - train: epoch 0039, iter [00540, 01251], lr: 0.000464, loss: 0.0918
2022-10-03 12:41:00 - train: epoch 0039, iter [00550, 01251], lr: 0.000464, loss: 0.0964
2022-10-03 12:41:28 - train: epoch 0039, iter [00560, 01251], lr: 0.000464, loss: 0.0932
2022-10-03 12:41:56 - train: epoch 0039, iter [00570, 01251], lr: 0.000464, loss: 0.0909
2022-10-03 12:42:24 - train: epoch 0039, iter [00580, 01251], lr: 0.000464, loss: 0.0972
2022-10-03 12:42:53 - train: epoch 0039, iter [00590, 01251], lr: 0.000464, loss: 0.1010
2022-10-03 12:43:21 - train: epoch 0039, iter [00600, 01251], lr: 0.000464, loss: 0.0900
2022-10-03 12:43:49 - train: epoch 0039, iter [00610, 01251], lr: 0.000464, loss: 0.0901
2022-10-03 12:44:18 - train: epoch 0039, iter [00620, 01251], lr: 0.000463, loss: 0.1000
2022-10-03 12:44:46 - train: epoch 0039, iter [00630, 01251], lr: 0.000463, loss: 0.1012
2022-10-03 12:45:14 - train: epoch 0039, iter [00640, 01251], lr: 0.000463, loss: 0.0916
2022-10-03 12:45:43 - train: epoch 0039, iter [00650, 01251], lr: 0.000463, loss: 0.0919
2022-10-03 12:46:11 - train: epoch 0039, iter [00660, 01251], lr: 0.000463, loss: 0.0957
2022-10-03 12:46:39 - train: epoch 0039, iter [00670, 01251], lr: 0.000463, loss: 0.0950
2022-10-03 12:47:08 - train: epoch 0039, iter [00680, 01251], lr: 0.000463, loss: 0.0951
2022-10-03 12:47:36 - train: epoch 0039, iter [00690, 01251], lr: 0.000463, loss: 0.0990
2022-10-03 12:48:04 - train: epoch 0039, iter [00700, 01251], lr: 0.000463, loss: 0.0994
2022-10-03 12:48:33 - train: epoch 0039, iter [00710, 01251], lr: 0.000463, loss: 0.0979
2022-10-03 12:49:01 - train: epoch 0039, iter [00720, 01251], lr: 0.000463, loss: 0.0944
2022-10-03 12:49:29 - train: epoch 0039, iter [00730, 01251], lr: 0.000463, loss: 0.0991
2022-10-03 12:49:58 - train: epoch 0039, iter [00740, 01251], lr: 0.000463, loss: 0.0944
2022-10-03 12:50:26 - train: epoch 0039, iter [00750, 01251], lr: 0.000463, loss: 0.0917
2022-10-03 12:50:54 - train: epoch 0039, iter [00760, 01251], lr: 0.000462, loss: 0.0921
2022-10-03 12:51:22 - train: epoch 0039, iter [00770, 01251], lr: 0.000462, loss: 0.0883
2022-10-03 12:51:50 - train: epoch 0039, iter [00780, 01251], lr: 0.000462, loss: 0.0914
2022-10-03 12:52:18 - train: epoch 0039, iter [00790, 01251], lr: 0.000462, loss: 0.0979
2022-10-03 12:52:46 - train: epoch 0039, iter [00800, 01251], lr: 0.000462, loss: 0.1071
2022-10-03 12:53:15 - train: epoch 0039, iter [00810, 01251], lr: 0.000462, loss: 0.0944
2022-10-03 12:53:43 - train: epoch 0039, iter [00820, 01251], lr: 0.000462, loss: 0.0941
2022-10-03 12:54:11 - train: epoch 0039, iter [00830, 01251], lr: 0.000462, loss: 0.0903
2022-10-03 12:54:39 - train: epoch 0039, iter [00840, 01251], lr: 0.000462, loss: 0.0909
2022-10-03 12:55:07 - train: epoch 0039, iter [00850, 01251], lr: 0.000462, loss: 0.0931
2022-10-03 12:55:35 - train: epoch 0039, iter [00860, 01251], lr: 0.000462, loss: 0.1012
2022-10-03 12:56:04 - train: epoch 0039, iter [00870, 01251], lr: 0.000462, loss: 0.0930
2022-10-03 12:56:32 - train: epoch 0039, iter [00880, 01251], lr: 0.000462, loss: 0.0939
2022-10-03 12:57:00 - train: epoch 0039, iter [00890, 01251], lr: 0.000462, loss: 0.0973
2022-10-03 12:57:28 - train: epoch 0039, iter [00900, 01251], lr: 0.000461, loss: 0.0920
2022-10-03 12:57:57 - train: epoch 0039, iter [00910, 01251], lr: 0.000461, loss: 0.0854
2022-10-03 12:58:25 - train: epoch 0039, iter [00920, 01251], lr: 0.000461, loss: 0.0978
2022-10-03 12:58:53 - train: epoch 0039, iter [00930, 01251], lr: 0.000461, loss: 0.0946
2022-10-03 12:59:21 - train: epoch 0039, iter [00940, 01251], lr: 0.000461, loss: 0.0991
2022-10-03 12:59:50 - train: epoch 0039, iter [00950, 01251], lr: 0.000461, loss: 0.0953
2022-10-03 13:00:18 - train: epoch 0039, iter [00960, 01251], lr: 0.000461, loss: 0.0884
2022-10-03 13:00:46 - train: epoch 0039, iter [00970, 01251], lr: 0.000461, loss: 0.0908
2022-10-03 13:01:14 - train: epoch 0039, iter [00980, 01251], lr: 0.000461, loss: 0.0974
2022-10-03 13:01:42 - train: epoch 0039, iter [00990, 01251], lr: 0.000461, loss: 0.1029
2022-10-03 13:02:11 - train: epoch 0039, iter [01000, 01251], lr: 0.000461, loss: 0.0942
2022-10-03 13:02:39 - train: epoch 0039, iter [01010, 01251], lr: 0.000461, loss: 0.0911
2022-10-03 13:03:07 - train: epoch 0039, iter [01020, 01251], lr: 0.000461, loss: 0.0970
2022-10-03 13:03:35 - train: epoch 0039, iter [01030, 01251], lr: 0.000461, loss: 0.0885
2022-10-03 13:04:03 - train: epoch 0039, iter [01040, 01251], lr: 0.000460, loss: 0.0972
2022-10-03 13:04:31 - train: epoch 0039, iter [01050, 01251], lr: 0.000460, loss: 0.0869
2022-10-03 13:04:59 - train: epoch 0039, iter [01060, 01251], lr: 0.000460, loss: 0.0824
2022-10-03 13:05:27 - train: epoch 0039, iter [01070, 01251], lr: 0.000460, loss: 0.0875
2022-10-03 13:05:55 - train: epoch 0039, iter [01080, 01251], lr: 0.000460, loss: 0.0971
2022-10-03 13:06:23 - train: epoch 0039, iter [01090, 01251], lr: 0.000460, loss: 0.0865
2022-10-03 13:06:52 - train: epoch 0039, iter [01100, 01251], lr: 0.000460, loss: 0.1032
2022-10-03 13:07:20 - train: epoch 0039, iter [01110, 01251], lr: 0.000460, loss: 0.0896
2022-10-03 13:07:48 - train: epoch 0039, iter [01120, 01251], lr: 0.000460, loss: 0.0890
2022-10-03 13:08:16 - train: epoch 0039, iter [01130, 01251], lr: 0.000460, loss: 0.0883
2022-10-03 13:08:44 - train: epoch 0039, iter [01140, 01251], lr: 0.000460, loss: 0.0956
2022-10-03 13:09:13 - train: epoch 0039, iter [01150, 01251], lr: 0.000460, loss: 0.0921
2022-10-03 13:09:41 - train: epoch 0039, iter [01160, 01251], lr: 0.000460, loss: 0.0876
2022-10-03 13:10:09 - train: epoch 0039, iter [01170, 01251], lr: 0.000460, loss: 0.0982
2022-10-03 13:10:37 - train: epoch 0039, iter [01180, 01251], lr: 0.000459, loss: 0.0899
2022-10-03 13:11:05 - train: epoch 0039, iter [01190, 01251], lr: 0.000459, loss: 0.0891
2022-10-03 13:11:33 - train: epoch 0039, iter [01200, 01251], lr: 0.000459, loss: 0.0951
2022-10-03 13:12:02 - train: epoch 0039, iter [01210, 01251], lr: 0.000459, loss: 0.0882
2022-10-03 13:12:30 - train: epoch 0039, iter [01220, 01251], lr: 0.000459, loss: 0.1016
2022-10-03 13:12:58 - train: epoch 0039, iter [01230, 01251], lr: 0.000459, loss: 0.0959
2022-10-03 13:13:26 - train: epoch 0039, iter [01240, 01251], lr: 0.000459, loss: 0.0959
2022-10-03 13:13:54 - train: epoch 0039, iter [01250, 01251], lr: 0.000459, loss: 0.0939
2022-10-03 13:13:59 - train: epoch 039, train_loss: 0.0937
2022-10-03 13:14:00 - until epoch: 039, best_loss: 0.0937
2022-10-03 13:14:00 - epoch 040 lr: 0.000459
2022-10-03 13:14:35 - train: epoch 0040, iter [00010, 01251], lr: 0.000459, loss: 0.0885
2022-10-03 13:15:03 - train: epoch 0040, iter [00020, 01251], lr: 0.000459, loss: 0.0914
2022-10-03 13:15:31 - train: epoch 0040, iter [00030, 01251], lr: 0.000459, loss: 0.0962
2022-10-03 13:16:00 - train: epoch 0040, iter [00040, 01251], lr: 0.000459, loss: 0.0955
2022-10-03 13:16:28 - train: epoch 0040, iter [00050, 01251], lr: 0.000459, loss: 0.0972
2022-10-03 13:16:56 - train: epoch 0040, iter [00060, 01251], lr: 0.000459, loss: 0.0906
2022-10-03 13:17:24 - train: epoch 0040, iter [00070, 01251], lr: 0.000458, loss: 0.0874
2022-10-03 13:17:53 - train: epoch 0040, iter [00080, 01251], lr: 0.000458, loss: 0.0891
2022-10-03 13:18:21 - train: epoch 0040, iter [00090, 01251], lr: 0.000458, loss: 0.0928
2022-10-03 13:18:49 - train: epoch 0040, iter [00100, 01251], lr: 0.000458, loss: 0.0979
2022-10-03 13:19:17 - train: epoch 0040, iter [00110, 01251], lr: 0.000458, loss: 0.0929
2022-10-03 13:19:45 - train: epoch 0040, iter [00120, 01251], lr: 0.000458, loss: 0.0938
2022-10-03 13:20:13 - train: epoch 0040, iter [00130, 01251], lr: 0.000458, loss: 0.0879
2022-10-03 13:20:41 - train: epoch 0040, iter [00140, 01251], lr: 0.000458, loss: 0.0951
2022-10-03 13:21:09 - train: epoch 0040, iter [00150, 01251], lr: 0.000458, loss: 0.0945
2022-10-03 13:21:37 - train: epoch 0040, iter [00160, 01251], lr: 0.000458, loss: 0.0947
2022-10-03 13:22:05 - train: epoch 0040, iter [00170, 01251], lr: 0.000458, loss: 0.0892
2022-10-03 13:22:33 - train: epoch 0040, iter [00180, 01251], lr: 0.000458, loss: 0.0945
2022-10-03 13:23:01 - train: epoch 0040, iter [00190, 01251], lr: 0.000458, loss: 0.0963
2022-10-03 13:23:29 - train: epoch 0040, iter [00200, 01251], lr: 0.000458, loss: 0.0943
2022-10-03 13:23:58 - train: epoch 0040, iter [00210, 01251], lr: 0.000457, loss: 0.0975
2022-10-03 13:24:26 - train: epoch 0040, iter [00220, 01251], lr: 0.000457, loss: 0.0954
2022-10-03 13:24:54 - train: epoch 0040, iter [00230, 01251], lr: 0.000457, loss: 0.0931
2022-10-03 13:25:22 - train: epoch 0040, iter [00240, 01251], lr: 0.000457, loss: 0.0943
2022-10-03 13:25:50 - train: epoch 0040, iter [00250, 01251], lr: 0.000457, loss: 0.0948
2022-10-03 13:26:18 - train: epoch 0040, iter [00260, 01251], lr: 0.000457, loss: 0.0984
2022-10-03 13:26:46 - train: epoch 0040, iter [00270, 01251], lr: 0.000457, loss: 0.0900
2022-10-03 13:27:14 - train: epoch 0040, iter [00280, 01251], lr: 0.000457, loss: 0.0954
2022-10-03 13:27:42 - train: epoch 0040, iter [00290, 01251], lr: 0.000457, loss: 0.0847
2022-10-03 13:28:11 - train: epoch 0040, iter [00300, 01251], lr: 0.000457, loss: 0.0816
2022-10-03 13:28:38 - train: epoch 0040, iter [00310, 01251], lr: 0.000457, loss: 0.0901
2022-10-03 13:29:07 - train: epoch 0040, iter [00320, 01251], lr: 0.000457, loss: 0.0926
2022-10-03 13:29:35 - train: epoch 0040, iter [00330, 01251], lr: 0.000457, loss: 0.0950
2022-10-03 13:30:03 - train: epoch 0040, iter [00340, 01251], lr: 0.000457, loss: 0.0946
2022-10-03 13:30:31 - train: epoch 0040, iter [00350, 01251], lr: 0.000456, loss: 0.0973
2022-10-03 13:31:00 - train: epoch 0040, iter [00360, 01251], lr: 0.000456, loss: 0.0984
2022-10-03 13:31:28 - train: epoch 0040, iter [00370, 01251], lr: 0.000456, loss: 0.0891
2022-10-03 13:31:56 - train: epoch 0040, iter [00380, 01251], lr: 0.000456, loss: 0.0974
2022-10-03 13:32:24 - train: epoch 0040, iter [00390, 01251], lr: 0.000456, loss: 0.1006
2022-10-03 13:32:52 - train: epoch 0040, iter [00400, 01251], lr: 0.000456, loss: 0.0908
2022-10-03 13:33:20 - train: epoch 0040, iter [00410, 01251], lr: 0.000456, loss: 0.0961
2022-10-03 13:33:49 - train: epoch 0040, iter [00420, 01251], lr: 0.000456, loss: 0.0992
2022-10-03 13:34:17 - train: epoch 0040, iter [00430, 01251], lr: 0.000456, loss: 0.0914
2022-10-03 13:34:45 - train: epoch 0040, iter [00440, 01251], lr: 0.000456, loss: 0.0924
2022-10-03 13:35:13 - train: epoch 0040, iter [00450, 01251], lr: 0.000456, loss: 0.0917
2022-10-03 13:35:41 - train: epoch 0040, iter [00460, 01251], lr: 0.000456, loss: 0.0901
2022-10-03 13:36:10 - train: epoch 0040, iter [00470, 01251], lr: 0.000456, loss: 0.0916
2022-10-03 13:36:38 - train: epoch 0040, iter [00480, 01251], lr: 0.000456, loss: 0.0910
2022-10-03 13:37:06 - train: epoch 0040, iter [00490, 01251], lr: 0.000455, loss: 0.0978
2022-10-03 13:37:34 - train: epoch 0040, iter [00500, 01251], lr: 0.000455, loss: 0.0975
2022-10-03 13:38:02 - train: epoch 0040, iter [00510, 01251], lr: 0.000455, loss: 0.0844
2022-10-03 13:38:31 - train: epoch 0040, iter [00520, 01251], lr: 0.000455, loss: 0.0913
2022-10-03 13:38:59 - train: epoch 0040, iter [00530, 01251], lr: 0.000455, loss: 0.1051
2022-10-03 13:39:27 - train: epoch 0040, iter [00540, 01251], lr: 0.000455, loss: 0.0980
2022-10-03 13:39:55 - train: epoch 0040, iter [00550, 01251], lr: 0.000455, loss: 0.0906
2022-10-03 13:40:23 - train: epoch 0040, iter [00560, 01251], lr: 0.000455, loss: 0.0918
2022-10-03 13:40:52 - train: epoch 0040, iter [00570, 01251], lr: 0.000455, loss: 0.0971
2022-10-03 13:41:20 - train: epoch 0040, iter [00580, 01251], lr: 0.000455, loss: 0.0888
2022-10-03 13:41:48 - train: epoch 0040, iter [00590, 01251], lr: 0.000455, loss: 0.0928
2022-10-03 13:42:16 - train: epoch 0040, iter [00600, 01251], lr: 0.000455, loss: 0.0938
2022-10-03 13:42:44 - train: epoch 0040, iter [00610, 01251], lr: 0.000455, loss: 0.0916
2022-10-03 13:43:12 - train: epoch 0040, iter [00620, 01251], lr: 0.000455, loss: 0.0944
2022-10-03 13:43:40 - train: epoch 0040, iter [00630, 01251], lr: 0.000454, loss: 0.0989
2022-10-03 13:44:08 - train: epoch 0040, iter [00640, 01251], lr: 0.000454, loss: 0.0858
2022-10-03 13:44:36 - train: epoch 0040, iter [00650, 01251], lr: 0.000454, loss: 0.0952
2022-10-03 13:45:04 - train: epoch 0040, iter [00660, 01251], lr: 0.000454, loss: 0.0893
2022-10-03 13:45:32 - train: epoch 0040, iter [00670, 01251], lr: 0.000454, loss: 0.0882
2022-10-03 13:46:00 - train: epoch 0040, iter [00680, 01251], lr: 0.000454, loss: 0.0952
2022-10-03 13:46:28 - train: epoch 0040, iter [00690, 01251], lr: 0.000454, loss: 0.0937
2022-10-03 13:46:56 - train: epoch 0040, iter [00700, 01251], lr: 0.000454, loss: 0.0924
2022-10-03 13:47:25 - train: epoch 0040, iter [00710, 01251], lr: 0.000454, loss: 0.0913
2022-10-03 13:47:53 - train: epoch 0040, iter [00720, 01251], lr: 0.000454, loss: 0.0873
2022-10-03 13:48:21 - train: epoch 0040, iter [00730, 01251], lr: 0.000454, loss: 0.0929
2022-10-03 13:48:49 - train: epoch 0040, iter [00740, 01251], lr: 0.000454, loss: 0.0809
2022-10-03 13:49:18 - train: epoch 0040, iter [00750, 01251], lr: 0.000454, loss: 0.0892
2022-10-03 13:49:46 - train: epoch 0040, iter [00760, 01251], lr: 0.000454, loss: 0.0921
2022-10-03 13:50:14 - train: epoch 0040, iter [00770, 01251], lr: 0.000453, loss: 0.0998
2022-10-03 13:50:42 - train: epoch 0040, iter [00780, 01251], lr: 0.000453, loss: 0.0830
2022-10-03 13:51:10 - train: epoch 0040, iter [00790, 01251], lr: 0.000453, loss: 0.1018
2022-10-03 13:51:38 - train: epoch 0040, iter [00800, 01251], lr: 0.000453, loss: 0.0972
2022-10-03 13:52:06 - train: epoch 0040, iter [00810, 01251], lr: 0.000453, loss: 0.1008
2022-10-03 13:52:35 - train: epoch 0040, iter [00820, 01251], lr: 0.000453, loss: 0.0875
2022-10-03 13:53:03 - train: epoch 0040, iter [00830, 01251], lr: 0.000453, loss: 0.0889
2022-10-03 13:53:31 - train: epoch 0040, iter [00840, 01251], lr: 0.000453, loss: 0.0976
2022-10-03 13:53:59 - train: epoch 0040, iter [00850, 01251], lr: 0.000453, loss: 0.1018
2022-10-03 13:54:27 - train: epoch 0040, iter [00860, 01251], lr: 0.000453, loss: 0.0945
2022-10-03 13:54:55 - train: epoch 0040, iter [00870, 01251], lr: 0.000453, loss: 0.0972
2022-10-03 13:55:23 - train: epoch 0040, iter [00880, 01251], lr: 0.000453, loss: 0.0857
2022-10-03 13:55:51 - train: epoch 0040, iter [00890, 01251], lr: 0.000453, loss: 0.0940
2022-10-03 13:56:19 - train: epoch 0040, iter [00900, 01251], lr: 0.000453, loss: 0.0910
2022-10-03 13:56:47 - train: epoch 0040, iter [00910, 01251], lr: 0.000452, loss: 0.0890
2022-10-03 13:57:15 - train: epoch 0040, iter [00920, 01251], lr: 0.000452, loss: 0.0962
2022-10-03 13:57:43 - train: epoch 0040, iter [00930, 01251], lr: 0.000452, loss: 0.0892
2022-10-03 13:58:11 - train: epoch 0040, iter [00940, 01251], lr: 0.000452, loss: 0.0928
2022-10-03 13:58:39 - train: epoch 0040, iter [00950, 01251], lr: 0.000452, loss: 0.0912
2022-10-03 13:59:07 - train: epoch 0040, iter [00960, 01251], lr: 0.000452, loss: 0.0925
2022-10-03 13:59:35 - train: epoch 0040, iter [00970, 01251], lr: 0.000452, loss: 0.0988
2022-10-03 14:00:03 - train: epoch 0040, iter [00980, 01251], lr: 0.000452, loss: 0.0935
2022-10-03 14:00:31 - train: epoch 0040, iter [00990, 01251], lr: 0.000452, loss: 0.0916
2022-10-03 14:01:00 - train: epoch 0040, iter [01000, 01251], lr: 0.000452, loss: 0.0895
2022-10-03 14:01:28 - train: epoch 0040, iter [01010, 01251], lr: 0.000452, loss: 0.0929
2022-10-03 14:01:56 - train: epoch 0040, iter [01020, 01251], lr: 0.000452, loss: 0.0921
2022-10-03 14:02:24 - train: epoch 0040, iter [01030, 01251], lr: 0.000452, loss: 0.1010
2022-10-03 14:02:53 - train: epoch 0040, iter [01040, 01251], lr: 0.000452, loss: 0.0973
2022-10-03 14:03:21 - train: epoch 0040, iter [01050, 01251], lr: 0.000451, loss: 0.1073
2022-10-03 14:03:49 - train: epoch 0040, iter [01060, 01251], lr: 0.000451, loss: 0.0938
2022-10-03 14:04:17 - train: epoch 0040, iter [01070, 01251], lr: 0.000451, loss: 0.0933
2022-10-03 14:04:46 - train: epoch 0040, iter [01080, 01251], lr: 0.000451, loss: 0.0933
2022-10-03 14:05:14 - train: epoch 0040, iter [01090, 01251], lr: 0.000451, loss: 0.0945
2022-10-03 14:05:42 - train: epoch 0040, iter [01100, 01251], lr: 0.000451, loss: 0.0907
2022-10-03 14:06:10 - train: epoch 0040, iter [01110, 01251], lr: 0.000451, loss: 0.0866
2022-10-03 14:06:38 - train: epoch 0040, iter [01120, 01251], lr: 0.000451, loss: 0.0951
2022-10-03 14:07:06 - train: epoch 0040, iter [01130, 01251], lr: 0.000451, loss: 0.0876
2022-10-03 14:07:34 - train: epoch 0040, iter [01140, 01251], lr: 0.000451, loss: 0.1022
2022-10-03 14:08:02 - train: epoch 0040, iter [01150, 01251], lr: 0.000451, loss: 0.0916
2022-10-03 14:08:31 - train: epoch 0040, iter [01160, 01251], lr: 0.000451, loss: 0.0993
2022-10-03 14:08:59 - train: epoch 0040, iter [01170, 01251], lr: 0.000451, loss: 0.0936
2022-10-03 14:09:27 - train: epoch 0040, iter [01180, 01251], lr: 0.000451, loss: 0.0953
2022-10-03 14:09:55 - train: epoch 0040, iter [01190, 01251], lr: 0.000450, loss: 0.0911
2022-10-03 14:10:23 - train: epoch 0040, iter [01200, 01251], lr: 0.000450, loss: 0.0880
2022-10-03 14:10:51 - train: epoch 0040, iter [01210, 01251], lr: 0.000450, loss: 0.0988
2022-10-03 14:11:19 - train: epoch 0040, iter [01220, 01251], lr: 0.000450, loss: 0.0987
2022-10-03 14:11:47 - train: epoch 0040, iter [01230, 01251], lr: 0.000450, loss: 0.0823
2022-10-03 14:12:15 - train: epoch 0040, iter [01240, 01251], lr: 0.000450, loss: 0.0975
2022-10-03 14:12:43 - train: epoch 0040, iter [01250, 01251], lr: 0.000450, loss: 0.0981
2022-10-03 14:12:47 - train: epoch 040, train_loss: 0.0933
2022-10-03 14:12:49 - until epoch: 040, best_loss: 0.0933
2022-10-03 14:12:49 - epoch 041 lr: 0.000450
2022-10-03 14:13:24 - train: epoch 0041, iter [00010, 01251], lr: 0.000450, loss: 0.0914
2022-10-03 14:13:52 - train: epoch 0041, iter [00020, 01251], lr: 0.000450, loss: 0.0937
2022-10-03 14:14:20 - train: epoch 0041, iter [00030, 01251], lr: 0.000450, loss: 0.0995
2022-10-03 14:14:49 - train: epoch 0041, iter [00040, 01251], lr: 0.000450, loss: 0.0981
2022-10-03 14:15:17 - train: epoch 0041, iter [00050, 01251], lr: 0.000450, loss: 0.0918
2022-10-03 14:15:45 - train: epoch 0041, iter [00060, 01251], lr: 0.000450, loss: 0.0917
2022-10-03 14:16:13 - train: epoch 0041, iter [00070, 01251], lr: 0.000449, loss: 0.0935
2022-10-03 14:16:41 - train: epoch 0041, iter [00080, 01251], lr: 0.000449, loss: 0.0994
2022-10-03 14:17:09 - train: epoch 0041, iter [00090, 01251], lr: 0.000449, loss: 0.0896
2022-10-03 14:17:38 - train: epoch 0041, iter [00100, 01251], lr: 0.000449, loss: 0.0885
2022-10-03 14:18:06 - train: epoch 0041, iter [00110, 01251], lr: 0.000449, loss: 0.0888
2022-10-03 14:18:34 - train: epoch 0041, iter [00120, 01251], lr: 0.000449, loss: 0.0979
2022-10-03 14:19:02 - train: epoch 0041, iter [00130, 01251], lr: 0.000449, loss: 0.0887
2022-10-03 14:19:30 - train: epoch 0041, iter [00140, 01251], lr: 0.000449, loss: 0.0919
2022-10-03 14:19:59 - train: epoch 0041, iter [00150, 01251], lr: 0.000449, loss: 0.0947
2022-10-03 14:20:27 - train: epoch 0041, iter [00160, 01251], lr: 0.000449, loss: 0.0923
2022-10-03 14:20:55 - train: epoch 0041, iter [00170, 01251], lr: 0.000449, loss: 0.0962
2022-10-03 14:21:23 - train: epoch 0041, iter [00180, 01251], lr: 0.000449, loss: 0.0935
2022-10-03 14:21:51 - train: epoch 0041, iter [00190, 01251], lr: 0.000449, loss: 0.0927
2022-10-03 14:22:20 - train: epoch 0041, iter [00200, 01251], lr: 0.000449, loss: 0.0894
2022-10-03 14:22:48 - train: epoch 0041, iter [00210, 01251], lr: 0.000448, loss: 0.1027
2022-10-03 14:23:16 - train: epoch 0041, iter [00220, 01251], lr: 0.000448, loss: 0.1002
2022-10-03 14:23:44 - train: epoch 0041, iter [00230, 01251], lr: 0.000448, loss: 0.0903
2022-10-03 14:24:12 - train: epoch 0041, iter [00240, 01251], lr: 0.000448, loss: 0.0937
2022-10-03 14:24:40 - train: epoch 0041, iter [00250, 01251], lr: 0.000448, loss: 0.0920
2022-10-03 14:25:08 - train: epoch 0041, iter [00260, 01251], lr: 0.000448, loss: 0.0940
2022-10-03 14:25:36 - train: epoch 0041, iter [00270, 01251], lr: 0.000448, loss: 0.0972
2022-10-03 14:26:04 - train: epoch 0041, iter [00280, 01251], lr: 0.000448, loss: 0.0882
2022-10-03 14:26:33 - train: epoch 0041, iter [00290, 01251], lr: 0.000448, loss: 0.1005
2022-10-03 14:27:01 - train: epoch 0041, iter [00300, 01251], lr: 0.000448, loss: 0.0977
2022-10-03 14:27:29 - train: epoch 0041, iter [00310, 01251], lr: 0.000448, loss: 0.0954
2022-10-03 14:27:57 - train: epoch 0041, iter [00320, 01251], lr: 0.000448, loss: 0.0912
2022-10-03 14:28:25 - train: epoch 0041, iter [00330, 01251], lr: 0.000448, loss: 0.0952
2022-10-03 14:28:53 - train: epoch 0041, iter [00340, 01251], lr: 0.000448, loss: 0.0917
2022-10-03 14:29:21 - train: epoch 0041, iter [00350, 01251], lr: 0.000447, loss: 0.0882
2022-10-03 14:29:49 - train: epoch 0041, iter [00360, 01251], lr: 0.000447, loss: 0.0892
2022-10-03 14:30:17 - train: epoch 0041, iter [00370, 01251], lr: 0.000447, loss: 0.0971
2022-10-03 14:30:45 - train: epoch 0041, iter [00380, 01251], lr: 0.000447, loss: 0.0884
2022-10-03 14:31:13 - train: epoch 0041, iter [00390, 01251], lr: 0.000447, loss: 0.0905
2022-10-03 14:31:41 - train: epoch 0041, iter [00400, 01251], lr: 0.000447, loss: 0.0871
2022-10-03 14:32:09 - train: epoch 0041, iter [00410, 01251], lr: 0.000447, loss: 0.0872
2022-10-03 14:32:37 - train: epoch 0041, iter [00420, 01251], lr: 0.000447, loss: 0.0906
2022-10-03 14:33:05 - train: epoch 0041, iter [00430, 01251], lr: 0.000447, loss: 0.0910
2022-10-03 14:33:33 - train: epoch 0041, iter [00440, 01251], lr: 0.000447, loss: 0.0904
2022-10-03 14:34:02 - train: epoch 0041, iter [00450, 01251], lr: 0.000447, loss: 0.0917
2022-10-03 14:34:30 - train: epoch 0041, iter [00460, 01251], lr: 0.000447, loss: 0.0880
2022-10-03 14:34:58 - train: epoch 0041, iter [00470, 01251], lr: 0.000447, loss: 0.0971
2022-10-03 14:35:26 - train: epoch 0041, iter [00480, 01251], lr: 0.000447, loss: 0.0950
2022-10-03 14:35:54 - train: epoch 0041, iter [00490, 01251], lr: 0.000446, loss: 0.0811
2022-10-03 14:36:22 - train: epoch 0041, iter [00500, 01251], lr: 0.000446, loss: 0.1000
2022-10-03 14:36:51 - train: epoch 0041, iter [00510, 01251], lr: 0.000446, loss: 0.0952
2022-10-03 14:37:19 - train: epoch 0041, iter [00520, 01251], lr: 0.000446, loss: 0.0898
2022-10-03 14:37:47 - train: epoch 0041, iter [00530, 01251], lr: 0.000446, loss: 0.0959
2022-10-03 14:38:15 - train: epoch 0041, iter [00540, 01251], lr: 0.000446, loss: 0.0980
2022-10-03 14:38:43 - train: epoch 0041, iter [00550, 01251], lr: 0.000446, loss: 0.0931
2022-10-03 14:39:11 - train: epoch 0041, iter [00560, 01251], lr: 0.000446, loss: 0.0951
2022-10-03 14:39:39 - train: epoch 0041, iter [00570, 01251], lr: 0.000446, loss: 0.0961
2022-10-03 14:40:07 - train: epoch 0041, iter [00580, 01251], lr: 0.000446, loss: 0.0974
2022-10-03 14:40:35 - train: epoch 0041, iter [00590, 01251], lr: 0.000446, loss: 0.0962
2022-10-03 14:41:03 - train: epoch 0041, iter [00600, 01251], lr: 0.000446, loss: 0.0936
2022-10-03 14:41:31 - train: epoch 0041, iter [00610, 01251], lr: 0.000446, loss: 0.0873
2022-10-03 14:41:59 - train: epoch 0041, iter [00620, 01251], lr: 0.000445, loss: 0.0929
2022-10-03 14:42:27 - train: epoch 0041, iter [00630, 01251], lr: 0.000445, loss: 0.0887
2022-10-03 14:42:55 - train: epoch 0041, iter [00640, 01251], lr: 0.000445, loss: 0.0916
2022-10-03 14:43:23 - train: epoch 0041, iter [00650, 01251], lr: 0.000445, loss: 0.0905
2022-10-03 14:43:51 - train: epoch 0041, iter [00660, 01251], lr: 0.000445, loss: 0.0890
2022-10-03 14:44:19 - train: epoch 0041, iter [00670, 01251], lr: 0.000445, loss: 0.0936
2022-10-03 14:44:48 - train: epoch 0041, iter [00680, 01251], lr: 0.000445, loss: 0.0863
2022-10-03 14:45:16 - train: epoch 0041, iter [00690, 01251], lr: 0.000445, loss: 0.0940
2022-10-03 14:45:44 - train: epoch 0041, iter [00700, 01251], lr: 0.000445, loss: 0.0846
2022-10-03 14:46:12 - train: epoch 0041, iter [00710, 01251], lr: 0.000445, loss: 0.0981
2022-10-03 14:46:40 - train: epoch 0041, iter [00720, 01251], lr: 0.000445, loss: 0.0863
2022-10-03 14:47:08 - train: epoch 0041, iter [00730, 01251], lr: 0.000445, loss: 0.0926
2022-10-03 14:47:36 - train: epoch 0041, iter [00740, 01251], lr: 0.000445, loss: 0.0903
2022-10-03 14:48:04 - train: epoch 0041, iter [00750, 01251], lr: 0.000445, loss: 0.0938
2022-10-03 14:48:32 - train: epoch 0041, iter [00760, 01251], lr: 0.000444, loss: 0.0913
2022-10-03 14:49:00 - train: epoch 0041, iter [00770, 01251], lr: 0.000444, loss: 0.0899
2022-10-03 14:49:28 - train: epoch 0041, iter [00780, 01251], lr: 0.000444, loss: 0.0966
2022-10-03 14:49:57 - train: epoch 0041, iter [00790, 01251], lr: 0.000444, loss: 0.0860
2022-10-03 14:50:25 - train: epoch 0041, iter [00800, 01251], lr: 0.000444, loss: 0.0880
2022-10-03 14:50:53 - train: epoch 0041, iter [00810, 01251], lr: 0.000444, loss: 0.0953
2022-10-03 14:51:21 - train: epoch 0041, iter [00820, 01251], lr: 0.000444, loss: 0.0899
2022-10-03 14:51:49 - train: epoch 0041, iter [00830, 01251], lr: 0.000444, loss: 0.0945
2022-10-03 14:52:17 - train: epoch 0041, iter [00840, 01251], lr: 0.000444, loss: 0.1017
2022-10-03 14:52:45 - train: epoch 0041, iter [00850, 01251], lr: 0.000444, loss: 0.0929
2022-10-03 14:53:13 - train: epoch 0041, iter [00860, 01251], lr: 0.000444, loss: 0.0969
2022-10-03 14:53:41 - train: epoch 0041, iter [00870, 01251], lr: 0.000444, loss: 0.0921
2022-10-03 14:54:09 - train: epoch 0041, iter [00880, 01251], lr: 0.000444, loss: 0.0989
2022-10-03 14:54:37 - train: epoch 0041, iter [00890, 01251], lr: 0.000444, loss: 0.0960
2022-10-03 14:55:05 - train: epoch 0041, iter [00900, 01251], lr: 0.000443, loss: 0.0927
2022-10-03 14:55:33 - train: epoch 0041, iter [00910, 01251], lr: 0.000443, loss: 0.0917
2022-10-03 14:56:01 - train: epoch 0041, iter [00920, 01251], lr: 0.000443, loss: 0.0949
2022-10-03 14:56:29 - train: epoch 0041, iter [00930, 01251], lr: 0.000443, loss: 0.0898
2022-10-03 14:56:57 - train: epoch 0041, iter [00940, 01251], lr: 0.000443, loss: 0.0918
2022-10-03 14:57:25 - train: epoch 0041, iter [00950, 01251], lr: 0.000443, loss: 0.0914
2022-10-03 14:57:53 - train: epoch 0041, iter [00960, 01251], lr: 0.000443, loss: 0.0929
2022-10-03 14:58:21 - train: epoch 0041, iter [00970, 01251], lr: 0.000443, loss: 0.0966
2022-10-03 14:58:49 - train: epoch 0041, iter [00980, 01251], lr: 0.000443, loss: 0.0980
2022-10-03 14:59:17 - train: epoch 0041, iter [00990, 01251], lr: 0.000443, loss: 0.0942
2022-10-03 14:59:45 - train: epoch 0041, iter [01000, 01251], lr: 0.000443, loss: 0.0887
2022-10-03 15:00:13 - train: epoch 0041, iter [01010, 01251], lr: 0.000443, loss: 0.0952
2022-10-03 15:00:41 - train: epoch 0041, iter [01020, 01251], lr: 0.000443, loss: 0.0972
2022-10-03 15:01:09 - train: epoch 0041, iter [01030, 01251], lr: 0.000442, loss: 0.0934
2022-10-03 15:01:37 - train: epoch 0041, iter [01040, 01251], lr: 0.000442, loss: 0.0940
2022-10-03 15:02:05 - train: epoch 0041, iter [01050, 01251], lr: 0.000442, loss: 0.0819
2022-10-03 15:02:33 - train: epoch 0041, iter [01060, 01251], lr: 0.000442, loss: 0.0892
2022-10-03 15:03:01 - train: epoch 0041, iter [01070, 01251], lr: 0.000442, loss: 0.0894
2022-10-03 15:03:29 - train: epoch 0041, iter [01080, 01251], lr: 0.000442, loss: 0.0938
2022-10-03 15:03:57 - train: epoch 0041, iter [01090, 01251], lr: 0.000442, loss: 0.1004
2022-10-03 15:04:25 - train: epoch 0041, iter [01100, 01251], lr: 0.000442, loss: 0.0861
2022-10-03 15:04:53 - train: epoch 0041, iter [01110, 01251], lr: 0.000442, loss: 0.0892
2022-10-03 15:05:21 - train: epoch 0041, iter [01120, 01251], lr: 0.000442, loss: 0.0989
2022-10-03 15:05:49 - train: epoch 0041, iter [01130, 01251], lr: 0.000442, loss: 0.0920
2022-10-03 15:06:17 - train: epoch 0041, iter [01140, 01251], lr: 0.000442, loss: 0.0936
2022-10-03 15:06:45 - train: epoch 0041, iter [01150, 01251], lr: 0.000442, loss: 0.0894
2022-10-03 15:07:13 - train: epoch 0041, iter [01160, 01251], lr: 0.000442, loss: 0.0920
2022-10-03 15:07:41 - train: epoch 0041, iter [01170, 01251], lr: 0.000441, loss: 0.0856
2022-10-03 15:08:09 - train: epoch 0041, iter [01180, 01251], lr: 0.000441, loss: 0.1014
2022-10-03 15:08:37 - train: epoch 0041, iter [01190, 01251], lr: 0.000441, loss: 0.0886
2022-10-03 15:09:05 - train: epoch 0041, iter [01200, 01251], lr: 0.000441, loss: 0.0950
2022-10-03 15:09:33 - train: epoch 0041, iter [01210, 01251], lr: 0.000441, loss: 0.0972
2022-10-03 15:10:01 - train: epoch 0041, iter [01220, 01251], lr: 0.000441, loss: 0.0907
2022-10-03 15:10:29 - train: epoch 0041, iter [01230, 01251], lr: 0.000441, loss: 0.0904
2022-10-03 15:10:57 - train: epoch 0041, iter [01240, 01251], lr: 0.000441, loss: 0.0849
2022-10-03 15:11:25 - train: epoch 0041, iter [01250, 01251], lr: 0.000441, loss: 0.0971
2022-10-03 15:11:29 - train: epoch 041, train_loss: 0.0928
2022-10-03 15:11:31 - until epoch: 041, best_loss: 0.0928
2022-10-03 15:11:31 - epoch 042 lr: 0.000441
2022-10-03 15:12:07 - train: epoch 0042, iter [00010, 01251], lr: 0.000441, loss: 0.0915
2022-10-03 15:12:35 - train: epoch 0042, iter [00020, 01251], lr: 0.000441, loss: 0.0874
2022-10-03 15:13:03 - train: epoch 0042, iter [00030, 01251], lr: 0.000441, loss: 0.0903
2022-10-03 15:13:31 - train: epoch 0042, iter [00040, 01251], lr: 0.000441, loss: 0.0859
2022-10-03 15:13:59 - train: epoch 0042, iter [00050, 01251], lr: 0.000440, loss: 0.0941
2022-10-03 15:14:27 - train: epoch 0042, iter [00060, 01251], lr: 0.000440, loss: 0.0936
2022-10-03 15:14:55 - train: epoch 0042, iter [00070, 01251], lr: 0.000440, loss: 0.0836
2022-10-03 15:15:23 - train: epoch 0042, iter [00080, 01251], lr: 0.000440, loss: 0.0856
2022-10-03 15:15:51 - train: epoch 0042, iter [00090, 01251], lr: 0.000440, loss: 0.0855
2022-10-03 15:16:19 - train: epoch 0042, iter [00100, 01251], lr: 0.000440, loss: 0.0852
2022-10-03 15:16:48 - train: epoch 0042, iter [00110, 01251], lr: 0.000440, loss: 0.0856
2022-10-03 15:17:16 - train: epoch 0042, iter [00120, 01251], lr: 0.000440, loss: 0.0979
2022-10-03 15:17:44 - train: epoch 0042, iter [00130, 01251], lr: 0.000440, loss: 0.0858
2022-10-03 15:18:12 - train: epoch 0042, iter [00140, 01251], lr: 0.000440, loss: 0.0908
2022-10-03 15:18:40 - train: epoch 0042, iter [00150, 01251], lr: 0.000440, loss: 0.0868
2022-10-03 15:19:08 - train: epoch 0042, iter [00160, 01251], lr: 0.000440, loss: 0.0915
2022-10-03 15:19:36 - train: epoch 0042, iter [00170, 01251], lr: 0.000440, loss: 0.0889
2022-10-03 15:20:04 - train: epoch 0042, iter [00180, 01251], lr: 0.000440, loss: 0.0962
2022-10-03 15:20:32 - train: epoch 0042, iter [00190, 01251], lr: 0.000439, loss: 0.0918
2022-10-03 15:21:00 - train: epoch 0042, iter [00200, 01251], lr: 0.000439, loss: 0.0877
2022-10-03 15:21:28 - train: epoch 0042, iter [00210, 01251], lr: 0.000439, loss: 0.0945
2022-10-03 15:21:56 - train: epoch 0042, iter [00220, 01251], lr: 0.000439, loss: 0.0972
2022-10-03 15:22:25 - train: epoch 0042, iter [00230, 01251], lr: 0.000439, loss: 0.0924
2022-10-03 15:22:53 - train: epoch 0042, iter [00240, 01251], lr: 0.000439, loss: 0.0859
2022-10-03 15:23:21 - train: epoch 0042, iter [00250, 01251], lr: 0.000439, loss: 0.0902
2022-10-03 15:23:49 - train: epoch 0042, iter [00260, 01251], lr: 0.000439, loss: 0.0892
2022-10-03 15:24:16 - train: epoch 0042, iter [00270, 01251], lr: 0.000439, loss: 0.0936
2022-10-03 15:24:45 - train: epoch 0042, iter [00280, 01251], lr: 0.000439, loss: 0.0975
2022-10-03 15:25:13 - train: epoch 0042, iter [00290, 01251], lr: 0.000439, loss: 0.0961
2022-10-03 15:25:41 - train: epoch 0042, iter [00300, 01251], lr: 0.000439, loss: 0.0907
2022-10-03 15:26:09 - train: epoch 0042, iter [00310, 01251], lr: 0.000439, loss: 0.0848
2022-10-03 15:26:37 - train: epoch 0042, iter [00320, 01251], lr: 0.000438, loss: 0.0878
2022-10-03 15:27:06 - train: epoch 0042, iter [00330, 01251], lr: 0.000438, loss: 0.1018
2022-10-03 15:27:34 - train: epoch 0042, iter [00340, 01251], lr: 0.000438, loss: 0.0920
2022-10-03 15:28:02 - train: epoch 0042, iter [00350, 01251], lr: 0.000438, loss: 0.0938
2022-10-03 15:28:29 - train: epoch 0042, iter [00360, 01251], lr: 0.000438, loss: 0.0962
2022-10-03 15:28:57 - train: epoch 0042, iter [00370, 01251], lr: 0.000438, loss: 0.0912
2022-10-03 15:29:25 - train: epoch 0042, iter [00380, 01251], lr: 0.000438, loss: 0.0902
2022-10-03 15:29:53 - train: epoch 0042, iter [00390, 01251], lr: 0.000438, loss: 0.0920
2022-10-03 15:30:21 - train: epoch 0042, iter [00400, 01251], lr: 0.000438, loss: 0.0989
2022-10-03 15:30:50 - train: epoch 0042, iter [00410, 01251], lr: 0.000438, loss: 0.0915
2022-10-03 15:31:18 - train: epoch 0042, iter [00420, 01251], lr: 0.000438, loss: 0.0922
2022-10-03 15:31:46 - train: epoch 0042, iter [00430, 01251], lr: 0.000438, loss: 0.0970
2022-10-03 15:32:14 - train: epoch 0042, iter [00440, 01251], lr: 0.000438, loss: 0.0956
2022-10-03 15:32:42 - train: epoch 0042, iter [00450, 01251], lr: 0.000438, loss: 0.0900
2022-10-03 15:33:10 - train: epoch 0042, iter [00460, 01251], lr: 0.000437, loss: 0.0887
2022-10-03 15:33:38 - train: epoch 0042, iter [00470, 01251], lr: 0.000437, loss: 0.0904
2022-10-03 15:34:06 - train: epoch 0042, iter [00480, 01251], lr: 0.000437, loss: 0.0928
2022-10-03 15:34:34 - train: epoch 0042, iter [00490, 01251], lr: 0.000437, loss: 0.0880
2022-10-03 15:35:02 - train: epoch 0042, iter [00500, 01251], lr: 0.000437, loss: 0.0871
2022-10-03 15:35:30 - train: epoch 0042, iter [00510, 01251], lr: 0.000437, loss: 0.0898
2022-10-03 15:35:58 - train: epoch 0042, iter [00520, 01251], lr: 0.000437, loss: 0.0898
2022-10-03 15:36:26 - train: epoch 0042, iter [00530, 01251], lr: 0.000437, loss: 0.0892
2022-10-03 15:36:54 - train: epoch 0042, iter [00540, 01251], lr: 0.000437, loss: 0.0908
2022-10-03 15:37:22 - train: epoch 0042, iter [00550, 01251], lr: 0.000437, loss: 0.0916
2022-10-03 15:37:50 - train: epoch 0042, iter [00560, 01251], lr: 0.000437, loss: 0.0955
2022-10-03 15:38:18 - train: epoch 0042, iter [00570, 01251], lr: 0.000437, loss: 0.0976
2022-10-03 15:38:46 - train: epoch 0042, iter [00580, 01251], lr: 0.000437, loss: 0.0910
2022-10-03 15:39:14 - train: epoch 0042, iter [00590, 01251], lr: 0.000436, loss: 0.0917
2022-10-03 15:39:42 - train: epoch 0042, iter [00600, 01251], lr: 0.000436, loss: 0.0936
2022-10-03 15:40:11 - train: epoch 0042, iter [00610, 01251], lr: 0.000436, loss: 0.0882
2022-10-03 15:40:39 - train: epoch 0042, iter [00620, 01251], lr: 0.000436, loss: 0.0937
2022-10-03 15:41:07 - train: epoch 0042, iter [00630, 01251], lr: 0.000436, loss: 0.0902
2022-10-03 15:41:35 - train: epoch 0042, iter [00640, 01251], lr: 0.000436, loss: 0.0900
2022-10-03 15:42:03 - train: epoch 0042, iter [00650, 01251], lr: 0.000436, loss: 0.0898
2022-10-03 15:42:31 - train: epoch 0042, iter [00660, 01251], lr: 0.000436, loss: 0.0961
2022-10-03 15:42:59 - train: epoch 0042, iter [00670, 01251], lr: 0.000436, loss: 0.0894
2022-10-03 15:43:27 - train: epoch 0042, iter [00680, 01251], lr: 0.000436, loss: 0.0952
2022-10-03 15:43:56 - train: epoch 0042, iter [00690, 01251], lr: 0.000436, loss: 0.0969
2022-10-03 15:44:23 - train: epoch 0042, iter [00700, 01251], lr: 0.000436, loss: 0.1019
2022-10-03 15:44:52 - train: epoch 0042, iter [00710, 01251], lr: 0.000436, loss: 0.0904
2022-10-03 15:45:20 - train: epoch 0042, iter [00720, 01251], lr: 0.000435, loss: 0.0905
2022-10-03 15:45:48 - train: epoch 0042, iter [00730, 01251], lr: 0.000435, loss: 0.0916
2022-10-03 15:46:16 - train: epoch 0042, iter [00740, 01251], lr: 0.000435, loss: 0.0896
2022-10-03 15:46:44 - train: epoch 0042, iter [00750, 01251], lr: 0.000435, loss: 0.0941
2022-10-03 15:47:12 - train: epoch 0042, iter [00760, 01251], lr: 0.000435, loss: 0.0937
2022-10-03 15:47:40 - train: epoch 0042, iter [00770, 01251], lr: 0.000435, loss: 0.0955
2022-10-03 15:48:09 - train: epoch 0042, iter [00780, 01251], lr: 0.000435, loss: 0.0983
2022-10-03 15:48:37 - train: epoch 0042, iter [00790, 01251], lr: 0.000435, loss: 0.0956
2022-10-03 15:49:05 - train: epoch 0042, iter [00800, 01251], lr: 0.000435, loss: 0.0947
2022-10-03 15:49:33 - train: epoch 0042, iter [00810, 01251], lr: 0.000435, loss: 0.0889
2022-10-03 15:50:01 - train: epoch 0042, iter [00820, 01251], lr: 0.000435, loss: 0.0938
2022-10-03 15:50:30 - train: epoch 0042, iter [00830, 01251], lr: 0.000435, loss: 0.0950
2022-10-03 15:50:58 - train: epoch 0042, iter [00840, 01251], lr: 0.000435, loss: 0.0865
2022-10-03 15:51:26 - train: epoch 0042, iter [00850, 01251], lr: 0.000435, loss: 0.0971
2022-10-03 15:51:54 - train: epoch 0042, iter [00860, 01251], lr: 0.000434, loss: 0.0908
2022-10-03 15:52:22 - train: epoch 0042, iter [00870, 01251], lr: 0.000434, loss: 0.0975
2022-10-03 15:52:50 - train: epoch 0042, iter [00880, 01251], lr: 0.000434, loss: 0.0901
2022-10-03 15:53:18 - train: epoch 0042, iter [00890, 01251], lr: 0.000434, loss: 0.0924
2022-10-03 15:53:46 - train: epoch 0042, iter [00900, 01251], lr: 0.000434, loss: 0.0985
2022-10-03 15:54:14 - train: epoch 0042, iter [00910, 01251], lr: 0.000434, loss: 0.0903
2022-10-03 15:54:42 - train: epoch 0042, iter [00920, 01251], lr: 0.000434, loss: 0.0994
2022-10-03 15:55:10 - train: epoch 0042, iter [00930, 01251], lr: 0.000434, loss: 0.0955
2022-10-03 15:55:38 - train: epoch 0042, iter [00940, 01251], lr: 0.000434, loss: 0.0946
2022-10-03 15:56:06 - train: epoch 0042, iter [00950, 01251], lr: 0.000434, loss: 0.0904
2022-10-03 15:56:34 - train: epoch 0042, iter [00960, 01251], lr: 0.000434, loss: 0.0922
2022-10-03 15:57:03 - train: epoch 0042, iter [00970, 01251], lr: 0.000434, loss: 0.0959
2022-10-03 15:57:31 - train: epoch 0042, iter [00980, 01251], lr: 0.000434, loss: 0.0905
2022-10-03 15:57:59 - train: epoch 0042, iter [00990, 01251], lr: 0.000433, loss: 0.0875
2022-10-03 15:58:27 - train: epoch 0042, iter [01000, 01251], lr: 0.000433, loss: 0.0907
2022-10-03 15:58:55 - train: epoch 0042, iter [01010, 01251], lr: 0.000433, loss: 0.0922
2022-10-03 15:59:23 - train: epoch 0042, iter [01020, 01251], lr: 0.000433, loss: 0.0861
2022-10-03 15:59:51 - train: epoch 0042, iter [01030, 01251], lr: 0.000433, loss: 0.0917
2022-10-03 16:00:20 - train: epoch 0042, iter [01040, 01251], lr: 0.000433, loss: 0.0921
2022-10-03 16:00:48 - train: epoch 0042, iter [01050, 01251], lr: 0.000433, loss: 0.0962
2022-10-03 16:01:16 - train: epoch 0042, iter [01060, 01251], lr: 0.000433, loss: 0.0952
2022-10-03 16:01:44 - train: epoch 0042, iter [01070, 01251], lr: 0.000433, loss: 0.0999
2022-10-03 16:02:12 - train: epoch 0042, iter [01080, 01251], lr: 0.000433, loss: 0.0954
2022-10-03 16:02:40 - train: epoch 0042, iter [01090, 01251], lr: 0.000433, loss: 0.0882
2022-10-03 16:03:08 - train: epoch 0042, iter [01100, 01251], lr: 0.000433, loss: 0.0897
2022-10-03 16:03:36 - train: epoch 0042, iter [01110, 01251], lr: 0.000433, loss: 0.0994
2022-10-03 16:04:04 - train: epoch 0042, iter [01120, 01251], lr: 0.000432, loss: 0.0887
2022-10-03 16:04:32 - train: epoch 0042, iter [01130, 01251], lr: 0.000432, loss: 0.0867
2022-10-03 16:05:00 - train: epoch 0042, iter [01140, 01251], lr: 0.000432, loss: 0.0864
2022-10-03 16:05:28 - train: epoch 0042, iter [01150, 01251], lr: 0.000432, loss: 0.0862
2022-10-03 16:05:56 - train: epoch 0042, iter [01160, 01251], lr: 0.000432, loss: 0.0929
2022-10-03 16:06:24 - train: epoch 0042, iter [01170, 01251], lr: 0.000432, loss: 0.0931
2022-10-03 16:06:53 - train: epoch 0042, iter [01180, 01251], lr: 0.000432, loss: 0.0858
2022-10-03 16:07:21 - train: epoch 0042, iter [01190, 01251], lr: 0.000432, loss: 0.0890
2022-10-03 16:07:49 - train: epoch 0042, iter [01200, 01251], lr: 0.000432, loss: 0.0930
2022-10-03 16:08:17 - train: epoch 0042, iter [01210, 01251], lr: 0.000432, loss: 0.0897
2022-10-03 16:08:45 - train: epoch 0042, iter [01220, 01251], lr: 0.000432, loss: 0.0988
2022-10-03 16:09:13 - train: epoch 0042, iter [01230, 01251], lr: 0.000432, loss: 0.1021
2022-10-03 16:09:41 - train: epoch 0042, iter [01240, 01251], lr: 0.000432, loss: 0.0946
2022-10-03 16:10:09 - train: epoch 0042, iter [01250, 01251], lr: 0.000432, loss: 0.0874
2022-10-03 16:10:14 - train: epoch 042, train_loss: 0.0925
2022-10-03 16:10:15 - until epoch: 042, best_loss: 0.0925
2022-10-03 16:10:15 - epoch 043 lr: 0.000432
2022-10-03 16:10:51 - train: epoch 0043, iter [00010, 01251], lr: 0.000431, loss: 0.0923
2022-10-03 16:11:20 - train: epoch 0043, iter [00020, 01251], lr: 0.000431, loss: 0.0830
2022-10-03 16:11:48 - train: epoch 0043, iter [00030, 01251], lr: 0.000431, loss: 0.0913
2022-10-03 16:12:16 - train: epoch 0043, iter [00040, 01251], lr: 0.000431, loss: 0.0942
2022-10-03 16:12:44 - train: epoch 0043, iter [00050, 01251], lr: 0.000431, loss: 0.0983
2022-10-03 16:13:12 - train: epoch 0043, iter [00060, 01251], lr: 0.000431, loss: 0.0903
2022-10-03 16:13:41 - train: epoch 0043, iter [00070, 01251], lr: 0.000431, loss: 0.1008
2022-10-03 16:14:09 - train: epoch 0043, iter [00080, 01251], lr: 0.000431, loss: 0.0873
2022-10-03 16:14:37 - train: epoch 0043, iter [00090, 01251], lr: 0.000431, loss: 0.0859
2022-10-03 16:15:06 - train: epoch 0043, iter [00100, 01251], lr: 0.000431, loss: 0.0970
2022-10-03 16:15:34 - train: epoch 0043, iter [00110, 01251], lr: 0.000431, loss: 0.0990
2022-10-03 16:16:02 - train: epoch 0043, iter [00120, 01251], lr: 0.000431, loss: 0.0913
2022-10-03 16:16:30 - train: epoch 0043, iter [00130, 01251], lr: 0.000431, loss: 0.0957
2022-10-03 16:16:59 - train: epoch 0043, iter [00140, 01251], lr: 0.000430, loss: 0.0953
2022-10-03 16:17:27 - train: epoch 0043, iter [00150, 01251], lr: 0.000430, loss: 0.1023
2022-10-03 16:17:55 - train: epoch 0043, iter [00160, 01251], lr: 0.000430, loss: 0.0891
2022-10-03 16:18:23 - train: epoch 0043, iter [00170, 01251], lr: 0.000430, loss: 0.0887
2022-10-03 16:18:51 - train: epoch 0043, iter [00180, 01251], lr: 0.000430, loss: 0.0929
2022-10-03 16:19:20 - train: epoch 0043, iter [00190, 01251], lr: 0.000430, loss: 0.0869
2022-10-03 16:19:48 - train: epoch 0043, iter [00200, 01251], lr: 0.000430, loss: 0.0846
2022-10-03 16:20:16 - train: epoch 0043, iter [00210, 01251], lr: 0.000430, loss: 0.0972
2022-10-03 16:20:44 - train: epoch 0043, iter [00220, 01251], lr: 0.000430, loss: 0.0953
2022-10-03 16:21:13 - train: epoch 0043, iter [00230, 01251], lr: 0.000430, loss: 0.0906
2022-10-03 16:21:41 - train: epoch 0043, iter [00240, 01251], lr: 0.000430, loss: 0.1006
2022-10-03 16:22:09 - train: epoch 0043, iter [00250, 01251], lr: 0.000430, loss: 0.0902
2022-10-03 16:22:37 - train: epoch 0043, iter [00260, 01251], lr: 0.000430, loss: 0.0897
2022-10-03 16:23:05 - train: epoch 0043, iter [00270, 01251], lr: 0.000429, loss: 0.0986
2022-10-03 16:23:34 - train: epoch 0043, iter [00280, 01251], lr: 0.000429, loss: 0.0911
2022-10-03 16:24:02 - train: epoch 0043, iter [00290, 01251], lr: 0.000429, loss: 0.1021
2022-10-03 16:24:30 - train: epoch 0043, iter [00300, 01251], lr: 0.000429, loss: 0.0847
2022-10-03 16:24:58 - train: epoch 0043, iter [00310, 01251], lr: 0.000429, loss: 0.0940
2022-10-03 16:25:26 - train: epoch 0043, iter [00320, 01251], lr: 0.000429, loss: 0.0923
2022-10-03 16:25:54 - train: epoch 0043, iter [00330, 01251], lr: 0.000429, loss: 0.0886
2022-10-03 16:26:23 - train: epoch 0043, iter [00340, 01251], lr: 0.000429, loss: 0.0973
2022-10-03 16:26:51 - train: epoch 0043, iter [00350, 01251], lr: 0.000429, loss: 0.0897
2022-10-03 16:27:19 - train: epoch 0043, iter [00360, 01251], lr: 0.000429, loss: 0.0963
2022-10-03 16:27:47 - train: epoch 0043, iter [00370, 01251], lr: 0.000429, loss: 0.0907
2022-10-03 16:28:15 - train: epoch 0043, iter [00380, 01251], lr: 0.000429, loss: 0.0882
2022-10-03 16:28:44 - train: epoch 0043, iter [00390, 01251], lr: 0.000429, loss: 0.0886
2022-10-03 16:29:12 - train: epoch 0043, iter [00400, 01251], lr: 0.000428, loss: 0.0882
2022-10-03 16:29:40 - train: epoch 0043, iter [00410, 01251], lr: 0.000428, loss: 0.0880
2022-10-03 16:30:08 - train: epoch 0043, iter [00420, 01251], lr: 0.000428, loss: 0.0965
2022-10-03 16:30:36 - train: epoch 0043, iter [00430, 01251], lr: 0.000428, loss: 0.0911
2022-10-03 16:31:04 - train: epoch 0043, iter [00440, 01251], lr: 0.000428, loss: 0.0924
2022-10-03 16:31:32 - train: epoch 0043, iter [00450, 01251], lr: 0.000428, loss: 0.0889
2022-10-03 16:32:01 - train: epoch 0043, iter [00460, 01251], lr: 0.000428, loss: 0.0963
2022-10-03 16:32:29 - train: epoch 0043, iter [00470, 01251], lr: 0.000428, loss: 0.0913
2022-10-03 16:32:57 - train: epoch 0043, iter [00480, 01251], lr: 0.000428, loss: 0.0906
2022-10-03 16:33:25 - train: epoch 0043, iter [00490, 01251], lr: 0.000428, loss: 0.0954
2022-10-03 16:33:53 - train: epoch 0043, iter [00500, 01251], lr: 0.000428, loss: 0.1046
2022-10-03 16:34:21 - train: epoch 0043, iter [00510, 01251], lr: 0.000428, loss: 0.0909
2022-10-03 16:34:49 - train: epoch 0043, iter [00520, 01251], lr: 0.000428, loss: 0.0901
2022-10-03 16:35:17 - train: epoch 0043, iter [00530, 01251], lr: 0.000428, loss: 0.0889
2022-10-03 16:35:46 - train: epoch 0043, iter [00540, 01251], lr: 0.000427, loss: 0.0931
2022-10-03 16:36:14 - train: epoch 0043, iter [00550, 01251], lr: 0.000427, loss: 0.0962
2022-10-03 16:36:42 - train: epoch 0043, iter [00560, 01251], lr: 0.000427, loss: 0.0956
2022-10-03 16:37:10 - train: epoch 0043, iter [00570, 01251], lr: 0.000427, loss: 0.0921
2022-10-03 16:37:38 - train: epoch 0043, iter [00580, 01251], lr: 0.000427, loss: 0.0948
2022-10-03 16:38:06 - train: epoch 0043, iter [00590, 01251], lr: 0.000427, loss: 0.0935
2022-10-03 16:38:34 - train: epoch 0043, iter [00600, 01251], lr: 0.000427, loss: 0.0896
2022-10-03 16:39:02 - train: epoch 0043, iter [00610, 01251], lr: 0.000427, loss: 0.0940
2022-10-03 16:39:30 - train: epoch 0043, iter [00620, 01251], lr: 0.000427, loss: 0.0967
2022-10-03 16:39:58 - train: epoch 0043, iter [00630, 01251], lr: 0.000427, loss: 0.0851
2022-10-03 16:40:26 - train: epoch 0043, iter [00640, 01251], lr: 0.000427, loss: 0.0946
2022-10-03 16:40:54 - train: epoch 0043, iter [00650, 01251], lr: 0.000427, loss: 0.0961
2022-10-03 16:41:22 - train: epoch 0043, iter [00660, 01251], lr: 0.000427, loss: 0.0906
2022-10-03 16:41:50 - train: epoch 0043, iter [00670, 01251], lr: 0.000426, loss: 0.0975
2022-10-03 16:42:18 - train: epoch 0043, iter [00680, 01251], lr: 0.000426, loss: 0.0936
2022-10-03 16:42:46 - train: epoch 0043, iter [00690, 01251], lr: 0.000426, loss: 0.0862
2022-10-03 16:43:14 - train: epoch 0043, iter [00700, 01251], lr: 0.000426, loss: 0.0892
2022-10-03 16:43:42 - train: epoch 0043, iter [00710, 01251], lr: 0.000426, loss: 0.0921
2022-10-03 16:44:10 - train: epoch 0043, iter [00720, 01251], lr: 0.000426, loss: 0.0886
2022-10-03 16:44:38 - train: epoch 0043, iter [00730, 01251], lr: 0.000426, loss: 0.0908
2022-10-03 16:45:06 - train: epoch 0043, iter [00740, 01251], lr: 0.000426, loss: 0.0932
2022-10-03 16:45:34 - train: epoch 0043, iter [00750, 01251], lr: 0.000426, loss: 0.0938
2022-10-03 16:46:02 - train: epoch 0043, iter [00760, 01251], lr: 0.000426, loss: 0.0954
2022-10-03 16:46:31 - train: epoch 0043, iter [00770, 01251], lr: 0.000426, loss: 0.0859
2022-10-03 16:46:59 - train: epoch 0043, iter [00780, 01251], lr: 0.000426, loss: 0.0977
2022-10-03 16:47:27 - train: epoch 0043, iter [00790, 01251], lr: 0.000426, loss: 0.0890
2022-10-03 16:47:55 - train: epoch 0043, iter [00800, 01251], lr: 0.000425, loss: 0.0959
2022-10-03 16:48:23 - train: epoch 0043, iter [00810, 01251], lr: 0.000425, loss: 0.0946
2022-10-03 16:48:51 - train: epoch 0043, iter [00820, 01251], lr: 0.000425, loss: 0.0904
2022-10-03 16:49:19 - train: epoch 0043, iter [00830, 01251], lr: 0.000425, loss: 0.0892
2022-10-03 16:49:47 - train: epoch 0043, iter [00840, 01251], lr: 0.000425, loss: 0.0801
2022-10-03 16:50:16 - train: epoch 0043, iter [00850, 01251], lr: 0.000425, loss: 0.0913
2022-10-03 16:50:43 - train: epoch 0043, iter [00860, 01251], lr: 0.000425, loss: 0.0975
2022-10-03 16:51:12 - train: epoch 0043, iter [00870, 01251], lr: 0.000425, loss: 0.0847
2022-10-03 16:51:40 - train: epoch 0043, iter [00880, 01251], lr: 0.000425, loss: 0.0937
2022-10-03 16:52:08 - train: epoch 0043, iter [00890, 01251], lr: 0.000425, loss: 0.0902
2022-10-03 16:52:36 - train: epoch 0043, iter [00900, 01251], lr: 0.000425, loss: 0.0834
2022-10-03 16:53:04 - train: epoch 0043, iter [00910, 01251], lr: 0.000425, loss: 0.0886
2022-10-03 16:53:32 - train: epoch 0043, iter [00920, 01251], lr: 0.000425, loss: 0.0896
2022-10-03 16:54:00 - train: epoch 0043, iter [00930, 01251], lr: 0.000424, loss: 0.0855
2022-10-03 16:54:28 - train: epoch 0043, iter [00940, 01251], lr: 0.000424, loss: 0.0897
2022-10-03 16:54:56 - train: epoch 0043, iter [00950, 01251], lr: 0.000424, loss: 0.0843
2022-10-03 16:55:24 - train: epoch 0043, iter [00960, 01251], lr: 0.000424, loss: 0.0875
2022-10-03 16:55:52 - train: epoch 0043, iter [00970, 01251], lr: 0.000424, loss: 0.0902
2022-10-03 16:56:20 - train: epoch 0043, iter [00980, 01251], lr: 0.000424, loss: 0.0951
2022-10-03 16:56:48 - train: epoch 0043, iter [00990, 01251], lr: 0.000424, loss: 0.0879
2022-10-03 16:57:16 - train: epoch 0043, iter [01000, 01251], lr: 0.000424, loss: 0.0873
2022-10-03 16:57:44 - train: epoch 0043, iter [01010, 01251], lr: 0.000424, loss: 0.0989
2022-10-03 16:58:12 - train: epoch 0043, iter [01020, 01251], lr: 0.000424, loss: 0.0854
2022-10-03 16:58:40 - train: epoch 0043, iter [01030, 01251], lr: 0.000424, loss: 0.0932
2022-10-03 16:59:08 - train: epoch 0043, iter [01040, 01251], lr: 0.000424, loss: 0.0979
2022-10-03 16:59:36 - train: epoch 0043, iter [01050, 01251], lr: 0.000424, loss: 0.0993
2022-10-03 17:00:04 - train: epoch 0043, iter [01060, 01251], lr: 0.000423, loss: 0.0938
2022-10-03 17:00:32 - train: epoch 0043, iter [01070, 01251], lr: 0.000423, loss: 0.0874
2022-10-03 17:01:00 - train: epoch 0043, iter [01080, 01251], lr: 0.000423, loss: 0.0940
2022-10-03 17:01:28 - train: epoch 0043, iter [01090, 01251], lr: 0.000423, loss: 0.0892
2022-10-03 17:01:56 - train: epoch 0043, iter [01100, 01251], lr: 0.000423, loss: 0.0911
2022-10-03 17:02:24 - train: epoch 0043, iter [01110, 01251], lr: 0.000423, loss: 0.0860
2022-10-03 17:02:52 - train: epoch 0043, iter [01120, 01251], lr: 0.000423, loss: 0.0971
2022-10-03 17:03:20 - train: epoch 0043, iter [01130, 01251], lr: 0.000423, loss: 0.0882
2022-10-03 17:03:48 - train: epoch 0043, iter [01140, 01251], lr: 0.000423, loss: 0.0913
2022-10-03 17:04:16 - train: epoch 0043, iter [01150, 01251], lr: 0.000423, loss: 0.0923
2022-10-03 17:04:44 - train: epoch 0043, iter [01160, 01251], lr: 0.000423, loss: 0.0798
2022-10-03 17:05:12 - train: epoch 0043, iter [01170, 01251], lr: 0.000423, loss: 0.0835
2022-10-03 17:05:40 - train: epoch 0043, iter [01180, 01251], lr: 0.000423, loss: 0.0900
2022-10-03 17:06:08 - train: epoch 0043, iter [01190, 01251], lr: 0.000422, loss: 0.0875
2022-10-03 17:06:36 - train: epoch 0043, iter [01200, 01251], lr: 0.000422, loss: 0.0918
2022-10-03 17:07:04 - train: epoch 0043, iter [01210, 01251], lr: 0.000422, loss: 0.0940
2022-10-03 17:07:32 - train: epoch 0043, iter [01220, 01251], lr: 0.000422, loss: 0.0943
2022-10-03 17:08:00 - train: epoch 0043, iter [01230, 01251], lr: 0.000422, loss: 0.0976
2022-10-03 17:08:28 - train: epoch 0043, iter [01240, 01251], lr: 0.000422, loss: 0.0949
2022-10-03 17:08:56 - train: epoch 0043, iter [01250, 01251], lr: 0.000422, loss: 0.0910
2022-10-03 17:09:01 - train: epoch 043, train_loss: 0.0919
2022-10-03 17:09:02 - until epoch: 043, best_loss: 0.0919
2022-10-03 17:09:02 - epoch 044 lr: 0.000422
2022-10-03 17:09:38 - train: epoch 0044, iter [00010, 01251], lr: 0.000422, loss: 0.1019
2022-10-03 17:10:05 - train: epoch 0044, iter [00020, 01251], lr: 0.000422, loss: 0.0872
2022-10-03 17:10:33 - train: epoch 0044, iter [00030, 01251], lr: 0.000422, loss: 0.0932
2022-10-03 17:11:02 - train: epoch 0044, iter [00040, 01251], lr: 0.000422, loss: 0.0892
2022-10-03 17:11:30 - train: epoch 0044, iter [00050, 01251], lr: 0.000422, loss: 0.0893
2022-10-03 17:11:58 - train: epoch 0044, iter [00060, 01251], lr: 0.000422, loss: 0.0894
2022-10-03 17:12:26 - train: epoch 0044, iter [00070, 01251], lr: 0.000421, loss: 0.0931
2022-10-03 17:12:54 - train: epoch 0044, iter [00080, 01251], lr: 0.000421, loss: 0.0938
2022-10-03 17:13:22 - train: epoch 0044, iter [00090, 01251], lr: 0.000421, loss: 0.0980
2022-10-03 17:13:50 - train: epoch 0044, iter [00100, 01251], lr: 0.000421, loss: 0.0861
2022-10-03 17:14:18 - train: epoch 0044, iter [00110, 01251], lr: 0.000421, loss: 0.0810
2022-10-03 17:14:47 - train: epoch 0044, iter [00120, 01251], lr: 0.000421, loss: 0.0913
2022-10-03 17:15:15 - train: epoch 0044, iter [00130, 01251], lr: 0.000421, loss: 0.0931
2022-10-03 17:15:43 - train: epoch 0044, iter [00140, 01251], lr: 0.000421, loss: 0.0920
2022-10-03 17:16:11 - train: epoch 0044, iter [00150, 01251], lr: 0.000421, loss: 0.0977
2022-10-03 17:16:39 - train: epoch 0044, iter [00160, 01251], lr: 0.000421, loss: 0.0950
2022-10-03 17:17:07 - train: epoch 0044, iter [00170, 01251], lr: 0.000421, loss: 0.0873
2022-10-03 17:17:35 - train: epoch 0044, iter [00180, 01251], lr: 0.000421, loss: 0.0999
2022-10-03 17:18:03 - train: epoch 0044, iter [00190, 01251], lr: 0.000421, loss: 0.0869
2022-10-03 17:18:31 - train: epoch 0044, iter [00200, 01251], lr: 0.000420, loss: 0.0949
2022-10-03 17:19:00 - train: epoch 0044, iter [00210, 01251], lr: 0.000420, loss: 0.0937
2022-10-03 17:19:28 - train: epoch 0044, iter [00220, 01251], lr: 0.000420, loss: 0.0981
2022-10-03 17:19:56 - train: epoch 0044, iter [00230, 01251], lr: 0.000420, loss: 0.0919
2022-10-03 17:20:24 - train: epoch 0044, iter [00240, 01251], lr: 0.000420, loss: 0.0918
2022-10-03 17:20:52 - train: epoch 0044, iter [00250, 01251], lr: 0.000420, loss: 0.0861
2022-10-03 17:21:20 - train: epoch 0044, iter [00260, 01251], lr: 0.000420, loss: 0.0900
2022-10-03 17:21:48 - train: epoch 0044, iter [00270, 01251], lr: 0.000420, loss: 0.0908
2022-10-03 17:22:16 - train: epoch 0044, iter [00280, 01251], lr: 0.000420, loss: 0.0985
2022-10-03 17:22:44 - train: epoch 0044, iter [00290, 01251], lr: 0.000420, loss: 0.0951
2022-10-03 17:23:12 - train: epoch 0044, iter [00300, 01251], lr: 0.000420, loss: 0.0855
2022-10-03 17:23:40 - train: epoch 0044, iter [00310, 01251], lr: 0.000420, loss: 0.0862
2022-10-03 17:24:08 - train: epoch 0044, iter [00320, 01251], lr: 0.000420, loss: 0.0897
2022-10-03 17:24:36 - train: epoch 0044, iter [00330, 01251], lr: 0.000419, loss: 0.0976
2022-10-03 17:25:04 - train: epoch 0044, iter [00340, 01251], lr: 0.000419, loss: 0.0885
2022-10-03 17:25:32 - train: epoch 0044, iter [00350, 01251], lr: 0.000419, loss: 0.0831
2022-10-03 17:26:00 - train: epoch 0044, iter [00360, 01251], lr: 0.000419, loss: 0.0923
2022-10-03 17:26:28 - train: epoch 0044, iter [00370, 01251], lr: 0.000419, loss: 0.0881
2022-10-03 17:26:56 - train: epoch 0044, iter [00380, 01251], lr: 0.000419, loss: 0.0967
2022-10-03 17:27:24 - train: epoch 0044, iter [00390, 01251], lr: 0.000419, loss: 0.0933
2022-10-03 17:27:52 - train: epoch 0044, iter [00400, 01251], lr: 0.000419, loss: 0.0890
2022-10-03 17:28:20 - train: epoch 0044, iter [00410, 01251], lr: 0.000419, loss: 0.0965
2022-10-03 17:28:48 - train: epoch 0044, iter [00420, 01251], lr: 0.000419, loss: 0.0953
2022-10-03 17:29:16 - train: epoch 0044, iter [00430, 01251], lr: 0.000419, loss: 0.0874
2022-10-03 17:29:44 - train: epoch 0044, iter [00440, 01251], lr: 0.000419, loss: 0.0924
2022-10-03 17:30:12 - train: epoch 0044, iter [00450, 01251], lr: 0.000419, loss: 0.0917
2022-10-03 17:30:40 - train: epoch 0044, iter [00460, 01251], lr: 0.000418, loss: 0.0875
2022-10-03 17:31:08 - train: epoch 0044, iter [00470, 01251], lr: 0.000418, loss: 0.0831
2022-10-03 17:31:37 - train: epoch 0044, iter [00480, 01251], lr: 0.000418, loss: 0.0864
2022-10-03 17:32:04 - train: epoch 0044, iter [00490, 01251], lr: 0.000418, loss: 0.0849
2022-10-03 17:32:33 - train: epoch 0044, iter [00500, 01251], lr: 0.000418, loss: 0.0892
2022-10-03 17:33:00 - train: epoch 0044, iter [00510, 01251], lr: 0.000418, loss: 0.0868
2022-10-03 17:33:29 - train: epoch 0044, iter [00520, 01251], lr: 0.000418, loss: 0.0909
2022-10-03 17:33:57 - train: epoch 0044, iter [00530, 01251], lr: 0.000418, loss: 0.0940
2022-10-03 17:34:25 - train: epoch 0044, iter [00540, 01251], lr: 0.000418, loss: 0.0927
2022-10-03 17:34:53 - train: epoch 0044, iter [00550, 01251], lr: 0.000418, loss: 0.0905
2022-10-03 17:35:22 - train: epoch 0044, iter [00560, 01251], lr: 0.000418, loss: 0.0908
2022-10-03 17:35:50 - train: epoch 0044, iter [00570, 01251], lr: 0.000418, loss: 0.0975
2022-10-03 17:36:18 - train: epoch 0044, iter [00580, 01251], lr: 0.000418, loss: 0.0965
2022-10-03 17:36:46 - train: epoch 0044, iter [00590, 01251], lr: 0.000417, loss: 0.0929
2022-10-03 17:37:14 - train: epoch 0044, iter [00600, 01251], lr: 0.000417, loss: 0.0892
2022-10-03 17:37:42 - train: epoch 0044, iter [00610, 01251], lr: 0.000417, loss: 0.0938
2022-10-03 17:38:10 - train: epoch 0044, iter [00620, 01251], lr: 0.000417, loss: 0.1021
2022-10-03 17:38:38 - train: epoch 0044, iter [00630, 01251], lr: 0.000417, loss: 0.0919
2022-10-03 17:39:06 - train: epoch 0044, iter [00640, 01251], lr: 0.000417, loss: 0.0942
2022-10-03 17:39:34 - train: epoch 0044, iter [00650, 01251], lr: 0.000417, loss: 0.0882
2022-10-03 17:40:02 - train: epoch 0044, iter [00660, 01251], lr: 0.000417, loss: 0.1010
2022-10-03 17:40:30 - train: epoch 0044, iter [00670, 01251], lr: 0.000417, loss: 0.0976
2022-10-03 17:40:58 - train: epoch 0044, iter [00680, 01251], lr: 0.000417, loss: 0.0968
2022-10-03 17:41:26 - train: epoch 0044, iter [00690, 01251], lr: 0.000417, loss: 0.0923
2022-10-03 17:41:54 - train: epoch 0044, iter [00700, 01251], lr: 0.000417, loss: 0.0996
2022-10-03 17:42:23 - train: epoch 0044, iter [00710, 01251], lr: 0.000417, loss: 0.0942
2022-10-03 17:42:51 - train: epoch 0044, iter [00720, 01251], lr: 0.000416, loss: 0.0972
2022-10-03 17:43:18 - train: epoch 0044, iter [00730, 01251], lr: 0.000416, loss: 0.0884
2022-10-03 17:43:46 - train: epoch 0044, iter [00740, 01251], lr: 0.000416, loss: 0.0940
2022-10-03 17:44:14 - train: epoch 0044, iter [00750, 01251], lr: 0.000416, loss: 0.0918
2022-10-03 17:44:42 - train: epoch 0044, iter [00760, 01251], lr: 0.000416, loss: 0.0895
2022-10-03 17:45:10 - train: epoch 0044, iter [00770, 01251], lr: 0.000416, loss: 0.0901
2022-10-03 17:45:38 - train: epoch 0044, iter [00780, 01251], lr: 0.000416, loss: 0.0877
2022-10-03 17:46:06 - train: epoch 0044, iter [00790, 01251], lr: 0.000416, loss: 0.0969
2022-10-03 17:46:35 - train: epoch 0044, iter [00800, 01251], lr: 0.000416, loss: 0.0987
2022-10-03 17:47:02 - train: epoch 0044, iter [00810, 01251], lr: 0.000416, loss: 0.0894
2022-10-03 17:47:31 - train: epoch 0044, iter [00820, 01251], lr: 0.000416, loss: 0.0874
2022-10-03 17:47:59 - train: epoch 0044, iter [00830, 01251], lr: 0.000416, loss: 0.0920
2022-10-03 17:48:27 - train: epoch 0044, iter [00840, 01251], lr: 0.000416, loss: 0.0912
2022-10-03 17:48:55 - train: epoch 0044, iter [00850, 01251], lr: 0.000415, loss: 0.0906
2022-10-03 17:49:22 - train: epoch 0044, iter [00860, 01251], lr: 0.000415, loss: 0.0816
2022-10-03 17:49:50 - train: epoch 0044, iter [00870, 01251], lr: 0.000415, loss: 0.0900
2022-10-03 17:50:19 - train: epoch 0044, iter [00880, 01251], lr: 0.000415, loss: 0.0992
2022-10-03 17:50:46 - train: epoch 0044, iter [00890, 01251], lr: 0.000415, loss: 0.0880
2022-10-03 17:51:14 - train: epoch 0044, iter [00900, 01251], lr: 0.000415, loss: 0.0871
2022-10-03 17:51:43 - train: epoch 0044, iter [00910, 01251], lr: 0.000415, loss: 0.0928
2022-10-03 17:52:11 - train: epoch 0044, iter [00920, 01251], lr: 0.000415, loss: 0.0863
2022-10-03 17:52:39 - train: epoch 0044, iter [00930, 01251], lr: 0.000415, loss: 0.0909
2022-10-03 17:53:07 - train: epoch 0044, iter [00940, 01251], lr: 0.000415, loss: 0.0890
2022-10-03 17:53:35 - train: epoch 0044, iter [00950, 01251], lr: 0.000415, loss: 0.0875
2022-10-03 17:54:03 - train: epoch 0044, iter [00960, 01251], lr: 0.000415, loss: 0.0885
2022-10-03 17:54:31 - train: epoch 0044, iter [00970, 01251], lr: 0.000415, loss: 0.0896
2022-10-03 17:54:59 - train: epoch 0044, iter [00980, 01251], lr: 0.000414, loss: 0.0894
2022-10-03 17:55:27 - train: epoch 0044, iter [00990, 01251], lr: 0.000414, loss: 0.0917
2022-10-03 17:55:55 - train: epoch 0044, iter [01000, 01251], lr: 0.000414, loss: 0.0868
2022-10-03 17:56:24 - train: epoch 0044, iter [01010, 01251], lr: 0.000414, loss: 0.0962
2022-10-03 17:56:52 - train: epoch 0044, iter [01020, 01251], lr: 0.000414, loss: 0.0906
2022-10-03 17:57:20 - train: epoch 0044, iter [01030, 01251], lr: 0.000414, loss: 0.0928
2022-10-03 17:57:48 - train: epoch 0044, iter [01040, 01251], lr: 0.000414, loss: 0.0883
2022-10-03 17:58:16 - train: epoch 0044, iter [01050, 01251], lr: 0.000414, loss: 0.0873
2022-10-03 17:58:44 - train: epoch 0044, iter [01060, 01251], lr: 0.000414, loss: 0.0960
2022-10-03 17:59:12 - train: epoch 0044, iter [01070, 01251], lr: 0.000414, loss: 0.0916
2022-10-03 17:59:40 - train: epoch 0044, iter [01080, 01251], lr: 0.000414, loss: 0.0932
2022-10-03 18:00:08 - train: epoch 0044, iter [01090, 01251], lr: 0.000414, loss: 0.0902
2022-10-03 18:00:36 - train: epoch 0044, iter [01100, 01251], lr: 0.000414, loss: 0.0933
2022-10-03 18:01:04 - train: epoch 0044, iter [01110, 01251], lr: 0.000413, loss: 0.0987
2022-10-03 18:01:32 - train: epoch 0044, iter [01120, 01251], lr: 0.000413, loss: 0.0906
2022-10-03 18:02:00 - train: epoch 0044, iter [01130, 01251], lr: 0.000413, loss: 0.0927
2022-10-03 18:02:28 - train: epoch 0044, iter [01140, 01251], lr: 0.000413, loss: 0.0983
2022-10-03 18:02:56 - train: epoch 0044, iter [01150, 01251], lr: 0.000413, loss: 0.0932
2022-10-03 18:03:24 - train: epoch 0044, iter [01160, 01251], lr: 0.000413, loss: 0.0933
2022-10-03 18:03:52 - train: epoch 0044, iter [01170, 01251], lr: 0.000413, loss: 0.0858
2022-10-03 18:04:20 - train: epoch 0044, iter [01180, 01251], lr: 0.000413, loss: 0.0982
2022-10-03 18:04:48 - train: epoch 0044, iter [01190, 01251], lr: 0.000413, loss: 0.0834
2022-10-03 18:05:16 - train: epoch 0044, iter [01200, 01251], lr: 0.000413, loss: 0.0784
2022-10-03 18:05:44 - train: epoch 0044, iter [01210, 01251], lr: 0.000413, loss: 0.0891
2022-10-03 18:06:12 - train: epoch 0044, iter [01220, 01251], lr: 0.000413, loss: 0.0922
2022-10-03 18:06:40 - train: epoch 0044, iter [01230, 01251], lr: 0.000413, loss: 0.0948
2022-10-03 18:07:08 - train: epoch 0044, iter [01240, 01251], lr: 0.000412, loss: 0.0892
2022-10-03 18:07:36 - train: epoch 0044, iter [01250, 01251], lr: 0.000412, loss: 0.0880
2022-10-03 18:07:40 - train: epoch 044, train_loss: 0.0916
2022-10-03 18:07:42 - until epoch: 044, best_loss: 0.0916
2022-10-03 18:07:42 - epoch 045 lr: 0.000412
2022-10-03 18:08:17 - train: epoch 0045, iter [00010, 01251], lr: 0.000412, loss: 0.0962
2022-10-03 18:08:45 - train: epoch 0045, iter [00020, 01251], lr: 0.000412, loss: 0.0791
2022-10-03 18:09:14 - train: epoch 0045, iter [00030, 01251], lr: 0.000412, loss: 0.0947
2022-10-03 18:09:42 - train: epoch 0045, iter [00040, 01251], lr: 0.000412, loss: 0.0959
2022-10-03 18:10:10 - train: epoch 0045, iter [00050, 01251], lr: 0.000412, loss: 0.0920
2022-10-03 18:10:38 - train: epoch 0045, iter [00060, 01251], lr: 0.000412, loss: 0.0936
2022-10-03 18:11:07 - train: epoch 0045, iter [00070, 01251], lr: 0.000412, loss: 0.0938
2022-10-03 18:11:35 - train: epoch 0045, iter [00080, 01251], lr: 0.000412, loss: 0.0926
2022-10-03 18:12:03 - train: epoch 0045, iter [00090, 01251], lr: 0.000412, loss: 0.0893
2022-10-03 18:12:31 - train: epoch 0045, iter [00100, 01251], lr: 0.000412, loss: 0.0932
2022-10-03 18:12:59 - train: epoch 0045, iter [00110, 01251], lr: 0.000412, loss: 0.0861
2022-10-03 18:13:27 - train: epoch 0045, iter [00120, 01251], lr: 0.000411, loss: 0.0972
2022-10-03 18:13:55 - train: epoch 0045, iter [00130, 01251], lr: 0.000411, loss: 0.0968
2022-10-03 18:14:23 - train: epoch 0045, iter [00140, 01251], lr: 0.000411, loss: 0.0840
2022-10-03 18:14:51 - train: epoch 0045, iter [00150, 01251], lr: 0.000411, loss: 0.0828
2022-10-03 18:15:19 - train: epoch 0045, iter [00160, 01251], lr: 0.000411, loss: 0.0849
2022-10-03 18:15:47 - train: epoch 0045, iter [00170, 01251], lr: 0.000411, loss: 0.0901
2022-10-03 18:16:15 - train: epoch 0045, iter [00180, 01251], lr: 0.000411, loss: 0.0856
2022-10-03 18:16:43 - train: epoch 0045, iter [00190, 01251], lr: 0.000411, loss: 0.0872
2022-10-03 18:17:11 - train: epoch 0045, iter [00200, 01251], lr: 0.000411, loss: 0.0932
2022-10-03 18:17:39 - train: epoch 0045, iter [00210, 01251], lr: 0.000411, loss: 0.0907
2022-10-03 18:18:08 - train: epoch 0045, iter [00220, 01251], lr: 0.000411, loss: 0.0968
2022-10-03 18:18:36 - train: epoch 0045, iter [00230, 01251], lr: 0.000411, loss: 0.0964
2022-10-03 18:19:04 - train: epoch 0045, iter [00240, 01251], lr: 0.000411, loss: 0.0956
2022-10-03 18:19:32 - train: epoch 0045, iter [00250, 01251], lr: 0.000410, loss: 0.0987
2022-10-03 18:20:00 - train: epoch 0045, iter [00260, 01251], lr: 0.000410, loss: 0.0892
2022-10-03 18:20:29 - train: epoch 0045, iter [00270, 01251], lr: 0.000410, loss: 0.0923
2022-10-03 18:20:57 - train: epoch 0045, iter [00280, 01251], lr: 0.000410, loss: 0.0872
2022-10-03 18:21:25 - train: epoch 0045, iter [00290, 01251], lr: 0.000410, loss: 0.0934
2022-10-03 18:21:53 - train: epoch 0045, iter [00300, 01251], lr: 0.000410, loss: 0.0937
2022-10-03 18:22:21 - train: epoch 0045, iter [00310, 01251], lr: 0.000410, loss: 0.0896
2022-10-03 18:22:50 - train: epoch 0045, iter [00320, 01251], lr: 0.000410, loss: 0.0889
2022-10-03 18:23:18 - train: epoch 0045, iter [00330, 01251], lr: 0.000410, loss: 0.0876
2022-10-03 18:23:46 - train: epoch 0045, iter [00340, 01251], lr: 0.000410, loss: 0.0947
2022-10-03 18:24:14 - train: epoch 0045, iter [00350, 01251], lr: 0.000410, loss: 0.0856
2022-10-03 18:24:42 - train: epoch 0045, iter [00360, 01251], lr: 0.000410, loss: 0.0981
2022-10-03 18:25:11 - train: epoch 0045, iter [00370, 01251], lr: 0.000410, loss: 0.0879
2022-10-03 18:25:39 - train: epoch 0045, iter [00380, 01251], lr: 0.000409, loss: 0.0929
2022-10-03 18:26:07 - train: epoch 0045, iter [00390, 01251], lr: 0.000409, loss: 0.0914
2022-10-03 18:26:35 - train: epoch 0045, iter [00400, 01251], lr: 0.000409, loss: 0.0860
2022-10-03 18:27:03 - train: epoch 0045, iter [00410, 01251], lr: 0.000409, loss: 0.0852
2022-10-03 18:27:31 - train: epoch 0045, iter [00420, 01251], lr: 0.000409, loss: 0.0952
2022-10-03 18:28:00 - train: epoch 0045, iter [00430, 01251], lr: 0.000409, loss: 0.0926
2022-10-03 18:28:28 - train: epoch 0045, iter [00440, 01251], lr: 0.000409, loss: 0.0924
2022-10-03 18:28:56 - train: epoch 0045, iter [00450, 01251], lr: 0.000409, loss: 0.0871
2022-10-03 18:29:24 - train: epoch 0045, iter [00460, 01251], lr: 0.000409, loss: 0.0943
2022-10-03 18:29:52 - train: epoch 0045, iter [00470, 01251], lr: 0.000409, loss: 0.0937
2022-10-03 18:30:20 - train: epoch 0045, iter [00480, 01251], lr: 0.000409, loss: 0.0938
2022-10-03 18:30:48 - train: epoch 0045, iter [00490, 01251], lr: 0.000409, loss: 0.0978
2022-10-03 18:31:16 - train: epoch 0045, iter [00500, 01251], lr: 0.000408, loss: 0.0935
2022-10-03 18:31:44 - train: epoch 0045, iter [00510, 01251], lr: 0.000408, loss: 0.0916
2022-10-03 18:32:13 - train: epoch 0045, iter [00520, 01251], lr: 0.000408, loss: 0.0867
2022-10-03 18:32:41 - train: epoch 0045, iter [00530, 01251], lr: 0.000408, loss: 0.0857
2022-10-03 18:33:09 - train: epoch 0045, iter [00540, 01251], lr: 0.000408, loss: 0.0875
2022-10-03 18:33:37 - train: epoch 0045, iter [00550, 01251], lr: 0.000408, loss: 0.0863
2022-10-03 18:34:05 - train: epoch 0045, iter [00560, 01251], lr: 0.000408, loss: 0.0882
2022-10-03 18:34:33 - train: epoch 0045, iter [00570, 01251], lr: 0.000408, loss: 0.0872
2022-10-03 18:35:01 - train: epoch 0045, iter [00580, 01251], lr: 0.000408, loss: 0.0926
2022-10-03 18:35:29 - train: epoch 0045, iter [00590, 01251], lr: 0.000408, loss: 0.0957
2022-10-03 18:35:57 - train: epoch 0045, iter [00600, 01251], lr: 0.000408, loss: 0.0971
2022-10-03 18:36:26 - train: epoch 0045, iter [00610, 01251], lr: 0.000408, loss: 0.0902
2022-10-03 18:36:54 - train: epoch 0045, iter [00620, 01251], lr: 0.000408, loss: 0.0867
2022-10-03 18:37:22 - train: epoch 0045, iter [00630, 01251], lr: 0.000407, loss: 0.0973
2022-10-03 18:37:50 - train: epoch 0045, iter [00640, 01251], lr: 0.000407, loss: 0.0965
2022-10-03 18:38:19 - train: epoch 0045, iter [00650, 01251], lr: 0.000407, loss: 0.0870
2022-10-03 18:38:47 - train: epoch 0045, iter [00660, 01251], lr: 0.000407, loss: 0.0867
2022-10-03 18:39:15 - train: epoch 0045, iter [00670, 01251], lr: 0.000407, loss: 0.0903
2022-10-03 18:39:43 - train: epoch 0045, iter [00680, 01251], lr: 0.000407, loss: 0.0966
2022-10-03 18:40:11 - train: epoch 0045, iter [00690, 01251], lr: 0.000407, loss: 0.0834
2022-10-03 18:40:40 - train: epoch 0045, iter [00700, 01251], lr: 0.000407, loss: 0.0845
2022-10-03 18:41:08 - train: epoch 0045, iter [00710, 01251], lr: 0.000407, loss: 0.0912
2022-10-03 18:41:36 - train: epoch 0045, iter [00720, 01251], lr: 0.000407, loss: 0.0908
2022-10-03 18:42:04 - train: epoch 0045, iter [00730, 01251], lr: 0.000407, loss: 0.0930
2022-10-03 18:42:32 - train: epoch 0045, iter [00740, 01251], lr: 0.000407, loss: 0.0989
2022-10-03 18:43:00 - train: epoch 0045, iter [00750, 01251], lr: 0.000407, loss: 0.0845
2022-10-03 18:43:28 - train: epoch 0045, iter [00760, 01251], lr: 0.000406, loss: 0.0950
2022-10-03 18:43:56 - train: epoch 0045, iter [00770, 01251], lr: 0.000406, loss: 0.0882
2022-10-03 18:44:24 - train: epoch 0045, iter [00780, 01251], lr: 0.000406, loss: 0.0961
2022-10-03 18:44:53 - train: epoch 0045, iter [00790, 01251], lr: 0.000406, loss: 0.0862
2022-10-03 18:45:21 - train: epoch 0045, iter [00800, 01251], lr: 0.000406, loss: 0.0941
2022-10-03 18:45:49 - train: epoch 0045, iter [00810, 01251], lr: 0.000406, loss: 0.0860
2022-10-03 18:46:17 - train: epoch 0045, iter [00820, 01251], lr: 0.000406, loss: 0.0901
2022-10-03 18:46:45 - train: epoch 0045, iter [00830, 01251], lr: 0.000406, loss: 0.0938
2022-10-03 18:47:13 - train: epoch 0045, iter [00840, 01251], lr: 0.000406, loss: 0.0951
2022-10-03 18:47:41 - train: epoch 0045, iter [00850, 01251], lr: 0.000406, loss: 0.0866
2022-10-03 18:48:09 - train: epoch 0045, iter [00860, 01251], lr: 0.000406, loss: 0.0886
2022-10-03 18:48:37 - train: epoch 0045, iter [00870, 01251], lr: 0.000406, loss: 0.0965
2022-10-03 18:49:05 - train: epoch 0045, iter [00880, 01251], lr: 0.000406, loss: 0.0841
2022-10-03 18:49:33 - train: epoch 0045, iter [00890, 01251], lr: 0.000405, loss: 0.0994
2022-10-03 18:50:01 - train: epoch 0045, iter [00900, 01251], lr: 0.000405, loss: 0.0908
2022-10-03 18:50:30 - train: epoch 0045, iter [00910, 01251], lr: 0.000405, loss: 0.0870
2022-10-03 18:50:58 - train: epoch 0045, iter [00920, 01251], lr: 0.000405, loss: 0.1000
2022-10-03 18:51:26 - train: epoch 0045, iter [00930, 01251], lr: 0.000405, loss: 0.0911
2022-10-03 18:51:54 - train: epoch 0045, iter [00940, 01251], lr: 0.000405, loss: 0.0934
2022-10-03 18:52:22 - train: epoch 0045, iter [00950, 01251], lr: 0.000405, loss: 0.0879
2022-10-03 18:52:50 - train: epoch 0045, iter [00960, 01251], lr: 0.000405, loss: 0.0825
2022-10-03 18:53:18 - train: epoch 0045, iter [00970, 01251], lr: 0.000405, loss: 0.0900
2022-10-03 18:53:46 - train: epoch 0045, iter [00980, 01251], lr: 0.000405, loss: 0.0896
2022-10-03 18:54:14 - train: epoch 0045, iter [00990, 01251], lr: 0.000405, loss: 0.0913
2022-10-03 18:54:42 - train: epoch 0045, iter [01000, 01251], lr: 0.000405, loss: 0.0789
2022-10-03 18:55:10 - train: epoch 0045, iter [01010, 01251], lr: 0.000404, loss: 0.0948
2022-10-03 18:55:38 - train: epoch 0045, iter [01020, 01251], lr: 0.000404, loss: 0.0886
2022-10-03 18:56:06 - train: epoch 0045, iter [01030, 01251], lr: 0.000404, loss: 0.0893
2022-10-03 18:56:34 - train: epoch 0045, iter [01040, 01251], lr: 0.000404, loss: 0.0979
2022-10-03 18:57:02 - train: epoch 0045, iter [01050, 01251], lr: 0.000404, loss: 0.0961
2022-10-03 18:57:30 - train: epoch 0045, iter [01060, 01251], lr: 0.000404, loss: 0.0923
2022-10-03 18:57:58 - train: epoch 0045, iter [01070, 01251], lr: 0.000404, loss: 0.0879
2022-10-03 18:58:26 - train: epoch 0045, iter [01080, 01251], lr: 0.000404, loss: 0.0870
2022-10-03 18:58:55 - train: epoch 0045, iter [01090, 01251], lr: 0.000404, loss: 0.1048
2022-10-03 18:59:23 - train: epoch 0045, iter [01100, 01251], lr: 0.000404, loss: 0.0895
2022-10-03 18:59:51 - train: epoch 0045, iter [01110, 01251], lr: 0.000404, loss: 0.0933
2022-10-03 19:00:19 - train: epoch 0045, iter [01120, 01251], lr: 0.000404, loss: 0.0891
2022-10-03 19:00:47 - train: epoch 0045, iter [01130, 01251], lr: 0.000404, loss: 0.0885
2022-10-03 19:01:15 - train: epoch 0045, iter [01140, 01251], lr: 0.000403, loss: 0.1004
2022-10-03 19:01:43 - train: epoch 0045, iter [01150, 01251], lr: 0.000403, loss: 0.0876
2022-10-03 19:02:11 - train: epoch 0045, iter [01160, 01251], lr: 0.000403, loss: 0.0877
2022-10-03 19:02:39 - train: epoch 0045, iter [01170, 01251], lr: 0.000403, loss: 0.0950
2022-10-03 19:03:08 - train: epoch 0045, iter [01180, 01251], lr: 0.000403, loss: 0.1011
2022-10-03 19:03:36 - train: epoch 0045, iter [01190, 01251], lr: 0.000403, loss: 0.0894
2022-10-03 19:04:04 - train: epoch 0045, iter [01200, 01251], lr: 0.000403, loss: 0.0943
2022-10-03 19:04:32 - train: epoch 0045, iter [01210, 01251], lr: 0.000403, loss: 0.0884
2022-10-03 19:05:00 - train: epoch 0045, iter [01220, 01251], lr: 0.000403, loss: 0.0886
2022-10-03 19:05:28 - train: epoch 0045, iter [01230, 01251], lr: 0.000403, loss: 0.0935
2022-10-03 19:05:56 - train: epoch 0045, iter [01240, 01251], lr: 0.000403, loss: 0.0835
2022-10-03 19:06:24 - train: epoch 0045, iter [01250, 01251], lr: 0.000403, loss: 0.0899
2022-10-03 19:06:28 - train: epoch 045, train_loss: 0.0912
2022-10-03 19:06:30 - until epoch: 045, best_loss: 0.0912
2022-10-03 19:06:30 - epoch 046 lr: 0.000403
2022-10-03 19:07:05 - train: epoch 0046, iter [00010, 01251], lr: 0.000403, loss: 0.0880
2022-10-03 19:07:33 - train: epoch 0046, iter [00020, 01251], lr: 0.000402, loss: 0.0930
2022-10-03 19:08:02 - train: epoch 0046, iter [00030, 01251], lr: 0.000402, loss: 0.0895
2022-10-03 19:08:30 - train: epoch 0046, iter [00040, 01251], lr: 0.000402, loss: 0.0942
2022-10-03 19:08:59 - train: epoch 0046, iter [00050, 01251], lr: 0.000402, loss: 0.0945
2022-10-03 19:09:27 - train: epoch 0046, iter [00060, 01251], lr: 0.000402, loss: 0.0895
2022-10-03 19:09:55 - train: epoch 0046, iter [00070, 01251], lr: 0.000402, loss: 0.0878
2022-10-03 19:10:23 - train: epoch 0046, iter [00080, 01251], lr: 0.000402, loss: 0.0925
2022-10-03 19:10:52 - train: epoch 0046, iter [00090, 01251], lr: 0.000402, loss: 0.0865
2022-10-03 19:11:20 - train: epoch 0046, iter [00100, 01251], lr: 0.000402, loss: 0.0901
2022-10-03 19:11:49 - train: epoch 0046, iter [00110, 01251], lr: 0.000402, loss: 0.0979
2022-10-03 19:12:17 - train: epoch 0046, iter [00120, 01251], lr: 0.000402, loss: 0.0877
2022-10-03 19:12:46 - train: epoch 0046, iter [00130, 01251], lr: 0.000402, loss: 0.0918
2022-10-03 19:13:14 - train: epoch 0046, iter [00140, 01251], lr: 0.000402, loss: 0.0879
2022-10-03 19:13:42 - train: epoch 0046, iter [00150, 01251], lr: 0.000401, loss: 0.0897
2022-10-03 19:14:10 - train: epoch 0046, iter [00160, 01251], lr: 0.000401, loss: 0.0910
2022-10-03 19:14:39 - train: epoch 0046, iter [00170, 01251], lr: 0.000401, loss: 0.0925
2022-10-03 19:15:07 - train: epoch 0046, iter [00180, 01251], lr: 0.000401, loss: 0.0905
2022-10-03 19:15:35 - train: epoch 0046, iter [00190, 01251], lr: 0.000401, loss: 0.0832
2022-10-03 19:16:04 - train: epoch 0046, iter [00200, 01251], lr: 0.000401, loss: 0.0876
2022-10-03 19:16:32 - train: epoch 0046, iter [00210, 01251], lr: 0.000401, loss: 0.0959
2022-10-03 19:17:00 - train: epoch 0046, iter [00220, 01251], lr: 0.000401, loss: 0.0922
2022-10-03 19:17:28 - train: epoch 0046, iter [00230, 01251], lr: 0.000401, loss: 0.0982
2022-10-03 19:17:56 - train: epoch 0046, iter [00240, 01251], lr: 0.000401, loss: 0.0878
2022-10-03 19:18:24 - train: epoch 0046, iter [00250, 01251], lr: 0.000401, loss: 0.0831
2022-10-03 19:18:53 - train: epoch 0046, iter [00260, 01251], lr: 0.000401, loss: 0.0934
2022-10-03 19:19:21 - train: epoch 0046, iter [00270, 01251], lr: 0.000400, loss: 0.0915
2022-10-03 19:19:49 - train: epoch 0046, iter [00280, 01251], lr: 0.000400, loss: 0.0977
2022-10-03 19:20:17 - train: epoch 0046, iter [00290, 01251], lr: 0.000400, loss: 0.0985
2022-10-03 19:20:45 - train: epoch 0046, iter [00300, 01251], lr: 0.000400, loss: 0.0922
2022-10-03 19:21:14 - train: epoch 0046, iter [00310, 01251], lr: 0.000400, loss: 0.0947
2022-10-03 19:21:42 - train: epoch 0046, iter [00320, 01251], lr: 0.000400, loss: 0.0890
2022-10-03 19:22:10 - train: epoch 0046, iter [00330, 01251], lr: 0.000400, loss: 0.0911
2022-10-03 19:22:38 - train: epoch 0046, iter [00340, 01251], lr: 0.000400, loss: 0.0879
2022-10-03 19:23:06 - train: epoch 0046, iter [00350, 01251], lr: 0.000400, loss: 0.0921
2022-10-03 19:23:34 - train: epoch 0046, iter [00360, 01251], lr: 0.000400, loss: 0.0878
2022-10-03 19:24:03 - train: epoch 0046, iter [00370, 01251], lr: 0.000400, loss: 0.0927
2022-10-03 19:24:31 - train: epoch 0046, iter [00380, 01251], lr: 0.000400, loss: 0.0961
2022-10-03 19:24:59 - train: epoch 0046, iter [00390, 01251], lr: 0.000400, loss: 0.0933
2022-10-03 19:25:27 - train: epoch 0046, iter [00400, 01251], lr: 0.000399, loss: 0.0879
2022-10-03 19:25:55 - train: epoch 0046, iter [00410, 01251], lr: 0.000399, loss: 0.0970
2022-10-03 19:26:23 - train: epoch 0046, iter [00420, 01251], lr: 0.000399, loss: 0.0922
2022-10-03 19:26:51 - train: epoch 0046, iter [00430, 01251], lr: 0.000399, loss: 0.0953
2022-10-03 19:27:19 - train: epoch 0046, iter [00440, 01251], lr: 0.000399, loss: 0.0891
2022-10-03 19:27:48 - train: epoch 0046, iter [00450, 01251], lr: 0.000399, loss: 0.0955
2022-10-03 19:28:16 - train: epoch 0046, iter [00460, 01251], lr: 0.000399, loss: 0.0931
2022-10-03 19:28:44 - train: epoch 0046, iter [00470, 01251], lr: 0.000399, loss: 0.0937
2022-10-03 19:29:13 - train: epoch 0046, iter [00480, 01251], lr: 0.000399, loss: 0.0860
2022-10-03 19:29:41 - train: epoch 0046, iter [00490, 01251], lr: 0.000399, loss: 0.0923
2022-10-03 19:30:09 - train: epoch 0046, iter [00500, 01251], lr: 0.000399, loss: 0.0830
2022-10-03 19:30:37 - train: epoch 0046, iter [00510, 01251], lr: 0.000399, loss: 0.0935
2022-10-03 19:31:05 - train: epoch 0046, iter [00520, 01251], lr: 0.000399, loss: 0.0977
2022-10-03 19:31:33 - train: epoch 0046, iter [00530, 01251], lr: 0.000398, loss: 0.0948
2022-10-03 19:32:02 - train: epoch 0046, iter [00540, 01251], lr: 0.000398, loss: 0.0882
2022-10-03 19:32:30 - train: epoch 0046, iter [00550, 01251], lr: 0.000398, loss: 0.0903
2022-10-03 19:32:58 - train: epoch 0046, iter [00560, 01251], lr: 0.000398, loss: 0.0914
2022-10-03 19:33:26 - train: epoch 0046, iter [00570, 01251], lr: 0.000398, loss: 0.0934
2022-10-03 19:33:54 - train: epoch 0046, iter [00580, 01251], lr: 0.000398, loss: 0.0891
2022-10-03 19:34:23 - train: epoch 0046, iter [00590, 01251], lr: 0.000398, loss: 0.0890
2022-10-03 19:34:51 - train: epoch 0046, iter [00600, 01251], lr: 0.000398, loss: 0.1041
2022-10-03 19:35:19 - train: epoch 0046, iter [00610, 01251], lr: 0.000398, loss: 0.0956
2022-10-03 19:35:47 - train: epoch 0046, iter [00620, 01251], lr: 0.000398, loss: 0.0945
2022-10-03 19:36:15 - train: epoch 0046, iter [00630, 01251], lr: 0.000398, loss: 0.0890
2022-10-03 19:36:44 - train: epoch 0046, iter [00640, 01251], lr: 0.000398, loss: 0.0980
2022-10-03 19:37:12 - train: epoch 0046, iter [00650, 01251], lr: 0.000397, loss: 0.0898
2022-10-03 19:37:40 - train: epoch 0046, iter [00660, 01251], lr: 0.000397, loss: 0.0937
2022-10-03 19:38:08 - train: epoch 0046, iter [00670, 01251], lr: 0.000397, loss: 0.0984
2022-10-03 19:38:37 - train: epoch 0046, iter [00680, 01251], lr: 0.000397, loss: 0.0877
2022-10-03 19:39:05 - train: epoch 0046, iter [00690, 01251], lr: 0.000397, loss: 0.0921
2022-10-03 19:39:33 - train: epoch 0046, iter [00700, 01251], lr: 0.000397, loss: 0.0889
2022-10-03 19:40:01 - train: epoch 0046, iter [00710, 01251], lr: 0.000397, loss: 0.0975
2022-10-03 19:40:29 - train: epoch 0046, iter [00720, 01251], lr: 0.000397, loss: 0.0784
2022-10-03 19:40:57 - train: epoch 0046, iter [00730, 01251], lr: 0.000397, loss: 0.0852
2022-10-03 19:41:26 - train: epoch 0046, iter [00740, 01251], lr: 0.000397, loss: 0.0871
2022-10-03 19:41:54 - train: epoch 0046, iter [00750, 01251], lr: 0.000397, loss: 0.0902
2022-10-03 19:42:22 - train: epoch 0046, iter [00760, 01251], lr: 0.000397, loss: 0.0888
2022-10-03 19:42:50 - train: epoch 0046, iter [00770, 01251], lr: 0.000397, loss: 0.0903
2022-10-03 19:43:18 - train: epoch 0046, iter [00780, 01251], lr: 0.000396, loss: 0.0814
2022-10-03 19:43:46 - train: epoch 0046, iter [00790, 01251], lr: 0.000396, loss: 0.1007
2022-10-03 19:44:14 - train: epoch 0046, iter [00800, 01251], lr: 0.000396, loss: 0.0907
2022-10-03 19:44:42 - train: epoch 0046, iter [00810, 01251], lr: 0.000396, loss: 0.0842
2022-10-03 19:45:11 - train: epoch 0046, iter [00820, 01251], lr: 0.000396, loss: 0.0845
2022-10-03 19:45:39 - train: epoch 0046, iter [00830, 01251], lr: 0.000396, loss: 0.0841
2022-10-03 19:46:07 - train: epoch 0046, iter [00840, 01251], lr: 0.000396, loss: 0.0886
2022-10-03 19:46:35 - train: epoch 0046, iter [00850, 01251], lr: 0.000396, loss: 0.0896
2022-10-03 19:47:03 - train: epoch 0046, iter [00860, 01251], lr: 0.000396, loss: 0.0989
2022-10-03 19:47:31 - train: epoch 0046, iter [00870, 01251], lr: 0.000396, loss: 0.1000
2022-10-03 19:47:59 - train: epoch 0046, iter [00880, 01251], lr: 0.000396, loss: 0.0848
2022-10-03 19:48:27 - train: epoch 0046, iter [00890, 01251], lr: 0.000396, loss: 0.0883
2022-10-03 19:48:55 - train: epoch 0046, iter [00900, 01251], lr: 0.000395, loss: 0.0961
2022-10-03 19:49:24 - train: epoch 0046, iter [00910, 01251], lr: 0.000395, loss: 0.0881
2022-10-03 19:49:52 - train: epoch 0046, iter [00920, 01251], lr: 0.000395, loss: 0.0921
2022-10-03 19:50:20 - train: epoch 0046, iter [00930, 01251], lr: 0.000395, loss: 0.0901
2022-10-03 19:50:48 - train: epoch 0046, iter [00940, 01251], lr: 0.000395, loss: 0.0952
2022-10-03 19:51:16 - train: epoch 0046, iter [00950, 01251], lr: 0.000395, loss: 0.0911
2022-10-03 19:51:44 - train: epoch 0046, iter [00960, 01251], lr: 0.000395, loss: 0.0938
2022-10-03 19:52:12 - train: epoch 0046, iter [00970, 01251], lr: 0.000395, loss: 0.0909
2022-10-03 19:52:40 - train: epoch 0046, iter [00980, 01251], lr: 0.000395, loss: 0.0934
2022-10-03 19:53:08 - train: epoch 0046, iter [00990, 01251], lr: 0.000395, loss: 0.0995
2022-10-03 19:53:37 - train: epoch 0046, iter [01000, 01251], lr: 0.000395, loss: 0.0961
2022-10-03 19:54:05 - train: epoch 0046, iter [01010, 01251], lr: 0.000395, loss: 0.0916
2022-10-03 19:54:33 - train: epoch 0046, iter [01020, 01251], lr: 0.000395, loss: 0.0853
2022-10-03 19:55:01 - train: epoch 0046, iter [01030, 01251], lr: 0.000394, loss: 0.0997
2022-10-03 19:55:29 - train: epoch 0046, iter [01040, 01251], lr: 0.000394, loss: 0.0974
2022-10-03 19:55:57 - train: epoch 0046, iter [01050, 01251], lr: 0.000394, loss: 0.0953
2022-10-03 19:56:25 - train: epoch 0046, iter [01060, 01251], lr: 0.000394, loss: 0.0907
2022-10-03 19:56:53 - train: epoch 0046, iter [01070, 01251], lr: 0.000394, loss: 0.0951
2022-10-03 19:57:22 - train: epoch 0046, iter [01080, 01251], lr: 0.000394, loss: 0.0892
2022-10-03 19:57:50 - train: epoch 0046, iter [01090, 01251], lr: 0.000394, loss: 0.0924
2022-10-03 19:58:18 - train: epoch 0046, iter [01100, 01251], lr: 0.000394, loss: 0.0860
2022-10-03 19:58:46 - train: epoch 0046, iter [01110, 01251], lr: 0.000394, loss: 0.0965
2022-10-03 19:59:14 - train: epoch 0046, iter [01120, 01251], lr: 0.000394, loss: 0.0953
2022-10-03 19:59:42 - train: epoch 0046, iter [01130, 01251], lr: 0.000394, loss: 0.0942
2022-10-03 20:00:10 - train: epoch 0046, iter [01140, 01251], lr: 0.000394, loss: 0.0906
2022-10-03 20:00:38 - train: epoch 0046, iter [01150, 01251], lr: 0.000394, loss: 0.0926
2022-10-03 20:01:07 - train: epoch 0046, iter [01160, 01251], lr: 0.000393, loss: 0.0893
2022-10-03 20:01:35 - train: epoch 0046, iter [01170, 01251], lr: 0.000393, loss: 0.0915
2022-10-03 20:02:03 - train: epoch 0046, iter [01180, 01251], lr: 0.000393, loss: 0.0880
2022-10-03 20:02:31 - train: epoch 0046, iter [01190, 01251], lr: 0.000393, loss: 0.1067
2022-10-03 20:02:59 - train: epoch 0046, iter [01200, 01251], lr: 0.000393, loss: 0.0911
2022-10-03 20:03:28 - train: epoch 0046, iter [01210, 01251], lr: 0.000393, loss: 0.0782
2022-10-03 20:03:56 - train: epoch 0046, iter [01220, 01251], lr: 0.000393, loss: 0.0885
2022-10-03 20:04:24 - train: epoch 0046, iter [01230, 01251], lr: 0.000393, loss: 0.0947
2022-10-03 20:04:52 - train: epoch 0046, iter [01240, 01251], lr: 0.000393, loss: 0.0953
2022-10-03 20:05:20 - train: epoch 0046, iter [01250, 01251], lr: 0.000393, loss: 0.0858
2022-10-03 20:05:24 - train: epoch 046, train_loss: 0.0909
2022-10-03 20:05:26 - until epoch: 046, best_loss: 0.0909
2022-10-03 20:05:26 - epoch 047 lr: 0.000393
2022-10-03 20:06:02 - train: epoch 0047, iter [00010, 01251], lr: 0.000393, loss: 0.0911
2022-10-03 20:06:30 - train: epoch 0047, iter [00020, 01251], lr: 0.000393, loss: 0.0938
2022-10-03 20:06:58 - train: epoch 0047, iter [00030, 01251], lr: 0.000392, loss: 0.0892
2022-10-03 20:07:26 - train: epoch 0047, iter [00040, 01251], lr: 0.000392, loss: 0.0940
2022-10-03 20:07:54 - train: epoch 0047, iter [00050, 01251], lr: 0.000392, loss: 0.0905
2022-10-03 20:08:22 - train: epoch 0047, iter [00060, 01251], lr: 0.000392, loss: 0.0980
2022-10-03 20:08:50 - train: epoch 0047, iter [00070, 01251], lr: 0.000392, loss: 0.0919
2022-10-03 20:09:18 - train: epoch 0047, iter [00080, 01251], lr: 0.000392, loss: 0.0942
2022-10-03 20:09:47 - train: epoch 0047, iter [00090, 01251], lr: 0.000392, loss: 0.0890
2022-10-03 20:10:15 - train: epoch 0047, iter [00100, 01251], lr: 0.000392, loss: 0.0945
2022-10-03 20:10:43 - train: epoch 0047, iter [00110, 01251], lr: 0.000392, loss: 0.0870
2022-10-03 20:11:11 - train: epoch 0047, iter [00120, 01251], lr: 0.000392, loss: 0.0915
2022-10-03 20:11:39 - train: epoch 0047, iter [00130, 01251], lr: 0.000392, loss: 0.0987
2022-10-03 20:12:07 - train: epoch 0047, iter [00140, 01251], lr: 0.000392, loss: 0.0920
2022-10-03 20:12:35 - train: epoch 0047, iter [00150, 01251], lr: 0.000392, loss: 0.0997
2022-10-03 20:13:03 - train: epoch 0047, iter [00160, 01251], lr: 0.000391, loss: 0.0934
2022-10-03 20:13:32 - train: epoch 0047, iter [00170, 01251], lr: 0.000391, loss: 0.0885
2022-10-03 20:14:00 - train: epoch 0047, iter [00180, 01251], lr: 0.000391, loss: 0.0937
2022-10-03 20:14:28 - train: epoch 0047, iter [00190, 01251], lr: 0.000391, loss: 0.0967
2022-10-03 20:14:56 - train: epoch 0047, iter [00200, 01251], lr: 0.000391, loss: 0.0833
2022-10-03 20:15:24 - train: epoch 0047, iter [00210, 01251], lr: 0.000391, loss: 0.0921
2022-10-03 20:15:52 - train: epoch 0047, iter [00220, 01251], lr: 0.000391, loss: 0.0942
2022-10-03 20:16:20 - train: epoch 0047, iter [00230, 01251], lr: 0.000391, loss: 0.0959
2022-10-03 20:16:49 - train: epoch 0047, iter [00240, 01251], lr: 0.000391, loss: 0.1050
2022-10-03 20:17:17 - train: epoch 0047, iter [00250, 01251], lr: 0.000391, loss: 0.0997
2022-10-03 20:17:45 - train: epoch 0047, iter [00260, 01251], lr: 0.000391, loss: 0.0940
2022-10-03 20:18:13 - train: epoch 0047, iter [00270, 01251], lr: 0.000391, loss: 0.0882
2022-10-03 20:18:41 - train: epoch 0047, iter [00280, 01251], lr: 0.000390, loss: 0.0898
2022-10-03 20:19:09 - train: epoch 0047, iter [00290, 01251], lr: 0.000390, loss: 0.0936
2022-10-03 20:19:37 - train: epoch 0047, iter [00300, 01251], lr: 0.000390, loss: 0.0939
2022-10-03 20:20:05 - train: epoch 0047, iter [00310, 01251], lr: 0.000390, loss: 0.0920
2022-10-03 20:20:33 - train: epoch 0047, iter [00320, 01251], lr: 0.000390, loss: 0.0886
2022-10-03 20:21:01 - train: epoch 0047, iter [00330, 01251], lr: 0.000390, loss: 0.0921
2022-10-03 20:21:29 - train: epoch 0047, iter [00340, 01251], lr: 0.000390, loss: 0.0876
2022-10-03 20:21:57 - train: epoch 0047, iter [00350, 01251], lr: 0.000390, loss: 0.0928
2022-10-03 20:22:26 - train: epoch 0047, iter [00360, 01251], lr: 0.000390, loss: 0.0873
2022-10-03 20:22:54 - train: epoch 0047, iter [00370, 01251], lr: 0.000390, loss: 0.0875
2022-10-03 20:23:22 - train: epoch 0047, iter [00380, 01251], lr: 0.000390, loss: 0.0887
2022-10-03 20:23:50 - train: epoch 0047, iter [00390, 01251], lr: 0.000390, loss: 0.0796
2022-10-03 20:24:18 - train: epoch 0047, iter [00400, 01251], lr: 0.000390, loss: 0.0945
2022-10-03 20:24:46 - train: epoch 0047, iter [00410, 01251], lr: 0.000389, loss: 0.1022
2022-10-03 20:25:14 - train: epoch 0047, iter [00420, 01251], lr: 0.000389, loss: 0.0941
2022-10-03 20:25:42 - train: epoch 0047, iter [00430, 01251], lr: 0.000389, loss: 0.0922
2022-10-03 20:26:10 - train: epoch 0047, iter [00440, 01251], lr: 0.000389, loss: 0.0998
2022-10-03 20:26:39 - train: epoch 0047, iter [00450, 01251], lr: 0.000389, loss: 0.0968
2022-10-03 20:27:07 - train: epoch 0047, iter [00460, 01251], lr: 0.000389, loss: 0.0909
2022-10-03 20:27:35 - train: epoch 0047, iter [00470, 01251], lr: 0.000389, loss: 0.0897
2022-10-03 20:28:03 - train: epoch 0047, iter [00480, 01251], lr: 0.000389, loss: 0.0962
2022-10-03 20:28:31 - train: epoch 0047, iter [00490, 01251], lr: 0.000389, loss: 0.0824
2022-10-03 20:28:59 - train: epoch 0047, iter [00500, 01251], lr: 0.000389, loss: 0.1022
2022-10-03 20:29:27 - train: epoch 0047, iter [00510, 01251], lr: 0.000389, loss: 0.0911
2022-10-03 20:29:55 - train: epoch 0047, iter [00520, 01251], lr: 0.000389, loss: 0.0921
2022-10-03 20:30:23 - train: epoch 0047, iter [00530, 01251], lr: 0.000388, loss: 0.0925
2022-10-03 20:30:51 - train: epoch 0047, iter [00540, 01251], lr: 0.000388, loss: 0.0938
2022-10-03 20:31:19 - train: epoch 0047, iter [00550, 01251], lr: 0.000388, loss: 0.0883
2022-10-03 20:31:47 - train: epoch 0047, iter [00560, 01251], lr: 0.000388, loss: 0.0920
2022-10-03 20:32:15 - train: epoch 0047, iter [00570, 01251], lr: 0.000388, loss: 0.0920
2022-10-03 20:32:43 - train: epoch 0047, iter [00580, 01251], lr: 0.000388, loss: 0.0963
2022-10-03 20:33:11 - train: epoch 0047, iter [00590, 01251], lr: 0.000388, loss: 0.0966
2022-10-03 20:33:39 - train: epoch 0047, iter [00600, 01251], lr: 0.000388, loss: 0.0918
2022-10-03 20:34:07 - train: epoch 0047, iter [00610, 01251], lr: 0.000388, loss: 0.0924
2022-10-03 20:34:35 - train: epoch 0047, iter [00620, 01251], lr: 0.000388, loss: 0.0865
2022-10-03 20:35:03 - train: epoch 0047, iter [00630, 01251], lr: 0.000388, loss: 0.0934
2022-10-03 20:35:31 - train: epoch 0047, iter [00640, 01251], lr: 0.000388, loss: 0.0921
2022-10-03 20:36:00 - train: epoch 0047, iter [00650, 01251], lr: 0.000388, loss: 0.0865
2022-10-03 20:36:28 - train: epoch 0047, iter [00660, 01251], lr: 0.000387, loss: 0.0840
2022-10-03 20:36:56 - train: epoch 0047, iter [00670, 01251], lr: 0.000387, loss: 0.0870
2022-10-03 20:37:24 - train: epoch 0047, iter [00680, 01251], lr: 0.000387, loss: 0.0880
2022-10-03 20:37:52 - train: epoch 0047, iter [00690, 01251], lr: 0.000387, loss: 0.0829
2022-10-03 20:38:20 - train: epoch 0047, iter [00700, 01251], lr: 0.000387, loss: 0.0897
2022-10-03 20:38:48 - train: epoch 0047, iter [00710, 01251], lr: 0.000387, loss: 0.0929
2022-10-03 20:39:16 - train: epoch 0047, iter [00720, 01251], lr: 0.000387, loss: 0.0960
2022-10-03 20:39:44 - train: epoch 0047, iter [00730, 01251], lr: 0.000387, loss: 0.0882
2022-10-03 20:40:13 - train: epoch 0047, iter [00740, 01251], lr: 0.000387, loss: 0.0990
2022-10-03 20:40:41 - train: epoch 0047, iter [00750, 01251], lr: 0.000387, loss: 0.0895
2022-10-03 20:41:09 - train: epoch 0047, iter [00760, 01251], lr: 0.000387, loss: 0.0893
2022-10-03 20:41:37 - train: epoch 0047, iter [00770, 01251], lr: 0.000387, loss: 0.0879
2022-10-03 20:42:05 - train: epoch 0047, iter [00780, 01251], lr: 0.000386, loss: 0.0924
2022-10-03 20:42:33 - train: epoch 0047, iter [00790, 01251], lr: 0.000386, loss: 0.0921
2022-10-03 20:43:01 - train: epoch 0047, iter [00800, 01251], lr: 0.000386, loss: 0.0886
2022-10-03 20:43:29 - train: epoch 0047, iter [00810, 01251], lr: 0.000386, loss: 0.0921
2022-10-03 20:43:57 - train: epoch 0047, iter [00820, 01251], lr: 0.000386, loss: 0.0945
2022-10-03 20:44:26 - train: epoch 0047, iter [00830, 01251], lr: 0.000386, loss: 0.1054
2022-10-03 20:44:54 - train: epoch 0047, iter [00840, 01251], lr: 0.000386, loss: 0.0867
2022-10-03 20:45:22 - train: epoch 0047, iter [00850, 01251], lr: 0.000386, loss: 0.0879
2022-10-03 20:45:50 - train: epoch 0047, iter [00860, 01251], lr: 0.000386, loss: 0.0851
2022-10-03 20:46:18 - train: epoch 0047, iter [00870, 01251], lr: 0.000386, loss: 0.0875
2022-10-03 20:46:47 - train: epoch 0047, iter [00880, 01251], lr: 0.000386, loss: 0.0898
2022-10-03 20:47:15 - train: epoch 0047, iter [00890, 01251], lr: 0.000386, loss: 0.0907
2022-10-03 20:47:43 - train: epoch 0047, iter [00900, 01251], lr: 0.000386, loss: 0.0957
2022-10-03 20:48:12 - train: epoch 0047, iter [00910, 01251], lr: 0.000385, loss: 0.0893
2022-10-03 20:48:40 - train: epoch 0047, iter [00920, 01251], lr: 0.000385, loss: 0.0984
2022-10-03 20:49:08 - train: epoch 0047, iter [00930, 01251], lr: 0.000385, loss: 0.0885
2022-10-03 20:49:36 - train: epoch 0047, iter [00940, 01251], lr: 0.000385, loss: 0.0915
2022-10-03 20:50:05 - train: epoch 0047, iter [00950, 01251], lr: 0.000385, loss: 0.0942
2022-10-03 20:50:33 - train: epoch 0047, iter [00960, 01251], lr: 0.000385, loss: 0.0829
2022-10-03 20:51:01 - train: epoch 0047, iter [00970, 01251], lr: 0.000385, loss: 0.0926
2022-10-03 20:51:29 - train: epoch 0047, iter [00980, 01251], lr: 0.000385, loss: 0.0876
2022-10-03 20:51:57 - train: epoch 0047, iter [00990, 01251], lr: 0.000385, loss: 0.0920
2022-10-03 20:52:25 - train: epoch 0047, iter [01000, 01251], lr: 0.000385, loss: 0.0975
2022-10-03 20:52:53 - train: epoch 0047, iter [01010, 01251], lr: 0.000385, loss: 0.0868
2022-10-03 20:53:22 - train: epoch 0047, iter [01020, 01251], lr: 0.000385, loss: 0.0954
2022-10-03 20:53:50 - train: epoch 0047, iter [01030, 01251], lr: 0.000384, loss: 0.0882
2022-10-03 20:54:18 - train: epoch 0047, iter [01040, 01251], lr: 0.000384, loss: 0.0943
2022-10-03 20:54:46 - train: epoch 0047, iter [01050, 01251], lr: 0.000384, loss: 0.0913
2022-10-03 20:55:14 - train: epoch 0047, iter [01060, 01251], lr: 0.000384, loss: 0.0885
2022-10-03 20:55:42 - train: epoch 0047, iter [01070, 01251], lr: 0.000384, loss: 0.0856
2022-10-03 20:56:11 - train: epoch 0047, iter [01080, 01251], lr: 0.000384, loss: 0.0935
2022-10-03 20:56:39 - train: epoch 0047, iter [01090, 01251], lr: 0.000384, loss: 0.0883
2022-10-03 20:57:07 - train: epoch 0047, iter [01100, 01251], lr: 0.000384, loss: 0.0952
2022-10-03 20:57:35 - train: epoch 0047, iter [01110, 01251], lr: 0.000384, loss: 0.0908
2022-10-03 20:58:04 - train: epoch 0047, iter [01120, 01251], lr: 0.000384, loss: 0.0853
2022-10-03 20:58:32 - train: epoch 0047, iter [01130, 01251], lr: 0.000384, loss: 0.0895
2022-10-03 20:59:00 - train: epoch 0047, iter [01140, 01251], lr: 0.000384, loss: 0.0878
2022-10-03 20:59:28 - train: epoch 0047, iter [01150, 01251], lr: 0.000384, loss: 0.0891
2022-10-03 20:59:56 - train: epoch 0047, iter [01160, 01251], lr: 0.000383, loss: 0.0907
2022-10-03 21:00:24 - train: epoch 0047, iter [01170, 01251], lr: 0.000383, loss: 0.0949
2022-10-03 21:00:52 - train: epoch 0047, iter [01180, 01251], lr: 0.000383, loss: 0.0920
2022-10-03 21:01:20 - train: epoch 0047, iter [01190, 01251], lr: 0.000383, loss: 0.0882
2022-10-03 21:01:48 - train: epoch 0047, iter [01200, 01251], lr: 0.000383, loss: 0.0950
2022-10-03 21:02:17 - train: epoch 0047, iter [01210, 01251], lr: 0.000383, loss: 0.0825
2022-10-03 21:02:45 - train: epoch 0047, iter [01220, 01251], lr: 0.000383, loss: 0.0825
2022-10-03 21:03:13 - train: epoch 0047, iter [01230, 01251], lr: 0.000383, loss: 0.0982
2022-10-03 21:03:42 - train: epoch 0047, iter [01240, 01251], lr: 0.000383, loss: 0.0944
2022-10-03 21:04:10 - train: epoch 0047, iter [01250, 01251], lr: 0.000383, loss: 0.0828
2022-10-03 21:04:14 - train: epoch 047, train_loss: 0.0907
2022-10-03 21:04:16 - until epoch: 047, best_loss: 0.0907
2022-10-03 21:04:16 - epoch 048 lr: 0.000383
2022-10-03 21:04:51 - train: epoch 0048, iter [00010, 01251], lr: 0.000383, loss: 0.0909
2022-10-03 21:05:20 - train: epoch 0048, iter [00020, 01251], lr: 0.000383, loss: 0.0883
2022-10-03 21:05:48 - train: epoch 0048, iter [00030, 01251], lr: 0.000382, loss: 0.0984
2022-10-03 21:06:16 - train: epoch 0048, iter [00040, 01251], lr: 0.000382, loss: 0.0897
2022-10-03 21:06:45 - train: epoch 0048, iter [00050, 01251], lr: 0.000382, loss: 0.0904
2022-10-03 21:07:13 - train: epoch 0048, iter [00060, 01251], lr: 0.000382, loss: 0.0891
2022-10-03 21:07:41 - train: epoch 0048, iter [00070, 01251], lr: 0.000382, loss: 0.0907
2022-10-03 21:08:10 - train: epoch 0048, iter [00080, 01251], lr: 0.000382, loss: 0.0922
2022-10-03 21:08:38 - train: epoch 0048, iter [00090, 01251], lr: 0.000382, loss: 0.1001
2022-10-03 21:09:06 - train: epoch 0048, iter [00100, 01251], lr: 0.000382, loss: 0.0874
2022-10-03 21:09:34 - train: epoch 0048, iter [00110, 01251], lr: 0.000382, loss: 0.0918
2022-10-03 21:10:03 - train: epoch 0048, iter [00120, 01251], lr: 0.000382, loss: 0.0888
2022-10-03 21:10:31 - train: epoch 0048, iter [00130, 01251], lr: 0.000382, loss: 0.0877
2022-10-03 21:10:59 - train: epoch 0048, iter [00140, 01251], lr: 0.000382, loss: 0.0832
2022-10-03 21:11:27 - train: epoch 0048, iter [00150, 01251], lr: 0.000381, loss: 0.0892
2022-10-03 21:11:55 - train: epoch 0048, iter [00160, 01251], lr: 0.000381, loss: 0.0920
2022-10-03 21:12:23 - train: epoch 0048, iter [00170, 01251], lr: 0.000381, loss: 0.0904
2022-10-03 21:12:51 - train: epoch 0048, iter [00180, 01251], lr: 0.000381, loss: 0.0978
2022-10-03 21:13:19 - train: epoch 0048, iter [00190, 01251], lr: 0.000381, loss: 0.0769
2022-10-03 21:13:48 - train: epoch 0048, iter [00200, 01251], lr: 0.000381, loss: 0.0899
2022-10-03 21:14:16 - train: epoch 0048, iter [00210, 01251], lr: 0.000381, loss: 0.0890
2022-10-03 21:14:44 - train: epoch 0048, iter [00220, 01251], lr: 0.000381, loss: 0.0884
2022-10-03 21:15:12 - train: epoch 0048, iter [00230, 01251], lr: 0.000381, loss: 0.0924
2022-10-03 21:15:40 - train: epoch 0048, iter [00240, 01251], lr: 0.000381, loss: 0.0919
2022-10-03 21:16:08 - train: epoch 0048, iter [00250, 01251], lr: 0.000381, loss: 0.0902
2022-10-03 21:16:36 - train: epoch 0048, iter [00260, 01251], lr: 0.000381, loss: 0.0931
2022-10-03 21:17:04 - train: epoch 0048, iter [00270, 01251], lr: 0.000381, loss: 0.0882
2022-10-03 21:17:32 - train: epoch 0048, iter [00280, 01251], lr: 0.000380, loss: 0.0902
2022-10-03 21:18:01 - train: epoch 0048, iter [00290, 01251], lr: 0.000380, loss: 0.0927
2022-10-03 21:18:29 - train: epoch 0048, iter [00300, 01251], lr: 0.000380, loss: 0.0933
2022-10-03 21:18:57 - train: epoch 0048, iter [00310, 01251], lr: 0.000380, loss: 0.0961
2022-10-03 21:19:25 - train: epoch 0048, iter [00320, 01251], lr: 0.000380, loss: 0.0898
2022-10-03 21:19:53 - train: epoch 0048, iter [00330, 01251], lr: 0.000380, loss: 0.0892
2022-10-03 21:20:21 - train: epoch 0048, iter [00340, 01251], lr: 0.000380, loss: 0.0902
2022-10-03 21:20:49 - train: epoch 0048, iter [00350, 01251], lr: 0.000380, loss: 0.0947
2022-10-03 21:21:17 - train: epoch 0048, iter [00360, 01251], lr: 0.000380, loss: 0.0932
2022-10-03 21:21:46 - train: epoch 0048, iter [00370, 01251], lr: 0.000380, loss: 0.0929
2022-10-03 21:22:14 - train: epoch 0048, iter [00380, 01251], lr: 0.000380, loss: 0.0917
2022-10-03 21:22:42 - train: epoch 0048, iter [00390, 01251], lr: 0.000380, loss: 0.0863
2022-10-03 21:23:10 - train: epoch 0048, iter [00400, 01251], lr: 0.000379, loss: 0.0863
2022-10-03 21:23:38 - train: epoch 0048, iter [00410, 01251], lr: 0.000379, loss: 0.0985
2022-10-03 21:24:06 - train: epoch 0048, iter [00420, 01251], lr: 0.000379, loss: 0.0955
2022-10-03 21:24:34 - train: epoch 0048, iter [00430, 01251], lr: 0.000379, loss: 0.0903
2022-10-03 21:25:02 - train: epoch 0048, iter [00440, 01251], lr: 0.000379, loss: 0.0913
2022-10-03 21:25:31 - train: epoch 0048, iter [00450, 01251], lr: 0.000379, loss: 0.0926
2022-10-03 21:25:59 - train: epoch 0048, iter [00460, 01251], lr: 0.000379, loss: 0.0874
2022-10-03 21:26:27 - train: epoch 0048, iter [00470, 01251], lr: 0.000379, loss: 0.0893
2022-10-03 21:26:55 - train: epoch 0048, iter [00480, 01251], lr: 0.000379, loss: 0.0821
2022-10-03 21:27:23 - train: epoch 0048, iter [00490, 01251], lr: 0.000379, loss: 0.0952
2022-10-03 21:27:51 - train: epoch 0048, iter [00500, 01251], lr: 0.000379, loss: 0.0939
2022-10-03 21:28:19 - train: epoch 0048, iter [00510, 01251], lr: 0.000379, loss: 0.0900
2022-10-03 21:28:48 - train: epoch 0048, iter [00520, 01251], lr: 0.000378, loss: 0.0878
2022-10-03 21:29:16 - train: epoch 0048, iter [00530, 01251], lr: 0.000378, loss: 0.0903
2022-10-03 21:29:44 - train: epoch 0048, iter [00540, 01251], lr: 0.000378, loss: 0.0895
2022-10-03 21:30:12 - train: epoch 0048, iter [00550, 01251], lr: 0.000378, loss: 0.0936
2022-10-03 21:30:40 - train: epoch 0048, iter [00560, 01251], lr: 0.000378, loss: 0.0902
2022-10-03 21:31:08 - train: epoch 0048, iter [00570, 01251], lr: 0.000378, loss: 0.0894
2022-10-03 21:31:36 - train: epoch 0048, iter [00580, 01251], lr: 0.000378, loss: 0.0878
2022-10-03 21:32:04 - train: epoch 0048, iter [00590, 01251], lr: 0.000378, loss: 0.0847
2022-10-03 21:32:32 - train: epoch 0048, iter [00600, 01251], lr: 0.000378, loss: 0.0860
2022-10-03 21:33:00 - train: epoch 0048, iter [00610, 01251], lr: 0.000378, loss: 0.0880
2022-10-03 21:33:28 - train: epoch 0048, iter [00620, 01251], lr: 0.000378, loss: 0.0932
2022-10-03 21:33:57 - train: epoch 0048, iter [00630, 01251], lr: 0.000378, loss: 0.0913
2022-10-03 21:34:25 - train: epoch 0048, iter [00640, 01251], lr: 0.000378, loss: 0.0916
2022-10-03 21:34:53 - train: epoch 0048, iter [00650, 01251], lr: 0.000377, loss: 0.0937
2022-10-03 21:35:21 - train: epoch 0048, iter [00660, 01251], lr: 0.000377, loss: 0.0936
2022-10-03 21:35:49 - train: epoch 0048, iter [00670, 01251], lr: 0.000377, loss: 0.0874
2022-10-03 21:36:17 - train: epoch 0048, iter [00680, 01251], lr: 0.000377, loss: 0.0899
2022-10-03 21:36:45 - train: epoch 0048, iter [00690, 01251], lr: 0.000377, loss: 0.0961
2022-10-03 21:37:13 - train: epoch 0048, iter [00700, 01251], lr: 0.000377, loss: 0.0895
2022-10-03 21:37:41 - train: epoch 0048, iter [00710, 01251], lr: 0.000377, loss: 0.0951
2022-10-03 21:38:10 - train: epoch 0048, iter [00720, 01251], lr: 0.000377, loss: 0.0909
2022-10-03 21:38:38 - train: epoch 0048, iter [00730, 01251], lr: 0.000377, loss: 0.0882
2022-10-03 21:39:06 - train: epoch 0048, iter [00740, 01251], lr: 0.000377, loss: 0.0877
2022-10-03 21:39:34 - train: epoch 0048, iter [00750, 01251], lr: 0.000377, loss: 0.0844
2022-10-03 21:40:02 - train: epoch 0048, iter [00760, 01251], lr: 0.000377, loss: 0.1016
2022-10-03 21:40:30 - train: epoch 0048, iter [00770, 01251], lr: 0.000376, loss: 0.0945
2022-10-03 21:40:58 - train: epoch 0048, iter [00780, 01251], lr: 0.000376, loss: 0.0927
2022-10-03 21:41:27 - train: epoch 0048, iter [00790, 01251], lr: 0.000376, loss: 0.0905
2022-10-03 21:41:55 - train: epoch 0048, iter [00800, 01251], lr: 0.000376, loss: 0.0875
2022-10-03 21:42:23 - train: epoch 0048, iter [00810, 01251], lr: 0.000376, loss: 0.0985
2022-10-03 21:42:51 - train: epoch 0048, iter [00820, 01251], lr: 0.000376, loss: 0.0954
2022-10-03 21:43:19 - train: epoch 0048, iter [00830, 01251], lr: 0.000376, loss: 0.0894
2022-10-03 21:43:48 - train: epoch 0048, iter [00840, 01251], lr: 0.000376, loss: 0.0863
2022-10-03 21:44:16 - train: epoch 0048, iter [00850, 01251], lr: 0.000376, loss: 0.0936
2022-10-03 21:44:44 - train: epoch 0048, iter [00860, 01251], lr: 0.000376, loss: 0.1003
2022-10-03 21:45:12 - train: epoch 0048, iter [00870, 01251], lr: 0.000376, loss: 0.0913
2022-10-03 21:45:40 - train: epoch 0048, iter [00880, 01251], lr: 0.000376, loss: 0.0982
2022-10-03 21:46:08 - train: epoch 0048, iter [00890, 01251], lr: 0.000376, loss: 0.0929
2022-10-03 21:46:36 - train: epoch 0048, iter [00900, 01251], lr: 0.000375, loss: 0.0874
2022-10-03 21:47:04 - train: epoch 0048, iter [00910, 01251], lr: 0.000375, loss: 0.0863
2022-10-03 21:47:32 - train: epoch 0048, iter [00920, 01251], lr: 0.000375, loss: 0.0875
2022-10-03 21:48:00 - train: epoch 0048, iter [00930, 01251], lr: 0.000375, loss: 0.0845
2022-10-03 21:48:29 - train: epoch 0048, iter [00940, 01251], lr: 0.000375, loss: 0.0910
2022-10-03 21:48:57 - train: epoch 0048, iter [00950, 01251], lr: 0.000375, loss: 0.1006
2022-10-03 21:49:25 - train: epoch 0048, iter [00960, 01251], lr: 0.000375, loss: 0.0921
2022-10-03 21:49:53 - train: epoch 0048, iter [00970, 01251], lr: 0.000375, loss: 0.0944
2022-10-03 21:50:21 - train: epoch 0048, iter [00980, 01251], lr: 0.000375, loss: 0.0949
2022-10-03 21:50:49 - train: epoch 0048, iter [00990, 01251], lr: 0.000375, loss: 0.0856
2022-10-03 21:51:18 - train: epoch 0048, iter [01000, 01251], lr: 0.000375, loss: 0.1014
2022-10-03 21:51:46 - train: epoch 0048, iter [01010, 01251], lr: 0.000375, loss: 0.0879
2022-10-03 21:52:14 - train: epoch 0048, iter [01020, 01251], lr: 0.000374, loss: 0.0799
2022-10-03 21:52:42 - train: epoch 0048, iter [01030, 01251], lr: 0.000374, loss: 0.0867
2022-10-03 21:53:10 - train: epoch 0048, iter [01040, 01251], lr: 0.000374, loss: 0.0844
2022-10-03 21:53:38 - train: epoch 0048, iter [01050, 01251], lr: 0.000374, loss: 0.0972
2022-10-03 21:54:06 - train: epoch 0048, iter [01060, 01251], lr: 0.000374, loss: 0.0937
2022-10-03 21:54:34 - train: epoch 0048, iter [01070, 01251], lr: 0.000374, loss: 0.0891
2022-10-03 21:55:02 - train: epoch 0048, iter [01080, 01251], lr: 0.000374, loss: 0.0966
2022-10-03 21:55:30 - train: epoch 0048, iter [01090, 01251], lr: 0.000374, loss: 0.0914
2022-10-03 21:55:58 - train: epoch 0048, iter [01100, 01251], lr: 0.000374, loss: 0.0944
2022-10-03 21:56:26 - train: epoch 0048, iter [01110, 01251], lr: 0.000374, loss: 0.0941
2022-10-03 21:56:55 - train: epoch 0048, iter [01120, 01251], lr: 0.000374, loss: 0.0897
2022-10-03 21:57:23 - train: epoch 0048, iter [01130, 01251], lr: 0.000374, loss: 0.0928
2022-10-03 21:57:51 - train: epoch 0048, iter [01140, 01251], lr: 0.000373, loss: 0.0892
2022-10-03 21:58:19 - train: epoch 0048, iter [01150, 01251], lr: 0.000373, loss: 0.0906
2022-10-03 21:58:47 - train: epoch 0048, iter [01160, 01251], lr: 0.000373, loss: 0.0892
2022-10-03 21:59:15 - train: epoch 0048, iter [01170, 01251], lr: 0.000373, loss: 0.0944
2022-10-03 21:59:44 - train: epoch 0048, iter [01180, 01251], lr: 0.000373, loss: 0.0938
2022-10-03 22:00:12 - train: epoch 0048, iter [01190, 01251], lr: 0.000373, loss: 0.0812
2022-10-03 22:00:40 - train: epoch 0048, iter [01200, 01251], lr: 0.000373, loss: 0.0924
2022-10-03 22:01:08 - train: epoch 0048, iter [01210, 01251], lr: 0.000373, loss: 0.0895
2022-10-03 22:01:36 - train: epoch 0048, iter [01220, 01251], lr: 0.000373, loss: 0.0877
2022-10-03 22:02:05 - train: epoch 0048, iter [01230, 01251], lr: 0.000373, loss: 0.0852
2022-10-03 22:02:33 - train: epoch 0048, iter [01240, 01251], lr: 0.000373, loss: 0.0839
2022-10-03 22:03:00 - train: epoch 0048, iter [01250, 01251], lr: 0.000373, loss: 0.0955
2022-10-03 22:03:05 - train: epoch 048, train_loss: 0.0903
2022-10-03 22:03:07 - until epoch: 048, best_loss: 0.0903
2022-10-03 22:03:07 - epoch 049 lr: 0.000373
2022-10-03 22:03:42 - train: epoch 0049, iter [00010, 01251], lr: 0.000372, loss: 0.0927
2022-10-03 22:04:10 - train: epoch 0049, iter [00020, 01251], lr: 0.000372, loss: 0.0818
2022-10-03 22:04:38 - train: epoch 0049, iter [00030, 01251], lr: 0.000372, loss: 0.0858
2022-10-03 22:05:07 - train: epoch 0049, iter [00040, 01251], lr: 0.000372, loss: 0.0865
2022-10-03 22:05:35 - train: epoch 0049, iter [00050, 01251], lr: 0.000372, loss: 0.0869
2022-10-03 22:06:03 - train: epoch 0049, iter [00060, 01251], lr: 0.000372, loss: 0.0889
2022-10-03 22:06:31 - train: epoch 0049, iter [00070, 01251], lr: 0.000372, loss: 0.0892
2022-10-03 22:06:59 - train: epoch 0049, iter [00080, 01251], lr: 0.000372, loss: 0.0845
2022-10-03 22:07:28 - train: epoch 0049, iter [00090, 01251], lr: 0.000372, loss: 0.0884
2022-10-03 22:07:56 - train: epoch 0049, iter [00100, 01251], lr: 0.000372, loss: 0.0922
2022-10-03 22:08:25 - train: epoch 0049, iter [00110, 01251], lr: 0.000372, loss: 0.0943
2022-10-03 22:08:53 - train: epoch 0049, iter [00120, 01251], lr: 0.000372, loss: 0.0928
2022-10-03 22:09:21 - train: epoch 0049, iter [00130, 01251], lr: 0.000372, loss: 0.0940
2022-10-03 22:09:49 - train: epoch 0049, iter [00140, 01251], lr: 0.000371, loss: 0.0885
2022-10-03 22:10:18 - train: epoch 0049, iter [00150, 01251], lr: 0.000371, loss: 0.0848
2022-10-03 22:10:46 - train: epoch 0049, iter [00160, 01251], lr: 0.000371, loss: 0.0908
2022-10-03 22:11:14 - train: epoch 0049, iter [00170, 01251], lr: 0.000371, loss: 0.0889
2022-10-03 22:11:43 - train: epoch 0049, iter [00180, 01251], lr: 0.000371, loss: 0.0865
2022-10-03 22:12:11 - train: epoch 0049, iter [00190, 01251], lr: 0.000371, loss: 0.0872
2022-10-03 22:12:39 - train: epoch 0049, iter [00200, 01251], lr: 0.000371, loss: 0.0904
2022-10-03 22:13:07 - train: epoch 0049, iter [00210, 01251], lr: 0.000371, loss: 0.0805
2022-10-03 22:13:36 - train: epoch 0049, iter [00220, 01251], lr: 0.000371, loss: 0.0885
2022-10-03 22:14:04 - train: epoch 0049, iter [00230, 01251], lr: 0.000371, loss: 0.0875
2022-10-03 22:14:32 - train: epoch 0049, iter [00240, 01251], lr: 0.000371, loss: 0.0859
2022-10-03 22:15:00 - train: epoch 0049, iter [00250, 01251], lr: 0.000371, loss: 0.0897
2022-10-03 22:15:28 - train: epoch 0049, iter [00260, 01251], lr: 0.000370, loss: 0.0809
2022-10-03 22:15:56 - train: epoch 0049, iter [00270, 01251], lr: 0.000370, loss: 0.0899
2022-10-03 22:16:24 - train: epoch 0049, iter [00280, 01251], lr: 0.000370, loss: 0.0908
2022-10-03 22:16:53 - train: epoch 0049, iter [00290, 01251], lr: 0.000370, loss: 0.1043
2022-10-03 22:17:21 - train: epoch 0049, iter [00300, 01251], lr: 0.000370, loss: 0.0937
2022-10-03 22:17:49 - train: epoch 0049, iter [00310, 01251], lr: 0.000370, loss: 0.0842
2022-10-03 22:18:17 - train: epoch 0049, iter [00320, 01251], lr: 0.000370, loss: 0.0893
2022-10-03 22:18:45 - train: epoch 0049, iter [00330, 01251], lr: 0.000370, loss: 0.0790
2022-10-03 22:19:13 - train: epoch 0049, iter [00340, 01251], lr: 0.000370, loss: 0.0911
2022-10-03 22:19:41 - train: epoch 0049, iter [00350, 01251], lr: 0.000370, loss: 0.0892
2022-10-03 22:20:09 - train: epoch 0049, iter [00360, 01251], lr: 0.000370, loss: 0.0877
2022-10-03 22:20:37 - train: epoch 0049, iter [00370, 01251], lr: 0.000370, loss: 0.0918
2022-10-03 22:21:05 - train: epoch 0049, iter [00380, 01251], lr: 0.000369, loss: 0.0944
2022-10-03 22:21:33 - train: epoch 0049, iter [00390, 01251], lr: 0.000369, loss: 0.0924
2022-10-03 22:22:01 - train: epoch 0049, iter [00400, 01251], lr: 0.000369, loss: 0.0919
2022-10-03 22:22:29 - train: epoch 0049, iter [00410, 01251], lr: 0.000369, loss: 0.0926
2022-10-03 22:22:57 - train: epoch 0049, iter [00420, 01251], lr: 0.000369, loss: 0.0884
2022-10-03 22:23:26 - train: epoch 0049, iter [00430, 01251], lr: 0.000369, loss: 0.0835
2022-10-03 22:23:54 - train: epoch 0049, iter [00440, 01251], lr: 0.000369, loss: 0.0891
2022-10-03 22:24:22 - train: epoch 0049, iter [00450, 01251], lr: 0.000369, loss: 0.1012
2022-10-03 22:24:51 - train: epoch 0049, iter [00460, 01251], lr: 0.000369, loss: 0.0956
2022-10-03 22:25:19 - train: epoch 0049, iter [00470, 01251], lr: 0.000369, loss: 0.0833
2022-10-03 22:25:47 - train: epoch 0049, iter [00480, 01251], lr: 0.000369, loss: 0.0919
2022-10-03 22:26:16 - train: epoch 0049, iter [00490, 01251], lr: 0.000369, loss: 0.0887
2022-10-03 22:26:44 - train: epoch 0049, iter [00500, 01251], lr: 0.000369, loss: 0.0871
2022-10-03 22:27:12 - train: epoch 0049, iter [00510, 01251], lr: 0.000368, loss: 0.1004
2022-10-03 22:27:40 - train: epoch 0049, iter [00520, 01251], lr: 0.000368, loss: 0.0920
2022-10-03 22:28:08 - train: epoch 0049, iter [00530, 01251], lr: 0.000368, loss: 0.0929
2022-10-03 22:28:36 - train: epoch 0049, iter [00540, 01251], lr: 0.000368, loss: 0.0883
2022-10-03 22:29:04 - train: epoch 0049, iter [00550, 01251], lr: 0.000368, loss: 0.0879
2022-10-03 22:29:32 - train: epoch 0049, iter [00560, 01251], lr: 0.000368, loss: 0.0854
2022-10-03 22:30:00 - train: epoch 0049, iter [00570, 01251], lr: 0.000368, loss: 0.0826
2022-10-03 22:30:28 - train: epoch 0049, iter [00580, 01251], lr: 0.000368, loss: 0.0970
2022-10-03 22:30:57 - train: epoch 0049, iter [00590, 01251], lr: 0.000368, loss: 0.0876
2022-10-03 22:31:25 - train: epoch 0049, iter [00600, 01251], lr: 0.000368, loss: 0.0930
2022-10-03 22:31:53 - train: epoch 0049, iter [00610, 01251], lr: 0.000368, loss: 0.0853
2022-10-03 22:32:21 - train: epoch 0049, iter [00620, 01251], lr: 0.000368, loss: 0.0879
2022-10-03 22:32:49 - train: epoch 0049, iter [00630, 01251], lr: 0.000367, loss: 0.0868
2022-10-03 22:33:17 - train: epoch 0049, iter [00640, 01251], lr: 0.000367, loss: 0.0993
2022-10-03 22:33:45 - train: epoch 0049, iter [00650, 01251], lr: 0.000367, loss: 0.0977
2022-10-03 22:34:14 - train: epoch 0049, iter [00660, 01251], lr: 0.000367, loss: 0.0959
2022-10-03 22:34:42 - train: epoch 0049, iter [00670, 01251], lr: 0.000367, loss: 0.0875
2022-10-03 22:35:10 - train: epoch 0049, iter [00680, 01251], lr: 0.000367, loss: 0.0935
2022-10-03 22:35:38 - train: epoch 0049, iter [00690, 01251], lr: 0.000367, loss: 0.0882
2022-10-03 22:36:06 - train: epoch 0049, iter [00700, 01251], lr: 0.000367, loss: 0.0888
2022-10-03 22:36:34 - train: epoch 0049, iter [00710, 01251], lr: 0.000367, loss: 0.0952
2022-10-03 22:37:02 - train: epoch 0049, iter [00720, 01251], lr: 0.000367, loss: 0.0907
2022-10-03 22:37:30 - train: epoch 0049, iter [00730, 01251], lr: 0.000367, loss: 0.0893
2022-10-03 22:37:58 - train: epoch 0049, iter [00740, 01251], lr: 0.000367, loss: 0.0840
2022-10-03 22:38:26 - train: epoch 0049, iter [00750, 01251], lr: 0.000366, loss: 0.0893
2022-10-03 22:38:54 - train: epoch 0049, iter [00760, 01251], lr: 0.000366, loss: 0.0917
2022-10-03 22:39:22 - train: epoch 0049, iter [00770, 01251], lr: 0.000366, loss: 0.0848
2022-10-03 22:39:50 - train: epoch 0049, iter [00780, 01251], lr: 0.000366, loss: 0.0887
2022-10-03 22:40:18 - train: epoch 0049, iter [00790, 01251], lr: 0.000366, loss: 0.0938
2022-10-03 22:40:46 - train: epoch 0049, iter [00800, 01251], lr: 0.000366, loss: 0.0954
2022-10-03 22:41:14 - train: epoch 0049, iter [00810, 01251], lr: 0.000366, loss: 0.0871
2022-10-03 22:41:43 - train: epoch 0049, iter [00820, 01251], lr: 0.000366, loss: 0.0847
2022-10-03 22:42:11 - train: epoch 0049, iter [00830, 01251], lr: 0.000366, loss: 0.0868
2022-10-03 22:42:39 - train: epoch 0049, iter [00840, 01251], lr: 0.000366, loss: 0.0861
2022-10-03 22:43:07 - train: epoch 0049, iter [00850, 01251], lr: 0.000366, loss: 0.0888
2022-10-03 22:43:35 - train: epoch 0049, iter [00860, 01251], lr: 0.000366, loss: 0.0861
2022-10-03 22:44:03 - train: epoch 0049, iter [00870, 01251], lr: 0.000365, loss: 0.0838
2022-10-03 22:44:31 - train: epoch 0049, iter [00880, 01251], lr: 0.000365, loss: 0.0990
2022-10-03 22:44:59 - train: epoch 0049, iter [00890, 01251], lr: 0.000365, loss: 0.0931
2022-10-03 22:45:27 - train: epoch 0049, iter [00900, 01251], lr: 0.000365, loss: 0.0926
2022-10-03 22:45:55 - train: epoch 0049, iter [00910, 01251], lr: 0.000365, loss: 0.0897
2022-10-03 22:46:23 - train: epoch 0049, iter [00920, 01251], lr: 0.000365, loss: 0.1014
2022-10-03 22:46:52 - train: epoch 0049, iter [00930, 01251], lr: 0.000365, loss: 0.0850
2022-10-03 22:47:20 - train: epoch 0049, iter [00940, 01251], lr: 0.000365, loss: 0.0933
2022-10-03 22:47:48 - train: epoch 0049, iter [00950, 01251], lr: 0.000365, loss: 0.0899
2022-10-03 22:48:16 - train: epoch 0049, iter [00960, 01251], lr: 0.000365, loss: 0.0852
2022-10-03 22:48:44 - train: epoch 0049, iter [00970, 01251], lr: 0.000365, loss: 0.0887
2022-10-03 22:49:12 - train: epoch 0049, iter [00980, 01251], lr: 0.000365, loss: 0.0930
2022-10-03 22:49:40 - train: epoch 0049, iter [00990, 01251], lr: 0.000365, loss: 0.0920
2022-10-03 22:50:08 - train: epoch 0049, iter [01000, 01251], lr: 0.000364, loss: 0.0887
2022-10-03 22:50:36 - train: epoch 0049, iter [01010, 01251], lr: 0.000364, loss: 0.0907
2022-10-03 22:51:04 - train: epoch 0049, iter [01020, 01251], lr: 0.000364, loss: 0.0934
2022-10-03 22:51:32 - train: epoch 0049, iter [01030, 01251], lr: 0.000364, loss: 0.0911
2022-10-03 22:52:00 - train: epoch 0049, iter [01040, 01251], lr: 0.000364, loss: 0.0868
2022-10-03 22:52:28 - train: epoch 0049, iter [01050, 01251], lr: 0.000364, loss: 0.0972
2022-10-03 22:52:57 - train: epoch 0049, iter [01060, 01251], lr: 0.000364, loss: 0.0906
2022-10-03 22:53:24 - train: epoch 0049, iter [01070, 01251], lr: 0.000364, loss: 0.0898
2022-10-03 22:53:52 - train: epoch 0049, iter [01080, 01251], lr: 0.000364, loss: 0.0941
2022-10-03 22:54:21 - train: epoch 0049, iter [01090, 01251], lr: 0.000364, loss: 0.0915
2022-10-03 22:54:49 - train: epoch 0049, iter [01100, 01251], lr: 0.000364, loss: 0.0966
2022-10-03 22:55:17 - train: epoch 0049, iter [01110, 01251], lr: 0.000364, loss: 0.0896
2022-10-03 22:55:45 - train: epoch 0049, iter [01120, 01251], lr: 0.000363, loss: 0.0827
2022-10-03 22:56:13 - train: epoch 0049, iter [01130, 01251], lr: 0.000363, loss: 0.0939
2022-10-03 22:56:41 - train: epoch 0049, iter [01140, 01251], lr: 0.000363, loss: 0.0955
2022-10-03 22:57:10 - train: epoch 0049, iter [01150, 01251], lr: 0.000363, loss: 0.0869
2022-10-03 22:57:38 - train: epoch 0049, iter [01160, 01251], lr: 0.000363, loss: 0.0874
2022-10-03 22:58:06 - train: epoch 0049, iter [01170, 01251], lr: 0.000363, loss: 0.0947
2022-10-03 22:58:34 - train: epoch 0049, iter [01180, 01251], lr: 0.000363, loss: 0.0862
2022-10-03 22:59:02 - train: epoch 0049, iter [01190, 01251], lr: 0.000363, loss: 0.0921
2022-10-03 22:59:31 - train: epoch 0049, iter [01200, 01251], lr: 0.000363, loss: 0.0955
2022-10-03 22:59:59 - train: epoch 0049, iter [01210, 01251], lr: 0.000363, loss: 0.0892
2022-10-03 23:00:27 - train: epoch 0049, iter [01220, 01251], lr: 0.000363, loss: 0.0990
2022-10-03 23:00:55 - train: epoch 0049, iter [01230, 01251], lr: 0.000363, loss: 0.0919
2022-10-03 23:01:23 - train: epoch 0049, iter [01240, 01251], lr: 0.000362, loss: 0.0897
2022-10-03 23:01:51 - train: epoch 0049, iter [01250, 01251], lr: 0.000362, loss: 0.0895
2022-10-03 23:01:55 - train: epoch 049, train_loss: 0.0901
2022-10-03 23:01:57 - until epoch: 049, best_loss: 0.0901
2022-10-03 23:01:57 - epoch 050 lr: 0.000362
2022-10-03 23:02:32 - train: epoch 0050, iter [00010, 01251], lr: 0.000362, loss: 0.0849
2022-10-03 23:03:01 - train: epoch 0050, iter [00020, 01251], lr: 0.000362, loss: 0.0873
2022-10-03 23:03:29 - train: epoch 0050, iter [00030, 01251], lr: 0.000362, loss: 0.0907
2022-10-03 23:03:57 - train: epoch 0050, iter [00040, 01251], lr: 0.000362, loss: 0.0839
2022-10-03 23:04:25 - train: epoch 0050, iter [00050, 01251], lr: 0.000362, loss: 0.0974
2022-10-03 23:04:53 - train: epoch 0050, iter [00060, 01251], lr: 0.000362, loss: 0.0887
2022-10-03 23:05:21 - train: epoch 0050, iter [00070, 01251], lr: 0.000362, loss: 0.0917
2022-10-03 23:05:49 - train: epoch 0050, iter [00080, 01251], lr: 0.000362, loss: 0.0898
2022-10-03 23:06:17 - train: epoch 0050, iter [00090, 01251], lr: 0.000362, loss: 0.0903
2022-10-03 23:06:45 - train: epoch 0050, iter [00100, 01251], lr: 0.000362, loss: 0.0839
2022-10-03 23:07:13 - train: epoch 0050, iter [00110, 01251], lr: 0.000361, loss: 0.0904
2022-10-03 23:07:41 - train: epoch 0050, iter [00120, 01251], lr: 0.000361, loss: 0.0887
2022-10-03 23:08:09 - train: epoch 0050, iter [00130, 01251], lr: 0.000361, loss: 0.0879
2022-10-03 23:08:37 - train: epoch 0050, iter [00140, 01251], lr: 0.000361, loss: 0.0925
2022-10-03 23:09:05 - train: epoch 0050, iter [00150, 01251], lr: 0.000361, loss: 0.0901
2022-10-03 23:09:33 - train: epoch 0050, iter [00160, 01251], lr: 0.000361, loss: 0.0880
2022-10-03 23:10:01 - train: epoch 0050, iter [00170, 01251], lr: 0.000361, loss: 0.0915
2022-10-03 23:10:29 - train: epoch 0050, iter [00180, 01251], lr: 0.000361, loss: 0.0831
2022-10-03 23:10:58 - train: epoch 0050, iter [00190, 01251], lr: 0.000361, loss: 0.0903
2022-10-03 23:11:26 - train: epoch 0050, iter [00200, 01251], lr: 0.000361, loss: 0.0955
2022-10-03 23:11:54 - train: epoch 0050, iter [00210, 01251], lr: 0.000361, loss: 0.0861
2022-10-03 23:12:22 - train: epoch 0050, iter [00220, 01251], lr: 0.000361, loss: 0.0858
2022-10-03 23:12:50 - train: epoch 0050, iter [00230, 01251], lr: 0.000360, loss: 0.0934
2022-10-03 23:13:18 - train: epoch 0050, iter [00240, 01251], lr: 0.000360, loss: 0.0930
2022-10-03 23:13:46 - train: epoch 0050, iter [00250, 01251], lr: 0.000360, loss: 0.0855
2022-10-03 23:14:14 - train: epoch 0050, iter [00260, 01251], lr: 0.000360, loss: 0.0898
2022-10-03 23:14:43 - train: epoch 0050, iter [00270, 01251], lr: 0.000360, loss: 0.0926
2022-10-03 23:15:11 - train: epoch 0050, iter [00280, 01251], lr: 0.000360, loss: 0.0887
2022-10-03 23:15:39 - train: epoch 0050, iter [00290, 01251], lr: 0.000360, loss: 0.0920
2022-10-03 23:16:07 - train: epoch 0050, iter [00300, 01251], lr: 0.000360, loss: 0.0839
2022-10-03 23:16:35 - train: epoch 0050, iter [00310, 01251], lr: 0.000360, loss: 0.0884
2022-10-03 23:17:03 - train: epoch 0050, iter [00320, 01251], lr: 0.000360, loss: 0.0877
2022-10-03 23:17:31 - train: epoch 0050, iter [00330, 01251], lr: 0.000360, loss: 0.0979
2022-10-03 23:17:59 - train: epoch 0050, iter [00340, 01251], lr: 0.000360, loss: 0.0859
2022-10-03 23:18:27 - train: epoch 0050, iter [00350, 01251], lr: 0.000360, loss: 0.0929
2022-10-03 23:18:55 - train: epoch 0050, iter [00360, 01251], lr: 0.000359, loss: 0.0951
2022-10-03 23:19:23 - train: epoch 0050, iter [00370, 01251], lr: 0.000359, loss: 0.0939
2022-10-03 23:19:51 - train: epoch 0050, iter [00380, 01251], lr: 0.000359, loss: 0.0869
2022-10-03 23:20:19 - train: epoch 0050, iter [00390, 01251], lr: 0.000359, loss: 0.0870
2022-10-03 23:20:47 - train: epoch 0050, iter [00400, 01251], lr: 0.000359, loss: 0.0893
2022-10-03 23:21:15 - train: epoch 0050, iter [00410, 01251], lr: 0.000359, loss: 0.0868
2022-10-03 23:21:43 - train: epoch 0050, iter [00420, 01251], lr: 0.000359, loss: 0.0979
2022-10-03 23:22:11 - train: epoch 0050, iter [00430, 01251], lr: 0.000359, loss: 0.0876
2022-10-03 23:22:39 - train: epoch 0050, iter [00440, 01251], lr: 0.000359, loss: 0.0924
2022-10-03 23:23:07 - train: epoch 0050, iter [00450, 01251], lr: 0.000359, loss: 0.0829
2022-10-03 23:23:35 - train: epoch 0050, iter [00460, 01251], lr: 0.000359, loss: 0.0850
2022-10-03 23:24:03 - train: epoch 0050, iter [00470, 01251], lr: 0.000359, loss: 0.0892
2022-10-03 23:24:31 - train: epoch 0050, iter [00480, 01251], lr: 0.000358, loss: 0.0930
2022-10-03 23:25:00 - train: epoch 0050, iter [00490, 01251], lr: 0.000358, loss: 0.0924
2022-10-03 23:25:28 - train: epoch 0050, iter [00500, 01251], lr: 0.000358, loss: 0.0950
2022-10-03 23:25:56 - train: epoch 0050, iter [00510, 01251], lr: 0.000358, loss: 0.0845
2022-10-03 23:26:24 - train: epoch 0050, iter [00520, 01251], lr: 0.000358, loss: 0.0938
2022-10-03 23:26:52 - train: epoch 0050, iter [00530, 01251], lr: 0.000358, loss: 0.0937
2022-10-03 23:27:20 - train: epoch 0050, iter [00540, 01251], lr: 0.000358, loss: 0.0850
2022-10-03 23:27:48 - train: epoch 0050, iter [00550, 01251], lr: 0.000358, loss: 0.0853
2022-10-03 23:28:16 - train: epoch 0050, iter [00560, 01251], lr: 0.000358, loss: 0.0941
2022-10-03 23:28:45 - train: epoch 0050, iter [00570, 01251], lr: 0.000358, loss: 0.0908
2022-10-03 23:29:13 - train: epoch 0050, iter [00580, 01251], lr: 0.000358, loss: 0.0895
2022-10-03 23:29:41 - train: epoch 0050, iter [00590, 01251], lr: 0.000358, loss: 0.0900
2022-10-03 23:30:09 - train: epoch 0050, iter [00600, 01251], lr: 0.000357, loss: 0.0957
2022-10-03 23:30:37 - train: epoch 0050, iter [00610, 01251], lr: 0.000357, loss: 0.0986
2022-10-03 23:31:05 - train: epoch 0050, iter [00620, 01251], lr: 0.000357, loss: 0.0873
2022-10-03 23:31:33 - train: epoch 0050, iter [00630, 01251], lr: 0.000357, loss: 0.0920
2022-10-03 23:32:01 - train: epoch 0050, iter [00640, 01251], lr: 0.000357, loss: 0.0916
2022-10-03 23:32:29 - train: epoch 0050, iter [00650, 01251], lr: 0.000357, loss: 0.0874
2022-10-03 23:32:57 - train: epoch 0050, iter [00660, 01251], lr: 0.000357, loss: 0.0847
2022-10-03 23:33:25 - train: epoch 0050, iter [00670, 01251], lr: 0.000357, loss: 0.0873
2022-10-03 23:33:53 - train: epoch 0050, iter [00680, 01251], lr: 0.000357, loss: 0.0826
2022-10-03 23:34:21 - train: epoch 0050, iter [00690, 01251], lr: 0.000357, loss: 0.0825
2022-10-03 23:34:49 - train: epoch 0050, iter [00700, 01251], lr: 0.000357, loss: 0.0824
2022-10-03 23:35:17 - train: epoch 0050, iter [00710, 01251], lr: 0.000357, loss: 0.0949
2022-10-03 23:35:45 - train: epoch 0050, iter [00720, 01251], lr: 0.000356, loss: 0.0873
2022-10-03 23:36:13 - train: epoch 0050, iter [00730, 01251], lr: 0.000356, loss: 0.0909
2022-10-03 23:36:41 - train: epoch 0050, iter [00740, 01251], lr: 0.000356, loss: 0.0833
2022-10-03 23:37:09 - train: epoch 0050, iter [00750, 01251], lr: 0.000356, loss: 0.0899
2022-10-03 23:37:37 - train: epoch 0050, iter [00760, 01251], lr: 0.000356, loss: 0.0892
2022-10-03 23:38:05 - train: epoch 0050, iter [00770, 01251], lr: 0.000356, loss: 0.0895
2022-10-03 23:38:32 - train: epoch 0050, iter [00780, 01251], lr: 0.000356, loss: 0.0948
2022-10-03 23:39:01 - train: epoch 0050, iter [00790, 01251], lr: 0.000356, loss: 0.0866
2022-10-03 23:39:29 - train: epoch 0050, iter [00800, 01251], lr: 0.000356, loss: 0.0858
2022-10-03 23:39:57 - train: epoch 0050, iter [00810, 01251], lr: 0.000356, loss: 0.0873
2022-10-03 23:40:25 - train: epoch 0050, iter [00820, 01251], lr: 0.000356, loss: 0.0921
2022-10-03 23:40:53 - train: epoch 0050, iter [00830, 01251], lr: 0.000356, loss: 0.0913
2022-10-03 23:41:20 - train: epoch 0050, iter [00840, 01251], lr: 0.000355, loss: 0.0806
2022-10-03 23:41:49 - train: epoch 0050, iter [00850, 01251], lr: 0.000355, loss: 0.0921
2022-10-03 23:42:17 - train: epoch 0050, iter [00860, 01251], lr: 0.000355, loss: 0.0862
2022-10-03 23:42:45 - train: epoch 0050, iter [00870, 01251], lr: 0.000355, loss: 0.0909
2022-10-03 23:43:13 - train: epoch 0050, iter [00880, 01251], lr: 0.000355, loss: 0.0929
2022-10-03 23:43:41 - train: epoch 0050, iter [00890, 01251], lr: 0.000355, loss: 0.0961
2022-10-03 23:44:09 - train: epoch 0050, iter [00900, 01251], lr: 0.000355, loss: 0.0812
2022-10-03 23:44:37 - train: epoch 0050, iter [00910, 01251], lr: 0.000355, loss: 0.0896
2022-10-03 23:45:05 - train: epoch 0050, iter [00920, 01251], lr: 0.000355, loss: 0.0893
2022-10-03 23:45:33 - train: epoch 0050, iter [00930, 01251], lr: 0.000355, loss: 0.0874
2022-10-03 23:46:01 - train: epoch 0050, iter [00940, 01251], lr: 0.000355, loss: 0.0905
2022-10-03 23:46:29 - train: epoch 0050, iter [00950, 01251], lr: 0.000355, loss: 0.0901
2022-10-03 23:46:57 - train: epoch 0050, iter [00960, 01251], lr: 0.000354, loss: 0.0895
2022-10-03 23:47:25 - train: epoch 0050, iter [00970, 01251], lr: 0.000354, loss: 0.0935
2022-10-03 23:47:53 - train: epoch 0050, iter [00980, 01251], lr: 0.000354, loss: 0.0845
2022-10-03 23:48:21 - train: epoch 0050, iter [00990, 01251], lr: 0.000354, loss: 0.0860
2022-10-03 23:48:49 - train: epoch 0050, iter [01000, 01251], lr: 0.000354, loss: 0.0921
2022-10-03 23:49:17 - train: epoch 0050, iter [01010, 01251], lr: 0.000354, loss: 0.0967
2022-10-03 23:49:45 - train: epoch 0050, iter [01020, 01251], lr: 0.000354, loss: 0.0982
2022-10-03 23:50:13 - train: epoch 0050, iter [01030, 01251], lr: 0.000354, loss: 0.0855
2022-10-03 23:50:41 - train: epoch 0050, iter [01040, 01251], lr: 0.000354, loss: 0.0856
2022-10-03 23:51:09 - train: epoch 0050, iter [01050, 01251], lr: 0.000354, loss: 0.0911
2022-10-03 23:51:37 - train: epoch 0050, iter [01060, 01251], lr: 0.000354, loss: 0.0875
2022-10-03 23:52:05 - train: epoch 0050, iter [01070, 01251], lr: 0.000354, loss: 0.0799
2022-10-03 23:52:33 - train: epoch 0050, iter [01080, 01251], lr: 0.000354, loss: 0.0916
2022-10-03 23:53:01 - train: epoch 0050, iter [01090, 01251], lr: 0.000353, loss: 0.0917
2022-10-03 23:53:29 - train: epoch 0050, iter [01100, 01251], lr: 0.000353, loss: 0.0862
2022-10-03 23:53:57 - train: epoch 0050, iter [01110, 01251], lr: 0.000353, loss: 0.0969
2022-10-03 23:54:25 - train: epoch 0050, iter [01120, 01251], lr: 0.000353, loss: 0.0894
2022-10-03 23:54:53 - train: epoch 0050, iter [01130, 01251], lr: 0.000353, loss: 0.0883
2022-10-03 23:55:21 - train: epoch 0050, iter [01140, 01251], lr: 0.000353, loss: 0.0865
2022-10-03 23:55:49 - train: epoch 0050, iter [01150, 01251], lr: 0.000353, loss: 0.0882
2022-10-03 23:56:17 - train: epoch 0050, iter [01160, 01251], lr: 0.000353, loss: 0.0886
2022-10-03 23:56:45 - train: epoch 0050, iter [01170, 01251], lr: 0.000353, loss: 0.0854
2022-10-03 23:57:13 - train: epoch 0050, iter [01180, 01251], lr: 0.000353, loss: 0.0911
2022-10-03 23:57:41 - train: epoch 0050, iter [01190, 01251], lr: 0.000353, loss: 0.0915
2022-10-03 23:58:09 - train: epoch 0050, iter [01200, 01251], lr: 0.000353, loss: 0.0931
2022-10-03 23:58:37 - train: epoch 0050, iter [01210, 01251], lr: 0.000352, loss: 0.0887
2022-10-03 23:59:05 - train: epoch 0050, iter [01220, 01251], lr: 0.000352, loss: 0.0914
2022-10-03 23:59:34 - train: epoch 0050, iter [01230, 01251], lr: 0.000352, loss: 0.0894
