2022-10-02 07:21:31 - network: vit_base_patch16_224_mae_pretrain_model
2022-10-02 07:21:31 - input_image_size: 224
2022-10-02 07:21:31 - scale: 1.1428571428571428
2022-10-02 07:21:31 - trained_model_path: 
2022-10-02 07:21:31 - train_criterion: MSELoss()
2022-10-02 07:21:31 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f27e7baac10>
2022-10-02 07:21:31 - train_collater: <simpleAICV.masked_image_modeling.common.MAESelfSupervisedPretrainCollater object at 0x7f27e7baac40>
2022-10-02 07:21:31 - seed: 0
2022-10-02 07:21:31 - batch_size: 256
2022-10-02 07:21:31 - num_workers: 20
2022-10-02 07:21:31 - accumulation_steps: 4
2022-10-02 07:21:31 - optimizer: ('AdamW', {'lr': 0.0006, 'global_weight_decay': False, 'weight_decay': 0.05, 'no_weight_decay_layer_name_list': [], 'beta1': 0.9, 'beta2': 0.95})
2022-10-02 07:21:31 - scheduler: ('CosineLR', {'warm_up_epochs': 10})
2022-10-02 07:21:31 - epochs: 100
2022-10-02 07:21:31 - print_interval: 10
2022-10-02 07:21:31 - sync_bn: False
2022-10-02 07:21:31 - apex: True
2022-10-02 07:21:31 - use_ema_model: False
2022-10-02 07:21:31 - ema_model_decay: 0.9999
2022-10-02 07:21:31 - gpus_type: NVIDIA RTX A5000
2022-10-02 07:21:31 - gpus_num: 2
2022-10-02 07:21:31 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f27ee61fb70>
2022-10-02 07:21:31 - --------------------parameters--------------------
2022-10-02 07:21:31 - name: encoder.cls_token, grad: True
2022-10-02 07:21:31 - name: encoder.position_encoding, grad: False
2022-10-02 07:21:31 - name: encoder.patch_embedding.conv.weight, grad: True
2022-10-02 07:21:31 - name: encoder.patch_embedding.conv.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.0.norm1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.0.norm1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.0.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.0.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.0.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.0.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.0.norm2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.0.norm2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.0.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.0.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.0.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.0.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.1.norm1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.1.norm1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.1.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.1.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.1.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.1.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.1.norm2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.1.norm2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.1.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.1.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.1.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.1.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.2.norm1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.2.norm1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.2.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.2.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.2.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.2.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.2.norm2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.2.norm2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.2.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.2.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.2.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.2.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.3.norm1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.3.norm1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.3.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.3.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.3.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.3.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.3.norm2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.3.norm2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.3.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.3.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.3.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.3.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.4.norm1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.4.norm1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.4.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.4.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.4.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.4.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.4.norm2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.4.norm2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.4.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.4.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.4.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.4.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.5.norm1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.5.norm1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.5.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.5.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.5.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.5.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.5.norm2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.5.norm2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.5.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.5.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.5.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.5.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.6.norm1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.6.norm1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.6.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.6.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.6.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.6.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.6.norm2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.6.norm2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.6.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.6.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.6.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.6.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.7.norm1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.7.norm1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.7.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.7.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.7.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.7.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.7.norm2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.7.norm2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.7.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.7.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.7.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.7.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.8.norm1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.8.norm1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.8.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.8.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.8.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.8.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.8.norm2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.8.norm2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.8.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.8.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.8.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.8.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.9.norm1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.9.norm1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.9.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.9.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.9.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.9.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.9.norm2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.9.norm2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.9.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.9.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.9.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.9.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.10.norm1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.10.norm1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.10.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.10.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.10.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.10.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.10.norm2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.10.norm2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.10.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.10.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.10.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.10.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.11.norm1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.11.norm1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.11.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.11.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.11.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.11.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.11.norm2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.11.norm2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.11.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.11.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.11.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: encoder.blocks.11.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: encoder.norm.weight, grad: True
2022-10-02 07:21:31 - name: encoder.norm.bias, grad: True
2022-10-02 07:21:31 - name: decoder.mask_token, grad: True
2022-10-02 07:21:31 - name: decoder.position_encoding, grad: False
2022-10-02 07:21:31 - name: decoder.blocks.0.norm1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.0.norm1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.0.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.0.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.0.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.0.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.0.norm2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.0.norm2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.0.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.0.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.0.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.0.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.1.norm1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.1.norm1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.1.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.1.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.1.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.1.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.1.norm2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.1.norm2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.1.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.1.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.1.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.1.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.2.norm1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.2.norm1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.2.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.2.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.2.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.2.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.2.norm2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.2.norm2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.2.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.2.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.2.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.2.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.3.norm1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.3.norm1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.3.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.3.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.3.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.3.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.3.norm2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.3.norm2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.3.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.3.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.3.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.3.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.4.norm1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.4.norm1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.4.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.4.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.4.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.4.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.4.norm2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.4.norm2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.4.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.4.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.4.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.4.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.5.norm1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.5.norm1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.5.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.5.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.5.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.5.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.5.norm2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.5.norm2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.5.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.5.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.5.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.5.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.6.norm1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.6.norm1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.6.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.6.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.6.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.6.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.6.norm2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.6.norm2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.6.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.6.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.6.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.6.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.7.norm1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.7.norm1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.7.attention.qkv_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.7.attention.qkv_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.7.attention.out_linear.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.7.attention.out_linear.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.7.norm2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.7.norm2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.7.feed_forward.fc1.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.7.feed_forward.fc1.bias, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.7.feed_forward.fc2.weight, grad: True
2022-10-02 07:21:31 - name: decoder.blocks.7.feed_forward.fc2.bias, grad: True
2022-10-02 07:21:31 - name: decoder.norm.weight, grad: True
2022-10-02 07:21:31 - name: decoder.norm.bias, grad: True
2022-10-02 07:21:31 - name: decoder.fc.weight, grad: True
2022-10-02 07:21:31 - name: decoder.fc.bias, grad: True
2022-10-02 07:21:31 - name: encoder_to_decoder.weight, grad: True
2022-10-02 07:21:31 - name: encoder_to_decoder.bias, grad: True
2022-10-02 07:21:31 - --------------------buffers--------------------
2022-10-02 07:21:31 - -----------no weight decay layers--------------
2022-10-02 07:21:31 - name: encoder.patch_embedding.conv.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.0.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.0.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.0.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.0.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.0.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.0.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.0.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.0.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.1.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.1.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.1.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.1.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.1.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.1.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.1.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.1.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.2.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.2.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.2.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.2.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.2.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.2.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.2.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.2.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.3.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.3.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.3.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.3.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.3.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.3.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.3.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.3.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.4.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.4.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.4.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.4.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.4.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.4.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.4.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.4.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.5.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.5.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.5.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.5.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.5.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.5.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.5.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.5.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.6.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.6.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.6.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.6.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.6.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.6.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.6.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.6.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.7.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.7.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.7.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.7.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.7.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.7.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.7.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.7.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.8.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.8.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.8.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.8.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.8.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.8.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.8.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.8.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.9.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.9.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.9.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.9.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.9.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.9.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.9.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.9.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.10.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.10.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.10.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.10.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.10.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.10.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.10.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.10.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.11.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.11.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.11.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.11.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.11.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.11.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.11.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.11.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.norm.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.norm.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.0.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.0.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.0.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.0.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.0.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.0.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.0.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.0.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.1.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.1.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.1.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.1.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.1.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.1.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.1.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.1.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.2.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.2.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.2.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.2.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.2.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.2.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.2.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.2.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.3.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.3.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.3.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.3.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.3.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.3.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.3.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.3.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.4.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.4.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.4.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.4.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.4.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.4.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.4.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.4.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.5.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.5.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.5.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.5.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.5.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.5.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.5.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.5.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.6.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.6.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.6.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.6.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.6.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.6.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.6.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.6.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.7.norm1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.7.norm1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.7.attention.qkv_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.7.attention.out_linear.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.7.norm2.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.7.norm2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.7.feed_forward.fc1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.7.feed_forward.fc2.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.norm.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.norm.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder_to_decoder.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-02 07:21:31 - -------------weight decay layers---------------
2022-10-02 07:21:31 - name: encoder.cls_token, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.patch_embedding.conv.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.0.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.0.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.0.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.0.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.1.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.1.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.1.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.1.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.2.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.2.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.2.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.2.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.3.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.3.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.3.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.3.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.4.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.4.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.4.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.4.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.5.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.5.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.5.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.5.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.6.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.6.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.6.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.6.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.7.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.7.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.7.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.7.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.8.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.8.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.8.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.8.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.9.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.9.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.9.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.9.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.10.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.10.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.10.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.10.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.11.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.11.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.11.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder.blocks.11.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.mask_token, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.0.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.0.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.0.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.0.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.1.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.1.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.1.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.1.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.2.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.2.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.2.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.2.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.3.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.3.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.3.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.3.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.4.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.4.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.4.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.4.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.5.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.5.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.5.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.5.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.6.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.6.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.6.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.6.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.7.attention.qkv_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.7.attention.out_linear.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.7.feed_forward.fc1.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.blocks.7.feed_forward.fc2.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: decoder.fc.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:31 - name: encoder_to_decoder.weight, weight_decay: 0.05, lr_scale: not setting!
2022-10-02 07:21:32 - epoch 001 lr: 0.000600
2022-10-02 07:22:05 - train: epoch 0001, iter [00010, 01251], lr: 0.000060, loss: 1.1312
2022-10-02 07:22:24 - train: epoch 0001, iter [00020, 01251], lr: 0.000061, loss: 1.0169
2022-10-02 07:22:44 - train: epoch 0001, iter [00030, 01251], lr: 0.000061, loss: 0.9586
2022-10-02 07:23:03 - train: epoch 0001, iter [00040, 01251], lr: 0.000062, loss: 0.9154
2022-10-02 07:23:22 - train: epoch 0001, iter [00050, 01251], lr: 0.000062, loss: 0.8877
2022-10-02 07:23:42 - train: epoch 0001, iter [00060, 01251], lr: 0.000063, loss: 0.8589
2022-10-02 07:24:00 - train: epoch 0001, iter [00070, 01251], lr: 0.000063, loss: 0.8372
2022-10-02 07:24:20 - train: epoch 0001, iter [00080, 01251], lr: 0.000064, loss: 0.8479
2022-10-02 07:24:39 - train: epoch 0001, iter [00090, 01251], lr: 0.000064, loss: 0.8002
2022-10-02 07:24:58 - train: epoch 0001, iter [00100, 01251], lr: 0.000065, loss: 0.8161
2022-10-02 07:25:17 - train: epoch 0001, iter [00110, 01251], lr: 0.000065, loss: 0.7878
2022-10-02 07:25:36 - train: epoch 0001, iter [00120, 01251], lr: 0.000066, loss: 0.8141
2022-10-02 07:25:55 - train: epoch 0001, iter [00130, 01251], lr: 0.000066, loss: 0.7972
2022-10-02 07:26:14 - train: epoch 0001, iter [00140, 01251], lr: 0.000067, loss: 0.8244
2022-10-02 07:26:34 - train: epoch 0001, iter [00150, 01251], lr: 0.000067, loss: 0.8022
2022-10-02 07:26:53 - train: epoch 0001, iter [00160, 01251], lr: 0.000068, loss: 0.7890
2022-10-02 07:27:13 - train: epoch 0001, iter [00170, 01251], lr: 0.000068, loss: 0.7587
2022-10-02 07:27:33 - train: epoch 0001, iter [00180, 01251], lr: 0.000069, loss: 0.7809
2022-10-02 07:27:52 - train: epoch 0001, iter [00190, 01251], lr: 0.000069, loss: 0.7720
2022-10-02 07:28:11 - train: epoch 0001, iter [00200, 01251], lr: 0.000070, loss: 0.7625
2022-10-02 07:28:30 - train: epoch 0001, iter [00210, 01251], lr: 0.000070, loss: 0.7665
2022-10-02 07:28:50 - train: epoch 0001, iter [00220, 01251], lr: 0.000071, loss: 0.7578
2022-10-02 07:29:09 - train: epoch 0001, iter [00230, 01251], lr: 0.000071, loss: 0.7538
2022-10-02 07:29:28 - train: epoch 0001, iter [00240, 01251], lr: 0.000072, loss: 0.7385
2022-10-02 07:29:47 - train: epoch 0001, iter [00250, 01251], lr: 0.000072, loss: 0.7317
2022-10-02 07:30:06 - train: epoch 0001, iter [00260, 01251], lr: 0.000072, loss: 0.7264
2022-10-02 07:30:25 - train: epoch 0001, iter [00270, 01251], lr: 0.000073, loss: 0.7229
2022-10-02 07:30:44 - train: epoch 0001, iter [00280, 01251], lr: 0.000073, loss: 0.7082
2022-10-02 07:31:03 - train: epoch 0001, iter [00290, 01251], lr: 0.000074, loss: 0.7120
2022-10-02 07:31:22 - train: epoch 0001, iter [00300, 01251], lr: 0.000074, loss: 0.7158
2022-10-02 07:31:42 - train: epoch 0001, iter [00310, 01251], lr: 0.000075, loss: 0.6856
2022-10-02 07:32:01 - train: epoch 0001, iter [00320, 01251], lr: 0.000075, loss: 0.7113
2022-10-02 07:32:20 - train: epoch 0001, iter [00330, 01251], lr: 0.000076, loss: 0.7080
2022-10-02 07:32:40 - train: epoch 0001, iter [00340, 01251], lr: 0.000076, loss: 0.6903
2022-10-02 07:32:59 - train: epoch 0001, iter [00350, 01251], lr: 0.000077, loss: 0.6902
2022-10-02 07:33:18 - train: epoch 0001, iter [00360, 01251], lr: 0.000077, loss: 0.6692
2022-10-02 07:33:37 - train: epoch 0001, iter [00370, 01251], lr: 0.000078, loss: 0.6752
2022-10-02 07:33:57 - train: epoch 0001, iter [00380, 01251], lr: 0.000078, loss: 0.7105
2022-10-02 07:34:16 - train: epoch 0001, iter [00390, 01251], lr: 0.000079, loss: 0.6790
2022-10-02 07:34:35 - train: epoch 0001, iter [00400, 01251], lr: 0.000079, loss: 0.7057
2022-10-02 07:34:55 - train: epoch 0001, iter [00410, 01251], lr: 0.000080, loss: 0.6940
2022-10-02 07:35:15 - train: epoch 0001, iter [00420, 01251], lr: 0.000080, loss: 0.6743
2022-10-02 07:35:34 - train: epoch 0001, iter [00430, 01251], lr: 0.000081, loss: 0.6683
2022-10-02 07:35:53 - train: epoch 0001, iter [00440, 01251], lr: 0.000081, loss: 0.6643
2022-10-02 07:36:12 - train: epoch 0001, iter [00450, 01251], lr: 0.000082, loss: 0.6849
2022-10-02 07:36:31 - train: epoch 0001, iter [00460, 01251], lr: 0.000082, loss: 0.6746
2022-10-02 07:36:50 - train: epoch 0001, iter [00470, 01251], lr: 0.000083, loss: 0.6707
2022-10-02 07:37:10 - train: epoch 0001, iter [00480, 01251], lr: 0.000083, loss: 0.6895
2022-10-02 07:37:28 - train: epoch 0001, iter [00490, 01251], lr: 0.000084, loss: 0.6917
2022-10-02 07:37:48 - train: epoch 0001, iter [00500, 01251], lr: 0.000084, loss: 0.6847
2022-10-02 07:38:07 - train: epoch 0001, iter [00510, 01251], lr: 0.000084, loss: 0.6544
2022-10-02 07:38:26 - train: epoch 0001, iter [00520, 01251], lr: 0.000085, loss: 0.6483
2022-10-02 07:38:46 - train: epoch 0001, iter [00530, 01251], lr: 0.000085, loss: 0.6645
2022-10-02 07:39:05 - train: epoch 0001, iter [00540, 01251], lr: 0.000086, loss: 0.6403
2022-10-02 07:39:25 - train: epoch 0001, iter [00550, 01251], lr: 0.000086, loss: 0.6594
2022-10-02 07:39:45 - train: epoch 0001, iter [00560, 01251], lr: 0.000087, loss: 0.6557
2022-10-02 07:40:04 - train: epoch 0001, iter [00570, 01251], lr: 0.000087, loss: 0.6624
2022-10-02 07:40:24 - train: epoch 0001, iter [00580, 01251], lr: 0.000088, loss: 0.6473
2022-10-02 07:40:43 - train: epoch 0001, iter [00590, 01251], lr: 0.000088, loss: 0.6493
2022-10-02 07:41:03 - train: epoch 0001, iter [00600, 01251], lr: 0.000089, loss: 0.6600
2022-10-02 07:41:22 - train: epoch 0001, iter [00610, 01251], lr: 0.000089, loss: 0.6117
2022-10-02 07:41:42 - train: epoch 0001, iter [00620, 01251], lr: 0.000090, loss: 0.6486
2022-10-02 07:42:02 - train: epoch 0001, iter [00630, 01251], lr: 0.000090, loss: 0.6582
2022-10-02 07:42:22 - train: epoch 0001, iter [00640, 01251], lr: 0.000091, loss: 0.6545
2022-10-02 07:42:42 - train: epoch 0001, iter [00650, 01251], lr: 0.000091, loss: 0.6533
2022-10-02 07:43:02 - train: epoch 0001, iter [00660, 01251], lr: 0.000092, loss: 0.6291
2022-10-02 07:43:21 - train: epoch 0001, iter [00670, 01251], lr: 0.000092, loss: 0.6227
2022-10-02 07:43:42 - train: epoch 0001, iter [00680, 01251], lr: 0.000093, loss: 0.6608
2022-10-02 07:44:01 - train: epoch 0001, iter [00690, 01251], lr: 0.000093, loss: 0.6417
2022-10-02 07:44:21 - train: epoch 0001, iter [00700, 01251], lr: 0.000094, loss: 0.6448
2022-10-02 07:44:41 - train: epoch 0001, iter [00710, 01251], lr: 0.000094, loss: 0.6187
2022-10-02 07:45:01 - train: epoch 0001, iter [00720, 01251], lr: 0.000095, loss: 0.6416
2022-10-02 07:45:21 - train: epoch 0001, iter [00730, 01251], lr: 0.000095, loss: 0.6315
2022-10-02 07:45:40 - train: epoch 0001, iter [00740, 01251], lr: 0.000095, loss: 0.6444
2022-10-02 07:46:00 - train: epoch 0001, iter [00750, 01251], lr: 0.000096, loss: 0.6328
2022-10-02 07:46:20 - train: epoch 0001, iter [00760, 01251], lr: 0.000096, loss: 0.6215
2022-10-02 07:46:39 - train: epoch 0001, iter [00770, 01251], lr: 0.000097, loss: 0.6302
2022-10-02 07:46:59 - train: epoch 0001, iter [00780, 01251], lr: 0.000097, loss: 0.6144
2022-10-02 07:47:19 - train: epoch 0001, iter [00790, 01251], lr: 0.000098, loss: 0.5976
2022-10-02 07:47:38 - train: epoch 0001, iter [00800, 01251], lr: 0.000098, loss: 0.6013
2022-10-02 07:47:58 - train: epoch 0001, iter [00810, 01251], lr: 0.000099, loss: 0.6390
2022-10-02 07:48:18 - train: epoch 0001, iter [00820, 01251], lr: 0.000099, loss: 0.6211
2022-10-02 07:48:37 - train: epoch 0001, iter [00830, 01251], lr: 0.000100, loss: 0.5901
2022-10-02 07:48:57 - train: epoch 0001, iter [00840, 01251], lr: 0.000100, loss: 0.6138
2022-10-02 07:49:16 - train: epoch 0001, iter [00850, 01251], lr: 0.000101, loss: 0.5937
2022-10-02 07:49:36 - train: epoch 0001, iter [00860, 01251], lr: 0.000101, loss: 0.5840
2022-10-02 07:49:55 - train: epoch 0001, iter [00870, 01251], lr: 0.000102, loss: 0.6089
2022-10-02 07:50:15 - train: epoch 0001, iter [00880, 01251], lr: 0.000102, loss: 0.5947
2022-10-02 07:50:34 - train: epoch 0001, iter [00890, 01251], lr: 0.000103, loss: 0.5876
2022-10-02 07:50:54 - train: epoch 0001, iter [00900, 01251], lr: 0.000103, loss: 0.6255
2022-10-02 07:51:14 - train: epoch 0001, iter [00910, 01251], lr: 0.000104, loss: 0.5950
2022-10-02 07:51:33 - train: epoch 0001, iter [00920, 01251], lr: 0.000104, loss: 0.5840
2022-10-02 07:51:53 - train: epoch 0001, iter [00930, 01251], lr: 0.000105, loss: 0.6290
2022-10-02 07:52:12 - train: epoch 0001, iter [00940, 01251], lr: 0.000105, loss: 0.6360
2022-10-02 07:52:32 - train: epoch 0001, iter [00950, 01251], lr: 0.000106, loss: 0.5901
2022-10-02 07:52:52 - train: epoch 0001, iter [00960, 01251], lr: 0.000106, loss: 0.6039
2022-10-02 07:53:11 - train: epoch 0001, iter [00970, 01251], lr: 0.000107, loss: 0.6016
2022-10-02 07:53:31 - train: epoch 0001, iter [00980, 01251], lr: 0.000107, loss: 0.5896
2022-10-02 07:53:50 - train: epoch 0001, iter [00990, 01251], lr: 0.000107, loss: 0.6041
2022-10-02 07:54:10 - train: epoch 0001, iter [01000, 01251], lr: 0.000108, loss: 0.5931
2022-10-02 07:54:29 - train: epoch 0001, iter [01010, 01251], lr: 0.000108, loss: 0.5739
2022-10-02 07:54:49 - train: epoch 0001, iter [01020, 01251], lr: 0.000109, loss: 0.5933
2022-10-02 07:55:08 - train: epoch 0001, iter [01030, 01251], lr: 0.000109, loss: 0.6056
2022-10-02 07:55:28 - train: epoch 0001, iter [01040, 01251], lr: 0.000110, loss: 0.5990
2022-10-02 07:55:47 - train: epoch 0001, iter [01050, 01251], lr: 0.000110, loss: 0.5920
2022-10-02 07:56:07 - train: epoch 0001, iter [01060, 01251], lr: 0.000111, loss: 0.5933
2022-10-02 07:56:27 - train: epoch 0001, iter [01070, 01251], lr: 0.000111, loss: 0.5782
2022-10-02 07:56:46 - train: epoch 0001, iter [01080, 01251], lr: 0.000112, loss: 0.5931
2022-10-02 07:57:06 - train: epoch 0001, iter [01090, 01251], lr: 0.000112, loss: 0.5695
2022-10-02 07:57:26 - train: epoch 0001, iter [01100, 01251], lr: 0.000113, loss: 0.6009
2022-10-02 07:57:46 - train: epoch 0001, iter [01110, 01251], lr: 0.000113, loss: 0.5839
2022-10-02 07:58:05 - train: epoch 0001, iter [01120, 01251], lr: 0.000114, loss: 0.5920
2022-10-02 07:58:25 - train: epoch 0001, iter [01130, 01251], lr: 0.000114, loss: 0.5688
2022-10-02 07:58:44 - train: epoch 0001, iter [01140, 01251], lr: 0.000115, loss: 0.5913
2022-10-02 07:59:04 - train: epoch 0001, iter [01150, 01251], lr: 0.000115, loss: 0.5765
2022-10-02 07:59:24 - train: epoch 0001, iter [01160, 01251], lr: 0.000116, loss: 0.6099
2022-10-02 07:59:43 - train: epoch 0001, iter [01170, 01251], lr: 0.000116, loss: 0.5806
2022-10-02 08:00:03 - train: epoch 0001, iter [01180, 01251], lr: 0.000117, loss: 0.5613
2022-10-02 08:00:22 - train: epoch 0001, iter [01190, 01251], lr: 0.000117, loss: 0.5892
2022-10-02 08:00:42 - train: epoch 0001, iter [01200, 01251], lr: 0.000118, loss: 0.5794
2022-10-02 08:01:01 - train: epoch 0001, iter [01210, 01251], lr: 0.000118, loss: 0.5838
2022-10-02 08:01:20 - train: epoch 0001, iter [01220, 01251], lr: 0.000119, loss: 0.5743
2022-10-02 08:01:40 - train: epoch 0001, iter [01230, 01251], lr: 0.000119, loss: 0.5927
2022-10-02 08:02:00 - train: epoch 0001, iter [01240, 01251], lr: 0.000119, loss: 0.5575
2022-10-02 08:02:19 - train: epoch 0001, iter [01250, 01251], lr: 0.000120, loss: 0.5597
2022-10-02 08:02:22 - train: epoch 001, train_loss: 0.6740
2022-10-02 08:02:25 - until epoch: 001, best_loss: 0.6740
2022-10-02 08:02:25 - epoch 002 lr: 0.000120
2022-10-02 08:02:52 - train: epoch 0002, iter [00010, 01251], lr: 0.000120, loss: 0.5628
2022-10-02 08:03:12 - train: epoch 0002, iter [00020, 01251], lr: 0.000121, loss: 0.5652
2022-10-02 08:03:31 - train: epoch 0002, iter [00030, 01251], lr: 0.000121, loss: 0.5723
2022-10-02 08:03:51 - train: epoch 0002, iter [00040, 01251], lr: 0.000122, loss: 0.5737
2022-10-02 08:04:10 - train: epoch 0002, iter [00050, 01251], lr: 0.000122, loss: 0.5588
2022-10-02 08:04:30 - train: epoch 0002, iter [00060, 01251], lr: 0.000123, loss: 0.5747
2022-10-02 08:04:49 - train: epoch 0002, iter [00070, 01251], lr: 0.000123, loss: 0.5599
2022-10-02 08:05:08 - train: epoch 0002, iter [00080, 01251], lr: 0.000124, loss: 0.5663
2022-10-02 08:05:27 - train: epoch 0002, iter [00090, 01251], lr: 0.000124, loss: 0.5749
2022-10-02 08:05:47 - train: epoch 0002, iter [00100, 01251], lr: 0.000125, loss: 0.5631
2022-10-02 08:06:06 - train: epoch 0002, iter [00110, 01251], lr: 0.000125, loss: 0.5748
2022-10-02 08:06:25 - train: epoch 0002, iter [00120, 01251], lr: 0.000126, loss: 0.5570
2022-10-02 08:06:45 - train: epoch 0002, iter [00130, 01251], lr: 0.000126, loss: 0.5830
2022-10-02 08:07:04 - train: epoch 0002, iter [00140, 01251], lr: 0.000127, loss: 0.5783
2022-10-02 08:07:24 - train: epoch 0002, iter [00150, 01251], lr: 0.000127, loss: 0.5539
2022-10-02 08:07:43 - train: epoch 0002, iter [00160, 01251], lr: 0.000128, loss: 0.5368
2022-10-02 08:08:02 - train: epoch 0002, iter [00170, 01251], lr: 0.000128, loss: 0.5501
2022-10-02 08:08:22 - train: epoch 0002, iter [00180, 01251], lr: 0.000129, loss: 0.5586
2022-10-02 08:08:41 - train: epoch 0002, iter [00190, 01251], lr: 0.000129, loss: 0.5410
2022-10-02 08:09:01 - train: epoch 0002, iter [00200, 01251], lr: 0.000130, loss: 0.5747
2022-10-02 08:09:20 - train: epoch 0002, iter [00210, 01251], lr: 0.000130, loss: 0.5417
2022-10-02 08:09:40 - train: epoch 0002, iter [00220, 01251], lr: 0.000131, loss: 0.5851
2022-10-02 08:09:59 - train: epoch 0002, iter [00230, 01251], lr: 0.000131, loss: 0.5642
2022-10-02 08:10:18 - train: epoch 0002, iter [00240, 01251], lr: 0.000132, loss: 0.5633
2022-10-02 08:10:38 - train: epoch 0002, iter [00250, 01251], lr: 0.000132, loss: 0.5481
2022-10-02 08:10:58 - train: epoch 0002, iter [00260, 01251], lr: 0.000132, loss: 0.5477
2022-10-02 08:11:17 - train: epoch 0002, iter [00270, 01251], lr: 0.000133, loss: 0.5441
2022-10-02 08:11:36 - train: epoch 0002, iter [00280, 01251], lr: 0.000133, loss: 0.5588
2022-10-02 08:11:55 - train: epoch 0002, iter [00290, 01251], lr: 0.000134, loss: 0.5691
2022-10-02 08:12:15 - train: epoch 0002, iter [00300, 01251], lr: 0.000134, loss: 0.5560
2022-10-02 08:12:34 - train: epoch 0002, iter [00310, 01251], lr: 0.000135, loss: 0.5551
2022-10-02 08:12:54 - train: epoch 0002, iter [00320, 01251], lr: 0.000135, loss: 0.5449
2022-10-02 08:13:13 - train: epoch 0002, iter [00330, 01251], lr: 0.000136, loss: 0.5587
2022-10-02 08:13:33 - train: epoch 0002, iter [00340, 01251], lr: 0.000136, loss: 0.5309
2022-10-02 08:13:52 - train: epoch 0002, iter [00350, 01251], lr: 0.000137, loss: 0.5317
2022-10-02 08:14:12 - train: epoch 0002, iter [00360, 01251], lr: 0.000137, loss: 0.5549
2022-10-02 08:14:31 - train: epoch 0002, iter [00370, 01251], lr: 0.000138, loss: 0.5321
2022-10-02 08:14:50 - train: epoch 0002, iter [00380, 01251], lr: 0.000138, loss: 0.5633
2022-10-02 08:15:09 - train: epoch 0002, iter [00390, 01251], lr: 0.000139, loss: 0.5324
2022-10-02 08:15:29 - train: epoch 0002, iter [00400, 01251], lr: 0.000139, loss: 0.5431
2022-10-02 08:15:48 - train: epoch 0002, iter [00410, 01251], lr: 0.000140, loss: 0.5552
2022-10-02 08:16:07 - train: epoch 0002, iter [00420, 01251], lr: 0.000140, loss: 0.5443
2022-10-02 08:16:27 - train: epoch 0002, iter [00430, 01251], lr: 0.000141, loss: 0.5476
2022-10-02 08:16:46 - train: epoch 0002, iter [00440, 01251], lr: 0.000141, loss: 0.5294
2022-10-02 08:17:06 - train: epoch 0002, iter [00450, 01251], lr: 0.000142, loss: 0.5232
2022-10-02 08:17:25 - train: epoch 0002, iter [00460, 01251], lr: 0.000142, loss: 0.5403
2022-10-02 08:17:44 - train: epoch 0002, iter [00470, 01251], lr: 0.000143, loss: 0.5365
2022-10-02 08:18:04 - train: epoch 0002, iter [00480, 01251], lr: 0.000143, loss: 0.5427
2022-10-02 08:18:23 - train: epoch 0002, iter [00490, 01251], lr: 0.000144, loss: 0.5168
2022-10-02 08:18:43 - train: epoch 0002, iter [00500, 01251], lr: 0.000144, loss: 0.5450
2022-10-02 08:19:02 - train: epoch 0002, iter [00510, 01251], lr: 0.000144, loss: 0.5659
2022-10-02 08:19:22 - train: epoch 0002, iter [00520, 01251], lr: 0.000145, loss: 0.5295
2022-10-02 08:19:41 - train: epoch 0002, iter [00530, 01251], lr: 0.000145, loss: 0.5343
2022-10-02 08:20:01 - train: epoch 0002, iter [00540, 01251], lr: 0.000146, loss: 0.5324
2022-10-02 08:20:21 - train: epoch 0002, iter [00550, 01251], lr: 0.000146, loss: 0.5364
2022-10-02 08:20:40 - train: epoch 0002, iter [00560, 01251], lr: 0.000147, loss: 0.5381
2022-10-02 08:20:59 - train: epoch 0002, iter [00570, 01251], lr: 0.000147, loss: 0.5303
2022-10-02 08:21:19 - train: epoch 0002, iter [00580, 01251], lr: 0.000148, loss: 0.5383
2022-10-02 08:21:38 - train: epoch 0002, iter [00590, 01251], lr: 0.000148, loss: 0.5267
2022-10-02 08:21:57 - train: epoch 0002, iter [00600, 01251], lr: 0.000149, loss: 0.5402
2022-10-02 08:22:17 - train: epoch 0002, iter [00610, 01251], lr: 0.000149, loss: 0.5126
2022-10-02 08:22:36 - train: epoch 0002, iter [00620, 01251], lr: 0.000150, loss: 0.5212
2022-10-02 08:22:56 - train: epoch 0002, iter [00630, 01251], lr: 0.000150, loss: 0.5287
2022-10-02 08:23:15 - train: epoch 0002, iter [00640, 01251], lr: 0.000151, loss: 0.5288
2022-10-02 08:23:34 - train: epoch 0002, iter [00650, 01251], lr: 0.000151, loss: 0.5240
2022-10-02 08:23:54 - train: epoch 0002, iter [00660, 01251], lr: 0.000152, loss: 0.5271
2022-10-02 08:24:13 - train: epoch 0002, iter [00670, 01251], lr: 0.000152, loss: 0.5247
2022-10-02 08:24:33 - train: epoch 0002, iter [00680, 01251], lr: 0.000153, loss: 0.5313
2022-10-02 08:24:52 - train: epoch 0002, iter [00690, 01251], lr: 0.000153, loss: 0.5391
2022-10-02 08:25:11 - train: epoch 0002, iter [00700, 01251], lr: 0.000154, loss: 0.5304
2022-10-02 08:25:31 - train: epoch 0002, iter [00710, 01251], lr: 0.000154, loss: 0.5086
2022-10-02 08:25:50 - train: epoch 0002, iter [00720, 01251], lr: 0.000155, loss: 0.5262
2022-10-02 08:26:10 - train: epoch 0002, iter [00730, 01251], lr: 0.000155, loss: 0.5352
2022-10-02 08:26:29 - train: epoch 0002, iter [00740, 01251], lr: 0.000155, loss: 0.5352
2022-10-02 08:26:49 - train: epoch 0002, iter [00750, 01251], lr: 0.000156, loss: 0.5573
2022-10-02 08:27:08 - train: epoch 0002, iter [00760, 01251], lr: 0.000156, loss: 0.5375
2022-10-02 08:27:28 - train: epoch 0002, iter [00770, 01251], lr: 0.000157, loss: 0.5336
2022-10-02 08:27:47 - train: epoch 0002, iter [00780, 01251], lr: 0.000157, loss: 0.5236
2022-10-02 08:28:07 - train: epoch 0002, iter [00790, 01251], lr: 0.000158, loss: 0.5242
2022-10-02 08:28:26 - train: epoch 0002, iter [00800, 01251], lr: 0.000158, loss: 0.5185
2022-10-02 08:28:46 - train: epoch 0002, iter [00810, 01251], lr: 0.000159, loss: 0.5346
2022-10-02 08:29:05 - train: epoch 0002, iter [00820, 01251], lr: 0.000159, loss: 0.5148
2022-10-02 08:29:25 - train: epoch 0002, iter [00830, 01251], lr: 0.000160, loss: 0.5364
2022-10-02 08:29:45 - train: epoch 0002, iter [00840, 01251], lr: 0.000160, loss: 0.5397
2022-10-02 08:30:04 - train: epoch 0002, iter [00850, 01251], lr: 0.000161, loss: 0.5288
2022-10-02 08:30:23 - train: epoch 0002, iter [00860, 01251], lr: 0.000161, loss: 0.5250
2022-10-02 08:30:43 - train: epoch 0002, iter [00870, 01251], lr: 0.000162, loss: 0.5230
2022-10-02 08:31:02 - train: epoch 0002, iter [00880, 01251], lr: 0.000162, loss: 0.5038
2022-10-02 08:31:22 - train: epoch 0002, iter [00890, 01251], lr: 0.000163, loss: 0.5265
2022-10-02 08:31:41 - train: epoch 0002, iter [00900, 01251], lr: 0.000163, loss: 0.5324
2022-10-02 08:32:01 - train: epoch 0002, iter [00910, 01251], lr: 0.000164, loss: 0.5118
2022-10-02 08:32:21 - train: epoch 0002, iter [00920, 01251], lr: 0.000164, loss: 0.5142
2022-10-02 08:32:40 - train: epoch 0002, iter [00930, 01251], lr: 0.000165, loss: 0.5191
2022-10-02 08:33:00 - train: epoch 0002, iter [00940, 01251], lr: 0.000165, loss: 0.5287
2022-10-02 08:33:19 - train: epoch 0002, iter [00950, 01251], lr: 0.000166, loss: 0.5205
2022-10-02 08:33:38 - train: epoch 0002, iter [00960, 01251], lr: 0.000166, loss: 0.5308
2022-10-02 08:33:58 - train: epoch 0002, iter [00970, 01251], lr: 0.000167, loss: 0.5141
2022-10-02 08:34:18 - train: epoch 0002, iter [00980, 01251], lr: 0.000167, loss: 0.4971
2022-10-02 08:34:37 - train: epoch 0002, iter [00990, 01251], lr: 0.000167, loss: 0.5207
2022-10-02 08:34:57 - train: epoch 0002, iter [01000, 01251], lr: 0.000168, loss: 0.5216
2022-10-02 08:35:17 - train: epoch 0002, iter [01010, 01251], lr: 0.000168, loss: 0.5217
2022-10-02 08:35:36 - train: epoch 0002, iter [01020, 01251], lr: 0.000169, loss: 0.5137
2022-10-02 08:35:56 - train: epoch 0002, iter [01030, 01251], lr: 0.000169, loss: 0.5031
2022-10-02 08:36:15 - train: epoch 0002, iter [01040, 01251], lr: 0.000170, loss: 0.5220
2022-10-02 08:36:34 - train: epoch 0002, iter [01050, 01251], lr: 0.000170, loss: 0.5206
2022-10-02 08:36:54 - train: epoch 0002, iter [01060, 01251], lr: 0.000171, loss: 0.5161
2022-10-02 08:37:13 - train: epoch 0002, iter [01070, 01251], lr: 0.000171, loss: 0.5138
2022-10-02 08:37:33 - train: epoch 0002, iter [01080, 01251], lr: 0.000172, loss: 0.5327
2022-10-02 08:37:52 - train: epoch 0002, iter [01090, 01251], lr: 0.000172, loss: 0.5295
2022-10-02 08:38:11 - train: epoch 0002, iter [01100, 01251], lr: 0.000173, loss: 0.5167
2022-10-02 08:38:31 - train: epoch 0002, iter [01110, 01251], lr: 0.000173, loss: 0.5314
2022-10-02 08:38:50 - train: epoch 0002, iter [01120, 01251], lr: 0.000174, loss: 0.5281
2022-10-02 08:39:10 - train: epoch 0002, iter [01130, 01251], lr: 0.000174, loss: 0.5366
2022-10-02 08:39:29 - train: epoch 0002, iter [01140, 01251], lr: 0.000175, loss: 0.5032
2022-10-02 08:39:49 - train: epoch 0002, iter [01150, 01251], lr: 0.000175, loss: 0.4949
2022-10-02 08:40:08 - train: epoch 0002, iter [01160, 01251], lr: 0.000176, loss: 0.5313
2022-10-02 08:40:28 - train: epoch 0002, iter [01170, 01251], lr: 0.000176, loss: 0.5165
2022-10-02 08:40:47 - train: epoch 0002, iter [01180, 01251], lr: 0.000177, loss: 0.5116
2022-10-02 08:41:07 - train: epoch 0002, iter [01190, 01251], lr: 0.000177, loss: 0.5096
2022-10-02 08:41:26 - train: epoch 0002, iter [01200, 01251], lr: 0.000178, loss: 0.5085
2022-10-02 08:41:46 - train: epoch 0002, iter [01210, 01251], lr: 0.000178, loss: 0.4938
2022-10-02 08:42:05 - train: epoch 0002, iter [01220, 01251], lr: 0.000179, loss: 0.5166
2022-10-02 08:42:25 - train: epoch 0002, iter [01230, 01251], lr: 0.000179, loss: 0.5245
2022-10-02 08:42:45 - train: epoch 0002, iter [01240, 01251], lr: 0.000179, loss: 0.5096
2022-10-02 08:43:03 - train: epoch 0002, iter [01250, 01251], lr: 0.000180, loss: 0.5237
2022-10-02 08:43:06 - train: epoch 002, train_loss: 0.5351
2022-10-02 08:43:10 - until epoch: 002, best_loss: 0.5351
2022-10-02 08:43:10 - epoch 003 lr: 0.000180
2022-10-02 08:43:37 - train: epoch 0003, iter [00010, 01251], lr: 0.000180, loss: 0.5105
2022-10-02 08:43:56 - train: epoch 0003, iter [00020, 01251], lr: 0.000181, loss: 0.4907
2022-10-02 08:44:16 - train: epoch 0003, iter [00030, 01251], lr: 0.000181, loss: 0.5073
2022-10-02 08:44:35 - train: epoch 0003, iter [00040, 01251], lr: 0.000182, loss: 0.5047
2022-10-02 08:44:55 - train: epoch 0003, iter [00050, 01251], lr: 0.000182, loss: 0.4950
2022-10-02 08:45:14 - train: epoch 0003, iter [00060, 01251], lr: 0.000183, loss: 0.5110
2022-10-02 08:45:34 - train: epoch 0003, iter [00070, 01251], lr: 0.000183, loss: 0.5043
2022-10-02 08:45:53 - train: epoch 0003, iter [00080, 01251], lr: 0.000184, loss: 0.4953
2022-10-02 08:46:12 - train: epoch 0003, iter [00090, 01251], lr: 0.000184, loss: 0.4962
2022-10-02 08:46:32 - train: epoch 0003, iter [00100, 01251], lr: 0.000185, loss: 0.5022
2022-10-02 08:46:51 - train: epoch 0003, iter [00110, 01251], lr: 0.000185, loss: 0.4920
2022-10-02 08:47:10 - train: epoch 0003, iter [00120, 01251], lr: 0.000186, loss: 0.5108
2022-10-02 08:47:30 - train: epoch 0003, iter [00130, 01251], lr: 0.000186, loss: 0.4896
2022-10-02 08:47:49 - train: epoch 0003, iter [00140, 01251], lr: 0.000187, loss: 0.5000
2022-10-02 08:48:08 - train: epoch 0003, iter [00150, 01251], lr: 0.000187, loss: 0.5073
2022-10-02 08:48:28 - train: epoch 0003, iter [00160, 01251], lr: 0.000188, loss: 0.5007
2022-10-02 08:48:47 - train: epoch 0003, iter [00170, 01251], lr: 0.000188, loss: 0.5245
2022-10-02 08:49:07 - train: epoch 0003, iter [00180, 01251], lr: 0.000189, loss: 0.5014
2022-10-02 08:49:26 - train: epoch 0003, iter [00190, 01251], lr: 0.000189, loss: 0.5024
2022-10-02 08:49:45 - train: epoch 0003, iter [00200, 01251], lr: 0.000190, loss: 0.5127
2022-10-02 08:50:05 - train: epoch 0003, iter [00210, 01251], lr: 0.000190, loss: 0.4997
2022-10-02 08:50:24 - train: epoch 0003, iter [00220, 01251], lr: 0.000191, loss: 0.5221
2022-10-02 08:50:43 - train: epoch 0003, iter [00230, 01251], lr: 0.000191, loss: 0.5021
2022-10-02 08:51:03 - train: epoch 0003, iter [00240, 01251], lr: 0.000192, loss: 0.5024
2022-10-02 08:51:23 - train: epoch 0003, iter [00250, 01251], lr: 0.000192, loss: 0.4977
2022-10-02 08:51:42 - train: epoch 0003, iter [00260, 01251], lr: 0.000192, loss: 0.5022
2022-10-02 08:52:01 - train: epoch 0003, iter [00270, 01251], lr: 0.000193, loss: 0.5063
2022-10-02 08:52:20 - train: epoch 0003, iter [00280, 01251], lr: 0.000193, loss: 0.5141
2022-10-02 08:52:39 - train: epoch 0003, iter [00290, 01251], lr: 0.000194, loss: 0.5169
2022-10-02 08:52:59 - train: epoch 0003, iter [00300, 01251], lr: 0.000194, loss: 0.5164
2022-10-02 08:53:18 - train: epoch 0003, iter [00310, 01251], lr: 0.000195, loss: 0.4973
2022-10-02 08:53:38 - train: epoch 0003, iter [00320, 01251], lr: 0.000195, loss: 0.5201
2022-10-02 08:53:57 - train: epoch 0003, iter [00330, 01251], lr: 0.000196, loss: 0.4978
2022-10-02 08:54:16 - train: epoch 0003, iter [00340, 01251], lr: 0.000196, loss: 0.4930
2022-10-02 08:54:36 - train: epoch 0003, iter [00350, 01251], lr: 0.000197, loss: 0.4905
2022-10-02 08:54:55 - train: epoch 0003, iter [00360, 01251], lr: 0.000197, loss: 0.5027
2022-10-02 08:55:15 - train: epoch 0003, iter [00370, 01251], lr: 0.000198, loss: 0.5081
2022-10-02 08:55:34 - train: epoch 0003, iter [00380, 01251], lr: 0.000198, loss: 0.5002
2022-10-02 08:55:53 - train: epoch 0003, iter [00390, 01251], lr: 0.000199, loss: 0.5125
2022-10-02 08:56:13 - train: epoch 0003, iter [00400, 01251], lr: 0.000199, loss: 0.5066
2022-10-02 08:56:32 - train: epoch 0003, iter [00410, 01251], lr: 0.000200, loss: 0.5102
2022-10-02 08:56:52 - train: epoch 0003, iter [00420, 01251], lr: 0.000200, loss: 0.4921
2022-10-02 08:57:12 - train: epoch 0003, iter [00430, 01251], lr: 0.000201, loss: 0.4916
2022-10-02 08:57:31 - train: epoch 0003, iter [00440, 01251], lr: 0.000201, loss: 0.5068
2022-10-02 08:57:50 - train: epoch 0003, iter [00450, 01251], lr: 0.000202, loss: 0.5246
2022-10-02 08:58:09 - train: epoch 0003, iter [00460, 01251], lr: 0.000202, loss: 0.4991
2022-10-02 08:58:28 - train: epoch 0003, iter [00470, 01251], lr: 0.000203, loss: 0.4897
2022-10-02 08:58:48 - train: epoch 0003, iter [00480, 01251], lr: 0.000203, loss: 0.5039
2022-10-02 08:59:07 - train: epoch 0003, iter [00490, 01251], lr: 0.000204, loss: 0.5040
2022-10-02 08:59:26 - train: epoch 0003, iter [00500, 01251], lr: 0.000204, loss: 0.4890
2022-10-02 08:59:46 - train: epoch 0003, iter [00510, 01251], lr: 0.000204, loss: 0.5169
2022-10-02 09:00:05 - train: epoch 0003, iter [00520, 01251], lr: 0.000205, loss: 0.4904
2022-10-02 09:00:24 - train: epoch 0003, iter [00530, 01251], lr: 0.000205, loss: 0.5020
2022-10-02 09:00:44 - train: epoch 0003, iter [00540, 01251], lr: 0.000206, loss: 0.4859
2022-10-02 09:01:03 - train: epoch 0003, iter [00550, 01251], lr: 0.000206, loss: 0.4815
2022-10-02 09:01:23 - train: epoch 0003, iter [00560, 01251], lr: 0.000207, loss: 0.5090
2022-10-02 09:01:42 - train: epoch 0003, iter [00570, 01251], lr: 0.000207, loss: 0.4764
2022-10-02 09:02:01 - train: epoch 0003, iter [00580, 01251], lr: 0.000208, loss: 0.4944
2022-10-02 09:02:20 - train: epoch 0003, iter [00590, 01251], lr: 0.000208, loss: 0.5154
2022-10-02 09:02:40 - train: epoch 0003, iter [00600, 01251], lr: 0.000209, loss: 0.5166
2022-10-02 09:02:59 - train: epoch 0003, iter [00610, 01251], lr: 0.000209, loss: 0.5047
2022-10-02 09:03:18 - train: epoch 0003, iter [00620, 01251], lr: 0.000210, loss: 0.4897
2022-10-02 09:03:38 - train: epoch 0003, iter [00630, 01251], lr: 0.000210, loss: 0.4796
2022-10-02 09:03:57 - train: epoch 0003, iter [00640, 01251], lr: 0.000211, loss: 0.5023
2022-10-02 09:04:16 - train: epoch 0003, iter [00650, 01251], lr: 0.000211, loss: 0.4798
2022-10-02 09:04:36 - train: epoch 0003, iter [00660, 01251], lr: 0.000212, loss: 0.4865
2022-10-02 09:04:55 - train: epoch 0003, iter [00670, 01251], lr: 0.000212, loss: 0.5011
2022-10-02 09:05:15 - train: epoch 0003, iter [00680, 01251], lr: 0.000213, loss: 0.5110
2022-10-02 09:05:34 - train: epoch 0003, iter [00690, 01251], lr: 0.000213, loss: 0.4837
2022-10-02 09:05:54 - train: epoch 0003, iter [00700, 01251], lr: 0.000214, loss: 0.5037
2022-10-02 09:06:13 - train: epoch 0003, iter [00710, 01251], lr: 0.000214, loss: 0.5017
2022-10-02 09:06:32 - train: epoch 0003, iter [00720, 01251], lr: 0.000215, loss: 0.4983
2022-10-02 09:06:52 - train: epoch 0003, iter [00730, 01251], lr: 0.000215, loss: 0.4679
2022-10-02 09:07:11 - train: epoch 0003, iter [00740, 01251], lr: 0.000215, loss: 0.4810
2022-10-02 09:07:31 - train: epoch 0003, iter [00750, 01251], lr: 0.000216, loss: 0.4799
2022-10-02 09:07:50 - train: epoch 0003, iter [00760, 01251], lr: 0.000216, loss: 0.4909
2022-10-02 09:08:10 - train: epoch 0003, iter [00770, 01251], lr: 0.000217, loss: 0.4647
2022-10-02 09:08:29 - train: epoch 0003, iter [00780, 01251], lr: 0.000217, loss: 0.4946
2022-10-02 09:08:49 - train: epoch 0003, iter [00790, 01251], lr: 0.000218, loss: 0.4903
2022-10-02 09:09:08 - train: epoch 0003, iter [00800, 01251], lr: 0.000218, loss: 0.4835
2022-10-02 09:09:28 - train: epoch 0003, iter [00810, 01251], lr: 0.000219, loss: 0.4862
2022-10-02 09:09:48 - train: epoch 0003, iter [00820, 01251], lr: 0.000219, loss: 0.5019
2022-10-02 09:10:07 - train: epoch 0003, iter [00830, 01251], lr: 0.000220, loss: 0.5037
2022-10-02 09:10:26 - train: epoch 0003, iter [00840, 01251], lr: 0.000220, loss: 0.4993
2022-10-02 09:10:46 - train: epoch 0003, iter [00850, 01251], lr: 0.000221, loss: 0.4852
2022-10-02 09:11:05 - train: epoch 0003, iter [00860, 01251], lr: 0.000221, loss: 0.5083
2022-10-02 09:11:25 - train: epoch 0003, iter [00870, 01251], lr: 0.000222, loss: 0.4968
2022-10-02 09:11:44 - train: epoch 0003, iter [00880, 01251], lr: 0.000222, loss: 0.5008
2022-10-02 09:12:04 - train: epoch 0003, iter [00890, 01251], lr: 0.000223, loss: 0.4898
2022-10-02 09:12:23 - train: epoch 0003, iter [00900, 01251], lr: 0.000223, loss: 0.5071
2022-10-02 09:12:42 - train: epoch 0003, iter [00910, 01251], lr: 0.000224, loss: 0.4898
2022-10-02 09:13:02 - train: epoch 0003, iter [00920, 01251], lr: 0.000224, loss: 0.4911
2022-10-02 09:13:22 - train: epoch 0003, iter [00930, 01251], lr: 0.000225, loss: 0.4969
2022-10-02 09:13:42 - train: epoch 0003, iter [00940, 01251], lr: 0.000225, loss: 0.5009
2022-10-02 09:14:01 - train: epoch 0003, iter [00950, 01251], lr: 0.000226, loss: 0.4839
2022-10-02 09:14:20 - train: epoch 0003, iter [00960, 01251], lr: 0.000226, loss: 0.5232
2022-10-02 09:14:40 - train: epoch 0003, iter [00970, 01251], lr: 0.000227, loss: 0.5117
2022-10-02 09:14:59 - train: epoch 0003, iter [00980, 01251], lr: 0.000227, loss: 0.4856
2022-10-02 09:15:19 - train: epoch 0003, iter [00990, 01251], lr: 0.000227, loss: 0.4849
2022-10-02 09:15:38 - train: epoch 0003, iter [01000, 01251], lr: 0.000228, loss: 0.4978
2022-10-02 09:15:57 - train: epoch 0003, iter [01010, 01251], lr: 0.000228, loss: 0.4876
2022-10-02 09:16:17 - train: epoch 0003, iter [01020, 01251], lr: 0.000229, loss: 0.4759
2022-10-02 09:16:36 - train: epoch 0003, iter [01030, 01251], lr: 0.000229, loss: 0.5071
2022-10-02 09:16:56 - train: epoch 0003, iter [01040, 01251], lr: 0.000230, loss: 0.5089
2022-10-02 09:17:15 - train: epoch 0003, iter [01050, 01251], lr: 0.000230, loss: 0.4912
2022-10-02 09:17:34 - train: epoch 0003, iter [01060, 01251], lr: 0.000231, loss: 0.4934
2022-10-02 09:17:54 - train: epoch 0003, iter [01070, 01251], lr: 0.000231, loss: 0.4904
2022-10-02 09:18:13 - train: epoch 0003, iter [01080, 01251], lr: 0.000232, loss: 0.4928
2022-10-02 09:18:32 - train: epoch 0003, iter [01090, 01251], lr: 0.000232, loss: 0.4928
2022-10-02 09:18:52 - train: epoch 0003, iter [01100, 01251], lr: 0.000233, loss: 0.4961
2022-10-02 09:19:11 - train: epoch 0003, iter [01110, 01251], lr: 0.000233, loss: 0.4782
2022-10-02 09:19:31 - train: epoch 0003, iter [01120, 01251], lr: 0.000234, loss: 0.4943
2022-10-02 09:19:50 - train: epoch 0003, iter [01130, 01251], lr: 0.000234, loss: 0.4891
2022-10-02 09:20:09 - train: epoch 0003, iter [01140, 01251], lr: 0.000235, loss: 0.4790
2022-10-02 09:20:29 - train: epoch 0003, iter [01150, 01251], lr: 0.000235, loss: 0.4909
2022-10-02 09:20:48 - train: epoch 0003, iter [01160, 01251], lr: 0.000236, loss: 0.4805
2022-10-02 09:21:07 - train: epoch 0003, iter [01170, 01251], lr: 0.000236, loss: 0.4785
2022-10-02 09:21:27 - train: epoch 0003, iter [01180, 01251], lr: 0.000237, loss: 0.4623
2022-10-02 09:21:46 - train: epoch 0003, iter [01190, 01251], lr: 0.000237, loss: 0.4769
2022-10-02 09:22:06 - train: epoch 0003, iter [01200, 01251], lr: 0.000238, loss: 0.4958
2022-10-02 09:22:25 - train: epoch 0003, iter [01210, 01251], lr: 0.000238, loss: 0.4912
2022-10-02 09:22:45 - train: epoch 0003, iter [01220, 01251], lr: 0.000239, loss: 0.4876
2022-10-02 09:23:04 - train: epoch 0003, iter [01230, 01251], lr: 0.000239, loss: 0.4804
2022-10-02 09:23:23 - train: epoch 0003, iter [01240, 01251], lr: 0.000239, loss: 0.5016
2022-10-02 09:23:42 - train: epoch 0003, iter [01250, 01251], lr: 0.000240, loss: 0.5028
2022-10-02 09:23:45 - train: epoch 003, train_loss: 0.4978
2022-10-02 09:23:50 - until epoch: 003, best_loss: 0.4978
2022-10-02 09:23:50 - epoch 004 lr: 0.000240
2022-10-02 09:24:16 - train: epoch 0004, iter [00010, 01251], lr: 0.000240, loss: 0.5029
2022-10-02 09:24:35 - train: epoch 0004, iter [00020, 01251], lr: 0.000241, loss: 0.4836
2022-10-02 09:24:54 - train: epoch 0004, iter [00030, 01251], lr: 0.000241, loss: 0.4861
2022-10-02 09:25:13 - train: epoch 0004, iter [00040, 01251], lr: 0.000242, loss: 0.4971
2022-10-02 09:25:33 - train: epoch 0004, iter [00050, 01251], lr: 0.000242, loss: 0.4990
2022-10-02 09:25:52 - train: epoch 0004, iter [00060, 01251], lr: 0.000243, loss: 0.5075
2022-10-02 09:26:11 - train: epoch 0004, iter [00070, 01251], lr: 0.000243, loss: 0.4827
2022-10-02 09:26:31 - train: epoch 0004, iter [00080, 01251], lr: 0.000244, loss: 0.4882
2022-10-02 09:26:51 - train: epoch 0004, iter [00090, 01251], lr: 0.000244, loss: 0.4937
2022-10-02 09:27:10 - train: epoch 0004, iter [00100, 01251], lr: 0.000245, loss: 0.4893
2022-10-02 09:27:30 - train: epoch 0004, iter [00110, 01251], lr: 0.000245, loss: 0.4958
2022-10-02 09:27:49 - train: epoch 0004, iter [00120, 01251], lr: 0.000246, loss: 0.4792
2022-10-02 09:28:08 - train: epoch 0004, iter [00130, 01251], lr: 0.000246, loss: 0.4974
2022-10-02 09:28:28 - train: epoch 0004, iter [00140, 01251], lr: 0.000247, loss: 0.4952
2022-10-02 09:28:47 - train: epoch 0004, iter [00150, 01251], lr: 0.000247, loss: 0.4815
2022-10-02 09:29:06 - train: epoch 0004, iter [00160, 01251], lr: 0.000248, loss: 0.4761
2022-10-02 09:29:26 - train: epoch 0004, iter [00170, 01251], lr: 0.000248, loss: 0.4597
2022-10-02 09:29:45 - train: epoch 0004, iter [00180, 01251], lr: 0.000249, loss: 0.4751
2022-10-02 09:30:04 - train: epoch 0004, iter [00190, 01251], lr: 0.000249, loss: 0.4700
2022-10-02 09:30:23 - train: epoch 0004, iter [00200, 01251], lr: 0.000250, loss: 0.4871
2022-10-02 09:30:42 - train: epoch 0004, iter [00210, 01251], lr: 0.000250, loss: 0.4783
2022-10-02 09:31:02 - train: epoch 0004, iter [00220, 01251], lr: 0.000251, loss: 0.4905
2022-10-02 09:31:21 - train: epoch 0004, iter [00230, 01251], lr: 0.000251, loss: 0.4777
2022-10-02 09:31:40 - train: epoch 0004, iter [00240, 01251], lr: 0.000252, loss: 0.4727
2022-10-02 09:31:59 - train: epoch 0004, iter [00250, 01251], lr: 0.000252, loss: 0.4770
2022-10-02 09:32:18 - train: epoch 0004, iter [00260, 01251], lr: 0.000252, loss: 0.4841
2022-10-02 09:32:38 - train: epoch 0004, iter [00270, 01251], lr: 0.000253, loss: 0.4916
2022-10-02 09:32:57 - train: epoch 0004, iter [00280, 01251], lr: 0.000253, loss: 0.5000
2022-10-02 09:33:17 - train: epoch 0004, iter [00290, 01251], lr: 0.000254, loss: 0.4603
2022-10-02 09:33:36 - train: epoch 0004, iter [00300, 01251], lr: 0.000254, loss: 0.4867
2022-10-02 09:33:55 - train: epoch 0004, iter [00310, 01251], lr: 0.000255, loss: 0.4904
2022-10-02 09:34:15 - train: epoch 0004, iter [00320, 01251], lr: 0.000255, loss: 0.4700
2022-10-02 09:34:34 - train: epoch 0004, iter [00330, 01251], lr: 0.000256, loss: 0.4786
2022-10-02 09:34:53 - train: epoch 0004, iter [00340, 01251], lr: 0.000256, loss: 0.4973
2022-10-02 09:35:12 - train: epoch 0004, iter [00350, 01251], lr: 0.000257, loss: 0.4841
2022-10-02 09:35:31 - train: epoch 0004, iter [00360, 01251], lr: 0.000257, loss: 0.4832
2022-10-02 09:35:51 - train: epoch 0004, iter [00370, 01251], lr: 0.000258, loss: 0.4803
2022-10-02 09:36:10 - train: epoch 0004, iter [00380, 01251], lr: 0.000258, loss: 0.4744
2022-10-02 09:36:29 - train: epoch 0004, iter [00390, 01251], lr: 0.000259, loss: 0.5017
2022-10-02 09:36:49 - train: epoch 0004, iter [00400, 01251], lr: 0.000259, loss: 0.4808
2022-10-02 09:37:09 - train: epoch 0004, iter [00410, 01251], lr: 0.000260, loss: 0.4907
2022-10-02 09:37:28 - train: epoch 0004, iter [00420, 01251], lr: 0.000260, loss: 0.5026
2022-10-02 09:37:48 - train: epoch 0004, iter [00430, 01251], lr: 0.000261, loss: 0.4752
2022-10-02 09:38:07 - train: epoch 0004, iter [00440, 01251], lr: 0.000261, loss: 0.4721
2022-10-02 09:38:26 - train: epoch 0004, iter [00450, 01251], lr: 0.000262, loss: 0.4980
2022-10-02 09:38:46 - train: epoch 0004, iter [00460, 01251], lr: 0.000262, loss: 0.4694
2022-10-02 09:39:05 - train: epoch 0004, iter [00470, 01251], lr: 0.000263, loss: 0.4900
2022-10-02 09:39:24 - train: epoch 0004, iter [00480, 01251], lr: 0.000263, loss: 0.4537
2022-10-02 09:39:44 - train: epoch 0004, iter [00490, 01251], lr: 0.000264, loss: 0.4659
2022-10-02 09:40:03 - train: epoch 0004, iter [00500, 01251], lr: 0.000264, loss: 0.4819
2022-10-02 09:40:23 - train: epoch 0004, iter [00510, 01251], lr: 0.000264, loss: 0.4674
2022-10-02 09:40:42 - train: epoch 0004, iter [00520, 01251], lr: 0.000265, loss: 0.4874
2022-10-02 09:41:01 - train: epoch 0004, iter [00530, 01251], lr: 0.000265, loss: 0.4782
2022-10-02 09:41:21 - train: epoch 0004, iter [00540, 01251], lr: 0.000266, loss: 0.4661
2022-10-02 09:41:40 - train: epoch 0004, iter [00550, 01251], lr: 0.000266, loss: 0.4674
2022-10-02 09:42:00 - train: epoch 0004, iter [00560, 01251], lr: 0.000267, loss: 0.4703
2022-10-02 09:42:19 - train: epoch 0004, iter [00570, 01251], lr: 0.000267, loss: 0.4716
2022-10-02 09:42:38 - train: epoch 0004, iter [00580, 01251], lr: 0.000268, loss: 0.4885
2022-10-02 09:42:57 - train: epoch 0004, iter [00590, 01251], lr: 0.000268, loss: 0.4705
2022-10-02 09:43:17 - train: epoch 0004, iter [00600, 01251], lr: 0.000269, loss: 0.4834
2022-10-02 09:43:36 - train: epoch 0004, iter [00610, 01251], lr: 0.000269, loss: 0.4804
2022-10-02 09:43:56 - train: epoch 0004, iter [00620, 01251], lr: 0.000270, loss: 0.4635
2022-10-02 09:44:15 - train: epoch 0004, iter [00630, 01251], lr: 0.000270, loss: 0.4834
2022-10-02 09:44:35 - train: epoch 0004, iter [00640, 01251], lr: 0.000271, loss: 0.4964
2022-10-02 09:44:54 - train: epoch 0004, iter [00650, 01251], lr: 0.000271, loss: 0.4403
2022-10-02 09:45:14 - train: epoch 0004, iter [00660, 01251], lr: 0.000272, loss: 0.4857
2022-10-02 09:45:33 - train: epoch 0004, iter [00670, 01251], lr: 0.000272, loss: 0.5024
2022-10-02 09:45:53 - train: epoch 0004, iter [00680, 01251], lr: 0.000273, loss: 0.4767
2022-10-02 09:46:12 - train: epoch 0004, iter [00690, 01251], lr: 0.000273, loss: 0.4735
2022-10-02 09:46:31 - train: epoch 0004, iter [00700, 01251], lr: 0.000274, loss: 0.4779
2022-10-02 09:46:51 - train: epoch 0004, iter [00710, 01251], lr: 0.000274, loss: 0.4817
2022-10-02 09:47:10 - train: epoch 0004, iter [00720, 01251], lr: 0.000275, loss: 0.4671
2022-10-02 09:47:29 - train: epoch 0004, iter [00730, 01251], lr: 0.000275, loss: 0.4768
2022-10-02 09:47:49 - train: epoch 0004, iter [00740, 01251], lr: 0.000275, loss: 0.4932
2022-10-02 09:48:08 - train: epoch 0004, iter [00750, 01251], lr: 0.000276, loss: 0.4766
2022-10-02 09:48:27 - train: epoch 0004, iter [00760, 01251], lr: 0.000276, loss: 0.4894
2022-10-02 09:48:47 - train: epoch 0004, iter [00770, 01251], lr: 0.000277, loss: 0.4795
2022-10-02 09:49:06 - train: epoch 0004, iter [00780, 01251], lr: 0.000277, loss: 0.4876
2022-10-02 09:49:25 - train: epoch 0004, iter [00790, 01251], lr: 0.000278, loss: 0.4822
2022-10-02 09:49:45 - train: epoch 0004, iter [00800, 01251], lr: 0.000278, loss: 0.4619
2022-10-02 09:50:04 - train: epoch 0004, iter [00810, 01251], lr: 0.000279, loss: 0.4765
2022-10-02 09:50:23 - train: epoch 0004, iter [00820, 01251], lr: 0.000279, loss: 0.4797
2022-10-02 09:50:42 - train: epoch 0004, iter [00830, 01251], lr: 0.000280, loss: 0.4484
2022-10-02 09:51:02 - train: epoch 0004, iter [00840, 01251], lr: 0.000280, loss: 0.4897
2022-10-02 09:51:21 - train: epoch 0004, iter [00850, 01251], lr: 0.000281, loss: 0.4749
2022-10-02 09:51:40 - train: epoch 0004, iter [00860, 01251], lr: 0.000281, loss: 0.4927
2022-10-02 09:51:59 - train: epoch 0004, iter [00870, 01251], lr: 0.000282, loss: 0.5069
2022-10-02 09:52:19 - train: epoch 0004, iter [00880, 01251], lr: 0.000282, loss: 0.4840
2022-10-02 09:52:38 - train: epoch 0004, iter [00890, 01251], lr: 0.000283, loss: 0.4727
2022-10-02 09:52:57 - train: epoch 0004, iter [00900, 01251], lr: 0.000283, loss: 0.4777
2022-10-02 09:53:16 - train: epoch 0004, iter [00910, 01251], lr: 0.000284, loss: 0.4948
2022-10-02 09:53:36 - train: epoch 0004, iter [00920, 01251], lr: 0.000284, loss: 0.4795
2022-10-02 09:53:55 - train: epoch 0004, iter [00930, 01251], lr: 0.000285, loss: 0.4676
2022-10-02 09:54:15 - train: epoch 0004, iter [00940, 01251], lr: 0.000285, loss: 0.4937
2022-10-02 09:54:34 - train: epoch 0004, iter [00950, 01251], lr: 0.000286, loss: 0.4747
2022-10-02 09:54:54 - train: epoch 0004, iter [00960, 01251], lr: 0.000286, loss: 0.4853
2022-10-02 09:55:13 - train: epoch 0004, iter [00970, 01251], lr: 0.000287, loss: 0.4559
2022-10-02 09:55:32 - train: epoch 0004, iter [00980, 01251], lr: 0.000287, loss: 0.4814
2022-10-02 09:55:52 - train: epoch 0004, iter [00990, 01251], lr: 0.000287, loss: 0.4557
2022-10-02 09:56:11 - train: epoch 0004, iter [01000, 01251], lr: 0.000288, loss: 0.4838
2022-10-02 09:56:31 - train: epoch 0004, iter [01010, 01251], lr: 0.000288, loss: 0.4770
2022-10-02 09:56:50 - train: epoch 0004, iter [01020, 01251], lr: 0.000289, loss: 0.4586
2022-10-02 09:57:10 - train: epoch 0004, iter [01030, 01251], lr: 0.000289, loss: 0.4571
2022-10-02 09:57:29 - train: epoch 0004, iter [01040, 01251], lr: 0.000290, loss: 0.4775
2022-10-02 09:57:49 - train: epoch 0004, iter [01050, 01251], lr: 0.000290, loss: 0.4703
2022-10-02 09:58:08 - train: epoch 0004, iter [01060, 01251], lr: 0.000291, loss: 0.4627
2022-10-02 09:58:28 - train: epoch 0004, iter [01070, 01251], lr: 0.000291, loss: 0.4601
2022-10-02 09:58:47 - train: epoch 0004, iter [01080, 01251], lr: 0.000292, loss: 0.4805
2022-10-02 09:59:07 - train: epoch 0004, iter [01090, 01251], lr: 0.000292, loss: 0.4726
2022-10-02 09:59:26 - train: epoch 0004, iter [01100, 01251], lr: 0.000293, loss: 0.4749
2022-10-02 09:59:45 - train: epoch 0004, iter [01110, 01251], lr: 0.000293, loss: 0.4568
2022-10-02 10:00:05 - train: epoch 0004, iter [01120, 01251], lr: 0.000294, loss: 0.4752
2022-10-02 10:00:24 - train: epoch 0004, iter [01130, 01251], lr: 0.000294, loss: 0.4894
2022-10-02 10:00:43 - train: epoch 0004, iter [01140, 01251], lr: 0.000295, loss: 0.4519
2022-10-02 10:01:03 - train: epoch 0004, iter [01150, 01251], lr: 0.000295, loss: 0.4908
2022-10-02 10:01:22 - train: epoch 0004, iter [01160, 01251], lr: 0.000296, loss: 0.4643
2022-10-02 10:01:41 - train: epoch 0004, iter [01170, 01251], lr: 0.000296, loss: 0.4625
2022-10-02 10:02:01 - train: epoch 0004, iter [01180, 01251], lr: 0.000297, loss: 0.4765
2022-10-02 10:02:20 - train: epoch 0004, iter [01190, 01251], lr: 0.000297, loss: 0.4861
2022-10-02 10:02:39 - train: epoch 0004, iter [01200, 01251], lr: 0.000298, loss: 0.4792
2022-10-02 10:02:59 - train: epoch 0004, iter [01210, 01251], lr: 0.000298, loss: 0.4728
2022-10-02 10:03:18 - train: epoch 0004, iter [01220, 01251], lr: 0.000299, loss: 0.4697
2022-10-02 10:03:38 - train: epoch 0004, iter [01230, 01251], lr: 0.000299, loss: 0.4667
2022-10-02 10:03:57 - train: epoch 0004, iter [01240, 01251], lr: 0.000299, loss: 0.4770
2022-10-02 10:04:15 - train: epoch 0004, iter [01250, 01251], lr: 0.000300, loss: 0.4724
2022-10-02 10:04:18 - train: epoch 004, train_loss: 0.4800
2022-10-02 10:04:23 - until epoch: 004, best_loss: 0.4800
2022-10-02 10:04:23 - epoch 005 lr: 0.000300
2022-10-02 10:04:49 - train: epoch 0005, iter [00010, 01251], lr: 0.000300, loss: 0.4732
2022-10-02 10:05:08 - train: epoch 0005, iter [00020, 01251], lr: 0.000301, loss: 0.4989
2022-10-02 10:05:28 - train: epoch 0005, iter [00030, 01251], lr: 0.000301, loss: 0.4713
2022-10-02 10:05:48 - train: epoch 0005, iter [00040, 01251], lr: 0.000302, loss: 0.4880
2022-10-02 10:06:07 - train: epoch 0005, iter [00050, 01251], lr: 0.000302, loss: 0.4864
2022-10-02 10:06:27 - train: epoch 0005, iter [00060, 01251], lr: 0.000303, loss: 0.4847
2022-10-02 10:06:46 - train: epoch 0005, iter [00070, 01251], lr: 0.000303, loss: 0.4681
2022-10-02 10:07:06 - train: epoch 0005, iter [00080, 01251], lr: 0.000304, loss: 0.4717
2022-10-02 10:07:25 - train: epoch 0005, iter [00090, 01251], lr: 0.000304, loss: 0.4605
2022-10-02 10:07:44 - train: epoch 0005, iter [00100, 01251], lr: 0.000305, loss: 0.4783
2022-10-02 10:08:04 - train: epoch 0005, iter [00110, 01251], lr: 0.000305, loss: 0.4736
2022-10-02 10:08:23 - train: epoch 0005, iter [00120, 01251], lr: 0.000306, loss: 0.4562
2022-10-02 10:08:43 - train: epoch 0005, iter [00130, 01251], lr: 0.000306, loss: 0.4938
2022-10-02 10:09:02 - train: epoch 0005, iter [00140, 01251], lr: 0.000307, loss: 0.4771
2022-10-02 10:09:22 - train: epoch 0005, iter [00150, 01251], lr: 0.000307, loss: 0.4853
2022-10-02 10:09:41 - train: epoch 0005, iter [00160, 01251], lr: 0.000308, loss: 0.4789
2022-10-02 10:10:00 - train: epoch 0005, iter [00170, 01251], lr: 0.000308, loss: 0.4639
2022-10-02 10:10:20 - train: epoch 0005, iter [00180, 01251], lr: 0.000309, loss: 0.4796
2022-10-02 10:10:40 - train: epoch 0005, iter [00190, 01251], lr: 0.000309, loss: 0.4858
2022-10-02 10:10:59 - train: epoch 0005, iter [00200, 01251], lr: 0.000310, loss: 0.4525
2022-10-02 10:11:18 - train: epoch 0005, iter [00210, 01251], lr: 0.000310, loss: 0.4887
2022-10-02 10:11:37 - train: epoch 0005, iter [00220, 01251], lr: 0.000311, loss: 0.4736
2022-10-02 10:11:56 - train: epoch 0005, iter [00230, 01251], lr: 0.000311, loss: 0.4820
2022-10-02 10:12:15 - train: epoch 0005, iter [00240, 01251], lr: 0.000312, loss: 0.4758
2022-10-02 10:12:35 - train: epoch 0005, iter [00250, 01251], lr: 0.000312, loss: 0.4726
2022-10-02 10:12:54 - train: epoch 0005, iter [00260, 01251], lr: 0.000312, loss: 0.4512
2022-10-02 10:13:13 - train: epoch 0005, iter [00270, 01251], lr: 0.000313, loss: 0.4893
2022-10-02 10:13:32 - train: epoch 0005, iter [00280, 01251], lr: 0.000313, loss: 0.4682
2022-10-02 10:13:52 - train: epoch 0005, iter [00290, 01251], lr: 0.000314, loss: 0.4741
2022-10-02 10:14:11 - train: epoch 0005, iter [00300, 01251], lr: 0.000314, loss: 0.4560
2022-10-02 10:14:30 - train: epoch 0005, iter [00310, 01251], lr: 0.000315, loss: 0.4718
2022-10-02 10:14:49 - train: epoch 0005, iter [00320, 01251], lr: 0.000315, loss: 0.4846
2022-10-02 10:15:09 - train: epoch 0005, iter [00330, 01251], lr: 0.000316, loss: 0.4785
2022-10-02 10:15:28 - train: epoch 0005, iter [00340, 01251], lr: 0.000316, loss: 0.4732
2022-10-02 10:15:47 - train: epoch 0005, iter [00350, 01251], lr: 0.000317, loss: 0.4557
2022-10-02 10:16:06 - train: epoch 0005, iter [00360, 01251], lr: 0.000317, loss: 0.4850
2022-10-02 10:16:25 - train: epoch 0005, iter [00370, 01251], lr: 0.000318, loss: 0.4665
2022-10-02 10:16:44 - train: epoch 0005, iter [00380, 01251], lr: 0.000318, loss: 0.4900
2022-10-02 10:17:04 - train: epoch 0005, iter [00390, 01251], lr: 0.000319, loss: 0.4736
2022-10-02 10:17:23 - train: epoch 0005, iter [00400, 01251], lr: 0.000319, loss: 0.4944
2022-10-02 10:17:42 - train: epoch 0005, iter [00410, 01251], lr: 0.000320, loss: 0.4767
2022-10-02 10:18:01 - train: epoch 0005, iter [00420, 01251], lr: 0.000320, loss: 0.4663
2022-10-02 10:18:21 - train: epoch 0005, iter [00430, 01251], lr: 0.000321, loss: 0.4356
2022-10-02 10:18:40 - train: epoch 0005, iter [00440, 01251], lr: 0.000321, loss: 0.4734
2022-10-02 10:18:59 - train: epoch 0005, iter [00450, 01251], lr: 0.000322, loss: 0.4705
2022-10-02 10:19:19 - train: epoch 0005, iter [00460, 01251], lr: 0.000322, loss: 0.4574
2022-10-02 10:19:38 - train: epoch 0005, iter [00470, 01251], lr: 0.000323, loss: 0.4755
2022-10-02 10:19:57 - train: epoch 0005, iter [00480, 01251], lr: 0.000323, loss: 0.4780
2022-10-02 10:20:17 - train: epoch 0005, iter [00490, 01251], lr: 0.000324, loss: 0.4636
2022-10-02 10:20:36 - train: epoch 0005, iter [00500, 01251], lr: 0.000324, loss: 0.4796
2022-10-02 10:20:55 - train: epoch 0005, iter [00510, 01251], lr: 0.000324, loss: 0.4966
2022-10-02 10:21:15 - train: epoch 0005, iter [00520, 01251], lr: 0.000325, loss: 0.4830
2022-10-02 10:21:34 - train: epoch 0005, iter [00530, 01251], lr: 0.000325, loss: 0.4594
2022-10-02 10:21:54 - train: epoch 0005, iter [00540, 01251], lr: 0.000326, loss: 0.4648
2022-10-02 10:22:13 - train: epoch 0005, iter [00550, 01251], lr: 0.000326, loss: 0.4858
2022-10-02 10:22:33 - train: epoch 0005, iter [00560, 01251], lr: 0.000327, loss: 0.4811
2022-10-02 10:22:52 - train: epoch 0005, iter [00570, 01251], lr: 0.000327, loss: 0.4821
2022-10-02 10:23:11 - train: epoch 0005, iter [00580, 01251], lr: 0.000328, loss: 0.4771
2022-10-02 10:23:31 - train: epoch 0005, iter [00590, 01251], lr: 0.000328, loss: 0.4674
2022-10-02 10:23:50 - train: epoch 0005, iter [00600, 01251], lr: 0.000329, loss: 0.4707
2022-10-02 10:24:09 - train: epoch 0005, iter [00610, 01251], lr: 0.000329, loss: 0.4839
2022-10-02 10:24:29 - train: epoch 0005, iter [00620, 01251], lr: 0.000330, loss: 0.4414
2022-10-02 10:24:49 - train: epoch 0005, iter [00630, 01251], lr: 0.000330, loss: 0.4685
2022-10-02 10:25:09 - train: epoch 0005, iter [00640, 01251], lr: 0.000331, loss: 0.4567
2022-10-02 10:25:28 - train: epoch 0005, iter [00650, 01251], lr: 0.000331, loss: 0.4823
2022-10-02 10:25:47 - train: epoch 0005, iter [00660, 01251], lr: 0.000332, loss: 0.4818
2022-10-02 10:26:07 - train: epoch 0005, iter [00670, 01251], lr: 0.000332, loss: 0.4665
2022-10-02 10:26:27 - train: epoch 0005, iter [00680, 01251], lr: 0.000333, loss: 0.4661
2022-10-02 10:26:47 - train: epoch 0005, iter [00690, 01251], lr: 0.000333, loss: 0.4542
2022-10-02 10:27:06 - train: epoch 0005, iter [00700, 01251], lr: 0.000334, loss: 0.4594
2022-10-02 10:27:26 - train: epoch 0005, iter [00710, 01251], lr: 0.000334, loss: 0.4747
2022-10-02 10:27:45 - train: epoch 0005, iter [00720, 01251], lr: 0.000335, loss: 0.4764
2022-10-02 10:28:05 - train: epoch 0005, iter [00730, 01251], lr: 0.000335, loss: 0.4666
2022-10-02 10:28:24 - train: epoch 0005, iter [00740, 01251], lr: 0.000335, loss: 0.4619
2022-10-02 10:28:43 - train: epoch 0005, iter [00750, 01251], lr: 0.000336, loss: 0.4790
2022-10-02 10:29:02 - train: epoch 0005, iter [00760, 01251], lr: 0.000336, loss: 0.4559
2022-10-02 10:29:22 - train: epoch 0005, iter [00770, 01251], lr: 0.000337, loss: 0.4759
2022-10-02 10:29:42 - train: epoch 0005, iter [00780, 01251], lr: 0.000337, loss: 0.4640
2022-10-02 10:30:01 - train: epoch 0005, iter [00790, 01251], lr: 0.000338, loss: 0.4571
2022-10-02 10:30:20 - train: epoch 0005, iter [00800, 01251], lr: 0.000338, loss: 0.4643
2022-10-02 10:30:40 - train: epoch 0005, iter [00810, 01251], lr: 0.000339, loss: 0.4569
2022-10-02 10:30:59 - train: epoch 0005, iter [00820, 01251], lr: 0.000339, loss: 0.4618
2022-10-02 10:31:19 - train: epoch 0005, iter [00830, 01251], lr: 0.000340, loss: 0.4510
2022-10-02 10:31:38 - train: epoch 0005, iter [00840, 01251], lr: 0.000340, loss: 0.4600
2022-10-02 10:31:58 - train: epoch 0005, iter [00850, 01251], lr: 0.000341, loss: 0.4713
2022-10-02 10:32:17 - train: epoch 0005, iter [00860, 01251], lr: 0.000341, loss: 0.4503
2022-10-02 10:32:36 - train: epoch 0005, iter [00870, 01251], lr: 0.000342, loss: 0.4960
2022-10-02 10:32:55 - train: epoch 0005, iter [00880, 01251], lr: 0.000342, loss: 0.4684
2022-10-02 10:33:15 - train: epoch 0005, iter [00890, 01251], lr: 0.000343, loss: 0.4624
2022-10-02 10:33:34 - train: epoch 0005, iter [00900, 01251], lr: 0.000343, loss: 0.4511
2022-10-02 10:33:53 - train: epoch 0005, iter [00910, 01251], lr: 0.000344, loss: 0.4660
2022-10-02 10:34:13 - train: epoch 0005, iter [00920, 01251], lr: 0.000344, loss: 0.4769
2022-10-02 10:34:33 - train: epoch 0005, iter [00930, 01251], lr: 0.000345, loss: 0.4913
2022-10-02 10:34:52 - train: epoch 0005, iter [00940, 01251], lr: 0.000345, loss: 0.4850
2022-10-02 10:35:11 - train: epoch 0005, iter [00950, 01251], lr: 0.000346, loss: 0.4716
2022-10-02 10:35:31 - train: epoch 0005, iter [00960, 01251], lr: 0.000346, loss: 0.4569
2022-10-02 10:35:51 - train: epoch 0005, iter [00970, 01251], lr: 0.000347, loss: 0.4463
2022-10-02 10:36:10 - train: epoch 0005, iter [00980, 01251], lr: 0.000347, loss: 0.4894
2022-10-02 10:36:30 - train: epoch 0005, iter [00990, 01251], lr: 0.000347, loss: 0.4856
2022-10-02 10:36:49 - train: epoch 0005, iter [01000, 01251], lr: 0.000348, loss: 0.4500
2022-10-02 10:37:09 - train: epoch 0005, iter [01010, 01251], lr: 0.000348, loss: 0.4691
2022-10-02 10:37:29 - train: epoch 0005, iter [01020, 01251], lr: 0.000349, loss: 0.4614
2022-10-02 10:37:48 - train: epoch 0005, iter [01030, 01251], lr: 0.000349, loss: 0.4597
2022-10-02 10:38:07 - train: epoch 0005, iter [01040, 01251], lr: 0.000350, loss: 0.4736
2022-10-02 10:38:27 - train: epoch 0005, iter [01050, 01251], lr: 0.000350, loss: 0.4566
2022-10-02 10:38:46 - train: epoch 0005, iter [01060, 01251], lr: 0.000351, loss: 0.4618
2022-10-02 10:39:06 - train: epoch 0005, iter [01070, 01251], lr: 0.000351, loss: 0.4535
2022-10-02 10:39:25 - train: epoch 0005, iter [01080, 01251], lr: 0.000352, loss: 0.4660
2022-10-02 10:39:45 - train: epoch 0005, iter [01090, 01251], lr: 0.000352, loss: 0.4672
2022-10-02 10:40:05 - train: epoch 0005, iter [01100, 01251], lr: 0.000353, loss: 0.4643
2022-10-02 10:40:24 - train: epoch 0005, iter [01110, 01251], lr: 0.000353, loss: 0.4513
2022-10-02 10:40:44 - train: epoch 0005, iter [01120, 01251], lr: 0.000354, loss: 0.4697
2022-10-02 10:41:04 - train: epoch 0005, iter [01130, 01251], lr: 0.000354, loss: 0.4436
2022-10-02 10:41:23 - train: epoch 0005, iter [01140, 01251], lr: 0.000355, loss: 0.4658
2022-10-02 10:41:43 - train: epoch 0005, iter [01150, 01251], lr: 0.000355, loss: 0.4591
2022-10-02 10:42:02 - train: epoch 0005, iter [01160, 01251], lr: 0.000356, loss: 0.4802
2022-10-02 10:42:22 - train: epoch 0005, iter [01170, 01251], lr: 0.000356, loss: 0.4668
2022-10-02 10:42:41 - train: epoch 0005, iter [01180, 01251], lr: 0.000357, loss: 0.4499
2022-10-02 10:43:01 - train: epoch 0005, iter [01190, 01251], lr: 0.000357, loss: 0.4682
2022-10-02 10:43:21 - train: epoch 0005, iter [01200, 01251], lr: 0.000358, loss: 0.4694
2022-10-02 10:43:40 - train: epoch 0005, iter [01210, 01251], lr: 0.000358, loss: 0.4814
2022-10-02 10:44:00 - train: epoch 0005, iter [01220, 01251], lr: 0.000359, loss: 0.4710
2022-10-02 10:44:19 - train: epoch 0005, iter [01230, 01251], lr: 0.000359, loss: 0.4501
2022-10-02 10:44:38 - train: epoch 0005, iter [01240, 01251], lr: 0.000359, loss: 0.4571
2022-10-02 10:44:57 - train: epoch 0005, iter [01250, 01251], lr: 0.000360, loss: 0.4672
2022-10-02 10:45:00 - train: epoch 005, train_loss: 0.4687
2022-10-02 10:45:05 - until epoch: 005, best_loss: 0.4687
2022-10-02 10:45:05 - epoch 006 lr: 0.000360
2022-10-02 10:45:30 - train: epoch 0006, iter [00010, 01251], lr: 0.000360, loss: 0.4827
2022-10-02 10:45:50 - train: epoch 0006, iter [00020, 01251], lr: 0.000361, loss: 0.4530
2022-10-02 10:46:09 - train: epoch 0006, iter [00030, 01251], lr: 0.000361, loss: 0.4612
2022-10-02 10:46:29 - train: epoch 0006, iter [00040, 01251], lr: 0.000362, loss: 0.4589
2022-10-02 10:46:48 - train: epoch 0006, iter [00050, 01251], lr: 0.000362, loss: 0.4676
2022-10-02 10:47:07 - train: epoch 0006, iter [00060, 01251], lr: 0.000363, loss: 0.4723
2022-10-02 10:47:26 - train: epoch 0006, iter [00070, 01251], lr: 0.000363, loss: 0.4665
2022-10-02 10:47:46 - train: epoch 0006, iter [00080, 01251], lr: 0.000364, loss: 0.4518
2022-10-02 10:48:05 - train: epoch 0006, iter [00090, 01251], lr: 0.000364, loss: 0.4517
2022-10-02 10:48:24 - train: epoch 0006, iter [00100, 01251], lr: 0.000365, loss: 0.4606
2022-10-02 10:48:44 - train: epoch 0006, iter [00110, 01251], lr: 0.000365, loss: 0.4633
2022-10-02 10:49:03 - train: epoch 0006, iter [00120, 01251], lr: 0.000366, loss: 0.4616
2022-10-02 10:49:23 - train: epoch 0006, iter [00130, 01251], lr: 0.000366, loss: 0.4478
2022-10-02 10:49:42 - train: epoch 0006, iter [00140, 01251], lr: 0.000367, loss: 0.4769
2022-10-02 10:50:02 - train: epoch 0006, iter [00150, 01251], lr: 0.000367, loss: 0.4626
2022-10-02 10:50:21 - train: epoch 0006, iter [00160, 01251], lr: 0.000368, loss: 0.4498
2022-10-02 10:50:40 - train: epoch 0006, iter [00170, 01251], lr: 0.000368, loss: 0.4705
2022-10-02 10:50:59 - train: epoch 0006, iter [00180, 01251], lr: 0.000369, loss: 0.4820
2022-10-02 10:51:19 - train: epoch 0006, iter [00190, 01251], lr: 0.000369, loss: 0.4632
2022-10-02 10:51:38 - train: epoch 0006, iter [00200, 01251], lr: 0.000370, loss: 0.4577
2022-10-02 10:51:57 - train: epoch 0006, iter [00210, 01251], lr: 0.000370, loss: 0.4693
2022-10-02 10:52:16 - train: epoch 0006, iter [00220, 01251], lr: 0.000371, loss: 0.4559
2022-10-02 10:52:36 - train: epoch 0006, iter [00230, 01251], lr: 0.000371, loss: 0.4886
2022-10-02 10:52:55 - train: epoch 0006, iter [00240, 01251], lr: 0.000372, loss: 0.4616
2022-10-02 10:53:14 - train: epoch 0006, iter [00250, 01251], lr: 0.000372, loss: 0.4567
2022-10-02 10:53:34 - train: epoch 0006, iter [00260, 01251], lr: 0.000372, loss: 0.4716
2022-10-02 10:53:53 - train: epoch 0006, iter [00270, 01251], lr: 0.000373, loss: 0.4575
2022-10-02 10:54:12 - train: epoch 0006, iter [00280, 01251], lr: 0.000373, loss: 0.4625
2022-10-02 10:54:32 - train: epoch 0006, iter [00290, 01251], lr: 0.000374, loss: 0.4627
2022-10-02 10:54:52 - train: epoch 0006, iter [00300, 01251], lr: 0.000374, loss: 0.4498
2022-10-02 10:55:11 - train: epoch 0006, iter [00310, 01251], lr: 0.000375, loss: 0.4539
2022-10-02 10:55:30 - train: epoch 0006, iter [00320, 01251], lr: 0.000375, loss: 0.4569
2022-10-02 10:55:49 - train: epoch 0006, iter [00330, 01251], lr: 0.000376, loss: 0.4582
2022-10-02 10:56:08 - train: epoch 0006, iter [00340, 01251], lr: 0.000376, loss: 0.4639
2022-10-02 10:56:28 - train: epoch 0006, iter [00350, 01251], lr: 0.000377, loss: 0.4688
2022-10-02 10:56:47 - train: epoch 0006, iter [00360, 01251], lr: 0.000377, loss: 0.4614
2022-10-02 10:57:06 - train: epoch 0006, iter [00370, 01251], lr: 0.000378, loss: 0.4860
2022-10-02 10:57:26 - train: epoch 0006, iter [00380, 01251], lr: 0.000378, loss: 0.4648
2022-10-02 10:57:45 - train: epoch 0006, iter [00390, 01251], lr: 0.000379, loss: 0.4542
2022-10-02 10:58:04 - train: epoch 0006, iter [00400, 01251], lr: 0.000379, loss: 0.4626
2022-10-02 10:58:24 - train: epoch 0006, iter [00410, 01251], lr: 0.000380, loss: 0.4636
2022-10-02 10:58:43 - train: epoch 0006, iter [00420, 01251], lr: 0.000380, loss: 0.4667
2022-10-02 10:59:02 - train: epoch 0006, iter [00430, 01251], lr: 0.000381, loss: 0.4637
2022-10-02 10:59:22 - train: epoch 0006, iter [00440, 01251], lr: 0.000381, loss: 0.4820
2022-10-02 10:59:41 - train: epoch 0006, iter [00450, 01251], lr: 0.000382, loss: 0.4744
2022-10-02 11:00:01 - train: epoch 0006, iter [00460, 01251], lr: 0.000382, loss: 0.4758
2022-10-02 11:00:20 - train: epoch 0006, iter [00470, 01251], lr: 0.000383, loss: 0.4571
2022-10-02 11:00:40 - train: epoch 0006, iter [00480, 01251], lr: 0.000383, loss: 0.4506
2022-10-02 11:00:59 - train: epoch 0006, iter [00490, 01251], lr: 0.000384, loss: 0.4684
2022-10-02 11:01:18 - train: epoch 0006, iter [00500, 01251], lr: 0.000384, loss: 0.4850
2022-10-02 11:01:38 - train: epoch 0006, iter [00510, 01251], lr: 0.000384, loss: 0.4633
2022-10-02 11:01:57 - train: epoch 0006, iter [00520, 01251], lr: 0.000385, loss: 0.4520
2022-10-02 11:02:16 - train: epoch 0006, iter [00530, 01251], lr: 0.000385, loss: 0.4791
2022-10-02 11:02:35 - train: epoch 0006, iter [00540, 01251], lr: 0.000386, loss: 0.4516
2022-10-02 11:02:54 - train: epoch 0006, iter [00550, 01251], lr: 0.000386, loss: 0.4414
2022-10-02 11:03:14 - train: epoch 0006, iter [00560, 01251], lr: 0.000387, loss: 0.4572
2022-10-02 11:03:33 - train: epoch 0006, iter [00570, 01251], lr: 0.000387, loss: 0.4287
2022-10-02 11:03:52 - train: epoch 0006, iter [00580, 01251], lr: 0.000388, loss: 0.4473
2022-10-02 11:04:12 - train: epoch 0006, iter [00590, 01251], lr: 0.000388, loss: 0.4612
2022-10-02 11:04:31 - train: epoch 0006, iter [00600, 01251], lr: 0.000389, loss: 0.4748
2022-10-02 11:04:50 - train: epoch 0006, iter [00610, 01251], lr: 0.000389, loss: 0.4757
2022-10-02 11:05:10 - train: epoch 0006, iter [00620, 01251], lr: 0.000390, loss: 0.4613
2022-10-02 11:05:29 - train: epoch 0006, iter [00630, 01251], lr: 0.000390, loss: 0.4248
2022-10-02 11:05:48 - train: epoch 0006, iter [00640, 01251], lr: 0.000391, loss: 0.4548
2022-10-02 11:06:08 - train: epoch 0006, iter [00650, 01251], lr: 0.000391, loss: 0.4756
2022-10-02 11:06:27 - train: epoch 0006, iter [00660, 01251], lr: 0.000392, loss: 0.4622
2022-10-02 11:06:46 - train: epoch 0006, iter [00670, 01251], lr: 0.000392, loss: 0.4592
2022-10-02 11:07:06 - train: epoch 0006, iter [00680, 01251], lr: 0.000393, loss: 0.4739
2022-10-02 11:07:25 - train: epoch 0006, iter [00690, 01251], lr: 0.000393, loss: 0.4512
2022-10-02 11:07:44 - train: epoch 0006, iter [00700, 01251], lr: 0.000394, loss: 0.4538
2022-10-02 11:08:04 - train: epoch 0006, iter [00710, 01251], lr: 0.000394, loss: 0.4808
2022-10-02 11:08:23 - train: epoch 0006, iter [00720, 01251], lr: 0.000395, loss: 0.4768
2022-10-02 11:08:42 - train: epoch 0006, iter [00730, 01251], lr: 0.000395, loss: 0.4659
2022-10-02 11:09:02 - train: epoch 0006, iter [00740, 01251], lr: 0.000395, loss: 0.4492
2022-10-02 11:09:21 - train: epoch 0006, iter [00750, 01251], lr: 0.000396, loss: 0.4465
2022-10-02 11:09:40 - train: epoch 0006, iter [00760, 01251], lr: 0.000396, loss: 0.4425
2022-10-02 11:10:00 - train: epoch 0006, iter [00770, 01251], lr: 0.000397, loss: 0.4502
2022-10-02 11:10:19 - train: epoch 0006, iter [00780, 01251], lr: 0.000397, loss: 0.4673
2022-10-02 11:10:38 - train: epoch 0006, iter [00790, 01251], lr: 0.000398, loss: 0.4786
2022-10-02 11:10:57 - train: epoch 0006, iter [00800, 01251], lr: 0.000398, loss: 0.4464
2022-10-02 11:11:16 - train: epoch 0006, iter [00810, 01251], lr: 0.000399, loss: 0.4671
2022-10-02 11:11:35 - train: epoch 0006, iter [00820, 01251], lr: 0.000399, loss: 0.4584
2022-10-02 11:11:55 - train: epoch 0006, iter [00830, 01251], lr: 0.000400, loss: 0.4515
2022-10-02 11:12:14 - train: epoch 0006, iter [00840, 01251], lr: 0.000400, loss: 0.4314
2022-10-02 11:12:33 - train: epoch 0006, iter [00850, 01251], lr: 0.000401, loss: 0.4663
2022-10-02 11:12:53 - train: epoch 0006, iter [00860, 01251], lr: 0.000401, loss: 0.4596
2022-10-02 11:13:12 - train: epoch 0006, iter [00870, 01251], lr: 0.000402, loss: 0.4523
2022-10-02 11:13:31 - train: epoch 0006, iter [00880, 01251], lr: 0.000402, loss: 0.4709
2022-10-02 11:13:50 - train: epoch 0006, iter [00890, 01251], lr: 0.000403, loss: 0.4635
2022-10-02 11:14:10 - train: epoch 0006, iter [00900, 01251], lr: 0.000403, loss: 0.4537
2022-10-02 11:14:29 - train: epoch 0006, iter [00910, 01251], lr: 0.000404, loss: 0.4510
2022-10-02 11:14:48 - train: epoch 0006, iter [00920, 01251], lr: 0.000404, loss: 0.4604
2022-10-02 11:15:08 - train: epoch 0006, iter [00930, 01251], lr: 0.000405, loss: 0.4627
2022-10-02 11:15:27 - train: epoch 0006, iter [00940, 01251], lr: 0.000405, loss: 0.4457
2022-10-02 11:15:46 - train: epoch 0006, iter [00950, 01251], lr: 0.000406, loss: 0.4505
2022-10-02 11:16:06 - train: epoch 0006, iter [00960, 01251], lr: 0.000406, loss: 0.4705
2022-10-02 11:16:25 - train: epoch 0006, iter [00970, 01251], lr: 0.000407, loss: 0.4527
2022-10-02 11:16:45 - train: epoch 0006, iter [00980, 01251], lr: 0.000407, loss: 0.4435
2022-10-02 11:17:04 - train: epoch 0006, iter [00990, 01251], lr: 0.000407, loss: 0.4762
2022-10-02 11:17:23 - train: epoch 0006, iter [01000, 01251], lr: 0.000408, loss: 0.4395
2022-10-02 11:17:43 - train: epoch 0006, iter [01010, 01251], lr: 0.000408, loss: 0.4739
2022-10-02 11:18:02 - train: epoch 0006, iter [01020, 01251], lr: 0.000409, loss: 0.4249
2022-10-02 11:18:22 - train: epoch 0006, iter [01030, 01251], lr: 0.000409, loss: 0.4590
2022-10-02 11:18:42 - train: epoch 0006, iter [01040, 01251], lr: 0.000410, loss: 0.4643
2022-10-02 11:19:01 - train: epoch 0006, iter [01050, 01251], lr: 0.000410, loss: 0.4553
2022-10-02 11:19:20 - train: epoch 0006, iter [01060, 01251], lr: 0.000411, loss: 0.4713
2022-10-02 11:19:40 - train: epoch 0006, iter [01070, 01251], lr: 0.000411, loss: 0.4817
2022-10-02 11:19:59 - train: epoch 0006, iter [01080, 01251], lr: 0.000412, loss: 0.4699
2022-10-02 11:20:18 - train: epoch 0006, iter [01090, 01251], lr: 0.000412, loss: 0.4637
2022-10-02 11:20:38 - train: epoch 0006, iter [01100, 01251], lr: 0.000413, loss: 0.4547
2022-10-02 11:20:57 - train: epoch 0006, iter [01110, 01251], lr: 0.000413, loss: 0.4651
2022-10-02 11:21:16 - train: epoch 0006, iter [01120, 01251], lr: 0.000414, loss: 0.4381
2022-10-02 11:21:36 - train: epoch 0006, iter [01130, 01251], lr: 0.000414, loss: 0.4677
2022-10-02 11:21:55 - train: epoch 0006, iter [01140, 01251], lr: 0.000415, loss: 0.4693
2022-10-02 11:22:15 - train: epoch 0006, iter [01150, 01251], lr: 0.000415, loss: 0.4658
2022-10-02 11:22:34 - train: epoch 0006, iter [01160, 01251], lr: 0.000416, loss: 0.4508
2022-10-02 11:22:54 - train: epoch 0006, iter [01170, 01251], lr: 0.000416, loss: 0.4712
2022-10-02 11:23:13 - train: epoch 0006, iter [01180, 01251], lr: 0.000417, loss: 0.4435
2022-10-02 11:23:32 - train: epoch 0006, iter [01190, 01251], lr: 0.000417, loss: 0.4763
2022-10-02 11:23:52 - train: epoch 0006, iter [01200, 01251], lr: 0.000418, loss: 0.4598
2022-10-02 11:24:11 - train: epoch 0006, iter [01210, 01251], lr: 0.000418, loss: 0.4370
2022-10-02 11:24:30 - train: epoch 0006, iter [01220, 01251], lr: 0.000419, loss: 0.4899
2022-10-02 11:24:49 - train: epoch 0006, iter [01230, 01251], lr: 0.000419, loss: 0.4585
2022-10-02 11:25:09 - train: epoch 0006, iter [01240, 01251], lr: 0.000419, loss: 0.4542
2022-10-02 11:25:27 - train: epoch 0006, iter [01250, 01251], lr: 0.000420, loss: 0.4736
2022-10-02 11:25:30 - train: epoch 006, train_loss: 0.4610
2022-10-02 11:25:35 - until epoch: 006, best_loss: 0.4610
2022-10-02 11:25:35 - epoch 007 lr: 0.000420
2022-10-02 11:26:00 - train: epoch 0007, iter [00010, 01251], lr: 0.000420, loss: 0.4560
2022-10-02 11:26:20 - train: epoch 0007, iter [00020, 01251], lr: 0.000421, loss: 0.4443
2022-10-02 11:26:40 - train: epoch 0007, iter [00030, 01251], lr: 0.000421, loss: 0.4536
2022-10-02 11:26:59 - train: epoch 0007, iter [00040, 01251], lr: 0.000422, loss: 0.4410
2022-10-02 11:27:18 - train: epoch 0007, iter [00050, 01251], lr: 0.000422, loss: 0.4537
2022-10-02 11:27:38 - train: epoch 0007, iter [00060, 01251], lr: 0.000423, loss: 0.4655
2022-10-02 11:27:57 - train: epoch 0007, iter [00070, 01251], lr: 0.000423, loss: 0.4531
2022-10-02 11:28:17 - train: epoch 0007, iter [00080, 01251], lr: 0.000424, loss: 0.4646
2022-10-02 11:28:36 - train: epoch 0007, iter [00090, 01251], lr: 0.000424, loss: 0.4538
2022-10-02 11:28:56 - train: epoch 0007, iter [00100, 01251], lr: 0.000425, loss: 0.4834
2022-10-02 11:29:15 - train: epoch 0007, iter [00110, 01251], lr: 0.000425, loss: 0.4609
2022-10-02 11:29:34 - train: epoch 0007, iter [00120, 01251], lr: 0.000426, loss: 0.4487
2022-10-02 11:29:54 - train: epoch 0007, iter [00130, 01251], lr: 0.000426, loss: 0.4480
2022-10-02 11:30:13 - train: epoch 0007, iter [00140, 01251], lr: 0.000427, loss: 0.4526
2022-10-02 11:30:33 - train: epoch 0007, iter [00150, 01251], lr: 0.000427, loss: 0.4665
2022-10-02 11:30:52 - train: epoch 0007, iter [00160, 01251], lr: 0.000428, loss: 0.4559
2022-10-02 11:31:11 - train: epoch 0007, iter [00170, 01251], lr: 0.000428, loss: 0.4671
2022-10-02 11:31:31 - train: epoch 0007, iter [00180, 01251], lr: 0.000429, loss: 0.4610
2022-10-02 11:31:50 - train: epoch 0007, iter [00190, 01251], lr: 0.000429, loss: 0.4493
2022-10-02 11:32:09 - train: epoch 0007, iter [00200, 01251], lr: 0.000430, loss: 0.4342
2022-10-02 11:32:28 - train: epoch 0007, iter [00210, 01251], lr: 0.000430, loss: 0.4486
2022-10-02 11:32:47 - train: epoch 0007, iter [00220, 01251], lr: 0.000431, loss: 0.4766
2022-10-02 11:33:07 - train: epoch 0007, iter [00230, 01251], lr: 0.000431, loss: 0.4451
2022-10-02 11:33:26 - train: epoch 0007, iter [00240, 01251], lr: 0.000432, loss: 0.4401
2022-10-02 11:33:45 - train: epoch 0007, iter [00250, 01251], lr: 0.000432, loss: 0.4635
2022-10-02 11:34:04 - train: epoch 0007, iter [00260, 01251], lr: 0.000432, loss: 0.4654
2022-10-02 11:34:24 - train: epoch 0007, iter [00270, 01251], lr: 0.000433, loss: 0.4606
2022-10-02 11:34:43 - train: epoch 0007, iter [00280, 01251], lr: 0.000433, loss: 0.4517
2022-10-02 11:35:02 - train: epoch 0007, iter [00290, 01251], lr: 0.000434, loss: 0.4466
2022-10-02 11:35:22 - train: epoch 0007, iter [00300, 01251], lr: 0.000434, loss: 0.4520
2022-10-02 11:35:41 - train: epoch 0007, iter [00310, 01251], lr: 0.000435, loss: 0.4662
2022-10-02 11:36:00 - train: epoch 0007, iter [00320, 01251], lr: 0.000435, loss: 0.4512
2022-10-02 11:36:19 - train: epoch 0007, iter [00330, 01251], lr: 0.000436, loss: 0.4711
2022-10-02 11:36:39 - train: epoch 0007, iter [00340, 01251], lr: 0.000436, loss: 0.4387
2022-10-02 11:36:58 - train: epoch 0007, iter [00350, 01251], lr: 0.000437, loss: 0.4487
2022-10-02 11:37:18 - train: epoch 0007, iter [00360, 01251], lr: 0.000437, loss: 0.4576
2022-10-02 11:37:37 - train: epoch 0007, iter [00370, 01251], lr: 0.000438, loss: 0.4476
2022-10-02 11:37:56 - train: epoch 0007, iter [00380, 01251], lr: 0.000438, loss: 0.4623
2022-10-02 11:38:15 - train: epoch 0007, iter [00390, 01251], lr: 0.000439, loss: 0.4447
2022-10-02 11:38:34 - train: epoch 0007, iter [00400, 01251], lr: 0.000439, loss: 0.4713
2022-10-02 11:38:54 - train: epoch 0007, iter [00410, 01251], lr: 0.000440, loss: 0.4459
2022-10-02 11:39:13 - train: epoch 0007, iter [00420, 01251], lr: 0.000440, loss: 0.4556
2022-10-02 11:39:32 - train: epoch 0007, iter [00430, 01251], lr: 0.000441, loss: 0.4505
2022-10-02 11:39:52 - train: epoch 0007, iter [00440, 01251], lr: 0.000441, loss: 0.4381
2022-10-02 11:40:11 - train: epoch 0007, iter [00450, 01251], lr: 0.000442, loss: 0.4420
2022-10-02 11:40:30 - train: epoch 0007, iter [00460, 01251], lr: 0.000442, loss: 0.4390
2022-10-02 11:40:49 - train: epoch 0007, iter [00470, 01251], lr: 0.000443, loss: 0.4730
2022-10-02 11:41:09 - train: epoch 0007, iter [00480, 01251], lr: 0.000443, loss: 0.4546
2022-10-02 11:41:28 - train: epoch 0007, iter [00490, 01251], lr: 0.000444, loss: 0.4561
2022-10-02 11:41:47 - train: epoch 0007, iter [00500, 01251], lr: 0.000444, loss: 0.4561
2022-10-02 11:42:07 - train: epoch 0007, iter [00510, 01251], lr: 0.000444, loss: 0.4559
2022-10-02 11:42:26 - train: epoch 0007, iter [00520, 01251], lr: 0.000445, loss: 0.4598
2022-10-02 11:42:45 - train: epoch 0007, iter [00530, 01251], lr: 0.000445, loss: 0.4663
2022-10-02 11:43:05 - train: epoch 0007, iter [00540, 01251], lr: 0.000446, loss: 0.4499
2022-10-02 11:43:24 - train: epoch 0007, iter [00550, 01251], lr: 0.000446, loss: 0.4635
2022-10-02 11:43:44 - train: epoch 0007, iter [00560, 01251], lr: 0.000447, loss: 0.4521
2022-10-02 11:44:03 - train: epoch 0007, iter [00570, 01251], lr: 0.000447, loss: 0.4760
2022-10-02 11:44:22 - train: epoch 0007, iter [00580, 01251], lr: 0.000448, loss: 0.4705
2022-10-02 11:44:41 - train: epoch 0007, iter [00590, 01251], lr: 0.000448, loss: 0.4732
2022-10-02 11:45:00 - train: epoch 0007, iter [00600, 01251], lr: 0.000449, loss: 0.4721
2022-10-02 11:45:20 - train: epoch 0007, iter [00610, 01251], lr: 0.000449, loss: 0.4268
2022-10-02 11:45:39 - train: epoch 0007, iter [00620, 01251], lr: 0.000450, loss: 0.4526
2022-10-02 11:45:58 - train: epoch 0007, iter [00630, 01251], lr: 0.000450, loss: 0.4458
2022-10-02 11:46:18 - train: epoch 0007, iter [00640, 01251], lr: 0.000451, loss: 0.4484
2022-10-02 11:46:37 - train: epoch 0007, iter [00650, 01251], lr: 0.000451, loss: 0.4692
2022-10-02 11:46:56 - train: epoch 0007, iter [00660, 01251], lr: 0.000452, loss: 0.4515
2022-10-02 11:47:15 - train: epoch 0007, iter [00670, 01251], lr: 0.000452, loss: 0.4505
2022-10-02 11:47:34 - train: epoch 0007, iter [00680, 01251], lr: 0.000453, loss: 0.4591
2022-10-02 11:47:54 - train: epoch 0007, iter [00690, 01251], lr: 0.000453, loss: 0.4429
2022-10-02 11:48:13 - train: epoch 0007, iter [00700, 01251], lr: 0.000454, loss: 0.4844
2022-10-02 11:48:32 - train: epoch 0007, iter [00710, 01251], lr: 0.000454, loss: 0.4554
2022-10-02 11:48:52 - train: epoch 0007, iter [00720, 01251], lr: 0.000455, loss: 0.4420
2022-10-02 11:49:11 - train: epoch 0007, iter [00730, 01251], lr: 0.000455, loss: 0.4686
2022-10-02 11:49:30 - train: epoch 0007, iter [00740, 01251], lr: 0.000455, loss: 0.4571
2022-10-02 11:49:49 - train: epoch 0007, iter [00750, 01251], lr: 0.000456, loss: 0.4496
2022-10-02 11:50:08 - train: epoch 0007, iter [00760, 01251], lr: 0.000456, loss: 0.4450
2022-10-02 11:50:28 - train: epoch 0007, iter [00770, 01251], lr: 0.000457, loss: 0.4417
2022-10-02 11:50:47 - train: epoch 0007, iter [00780, 01251], lr: 0.000457, loss: 0.4395
2022-10-02 11:51:06 - train: epoch 0007, iter [00790, 01251], lr: 0.000458, loss: 0.4500
2022-10-02 11:51:25 - train: epoch 0007, iter [00800, 01251], lr: 0.000458, loss: 0.4481
2022-10-02 11:51:45 - train: epoch 0007, iter [00810, 01251], lr: 0.000459, loss: 0.4402
2022-10-02 11:52:04 - train: epoch 0007, iter [00820, 01251], lr: 0.000459, loss: 0.4526
2022-10-02 11:52:23 - train: epoch 0007, iter [00830, 01251], lr: 0.000460, loss: 0.4541
2022-10-02 11:52:43 - train: epoch 0007, iter [00840, 01251], lr: 0.000460, loss: 0.4629
2022-10-02 11:53:02 - train: epoch 0007, iter [00850, 01251], lr: 0.000461, loss: 0.4667
2022-10-02 11:53:21 - train: epoch 0007, iter [00860, 01251], lr: 0.000461, loss: 0.4612
2022-10-02 11:53:40 - train: epoch 0007, iter [00870, 01251], lr: 0.000462, loss: 0.4357
2022-10-02 11:54:00 - train: epoch 0007, iter [00880, 01251], lr: 0.000462, loss: 0.4477
2022-10-02 11:54:19 - train: epoch 0007, iter [00890, 01251], lr: 0.000463, loss: 0.4734
2022-10-02 11:54:38 - train: epoch 0007, iter [00900, 01251], lr: 0.000463, loss: 0.4574
2022-10-02 11:54:57 - train: epoch 0007, iter [00910, 01251], lr: 0.000464, loss: 0.4461
2022-10-02 11:55:17 - train: epoch 0007, iter [00920, 01251], lr: 0.000464, loss: 0.4621
2022-10-02 11:55:36 - train: epoch 0007, iter [00930, 01251], lr: 0.000465, loss: 0.4491
2022-10-02 11:55:55 - train: epoch 0007, iter [00940, 01251], lr: 0.000465, loss: 0.4459
2022-10-02 11:56:14 - train: epoch 0007, iter [00950, 01251], lr: 0.000466, loss: 0.4553
2022-10-02 11:56:33 - train: epoch 0007, iter [00960, 01251], lr: 0.000466, loss: 0.4543
2022-10-02 11:56:53 - train: epoch 0007, iter [00970, 01251], lr: 0.000467, loss: 0.4593
2022-10-02 11:57:12 - train: epoch 0007, iter [00980, 01251], lr: 0.000467, loss: 0.4433
2022-10-02 11:57:32 - train: epoch 0007, iter [00990, 01251], lr: 0.000467, loss: 0.4638
2022-10-02 11:57:51 - train: epoch 0007, iter [01000, 01251], lr: 0.000468, loss: 0.4474
2022-10-02 11:58:10 - train: epoch 0007, iter [01010, 01251], lr: 0.000468, loss: 0.4470
2022-10-02 11:58:29 - train: epoch 0007, iter [01020, 01251], lr: 0.000469, loss: 0.4763
2022-10-02 11:58:49 - train: epoch 0007, iter [01030, 01251], lr: 0.000469, loss: 0.4396
2022-10-02 11:59:08 - train: epoch 0007, iter [01040, 01251], lr: 0.000470, loss: 0.4685
2022-10-02 11:59:27 - train: epoch 0007, iter [01050, 01251], lr: 0.000470, loss: 0.4654
2022-10-02 11:59:47 - train: epoch 0007, iter [01060, 01251], lr: 0.000471, loss: 0.4570
2022-10-02 12:00:06 - train: epoch 0007, iter [01070, 01251], lr: 0.000471, loss: 0.4568
2022-10-02 12:00:25 - train: epoch 0007, iter [01080, 01251], lr: 0.000472, loss: 0.4466
2022-10-02 12:00:44 - train: epoch 0007, iter [01090, 01251], lr: 0.000472, loss: 0.4369
2022-10-02 12:01:04 - train: epoch 0007, iter [01100, 01251], lr: 0.000473, loss: 0.4747
2022-10-02 12:01:23 - train: epoch 0007, iter [01110, 01251], lr: 0.000473, loss: 0.4644
2022-10-02 12:01:42 - train: epoch 0007, iter [01120, 01251], lr: 0.000474, loss: 0.4624
2022-10-02 12:02:02 - train: epoch 0007, iter [01130, 01251], lr: 0.000474, loss: 0.4574
2022-10-02 12:02:21 - train: epoch 0007, iter [01140, 01251], lr: 0.000475, loss: 0.4783
2022-10-02 12:02:40 - train: epoch 0007, iter [01150, 01251], lr: 0.000475, loss: 0.4441
2022-10-02 12:03:00 - train: epoch 0007, iter [01160, 01251], lr: 0.000476, loss: 0.4537
2022-10-02 12:03:19 - train: epoch 0007, iter [01170, 01251], lr: 0.000476, loss: 0.4504
2022-10-02 12:03:39 - train: epoch 0007, iter [01180, 01251], lr: 0.000477, loss: 0.4687
2022-10-02 12:03:58 - train: epoch 0007, iter [01190, 01251], lr: 0.000477, loss: 0.4399
2022-10-02 12:04:17 - train: epoch 0007, iter [01200, 01251], lr: 0.000478, loss: 0.4463
2022-10-02 12:04:37 - train: epoch 0007, iter [01210, 01251], lr: 0.000478, loss: 0.4459
2022-10-02 12:04:56 - train: epoch 0007, iter [01220, 01251], lr: 0.000479, loss: 0.4396
2022-10-02 12:05:16 - train: epoch 0007, iter [01230, 01251], lr: 0.000479, loss: 0.4567
2022-10-02 12:05:35 - train: epoch 0007, iter [01240, 01251], lr: 0.000479, loss: 0.4651
2022-10-02 12:05:54 - train: epoch 0007, iter [01250, 01251], lr: 0.000480, loss: 0.4626
2022-10-02 12:05:57 - train: epoch 007, train_loss: 0.4553
2022-10-02 12:06:02 - until epoch: 007, best_loss: 0.4553
2022-10-02 12:06:02 - epoch 008 lr: 0.000480
2022-10-02 12:06:28 - train: epoch 0008, iter [00010, 01251], lr: 0.000480, loss: 0.4496
2022-10-02 12:06:48 - train: epoch 0008, iter [00020, 01251], lr: 0.000481, loss: 0.4514
2022-10-02 12:07:07 - train: epoch 0008, iter [00030, 01251], lr: 0.000481, loss: 0.4606
2022-10-02 12:07:26 - train: epoch 0008, iter [00040, 01251], lr: 0.000482, loss: 0.4404
2022-10-02 12:07:46 - train: epoch 0008, iter [00050, 01251], lr: 0.000482, loss: 0.4464
2022-10-02 12:08:05 - train: epoch 0008, iter [00060, 01251], lr: 0.000483, loss: 0.4602
2022-10-02 12:08:24 - train: epoch 0008, iter [00070, 01251], lr: 0.000483, loss: 0.4458
2022-10-02 12:08:43 - train: epoch 0008, iter [00080, 01251], lr: 0.000484, loss: 0.4396
2022-10-02 12:09:03 - train: epoch 0008, iter [00090, 01251], lr: 0.000484, loss: 0.4561
2022-10-02 12:09:22 - train: epoch 0008, iter [00100, 01251], lr: 0.000485, loss: 0.4611
2022-10-02 12:09:41 - train: epoch 0008, iter [00110, 01251], lr: 0.000485, loss: 0.4411
2022-10-02 12:10:00 - train: epoch 0008, iter [00120, 01251], lr: 0.000486, loss: 0.4646
2022-10-02 12:10:20 - train: epoch 0008, iter [00130, 01251], lr: 0.000486, loss: 0.4567
2022-10-02 12:10:39 - train: epoch 0008, iter [00140, 01251], lr: 0.000487, loss: 0.4536
2022-10-02 12:10:58 - train: epoch 0008, iter [00150, 01251], lr: 0.000487, loss: 0.4494
2022-10-02 12:11:18 - train: epoch 0008, iter [00160, 01251], lr: 0.000488, loss: 0.4772
2022-10-02 12:11:37 - train: epoch 0008, iter [00170, 01251], lr: 0.000488, loss: 0.4324
2022-10-02 12:11:57 - train: epoch 0008, iter [00180, 01251], lr: 0.000489, loss: 0.4405
2022-10-02 12:12:16 - train: epoch 0008, iter [00190, 01251], lr: 0.000489, loss: 0.4511
2022-10-02 12:12:36 - train: epoch 0008, iter [00200, 01251], lr: 0.000490, loss: 0.4649
2022-10-02 12:12:55 - train: epoch 0008, iter [00210, 01251], lr: 0.000490, loss: 0.4202
2022-10-02 12:13:15 - train: epoch 0008, iter [00220, 01251], lr: 0.000491, loss: 0.4521
2022-10-02 12:13:34 - train: epoch 0008, iter [00230, 01251], lr: 0.000491, loss: 0.4488
2022-10-02 12:13:53 - train: epoch 0008, iter [00240, 01251], lr: 0.000492, loss: 0.4404
2022-10-02 12:14:13 - train: epoch 0008, iter [00250, 01251], lr: 0.000492, loss: 0.4435
2022-10-02 12:14:32 - train: epoch 0008, iter [00260, 01251], lr: 0.000492, loss: 0.4621
2022-10-02 12:14:51 - train: epoch 0008, iter [00270, 01251], lr: 0.000493, loss: 0.4552
2022-10-02 12:15:11 - train: epoch 0008, iter [00280, 01251], lr: 0.000493, loss: 0.4426
2022-10-02 12:15:30 - train: epoch 0008, iter [00290, 01251], lr: 0.000494, loss: 0.4475
2022-10-02 12:15:49 - train: epoch 0008, iter [00300, 01251], lr: 0.000494, loss: 0.4407
2022-10-02 12:16:09 - train: epoch 0008, iter [00310, 01251], lr: 0.000495, loss: 0.4538
2022-10-02 12:16:28 - train: epoch 0008, iter [00320, 01251], lr: 0.000495, loss: 0.4484
2022-10-02 12:16:47 - train: epoch 0008, iter [00330, 01251], lr: 0.000496, loss: 0.4453
2022-10-02 12:17:07 - train: epoch 0008, iter [00340, 01251], lr: 0.000496, loss: 0.4557
2022-10-02 12:17:26 - train: epoch 0008, iter [00350, 01251], lr: 0.000497, loss: 0.4370
2022-10-02 12:17:45 - train: epoch 0008, iter [00360, 01251], lr: 0.000497, loss: 0.4343
2022-10-02 12:18:05 - train: epoch 0008, iter [00370, 01251], lr: 0.000498, loss: 0.4423
2022-10-02 12:18:24 - train: epoch 0008, iter [00380, 01251], lr: 0.000498, loss: 0.4644
2022-10-02 12:18:45 - train: epoch 0008, iter [00390, 01251], lr: 0.000499, loss: 0.4512
2022-10-02 12:19:04 - train: epoch 0008, iter [00400, 01251], lr: 0.000499, loss: 0.4311
2022-10-02 12:19:23 - train: epoch 0008, iter [00410, 01251], lr: 0.000500, loss: 0.4540
2022-10-02 12:19:43 - train: epoch 0008, iter [00420, 01251], lr: 0.000500, loss: 0.4516
2022-10-02 12:20:02 - train: epoch 0008, iter [00430, 01251], lr: 0.000501, loss: 0.4654
2022-10-02 12:20:21 - train: epoch 0008, iter [00440, 01251], lr: 0.000501, loss: 0.4355
2022-10-02 12:20:40 - train: epoch 0008, iter [00450, 01251], lr: 0.000502, loss: 0.4615
2022-10-02 12:21:00 - train: epoch 0008, iter [00460, 01251], lr: 0.000502, loss: 0.4409
2022-10-02 12:21:19 - train: epoch 0008, iter [00470, 01251], lr: 0.000503, loss: 0.4411
2022-10-02 12:21:39 - train: epoch 0008, iter [00480, 01251], lr: 0.000503, loss: 0.4639
2022-10-02 12:21:58 - train: epoch 0008, iter [00490, 01251], lr: 0.000504, loss: 0.4601
2022-10-02 12:22:17 - train: epoch 0008, iter [00500, 01251], lr: 0.000504, loss: 0.4544
2022-10-02 12:22:37 - train: epoch 0008, iter [00510, 01251], lr: 0.000504, loss: 0.4651
2022-10-02 12:22:56 - train: epoch 0008, iter [00520, 01251], lr: 0.000505, loss: 0.4404
2022-10-02 12:23:16 - train: epoch 0008, iter [00530, 01251], lr: 0.000505, loss: 0.4485
2022-10-02 12:23:36 - train: epoch 0008, iter [00540, 01251], lr: 0.000506, loss: 0.4701
2022-10-02 12:23:55 - train: epoch 0008, iter [00550, 01251], lr: 0.000506, loss: 0.4549
2022-10-02 12:24:15 - train: epoch 0008, iter [00560, 01251], lr: 0.000507, loss: 0.4514
2022-10-02 12:24:34 - train: epoch 0008, iter [00570, 01251], lr: 0.000507, loss: 0.4414
2022-10-02 12:24:53 - train: epoch 0008, iter [00580, 01251], lr: 0.000508, loss: 0.4569
2022-10-02 12:25:13 - train: epoch 0008, iter [00590, 01251], lr: 0.000508, loss: 0.4614
2022-10-02 12:25:32 - train: epoch 0008, iter [00600, 01251], lr: 0.000509, loss: 0.4510
2022-10-02 12:25:51 - train: epoch 0008, iter [00610, 01251], lr: 0.000509, loss: 0.4461
2022-10-02 12:26:11 - train: epoch 0008, iter [00620, 01251], lr: 0.000510, loss: 0.4444
2022-10-02 12:26:30 - train: epoch 0008, iter [00630, 01251], lr: 0.000510, loss: 0.4336
2022-10-02 12:26:49 - train: epoch 0008, iter [00640, 01251], lr: 0.000511, loss: 0.4482
2022-10-02 12:27:09 - train: epoch 0008, iter [00650, 01251], lr: 0.000511, loss: 0.4438
2022-10-02 12:27:28 - train: epoch 0008, iter [00660, 01251], lr: 0.000512, loss: 0.4422
2022-10-02 12:27:47 - train: epoch 0008, iter [00670, 01251], lr: 0.000512, loss: 0.4713
2022-10-02 12:28:07 - train: epoch 0008, iter [00680, 01251], lr: 0.000513, loss: 0.4572
2022-10-02 12:28:26 - train: epoch 0008, iter [00690, 01251], lr: 0.000513, loss: 0.4732
2022-10-02 12:28:46 - train: epoch 0008, iter [00700, 01251], lr: 0.000514, loss: 0.4415
2022-10-02 12:29:05 - train: epoch 0008, iter [00710, 01251], lr: 0.000514, loss: 0.4406
2022-10-02 12:29:25 - train: epoch 0008, iter [00720, 01251], lr: 0.000515, loss: 0.4598
2022-10-02 12:29:44 - train: epoch 0008, iter [00730, 01251], lr: 0.000515, loss: 0.4442
2022-10-02 12:30:03 - train: epoch 0008, iter [00740, 01251], lr: 0.000515, loss: 0.4604
2022-10-02 12:30:23 - train: epoch 0008, iter [00750, 01251], lr: 0.000516, loss: 0.4463
2022-10-02 12:30:42 - train: epoch 0008, iter [00760, 01251], lr: 0.000516, loss: 0.4442
2022-10-02 12:31:02 - train: epoch 0008, iter [00770, 01251], lr: 0.000517, loss: 0.4623
2022-10-02 12:31:22 - train: epoch 0008, iter [00780, 01251], lr: 0.000517, loss: 0.4546
2022-10-02 12:31:41 - train: epoch 0008, iter [00790, 01251], lr: 0.000518, loss: 0.4759
2022-10-02 12:32:01 - train: epoch 0008, iter [00800, 01251], lr: 0.000518, loss: 0.4555
2022-10-02 12:32:20 - train: epoch 0008, iter [00810, 01251], lr: 0.000519, loss: 0.4589
2022-10-02 12:32:40 - train: epoch 0008, iter [00820, 01251], lr: 0.000519, loss: 0.4657
2022-10-02 12:32:59 - train: epoch 0008, iter [00830, 01251], lr: 0.000520, loss: 0.4435
2022-10-02 12:33:18 - train: epoch 0008, iter [00840, 01251], lr: 0.000520, loss: 0.4520
2022-10-02 12:33:37 - train: epoch 0008, iter [00850, 01251], lr: 0.000521, loss: 0.4399
2022-10-02 12:33:57 - train: epoch 0008, iter [00860, 01251], lr: 0.000521, loss: 0.4418
2022-10-02 12:34:16 - train: epoch 0008, iter [00870, 01251], lr: 0.000522, loss: 0.4529
2022-10-02 12:34:36 - train: epoch 0008, iter [00880, 01251], lr: 0.000522, loss: 0.4410
2022-10-02 12:34:55 - train: epoch 0008, iter [00890, 01251], lr: 0.000523, loss: 0.4531
2022-10-02 12:35:15 - train: epoch 0008, iter [00900, 01251], lr: 0.000523, loss: 0.4390
2022-10-02 12:35:34 - train: epoch 0008, iter [00910, 01251], lr: 0.000524, loss: 0.4523
2022-10-02 12:35:54 - train: epoch 0008, iter [00920, 01251], lr: 0.000524, loss: 0.4555
2022-10-02 12:36:13 - train: epoch 0008, iter [00930, 01251], lr: 0.000525, loss: 0.4517
2022-10-02 12:36:33 - train: epoch 0008, iter [00940, 01251], lr: 0.000525, loss: 0.4526
2022-10-02 12:36:52 - train: epoch 0008, iter [00950, 01251], lr: 0.000526, loss: 0.4480
2022-10-02 12:37:11 - train: epoch 0008, iter [00960, 01251], lr: 0.000526, loss: 0.4402
2022-10-02 12:37:31 - train: epoch 0008, iter [00970, 01251], lr: 0.000527, loss: 0.4637
2022-10-02 12:37:50 - train: epoch 0008, iter [00980, 01251], lr: 0.000527, loss: 0.4432
2022-10-02 12:38:10 - train: epoch 0008, iter [00990, 01251], lr: 0.000527, loss: 0.4392
2022-10-02 12:38:29 - train: epoch 0008, iter [01000, 01251], lr: 0.000528, loss: 0.4257
2022-10-02 12:38:49 - train: epoch 0008, iter [01010, 01251], lr: 0.000528, loss: 0.4473
2022-10-02 12:39:08 - train: epoch 0008, iter [01020, 01251], lr: 0.000529, loss: 0.4439
2022-10-02 12:39:27 - train: epoch 0008, iter [01030, 01251], lr: 0.000529, loss: 0.4615
2022-10-02 12:39:47 - train: epoch 0008, iter [01040, 01251], lr: 0.000530, loss: 0.4801
2022-10-02 12:40:06 - train: epoch 0008, iter [01050, 01251], lr: 0.000530, loss: 0.4462
2022-10-02 12:40:26 - train: epoch 0008, iter [01060, 01251], lr: 0.000531, loss: 0.4398
2022-10-02 12:40:45 - train: epoch 0008, iter [01070, 01251], lr: 0.000531, loss: 0.4481
2022-10-02 12:41:04 - train: epoch 0008, iter [01080, 01251], lr: 0.000532, loss: 0.4632
2022-10-02 12:41:24 - train: epoch 0008, iter [01090, 01251], lr: 0.000532, loss: 0.4493
2022-10-02 12:41:43 - train: epoch 0008, iter [01100, 01251], lr: 0.000533, loss: 0.4661
2022-10-02 12:42:03 - train: epoch 0008, iter [01110, 01251], lr: 0.000533, loss: 0.4492
2022-10-02 12:42:22 - train: epoch 0008, iter [01120, 01251], lr: 0.000534, loss: 0.4578
2022-10-02 12:42:42 - train: epoch 0008, iter [01130, 01251], lr: 0.000534, loss: 0.4495
2022-10-02 12:43:02 - train: epoch 0008, iter [01140, 01251], lr: 0.000535, loss: 0.4670
2022-10-02 12:43:21 - train: epoch 0008, iter [01150, 01251], lr: 0.000535, loss: 0.4588
2022-10-02 12:43:40 - train: epoch 0008, iter [01160, 01251], lr: 0.000536, loss: 0.4572
2022-10-02 12:44:00 - train: epoch 0008, iter [01170, 01251], lr: 0.000536, loss: 0.4626
2022-10-02 12:44:20 - train: epoch 0008, iter [01180, 01251], lr: 0.000537, loss: 0.4589
2022-10-02 12:44:40 - train: epoch 0008, iter [01190, 01251], lr: 0.000537, loss: 0.4445
2022-10-02 12:44:59 - train: epoch 0008, iter [01200, 01251], lr: 0.000538, loss: 0.4632
2022-10-02 12:45:19 - train: epoch 0008, iter [01210, 01251], lr: 0.000538, loss: 0.4426
2022-10-02 12:45:39 - train: epoch 0008, iter [01220, 01251], lr: 0.000539, loss: 0.4496
2022-10-02 12:45:59 - train: epoch 0008, iter [01230, 01251], lr: 0.000539, loss: 0.4363
2022-10-02 12:46:19 - train: epoch 0008, iter [01240, 01251], lr: 0.000539, loss: 0.4570
2022-10-02 12:46:38 - train: epoch 0008, iter [01250, 01251], lr: 0.000540, loss: 0.4426
2022-10-02 12:46:41 - train: epoch 008, train_loss: 0.4505
2022-10-02 12:46:46 - until epoch: 008, best_loss: 0.4505
2022-10-02 12:46:46 - epoch 009 lr: 0.000540
2022-10-02 12:47:13 - train: epoch 0009, iter [00010, 01251], lr: 0.000540, loss: 0.4377
2022-10-02 12:47:33 - train: epoch 0009, iter [00020, 01251], lr: 0.000541, loss: 0.4455
2022-10-02 12:47:53 - train: epoch 0009, iter [00030, 01251], lr: 0.000541, loss: 0.4297
2022-10-02 12:48:14 - train: epoch 0009, iter [00040, 01251], lr: 0.000542, loss: 0.4314
2022-10-02 12:48:34 - train: epoch 0009, iter [00050, 01251], lr: 0.000542, loss: 0.4445
2022-10-02 12:48:54 - train: epoch 0009, iter [00060, 01251], lr: 0.000543, loss: 0.4568
2022-10-02 12:49:14 - train: epoch 0009, iter [00070, 01251], lr: 0.000543, loss: 0.4388
2022-10-02 12:49:34 - train: epoch 0009, iter [00080, 01251], lr: 0.000544, loss: 0.4267
2022-10-02 12:49:55 - train: epoch 0009, iter [00090, 01251], lr: 0.000544, loss: 0.4453
2022-10-02 12:50:15 - train: epoch 0009, iter [00100, 01251], lr: 0.000545, loss: 0.4384
2022-10-02 12:50:35 - train: epoch 0009, iter [00110, 01251], lr: 0.000545, loss: 0.4533
2022-10-02 12:50:56 - train: epoch 0009, iter [00120, 01251], lr: 0.000546, loss: 0.4241
2022-10-02 12:51:16 - train: epoch 0009, iter [00130, 01251], lr: 0.000546, loss: 0.4559
2022-10-02 12:51:36 - train: epoch 0009, iter [00140, 01251], lr: 0.000547, loss: 0.4534
2022-10-02 12:51:57 - train: epoch 0009, iter [00150, 01251], lr: 0.000547, loss: 0.4448
2022-10-02 12:52:17 - train: epoch 0009, iter [00160, 01251], lr: 0.000548, loss: 0.4401
2022-10-02 12:52:37 - train: epoch 0009, iter [00170, 01251], lr: 0.000548, loss: 0.4291
2022-10-02 12:52:57 - train: epoch 0009, iter [00180, 01251], lr: 0.000549, loss: 0.4380
2022-10-02 12:53:17 - train: epoch 0009, iter [00190, 01251], lr: 0.000549, loss: 0.4196
2022-10-02 12:53:37 - train: epoch 0009, iter [00200, 01251], lr: 0.000550, loss: 0.4370
2022-10-02 12:53:56 - train: epoch 0009, iter [00210, 01251], lr: 0.000550, loss: 0.4670
2022-10-02 12:54:16 - train: epoch 0009, iter [00220, 01251], lr: 0.000551, loss: 0.4493
2022-10-02 12:54:36 - train: epoch 0009, iter [00230, 01251], lr: 0.000551, loss: 0.4339
2022-10-02 12:54:56 - train: epoch 0009, iter [00240, 01251], lr: 0.000552, loss: 0.4466
2022-10-02 12:55:16 - train: epoch 0009, iter [00250, 01251], lr: 0.000552, loss: 0.4467
2022-10-02 12:55:35 - train: epoch 0009, iter [00260, 01251], lr: 0.000552, loss: 0.4364
2022-10-02 12:55:55 - train: epoch 0009, iter [00270, 01251], lr: 0.000553, loss: 0.4402
2022-10-02 12:56:15 - train: epoch 0009, iter [00280, 01251], lr: 0.000553, loss: 0.4549
2022-10-02 12:56:34 - train: epoch 0009, iter [00290, 01251], lr: 0.000554, loss: 0.4593
2022-10-02 12:56:54 - train: epoch 0009, iter [00300, 01251], lr: 0.000554, loss: 0.4270
2022-10-02 12:57:14 - train: epoch 0009, iter [00310, 01251], lr: 0.000555, loss: 0.4363
2022-10-02 12:57:33 - train: epoch 0009, iter [00320, 01251], lr: 0.000555, loss: 0.4415
2022-10-02 12:57:54 - train: epoch 0009, iter [00330, 01251], lr: 0.000556, loss: 0.4384
2022-10-02 12:58:13 - train: epoch 0009, iter [00340, 01251], lr: 0.000556, loss: 0.4675
2022-10-02 12:58:33 - train: epoch 0009, iter [00350, 01251], lr: 0.000557, loss: 0.4457
2022-10-02 12:58:53 - train: epoch 0009, iter [00360, 01251], lr: 0.000557, loss: 0.4091
2022-10-02 12:59:13 - train: epoch 0009, iter [00370, 01251], lr: 0.000558, loss: 0.4503
2022-10-02 12:59:33 - train: epoch 0009, iter [00380, 01251], lr: 0.000558, loss: 0.4498
2022-10-02 12:59:53 - train: epoch 0009, iter [00390, 01251], lr: 0.000559, loss: 0.4149
2022-10-02 13:00:13 - train: epoch 0009, iter [00400, 01251], lr: 0.000559, loss: 0.4447
2022-10-02 13:00:33 - train: epoch 0009, iter [00410, 01251], lr: 0.000560, loss: 0.4286
2022-10-02 13:00:53 - train: epoch 0009, iter [00420, 01251], lr: 0.000560, loss: 0.4511
2022-10-02 13:01:12 - train: epoch 0009, iter [00430, 01251], lr: 0.000561, loss: 0.4559
2022-10-02 13:01:32 - train: epoch 0009, iter [00440, 01251], lr: 0.000561, loss: 0.4476
2022-10-02 13:01:52 - train: epoch 0009, iter [00450, 01251], lr: 0.000562, loss: 0.4457
2022-10-02 13:02:12 - train: epoch 0009, iter [00460, 01251], lr: 0.000562, loss: 0.4728
2022-10-02 13:02:31 - train: epoch 0009, iter [00470, 01251], lr: 0.000563, loss: 0.4532
2022-10-02 13:02:51 - train: epoch 0009, iter [00480, 01251], lr: 0.000563, loss: 0.4558
2022-10-02 13:03:11 - train: epoch 0009, iter [00490, 01251], lr: 0.000564, loss: 0.4560
2022-10-02 13:03:31 - train: epoch 0009, iter [00500, 01251], lr: 0.000564, loss: 0.4472
2022-10-02 13:03:51 - train: epoch 0009, iter [00510, 01251], lr: 0.000564, loss: 0.4647
2022-10-02 13:04:11 - train: epoch 0009, iter [00520, 01251], lr: 0.000565, loss: 0.4406
2022-10-02 13:04:30 - train: epoch 0009, iter [00530, 01251], lr: 0.000565, loss: 0.4534
2022-10-02 13:04:50 - train: epoch 0009, iter [00540, 01251], lr: 0.000566, loss: 0.4408
2022-10-02 13:05:10 - train: epoch 0009, iter [00550, 01251], lr: 0.000566, loss: 0.4385
2022-10-02 13:05:30 - train: epoch 0009, iter [00560, 01251], lr: 0.000567, loss: 0.4558
2022-10-02 13:05:50 - train: epoch 0009, iter [00570, 01251], lr: 0.000567, loss: 0.4214
2022-10-02 13:06:10 - train: epoch 0009, iter [00580, 01251], lr: 0.000568, loss: 0.4660
2022-10-02 13:06:30 - train: epoch 0009, iter [00590, 01251], lr: 0.000568, loss: 0.4387
2022-10-02 13:06:50 - train: epoch 0009, iter [00600, 01251], lr: 0.000569, loss: 0.4431
2022-10-02 13:07:10 - train: epoch 0009, iter [00610, 01251], lr: 0.000569, loss: 0.4643
2022-10-02 13:07:29 - train: epoch 0009, iter [00620, 01251], lr: 0.000570, loss: 0.4392
2022-10-02 13:07:49 - train: epoch 0009, iter [00630, 01251], lr: 0.000570, loss: 0.4549
2022-10-02 13:08:09 - train: epoch 0009, iter [00640, 01251], lr: 0.000571, loss: 0.4614
2022-10-02 13:08:29 - train: epoch 0009, iter [00650, 01251], lr: 0.000571, loss: 0.4434
2022-10-02 13:08:49 - train: epoch 0009, iter [00660, 01251], lr: 0.000572, loss: 0.4586
2022-10-02 13:09:09 - train: epoch 0009, iter [00670, 01251], lr: 0.000572, loss: 0.4557
2022-10-02 13:09:28 - train: epoch 0009, iter [00680, 01251], lr: 0.000573, loss: 0.4451
2022-10-02 13:09:48 - train: epoch 0009, iter [00690, 01251], lr: 0.000573, loss: 0.4355
2022-10-02 13:10:08 - train: epoch 0009, iter [00700, 01251], lr: 0.000574, loss: 0.4657
2022-10-02 13:10:28 - train: epoch 0009, iter [00710, 01251], lr: 0.000574, loss: 0.4412
2022-10-02 13:10:47 - train: epoch 0009, iter [00720, 01251], lr: 0.000575, loss: 0.4410
2022-10-02 13:11:07 - train: epoch 0009, iter [00730, 01251], lr: 0.000575, loss: 0.4223
2022-10-02 13:11:27 - train: epoch 0009, iter [00740, 01251], lr: 0.000575, loss: 0.4569
2022-10-02 13:11:47 - train: epoch 0009, iter [00750, 01251], lr: 0.000576, loss: 0.4375
2022-10-02 13:12:07 - train: epoch 0009, iter [00760, 01251], lr: 0.000576, loss: 0.4226
2022-10-02 13:12:26 - train: epoch 0009, iter [00770, 01251], lr: 0.000577, loss: 0.4419
2022-10-02 13:12:46 - train: epoch 0009, iter [00780, 01251], lr: 0.000577, loss: 0.4283
2022-10-02 13:13:06 - train: epoch 0009, iter [00790, 01251], lr: 0.000578, loss: 0.4506
2022-10-02 13:13:26 - train: epoch 0009, iter [00800, 01251], lr: 0.000578, loss: 0.4511
2022-10-02 13:13:46 - train: epoch 0009, iter [00810, 01251], lr: 0.000579, loss: 0.4357
2022-10-02 13:14:06 - train: epoch 0009, iter [00820, 01251], lr: 0.000579, loss: 0.4310
2022-10-02 13:14:25 - train: epoch 0009, iter [00830, 01251], lr: 0.000580, loss: 0.4593
2022-10-02 13:14:45 - train: epoch 0009, iter [00840, 01251], lr: 0.000580, loss: 0.4367
2022-10-02 13:15:04 - train: epoch 0009, iter [00850, 01251], lr: 0.000581, loss: 0.4555
2022-10-02 13:15:24 - train: epoch 0009, iter [00860, 01251], lr: 0.000581, loss: 0.4451
2022-10-02 13:15:44 - train: epoch 0009, iter [00870, 01251], lr: 0.000582, loss: 0.4611
2022-10-02 13:16:04 - train: epoch 0009, iter [00880, 01251], lr: 0.000582, loss: 0.4709
2022-10-02 13:16:23 - train: epoch 0009, iter [00890, 01251], lr: 0.000583, loss: 0.4525
2022-10-02 13:16:43 - train: epoch 0009, iter [00900, 01251], lr: 0.000583, loss: 0.4579
2022-10-02 13:17:03 - train: epoch 0009, iter [00910, 01251], lr: 0.000584, loss: 0.4443
2022-10-02 13:17:23 - train: epoch 0009, iter [00920, 01251], lr: 0.000584, loss: 0.4335
2022-10-02 13:17:43 - train: epoch 0009, iter [00930, 01251], lr: 0.000585, loss: 0.4511
2022-10-02 13:18:02 - train: epoch 0009, iter [00940, 01251], lr: 0.000585, loss: 0.4466
2022-10-02 13:18:22 - train: epoch 0009, iter [00950, 01251], lr: 0.000586, loss: 0.4428
2022-10-02 13:18:42 - train: epoch 0009, iter [00960, 01251], lr: 0.000586, loss: 0.4429
2022-10-02 13:19:02 - train: epoch 0009, iter [00970, 01251], lr: 0.000587, loss: 0.4305
2022-10-02 13:19:21 - train: epoch 0009, iter [00980, 01251], lr: 0.000587, loss: 0.4564
2022-10-02 13:19:41 - train: epoch 0009, iter [00990, 01251], lr: 0.000587, loss: 0.4433
2022-10-02 13:20:01 - train: epoch 0009, iter [01000, 01251], lr: 0.000588, loss: 0.4578
2022-10-02 13:20:21 - train: epoch 0009, iter [01010, 01251], lr: 0.000588, loss: 0.4568
2022-10-02 13:20:41 - train: epoch 0009, iter [01020, 01251], lr: 0.000589, loss: 0.4311
2022-10-02 13:21:00 - train: epoch 0009, iter [01030, 01251], lr: 0.000589, loss: 0.4354
2022-10-02 13:21:20 - train: epoch 0009, iter [01040, 01251], lr: 0.000590, loss: 0.4538
2022-10-02 13:21:40 - train: epoch 0009, iter [01050, 01251], lr: 0.000590, loss: 0.4531
2022-10-02 13:22:00 - train: epoch 0009, iter [01060, 01251], lr: 0.000591, loss: 0.4378
2022-10-02 13:22:20 - train: epoch 0009, iter [01070, 01251], lr: 0.000591, loss: 0.4366
2022-10-02 13:22:40 - train: epoch 0009, iter [01080, 01251], lr: 0.000592, loss: 0.4591
2022-10-02 13:22:59 - train: epoch 0009, iter [01090, 01251], lr: 0.000592, loss: 0.4603
2022-10-02 13:23:19 - train: epoch 0009, iter [01100, 01251], lr: 0.000593, loss: 0.4509
2022-10-02 13:23:39 - train: epoch 0009, iter [01110, 01251], lr: 0.000593, loss: 0.4389
2022-10-02 13:23:59 - train: epoch 0009, iter [01120, 01251], lr: 0.000594, loss: 0.4434
2022-10-02 13:24:18 - train: epoch 0009, iter [01130, 01251], lr: 0.000594, loss: 0.4467
2022-10-02 13:24:38 - train: epoch 0009, iter [01140, 01251], lr: 0.000595, loss: 0.4337
2022-10-02 13:24:58 - train: epoch 0009, iter [01150, 01251], lr: 0.000595, loss: 0.4312
2022-10-02 13:25:18 - train: epoch 0009, iter [01160, 01251], lr: 0.000596, loss: 0.4387
2022-10-02 13:25:37 - train: epoch 0009, iter [01170, 01251], lr: 0.000596, loss: 0.4734
2022-10-02 13:25:57 - train: epoch 0009, iter [01180, 01251], lr: 0.000597, loss: 0.4360
2022-10-02 13:26:17 - train: epoch 0009, iter [01190, 01251], lr: 0.000597, loss: 0.4375
2022-10-02 13:26:37 - train: epoch 0009, iter [01200, 01251], lr: 0.000598, loss: 0.4504
2022-10-02 13:26:57 - train: epoch 0009, iter [01210, 01251], lr: 0.000598, loss: 0.4400
2022-10-02 13:27:17 - train: epoch 0009, iter [01220, 01251], lr: 0.000599, loss: 0.4509
2022-10-02 13:27:36 - train: epoch 0009, iter [01230, 01251], lr: 0.000599, loss: 0.4607
2022-10-02 13:27:56 - train: epoch 0009, iter [01240, 01251], lr: 0.000599, loss: 0.4353
2022-10-02 13:28:14 - train: epoch 0009, iter [01250, 01251], lr: 0.000600, loss: 0.4394
2022-10-02 13:28:18 - train: epoch 009, train_loss: 0.4466
2022-10-02 13:28:21 - until epoch: 009, best_loss: 0.4466
2022-10-02 13:28:21 - epoch 010 lr: 0.000600
2022-10-02 13:28:48 - train: epoch 0010, iter [00010, 01251], lr: 0.000600, loss: 0.4387
2022-10-02 13:29:08 - train: epoch 0010, iter [00020, 01251], lr: 0.000601, loss: 0.4526
2022-10-02 13:29:28 - train: epoch 0010, iter [00030, 01251], lr: 0.000601, loss: 0.4564
2022-10-02 13:29:48 - train: epoch 0010, iter [00040, 01251], lr: 0.000602, loss: 0.4434
2022-10-02 13:30:08 - train: epoch 0010, iter [00050, 01251], lr: 0.000602, loss: 0.4278
2022-10-02 13:30:29 - train: epoch 0010, iter [00060, 01251], lr: 0.000603, loss: 0.4441
2022-10-02 13:30:49 - train: epoch 0010, iter [00070, 01251], lr: 0.000603, loss: 0.4564
2022-10-02 13:31:09 - train: epoch 0010, iter [00080, 01251], lr: 0.000604, loss: 0.4561
2022-10-02 13:31:29 - train: epoch 0010, iter [00090, 01251], lr: 0.000604, loss: 0.4624
2022-10-02 13:31:49 - train: epoch 0010, iter [00100, 01251], lr: 0.000605, loss: 0.4390
2022-10-02 13:32:08 - train: epoch 0010, iter [00110, 01251], lr: 0.000605, loss: 0.4488
2022-10-02 13:32:28 - train: epoch 0010, iter [00120, 01251], lr: 0.000606, loss: 0.4314
2022-10-02 13:32:47 - train: epoch 0010, iter [00130, 01251], lr: 0.000606, loss: 0.4331
2022-10-02 13:33:08 - train: epoch 0010, iter [00140, 01251], lr: 0.000607, loss: 0.4490
2022-10-02 13:33:28 - train: epoch 0010, iter [00150, 01251], lr: 0.000607, loss: 0.4465
2022-10-02 13:33:48 - train: epoch 0010, iter [00160, 01251], lr: 0.000608, loss: 0.4566
2022-10-02 13:34:08 - train: epoch 0010, iter [00170, 01251], lr: 0.000608, loss: 0.4492
2022-10-02 13:34:27 - train: epoch 0010, iter [00180, 01251], lr: 0.000609, loss: 0.4536
2022-10-02 13:34:47 - train: epoch 0010, iter [00190, 01251], lr: 0.000609, loss: 0.4345
2022-10-02 13:35:07 - train: epoch 0010, iter [00200, 01251], lr: 0.000610, loss: 0.4482
2022-10-02 13:35:27 - train: epoch 0010, iter [00210, 01251], lr: 0.000610, loss: 0.4350
2022-10-02 13:35:47 - train: epoch 0010, iter [00220, 01251], lr: 0.000611, loss: 0.4458
2022-10-02 13:36:07 - train: epoch 0010, iter [00230, 01251], lr: 0.000611, loss: 0.4478
2022-10-02 13:36:27 - train: epoch 0010, iter [00240, 01251], lr: 0.000612, loss: 0.4263
2022-10-02 13:36:47 - train: epoch 0010, iter [00250, 01251], lr: 0.000612, loss: 0.4441
2022-10-02 13:37:07 - train: epoch 0010, iter [00260, 01251], lr: 0.000612, loss: 0.4245
2022-10-02 13:37:26 - train: epoch 0010, iter [00270, 01251], lr: 0.000613, loss: 0.4505
2022-10-02 13:37:46 - train: epoch 0010, iter [00280, 01251], lr: 0.000613, loss: 0.4661
2022-10-02 13:38:06 - train: epoch 0010, iter [00290, 01251], lr: 0.000614, loss: 0.4482
2022-10-02 13:38:25 - train: epoch 0010, iter [00300, 01251], lr: 0.000614, loss: 0.4390
2022-10-02 13:38:46 - train: epoch 0010, iter [00310, 01251], lr: 0.000615, loss: 0.4482
2022-10-02 13:39:05 - train: epoch 0010, iter [00320, 01251], lr: 0.000615, loss: 0.4478
2022-10-02 13:39:25 - train: epoch 0010, iter [00330, 01251], lr: 0.000616, loss: 0.4627
2022-10-02 13:39:45 - train: epoch 0010, iter [00340, 01251], lr: 0.000616, loss: 0.4467
2022-10-02 13:40:05 - train: epoch 0010, iter [00350, 01251], lr: 0.000617, loss: 0.4310
2022-10-02 13:40:25 - train: epoch 0010, iter [00360, 01251], lr: 0.000617, loss: 0.4690
2022-10-02 13:40:45 - train: epoch 0010, iter [00370, 01251], lr: 0.000618, loss: 0.4565
2022-10-02 13:41:05 - train: epoch 0010, iter [00380, 01251], lr: 0.000618, loss: 0.4549
2022-10-02 13:41:25 - train: epoch 0010, iter [00390, 01251], lr: 0.000619, loss: 0.4249
2022-10-02 13:41:44 - train: epoch 0010, iter [00400, 01251], lr: 0.000619, loss: 0.4506
2022-10-02 13:42:04 - train: epoch 0010, iter [00410, 01251], lr: 0.000620, loss: 0.4411
2022-10-02 13:42:24 - train: epoch 0010, iter [00420, 01251], lr: 0.000620, loss: 0.4272
2022-10-02 13:42:44 - train: epoch 0010, iter [00430, 01251], lr: 0.000621, loss: 0.4199
2022-10-02 13:43:03 - train: epoch 0010, iter [00440, 01251], lr: 0.000621, loss: 0.4554
2022-10-02 13:43:24 - train: epoch 0010, iter [00450, 01251], lr: 0.000622, loss: 0.4474
2022-10-02 13:43:43 - train: epoch 0010, iter [00460, 01251], lr: 0.000622, loss: 0.4390
2022-10-02 13:44:03 - train: epoch 0010, iter [00470, 01251], lr: 0.000623, loss: 0.4628
2022-10-02 13:44:23 - train: epoch 0010, iter [00480, 01251], lr: 0.000623, loss: 0.4406
2022-10-02 13:44:43 - train: epoch 0010, iter [00490, 01251], lr: 0.000624, loss: 0.4446
2022-10-02 13:45:03 - train: epoch 0010, iter [00500, 01251], lr: 0.000624, loss: 0.4430
2022-10-02 13:45:23 - train: epoch 0010, iter [00510, 01251], lr: 0.000624, loss: 0.4444
2022-10-02 13:45:43 - train: epoch 0010, iter [00520, 01251], lr: 0.000625, loss: 0.4328
2022-10-02 13:46:03 - train: epoch 0010, iter [00530, 01251], lr: 0.000625, loss: 0.4421
2022-10-02 13:46:23 - train: epoch 0010, iter [00540, 01251], lr: 0.000626, loss: 0.4461
2022-10-02 13:46:43 - train: epoch 0010, iter [00550, 01251], lr: 0.000626, loss: 0.4561
2022-10-02 13:47:03 - train: epoch 0010, iter [00560, 01251], lr: 0.000627, loss: 0.4331
2022-10-02 13:47:22 - train: epoch 0010, iter [00570, 01251], lr: 0.000627, loss: 0.4402
2022-10-02 13:47:42 - train: epoch 0010, iter [00580, 01251], lr: 0.000628, loss: 0.4345
2022-10-02 13:48:02 - train: epoch 0010, iter [00590, 01251], lr: 0.000628, loss: 0.4325
2022-10-02 13:48:22 - train: epoch 0010, iter [00600, 01251], lr: 0.000629, loss: 0.4406
2022-10-02 13:48:42 - train: epoch 0010, iter [00610, 01251], lr: 0.000629, loss: 0.4535
2022-10-02 13:49:02 - train: epoch 0010, iter [00620, 01251], lr: 0.000630, loss: 0.4535
2022-10-02 13:49:22 - train: epoch 0010, iter [00630, 01251], lr: 0.000630, loss: 0.4326
2022-10-02 13:49:42 - train: epoch 0010, iter [00640, 01251], lr: 0.000631, loss: 0.4578
2022-10-02 13:50:02 - train: epoch 0010, iter [00650, 01251], lr: 0.000631, loss: 0.4365
2022-10-02 13:50:22 - train: epoch 0010, iter [00660, 01251], lr: 0.000632, loss: 0.4409
2022-10-02 13:50:42 - train: epoch 0010, iter [00670, 01251], lr: 0.000632, loss: 0.4522
2022-10-02 13:51:02 - train: epoch 0010, iter [00680, 01251], lr: 0.000633, loss: 0.4453
2022-10-02 13:51:22 - train: epoch 0010, iter [00690, 01251], lr: 0.000633, loss: 0.4322
2022-10-02 13:51:42 - train: epoch 0010, iter [00700, 01251], lr: 0.000634, loss: 0.4413
2022-10-02 13:52:02 - train: epoch 0010, iter [00710, 01251], lr: 0.000634, loss: 0.4383
2022-10-02 13:52:22 - train: epoch 0010, iter [00720, 01251], lr: 0.000635, loss: 0.4358
2022-10-02 13:52:42 - train: epoch 0010, iter [00730, 01251], lr: 0.000635, loss: 0.4560
2022-10-02 13:53:01 - train: epoch 0010, iter [00740, 01251], lr: 0.000635, loss: 0.4460
2022-10-02 13:53:21 - train: epoch 0010, iter [00750, 01251], lr: 0.000636, loss: 0.4488
2022-10-02 13:53:41 - train: epoch 0010, iter [00760, 01251], lr: 0.000636, loss: 0.4213
2022-10-02 13:54:00 - train: epoch 0010, iter [00770, 01251], lr: 0.000637, loss: 0.4644
2022-10-02 13:54:20 - train: epoch 0010, iter [00780, 01251], lr: 0.000637, loss: 0.4640
2022-10-02 13:54:40 - train: epoch 0010, iter [00790, 01251], lr: 0.000638, loss: 0.4293
2022-10-02 13:55:00 - train: epoch 0010, iter [00800, 01251], lr: 0.000638, loss: 0.4466
2022-10-02 13:55:19 - train: epoch 0010, iter [00810, 01251], lr: 0.000639, loss: 0.4558
2022-10-02 13:55:39 - train: epoch 0010, iter [00820, 01251], lr: 0.000639, loss: 0.4615
2022-10-02 13:55:59 - train: epoch 0010, iter [00830, 01251], lr: 0.000640, loss: 0.4503
2022-10-02 13:56:19 - train: epoch 0010, iter [00840, 01251], lr: 0.000640, loss: 0.4460
2022-10-02 13:56:39 - train: epoch 0010, iter [00850, 01251], lr: 0.000641, loss: 0.4310
2022-10-02 13:56:58 - train: epoch 0010, iter [00860, 01251], lr: 0.000641, loss: 0.4450
2022-10-02 13:57:18 - train: epoch 0010, iter [00870, 01251], lr: 0.000642, loss: 0.4358
2022-10-02 13:57:38 - train: epoch 0010, iter [00880, 01251], lr: 0.000642, loss: 0.4322
2022-10-02 13:57:58 - train: epoch 0010, iter [00890, 01251], lr: 0.000643, loss: 0.4322
2022-10-02 13:58:17 - train: epoch 0010, iter [00900, 01251], lr: 0.000643, loss: 0.4417
2022-10-02 13:58:37 - train: epoch 0010, iter [00910, 01251], lr: 0.000644, loss: 0.4313
2022-10-02 13:58:57 - train: epoch 0010, iter [00920, 01251], lr: 0.000644, loss: 0.4585
2022-10-02 13:59:16 - train: epoch 0010, iter [00930, 01251], lr: 0.000645, loss: 0.4562
2022-10-02 13:59:36 - train: epoch 0010, iter [00940, 01251], lr: 0.000645, loss: 0.4283
2022-10-02 13:59:56 - train: epoch 0010, iter [00950, 01251], lr: 0.000646, loss: 0.4305
2022-10-02 14:00:15 - train: epoch 0010, iter [00960, 01251], lr: 0.000646, loss: 0.4366
2022-10-02 14:00:36 - train: epoch 0010, iter [00970, 01251], lr: 0.000647, loss: 0.4454
2022-10-02 14:00:55 - train: epoch 0010, iter [00980, 01251], lr: 0.000647, loss: 0.4629
2022-10-02 14:01:15 - train: epoch 0010, iter [00990, 01251], lr: 0.000647, loss: 0.4463
2022-10-02 14:01:35 - train: epoch 0010, iter [01000, 01251], lr: 0.000648, loss: 0.4475
2022-10-02 14:01:55 - train: epoch 0010, iter [01010, 01251], lr: 0.000648, loss: 0.4376
2022-10-02 14:02:15 - train: epoch 0010, iter [01020, 01251], lr: 0.000649, loss: 0.4399
2022-10-02 14:02:35 - train: epoch 0010, iter [01030, 01251], lr: 0.000649, loss: 0.4661
2022-10-02 14:02:55 - train: epoch 0010, iter [01040, 01251], lr: 0.000650, loss: 0.4338
2022-10-02 14:03:14 - train: epoch 0010, iter [01050, 01251], lr: 0.000650, loss: 0.4713
2022-10-02 14:03:33 - train: epoch 0010, iter [01060, 01251], lr: 0.000651, loss: 0.4233
2022-10-02 14:03:53 - train: epoch 0010, iter [01070, 01251], lr: 0.000651, loss: 0.4290
2022-10-02 14:04:13 - train: epoch 0010, iter [01080, 01251], lr: 0.000652, loss: 0.4476
2022-10-02 14:04:33 - train: epoch 0010, iter [01090, 01251], lr: 0.000652, loss: 0.4494
2022-10-02 14:04:53 - train: epoch 0010, iter [01100, 01251], lr: 0.000653, loss: 0.4405
2022-10-02 14:05:12 - train: epoch 0010, iter [01110, 01251], lr: 0.000653, loss: 0.4532
2022-10-02 14:05:32 - train: epoch 0010, iter [01120, 01251], lr: 0.000654, loss: 0.4317
2022-10-02 14:05:52 - train: epoch 0010, iter [01130, 01251], lr: 0.000654, loss: 0.4440
2022-10-02 14:06:12 - train: epoch 0010, iter [01140, 01251], lr: 0.000655, loss: 0.4478
2022-10-02 14:06:32 - train: epoch 0010, iter [01150, 01251], lr: 0.000655, loss: 0.4335
2022-10-02 14:06:52 - train: epoch 0010, iter [01160, 01251], lr: 0.000656, loss: 0.4554
2022-10-02 14:07:11 - train: epoch 0010, iter [01170, 01251], lr: 0.000656, loss: 0.4354
2022-10-02 14:07:31 - train: epoch 0010, iter [01180, 01251], lr: 0.000657, loss: 0.4393
2022-10-02 14:07:51 - train: epoch 0010, iter [01190, 01251], lr: 0.000657, loss: 0.4180
2022-10-02 14:08:10 - train: epoch 0010, iter [01200, 01251], lr: 0.000658, loss: 0.4305
2022-10-02 14:08:30 - train: epoch 0010, iter [01210, 01251], lr: 0.000658, loss: 0.4623
2022-10-02 14:08:50 - train: epoch 0010, iter [01220, 01251], lr: 0.000659, loss: 0.4510
2022-10-02 14:09:11 - train: epoch 0010, iter [01230, 01251], lr: 0.000659, loss: 0.4064
2022-10-02 14:09:30 - train: epoch 0010, iter [01240, 01251], lr: 0.000659, loss: 0.4692
2022-10-02 14:09:49 - train: epoch 0010, iter [01250, 01251], lr: 0.000660, loss: 0.4268
2022-10-02 14:09:52 - train: epoch 010, train_loss: 0.4435
2022-10-02 14:09:57 - until epoch: 010, best_loss: 0.4435
2022-10-02 14:09:57 - epoch 011 lr: 0.000600
2022-10-02 14:10:23 - train: epoch 0011, iter [00010, 01251], lr: 0.000600, loss: 0.4625
2022-10-02 14:10:42 - train: epoch 0011, iter [00020, 01251], lr: 0.000600, loss: 0.4347
2022-10-02 14:11:02 - train: epoch 0011, iter [00030, 01251], lr: 0.000600, loss: 0.4406
2022-10-02 14:11:21 - train: epoch 0011, iter [00040, 01251], lr: 0.000600, loss: 0.4287
2022-10-02 14:11:41 - train: epoch 0011, iter [00050, 01251], lr: 0.000600, loss: 0.4241
2022-10-02 14:12:01 - train: epoch 0011, iter [00060, 01251], lr: 0.000600, loss: 0.4549
2022-10-02 14:12:20 - train: epoch 0011, iter [00070, 01251], lr: 0.000600, loss: 0.4563
2022-10-02 14:12:40 - train: epoch 0011, iter [00080, 01251], lr: 0.000600, loss: 0.4523
2022-10-02 14:12:59 - train: epoch 0011, iter [00090, 01251], lr: 0.000600, loss: 0.4549
2022-10-02 14:13:19 - train: epoch 0011, iter [00100, 01251], lr: 0.000600, loss: 0.4228
2022-10-02 14:13:39 - train: epoch 0011, iter [00110, 01251], lr: 0.000600, loss: 0.4324
2022-10-02 14:13:59 - train: epoch 0011, iter [00120, 01251], lr: 0.000600, loss: 0.4338
2022-10-02 14:14:18 - train: epoch 0011, iter [00130, 01251], lr: 0.000600, loss: 0.4531
2022-10-02 14:14:38 - train: epoch 0011, iter [00140, 01251], lr: 0.000600, loss: 0.4355
2022-10-02 14:14:58 - train: epoch 0011, iter [00150, 01251], lr: 0.000600, loss: 0.4445
2022-10-02 14:15:17 - train: epoch 0011, iter [00160, 01251], lr: 0.000600, loss: 0.4213
2022-10-02 14:15:37 - train: epoch 0011, iter [00170, 01251], lr: 0.000600, loss: 0.4347
2022-10-02 14:15:56 - train: epoch 0011, iter [00180, 01251], lr: 0.000600, loss: 0.4319
2022-10-02 14:16:16 - train: epoch 0011, iter [00190, 01251], lr: 0.000600, loss: 0.4172
2022-10-02 14:16:36 - train: epoch 0011, iter [00200, 01251], lr: 0.000600, loss: 0.4417
2022-10-02 14:16:56 - train: epoch 0011, iter [00210, 01251], lr: 0.000600, loss: 0.4517
2022-10-02 14:17:15 - train: epoch 0011, iter [00220, 01251], lr: 0.000600, loss: 0.4350
2022-10-02 14:17:35 - train: epoch 0011, iter [00230, 01251], lr: 0.000600, loss: 0.4408
2022-10-02 14:17:55 - train: epoch 0011, iter [00240, 01251], lr: 0.000600, loss: 0.4289
2022-10-02 14:18:14 - train: epoch 0011, iter [00250, 01251], lr: 0.000600, loss: 0.4421
2022-10-02 14:18:34 - train: epoch 0011, iter [00260, 01251], lr: 0.000600, loss: 0.4328
2022-10-02 14:18:53 - train: epoch 0011, iter [00270, 01251], lr: 0.000600, loss: 0.4148
2022-10-02 14:19:12 - train: epoch 0011, iter [00280, 01251], lr: 0.000600, loss: 0.4461
2022-10-02 14:19:32 - train: epoch 0011, iter [00290, 01251], lr: 0.000600, loss: 0.4257
2022-10-02 14:19:52 - train: epoch 0011, iter [00300, 01251], lr: 0.000600, loss: 0.4169
2022-10-02 14:20:12 - train: epoch 0011, iter [00310, 01251], lr: 0.000600, loss: 0.4274
2022-10-02 14:20:32 - train: epoch 0011, iter [00320, 01251], lr: 0.000600, loss: 0.4320
2022-10-02 14:20:51 - train: epoch 0011, iter [00330, 01251], lr: 0.000600, loss: 0.4423
2022-10-02 14:21:11 - train: epoch 0011, iter [00340, 01251], lr: 0.000600, loss: 0.4396
2022-10-02 14:21:31 - train: epoch 0011, iter [00350, 01251], lr: 0.000600, loss: 0.4604
2022-10-02 14:21:50 - train: epoch 0011, iter [00360, 01251], lr: 0.000600, loss: 0.4341
2022-10-02 14:22:10 - train: epoch 0011, iter [00370, 01251], lr: 0.000600, loss: 0.4630
2022-10-02 14:22:30 - train: epoch 0011, iter [00380, 01251], lr: 0.000600, loss: 0.4483
2022-10-02 14:22:50 - train: epoch 0011, iter [00390, 01251], lr: 0.000600, loss: 0.4409
2022-10-02 14:23:09 - train: epoch 0011, iter [00400, 01251], lr: 0.000600, loss: 0.4523
2022-10-02 14:23:29 - train: epoch 0011, iter [00410, 01251], lr: 0.000600, loss: 0.4391
2022-10-02 14:23:49 - train: epoch 0011, iter [00420, 01251], lr: 0.000600, loss: 0.4451
2022-10-02 14:24:09 - train: epoch 0011, iter [00430, 01251], lr: 0.000600, loss: 0.4336
2022-10-02 14:24:28 - train: epoch 0011, iter [00440, 01251], lr: 0.000600, loss: 0.4191
2022-10-02 14:24:48 - train: epoch 0011, iter [00450, 01251], lr: 0.000600, loss: 0.4473
2022-10-02 14:25:08 - train: epoch 0011, iter [00460, 01251], lr: 0.000600, loss: 0.4517
2022-10-02 14:25:28 - train: epoch 0011, iter [00470, 01251], lr: 0.000600, loss: 0.4269
2022-10-02 14:25:47 - train: epoch 0011, iter [00480, 01251], lr: 0.000600, loss: 0.4490
2022-10-02 14:26:07 - train: epoch 0011, iter [00490, 01251], lr: 0.000600, loss: 0.4266
2022-10-02 14:26:26 - train: epoch 0011, iter [00500, 01251], lr: 0.000600, loss: 0.4485
2022-10-02 14:26:46 - train: epoch 0011, iter [00510, 01251], lr: 0.000600, loss: 0.4532
2022-10-02 14:27:06 - train: epoch 0011, iter [00520, 01251], lr: 0.000600, loss: 0.4313
2022-10-02 14:27:25 - train: epoch 0011, iter [00530, 01251], lr: 0.000600, loss: 0.4336
2022-10-02 14:27:45 - train: epoch 0011, iter [00540, 01251], lr: 0.000600, loss: 0.4298
2022-10-02 14:28:05 - train: epoch 0011, iter [00550, 01251], lr: 0.000600, loss: 0.4479
2022-10-02 14:28:24 - train: epoch 0011, iter [00560, 01251], lr: 0.000600, loss: 0.4529
2022-10-02 14:28:44 - train: epoch 0011, iter [00570, 01251], lr: 0.000600, loss: 0.4460
2022-10-02 14:29:03 - train: epoch 0011, iter [00580, 01251], lr: 0.000600, loss: 0.3963
2022-10-02 14:29:23 - train: epoch 0011, iter [00590, 01251], lr: 0.000600, loss: 0.4429
2022-10-02 14:29:43 - train: epoch 0011, iter [00600, 01251], lr: 0.000600, loss: 0.4272
2022-10-02 14:30:02 - train: epoch 0011, iter [00610, 01251], lr: 0.000600, loss: 0.4423
2022-10-02 14:30:21 - train: epoch 0011, iter [00620, 01251], lr: 0.000600, loss: 0.4590
2022-10-02 14:30:41 - train: epoch 0011, iter [00630, 01251], lr: 0.000600, loss: 0.4338
2022-10-02 14:31:01 - train: epoch 0011, iter [00640, 01251], lr: 0.000600, loss: 0.4357
2022-10-02 14:31:20 - train: epoch 0011, iter [00650, 01251], lr: 0.000600, loss: 0.4554
2022-10-02 14:31:40 - train: epoch 0011, iter [00660, 01251], lr: 0.000600, loss: 0.4557
2022-10-02 14:32:00 - train: epoch 0011, iter [00670, 01251], lr: 0.000600, loss: 0.4430
2022-10-02 14:32:20 - train: epoch 0011, iter [00680, 01251], lr: 0.000600, loss: 0.4311
2022-10-02 14:32:39 - train: epoch 0011, iter [00690, 01251], lr: 0.000600, loss: 0.4332
2022-10-02 14:32:59 - train: epoch 0011, iter [00700, 01251], lr: 0.000600, loss: 0.4186
2022-10-02 14:33:19 - train: epoch 0011, iter [00710, 01251], lr: 0.000600, loss: 0.4215
2022-10-02 14:33:38 - train: epoch 0011, iter [00720, 01251], lr: 0.000600, loss: 0.4317
2022-10-02 14:33:58 - train: epoch 0011, iter [00730, 01251], lr: 0.000600, loss: 0.4275
2022-10-02 14:34:17 - train: epoch 0011, iter [00740, 01251], lr: 0.000600, loss: 0.4174
2022-10-02 14:34:37 - train: epoch 0011, iter [00750, 01251], lr: 0.000600, loss: 0.4548
2022-10-02 14:34:57 - train: epoch 0011, iter [00760, 01251], lr: 0.000600, loss: 0.4221
2022-10-02 14:35:16 - train: epoch 0011, iter [00770, 01251], lr: 0.000600, loss: 0.4434
2022-10-02 14:35:36 - train: epoch 0011, iter [00780, 01251], lr: 0.000600, loss: 0.4306
2022-10-02 14:35:56 - train: epoch 0011, iter [00790, 01251], lr: 0.000600, loss: 0.4383
2022-10-02 14:36:15 - train: epoch 0011, iter [00800, 01251], lr: 0.000600, loss: 0.4493
2022-10-02 14:36:35 - train: epoch 0011, iter [00810, 01251], lr: 0.000600, loss: 0.4507
2022-10-02 14:36:55 - train: epoch 0011, iter [00820, 01251], lr: 0.000600, loss: 0.4507
2022-10-02 14:37:14 - train: epoch 0011, iter [00830, 01251], lr: 0.000600, loss: 0.4229
2022-10-02 14:37:34 - train: epoch 0011, iter [00840, 01251], lr: 0.000600, loss: 0.4368
2022-10-02 14:37:54 - train: epoch 0011, iter [00850, 01251], lr: 0.000600, loss: 0.4595
2022-10-02 14:38:13 - train: epoch 0011, iter [00860, 01251], lr: 0.000600, loss: 0.4307
2022-10-02 14:38:33 - train: epoch 0011, iter [00870, 01251], lr: 0.000600, loss: 0.4331
2022-10-02 14:38:52 - train: epoch 0011, iter [00880, 01251], lr: 0.000600, loss: 0.4324
2022-10-02 14:39:12 - train: epoch 0011, iter [00890, 01251], lr: 0.000600, loss: 0.4478
2022-10-02 14:39:32 - train: epoch 0011, iter [00900, 01251], lr: 0.000600, loss: 0.4348
2022-10-02 14:39:51 - train: epoch 0011, iter [00910, 01251], lr: 0.000600, loss: 0.4168
2022-10-02 14:40:10 - train: epoch 0011, iter [00920, 01251], lr: 0.000600, loss: 0.4548
2022-10-02 14:40:30 - train: epoch 0011, iter [00930, 01251], lr: 0.000600, loss: 0.4289
2022-10-02 14:40:49 - train: epoch 0011, iter [00940, 01251], lr: 0.000600, loss: 0.4463
2022-10-02 14:41:09 - train: epoch 0011, iter [00950, 01251], lr: 0.000600, loss: 0.4360
2022-10-02 14:41:29 - train: epoch 0011, iter [00960, 01251], lr: 0.000600, loss: 0.4344
2022-10-02 14:41:48 - train: epoch 0011, iter [00970, 01251], lr: 0.000600, loss: 0.4208
2022-10-02 14:42:08 - train: epoch 0011, iter [00980, 01251], lr: 0.000600, loss: 0.4437
2022-10-02 14:42:27 - train: epoch 0011, iter [00990, 01251], lr: 0.000600, loss: 0.4437
2022-10-02 14:42:47 - train: epoch 0011, iter [01000, 01251], lr: 0.000600, loss: 0.4430
2022-10-02 14:43:07 - train: epoch 0011, iter [01010, 01251], lr: 0.000600, loss: 0.4403
2022-10-02 14:43:27 - train: epoch 0011, iter [01020, 01251], lr: 0.000600, loss: 0.4564
2022-10-02 14:43:46 - train: epoch 0011, iter [01030, 01251], lr: 0.000600, loss: 0.4537
2022-10-02 14:44:06 - train: epoch 0011, iter [01040, 01251], lr: 0.000600, loss: 0.4356
2022-10-02 14:44:26 - train: epoch 0011, iter [01050, 01251], lr: 0.000600, loss: 0.4444
2022-10-02 14:44:45 - train: epoch 0011, iter [01060, 01251], lr: 0.000600, loss: 0.4480
2022-10-02 14:45:05 - train: epoch 0011, iter [01070, 01251], lr: 0.000600, loss: 0.4416
2022-10-02 14:45:25 - train: epoch 0011, iter [01080, 01251], lr: 0.000600, loss: 0.4265
2022-10-02 14:45:45 - train: epoch 0011, iter [01090, 01251], lr: 0.000600, loss: 0.4271
2022-10-02 14:46:04 - train: epoch 0011, iter [01100, 01251], lr: 0.000600, loss: 0.4400
2022-10-02 14:46:24 - train: epoch 0011, iter [01110, 01251], lr: 0.000600, loss: 0.4200
2022-10-02 14:46:44 - train: epoch 0011, iter [01120, 01251], lr: 0.000600, loss: 0.4453
2022-10-02 14:47:04 - train: epoch 0011, iter [01130, 01251], lr: 0.000600, loss: 0.4396
2022-10-02 14:47:23 - train: epoch 0011, iter [01140, 01251], lr: 0.000600, loss: 0.4351
2022-10-02 14:47:43 - train: epoch 0011, iter [01150, 01251], lr: 0.000600, loss: 0.4316
2022-10-02 14:48:03 - train: epoch 0011, iter [01160, 01251], lr: 0.000600, loss: 0.4430
2022-10-02 14:48:23 - train: epoch 0011, iter [01170, 01251], lr: 0.000600, loss: 0.4302
2022-10-02 14:48:43 - train: epoch 0011, iter [01180, 01251], lr: 0.000600, loss: 0.4450
2022-10-02 14:49:03 - train: epoch 0011, iter [01190, 01251], lr: 0.000600, loss: 0.4519
2022-10-02 14:49:23 - train: epoch 0011, iter [01200, 01251], lr: 0.000600, loss: 0.4372
2022-10-02 14:49:43 - train: epoch 0011, iter [01210, 01251], lr: 0.000600, loss: 0.4551
2022-10-02 14:50:02 - train: epoch 0011, iter [01220, 01251], lr: 0.000600, loss: 0.4307
2022-10-02 14:50:22 - train: epoch 0011, iter [01230, 01251], lr: 0.000600, loss: 0.4394
2022-10-02 14:50:42 - train: epoch 0011, iter [01240, 01251], lr: 0.000600, loss: 0.4261
2022-10-02 14:51:01 - train: epoch 0011, iter [01250, 01251], lr: 0.000600, loss: 0.4463
2022-10-02 14:51:04 - train: epoch 011, train_loss: 0.4394
2022-10-02 14:51:09 - until epoch: 011, best_loss: 0.4394
2022-10-02 14:51:09 - epoch 012 lr: 0.000600
2022-10-02 14:51:35 - train: epoch 0012, iter [00010, 01251], lr: 0.000600, loss: 0.4189
2022-10-02 14:51:55 - train: epoch 0012, iter [00020, 01251], lr: 0.000600, loss: 0.4309
2022-10-02 14:52:15 - train: epoch 0012, iter [00030, 01251], lr: 0.000600, loss: 0.4499
2022-10-02 14:52:35 - train: epoch 0012, iter [00040, 01251], lr: 0.000600, loss: 0.4358
2022-10-02 14:52:56 - train: epoch 0012, iter [00050, 01251], lr: 0.000600, loss: 0.4443
2022-10-02 14:53:16 - train: epoch 0012, iter [00060, 01251], lr: 0.000600, loss: 0.4459
2022-10-02 14:53:36 - train: epoch 0012, iter [00070, 01251], lr: 0.000600, loss: 0.4316
2022-10-02 14:53:56 - train: epoch 0012, iter [00080, 01251], lr: 0.000600, loss: 0.4341
2022-10-02 14:54:16 - train: epoch 0012, iter [00090, 01251], lr: 0.000600, loss: 0.4298
2022-10-02 14:54:36 - train: epoch 0012, iter [00100, 01251], lr: 0.000600, loss: 0.4227
2022-10-02 14:54:56 - train: epoch 0012, iter [00110, 01251], lr: 0.000600, loss: 0.4396
2022-10-02 14:55:16 - train: epoch 0012, iter [00120, 01251], lr: 0.000600, loss: 0.4663
2022-10-02 14:55:36 - train: epoch 0012, iter [00130, 01251], lr: 0.000600, loss: 0.4359
2022-10-02 14:55:56 - train: epoch 0012, iter [00140, 01251], lr: 0.000600, loss: 0.4189
2022-10-02 14:56:17 - train: epoch 0012, iter [00150, 01251], lr: 0.000600, loss: 0.4300
2022-10-02 14:56:37 - train: epoch 0012, iter [00160, 01251], lr: 0.000600, loss: 0.4649
2022-10-02 14:56:57 - train: epoch 0012, iter [00170, 01251], lr: 0.000600, loss: 0.4217
2022-10-02 14:57:17 - train: epoch 0012, iter [00180, 01251], lr: 0.000600, loss: 0.4638
2022-10-02 14:57:37 - train: epoch 0012, iter [00190, 01251], lr: 0.000600, loss: 0.4486
2022-10-02 14:57:58 - train: epoch 0012, iter [00200, 01251], lr: 0.000600, loss: 0.4244
2022-10-02 14:58:18 - train: epoch 0012, iter [00210, 01251], lr: 0.000600, loss: 0.4627
2022-10-02 14:58:38 - train: epoch 0012, iter [00220, 01251], lr: 0.000600, loss: 0.4475
2022-10-02 14:58:58 - train: epoch 0012, iter [00230, 01251], lr: 0.000600, loss: 0.4486
2022-10-02 14:59:18 - train: epoch 0012, iter [00240, 01251], lr: 0.000600, loss: 0.4685
2022-10-02 14:59:38 - train: epoch 0012, iter [00250, 01251], lr: 0.000600, loss: 0.4250
2022-10-02 14:59:58 - train: epoch 0012, iter [00260, 01251], lr: 0.000600, loss: 0.4351
2022-10-02 15:00:18 - train: epoch 0012, iter [00270, 01251], lr: 0.000600, loss: 0.4232
2022-10-02 15:00:38 - train: epoch 0012, iter [00280, 01251], lr: 0.000600, loss: 0.4272
2022-10-02 15:00:58 - train: epoch 0012, iter [00290, 01251], lr: 0.000600, loss: 0.4369
2022-10-02 15:01:19 - train: epoch 0012, iter [00300, 01251], lr: 0.000600, loss: 0.4352
2022-10-02 15:01:38 - train: epoch 0012, iter [00310, 01251], lr: 0.000600, loss: 0.4291
2022-10-02 15:01:58 - train: epoch 0012, iter [00320, 01251], lr: 0.000600, loss: 0.4331
2022-10-02 15:02:19 - train: epoch 0012, iter [00330, 01251], lr: 0.000600, loss: 0.4331
2022-10-02 15:02:39 - train: epoch 0012, iter [00340, 01251], lr: 0.000600, loss: 0.4447
2022-10-02 15:02:59 - train: epoch 0012, iter [00350, 01251], lr: 0.000600, loss: 0.4411
2022-10-02 15:03:19 - train: epoch 0012, iter [00360, 01251], lr: 0.000600, loss: 0.4346
2022-10-02 15:03:39 - train: epoch 0012, iter [00370, 01251], lr: 0.000600, loss: 0.4346
2022-10-02 15:03:59 - train: epoch 0012, iter [00380, 01251], lr: 0.000600, loss: 0.4352
2022-10-02 15:04:19 - train: epoch 0012, iter [00390, 01251], lr: 0.000600, loss: 0.4355
2022-10-02 15:04:39 - train: epoch 0012, iter [00400, 01251], lr: 0.000600, loss: 0.4334
2022-10-02 15:04:59 - train: epoch 0012, iter [00410, 01251], lr: 0.000600, loss: 0.4363
2022-10-02 15:05:19 - train: epoch 0012, iter [00420, 01251], lr: 0.000600, loss: 0.4428
2022-10-02 15:05:39 - train: epoch 0012, iter [00430, 01251], lr: 0.000600, loss: 0.4372
2022-10-02 15:05:59 - train: epoch 0012, iter [00440, 01251], lr: 0.000600, loss: 0.4305
2022-10-02 15:06:20 - train: epoch 0012, iter [00450, 01251], lr: 0.000600, loss: 0.4322
2022-10-02 15:06:39 - train: epoch 0012, iter [00460, 01251], lr: 0.000600, loss: 0.4228
2022-10-02 15:07:00 - train: epoch 0012, iter [00470, 01251], lr: 0.000600, loss: 0.4347
2022-10-02 15:07:20 - train: epoch 0012, iter [00480, 01251], lr: 0.000600, loss: 0.4395
2022-10-02 15:07:40 - train: epoch 0012, iter [00490, 01251], lr: 0.000600, loss: 0.4334
2022-10-02 15:08:00 - train: epoch 0012, iter [00500, 01251], lr: 0.000600, loss: 0.4170
2022-10-02 15:08:20 - train: epoch 0012, iter [00510, 01251], lr: 0.000600, loss: 0.4268
2022-10-02 15:08:40 - train: epoch 0012, iter [00520, 01251], lr: 0.000600, loss: 0.4401
2022-10-02 15:09:00 - train: epoch 0012, iter [00530, 01251], lr: 0.000600, loss: 0.4247
2022-10-02 15:09:20 - train: epoch 0012, iter [00540, 01251], lr: 0.000600, loss: 0.4316
2022-10-02 15:09:40 - train: epoch 0012, iter [00550, 01251], lr: 0.000600, loss: 0.4455
2022-10-02 15:10:00 - train: epoch 0012, iter [00560, 01251], lr: 0.000600, loss: 0.4312
2022-10-02 15:10:20 - train: epoch 0012, iter [00570, 01251], lr: 0.000600, loss: 0.4399
2022-10-02 15:10:41 - train: epoch 0012, iter [00580, 01251], lr: 0.000600, loss: 0.4505
2022-10-02 15:11:01 - train: epoch 0012, iter [00590, 01251], lr: 0.000600, loss: 0.4636
2022-10-02 15:11:20 - train: epoch 0012, iter [00600, 01251], lr: 0.000600, loss: 0.4380
2022-10-02 15:11:41 - train: epoch 0012, iter [00610, 01251], lr: 0.000600, loss: 0.4159
2022-10-02 15:12:01 - train: epoch 0012, iter [00620, 01251], lr: 0.000600, loss: 0.4449
2022-10-02 15:12:21 - train: epoch 0012, iter [00630, 01251], lr: 0.000600, loss: 0.4161
2022-10-02 15:12:41 - train: epoch 0012, iter [00640, 01251], lr: 0.000600, loss: 0.4180
2022-10-02 15:13:02 - train: epoch 0012, iter [00650, 01251], lr: 0.000600, loss: 0.4430
2022-10-02 15:13:22 - train: epoch 0012, iter [00660, 01251], lr: 0.000600, loss: 0.4249
2022-10-02 15:13:42 - train: epoch 0012, iter [00670, 01251], lr: 0.000600, loss: 0.4460
2022-10-02 15:14:02 - train: epoch 0012, iter [00680, 01251], lr: 0.000600, loss: 0.4428
2022-10-02 15:14:22 - train: epoch 0012, iter [00690, 01251], lr: 0.000600, loss: 0.4413
2022-10-02 15:14:42 - train: epoch 0012, iter [00700, 01251], lr: 0.000600, loss: 0.4150
2022-10-02 15:15:02 - train: epoch 0012, iter [00710, 01251], lr: 0.000600, loss: 0.4242
2022-10-02 15:15:22 - train: epoch 0012, iter [00720, 01251], lr: 0.000600, loss: 0.4197
2022-10-02 15:15:43 - train: epoch 0012, iter [00730, 01251], lr: 0.000600, loss: 0.4301
2022-10-02 15:16:03 - train: epoch 0012, iter [00740, 01251], lr: 0.000600, loss: 0.4110
2022-10-02 15:16:23 - train: epoch 0012, iter [00750, 01251], lr: 0.000600, loss: 0.4175
2022-10-02 15:16:43 - train: epoch 0012, iter [00760, 01251], lr: 0.000600, loss: 0.4486
2022-10-02 15:17:03 - train: epoch 0012, iter [00770, 01251], lr: 0.000600, loss: 0.4258
2022-10-02 15:17:23 - train: epoch 0012, iter [00780, 01251], lr: 0.000600, loss: 0.4285
2022-10-02 15:17:43 - train: epoch 0012, iter [00790, 01251], lr: 0.000600, loss: 0.4301
2022-10-02 15:18:03 - train: epoch 0012, iter [00800, 01251], lr: 0.000600, loss: 0.4456
2022-10-02 15:18:23 - train: epoch 0012, iter [00810, 01251], lr: 0.000600, loss: 0.4552
2022-10-02 15:18:43 - train: epoch 0012, iter [00820, 01251], lr: 0.000599, loss: 0.4375
2022-10-02 15:19:03 - train: epoch 0012, iter [00830, 01251], lr: 0.000599, loss: 0.4403
2022-10-02 15:19:23 - train: epoch 0012, iter [00840, 01251], lr: 0.000599, loss: 0.4395
2022-10-02 15:19:43 - train: epoch 0012, iter [00850, 01251], lr: 0.000599, loss: 0.4158
2022-10-02 15:20:03 - train: epoch 0012, iter [00860, 01251], lr: 0.000599, loss: 0.4430
2022-10-02 15:20:23 - train: epoch 0012, iter [00870, 01251], lr: 0.000599, loss: 0.4391
2022-10-02 15:20:43 - train: epoch 0012, iter [00880, 01251], lr: 0.000599, loss: 0.4231
2022-10-02 15:21:03 - train: epoch 0012, iter [00890, 01251], lr: 0.000599, loss: 0.4540
2022-10-02 15:21:23 - train: epoch 0012, iter [00900, 01251], lr: 0.000599, loss: 0.4192
2022-10-02 15:21:43 - train: epoch 0012, iter [00910, 01251], lr: 0.000599, loss: 0.4349
2022-10-02 15:22:03 - train: epoch 0012, iter [00920, 01251], lr: 0.000599, loss: 0.4352
2022-10-02 15:22:23 - train: epoch 0012, iter [00930, 01251], lr: 0.000599, loss: 0.4317
2022-10-02 15:22:44 - train: epoch 0012, iter [00940, 01251], lr: 0.000599, loss: 0.4498
2022-10-02 15:23:04 - train: epoch 0012, iter [00950, 01251], lr: 0.000599, loss: 0.4325
2022-10-02 15:23:23 - train: epoch 0012, iter [00960, 01251], lr: 0.000599, loss: 0.4443
2022-10-02 15:23:44 - train: epoch 0012, iter [00970, 01251], lr: 0.000599, loss: 0.4331
2022-10-02 15:24:04 - train: epoch 0012, iter [00980, 01251], lr: 0.000599, loss: 0.4471
2022-10-02 15:24:24 - train: epoch 0012, iter [00990, 01251], lr: 0.000599, loss: 0.4549
2022-10-02 15:24:45 - train: epoch 0012, iter [01000, 01251], lr: 0.000599, loss: 0.4357
2022-10-02 15:25:05 - train: epoch 0012, iter [01010, 01251], lr: 0.000599, loss: 0.4461
2022-10-02 15:25:25 - train: epoch 0012, iter [01020, 01251], lr: 0.000599, loss: 0.4225
2022-10-02 15:25:45 - train: epoch 0012, iter [01030, 01251], lr: 0.000599, loss: 0.4519
2022-10-02 15:26:05 - train: epoch 0012, iter [01040, 01251], lr: 0.000599, loss: 0.4099
2022-10-02 15:26:26 - train: epoch 0012, iter [01050, 01251], lr: 0.000599, loss: 0.4338
2022-10-02 15:26:46 - train: epoch 0012, iter [01060, 01251], lr: 0.000599, loss: 0.4185
2022-10-02 15:27:06 - train: epoch 0012, iter [01070, 01251], lr: 0.000599, loss: 0.4377
2022-10-02 15:27:26 - train: epoch 0012, iter [01080, 01251], lr: 0.000599, loss: 0.4233
2022-10-02 15:27:46 - train: epoch 0012, iter [01090, 01251], lr: 0.000599, loss: 0.4336
2022-10-02 15:28:06 - train: epoch 0012, iter [01100, 01251], lr: 0.000599, loss: 0.4415
2022-10-02 15:28:26 - train: epoch 0012, iter [01110, 01251], lr: 0.000599, loss: 0.4468
2022-10-02 15:28:46 - train: epoch 0012, iter [01120, 01251], lr: 0.000599, loss: 0.4367
2022-10-02 15:29:06 - train: epoch 0012, iter [01130, 01251], lr: 0.000599, loss: 0.4365
2022-10-02 15:29:27 - train: epoch 0012, iter [01140, 01251], lr: 0.000599, loss: 0.4356
2022-10-02 15:29:47 - train: epoch 0012, iter [01150, 01251], lr: 0.000599, loss: 0.4286
2022-10-02 15:30:07 - train: epoch 0012, iter [01160, 01251], lr: 0.000599, loss: 0.4432
2022-10-02 15:30:26 - train: epoch 0012, iter [01170, 01251], lr: 0.000599, loss: 0.4423
2022-10-02 15:30:47 - train: epoch 0012, iter [01180, 01251], lr: 0.000599, loss: 0.4142
2022-10-02 15:31:07 - train: epoch 0012, iter [01190, 01251], lr: 0.000599, loss: 0.4228
2022-10-02 15:31:27 - train: epoch 0012, iter [01200, 01251], lr: 0.000599, loss: 0.4446
2022-10-02 15:31:47 - train: epoch 0012, iter [01210, 01251], lr: 0.000599, loss: 0.4374
2022-10-02 15:32:07 - train: epoch 0012, iter [01220, 01251], lr: 0.000599, loss: 0.4250
2022-10-02 15:32:28 - train: epoch 0012, iter [01230, 01251], lr: 0.000599, loss: 0.4445
2022-10-02 15:32:48 - train: epoch 0012, iter [01240, 01251], lr: 0.000599, loss: 0.4388
2022-10-02 15:33:07 - train: epoch 0012, iter [01250, 01251], lr: 0.000599, loss: 0.4336
2022-10-02 15:33:10 - train: epoch 012, train_loss: 0.4367
2022-10-02 15:33:15 - until epoch: 012, best_loss: 0.4367
2022-10-02 15:33:15 - epoch 013 lr: 0.000599
2022-10-02 15:33:42 - train: epoch 0013, iter [00010, 01251], lr: 0.000599, loss: 0.4350
2022-10-02 15:34:02 - train: epoch 0013, iter [00020, 01251], lr: 0.000599, loss: 0.4459
2022-10-02 15:34:22 - train: epoch 0013, iter [00030, 01251], lr: 0.000599, loss: 0.4457
2022-10-02 15:34:43 - train: epoch 0013, iter [00040, 01251], lr: 0.000599, loss: 0.4290
2022-10-02 15:35:03 - train: epoch 0013, iter [00050, 01251], lr: 0.000599, loss: 0.4548
2022-10-02 15:35:23 - train: epoch 0013, iter [00060, 01251], lr: 0.000599, loss: 0.4353
2022-10-02 15:35:44 - train: epoch 0013, iter [00070, 01251], lr: 0.000599, loss: 0.4341
2022-10-02 15:36:04 - train: epoch 0013, iter [00080, 01251], lr: 0.000599, loss: 0.4262
2022-10-02 15:36:24 - train: epoch 0013, iter [00090, 01251], lr: 0.000599, loss: 0.4642
2022-10-02 15:36:44 - train: epoch 0013, iter [00100, 01251], lr: 0.000599, loss: 0.4377
2022-10-02 15:37:04 - train: epoch 0013, iter [00110, 01251], lr: 0.000599, loss: 0.4330
2022-10-02 15:37:25 - train: epoch 0013, iter [00120, 01251], lr: 0.000599, loss: 0.4313
2022-10-02 15:37:45 - train: epoch 0013, iter [00130, 01251], lr: 0.000599, loss: 0.4191
2022-10-02 15:38:05 - train: epoch 0013, iter [00140, 01251], lr: 0.000599, loss: 0.4317
2022-10-02 15:38:25 - train: epoch 0013, iter [00150, 01251], lr: 0.000599, loss: 0.4387
2022-10-02 15:38:45 - train: epoch 0013, iter [00160, 01251], lr: 0.000599, loss: 0.4181
2022-10-02 15:39:05 - train: epoch 0013, iter [00170, 01251], lr: 0.000599, loss: 0.4262
2022-10-02 15:39:26 - train: epoch 0013, iter [00180, 01251], lr: 0.000599, loss: 0.4412
2022-10-02 15:39:46 - train: epoch 0013, iter [00190, 01251], lr: 0.000599, loss: 0.4399
2022-10-02 15:40:06 - train: epoch 0013, iter [00200, 01251], lr: 0.000599, loss: 0.4346
2022-10-02 15:40:27 - train: epoch 0013, iter [00210, 01251], lr: 0.000599, loss: 0.4299
2022-10-02 15:40:47 - train: epoch 0013, iter [00220, 01251], lr: 0.000599, loss: 0.4194
2022-10-02 15:41:07 - train: epoch 0013, iter [00230, 01251], lr: 0.000599, loss: 0.4419
2022-10-02 15:41:27 - train: epoch 0013, iter [00240, 01251], lr: 0.000599, loss: 0.4293
2022-10-02 15:41:48 - train: epoch 0013, iter [00250, 01251], lr: 0.000599, loss: 0.4420
2022-10-02 15:42:08 - train: epoch 0013, iter [00260, 01251], lr: 0.000599, loss: 0.4168
2022-10-02 15:42:28 - train: epoch 0013, iter [00270, 01251], lr: 0.000599, loss: 0.4383
2022-10-02 15:42:48 - train: epoch 0013, iter [00280, 01251], lr: 0.000599, loss: 0.4304
2022-10-02 15:43:08 - train: epoch 0013, iter [00290, 01251], lr: 0.000599, loss: 0.4448
2022-10-02 15:43:28 - train: epoch 0013, iter [00300, 01251], lr: 0.000599, loss: 0.4293
2022-10-02 15:43:49 - train: epoch 0013, iter [00310, 01251], lr: 0.000599, loss: 0.4484
2022-10-02 15:44:09 - train: epoch 0013, iter [00320, 01251], lr: 0.000599, loss: 0.4452
2022-10-02 15:44:29 - train: epoch 0013, iter [00330, 01251], lr: 0.000599, loss: 0.4399
2022-10-02 15:44:49 - train: epoch 0013, iter [00340, 01251], lr: 0.000599, loss: 0.4310
2022-10-02 15:45:09 - train: epoch 0013, iter [00350, 01251], lr: 0.000599, loss: 0.4369
2022-10-02 15:45:29 - train: epoch 0013, iter [00360, 01251], lr: 0.000599, loss: 0.4341
2022-10-02 15:45:49 - train: epoch 0013, iter [00370, 01251], lr: 0.000599, loss: 0.4344
2022-10-02 15:46:09 - train: epoch 0013, iter [00380, 01251], lr: 0.000599, loss: 0.4227
2022-10-02 15:46:29 - train: epoch 0013, iter [00390, 01251], lr: 0.000599, loss: 0.4337
2022-10-02 15:46:49 - train: epoch 0013, iter [00400, 01251], lr: 0.000599, loss: 0.4289
2022-10-02 15:47:10 - train: epoch 0013, iter [00410, 01251], lr: 0.000599, loss: 0.4327
2022-10-02 15:47:30 - train: epoch 0013, iter [00420, 01251], lr: 0.000599, loss: 0.4408
2022-10-02 15:47:50 - train: epoch 0013, iter [00430, 01251], lr: 0.000599, loss: 0.4184
2022-10-02 15:48:10 - train: epoch 0013, iter [00440, 01251], lr: 0.000599, loss: 0.4476
2022-10-02 15:48:30 - train: epoch 0013, iter [00450, 01251], lr: 0.000599, loss: 0.4436
2022-10-02 15:48:50 - train: epoch 0013, iter [00460, 01251], lr: 0.000599, loss: 0.4313
2022-10-02 15:49:11 - train: epoch 0013, iter [00470, 01251], lr: 0.000599, loss: 0.4161
2022-10-02 15:49:31 - train: epoch 0013, iter [00480, 01251], lr: 0.000599, loss: 0.4346
2022-10-02 15:49:51 - train: epoch 0013, iter [00490, 01251], lr: 0.000599, loss: 0.4464
2022-10-02 15:50:11 - train: epoch 0013, iter [00500, 01251], lr: 0.000599, loss: 0.4400
2022-10-02 15:50:31 - train: epoch 0013, iter [00510, 01251], lr: 0.000599, loss: 0.4565
2022-10-02 15:50:51 - train: epoch 0013, iter [00520, 01251], lr: 0.000599, loss: 0.4343
2022-10-02 15:51:11 - train: epoch 0013, iter [00530, 01251], lr: 0.000599, loss: 0.4302
2022-10-02 15:51:31 - train: epoch 0013, iter [00540, 01251], lr: 0.000599, loss: 0.4232
2022-10-02 15:51:51 - train: epoch 0013, iter [00550, 01251], lr: 0.000599, loss: 0.4455
2022-10-02 15:52:12 - train: epoch 0013, iter [00560, 01251], lr: 0.000599, loss: 0.4252
2022-10-02 15:52:32 - train: epoch 0013, iter [00570, 01251], lr: 0.000599, loss: 0.4427
2022-10-02 15:52:52 - train: epoch 0013, iter [00580, 01251], lr: 0.000599, loss: 0.4183
2022-10-02 15:53:12 - train: epoch 0013, iter [00590, 01251], lr: 0.000599, loss: 0.4381
2022-10-02 15:53:32 - train: epoch 0013, iter [00600, 01251], lr: 0.000599, loss: 0.4581
2022-10-02 15:53:52 - train: epoch 0013, iter [00610, 01251], lr: 0.000599, loss: 0.4318
2022-10-02 15:54:13 - train: epoch 0013, iter [00620, 01251], lr: 0.000599, loss: 0.4295
2022-10-02 15:54:33 - train: epoch 0013, iter [00630, 01251], lr: 0.000599, loss: 0.4362
2022-10-02 15:54:52 - train: epoch 0013, iter [00640, 01251], lr: 0.000599, loss: 0.4511
2022-10-02 15:55:12 - train: epoch 0013, iter [00650, 01251], lr: 0.000599, loss: 0.4390
2022-10-02 15:55:32 - train: epoch 0013, iter [00660, 01251], lr: 0.000599, loss: 0.4396
2022-10-02 15:55:52 - train: epoch 0013, iter [00670, 01251], lr: 0.000599, loss: 0.4410
2022-10-02 15:56:13 - train: epoch 0013, iter [00680, 01251], lr: 0.000599, loss: 0.4494
2022-10-02 15:56:33 - train: epoch 0013, iter [00690, 01251], lr: 0.000599, loss: 0.4363
2022-10-02 15:56:53 - train: epoch 0013, iter [00700, 01251], lr: 0.000599, loss: 0.4381
2022-10-02 15:57:13 - train: epoch 0013, iter [00710, 01251], lr: 0.000599, loss: 0.4152
2022-10-02 15:57:33 - train: epoch 0013, iter [00720, 01251], lr: 0.000599, loss: 0.4302
2022-10-02 15:57:53 - train: epoch 0013, iter [00730, 01251], lr: 0.000599, loss: 0.4424
2022-10-02 15:58:13 - train: epoch 0013, iter [00740, 01251], lr: 0.000599, loss: 0.4475
2022-10-02 15:58:33 - train: epoch 0013, iter [00750, 01251], lr: 0.000599, loss: 0.4177
2022-10-02 15:58:53 - train: epoch 0013, iter [00760, 01251], lr: 0.000599, loss: 0.4296
2022-10-02 15:59:13 - train: epoch 0013, iter [00770, 01251], lr: 0.000599, loss: 0.4314
2022-10-02 15:59:33 - train: epoch 0013, iter [00780, 01251], lr: 0.000599, loss: 0.4334
2022-10-02 15:59:53 - train: epoch 0013, iter [00790, 01251], lr: 0.000599, loss: 0.4249
2022-10-02 16:00:13 - train: epoch 0013, iter [00800, 01251], lr: 0.000599, loss: 0.4424
2022-10-02 16:00:33 - train: epoch 0013, iter [00810, 01251], lr: 0.000599, loss: 0.4295
2022-10-02 16:00:53 - train: epoch 0013, iter [00820, 01251], lr: 0.000599, loss: 0.4442
2022-10-02 16:01:13 - train: epoch 0013, iter [00830, 01251], lr: 0.000599, loss: 0.4354
2022-10-02 16:01:33 - train: epoch 0013, iter [00840, 01251], lr: 0.000599, loss: 0.4273
2022-10-02 16:01:53 - train: epoch 0013, iter [00850, 01251], lr: 0.000599, loss: 0.4243
2022-10-02 16:02:13 - train: epoch 0013, iter [00860, 01251], lr: 0.000599, loss: 0.4240
2022-10-02 16:02:33 - train: epoch 0013, iter [00870, 01251], lr: 0.000599, loss: 0.4223
2022-10-02 16:02:53 - train: epoch 0013, iter [00880, 01251], lr: 0.000599, loss: 0.4355
2022-10-02 16:03:14 - train: epoch 0013, iter [00890, 01251], lr: 0.000599, loss: 0.4512
2022-10-02 16:03:33 - train: epoch 0013, iter [00900, 01251], lr: 0.000599, loss: 0.4425
2022-10-02 16:03:53 - train: epoch 0013, iter [00910, 01251], lr: 0.000599, loss: 0.4345
2022-10-02 16:04:13 - train: epoch 0013, iter [00920, 01251], lr: 0.000599, loss: 0.4312
2022-10-02 16:04:33 - train: epoch 0013, iter [00930, 01251], lr: 0.000599, loss: 0.4308
2022-10-02 16:04:53 - train: epoch 0013, iter [00940, 01251], lr: 0.000599, loss: 0.4421
2022-10-02 16:05:13 - train: epoch 0013, iter [00950, 01251], lr: 0.000599, loss: 0.4242
2022-10-02 16:05:32 - train: epoch 0013, iter [00960, 01251], lr: 0.000599, loss: 0.4138
2022-10-02 16:05:52 - train: epoch 0013, iter [00970, 01251], lr: 0.000599, loss: 0.4233
2022-10-02 16:06:12 - train: epoch 0013, iter [00980, 01251], lr: 0.000599, loss: 0.4433
2022-10-02 16:06:33 - train: epoch 0013, iter [00990, 01251], lr: 0.000599, loss: 0.4374
2022-10-02 16:06:53 - train: epoch 0013, iter [01000, 01251], lr: 0.000599, loss: 0.4366
2022-10-02 16:07:13 - train: epoch 0013, iter [01010, 01251], lr: 0.000599, loss: 0.4247
2022-10-02 16:07:33 - train: epoch 0013, iter [01020, 01251], lr: 0.000599, loss: 0.4146
2022-10-02 16:07:53 - train: epoch 0013, iter [01030, 01251], lr: 0.000599, loss: 0.4499
2022-10-02 16:08:13 - train: epoch 0013, iter [01040, 01251], lr: 0.000599, loss: 0.4338
2022-10-02 16:08:33 - train: epoch 0013, iter [01050, 01251], lr: 0.000599, loss: 0.4217
2022-10-02 16:08:53 - train: epoch 0013, iter [01060, 01251], lr: 0.000599, loss: 0.4278
2022-10-02 16:09:13 - train: epoch 0013, iter [01070, 01251], lr: 0.000599, loss: 0.4281
2022-10-02 16:09:33 - train: epoch 0013, iter [01080, 01251], lr: 0.000599, loss: 0.4303
2022-10-02 16:09:53 - train: epoch 0013, iter [01090, 01251], lr: 0.000598, loss: 0.4271
2022-10-02 16:10:14 - train: epoch 0013, iter [01100, 01251], lr: 0.000598, loss: 0.4630
2022-10-02 16:10:34 - train: epoch 0013, iter [01110, 01251], lr: 0.000598, loss: 0.4573
2022-10-02 16:10:54 - train: epoch 0013, iter [01120, 01251], lr: 0.000598, loss: 0.4244
2022-10-02 16:11:14 - train: epoch 0013, iter [01130, 01251], lr: 0.000598, loss: 0.4320
2022-10-02 16:11:34 - train: epoch 0013, iter [01140, 01251], lr: 0.000598, loss: 0.4352
2022-10-02 16:11:55 - train: epoch 0013, iter [01150, 01251], lr: 0.000598, loss: 0.4318
2022-10-02 16:12:15 - train: epoch 0013, iter [01160, 01251], lr: 0.000598, loss: 0.4339
2022-10-02 16:12:34 - train: epoch 0013, iter [01170, 01251], lr: 0.000598, loss: 0.4471
2022-10-02 16:12:55 - train: epoch 0013, iter [01180, 01251], lr: 0.000598, loss: 0.4348
2022-10-02 16:13:15 - train: epoch 0013, iter [01190, 01251], lr: 0.000598, loss: 0.4305
2022-10-02 16:13:35 - train: epoch 0013, iter [01200, 01251], lr: 0.000598, loss: 0.4351
2022-10-02 16:13:54 - train: epoch 0013, iter [01210, 01251], lr: 0.000598, loss: 0.4064
2022-10-02 16:14:14 - train: epoch 0013, iter [01220, 01251], lr: 0.000598, loss: 0.4345
2022-10-02 16:14:34 - train: epoch 0013, iter [01230, 01251], lr: 0.000598, loss: 0.4076
2022-10-02 16:14:54 - train: epoch 0013, iter [01240, 01251], lr: 0.000598, loss: 0.4442
2022-10-02 16:15:13 - train: epoch 0013, iter [01250, 01251], lr: 0.000598, loss: 0.4342
2022-10-02 16:15:17 - train: epoch 013, train_loss: 0.4344
2022-10-02 16:15:21 - until epoch: 013, best_loss: 0.4344
2022-10-02 16:15:21 - epoch 014 lr: 0.000598
2022-10-02 16:15:48 - train: epoch 0014, iter [00010, 01251], lr: 0.000598, loss: 0.4255
2022-10-02 16:16:08 - train: epoch 0014, iter [00020, 01251], lr: 0.000598, loss: 0.4372
2022-10-02 16:16:28 - train: epoch 0014, iter [00030, 01251], lr: 0.000598, loss: 0.4434
2022-10-02 16:16:49 - train: epoch 0014, iter [00040, 01251], lr: 0.000598, loss: 0.4385
2022-10-02 16:17:09 - train: epoch 0014, iter [00050, 01251], lr: 0.000598, loss: 0.4435
2022-10-02 16:17:29 - train: epoch 0014, iter [00060, 01251], lr: 0.000598, loss: 0.4312
2022-10-02 16:17:49 - train: epoch 0014, iter [00070, 01251], lr: 0.000598, loss: 0.4445
2022-10-02 16:18:09 - train: epoch 0014, iter [00080, 01251], lr: 0.000598, loss: 0.4199
2022-10-02 16:18:29 - train: epoch 0014, iter [00090, 01251], lr: 0.000598, loss: 0.4322
2022-10-02 16:18:49 - train: epoch 0014, iter [00100, 01251], lr: 0.000598, loss: 0.4207
2022-10-02 16:19:10 - train: epoch 0014, iter [00110, 01251], lr: 0.000598, loss: 0.4193
2022-10-02 16:19:30 - train: epoch 0014, iter [00120, 01251], lr: 0.000598, loss: 0.4454
2022-10-02 16:19:50 - train: epoch 0014, iter [00130, 01251], lr: 0.000598, loss: 0.4389
2022-10-02 16:20:10 - train: epoch 0014, iter [00140, 01251], lr: 0.000598, loss: 0.4202
2022-10-02 16:20:31 - train: epoch 0014, iter [00150, 01251], lr: 0.000598, loss: 0.4259
2022-10-02 16:20:51 - train: epoch 0014, iter [00160, 01251], lr: 0.000598, loss: 0.4246
2022-10-02 16:21:11 - train: epoch 0014, iter [00170, 01251], lr: 0.000598, loss: 0.4493
2022-10-02 16:21:31 - train: epoch 0014, iter [00180, 01251], lr: 0.000598, loss: 0.4413
2022-10-02 16:21:51 - train: epoch 0014, iter [00190, 01251], lr: 0.000598, loss: 0.4306
2022-10-02 16:22:11 - train: epoch 0014, iter [00200, 01251], lr: 0.000598, loss: 0.4263
2022-10-02 16:22:31 - train: epoch 0014, iter [00210, 01251], lr: 0.000598, loss: 0.4297
2022-10-02 16:22:51 - train: epoch 0014, iter [00220, 01251], lr: 0.000598, loss: 0.4353
2022-10-02 16:23:11 - train: epoch 0014, iter [00230, 01251], lr: 0.000598, loss: 0.4365
2022-10-02 16:23:31 - train: epoch 0014, iter [00240, 01251], lr: 0.000598, loss: 0.4463
2022-10-02 16:23:52 - train: epoch 0014, iter [00250, 01251], lr: 0.000598, loss: 0.4429
2022-10-02 16:24:12 - train: epoch 0014, iter [00260, 01251], lr: 0.000598, loss: 0.4384
2022-10-02 16:24:32 - train: epoch 0014, iter [00270, 01251], lr: 0.000598, loss: 0.4349
2022-10-02 16:24:52 - train: epoch 0014, iter [00280, 01251], lr: 0.000598, loss: 0.4364
2022-10-02 16:25:12 - train: epoch 0014, iter [00290, 01251], lr: 0.000598, loss: 0.4368
2022-10-02 16:25:32 - train: epoch 0014, iter [00300, 01251], lr: 0.000598, loss: 0.4249
2022-10-02 16:25:53 - train: epoch 0014, iter [00310, 01251], lr: 0.000598, loss: 0.4259
2022-10-02 16:26:13 - train: epoch 0014, iter [00320, 01251], lr: 0.000598, loss: 0.4280
2022-10-02 16:26:33 - train: epoch 0014, iter [00330, 01251], lr: 0.000598, loss: 0.4175
2022-10-02 16:26:53 - train: epoch 0014, iter [00340, 01251], lr: 0.000598, loss: 0.4525
2022-10-02 16:27:13 - train: epoch 0014, iter [00350, 01251], lr: 0.000598, loss: 0.4322
2022-10-02 16:27:34 - train: epoch 0014, iter [00360, 01251], lr: 0.000598, loss: 0.4415
2022-10-02 16:27:54 - train: epoch 0014, iter [00370, 01251], lr: 0.000598, loss: 0.4209
2022-10-02 16:28:14 - train: epoch 0014, iter [00380, 01251], lr: 0.000598, loss: 0.4333
2022-10-02 16:28:35 - train: epoch 0014, iter [00390, 01251], lr: 0.000598, loss: 0.4576
2022-10-02 16:28:55 - train: epoch 0014, iter [00400, 01251], lr: 0.000598, loss: 0.4208
2022-10-02 16:29:15 - train: epoch 0014, iter [00410, 01251], lr: 0.000598, loss: 0.4267
2022-10-02 16:29:35 - train: epoch 0014, iter [00420, 01251], lr: 0.000598, loss: 0.4372
2022-10-02 16:29:55 - train: epoch 0014, iter [00430, 01251], lr: 0.000598, loss: 0.4247
2022-10-02 16:30:15 - train: epoch 0014, iter [00440, 01251], lr: 0.000598, loss: 0.4408
2022-10-02 16:30:35 - train: epoch 0014, iter [00450, 01251], lr: 0.000598, loss: 0.4288
2022-10-02 16:30:56 - train: epoch 0014, iter [00460, 01251], lr: 0.000598, loss: 0.4262
2022-10-02 16:31:16 - train: epoch 0014, iter [00470, 01251], lr: 0.000598, loss: 0.4429
2022-10-02 16:31:36 - train: epoch 0014, iter [00480, 01251], lr: 0.000598, loss: 0.4268
2022-10-02 16:31:56 - train: epoch 0014, iter [00490, 01251], lr: 0.000598, loss: 0.4399
2022-10-02 16:32:17 - train: epoch 0014, iter [00500, 01251], lr: 0.000598, loss: 0.4322
2022-10-02 16:32:37 - train: epoch 0014, iter [00510, 01251], lr: 0.000598, loss: 0.4431
2022-10-02 16:32:57 - train: epoch 0014, iter [00520, 01251], lr: 0.000598, loss: 0.4389
2022-10-02 16:33:17 - train: epoch 0014, iter [00530, 01251], lr: 0.000598, loss: 0.4387
2022-10-02 16:33:37 - train: epoch 0014, iter [00540, 01251], lr: 0.000598, loss: 0.4292
2022-10-02 16:33:57 - train: epoch 0014, iter [00550, 01251], lr: 0.000598, loss: 0.4426
2022-10-02 16:34:17 - train: epoch 0014, iter [00560, 01251], lr: 0.000598, loss: 0.4257
2022-10-02 16:34:37 - train: epoch 0014, iter [00570, 01251], lr: 0.000598, loss: 0.4245
2022-10-02 16:34:57 - train: epoch 0014, iter [00580, 01251], lr: 0.000598, loss: 0.4362
2022-10-02 16:35:17 - train: epoch 0014, iter [00590, 01251], lr: 0.000598, loss: 0.4487
2022-10-02 16:35:37 - train: epoch 0014, iter [00600, 01251], lr: 0.000598, loss: 0.4331
2022-10-02 16:35:58 - train: epoch 0014, iter [00610, 01251], lr: 0.000598, loss: 0.4095
2022-10-02 16:36:18 - train: epoch 0014, iter [00620, 01251], lr: 0.000598, loss: 0.4408
2022-10-02 16:36:38 - train: epoch 0014, iter [00630, 01251], lr: 0.000598, loss: 0.4284
2022-10-02 16:36:58 - train: epoch 0014, iter [00640, 01251], lr: 0.000598, loss: 0.4430
2022-10-02 16:37:19 - train: epoch 0014, iter [00650, 01251], lr: 0.000598, loss: 0.4266
2022-10-02 16:37:39 - train: epoch 0014, iter [00660, 01251], lr: 0.000598, loss: 0.4217
2022-10-02 16:37:59 - train: epoch 0014, iter [00670, 01251], lr: 0.000598, loss: 0.4311
2022-10-02 16:38:19 - train: epoch 0014, iter [00680, 01251], lr: 0.000598, loss: 0.4416
2022-10-02 16:38:39 - train: epoch 0014, iter [00690, 01251], lr: 0.000598, loss: 0.4243
2022-10-02 16:38:59 - train: epoch 0014, iter [00700, 01251], lr: 0.000598, loss: 0.4132
2022-10-02 16:39:20 - train: epoch 0014, iter [00710, 01251], lr: 0.000598, loss: 0.4310
2022-10-02 16:39:40 - train: epoch 0014, iter [00720, 01251], lr: 0.000598, loss: 0.4383
2022-10-02 16:40:00 - train: epoch 0014, iter [00730, 01251], lr: 0.000598, loss: 0.4243
2022-10-02 16:40:20 - train: epoch 0014, iter [00740, 01251], lr: 0.000598, loss: 0.4284
2022-10-02 16:40:40 - train: epoch 0014, iter [00750, 01251], lr: 0.000598, loss: 0.4540
2022-10-02 16:41:01 - train: epoch 0014, iter [00760, 01251], lr: 0.000598, loss: 0.4181
2022-10-02 16:41:21 - train: epoch 0014, iter [00770, 01251], lr: 0.000598, loss: 0.4444
2022-10-02 16:41:41 - train: epoch 0014, iter [00780, 01251], lr: 0.000598, loss: 0.4276
2022-10-02 16:42:01 - train: epoch 0014, iter [00790, 01251], lr: 0.000598, loss: 0.4277
2022-10-02 16:42:21 - train: epoch 0014, iter [00800, 01251], lr: 0.000598, loss: 0.4299
2022-10-02 16:42:42 - train: epoch 0014, iter [00810, 01251], lr: 0.000598, loss: 0.4465
2022-10-02 16:43:02 - train: epoch 0014, iter [00820, 01251], lr: 0.000598, loss: 0.4399
2022-10-02 16:43:22 - train: epoch 0014, iter [00830, 01251], lr: 0.000598, loss: 0.4277
2022-10-02 16:43:43 - train: epoch 0014, iter [00840, 01251], lr: 0.000598, loss: 0.4253
2022-10-02 16:44:03 - train: epoch 0014, iter [00850, 01251], lr: 0.000598, loss: 0.4396
2022-10-02 16:44:23 - train: epoch 0014, iter [00860, 01251], lr: 0.000598, loss: 0.4495
2022-10-02 16:44:43 - train: epoch 0014, iter [00870, 01251], lr: 0.000598, loss: 0.3998
2022-10-02 16:45:03 - train: epoch 0014, iter [00880, 01251], lr: 0.000597, loss: 0.4356
2022-10-02 16:45:23 - train: epoch 0014, iter [00890, 01251], lr: 0.000597, loss: 0.4370
2022-10-02 16:45:44 - train: epoch 0014, iter [00900, 01251], lr: 0.000597, loss: 0.4520
2022-10-02 16:46:04 - train: epoch 0014, iter [00910, 01251], lr: 0.000597, loss: 0.4295
2022-10-02 16:46:24 - train: epoch 0014, iter [00920, 01251], lr: 0.000597, loss: 0.4414
2022-10-02 16:46:44 - train: epoch 0014, iter [00930, 01251], lr: 0.000597, loss: 0.4294
2022-10-02 16:47:04 - train: epoch 0014, iter [00940, 01251], lr: 0.000597, loss: 0.4313
2022-10-02 16:47:24 - train: epoch 0014, iter [00950, 01251], lr: 0.000597, loss: 0.4250
2022-10-02 16:47:45 - train: epoch 0014, iter [00960, 01251], lr: 0.000597, loss: 0.4360
2022-10-02 16:48:05 - train: epoch 0014, iter [00970, 01251], lr: 0.000597, loss: 0.4381
2022-10-02 16:48:25 - train: epoch 0014, iter [00980, 01251], lr: 0.000597, loss: 0.4416
2022-10-02 16:48:45 - train: epoch 0014, iter [00990, 01251], lr: 0.000597, loss: 0.4279
2022-10-02 16:49:06 - train: epoch 0014, iter [01000, 01251], lr: 0.000597, loss: 0.4375
2022-10-02 16:49:25 - train: epoch 0014, iter [01010, 01251], lr: 0.000597, loss: 0.4323
2022-10-02 16:49:46 - train: epoch 0014, iter [01020, 01251], lr: 0.000597, loss: 0.4188
2022-10-02 16:50:06 - train: epoch 0014, iter [01030, 01251], lr: 0.000597, loss: 0.4239
2022-10-02 16:50:26 - train: epoch 0014, iter [01040, 01251], lr: 0.000597, loss: 0.4353
2022-10-02 16:50:47 - train: epoch 0014, iter [01050, 01251], lr: 0.000597, loss: 0.4187
2022-10-02 16:51:07 - train: epoch 0014, iter [01060, 01251], lr: 0.000597, loss: 0.4378
2022-10-02 16:51:27 - train: epoch 0014, iter [01070, 01251], lr: 0.000597, loss: 0.4248
2022-10-02 16:51:48 - train: epoch 0014, iter [01080, 01251], lr: 0.000597, loss: 0.4641
2022-10-02 16:52:08 - train: epoch 0014, iter [01090, 01251], lr: 0.000597, loss: 0.4211
2022-10-02 16:52:28 - train: epoch 0014, iter [01100, 01251], lr: 0.000597, loss: 0.4236
2022-10-02 16:52:48 - train: epoch 0014, iter [01110, 01251], lr: 0.000597, loss: 0.4553
2022-10-02 16:53:09 - train: epoch 0014, iter [01120, 01251], lr: 0.000597, loss: 0.4156
2022-10-02 16:53:29 - train: epoch 0014, iter [01130, 01251], lr: 0.000597, loss: 0.4456
2022-10-02 16:53:49 - train: epoch 0014, iter [01140, 01251], lr: 0.000597, loss: 0.4096
2022-10-02 16:54:10 - train: epoch 0014, iter [01150, 01251], lr: 0.000597, loss: 0.4405
2022-10-02 16:54:30 - train: epoch 0014, iter [01160, 01251], lr: 0.000597, loss: 0.4225
2022-10-02 16:54:50 - train: epoch 0014, iter [01170, 01251], lr: 0.000597, loss: 0.4343
2022-10-02 16:55:11 - train: epoch 0014, iter [01180, 01251], lr: 0.000597, loss: 0.4353
2022-10-02 16:55:31 - train: epoch 0014, iter [01190, 01251], lr: 0.000597, loss: 0.4239
2022-10-02 16:55:51 - train: epoch 0014, iter [01200, 01251], lr: 0.000597, loss: 0.4194
2022-10-02 16:56:12 - train: epoch 0014, iter [01210, 01251], lr: 0.000597, loss: 0.4208
2022-10-02 16:56:32 - train: epoch 0014, iter [01220, 01251], lr: 0.000597, loss: 0.4394
2022-10-02 16:56:52 - train: epoch 0014, iter [01230, 01251], lr: 0.000597, loss: 0.4222
2022-10-02 16:57:12 - train: epoch 0014, iter [01240, 01251], lr: 0.000597, loss: 0.4188
2022-10-02 16:57:31 - train: epoch 0014, iter [01250, 01251], lr: 0.000597, loss: 0.4302
2022-10-02 16:57:34 - train: epoch 014, train_loss: 0.4325
2022-10-02 16:57:38 - until epoch: 014, best_loss: 0.4325
2022-10-02 16:57:38 - epoch 015 lr: 0.000597
2022-10-02 16:58:05 - train: epoch 0015, iter [00010, 01251], lr: 0.000597, loss: 0.4204
2022-10-02 16:58:25 - train: epoch 0015, iter [00020, 01251], lr: 0.000597, loss: 0.4320
2022-10-02 16:58:45 - train: epoch 0015, iter [00030, 01251], lr: 0.000597, loss: 0.4195
2022-10-02 16:59:05 - train: epoch 0015, iter [00040, 01251], lr: 0.000597, loss: 0.4222
2022-10-02 16:59:26 - train: epoch 0015, iter [00050, 01251], lr: 0.000597, loss: 0.4220
2022-10-02 16:59:45 - train: epoch 0015, iter [00060, 01251], lr: 0.000597, loss: 0.4323
2022-10-02 17:00:05 - train: epoch 0015, iter [00070, 01251], lr: 0.000597, loss: 0.4492
2022-10-02 17:00:26 - train: epoch 0015, iter [00080, 01251], lr: 0.000597, loss: 0.4287
2022-10-02 17:00:45 - train: epoch 0015, iter [00090, 01251], lr: 0.000597, loss: 0.4205
2022-10-02 17:01:05 - train: epoch 0015, iter [00100, 01251], lr: 0.000597, loss: 0.4430
2022-10-02 17:01:26 - train: epoch 0015, iter [00110, 01251], lr: 0.000597, loss: 0.4508
2022-10-02 17:01:46 - train: epoch 0015, iter [00120, 01251], lr: 0.000597, loss: 0.4350
2022-10-02 17:02:06 - train: epoch 0015, iter [00130, 01251], lr: 0.000597, loss: 0.4295
2022-10-02 17:02:26 - train: epoch 0015, iter [00140, 01251], lr: 0.000597, loss: 0.4403
2022-10-02 17:02:46 - train: epoch 0015, iter [00150, 01251], lr: 0.000597, loss: 0.4393
2022-10-02 17:03:06 - train: epoch 0015, iter [00160, 01251], lr: 0.000597, loss: 0.4230
2022-10-02 17:03:26 - train: epoch 0015, iter [00170, 01251], lr: 0.000597, loss: 0.4496
2022-10-02 17:03:46 - train: epoch 0015, iter [00180, 01251], lr: 0.000597, loss: 0.4580
2022-10-02 17:04:05 - train: epoch 0015, iter [00190, 01251], lr: 0.000597, loss: 0.4359
2022-10-02 17:04:25 - train: epoch 0015, iter [00200, 01251], lr: 0.000597, loss: 0.4344
2022-10-02 17:04:45 - train: epoch 0015, iter [00210, 01251], lr: 0.000597, loss: 0.4380
2022-10-02 17:05:06 - train: epoch 0015, iter [00220, 01251], lr: 0.000597, loss: 0.4286
2022-10-02 17:05:26 - train: epoch 0015, iter [00230, 01251], lr: 0.000597, loss: 0.4222
2022-10-02 17:05:46 - train: epoch 0015, iter [00240, 01251], lr: 0.000597, loss: 0.4464
2022-10-02 17:06:06 - train: epoch 0015, iter [00250, 01251], lr: 0.000597, loss: 0.4368
2022-10-02 17:06:25 - train: epoch 0015, iter [00260, 01251], lr: 0.000597, loss: 0.4359
2022-10-02 17:06:46 - train: epoch 0015, iter [00270, 01251], lr: 0.000597, loss: 0.4406
2022-10-02 17:07:06 - train: epoch 0015, iter [00280, 01251], lr: 0.000597, loss: 0.4458
2022-10-02 17:07:25 - train: epoch 0015, iter [00290, 01251], lr: 0.000597, loss: 0.4222
2022-10-02 17:07:45 - train: epoch 0015, iter [00300, 01251], lr: 0.000597, loss: 0.4390
2022-10-02 17:08:05 - train: epoch 0015, iter [00310, 01251], lr: 0.000597, loss: 0.4260
2022-10-02 17:08:25 - train: epoch 0015, iter [00320, 01251], lr: 0.000597, loss: 0.4210
2022-10-02 17:08:45 - train: epoch 0015, iter [00330, 01251], lr: 0.000597, loss: 0.4403
2022-10-02 17:09:05 - train: epoch 0015, iter [00340, 01251], lr: 0.000597, loss: 0.4248
2022-10-02 17:09:25 - train: epoch 0015, iter [00350, 01251], lr: 0.000597, loss: 0.4157
2022-10-02 17:09:45 - train: epoch 0015, iter [00360, 01251], lr: 0.000597, loss: 0.4371
2022-10-02 17:10:05 - train: epoch 0015, iter [00370, 01251], lr: 0.000597, loss: 0.4439
2022-10-02 17:10:25 - train: epoch 0015, iter [00380, 01251], lr: 0.000597, loss: 0.4282
2022-10-02 17:10:44 - train: epoch 0015, iter [00390, 01251], lr: 0.000597, loss: 0.4366
2022-10-02 17:11:04 - train: epoch 0015, iter [00400, 01251], lr: 0.000597, loss: 0.4549
2022-10-02 17:11:24 - train: epoch 0015, iter [00410, 01251], lr: 0.000597, loss: 0.4364
2022-10-02 17:11:44 - train: epoch 0015, iter [00420, 01251], lr: 0.000597, loss: 0.4483
2022-10-02 17:12:04 - train: epoch 0015, iter [00430, 01251], lr: 0.000597, loss: 0.4408
2022-10-02 17:12:24 - train: epoch 0015, iter [00440, 01251], lr: 0.000597, loss: 0.4384
2022-10-02 17:12:44 - train: epoch 0015, iter [00450, 01251], lr: 0.000597, loss: 0.4246
2022-10-02 17:13:04 - train: epoch 0015, iter [00460, 01251], lr: 0.000597, loss: 0.4177
2022-10-02 17:13:24 - train: epoch 0015, iter [00470, 01251], lr: 0.000597, loss: 0.4424
2022-10-02 17:13:44 - train: epoch 0015, iter [00480, 01251], lr: 0.000596, loss: 0.4161
2022-10-02 17:14:04 - train: epoch 0015, iter [00490, 01251], lr: 0.000596, loss: 0.4150
2022-10-02 17:14:24 - train: epoch 0015, iter [00500, 01251], lr: 0.000596, loss: 0.4238
2022-10-02 17:14:44 - train: epoch 0015, iter [00510, 01251], lr: 0.000596, loss: 0.4296
2022-10-02 17:15:04 - train: epoch 0015, iter [00520, 01251], lr: 0.000596, loss: 0.4401
2022-10-02 17:15:24 - train: epoch 0015, iter [00530, 01251], lr: 0.000596, loss: 0.4319
2022-10-02 17:15:44 - train: epoch 0015, iter [00540, 01251], lr: 0.000596, loss: 0.4239
2022-10-02 17:16:04 - train: epoch 0015, iter [00550, 01251], lr: 0.000596, loss: 0.4284
2022-10-02 17:16:24 - train: epoch 0015, iter [00560, 01251], lr: 0.000596, loss: 0.4104
2022-10-02 17:16:44 - train: epoch 0015, iter [00570, 01251], lr: 0.000596, loss: 0.4176
2022-10-02 17:17:04 - train: epoch 0015, iter [00580, 01251], lr: 0.000596, loss: 0.4352
2022-10-02 17:17:24 - train: epoch 0015, iter [00590, 01251], lr: 0.000596, loss: 0.4202
2022-10-02 17:17:44 - train: epoch 0015, iter [00600, 01251], lr: 0.000596, loss: 0.4255
2022-10-02 17:18:03 - train: epoch 0015, iter [00610, 01251], lr: 0.000596, loss: 0.4288
2022-10-02 17:18:24 - train: epoch 0015, iter [00620, 01251], lr: 0.000596, loss: 0.4427
2022-10-02 17:18:44 - train: epoch 0015, iter [00630, 01251], lr: 0.000596, loss: 0.4136
2022-10-02 17:19:04 - train: epoch 0015, iter [00640, 01251], lr: 0.000596, loss: 0.4404
2022-10-02 17:19:23 - train: epoch 0015, iter [00650, 01251], lr: 0.000596, loss: 0.4305
2022-10-02 17:19:43 - train: epoch 0015, iter [00660, 01251], lr: 0.000596, loss: 0.4377
2022-10-02 17:20:03 - train: epoch 0015, iter [00670, 01251], lr: 0.000596, loss: 0.4159
2022-10-02 17:20:24 - train: epoch 0015, iter [00680, 01251], lr: 0.000596, loss: 0.4259
2022-10-02 17:20:43 - train: epoch 0015, iter [00690, 01251], lr: 0.000596, loss: 0.4360
2022-10-02 17:21:03 - train: epoch 0015, iter [00700, 01251], lr: 0.000596, loss: 0.4160
2022-10-02 17:21:23 - train: epoch 0015, iter [00710, 01251], lr: 0.000596, loss: 0.4333
2022-10-02 17:21:43 - train: epoch 0015, iter [00720, 01251], lr: 0.000596, loss: 0.4256
2022-10-02 17:22:03 - train: epoch 0015, iter [00730, 01251], lr: 0.000596, loss: 0.4360
2022-10-02 17:22:23 - train: epoch 0015, iter [00740, 01251], lr: 0.000596, loss: 0.4171
2022-10-02 17:22:43 - train: epoch 0015, iter [00750, 01251], lr: 0.000596, loss: 0.4518
2022-10-02 17:23:03 - train: epoch 0015, iter [00760, 01251], lr: 0.000596, loss: 0.4263
2022-10-02 17:23:23 - train: epoch 0015, iter [00770, 01251], lr: 0.000596, loss: 0.4327
2022-10-02 17:23:43 - train: epoch 0015, iter [00780, 01251], lr: 0.000596, loss: 0.4295
2022-10-02 17:24:03 - train: epoch 0015, iter [00790, 01251], lr: 0.000596, loss: 0.4361
2022-10-02 17:24:23 - train: epoch 0015, iter [00800, 01251], lr: 0.000596, loss: 0.4264
2022-10-02 17:24:42 - train: epoch 0015, iter [00810, 01251], lr: 0.000596, loss: 0.4479
2022-10-02 17:25:02 - train: epoch 0015, iter [00820, 01251], lr: 0.000596, loss: 0.4271
2022-10-02 17:25:22 - train: epoch 0015, iter [00830, 01251], lr: 0.000596, loss: 0.4296
2022-10-02 17:25:42 - train: epoch 0015, iter [00840, 01251], lr: 0.000596, loss: 0.4319
2022-10-02 17:26:02 - train: epoch 0015, iter [00850, 01251], lr: 0.000596, loss: 0.4300
2022-10-02 17:26:22 - train: epoch 0015, iter [00860, 01251], lr: 0.000596, loss: 0.4233
2022-10-02 17:26:42 - train: epoch 0015, iter [00870, 01251], lr: 0.000596, loss: 0.4305
2022-10-02 17:27:03 - train: epoch 0015, iter [00880, 01251], lr: 0.000596, loss: 0.4262
2022-10-02 17:27:22 - train: epoch 0015, iter [00890, 01251], lr: 0.000596, loss: 0.4042
2022-10-02 17:27:42 - train: epoch 0015, iter [00900, 01251], lr: 0.000596, loss: 0.4213
2022-10-02 17:28:02 - train: epoch 0015, iter [00910, 01251], lr: 0.000596, loss: 0.4336
2022-10-02 17:28:22 - train: epoch 0015, iter [00920, 01251], lr: 0.000596, loss: 0.4065
2022-10-02 17:28:42 - train: epoch 0015, iter [00930, 01251], lr: 0.000596, loss: 0.4219
2022-10-02 17:29:02 - train: epoch 0015, iter [00940, 01251], lr: 0.000596, loss: 0.4450
2022-10-02 17:29:23 - train: epoch 0015, iter [00950, 01251], lr: 0.000596, loss: 0.4298
2022-10-02 17:29:43 - train: epoch 0015, iter [00960, 01251], lr: 0.000596, loss: 0.4511
2022-10-02 17:30:03 - train: epoch 0015, iter [00970, 01251], lr: 0.000596, loss: 0.4082
2022-10-02 17:30:22 - train: epoch 0015, iter [00980, 01251], lr: 0.000596, loss: 0.4299
2022-10-02 17:30:42 - train: epoch 0015, iter [00990, 01251], lr: 0.000596, loss: 0.4302
2022-10-02 17:31:02 - train: epoch 0015, iter [01000, 01251], lr: 0.000596, loss: 0.4181
2022-10-02 17:31:22 - train: epoch 0015, iter [01010, 01251], lr: 0.000596, loss: 0.4158
2022-10-02 17:31:42 - train: epoch 0015, iter [01020, 01251], lr: 0.000596, loss: 0.4248
2022-10-02 17:32:02 - train: epoch 0015, iter [01030, 01251], lr: 0.000596, loss: 0.4399
2022-10-02 17:32:22 - train: epoch 0015, iter [01040, 01251], lr: 0.000596, loss: 0.4394
2022-10-02 17:32:42 - train: epoch 0015, iter [01050, 01251], lr: 0.000596, loss: 0.4396
2022-10-02 17:33:01 - train: epoch 0015, iter [01060, 01251], lr: 0.000596, loss: 0.4506
2022-10-02 17:33:21 - train: epoch 0015, iter [01070, 01251], lr: 0.000596, loss: 0.4348
2022-10-02 17:33:41 - train: epoch 0015, iter [01080, 01251], lr: 0.000596, loss: 0.4466
2022-10-02 17:34:02 - train: epoch 0015, iter [01090, 01251], lr: 0.000596, loss: 0.4210
2022-10-02 17:34:22 - train: epoch 0015, iter [01100, 01251], lr: 0.000596, loss: 0.4369
2022-10-02 17:34:42 - train: epoch 0015, iter [01110, 01251], lr: 0.000596, loss: 0.4358
2022-10-02 17:35:01 - train: epoch 0015, iter [01120, 01251], lr: 0.000596, loss: 0.4411
2022-10-02 17:35:21 - train: epoch 0015, iter [01130, 01251], lr: 0.000596, loss: 0.4298
2022-10-02 17:35:41 - train: epoch 0015, iter [01140, 01251], lr: 0.000596, loss: 0.4227
2022-10-02 17:36:01 - train: epoch 0015, iter [01150, 01251], lr: 0.000596, loss: 0.4281
2022-10-02 17:36:20 - train: epoch 0015, iter [01160, 01251], lr: 0.000596, loss: 0.4423
2022-10-02 17:36:41 - train: epoch 0015, iter [01170, 01251], lr: 0.000596, loss: 0.4461
2022-10-02 17:37:01 - train: epoch 0015, iter [01180, 01251], lr: 0.000596, loss: 0.4030
2022-10-02 17:37:20 - train: epoch 0015, iter [01190, 01251], lr: 0.000596, loss: 0.4567
2022-10-02 17:37:40 - train: epoch 0015, iter [01200, 01251], lr: 0.000596, loss: 0.4344
2022-10-02 17:38:00 - train: epoch 0015, iter [01210, 01251], lr: 0.000596, loss: 0.4335
2022-10-02 17:38:20 - train: epoch 0015, iter [01220, 01251], lr: 0.000595, loss: 0.4296
2022-10-02 17:38:39 - train: epoch 0015, iter [01230, 01251], lr: 0.000595, loss: 0.4306
2022-10-02 17:38:59 - train: epoch 0015, iter [01240, 01251], lr: 0.000595, loss: 0.4357
2022-10-02 17:39:18 - train: epoch 0015, iter [01250, 01251], lr: 0.000595, loss: 0.4522
2022-10-02 17:39:21 - train: epoch 015, train_loss: 0.4309
2022-10-02 17:39:25 - until epoch: 015, best_loss: 0.4309
2022-10-02 17:39:25 - epoch 016 lr: 0.000595
2022-10-02 17:39:52 - train: epoch 0016, iter [00010, 01251], lr: 0.000595, loss: 0.4371
2022-10-02 17:40:12 - train: epoch 0016, iter [00020, 01251], lr: 0.000595, loss: 0.4374
2022-10-02 17:40:33 - train: epoch 0016, iter [00030, 01251], lr: 0.000595, loss: 0.4471
2022-10-02 17:40:53 - train: epoch 0016, iter [00040, 01251], lr: 0.000595, loss: 0.4306
2022-10-02 17:41:13 - train: epoch 0016, iter [00050, 01251], lr: 0.000595, loss: 0.4363
2022-10-02 17:41:33 - train: epoch 0016, iter [00060, 01251], lr: 0.000595, loss: 0.4302
2022-10-02 17:41:54 - train: epoch 0016, iter [00070, 01251], lr: 0.000595, loss: 0.4246
2022-10-02 17:42:14 - train: epoch 0016, iter [00080, 01251], lr: 0.000595, loss: 0.4306
2022-10-02 17:42:34 - train: epoch 0016, iter [00090, 01251], lr: 0.000595, loss: 0.4255
2022-10-02 17:42:55 - train: epoch 0016, iter [00100, 01251], lr: 0.000595, loss: 0.4204
2022-10-02 17:43:15 - train: epoch 0016, iter [00110, 01251], lr: 0.000595, loss: 0.4273
2022-10-02 17:43:35 - train: epoch 0016, iter [00120, 01251], lr: 0.000595, loss: 0.4446
2022-10-02 17:43:55 - train: epoch 0016, iter [00130, 01251], lr: 0.000595, loss: 0.4248
2022-10-02 17:44:15 - train: epoch 0016, iter [00140, 01251], lr: 0.000595, loss: 0.4334
2022-10-02 17:44:36 - train: epoch 0016, iter [00150, 01251], lr: 0.000595, loss: 0.4352
2022-10-02 17:44:56 - train: epoch 0016, iter [00160, 01251], lr: 0.000595, loss: 0.4436
2022-10-02 17:45:17 - train: epoch 0016, iter [00170, 01251], lr: 0.000595, loss: 0.4389
2022-10-02 17:45:37 - train: epoch 0016, iter [00180, 01251], lr: 0.000595, loss: 0.4387
2022-10-02 17:45:58 - train: epoch 0016, iter [00190, 01251], lr: 0.000595, loss: 0.4334
2022-10-02 17:46:18 - train: epoch 0016, iter [00200, 01251], lr: 0.000595, loss: 0.4327
2022-10-02 17:46:38 - train: epoch 0016, iter [00210, 01251], lr: 0.000595, loss: 0.4285
2022-10-02 17:46:58 - train: epoch 0016, iter [00220, 01251], lr: 0.000595, loss: 0.4399
2022-10-02 17:47:18 - train: epoch 0016, iter [00230, 01251], lr: 0.000595, loss: 0.4141
2022-10-02 17:47:38 - train: epoch 0016, iter [00240, 01251], lr: 0.000595, loss: 0.4352
2022-10-02 17:47:59 - train: epoch 0016, iter [00250, 01251], lr: 0.000595, loss: 0.4225
2022-10-02 17:48:19 - train: epoch 0016, iter [00260, 01251], lr: 0.000595, loss: 0.4181
2022-10-02 17:48:39 - train: epoch 0016, iter [00270, 01251], lr: 0.000595, loss: 0.4391
2022-10-02 17:48:59 - train: epoch 0016, iter [00280, 01251], lr: 0.000595, loss: 0.4308
2022-10-02 17:49:19 - train: epoch 0016, iter [00290, 01251], lr: 0.000595, loss: 0.4334
2022-10-02 17:49:40 - train: epoch 0016, iter [00300, 01251], lr: 0.000595, loss: 0.4389
2022-10-02 17:50:00 - train: epoch 0016, iter [00310, 01251], lr: 0.000595, loss: 0.4462
2022-10-02 17:50:20 - train: epoch 0016, iter [00320, 01251], lr: 0.000595, loss: 0.4414
2022-10-02 17:50:41 - train: epoch 0016, iter [00330, 01251], lr: 0.000595, loss: 0.4287
2022-10-02 17:51:01 - train: epoch 0016, iter [00340, 01251], lr: 0.000595, loss: 0.4282
2022-10-02 17:51:21 - train: epoch 0016, iter [00350, 01251], lr: 0.000595, loss: 0.4371
2022-10-02 17:51:41 - train: epoch 0016, iter [00360, 01251], lr: 0.000595, loss: 0.4375
2022-10-02 17:52:01 - train: epoch 0016, iter [00370, 01251], lr: 0.000595, loss: 0.4263
2022-10-02 17:52:21 - train: epoch 0016, iter [00380, 01251], lr: 0.000595, loss: 0.4382
2022-10-02 17:52:41 - train: epoch 0016, iter [00390, 01251], lr: 0.000595, loss: 0.4496
2022-10-02 17:53:01 - train: epoch 0016, iter [00400, 01251], lr: 0.000595, loss: 0.4291
2022-10-02 17:53:21 - train: epoch 0016, iter [00410, 01251], lr: 0.000595, loss: 0.4298
2022-10-02 17:53:41 - train: epoch 0016, iter [00420, 01251], lr: 0.000595, loss: 0.4167
2022-10-02 17:54:01 - train: epoch 0016, iter [00430, 01251], lr: 0.000595, loss: 0.4406
2022-10-02 17:54:21 - train: epoch 0016, iter [00440, 01251], lr: 0.000595, loss: 0.4307
2022-10-02 17:54:41 - train: epoch 0016, iter [00450, 01251], lr: 0.000595, loss: 0.4138
2022-10-02 17:55:01 - train: epoch 0016, iter [00460, 01251], lr: 0.000595, loss: 0.4087
2022-10-02 17:55:21 - train: epoch 0016, iter [00470, 01251], lr: 0.000595, loss: 0.4221
2022-10-02 17:55:41 - train: epoch 0016, iter [00480, 01251], lr: 0.000595, loss: 0.4395
2022-10-02 17:56:01 - train: epoch 0016, iter [00490, 01251], lr: 0.000595, loss: 0.4237
2022-10-02 17:56:22 - train: epoch 0016, iter [00500, 01251], lr: 0.000595, loss: 0.4091
2022-10-02 17:56:42 - train: epoch 0016, iter [00510, 01251], lr: 0.000595, loss: 0.4573
2022-10-02 17:57:02 - train: epoch 0016, iter [00520, 01251], lr: 0.000595, loss: 0.4457
2022-10-02 17:57:22 - train: epoch 0016, iter [00530, 01251], lr: 0.000595, loss: 0.4072
2022-10-02 17:57:42 - train: epoch 0016, iter [00540, 01251], lr: 0.000595, loss: 0.4405
2022-10-02 17:58:02 - train: epoch 0016, iter [00550, 01251], lr: 0.000595, loss: 0.4290
2022-10-02 17:58:22 - train: epoch 0016, iter [00560, 01251], lr: 0.000595, loss: 0.3994
2022-10-02 17:58:42 - train: epoch 0016, iter [00570, 01251], lr: 0.000595, loss: 0.4225
2022-10-02 17:59:02 - train: epoch 0016, iter [00580, 01251], lr: 0.000595, loss: 0.4414
2022-10-02 17:59:22 - train: epoch 0016, iter [00590, 01251], lr: 0.000595, loss: 0.4376
2022-10-02 17:59:42 - train: epoch 0016, iter [00600, 01251], lr: 0.000595, loss: 0.4351
2022-10-02 18:00:02 - train: epoch 0016, iter [00610, 01251], lr: 0.000595, loss: 0.4331
2022-10-02 18:00:22 - train: epoch 0016, iter [00620, 01251], lr: 0.000594, loss: 0.4180
2022-10-02 18:00:42 - train: epoch 0016, iter [00630, 01251], lr: 0.000594, loss: 0.4392
2022-10-02 18:01:02 - train: epoch 0016, iter [00640, 01251], lr: 0.000594, loss: 0.4335
2022-10-02 18:01:22 - train: epoch 0016, iter [00650, 01251], lr: 0.000594, loss: 0.4287
2022-10-02 18:01:42 - train: epoch 0016, iter [00660, 01251], lr: 0.000594, loss: 0.4208
2022-10-02 18:02:02 - train: epoch 0016, iter [00670, 01251], lr: 0.000594, loss: 0.4031
2022-10-02 18:02:22 - train: epoch 0016, iter [00680, 01251], lr: 0.000594, loss: 0.4309
2022-10-02 18:02:42 - train: epoch 0016, iter [00690, 01251], lr: 0.000594, loss: 0.4298
2022-10-02 18:03:02 - train: epoch 0016, iter [00700, 01251], lr: 0.000594, loss: 0.4237
2022-10-02 18:03:23 - train: epoch 0016, iter [00710, 01251], lr: 0.000594, loss: 0.4273
2022-10-02 18:03:43 - train: epoch 0016, iter [00720, 01251], lr: 0.000594, loss: 0.4342
2022-10-02 18:04:03 - train: epoch 0016, iter [00730, 01251], lr: 0.000594, loss: 0.4212
2022-10-02 18:04:23 - train: epoch 0016, iter [00740, 01251], lr: 0.000594, loss: 0.4291
2022-10-02 18:04:43 - train: epoch 0016, iter [00750, 01251], lr: 0.000594, loss: 0.4269
2022-10-02 18:05:03 - train: epoch 0016, iter [00760, 01251], lr: 0.000594, loss: 0.4237
2022-10-02 18:05:23 - train: epoch 0016, iter [00770, 01251], lr: 0.000594, loss: 0.4220
2022-10-02 18:05:43 - train: epoch 0016, iter [00780, 01251], lr: 0.000594, loss: 0.4383
2022-10-02 18:06:02 - train: epoch 0016, iter [00790, 01251], lr: 0.000594, loss: 0.4143
2022-10-02 18:06:22 - train: epoch 0016, iter [00800, 01251], lr: 0.000594, loss: 0.4291
2022-10-02 18:06:43 - train: epoch 0016, iter [00810, 01251], lr: 0.000594, loss: 0.4272
2022-10-02 18:07:03 - train: epoch 0016, iter [00820, 01251], lr: 0.000594, loss: 0.4226
2022-10-02 18:07:23 - train: epoch 0016, iter [00830, 01251], lr: 0.000594, loss: 0.4393
2022-10-02 18:07:43 - train: epoch 0016, iter [00840, 01251], lr: 0.000594, loss: 0.4261
2022-10-02 18:08:04 - train: epoch 0016, iter [00850, 01251], lr: 0.000594, loss: 0.4315
2022-10-02 18:08:24 - train: epoch 0016, iter [00860, 01251], lr: 0.000594, loss: 0.4430
2022-10-02 18:08:44 - train: epoch 0016, iter [00870, 01251], lr: 0.000594, loss: 0.4176
2022-10-02 18:09:04 - train: epoch 0016, iter [00880, 01251], lr: 0.000594, loss: 0.4162
2022-10-02 18:09:24 - train: epoch 0016, iter [00890, 01251], lr: 0.000594, loss: 0.4181
2022-10-02 18:09:44 - train: epoch 0016, iter [00900, 01251], lr: 0.000594, loss: 0.4294
2022-10-02 18:10:04 - train: epoch 0016, iter [00910, 01251], lr: 0.000594, loss: 0.4341
2022-10-02 18:10:25 - train: epoch 0016, iter [00920, 01251], lr: 0.000594, loss: 0.4228
2022-10-02 18:10:44 - train: epoch 0016, iter [00930, 01251], lr: 0.000594, loss: 0.4145
2022-10-02 18:11:05 - train: epoch 0016, iter [00940, 01251], lr: 0.000594, loss: 0.4154
2022-10-02 18:11:25 - train: epoch 0016, iter [00950, 01251], lr: 0.000594, loss: 0.4256
2022-10-02 18:11:45 - train: epoch 0016, iter [00960, 01251], lr: 0.000594, loss: 0.4574
2022-10-02 18:12:05 - train: epoch 0016, iter [00970, 01251], lr: 0.000594, loss: 0.4351
2022-10-02 18:12:25 - train: epoch 0016, iter [00980, 01251], lr: 0.000594, loss: 0.4306
2022-10-02 18:12:45 - train: epoch 0016, iter [00990, 01251], lr: 0.000594, loss: 0.4103
2022-10-02 18:13:05 - train: epoch 0016, iter [01000, 01251], lr: 0.000594, loss: 0.4425
2022-10-02 18:13:25 - train: epoch 0016, iter [01010, 01251], lr: 0.000594, loss: 0.4491
2022-10-02 18:13:45 - train: epoch 0016, iter [01020, 01251], lr: 0.000594, loss: 0.4288
2022-10-02 18:14:05 - train: epoch 0016, iter [01030, 01251], lr: 0.000594, loss: 0.4418
2022-10-02 18:14:25 - train: epoch 0016, iter [01040, 01251], lr: 0.000594, loss: 0.4383
2022-10-02 18:14:45 - train: epoch 0016, iter [01050, 01251], lr: 0.000594, loss: 0.4283
2022-10-02 18:15:05 - train: epoch 0016, iter [01060, 01251], lr: 0.000594, loss: 0.4448
2022-10-02 18:15:25 - train: epoch 0016, iter [01070, 01251], lr: 0.000594, loss: 0.4254
2022-10-02 18:15:46 - train: epoch 0016, iter [01080, 01251], lr: 0.000594, loss: 0.4324
2022-10-02 18:16:06 - train: epoch 0016, iter [01090, 01251], lr: 0.000594, loss: 0.4336
2022-10-02 18:16:26 - train: epoch 0016, iter [01100, 01251], lr: 0.000594, loss: 0.4169
2022-10-02 18:16:46 - train: epoch 0016, iter [01110, 01251], lr: 0.000594, loss: 0.4237
2022-10-02 18:17:06 - train: epoch 0016, iter [01120, 01251], lr: 0.000594, loss: 0.4063
2022-10-02 18:17:26 - train: epoch 0016, iter [01130, 01251], lr: 0.000594, loss: 0.4531
2022-10-02 18:17:46 - train: epoch 0016, iter [01140, 01251], lr: 0.000594, loss: 0.4215
2022-10-02 18:18:07 - train: epoch 0016, iter [01150, 01251], lr: 0.000594, loss: 0.4155
2022-10-02 18:18:27 - train: epoch 0016, iter [01160, 01251], lr: 0.000594, loss: 0.4208
2022-10-02 18:18:47 - train: epoch 0016, iter [01170, 01251], lr: 0.000594, loss: 0.4401
2022-10-02 18:19:07 - train: epoch 0016, iter [01180, 01251], lr: 0.000594, loss: 0.4149
2022-10-02 18:19:27 - train: epoch 0016, iter [01190, 01251], lr: 0.000594, loss: 0.4120
2022-10-02 18:19:47 - train: epoch 0016, iter [01200, 01251], lr: 0.000594, loss: 0.4292
2022-10-02 18:20:07 - train: epoch 0016, iter [01210, 01251], lr: 0.000594, loss: 0.4115
2022-10-02 18:20:26 - train: epoch 0016, iter [01220, 01251], lr: 0.000593, loss: 0.4453
2022-10-02 18:20:46 - train: epoch 0016, iter [01230, 01251], lr: 0.000593, loss: 0.4285
2022-10-02 18:21:06 - train: epoch 0016, iter [01240, 01251], lr: 0.000593, loss: 0.4373
2022-10-02 18:21:25 - train: epoch 0016, iter [01250, 01251], lr: 0.000593, loss: 0.4420
2022-10-02 18:21:28 - train: epoch 016, train_loss: 0.4294
2022-10-02 18:21:33 - until epoch: 016, best_loss: 0.4294
2022-10-02 18:21:33 - epoch 017 lr: 0.000593
2022-10-02 18:22:00 - train: epoch 0017, iter [00010, 01251], lr: 0.000593, loss: 0.4286
2022-10-02 18:22:20 - train: epoch 0017, iter [00020, 01251], lr: 0.000593, loss: 0.4197
2022-10-02 18:22:40 - train: epoch 0017, iter [00030, 01251], lr: 0.000593, loss: 0.4408
2022-10-02 18:23:01 - train: epoch 0017, iter [00040, 01251], lr: 0.000593, loss: 0.4169
2022-10-02 18:23:21 - train: epoch 0017, iter [00050, 01251], lr: 0.000593, loss: 0.4360
2022-10-02 18:23:41 - train: epoch 0017, iter [00060, 01251], lr: 0.000593, loss: 0.4217
2022-10-02 18:24:01 - train: epoch 0017, iter [00070, 01251], lr: 0.000593, loss: 0.4199
2022-10-02 18:24:21 - train: epoch 0017, iter [00080, 01251], lr: 0.000593, loss: 0.4279
2022-10-02 18:24:41 - train: epoch 0017, iter [00090, 01251], lr: 0.000593, loss: 0.4211
2022-10-02 18:25:02 - train: epoch 0017, iter [00100, 01251], lr: 0.000593, loss: 0.4161
2022-10-02 18:25:22 - train: epoch 0017, iter [00110, 01251], lr: 0.000593, loss: 0.4194
2022-10-02 18:25:42 - train: epoch 0017, iter [00120, 01251], lr: 0.000593, loss: 0.4367
2022-10-02 18:26:03 - train: epoch 0017, iter [00130, 01251], lr: 0.000593, loss: 0.4283
2022-10-02 18:26:23 - train: epoch 0017, iter [00140, 01251], lr: 0.000593, loss: 0.4153
2022-10-02 18:26:43 - train: epoch 0017, iter [00150, 01251], lr: 0.000593, loss: 0.4110
2022-10-02 18:27:03 - train: epoch 0017, iter [00160, 01251], lr: 0.000593, loss: 0.4023
2022-10-02 18:27:23 - train: epoch 0017, iter [00170, 01251], lr: 0.000593, loss: 0.4100
2022-10-02 18:27:43 - train: epoch 0017, iter [00180, 01251], lr: 0.000593, loss: 0.4222
2022-10-02 18:28:03 - train: epoch 0017, iter [00190, 01251], lr: 0.000593, loss: 0.4224
2022-10-02 18:28:23 - train: epoch 0017, iter [00200, 01251], lr: 0.000593, loss: 0.4267
2022-10-02 18:28:43 - train: epoch 0017, iter [00210, 01251], lr: 0.000593, loss: 0.4132
2022-10-02 18:29:03 - train: epoch 0017, iter [00220, 01251], lr: 0.000593, loss: 0.4319
2022-10-02 18:29:23 - train: epoch 0017, iter [00230, 01251], lr: 0.000593, loss: 0.4091
2022-10-02 18:29:43 - train: epoch 0017, iter [00240, 01251], lr: 0.000593, loss: 0.4115
2022-10-02 18:30:03 - train: epoch 0017, iter [00250, 01251], lr: 0.000593, loss: 0.4352
2022-10-02 18:30:23 - train: epoch 0017, iter [00260, 01251], lr: 0.000593, loss: 0.4292
2022-10-02 18:30:42 - train: epoch 0017, iter [00270, 01251], lr: 0.000593, loss: 0.4176
2022-10-02 18:31:02 - train: epoch 0017, iter [00280, 01251], lr: 0.000593, loss: 0.4314
2022-10-02 18:31:22 - train: epoch 0017, iter [00290, 01251], lr: 0.000593, loss: 0.4187
2022-10-02 18:31:42 - train: epoch 0017, iter [00300, 01251], lr: 0.000593, loss: 0.4102
2022-10-02 18:32:02 - train: epoch 0017, iter [00310, 01251], lr: 0.000593, loss: 0.4177
2022-10-02 18:32:22 - train: epoch 0017, iter [00320, 01251], lr: 0.000593, loss: 0.4181
2022-10-02 18:32:41 - train: epoch 0017, iter [00330, 01251], lr: 0.000593, loss: 0.4348
2022-10-02 18:33:01 - train: epoch 0017, iter [00340, 01251], lr: 0.000593, loss: 0.4217
2022-10-02 18:33:21 - train: epoch 0017, iter [00350, 01251], lr: 0.000593, loss: 0.4396
2022-10-02 18:33:41 - train: epoch 0017, iter [00360, 01251], lr: 0.000593, loss: 0.4351
2022-10-02 18:34:00 - train: epoch 0017, iter [00370, 01251], lr: 0.000593, loss: 0.4207
2022-10-02 18:34:20 - train: epoch 0017, iter [00380, 01251], lr: 0.000593, loss: 0.4445
2022-10-02 18:34:40 - train: epoch 0017, iter [00390, 01251], lr: 0.000593, loss: 0.4520
2022-10-02 18:35:00 - train: epoch 0017, iter [00400, 01251], lr: 0.000593, loss: 0.4285
2022-10-02 18:35:19 - train: epoch 0017, iter [00410, 01251], lr: 0.000593, loss: 0.4277
2022-10-02 18:35:39 - train: epoch 0017, iter [00420, 01251], lr: 0.000593, loss: 0.4470
2022-10-02 18:35:59 - train: epoch 0017, iter [00430, 01251], lr: 0.000593, loss: 0.4331
2022-10-02 18:36:19 - train: epoch 0017, iter [00440, 01251], lr: 0.000593, loss: 0.4437
2022-10-02 18:36:39 - train: epoch 0017, iter [00450, 01251], lr: 0.000593, loss: 0.4458
2022-10-02 18:36:58 - train: epoch 0017, iter [00460, 01251], lr: 0.000593, loss: 0.4170
2022-10-02 18:37:18 - train: epoch 0017, iter [00470, 01251], lr: 0.000593, loss: 0.4319
2022-10-02 18:37:38 - train: epoch 0017, iter [00480, 01251], lr: 0.000593, loss: 0.4209
2022-10-02 18:37:57 - train: epoch 0017, iter [00490, 01251], lr: 0.000593, loss: 0.4203
2022-10-02 18:38:17 - train: epoch 0017, iter [00500, 01251], lr: 0.000593, loss: 0.4383
2022-10-02 18:38:37 - train: epoch 0017, iter [00510, 01251], lr: 0.000593, loss: 0.4192
2022-10-02 18:38:57 - train: epoch 0017, iter [00520, 01251], lr: 0.000593, loss: 0.4256
2022-10-02 18:39:17 - train: epoch 0017, iter [00530, 01251], lr: 0.000592, loss: 0.4060
2022-10-02 18:39:37 - train: epoch 0017, iter [00540, 01251], lr: 0.000592, loss: 0.4286
2022-10-02 18:39:56 - train: epoch 0017, iter [00550, 01251], lr: 0.000592, loss: 0.4265
2022-10-02 18:40:16 - train: epoch 0017, iter [00560, 01251], lr: 0.000592, loss: 0.4355
2022-10-02 18:40:37 - train: epoch 0017, iter [00570, 01251], lr: 0.000592, loss: 0.4224
2022-10-02 18:40:56 - train: epoch 0017, iter [00580, 01251], lr: 0.000592, loss: 0.4330
2022-10-02 18:41:16 - train: epoch 0017, iter [00590, 01251], lr: 0.000592, loss: 0.4338
2022-10-02 18:41:36 - train: epoch 0017, iter [00600, 01251], lr: 0.000592, loss: 0.4344
2022-10-02 18:41:55 - train: epoch 0017, iter [00610, 01251], lr: 0.000592, loss: 0.4356
2022-10-02 18:42:15 - train: epoch 0017, iter [00620, 01251], lr: 0.000592, loss: 0.4290
2022-10-02 18:42:35 - train: epoch 0017, iter [00630, 01251], lr: 0.000592, loss: 0.4479
2022-10-02 18:42:55 - train: epoch 0017, iter [00640, 01251], lr: 0.000592, loss: 0.4253
2022-10-02 18:43:15 - train: epoch 0017, iter [00650, 01251], lr: 0.000592, loss: 0.4250
2022-10-02 18:43:35 - train: epoch 0017, iter [00660, 01251], lr: 0.000592, loss: 0.4351
2022-10-02 18:43:54 - train: epoch 0017, iter [00670, 01251], lr: 0.000592, loss: 0.4272
2022-10-02 18:44:14 - train: epoch 0017, iter [00680, 01251], lr: 0.000592, loss: 0.4363
2022-10-02 18:44:34 - train: epoch 0017, iter [00690, 01251], lr: 0.000592, loss: 0.4228
2022-10-02 18:44:54 - train: epoch 0017, iter [00700, 01251], lr: 0.000592, loss: 0.4365
2022-10-02 18:45:14 - train: epoch 0017, iter [00710, 01251], lr: 0.000592, loss: 0.4325
2022-10-02 18:45:34 - train: epoch 0017, iter [00720, 01251], lr: 0.000592, loss: 0.4285
2022-10-02 18:45:54 - train: epoch 0017, iter [00730, 01251], lr: 0.000592, loss: 0.4415
2022-10-02 18:46:13 - train: epoch 0017, iter [00740, 01251], lr: 0.000592, loss: 0.4182
2022-10-02 18:46:33 - train: epoch 0017, iter [00750, 01251], lr: 0.000592, loss: 0.4173
2022-10-02 18:46:53 - train: epoch 0017, iter [00760, 01251], lr: 0.000592, loss: 0.4187
2022-10-02 18:47:13 - train: epoch 0017, iter [00770, 01251], lr: 0.000592, loss: 0.4342
2022-10-02 18:47:33 - train: epoch 0017, iter [00780, 01251], lr: 0.000592, loss: 0.4234
2022-10-02 18:47:53 - train: epoch 0017, iter [00790, 01251], lr: 0.000592, loss: 0.4231
2022-10-02 18:48:12 - train: epoch 0017, iter [00800, 01251], lr: 0.000592, loss: 0.4413
2022-10-02 18:48:32 - train: epoch 0017, iter [00810, 01251], lr: 0.000592, loss: 0.4204
2022-10-02 18:48:52 - train: epoch 0017, iter [00820, 01251], lr: 0.000592, loss: 0.4232
2022-10-02 18:49:11 - train: epoch 0017, iter [00830, 01251], lr: 0.000592, loss: 0.4428
2022-10-02 18:49:31 - train: epoch 0017, iter [00840, 01251], lr: 0.000592, loss: 0.3920
2022-10-02 18:49:50 - train: epoch 0017, iter [00850, 01251], lr: 0.000592, loss: 0.4004
2022-10-02 18:50:10 - train: epoch 0017, iter [00860, 01251], lr: 0.000592, loss: 0.4262
2022-10-02 18:50:30 - train: epoch 0017, iter [00870, 01251], lr: 0.000592, loss: 0.4399
2022-10-02 18:50:50 - train: epoch 0017, iter [00880, 01251], lr: 0.000592, loss: 0.4223
2022-10-02 18:51:10 - train: epoch 0017, iter [00890, 01251], lr: 0.000592, loss: 0.4320
2022-10-02 18:51:30 - train: epoch 0017, iter [00900, 01251], lr: 0.000592, loss: 0.4250
2022-10-02 18:51:50 - train: epoch 0017, iter [00910, 01251], lr: 0.000592, loss: 0.4307
2022-10-02 18:52:10 - train: epoch 0017, iter [00920, 01251], lr: 0.000592, loss: 0.4306
2022-10-02 18:52:30 - train: epoch 0017, iter [00930, 01251], lr: 0.000592, loss: 0.4366
2022-10-02 18:52:49 - train: epoch 0017, iter [00940, 01251], lr: 0.000592, loss: 0.4066
2022-10-02 18:53:09 - train: epoch 0017, iter [00950, 01251], lr: 0.000592, loss: 0.4317
2022-10-02 18:53:29 - train: epoch 0017, iter [00960, 01251], lr: 0.000592, loss: 0.4303
2022-10-02 18:53:49 - train: epoch 0017, iter [00970, 01251], lr: 0.000592, loss: 0.4173
2022-10-02 18:54:08 - train: epoch 0017, iter [00980, 01251], lr: 0.000592, loss: 0.4370
2022-10-02 18:54:28 - train: epoch 0017, iter [00990, 01251], lr: 0.000592, loss: 0.4106
2022-10-02 18:54:48 - train: epoch 0017, iter [01000, 01251], lr: 0.000592, loss: 0.4182
2022-10-02 18:55:08 - train: epoch 0017, iter [01010, 01251], lr: 0.000592, loss: 0.4299
2022-10-02 18:55:28 - train: epoch 0017, iter [01020, 01251], lr: 0.000592, loss: 0.4350
2022-10-02 18:55:48 - train: epoch 0017, iter [01030, 01251], lr: 0.000592, loss: 0.4467
2022-10-02 18:56:07 - train: epoch 0017, iter [01040, 01251], lr: 0.000592, loss: 0.4293
2022-10-02 18:56:27 - train: epoch 0017, iter [01050, 01251], lr: 0.000591, loss: 0.4288
2022-10-02 18:56:47 - train: epoch 0017, iter [01060, 01251], lr: 0.000591, loss: 0.4309
2022-10-02 18:57:07 - train: epoch 0017, iter [01070, 01251], lr: 0.000591, loss: 0.4082
2022-10-02 18:57:27 - train: epoch 0017, iter [01080, 01251], lr: 0.000591, loss: 0.4268
2022-10-02 18:57:47 - train: epoch 0017, iter [01090, 01251], lr: 0.000591, loss: 0.4206
2022-10-02 18:58:07 - train: epoch 0017, iter [01100, 01251], lr: 0.000591, loss: 0.4269
2022-10-02 18:58:26 - train: epoch 0017, iter [01110, 01251], lr: 0.000591, loss: 0.4187
2022-10-02 18:58:46 - train: epoch 0017, iter [01120, 01251], lr: 0.000591, loss: 0.4373
2022-10-02 18:59:06 - train: epoch 0017, iter [01130, 01251], lr: 0.000591, loss: 0.4249
2022-10-02 18:59:26 - train: epoch 0017, iter [01140, 01251], lr: 0.000591, loss: 0.4349
2022-10-02 18:59:46 - train: epoch 0017, iter [01150, 01251], lr: 0.000591, loss: 0.4185
2022-10-02 19:00:06 - train: epoch 0017, iter [01160, 01251], lr: 0.000591, loss: 0.4301
2022-10-02 19:00:25 - train: epoch 0017, iter [01170, 01251], lr: 0.000591, loss: 0.4269
2022-10-02 19:00:45 - train: epoch 0017, iter [01180, 01251], lr: 0.000591, loss: 0.4503
2022-10-02 19:01:05 - train: epoch 0017, iter [01190, 01251], lr: 0.000591, loss: 0.4182
2022-10-02 19:01:25 - train: epoch 0017, iter [01200, 01251], lr: 0.000591, loss: 0.4411
2022-10-02 19:01:45 - train: epoch 0017, iter [01210, 01251], lr: 0.000591, loss: 0.4299
2022-10-02 19:02:05 - train: epoch 0017, iter [01220, 01251], lr: 0.000591, loss: 0.4210
2022-10-02 19:02:25 - train: epoch 0017, iter [01230, 01251], lr: 0.000591, loss: 0.4272
2022-10-02 19:02:45 - train: epoch 0017, iter [01240, 01251], lr: 0.000591, loss: 0.4146
2022-10-02 19:03:04 - train: epoch 0017, iter [01250, 01251], lr: 0.000591, loss: 0.4318
2022-10-02 19:03:07 - train: epoch 017, train_loss: 0.4281
2022-10-02 19:03:11 - until epoch: 017, best_loss: 0.4281
2022-10-02 19:03:11 - epoch 018 lr: 0.000591
2022-10-02 19:03:37 - train: epoch 0018, iter [00010, 01251], lr: 0.000591, loss: 0.4249
2022-10-02 19:03:57 - train: epoch 0018, iter [00020, 01251], lr: 0.000591, loss: 0.4303
2022-10-02 19:04:17 - train: epoch 0018, iter [00030, 01251], lr: 0.000591, loss: 0.4145
2022-10-02 19:04:37 - train: epoch 0018, iter [00040, 01251], lr: 0.000591, loss: 0.4167
2022-10-02 19:04:57 - train: epoch 0018, iter [00050, 01251], lr: 0.000591, loss: 0.4233
2022-10-02 19:05:16 - train: epoch 0018, iter [00060, 01251], lr: 0.000591, loss: 0.4295
2022-10-02 19:05:36 - train: epoch 0018, iter [00070, 01251], lr: 0.000591, loss: 0.4168
2022-10-02 19:05:56 - train: epoch 0018, iter [00080, 01251], lr: 0.000591, loss: 0.4380
2022-10-02 19:06:16 - train: epoch 0018, iter [00090, 01251], lr: 0.000591, loss: 0.4233
2022-10-02 19:06:36 - train: epoch 0018, iter [00100, 01251], lr: 0.000591, loss: 0.3992
2022-10-02 19:06:56 - train: epoch 0018, iter [00110, 01251], lr: 0.000591, loss: 0.4433
2022-10-02 19:07:16 - train: epoch 0018, iter [00120, 01251], lr: 0.000591, loss: 0.4191
2022-10-02 19:07:35 - train: epoch 0018, iter [00130, 01251], lr: 0.000591, loss: 0.4288
2022-10-02 19:07:55 - train: epoch 0018, iter [00140, 01251], lr: 0.000591, loss: 0.4278
2022-10-02 19:08:15 - train: epoch 0018, iter [00150, 01251], lr: 0.000591, loss: 0.4409
2022-10-02 19:08:35 - train: epoch 0018, iter [00160, 01251], lr: 0.000591, loss: 0.4322
2022-10-02 19:08:55 - train: epoch 0018, iter [00170, 01251], lr: 0.000591, loss: 0.4195
2022-10-02 19:09:15 - train: epoch 0018, iter [00180, 01251], lr: 0.000591, loss: 0.4237
2022-10-02 19:09:34 - train: epoch 0018, iter [00190, 01251], lr: 0.000591, loss: 0.4398
2022-10-02 19:09:54 - train: epoch 0018, iter [00200, 01251], lr: 0.000591, loss: 0.3979
2022-10-02 19:10:14 - train: epoch 0018, iter [00210, 01251], lr: 0.000591, loss: 0.4181
2022-10-02 19:10:34 - train: epoch 0018, iter [00220, 01251], lr: 0.000591, loss: 0.4355
2022-10-02 19:10:54 - train: epoch 0018, iter [00230, 01251], lr: 0.000591, loss: 0.4251
2022-10-02 19:11:14 - train: epoch 0018, iter [00240, 01251], lr: 0.000591, loss: 0.4291
2022-10-02 19:11:34 - train: epoch 0018, iter [00250, 01251], lr: 0.000591, loss: 0.4184
2022-10-02 19:11:54 - train: epoch 0018, iter [00260, 01251], lr: 0.000591, loss: 0.4412
2022-10-02 19:12:14 - train: epoch 0018, iter [00270, 01251], lr: 0.000591, loss: 0.4336
2022-10-02 19:12:34 - train: epoch 0018, iter [00280, 01251], lr: 0.000591, loss: 0.4251
2022-10-02 19:12:54 - train: epoch 0018, iter [00290, 01251], lr: 0.000590, loss: 0.4121
2022-10-02 19:13:14 - train: epoch 0018, iter [00300, 01251], lr: 0.000590, loss: 0.4107
2022-10-02 19:13:34 - train: epoch 0018, iter [00310, 01251], lr: 0.000590, loss: 0.4349
2022-10-02 19:13:54 - train: epoch 0018, iter [00320, 01251], lr: 0.000590, loss: 0.4369
2022-10-02 19:14:14 - train: epoch 0018, iter [00330, 01251], lr: 0.000590, loss: 0.4162
2022-10-02 19:14:34 - train: epoch 0018, iter [00340, 01251], lr: 0.000590, loss: 0.4178
2022-10-02 19:14:53 - train: epoch 0018, iter [00350, 01251], lr: 0.000590, loss: 0.4403
2022-10-02 19:15:13 - train: epoch 0018, iter [00360, 01251], lr: 0.000590, loss: 0.4412
2022-10-02 19:15:33 - train: epoch 0018, iter [00370, 01251], lr: 0.000590, loss: 0.4254
2022-10-02 19:15:53 - train: epoch 0018, iter [00380, 01251], lr: 0.000590, loss: 0.4349
2022-10-02 19:16:13 - train: epoch 0018, iter [00390, 01251], lr: 0.000590, loss: 0.4313
2022-10-02 19:16:33 - train: epoch 0018, iter [00400, 01251], lr: 0.000590, loss: 0.4314
2022-10-02 19:16:54 - train: epoch 0018, iter [00410, 01251], lr: 0.000590, loss: 0.4360
2022-10-02 19:17:13 - train: epoch 0018, iter [00420, 01251], lr: 0.000590, loss: 0.4174
2022-10-02 19:17:34 - train: epoch 0018, iter [00430, 01251], lr: 0.000590, loss: 0.4249
2022-10-02 19:17:54 - train: epoch 0018, iter [00440, 01251], lr: 0.000590, loss: 0.4231
2022-10-02 19:18:14 - train: epoch 0018, iter [00450, 01251], lr: 0.000590, loss: 0.4216
2022-10-02 19:18:33 - train: epoch 0018, iter [00460, 01251], lr: 0.000590, loss: 0.4062
2022-10-02 19:18:53 - train: epoch 0018, iter [00470, 01251], lr: 0.000590, loss: 0.4259
2022-10-02 19:19:13 - train: epoch 0018, iter [00480, 01251], lr: 0.000590, loss: 0.4474
2022-10-02 19:19:33 - train: epoch 0018, iter [00490, 01251], lr: 0.000590, loss: 0.4336
2022-10-02 19:19:53 - train: epoch 0018, iter [00500, 01251], lr: 0.000590, loss: 0.4190
2022-10-02 19:20:13 - train: epoch 0018, iter [00510, 01251], lr: 0.000590, loss: 0.4425
2022-10-02 19:20:32 - train: epoch 0018, iter [00520, 01251], lr: 0.000590, loss: 0.4379
2022-10-02 19:20:52 - train: epoch 0018, iter [00530, 01251], lr: 0.000590, loss: 0.4277
2022-10-02 19:21:12 - train: epoch 0018, iter [00540, 01251], lr: 0.000590, loss: 0.4267
2022-10-02 19:21:32 - train: epoch 0018, iter [00550, 01251], lr: 0.000590, loss: 0.4136
2022-10-02 19:21:52 - train: epoch 0018, iter [00560, 01251], lr: 0.000590, loss: 0.4400
2022-10-02 19:22:12 - train: epoch 0018, iter [00570, 01251], lr: 0.000590, loss: 0.4027
2022-10-02 19:22:32 - train: epoch 0018, iter [00580, 01251], lr: 0.000590, loss: 0.4213
2022-10-02 19:22:52 - train: epoch 0018, iter [00590, 01251], lr: 0.000590, loss: 0.4153
2022-10-02 19:23:12 - train: epoch 0018, iter [00600, 01251], lr: 0.000590, loss: 0.4100
2022-10-02 19:23:32 - train: epoch 0018, iter [00610, 01251], lr: 0.000590, loss: 0.4102
2022-10-02 19:23:51 - train: epoch 0018, iter [00620, 01251], lr: 0.000590, loss: 0.4354
2022-10-02 19:24:11 - train: epoch 0018, iter [00630, 01251], lr: 0.000590, loss: 0.4279
2022-10-02 19:24:31 - train: epoch 0018, iter [00640, 01251], lr: 0.000590, loss: 0.4252
2022-10-02 19:24:51 - train: epoch 0018, iter [00650, 01251], lr: 0.000590, loss: 0.4356
2022-10-02 19:25:10 - train: epoch 0018, iter [00660, 01251], lr: 0.000590, loss: 0.4325
2022-10-02 19:25:30 - train: epoch 0018, iter [00670, 01251], lr: 0.000590, loss: 0.4356
2022-10-02 19:25:50 - train: epoch 0018, iter [00680, 01251], lr: 0.000590, loss: 0.4164
2022-10-02 19:26:10 - train: epoch 0018, iter [00690, 01251], lr: 0.000590, loss: 0.4331
2022-10-02 19:26:30 - train: epoch 0018, iter [00700, 01251], lr: 0.000590, loss: 0.4360
2022-10-02 19:26:50 - train: epoch 0018, iter [00710, 01251], lr: 0.000590, loss: 0.4455
2022-10-02 19:27:10 - train: epoch 0018, iter [00720, 01251], lr: 0.000590, loss: 0.4207
2022-10-02 19:27:29 - train: epoch 0018, iter [00730, 01251], lr: 0.000590, loss: 0.4209
2022-10-02 19:27:49 - train: epoch 0018, iter [00740, 01251], lr: 0.000590, loss: 0.4127
2022-10-02 19:28:09 - train: epoch 0018, iter [00750, 01251], lr: 0.000590, loss: 0.4376
2022-10-02 19:28:29 - train: epoch 0018, iter [00760, 01251], lr: 0.000589, loss: 0.4254
2022-10-02 19:28:49 - train: epoch 0018, iter [00770, 01251], lr: 0.000589, loss: 0.4050
2022-10-02 19:29:08 - train: epoch 0018, iter [00780, 01251], lr: 0.000589, loss: 0.4363
2022-10-02 19:29:28 - train: epoch 0018, iter [00790, 01251], lr: 0.000589, loss: 0.4185
2022-10-02 19:29:48 - train: epoch 0018, iter [00800, 01251], lr: 0.000589, loss: 0.4090
2022-10-02 19:30:08 - train: epoch 0018, iter [00810, 01251], lr: 0.000589, loss: 0.4285
2022-10-02 19:30:27 - train: epoch 0018, iter [00820, 01251], lr: 0.000589, loss: 0.4248
2022-10-02 19:30:47 - train: epoch 0018, iter [00830, 01251], lr: 0.000589, loss: 0.4425
2022-10-02 19:31:07 - train: epoch 0018, iter [00840, 01251], lr: 0.000589, loss: 0.4362
2022-10-02 19:31:27 - train: epoch 0018, iter [00850, 01251], lr: 0.000589, loss: 0.4382
2022-10-02 19:31:46 - train: epoch 0018, iter [00860, 01251], lr: 0.000589, loss: 0.4341
2022-10-02 19:32:06 - train: epoch 0018, iter [00870, 01251], lr: 0.000589, loss: 0.4185
2022-10-02 19:32:26 - train: epoch 0018, iter [00880, 01251], lr: 0.000589, loss: 0.4288
2022-10-02 19:32:45 - train: epoch 0018, iter [00890, 01251], lr: 0.000589, loss: 0.4268
2022-10-02 19:33:05 - train: epoch 0018, iter [00900, 01251], lr: 0.000589, loss: 0.4316
2022-10-02 19:33:25 - train: epoch 0018, iter [00910, 01251], lr: 0.000589, loss: 0.4348
2022-10-02 19:33:45 - train: epoch 0018, iter [00920, 01251], lr: 0.000589, loss: 0.4346
2022-10-02 19:34:04 - train: epoch 0018, iter [00930, 01251], lr: 0.000589, loss: 0.4314
2022-10-02 19:34:24 - train: epoch 0018, iter [00940, 01251], lr: 0.000589, loss: 0.4246
2022-10-02 19:34:44 - train: epoch 0018, iter [00950, 01251], lr: 0.000589, loss: 0.4335
2022-10-02 19:35:04 - train: epoch 0018, iter [00960, 01251], lr: 0.000589, loss: 0.4268
2022-10-02 19:35:24 - train: epoch 0018, iter [00970, 01251], lr: 0.000589, loss: 0.4229
2022-10-02 19:35:43 - train: epoch 0018, iter [00980, 01251], lr: 0.000589, loss: 0.4342
2022-10-02 19:36:03 - train: epoch 0018, iter [00990, 01251], lr: 0.000589, loss: 0.4237
2022-10-02 19:36:23 - train: epoch 0018, iter [01000, 01251], lr: 0.000589, loss: 0.4293
2022-10-02 19:36:43 - train: epoch 0018, iter [01010, 01251], lr: 0.000589, loss: 0.4122
2022-10-02 19:37:02 - train: epoch 0018, iter [01020, 01251], lr: 0.000589, loss: 0.4253
2022-10-02 19:37:22 - train: epoch 0018, iter [01030, 01251], lr: 0.000589, loss: 0.4246
2022-10-02 19:37:42 - train: epoch 0018, iter [01040, 01251], lr: 0.000589, loss: 0.4378
2022-10-02 19:38:02 - train: epoch 0018, iter [01050, 01251], lr: 0.000589, loss: 0.4316
2022-10-02 19:38:22 - train: epoch 0018, iter [01060, 01251], lr: 0.000589, loss: 0.4242
2022-10-02 19:38:41 - train: epoch 0018, iter [01070, 01251], lr: 0.000589, loss: 0.4307
2022-10-02 19:39:01 - train: epoch 0018, iter [01080, 01251], lr: 0.000589, loss: 0.4434
2022-10-02 19:39:20 - train: epoch 0018, iter [01090, 01251], lr: 0.000589, loss: 0.3955
2022-10-02 19:39:40 - train: epoch 0018, iter [01100, 01251], lr: 0.000589, loss: 0.4358
2022-10-02 19:40:00 - train: epoch 0018, iter [01110, 01251], lr: 0.000589, loss: 0.4428
2022-10-02 19:40:20 - train: epoch 0018, iter [01120, 01251], lr: 0.000589, loss: 0.4208
2022-10-02 19:40:39 - train: epoch 0018, iter [01130, 01251], lr: 0.000589, loss: 0.4252
2022-10-02 19:40:59 - train: epoch 0018, iter [01140, 01251], lr: 0.000589, loss: 0.4132
2022-10-02 19:41:19 - train: epoch 0018, iter [01150, 01251], lr: 0.000589, loss: 0.4520
2022-10-02 19:41:39 - train: epoch 0018, iter [01160, 01251], lr: 0.000589, loss: 0.4371
2022-10-02 19:41:58 - train: epoch 0018, iter [01170, 01251], lr: 0.000589, loss: 0.4055
2022-10-02 19:42:18 - train: epoch 0018, iter [01180, 01251], lr: 0.000589, loss: 0.4299
2022-10-02 19:42:38 - train: epoch 0018, iter [01190, 01251], lr: 0.000589, loss: 0.4259
2022-10-02 19:42:58 - train: epoch 0018, iter [01200, 01251], lr: 0.000588, loss: 0.4303
2022-10-02 19:43:17 - train: epoch 0018, iter [01210, 01251], lr: 0.000588, loss: 0.4114
2022-10-02 19:43:37 - train: epoch 0018, iter [01220, 01251], lr: 0.000588, loss: 0.4149
2022-10-02 19:43:57 - train: epoch 0018, iter [01230, 01251], lr: 0.000588, loss: 0.4244
2022-10-02 19:44:17 - train: epoch 0018, iter [01240, 01251], lr: 0.000588, loss: 0.4265
2022-10-02 19:44:35 - train: epoch 0018, iter [01250, 01251], lr: 0.000588, loss: 0.4050
2022-10-02 19:44:38 - train: epoch 018, train_loss: 0.4269
2022-10-02 19:44:43 - until epoch: 018, best_loss: 0.4269
2022-10-02 19:44:43 - epoch 019 lr: 0.000588
2022-10-02 19:45:09 - train: epoch 0019, iter [00010, 01251], lr: 0.000588, loss: 0.4368
2022-10-02 19:45:29 - train: epoch 0019, iter [00020, 01251], lr: 0.000588, loss: 0.4427
2022-10-02 19:45:48 - train: epoch 0019, iter [00030, 01251], lr: 0.000588, loss: 0.4192
2022-10-02 19:46:08 - train: epoch 0019, iter [00040, 01251], lr: 0.000588, loss: 0.4252
2022-10-02 19:46:27 - train: epoch 0019, iter [00050, 01251], lr: 0.000588, loss: 0.4300
2022-10-02 19:46:47 - train: epoch 0019, iter [00060, 01251], lr: 0.000588, loss: 0.4394
2022-10-02 19:47:07 - train: epoch 0019, iter [00070, 01251], lr: 0.000588, loss: 0.4392
2022-10-02 19:47:27 - train: epoch 0019, iter [00080, 01251], lr: 0.000588, loss: 0.4112
2022-10-02 19:47:46 - train: epoch 0019, iter [00090, 01251], lr: 0.000588, loss: 0.4147
2022-10-02 19:48:06 - train: epoch 0019, iter [00100, 01251], lr: 0.000588, loss: 0.4300
2022-10-02 19:48:26 - train: epoch 0019, iter [00110, 01251], lr: 0.000588, loss: 0.4320
2022-10-02 19:48:45 - train: epoch 0019, iter [00120, 01251], lr: 0.000588, loss: 0.4113
2022-10-02 19:49:05 - train: epoch 0019, iter [00130, 01251], lr: 0.000588, loss: 0.4215
2022-10-02 19:49:25 - train: epoch 0019, iter [00140, 01251], lr: 0.000588, loss: 0.4092
2022-10-02 19:49:45 - train: epoch 0019, iter [00150, 01251], lr: 0.000588, loss: 0.4361
2022-10-02 19:50:05 - train: epoch 0019, iter [00160, 01251], lr: 0.000588, loss: 0.4386
2022-10-02 19:50:24 - train: epoch 0019, iter [00170, 01251], lr: 0.000588, loss: 0.4395
2022-10-02 19:50:44 - train: epoch 0019, iter [00180, 01251], lr: 0.000588, loss: 0.4071
2022-10-02 19:51:04 - train: epoch 0019, iter [00190, 01251], lr: 0.000588, loss: 0.4409
2022-10-02 19:51:24 - train: epoch 0019, iter [00200, 01251], lr: 0.000588, loss: 0.4352
2022-10-02 19:51:43 - train: epoch 0019, iter [00210, 01251], lr: 0.000588, loss: 0.4167
2022-10-02 19:52:03 - train: epoch 0019, iter [00220, 01251], lr: 0.000588, loss: 0.4336
2022-10-02 19:52:23 - train: epoch 0019, iter [00230, 01251], lr: 0.000588, loss: 0.4431
2022-10-02 19:52:43 - train: epoch 0019, iter [00240, 01251], lr: 0.000588, loss: 0.4269
2022-10-02 19:53:02 - train: epoch 0019, iter [00250, 01251], lr: 0.000588, loss: 0.4125
2022-10-02 19:53:22 - train: epoch 0019, iter [00260, 01251], lr: 0.000588, loss: 0.4363
2022-10-02 19:53:42 - train: epoch 0019, iter [00270, 01251], lr: 0.000588, loss: 0.4188
2022-10-02 19:54:02 - train: epoch 0019, iter [00280, 01251], lr: 0.000588, loss: 0.4298
2022-10-02 19:54:21 - train: epoch 0019, iter [00290, 01251], lr: 0.000588, loss: 0.4087
2022-10-02 19:54:41 - train: epoch 0019, iter [00300, 01251], lr: 0.000588, loss: 0.4206
2022-10-02 19:55:01 - train: epoch 0019, iter [00310, 01251], lr: 0.000588, loss: 0.4390
2022-10-02 19:55:21 - train: epoch 0019, iter [00320, 01251], lr: 0.000588, loss: 0.4356
2022-10-02 19:55:40 - train: epoch 0019, iter [00330, 01251], lr: 0.000588, loss: 0.4214
2022-10-02 19:56:00 - train: epoch 0019, iter [00340, 01251], lr: 0.000588, loss: 0.4303
2022-10-02 19:56:20 - train: epoch 0019, iter [00350, 01251], lr: 0.000588, loss: 0.4321
2022-10-02 19:56:40 - train: epoch 0019, iter [00360, 01251], lr: 0.000588, loss: 0.4279
2022-10-02 19:56:59 - train: epoch 0019, iter [00370, 01251], lr: 0.000588, loss: 0.4107
2022-10-02 19:57:19 - train: epoch 0019, iter [00380, 01251], lr: 0.000587, loss: 0.4245
2022-10-02 19:57:39 - train: epoch 0019, iter [00390, 01251], lr: 0.000587, loss: 0.4349
2022-10-02 19:57:59 - train: epoch 0019, iter [00400, 01251], lr: 0.000587, loss: 0.4251
2022-10-02 19:58:18 - train: epoch 0019, iter [00410, 01251], lr: 0.000587, loss: 0.4067
2022-10-02 19:58:39 - train: epoch 0019, iter [00420, 01251], lr: 0.000587, loss: 0.4319
2022-10-02 19:58:58 - train: epoch 0019, iter [00430, 01251], lr: 0.000587, loss: 0.4202
2022-10-02 19:59:18 - train: epoch 0019, iter [00440, 01251], lr: 0.000587, loss: 0.4253
2022-10-02 19:59:38 - train: epoch 0019, iter [00450, 01251], lr: 0.000587, loss: 0.4310
2022-10-02 19:59:58 - train: epoch 0019, iter [00460, 01251], lr: 0.000587, loss: 0.4088
2022-10-02 20:00:17 - train: epoch 0019, iter [00470, 01251], lr: 0.000587, loss: 0.4351
2022-10-02 20:00:37 - train: epoch 0019, iter [00480, 01251], lr: 0.000587, loss: 0.4203
2022-10-02 20:00:57 - train: epoch 0019, iter [00490, 01251], lr: 0.000587, loss: 0.4141
2022-10-02 20:01:17 - train: epoch 0019, iter [00500, 01251], lr: 0.000587, loss: 0.3866
2022-10-02 20:01:37 - train: epoch 0019, iter [00510, 01251], lr: 0.000587, loss: 0.4320
2022-10-02 20:01:56 - train: epoch 0019, iter [00520, 01251], lr: 0.000587, loss: 0.4282
2022-10-02 20:02:16 - train: epoch 0019, iter [00530, 01251], lr: 0.000587, loss: 0.4325
2022-10-02 20:02:36 - train: epoch 0019, iter [00540, 01251], lr: 0.000587, loss: 0.4257
2022-10-02 20:02:56 - train: epoch 0019, iter [00550, 01251], lr: 0.000587, loss: 0.4241
2022-10-02 20:03:16 - train: epoch 0019, iter [00560, 01251], lr: 0.000587, loss: 0.4198
2022-10-02 20:03:35 - train: epoch 0019, iter [00570, 01251], lr: 0.000587, loss: 0.4159
2022-10-02 20:03:55 - train: epoch 0019, iter [00580, 01251], lr: 0.000587, loss: 0.4406
2022-10-02 20:04:15 - train: epoch 0019, iter [00590, 01251], lr: 0.000587, loss: 0.4140
2022-10-02 20:04:34 - train: epoch 0019, iter [00600, 01251], lr: 0.000587, loss: 0.4231
2022-10-02 20:04:54 - train: epoch 0019, iter [00610, 01251], lr: 0.000587, loss: 0.4186
2022-10-02 20:05:14 - train: epoch 0019, iter [00620, 01251], lr: 0.000587, loss: 0.4238
2022-10-02 20:05:33 - train: epoch 0019, iter [00630, 01251], lr: 0.000587, loss: 0.4360
2022-10-02 20:05:53 - train: epoch 0019, iter [00640, 01251], lr: 0.000587, loss: 0.4301
2022-10-02 20:06:13 - train: epoch 0019, iter [00650, 01251], lr: 0.000587, loss: 0.4252
2022-10-02 20:06:33 - train: epoch 0019, iter [00660, 01251], lr: 0.000587, loss: 0.4303
2022-10-02 20:06:52 - train: epoch 0019, iter [00670, 01251], lr: 0.000587, loss: 0.4298
2022-10-02 20:07:12 - train: epoch 0019, iter [00680, 01251], lr: 0.000587, loss: 0.4123
2022-10-02 20:07:32 - train: epoch 0019, iter [00690, 01251], lr: 0.000587, loss: 0.4404
2022-10-02 20:07:52 - train: epoch 0019, iter [00700, 01251], lr: 0.000587, loss: 0.4153
2022-10-02 20:08:11 - train: epoch 0019, iter [00710, 01251], lr: 0.000587, loss: 0.4307
2022-10-02 20:08:31 - train: epoch 0019, iter [00720, 01251], lr: 0.000587, loss: 0.4399
2022-10-02 20:08:51 - train: epoch 0019, iter [00730, 01251], lr: 0.000587, loss: 0.4369
2022-10-02 20:09:11 - train: epoch 0019, iter [00740, 01251], lr: 0.000587, loss: 0.4405
2022-10-02 20:09:30 - train: epoch 0019, iter [00750, 01251], lr: 0.000587, loss: 0.4188
2022-10-02 20:09:50 - train: epoch 0019, iter [00760, 01251], lr: 0.000587, loss: 0.4682
2022-10-02 20:10:10 - train: epoch 0019, iter [00770, 01251], lr: 0.000587, loss: 0.4207
2022-10-02 20:10:29 - train: epoch 0019, iter [00780, 01251], lr: 0.000587, loss: 0.4115
2022-10-02 20:10:49 - train: epoch 0019, iter [00790, 01251], lr: 0.000586, loss: 0.4307
2022-10-02 20:11:09 - train: epoch 0019, iter [00800, 01251], lr: 0.000586, loss: 0.4298
2022-10-02 20:11:29 - train: epoch 0019, iter [00810, 01251], lr: 0.000586, loss: 0.4164
2022-10-02 20:11:48 - train: epoch 0019, iter [00820, 01251], lr: 0.000586, loss: 0.4229
2022-10-02 20:12:08 - train: epoch 0019, iter [00830, 01251], lr: 0.000586, loss: 0.4462
2022-10-02 20:12:28 - train: epoch 0019, iter [00840, 01251], lr: 0.000586, loss: 0.4099
2022-10-02 20:12:48 - train: epoch 0019, iter [00850, 01251], lr: 0.000586, loss: 0.4231
2022-10-02 20:13:07 - train: epoch 0019, iter [00860, 01251], lr: 0.000586, loss: 0.4107
2022-10-02 20:13:27 - train: epoch 0019, iter [00870, 01251], lr: 0.000586, loss: 0.4287
2022-10-02 20:13:47 - train: epoch 0019, iter [00880, 01251], lr: 0.000586, loss: 0.4221
2022-10-02 20:14:06 - train: epoch 0019, iter [00890, 01251], lr: 0.000586, loss: 0.4097
2022-10-02 20:14:26 - train: epoch 0019, iter [00900, 01251], lr: 0.000586, loss: 0.4170
2022-10-02 20:14:46 - train: epoch 0019, iter [00910, 01251], lr: 0.000586, loss: 0.4195
2022-10-02 20:15:06 - train: epoch 0019, iter [00920, 01251], lr: 0.000586, loss: 0.4409
2022-10-02 20:15:26 - train: epoch 0019, iter [00930, 01251], lr: 0.000586, loss: 0.4229
2022-10-02 20:15:45 - train: epoch 0019, iter [00940, 01251], lr: 0.000586, loss: 0.4187
2022-10-02 20:16:05 - train: epoch 0019, iter [00950, 01251], lr: 0.000586, loss: 0.4353
2022-10-02 20:16:25 - train: epoch 0019, iter [00960, 01251], lr: 0.000586, loss: 0.4206
2022-10-02 20:16:44 - train: epoch 0019, iter [00970, 01251], lr: 0.000586, loss: 0.4093
2022-10-02 20:17:04 - train: epoch 0019, iter [00980, 01251], lr: 0.000586, loss: 0.4295
2022-10-02 20:17:24 - train: epoch 0019, iter [00990, 01251], lr: 0.000586, loss: 0.4224
2022-10-02 20:17:44 - train: epoch 0019, iter [01000, 01251], lr: 0.000586, loss: 0.4341
2022-10-02 20:18:03 - train: epoch 0019, iter [01010, 01251], lr: 0.000586, loss: 0.4229
2022-10-02 20:18:23 - train: epoch 0019, iter [01020, 01251], lr: 0.000586, loss: 0.4252
2022-10-02 20:18:43 - train: epoch 0019, iter [01030, 01251], lr: 0.000586, loss: 0.4260
2022-10-02 20:19:03 - train: epoch 0019, iter [01040, 01251], lr: 0.000586, loss: 0.4135
2022-10-02 20:19:23 - train: epoch 0019, iter [01050, 01251], lr: 0.000586, loss: 0.4425
2022-10-02 20:19:43 - train: epoch 0019, iter [01060, 01251], lr: 0.000586, loss: 0.4348
2022-10-02 20:20:03 - train: epoch 0019, iter [01070, 01251], lr: 0.000586, loss: 0.4193
2022-10-02 20:20:22 - train: epoch 0019, iter [01080, 01251], lr: 0.000586, loss: 0.4236
2022-10-02 20:20:42 - train: epoch 0019, iter [01090, 01251], lr: 0.000586, loss: 0.4145
2022-10-02 20:21:02 - train: epoch 0019, iter [01100, 01251], lr: 0.000586, loss: 0.4360
2022-10-02 20:21:21 - train: epoch 0019, iter [01110, 01251], lr: 0.000586, loss: 0.4278
2022-10-02 20:21:41 - train: epoch 0019, iter [01120, 01251], lr: 0.000586, loss: 0.4229
2022-10-02 20:22:01 - train: epoch 0019, iter [01130, 01251], lr: 0.000586, loss: 0.4273
2022-10-02 20:22:20 - train: epoch 0019, iter [01140, 01251], lr: 0.000586, loss: 0.4205
2022-10-02 20:22:40 - train: epoch 0019, iter [01150, 01251], lr: 0.000586, loss: 0.4330
2022-10-02 20:23:00 - train: epoch 0019, iter [01160, 01251], lr: 0.000586, loss: 0.4297
2022-10-02 20:23:20 - train: epoch 0019, iter [01170, 01251], lr: 0.000586, loss: 0.4269
2022-10-02 20:23:40 - train: epoch 0019, iter [01180, 01251], lr: 0.000586, loss: 0.4360
2022-10-02 20:24:00 - train: epoch 0019, iter [01190, 01251], lr: 0.000585, loss: 0.4303
2022-10-02 20:24:19 - train: epoch 0019, iter [01200, 01251], lr: 0.000585, loss: 0.4263
2022-10-02 20:24:39 - train: epoch 0019, iter [01210, 01251], lr: 0.000585, loss: 0.4308
2022-10-02 20:24:59 - train: epoch 0019, iter [01220, 01251], lr: 0.000585, loss: 0.4400
2022-10-02 20:25:19 - train: epoch 0019, iter [01230, 01251], lr: 0.000585, loss: 0.4281
2022-10-02 20:25:39 - train: epoch 0019, iter [01240, 01251], lr: 0.000585, loss: 0.4282
2022-10-02 20:25:57 - train: epoch 0019, iter [01250, 01251], lr: 0.000585, loss: 0.4228
2022-10-02 20:26:01 - train: epoch 019, train_loss: 0.4260
2022-10-02 20:26:05 - until epoch: 019, best_loss: 0.4260
2022-10-02 20:26:05 - epoch 020 lr: 0.000585
2022-10-02 20:26:32 - train: epoch 0020, iter [00010, 01251], lr: 0.000585, loss: 0.4377
2022-10-02 20:26:52 - train: epoch 0020, iter [00020, 01251], lr: 0.000585, loss: 0.4221
2022-10-02 20:27:11 - train: epoch 0020, iter [00030, 01251], lr: 0.000585, loss: 0.4140
2022-10-02 20:27:31 - train: epoch 0020, iter [00040, 01251], lr: 0.000585, loss: 0.4124
2022-10-02 20:27:51 - train: epoch 0020, iter [00050, 01251], lr: 0.000585, loss: 0.4119
2022-10-02 20:28:11 - train: epoch 0020, iter [00060, 01251], lr: 0.000585, loss: 0.4088
2022-10-02 20:28:30 - train: epoch 0020, iter [00070, 01251], lr: 0.000585, loss: 0.4252
2022-10-02 20:28:50 - train: epoch 0020, iter [00080, 01251], lr: 0.000585, loss: 0.4273
2022-10-02 20:29:10 - train: epoch 0020, iter [00090, 01251], lr: 0.000585, loss: 0.4406
2022-10-02 20:29:30 - train: epoch 0020, iter [00100, 01251], lr: 0.000585, loss: 0.4193
2022-10-02 20:29:50 - train: epoch 0020, iter [00110, 01251], lr: 0.000585, loss: 0.4377
2022-10-02 20:30:10 - train: epoch 0020, iter [00120, 01251], lr: 0.000585, loss: 0.4345
2022-10-02 20:30:30 - train: epoch 0020, iter [00130, 01251], lr: 0.000585, loss: 0.4167
2022-10-02 20:30:50 - train: epoch 0020, iter [00140, 01251], lr: 0.000585, loss: 0.4231
2022-10-02 20:31:09 - train: epoch 0020, iter [00150, 01251], lr: 0.000585, loss: 0.4271
2022-10-02 20:31:29 - train: epoch 0020, iter [00160, 01251], lr: 0.000585, loss: 0.4304
2022-10-02 20:31:50 - train: epoch 0020, iter [00170, 01251], lr: 0.000585, loss: 0.4217
2022-10-02 20:32:10 - train: epoch 0020, iter [00180, 01251], lr: 0.000585, loss: 0.4177
2022-10-02 20:32:30 - train: epoch 0020, iter [00190, 01251], lr: 0.000585, loss: 0.4300
2022-10-02 20:32:50 - train: epoch 0020, iter [00200, 01251], lr: 0.000585, loss: 0.4091
2022-10-02 20:33:10 - train: epoch 0020, iter [00210, 01251], lr: 0.000585, loss: 0.4298
2022-10-02 20:33:30 - train: epoch 0020, iter [00220, 01251], lr: 0.000585, loss: 0.4186
2022-10-02 20:33:49 - train: epoch 0020, iter [00230, 01251], lr: 0.000585, loss: 0.4433
2022-10-02 20:34:09 - train: epoch 0020, iter [00240, 01251], lr: 0.000585, loss: 0.4340
2022-10-02 20:34:29 - train: epoch 0020, iter [00250, 01251], lr: 0.000585, loss: 0.4023
2022-10-02 20:34:49 - train: epoch 0020, iter [00260, 01251], lr: 0.000585, loss: 0.4557
2022-10-02 20:35:09 - train: epoch 0020, iter [00270, 01251], lr: 0.000585, loss: 0.4219
2022-10-02 20:35:29 - train: epoch 0020, iter [00280, 01251], lr: 0.000585, loss: 0.4254
2022-10-02 20:35:49 - train: epoch 0020, iter [00290, 01251], lr: 0.000585, loss: 0.4109
2022-10-02 20:36:09 - train: epoch 0020, iter [00300, 01251], lr: 0.000585, loss: 0.4306
2022-10-02 20:36:28 - train: epoch 0020, iter [00310, 01251], lr: 0.000585, loss: 0.4263
2022-10-02 20:36:48 - train: epoch 0020, iter [00320, 01251], lr: 0.000584, loss: 0.4135
2022-10-02 20:37:08 - train: epoch 0020, iter [00330, 01251], lr: 0.000584, loss: 0.4110
2022-10-02 20:37:28 - train: epoch 0020, iter [00340, 01251], lr: 0.000584, loss: 0.4324
2022-10-02 20:37:48 - train: epoch 0020, iter [00350, 01251], lr: 0.000584, loss: 0.4121
2022-10-02 20:38:08 - train: epoch 0020, iter [00360, 01251], lr: 0.000584, loss: 0.4321
2022-10-02 20:38:28 - train: epoch 0020, iter [00370, 01251], lr: 0.000584, loss: 0.4140
2022-10-02 20:38:48 - train: epoch 0020, iter [00380, 01251], lr: 0.000584, loss: 0.4290
2022-10-02 20:39:08 - train: epoch 0020, iter [00390, 01251], lr: 0.000584, loss: 0.4083
2022-10-02 20:39:28 - train: epoch 0020, iter [00400, 01251], lr: 0.000584, loss: 0.4313
2022-10-02 20:39:48 - train: epoch 0020, iter [00410, 01251], lr: 0.000584, loss: 0.4168
2022-10-02 20:40:08 - train: epoch 0020, iter [00420, 01251], lr: 0.000584, loss: 0.4177
2022-10-02 20:40:28 - train: epoch 0020, iter [00430, 01251], lr: 0.000584, loss: 0.4232
2022-10-02 20:40:48 - train: epoch 0020, iter [00440, 01251], lr: 0.000584, loss: 0.3989
2022-10-02 20:41:07 - train: epoch 0020, iter [00450, 01251], lr: 0.000584, loss: 0.4241
2022-10-02 20:41:27 - train: epoch 0020, iter [00460, 01251], lr: 0.000584, loss: 0.4256
2022-10-02 20:41:47 - train: epoch 0020, iter [00470, 01251], lr: 0.000584, loss: 0.4178
2022-10-02 20:42:07 - train: epoch 0020, iter [00480, 01251], lr: 0.000584, loss: 0.4251
2022-10-02 20:42:27 - train: epoch 0020, iter [00490, 01251], lr: 0.000584, loss: 0.4134
2022-10-02 20:42:46 - train: epoch 0020, iter [00500, 01251], lr: 0.000584, loss: 0.4144
2022-10-02 20:43:07 - train: epoch 0020, iter [00510, 01251], lr: 0.000584, loss: 0.4391
2022-10-02 20:43:26 - train: epoch 0020, iter [00520, 01251], lr: 0.000584, loss: 0.4032
2022-10-02 20:43:46 - train: epoch 0020, iter [00530, 01251], lr: 0.000584, loss: 0.4316
2022-10-02 20:44:07 - train: epoch 0020, iter [00540, 01251], lr: 0.000584, loss: 0.4371
2022-10-02 20:44:26 - train: epoch 0020, iter [00550, 01251], lr: 0.000584, loss: 0.4279
2022-10-02 20:44:46 - train: epoch 0020, iter [00560, 01251], lr: 0.000584, loss: 0.4194
2022-10-02 20:45:06 - train: epoch 0020, iter [00570, 01251], lr: 0.000584, loss: 0.4140
2022-10-02 20:45:26 - train: epoch 0020, iter [00580, 01251], lr: 0.000584, loss: 0.4198
2022-10-02 20:45:46 - train: epoch 0020, iter [00590, 01251], lr: 0.000584, loss: 0.4088
2022-10-02 20:46:06 - train: epoch 0020, iter [00600, 01251], lr: 0.000584, loss: 0.4163
2022-10-02 20:46:26 - train: epoch 0020, iter [00610, 01251], lr: 0.000584, loss: 0.4191
2022-10-02 20:46:47 - train: epoch 0020, iter [00620, 01251], lr: 0.000584, loss: 0.4123
2022-10-02 20:47:07 - train: epoch 0020, iter [00630, 01251], lr: 0.000584, loss: 0.4094
2022-10-02 20:47:27 - train: epoch 0020, iter [00640, 01251], lr: 0.000584, loss: 0.4409
2022-10-02 20:47:47 - train: epoch 0020, iter [00650, 01251], lr: 0.000584, loss: 0.4273
2022-10-02 20:48:07 - train: epoch 0020, iter [00660, 01251], lr: 0.000584, loss: 0.4391
2022-10-02 20:48:27 - train: epoch 0020, iter [00670, 01251], lr: 0.000584, loss: 0.4224
2022-10-02 20:48:47 - train: epoch 0020, iter [00680, 01251], lr: 0.000584, loss: 0.4464
2022-10-02 20:49:06 - train: epoch 0020, iter [00690, 01251], lr: 0.000583, loss: 0.4365
2022-10-02 20:49:26 - train: epoch 0020, iter [00700, 01251], lr: 0.000583, loss: 0.4092
2022-10-02 20:49:46 - train: epoch 0020, iter [00710, 01251], lr: 0.000583, loss: 0.4405
2022-10-02 20:50:06 - train: epoch 0020, iter [00720, 01251], lr: 0.000583, loss: 0.4232
2022-10-02 20:50:26 - train: epoch 0020, iter [00730, 01251], lr: 0.000583, loss: 0.4207
2022-10-02 20:50:47 - train: epoch 0020, iter [00740, 01251], lr: 0.000583, loss: 0.4042
2022-10-02 20:51:07 - train: epoch 0020, iter [00750, 01251], lr: 0.000583, loss: 0.4322
2022-10-02 20:51:26 - train: epoch 0020, iter [00760, 01251], lr: 0.000583, loss: 0.4141
2022-10-02 20:51:46 - train: epoch 0020, iter [00770, 01251], lr: 0.000583, loss: 0.4019
2022-10-02 20:52:05 - train: epoch 0020, iter [00780, 01251], lr: 0.000583, loss: 0.4265
2022-10-02 20:52:25 - train: epoch 0020, iter [00790, 01251], lr: 0.000583, loss: 0.4393
2022-10-02 20:52:45 - train: epoch 0020, iter [00800, 01251], lr: 0.000583, loss: 0.4270
2022-10-02 20:53:05 - train: epoch 0020, iter [00810, 01251], lr: 0.000583, loss: 0.4325
2022-10-02 20:53:24 - train: epoch 0020, iter [00820, 01251], lr: 0.000583, loss: 0.4395
2022-10-02 20:53:45 - train: epoch 0020, iter [00830, 01251], lr: 0.000583, loss: 0.4211
2022-10-02 20:54:04 - train: epoch 0020, iter [00840, 01251], lr: 0.000583, loss: 0.4218
2022-10-02 20:54:25 - train: epoch 0020, iter [00850, 01251], lr: 0.000583, loss: 0.4063
2022-10-02 20:54:45 - train: epoch 0020, iter [00860, 01251], lr: 0.000583, loss: 0.4417
2022-10-02 20:55:05 - train: epoch 0020, iter [00870, 01251], lr: 0.000583, loss: 0.4289
2022-10-02 20:55:25 - train: epoch 0020, iter [00880, 01251], lr: 0.000583, loss: 0.4357
2022-10-02 20:55:45 - train: epoch 0020, iter [00890, 01251], lr: 0.000583, loss: 0.4178
2022-10-02 20:56:05 - train: epoch 0020, iter [00900, 01251], lr: 0.000583, loss: 0.4480
2022-10-02 20:56:25 - train: epoch 0020, iter [00910, 01251], lr: 0.000583, loss: 0.4098
2022-10-02 20:56:45 - train: epoch 0020, iter [00920, 01251], lr: 0.000583, loss: 0.4252
2022-10-02 20:57:05 - train: epoch 0020, iter [00930, 01251], lr: 0.000583, loss: 0.4159
2022-10-02 20:57:25 - train: epoch 0020, iter [00940, 01251], lr: 0.000583, loss: 0.4274
2022-10-02 20:57:45 - train: epoch 0020, iter [00950, 01251], lr: 0.000583, loss: 0.4159
2022-10-02 20:58:05 - train: epoch 0020, iter [00960, 01251], lr: 0.000583, loss: 0.4185
2022-10-02 20:58:25 - train: epoch 0020, iter [00970, 01251], lr: 0.000583, loss: 0.4099
2022-10-02 20:58:46 - train: epoch 0020, iter [00980, 01251], lr: 0.000583, loss: 0.4130
2022-10-02 20:59:05 - train: epoch 0020, iter [00990, 01251], lr: 0.000583, loss: 0.4177
2022-10-02 20:59:25 - train: epoch 0020, iter [01000, 01251], lr: 0.000583, loss: 0.4316
2022-10-02 20:59:45 - train: epoch 0020, iter [01010, 01251], lr: 0.000583, loss: 0.4498
2022-10-02 21:00:05 - train: epoch 0020, iter [01020, 01251], lr: 0.000583, loss: 0.4136
2022-10-02 21:00:25 - train: epoch 0020, iter [01030, 01251], lr: 0.000583, loss: 0.4160
2022-10-02 21:00:45 - train: epoch 0020, iter [01040, 01251], lr: 0.000583, loss: 0.4242
2022-10-02 21:01:05 - train: epoch 0020, iter [01050, 01251], lr: 0.000582, loss: 0.4248
2022-10-02 21:01:25 - train: epoch 0020, iter [01060, 01251], lr: 0.000582, loss: 0.4310
2022-10-02 21:01:45 - train: epoch 0020, iter [01070, 01251], lr: 0.000582, loss: 0.4215
2022-10-02 21:02:05 - train: epoch 0020, iter [01080, 01251], lr: 0.000582, loss: 0.4298
2022-10-02 21:02:26 - train: epoch 0020, iter [01090, 01251], lr: 0.000582, loss: 0.4303
2022-10-02 21:02:45 - train: epoch 0020, iter [01100, 01251], lr: 0.000582, loss: 0.4275
2022-10-02 21:03:06 - train: epoch 0020, iter [01110, 01251], lr: 0.000582, loss: 0.4267
2022-10-02 21:03:26 - train: epoch 0020, iter [01120, 01251], lr: 0.000582, loss: 0.4300
2022-10-02 21:03:46 - train: epoch 0020, iter [01130, 01251], lr: 0.000582, loss: 0.4241
2022-10-02 21:04:06 - train: epoch 0020, iter [01140, 01251], lr: 0.000582, loss: 0.4252
2022-10-02 21:04:26 - train: epoch 0020, iter [01150, 01251], lr: 0.000582, loss: 0.4240
2022-10-02 21:04:46 - train: epoch 0020, iter [01160, 01251], lr: 0.000582, loss: 0.4230
2022-10-02 21:05:07 - train: epoch 0020, iter [01170, 01251], lr: 0.000582, loss: 0.4085
2022-10-02 21:05:26 - train: epoch 0020, iter [01180, 01251], lr: 0.000582, loss: 0.4308
2022-10-02 21:05:46 - train: epoch 0020, iter [01190, 01251], lr: 0.000582, loss: 0.4195
2022-10-02 21:06:06 - train: epoch 0020, iter [01200, 01251], lr: 0.000582, loss: 0.4226
2022-10-02 21:06:26 - train: epoch 0020, iter [01210, 01251], lr: 0.000582, loss: 0.4258
2022-10-02 21:06:47 - train: epoch 0020, iter [01220, 01251], lr: 0.000582, loss: 0.4090
2022-10-02 21:07:07 - train: epoch 0020, iter [01230, 01251], lr: 0.000582, loss: 0.4120
2022-10-02 21:07:27 - train: epoch 0020, iter [01240, 01251], lr: 0.000582, loss: 0.4300
2022-10-02 21:07:46 - train: epoch 0020, iter [01250, 01251], lr: 0.000582, loss: 0.4532
2022-10-02 21:07:49 - train: epoch 020, train_loss: 0.4251
2022-10-02 21:07:54 - until epoch: 020, best_loss: 0.4251
2022-10-02 21:07:54 - epoch 021 lr: 0.000582
2022-10-02 21:08:20 - train: epoch 0021, iter [00010, 01251], lr: 0.000582, loss: 0.4121
2022-10-02 21:08:40 - train: epoch 0021, iter [00020, 01251], lr: 0.000582, loss: 0.4238
2022-10-02 21:09:00 - train: epoch 0021, iter [00030, 01251], lr: 0.000582, loss: 0.4171
2022-10-02 21:09:19 - train: epoch 0021, iter [00040, 01251], lr: 0.000582, loss: 0.4181
2022-10-02 21:09:39 - train: epoch 0021, iter [00050, 01251], lr: 0.000582, loss: 0.4148
2022-10-02 21:09:59 - train: epoch 0021, iter [00060, 01251], lr: 0.000582, loss: 0.4139
2022-10-02 21:10:19 - train: epoch 0021, iter [00070, 01251], lr: 0.000582, loss: 0.4243
2022-10-02 21:10:38 - train: epoch 0021, iter [00080, 01251], lr: 0.000582, loss: 0.4358
2022-10-02 21:10:58 - train: epoch 0021, iter [00090, 01251], lr: 0.000582, loss: 0.4467
2022-10-02 21:11:18 - train: epoch 0021, iter [00100, 01251], lr: 0.000582, loss: 0.4318
2022-10-02 21:11:37 - train: epoch 0021, iter [00110, 01251], lr: 0.000582, loss: 0.4220
2022-10-02 21:11:57 - train: epoch 0021, iter [00120, 01251], lr: 0.000582, loss: 0.4305
2022-10-02 21:12:17 - train: epoch 0021, iter [00130, 01251], lr: 0.000582, loss: 0.4206
2022-10-02 21:12:37 - train: epoch 0021, iter [00140, 01251], lr: 0.000582, loss: 0.4371
2022-10-02 21:12:56 - train: epoch 0021, iter [00150, 01251], lr: 0.000581, loss: 0.4169
2022-10-02 21:13:16 - train: epoch 0021, iter [00160, 01251], lr: 0.000581, loss: 0.4200
2022-10-02 21:13:36 - train: epoch 0021, iter [00170, 01251], lr: 0.000581, loss: 0.4388
2022-10-02 21:13:56 - train: epoch 0021, iter [00180, 01251], lr: 0.000581, loss: 0.4131
2022-10-02 21:14:16 - train: epoch 0021, iter [00190, 01251], lr: 0.000581, loss: 0.4238
2022-10-02 21:14:36 - train: epoch 0021, iter [00200, 01251], lr: 0.000581, loss: 0.4315
2022-10-02 21:14:55 - train: epoch 0021, iter [00210, 01251], lr: 0.000581, loss: 0.4392
2022-10-02 21:15:15 - train: epoch 0021, iter [00220, 01251], lr: 0.000581, loss: 0.4236
2022-10-02 21:15:35 - train: epoch 0021, iter [00230, 01251], lr: 0.000581, loss: 0.4140
2022-10-02 21:15:55 - train: epoch 0021, iter [00240, 01251], lr: 0.000581, loss: 0.4335
2022-10-02 21:16:15 - train: epoch 0021, iter [00250, 01251], lr: 0.000581, loss: 0.4453
2022-10-02 21:16:35 - train: epoch 0021, iter [00260, 01251], lr: 0.000581, loss: 0.4227
2022-10-02 21:16:55 - train: epoch 0021, iter [00270, 01251], lr: 0.000581, loss: 0.4330
2022-10-02 21:17:14 - train: epoch 0021, iter [00280, 01251], lr: 0.000581, loss: 0.4006
2022-10-02 21:17:34 - train: epoch 0021, iter [00290, 01251], lr: 0.000581, loss: 0.4274
2022-10-02 21:17:54 - train: epoch 0021, iter [00300, 01251], lr: 0.000581, loss: 0.4371
2022-10-02 21:18:14 - train: epoch 0021, iter [00310, 01251], lr: 0.000581, loss: 0.4223
2022-10-02 21:18:34 - train: epoch 0021, iter [00320, 01251], lr: 0.000581, loss: 0.4158
2022-10-02 21:18:54 - train: epoch 0021, iter [00330, 01251], lr: 0.000581, loss: 0.4140
2022-10-02 21:19:13 - train: epoch 0021, iter [00340, 01251], lr: 0.000581, loss: 0.4328
2022-10-02 21:19:33 - train: epoch 0021, iter [00350, 01251], lr: 0.000581, loss: 0.4068
2022-10-02 21:19:53 - train: epoch 0021, iter [00360, 01251], lr: 0.000581, loss: 0.3991
2022-10-02 21:20:12 - train: epoch 0021, iter [00370, 01251], lr: 0.000581, loss: 0.4152
2022-10-02 21:20:32 - train: epoch 0021, iter [00380, 01251], lr: 0.000581, loss: 0.4220
2022-10-02 21:20:52 - train: epoch 0021, iter [00390, 01251], lr: 0.000581, loss: 0.4288
2022-10-02 21:21:12 - train: epoch 0021, iter [00400, 01251], lr: 0.000581, loss: 0.4085
2022-10-02 21:21:32 - train: epoch 0021, iter [00410, 01251], lr: 0.000581, loss: 0.4229
2022-10-02 21:21:52 - train: epoch 0021, iter [00420, 01251], lr: 0.000581, loss: 0.4181
2022-10-02 21:22:11 - train: epoch 0021, iter [00430, 01251], lr: 0.000581, loss: 0.4291
2022-10-02 21:22:31 - train: epoch 0021, iter [00440, 01251], lr: 0.000581, loss: 0.4154
2022-10-02 21:22:51 - train: epoch 0021, iter [00450, 01251], lr: 0.000581, loss: 0.4279
2022-10-02 21:23:11 - train: epoch 0021, iter [00460, 01251], lr: 0.000581, loss: 0.4188
2022-10-02 21:23:31 - train: epoch 0021, iter [00470, 01251], lr: 0.000581, loss: 0.4263
2022-10-02 21:23:51 - train: epoch 0021, iter [00480, 01251], lr: 0.000581, loss: 0.4381
2022-10-02 21:24:10 - train: epoch 0021, iter [00490, 01251], lr: 0.000580, loss: 0.4371
2022-10-02 21:24:31 - train: epoch 0021, iter [00500, 01251], lr: 0.000580, loss: 0.4038
2022-10-02 21:24:50 - train: epoch 0021, iter [00510, 01251], lr: 0.000580, loss: 0.4370
2022-10-02 21:25:10 - train: epoch 0021, iter [00520, 01251], lr: 0.000580, loss: 0.4069
2022-10-02 21:25:30 - train: epoch 0021, iter [00530, 01251], lr: 0.000580, loss: 0.4208
2022-10-02 21:25:50 - train: epoch 0021, iter [00540, 01251], lr: 0.000580, loss: 0.4191
2022-10-02 21:26:10 - train: epoch 0021, iter [00550, 01251], lr: 0.000580, loss: 0.4231
2022-10-02 21:26:29 - train: epoch 0021, iter [00560, 01251], lr: 0.000580, loss: 0.4324
2022-10-02 21:26:49 - train: epoch 0021, iter [00570, 01251], lr: 0.000580, loss: 0.4306
2022-10-02 21:27:09 - train: epoch 0021, iter [00580, 01251], lr: 0.000580, loss: 0.4352
2022-10-02 21:27:28 - train: epoch 0021, iter [00590, 01251], lr: 0.000580, loss: 0.4248
2022-10-02 21:27:48 - train: epoch 0021, iter [00600, 01251], lr: 0.000580, loss: 0.4261
2022-10-02 21:28:08 - train: epoch 0021, iter [00610, 01251], lr: 0.000580, loss: 0.4372
2022-10-02 21:28:27 - train: epoch 0021, iter [00620, 01251], lr: 0.000580, loss: 0.4196
2022-10-02 21:28:47 - train: epoch 0021, iter [00630, 01251], lr: 0.000580, loss: 0.4332
2022-10-02 21:29:07 - train: epoch 0021, iter [00640, 01251], lr: 0.000580, loss: 0.4325
2022-10-02 21:29:27 - train: epoch 0021, iter [00650, 01251], lr: 0.000580, loss: 0.4328
2022-10-02 21:29:47 - train: epoch 0021, iter [00660, 01251], lr: 0.000580, loss: 0.4357
2022-10-02 21:30:07 - train: epoch 0021, iter [00670, 01251], lr: 0.000580, loss: 0.4159
2022-10-02 21:30:26 - train: epoch 0021, iter [00680, 01251], lr: 0.000580, loss: 0.4156
2022-10-02 21:30:46 - train: epoch 0021, iter [00690, 01251], lr: 0.000580, loss: 0.4201
2022-10-02 21:31:06 - train: epoch 0021, iter [00700, 01251], lr: 0.000580, loss: 0.4287
2022-10-02 21:31:26 - train: epoch 0021, iter [00710, 01251], lr: 0.000580, loss: 0.4171
2022-10-02 21:31:46 - train: epoch 0021, iter [00720, 01251], lr: 0.000580, loss: 0.4090
2022-10-02 21:32:05 - train: epoch 0021, iter [00730, 01251], lr: 0.000580, loss: 0.4196
2022-10-02 21:32:25 - train: epoch 0021, iter [00740, 01251], lr: 0.000580, loss: 0.4305
2022-10-02 21:32:44 - train: epoch 0021, iter [00750, 01251], lr: 0.000580, loss: 0.4177
2022-10-02 21:33:04 - train: epoch 0021, iter [00760, 01251], lr: 0.000580, loss: 0.4264
2022-10-02 21:33:24 - train: epoch 0021, iter [00770, 01251], lr: 0.000580, loss: 0.4476
2022-10-02 21:33:43 - train: epoch 0021, iter [00780, 01251], lr: 0.000580, loss: 0.4133
2022-10-02 21:34:03 - train: epoch 0021, iter [00790, 01251], lr: 0.000580, loss: 0.4288
2022-10-02 21:34:23 - train: epoch 0021, iter [00800, 01251], lr: 0.000580, loss: 0.4009
2022-10-02 21:34:43 - train: epoch 0021, iter [00810, 01251], lr: 0.000580, loss: 0.4310
2022-10-02 21:35:02 - train: epoch 0021, iter [00820, 01251], lr: 0.000579, loss: 0.4054
2022-10-02 21:35:22 - train: epoch 0021, iter [00830, 01251], lr: 0.000579, loss: 0.4242
2022-10-02 21:35:42 - train: epoch 0021, iter [00840, 01251], lr: 0.000579, loss: 0.4278
2022-10-02 21:36:02 - train: epoch 0021, iter [00850, 01251], lr: 0.000579, loss: 0.4210
2022-10-02 21:36:22 - train: epoch 0021, iter [00860, 01251], lr: 0.000579, loss: 0.4350
2022-10-02 21:36:42 - train: epoch 0021, iter [00870, 01251], lr: 0.000579, loss: 0.4152
2022-10-02 21:37:01 - train: epoch 0021, iter [00880, 01251], lr: 0.000579, loss: 0.4432
2022-10-02 21:37:21 - train: epoch 0021, iter [00890, 01251], lr: 0.000579, loss: 0.4259
2022-10-02 21:37:40 - train: epoch 0021, iter [00900, 01251], lr: 0.000579, loss: 0.4454
2022-10-02 21:38:00 - train: epoch 0021, iter [00910, 01251], lr: 0.000579, loss: 0.4259
2022-10-02 21:38:20 - train: epoch 0021, iter [00920, 01251], lr: 0.000579, loss: 0.4316
2022-10-02 21:38:40 - train: epoch 0021, iter [00930, 01251], lr: 0.000579, loss: 0.4415
2022-10-02 21:38:59 - train: epoch 0021, iter [00940, 01251], lr: 0.000579, loss: 0.4246
2022-10-02 21:39:19 - train: epoch 0021, iter [00950, 01251], lr: 0.000579, loss: 0.4186
2022-10-02 21:39:39 - train: epoch 0021, iter [00960, 01251], lr: 0.000579, loss: 0.4526
2022-10-02 21:39:58 - train: epoch 0021, iter [00970, 01251], lr: 0.000579, loss: 0.4321
2022-10-02 21:40:18 - train: epoch 0021, iter [00980, 01251], lr: 0.000579, loss: 0.4094
2022-10-02 21:40:38 - train: epoch 0021, iter [00990, 01251], lr: 0.000579, loss: 0.3993
2022-10-02 21:40:58 - train: epoch 0021, iter [01000, 01251], lr: 0.000579, loss: 0.4418
2022-10-02 21:41:17 - train: epoch 0021, iter [01010, 01251], lr: 0.000579, loss: 0.4256
2022-10-02 21:41:37 - train: epoch 0021, iter [01020, 01251], lr: 0.000579, loss: 0.4321
2022-10-02 21:41:56 - train: epoch 0021, iter [01030, 01251], lr: 0.000579, loss: 0.4086
2022-10-02 21:42:16 - train: epoch 0021, iter [01040, 01251], lr: 0.000579, loss: 0.4151
2022-10-02 21:42:36 - train: epoch 0021, iter [01050, 01251], lr: 0.000579, loss: 0.4340
2022-10-02 21:42:56 - train: epoch 0021, iter [01060, 01251], lr: 0.000579, loss: 0.4259
2022-10-02 21:43:16 - train: epoch 0021, iter [01070, 01251], lr: 0.000579, loss: 0.4100
2022-10-02 21:43:36 - train: epoch 0021, iter [01080, 01251], lr: 0.000579, loss: 0.4270
2022-10-02 21:43:54 - train: epoch 0021, iter [01090, 01251], lr: 0.000579, loss: 0.4161
2022-10-02 21:44:14 - train: epoch 0021, iter [01100, 01251], lr: 0.000579, loss: 0.4326
2022-10-02 21:44:33 - train: epoch 0021, iter [01110, 01251], lr: 0.000579, loss: 0.4399
2022-10-02 21:44:53 - train: epoch 0021, iter [01120, 01251], lr: 0.000579, loss: 0.4432
2022-10-02 21:45:13 - train: epoch 0021, iter [01130, 01251], lr: 0.000579, loss: 0.4108
2022-10-02 21:45:32 - train: epoch 0021, iter [01140, 01251], lr: 0.000579, loss: 0.4123
2022-10-02 21:45:52 - train: epoch 0021, iter [01150, 01251], lr: 0.000578, loss: 0.4179
2022-10-02 21:46:12 - train: epoch 0021, iter [01160, 01251], lr: 0.000578, loss: 0.4145
2022-10-02 21:46:32 - train: epoch 0021, iter [01170, 01251], lr: 0.000578, loss: 0.4246
2022-10-02 21:46:51 - train: epoch 0021, iter [01180, 01251], lr: 0.000578, loss: 0.4082
2022-10-02 21:47:11 - train: epoch 0021, iter [01190, 01251], lr: 0.000578, loss: 0.4180
2022-10-02 21:47:31 - train: epoch 0021, iter [01200, 01251], lr: 0.000578, loss: 0.4312
2022-10-02 21:47:51 - train: epoch 0021, iter [01210, 01251], lr: 0.000578, loss: 0.4235
2022-10-02 21:48:11 - train: epoch 0021, iter [01220, 01251], lr: 0.000578, loss: 0.4085
2022-10-02 21:48:30 - train: epoch 0021, iter [01230, 01251], lr: 0.000578, loss: 0.4106
2022-10-02 21:48:50 - train: epoch 0021, iter [01240, 01251], lr: 0.000578, loss: 0.4017
2022-10-02 21:49:08 - train: epoch 0021, iter [01250, 01251], lr: 0.000578, loss: 0.4273
2022-10-02 21:49:12 - train: epoch 021, train_loss: 0.4241
2022-10-02 21:49:16 - until epoch: 021, best_loss: 0.4241
2022-10-02 21:49:16 - epoch 022 lr: 0.000578
2022-10-02 21:49:42 - train: epoch 0022, iter [00010, 01251], lr: 0.000578, loss: 0.4405
2022-10-02 21:50:02 - train: epoch 0022, iter [00020, 01251], lr: 0.000578, loss: 0.4423
2022-10-02 21:50:21 - train: epoch 0022, iter [00030, 01251], lr: 0.000578, loss: 0.4186
2022-10-02 21:50:41 - train: epoch 0022, iter [00040, 01251], lr: 0.000578, loss: 0.4068
2022-10-02 21:51:01 - train: epoch 0022, iter [00050, 01251], lr: 0.000578, loss: 0.4265
2022-10-02 21:51:21 - train: epoch 0022, iter [00060, 01251], lr: 0.000578, loss: 0.4443
2022-10-02 21:51:40 - train: epoch 0022, iter [00070, 01251], lr: 0.000578, loss: 0.4201
2022-10-02 21:52:00 - train: epoch 0022, iter [00080, 01251], lr: 0.000578, loss: 0.4267
2022-10-02 21:52:20 - train: epoch 0022, iter [00090, 01251], lr: 0.000578, loss: 0.4208
2022-10-02 21:52:39 - train: epoch 0022, iter [00100, 01251], lr: 0.000578, loss: 0.4097
2022-10-02 21:52:59 - train: epoch 0022, iter [00110, 01251], lr: 0.000578, loss: 0.4387
2022-10-02 21:53:18 - train: epoch 0022, iter [00120, 01251], lr: 0.000578, loss: 0.4414
2022-10-02 21:53:38 - train: epoch 0022, iter [00130, 01251], lr: 0.000578, loss: 0.4333
2022-10-02 21:53:58 - train: epoch 0022, iter [00140, 01251], lr: 0.000578, loss: 0.4380
2022-10-02 21:54:17 - train: epoch 0022, iter [00150, 01251], lr: 0.000578, loss: 0.4075
2022-10-02 21:54:37 - train: epoch 0022, iter [00160, 01251], lr: 0.000578, loss: 0.4116
2022-10-02 21:54:56 - train: epoch 0022, iter [00170, 01251], lr: 0.000578, loss: 0.4250
2022-10-02 21:55:16 - train: epoch 0022, iter [00180, 01251], lr: 0.000578, loss: 0.4227
2022-10-02 21:55:36 - train: epoch 0022, iter [00190, 01251], lr: 0.000578, loss: 0.4304
2022-10-02 21:55:56 - train: epoch 0022, iter [00200, 01251], lr: 0.000578, loss: 0.4350
2022-10-02 21:56:15 - train: epoch 0022, iter [00210, 01251], lr: 0.000577, loss: 0.4362
2022-10-02 21:56:35 - train: epoch 0022, iter [00220, 01251], lr: 0.000577, loss: 0.4258
2022-10-02 21:56:55 - train: epoch 0022, iter [00230, 01251], lr: 0.000577, loss: 0.4393
2022-10-02 21:57:15 - train: epoch 0022, iter [00240, 01251], lr: 0.000577, loss: 0.4273
2022-10-02 21:57:34 - train: epoch 0022, iter [00250, 01251], lr: 0.000577, loss: 0.4472
2022-10-02 21:57:54 - train: epoch 0022, iter [00260, 01251], lr: 0.000577, loss: 0.4200
2022-10-02 21:58:14 - train: epoch 0022, iter [00270, 01251], lr: 0.000577, loss: 0.4323
2022-10-02 21:58:34 - train: epoch 0022, iter [00280, 01251], lr: 0.000577, loss: 0.4281
2022-10-02 21:58:54 - train: epoch 0022, iter [00290, 01251], lr: 0.000577, loss: 0.4293
2022-10-02 21:59:13 - train: epoch 0022, iter [00300, 01251], lr: 0.000577, loss: 0.4322
2022-10-02 21:59:33 - train: epoch 0022, iter [00310, 01251], lr: 0.000577, loss: 0.4298
2022-10-02 21:59:53 - train: epoch 0022, iter [00320, 01251], lr: 0.000577, loss: 0.4266
2022-10-02 22:00:12 - train: epoch 0022, iter [00330, 01251], lr: 0.000577, loss: 0.4335
2022-10-02 22:00:32 - train: epoch 0022, iter [00340, 01251], lr: 0.000577, loss: 0.4021
2022-10-02 22:00:52 - train: epoch 0022, iter [00350, 01251], lr: 0.000577, loss: 0.4163
2022-10-02 22:01:12 - train: epoch 0022, iter [00360, 01251], lr: 0.000577, loss: 0.4308
2022-10-02 22:01:31 - train: epoch 0022, iter [00370, 01251], lr: 0.000577, loss: 0.4225
2022-10-02 22:01:51 - train: epoch 0022, iter [00380, 01251], lr: 0.000577, loss: 0.4176
2022-10-02 22:02:11 - train: epoch 0022, iter [00390, 01251], lr: 0.000577, loss: 0.4352
2022-10-02 22:02:30 - train: epoch 0022, iter [00400, 01251], lr: 0.000577, loss: 0.3930
2022-10-02 22:02:52 - train: epoch 0022, iter [00410, 01251], lr: 0.000577, loss: 0.4308
2022-10-02 22:03:13 - train: epoch 0022, iter [00420, 01251], lr: 0.000577, loss: 0.4240
2022-10-02 22:03:33 - train: epoch 0022, iter [00430, 01251], lr: 0.000577, loss: 0.4294
2022-10-02 22:03:52 - train: epoch 0022, iter [00440, 01251], lr: 0.000577, loss: 0.4144
2022-10-02 22:04:12 - train: epoch 0022, iter [00450, 01251], lr: 0.000577, loss: 0.4119
2022-10-02 22:04:32 - train: epoch 0022, iter [00460, 01251], lr: 0.000577, loss: 0.4234
2022-10-02 22:04:52 - train: epoch 0022, iter [00470, 01251], lr: 0.000577, loss: 0.4381
2022-10-02 22:05:12 - train: epoch 0022, iter [00480, 01251], lr: 0.000577, loss: 0.4311
2022-10-02 22:05:32 - train: epoch 0022, iter [00490, 01251], lr: 0.000577, loss: 0.4186
2022-10-02 22:05:51 - train: epoch 0022, iter [00500, 01251], lr: 0.000577, loss: 0.4315
2022-10-02 22:06:11 - train: epoch 0022, iter [00510, 01251], lr: 0.000577, loss: 0.4229
2022-10-02 22:06:31 - train: epoch 0022, iter [00520, 01251], lr: 0.000576, loss: 0.4274
2022-10-02 22:06:51 - train: epoch 0022, iter [00530, 01251], lr: 0.000576, loss: 0.4329
2022-10-02 22:07:11 - train: epoch 0022, iter [00540, 01251], lr: 0.000576, loss: 0.4201
2022-10-02 22:07:30 - train: epoch 0022, iter [00550, 01251], lr: 0.000576, loss: 0.4346
2022-10-02 22:07:50 - train: epoch 0022, iter [00560, 01251], lr: 0.000576, loss: 0.4193
2022-10-02 22:08:09 - train: epoch 0022, iter [00570, 01251], lr: 0.000576, loss: 0.4065
2022-10-02 22:08:29 - train: epoch 0022, iter [00580, 01251], lr: 0.000576, loss: 0.4194
2022-10-02 22:08:49 - train: epoch 0022, iter [00590, 01251], lr: 0.000576, loss: 0.4282
2022-10-02 22:09:08 - train: epoch 0022, iter [00600, 01251], lr: 0.000576, loss: 0.4033
2022-10-02 22:09:28 - train: epoch 0022, iter [00610, 01251], lr: 0.000576, loss: 0.4021
2022-10-02 22:09:48 - train: epoch 0022, iter [00620, 01251], lr: 0.000576, loss: 0.4191
2022-10-02 22:10:07 - train: epoch 0022, iter [00630, 01251], lr: 0.000576, loss: 0.4296
2022-10-02 22:10:27 - train: epoch 0022, iter [00640, 01251], lr: 0.000576, loss: 0.4112
2022-10-02 22:10:47 - train: epoch 0022, iter [00650, 01251], lr: 0.000576, loss: 0.4208
2022-10-02 22:11:06 - train: epoch 0022, iter [00660, 01251], lr: 0.000576, loss: 0.4311
2022-10-02 22:11:26 - train: epoch 0022, iter [00670, 01251], lr: 0.000576, loss: 0.4192
2022-10-02 22:11:46 - train: epoch 0022, iter [00680, 01251], lr: 0.000576, loss: 0.4097
2022-10-02 22:12:06 - train: epoch 0022, iter [00690, 01251], lr: 0.000576, loss: 0.4312
2022-10-02 22:12:25 - train: epoch 0022, iter [00700, 01251], lr: 0.000576, loss: 0.4286
2022-10-02 22:12:44 - train: epoch 0022, iter [00710, 01251], lr: 0.000576, loss: 0.4338
2022-10-02 22:13:04 - train: epoch 0022, iter [00720, 01251], lr: 0.000576, loss: 0.4238
2022-10-02 22:13:24 - train: epoch 0022, iter [00730, 01251], lr: 0.000576, loss: 0.4220
2022-10-02 22:13:43 - train: epoch 0022, iter [00740, 01251], lr: 0.000576, loss: 0.4093
2022-10-02 22:14:03 - train: epoch 0022, iter [00750, 01251], lr: 0.000576, loss: 0.4299
2022-10-02 22:14:22 - train: epoch 0022, iter [00760, 01251], lr: 0.000576, loss: 0.4140
2022-10-02 22:14:42 - train: epoch 0022, iter [00770, 01251], lr: 0.000576, loss: 0.4052
2022-10-02 22:15:02 - train: epoch 0022, iter [00780, 01251], lr: 0.000576, loss: 0.4151
2022-10-02 22:15:22 - train: epoch 0022, iter [00790, 01251], lr: 0.000576, loss: 0.4250
2022-10-02 22:15:41 - train: epoch 0022, iter [00800, 01251], lr: 0.000576, loss: 0.4114
2022-10-02 22:16:01 - train: epoch 0022, iter [00810, 01251], lr: 0.000576, loss: 0.4269
2022-10-02 22:16:21 - train: epoch 0022, iter [00820, 01251], lr: 0.000576, loss: 0.4137
2022-10-02 22:16:40 - train: epoch 0022, iter [00830, 01251], lr: 0.000575, loss: 0.4213
2022-10-02 22:17:00 - train: epoch 0022, iter [00840, 01251], lr: 0.000575, loss: 0.4378
2022-10-02 22:17:20 - train: epoch 0022, iter [00850, 01251], lr: 0.000575, loss: 0.4272
2022-10-02 22:17:40 - train: epoch 0022, iter [00860, 01251], lr: 0.000575, loss: 0.4090
2022-10-02 22:17:59 - train: epoch 0022, iter [00870, 01251], lr: 0.000575, loss: 0.4399
2022-10-02 22:18:19 - train: epoch 0022, iter [00880, 01251], lr: 0.000575, loss: 0.4455
2022-10-02 22:18:39 - train: epoch 0022, iter [00890, 01251], lr: 0.000575, loss: 0.4313
2022-10-02 22:18:58 - train: epoch 0022, iter [00900, 01251], lr: 0.000575, loss: 0.4291
2022-10-02 22:19:18 - train: epoch 0022, iter [00910, 01251], lr: 0.000575, loss: 0.4436
2022-10-02 22:19:37 - train: epoch 0022, iter [00920, 01251], lr: 0.000575, loss: 0.4297
2022-10-02 22:19:57 - train: epoch 0022, iter [00930, 01251], lr: 0.000575, loss: 0.4216
2022-10-02 22:20:17 - train: epoch 0022, iter [00940, 01251], lr: 0.000575, loss: 0.4119
2022-10-02 22:20:36 - train: epoch 0022, iter [00950, 01251], lr: 0.000575, loss: 0.4268
2022-10-02 22:20:56 - train: epoch 0022, iter [00960, 01251], lr: 0.000575, loss: 0.4194
2022-10-02 22:21:16 - train: epoch 0022, iter [00970, 01251], lr: 0.000575, loss: 0.4168
2022-10-02 22:21:35 - train: epoch 0022, iter [00980, 01251], lr: 0.000575, loss: 0.4190
2022-10-02 22:21:55 - train: epoch 0022, iter [00990, 01251], lr: 0.000575, loss: 0.4274
2022-10-02 22:22:15 - train: epoch 0022, iter [01000, 01251], lr: 0.000575, loss: 0.4307
2022-10-02 22:22:35 - train: epoch 0022, iter [01010, 01251], lr: 0.000575, loss: 0.4217
2022-10-02 22:22:54 - train: epoch 0022, iter [01020, 01251], lr: 0.000575, loss: 0.4083
2022-10-02 22:23:14 - train: epoch 0022, iter [01030, 01251], lr: 0.000575, loss: 0.4139
2022-10-02 22:23:34 - train: epoch 0022, iter [01040, 01251], lr: 0.000575, loss: 0.4193
2022-10-02 22:23:54 - train: epoch 0022, iter [01050, 01251], lr: 0.000575, loss: 0.4225
2022-10-02 22:24:13 - train: epoch 0022, iter [01060, 01251], lr: 0.000575, loss: 0.4177
2022-10-02 22:24:33 - train: epoch 0022, iter [01070, 01251], lr: 0.000575, loss: 0.4287
2022-10-02 22:24:53 - train: epoch 0022, iter [01080, 01251], lr: 0.000575, loss: 0.4145
2022-10-02 22:25:12 - train: epoch 0022, iter [01090, 01251], lr: 0.000575, loss: 0.4322
2022-10-02 22:25:32 - train: epoch 0022, iter [01100, 01251], lr: 0.000575, loss: 0.3968
2022-10-02 22:25:52 - train: epoch 0022, iter [01110, 01251], lr: 0.000575, loss: 0.4308
2022-10-02 22:26:12 - train: epoch 0022, iter [01120, 01251], lr: 0.000575, loss: 0.4270
2022-10-02 22:26:31 - train: epoch 0022, iter [01130, 01251], lr: 0.000574, loss: 0.4161
2022-10-02 22:26:51 - train: epoch 0022, iter [01140, 01251], lr: 0.000574, loss: 0.4058
2022-10-02 22:27:11 - train: epoch 0022, iter [01150, 01251], lr: 0.000574, loss: 0.4225
2022-10-02 22:27:31 - train: epoch 0022, iter [01160, 01251], lr: 0.000574, loss: 0.4245
2022-10-02 22:27:50 - train: epoch 0022, iter [01170, 01251], lr: 0.000574, loss: 0.4374
2022-10-02 22:28:10 - train: epoch 0022, iter [01180, 01251], lr: 0.000574, loss: 0.4356
2022-10-02 22:28:30 - train: epoch 0022, iter [01190, 01251], lr: 0.000574, loss: 0.4224
2022-10-02 22:28:50 - train: epoch 0022, iter [01200, 01251], lr: 0.000574, loss: 0.4157
2022-10-02 22:29:10 - train: epoch 0022, iter [01210, 01251], lr: 0.000574, loss: 0.4480
2022-10-02 22:29:29 - train: epoch 0022, iter [01220, 01251], lr: 0.000574, loss: 0.4322
2022-10-02 22:29:48 - train: epoch 0022, iter [01230, 01251], lr: 0.000574, loss: 0.4135
2022-10-02 22:30:08 - train: epoch 0022, iter [01240, 01251], lr: 0.000574, loss: 0.4258
2022-10-02 22:30:27 - train: epoch 0022, iter [01250, 01251], lr: 0.000574, loss: 0.4255
2022-10-02 22:30:30 - train: epoch 022, train_loss: 0.4235
2022-10-02 22:30:34 - until epoch: 022, best_loss: 0.4235
2022-10-02 22:30:34 - epoch 023 lr: 0.000574
2022-10-02 22:31:01 - train: epoch 0023, iter [00010, 01251], lr: 0.000574, loss: 0.4152
2022-10-02 22:31:20 - train: epoch 0023, iter [00020, 01251], lr: 0.000574, loss: 0.4141
2022-10-02 22:31:39 - train: epoch 0023, iter [00030, 01251], lr: 0.000574, loss: 0.4338
2022-10-02 22:31:58 - train: epoch 0023, iter [00040, 01251], lr: 0.000574, loss: 0.4294
2022-10-02 22:32:18 - train: epoch 0023, iter [00050, 01251], lr: 0.000574, loss: 0.4089
2022-10-02 22:32:37 - train: epoch 0023, iter [00060, 01251], lr: 0.000574, loss: 0.4159
2022-10-02 22:32:57 - train: epoch 0023, iter [00070, 01251], lr: 0.000574, loss: 0.4246
2022-10-02 22:33:16 - train: epoch 0023, iter [00080, 01251], lr: 0.000574, loss: 0.4124
2022-10-02 22:33:37 - train: epoch 0023, iter [00090, 01251], lr: 0.000574, loss: 0.4123
2022-10-02 22:33:56 - train: epoch 0023, iter [00100, 01251], lr: 0.000574, loss: 0.4003
2022-10-02 22:34:15 - train: epoch 0023, iter [00110, 01251], lr: 0.000574, loss: 0.4320
2022-10-02 22:34:35 - train: epoch 0023, iter [00120, 01251], lr: 0.000574, loss: 0.4154
2022-10-02 22:34:55 - train: epoch 0023, iter [00130, 01251], lr: 0.000574, loss: 0.4030
2022-10-02 22:35:15 - train: epoch 0023, iter [00140, 01251], lr: 0.000574, loss: 0.4375
2022-10-02 22:35:34 - train: epoch 0023, iter [00150, 01251], lr: 0.000574, loss: 0.4316
2022-10-02 22:35:54 - train: epoch 0023, iter [00160, 01251], lr: 0.000574, loss: 0.4131
2022-10-02 22:36:14 - train: epoch 0023, iter [00170, 01251], lr: 0.000573, loss: 0.4325
2022-10-02 22:36:34 - train: epoch 0023, iter [00180, 01251], lr: 0.000573, loss: 0.4102
2022-10-02 22:36:53 - train: epoch 0023, iter [00190, 01251], lr: 0.000573, loss: 0.4236
2022-10-02 22:37:13 - train: epoch 0023, iter [00200, 01251], lr: 0.000573, loss: 0.4445
2022-10-02 22:37:33 - train: epoch 0023, iter [00210, 01251], lr: 0.000573, loss: 0.4306
2022-10-02 22:37:52 - train: epoch 0023, iter [00220, 01251], lr: 0.000573, loss: 0.4338
2022-10-02 22:38:12 - train: epoch 0023, iter [00230, 01251], lr: 0.000573, loss: 0.4280
2022-10-02 22:38:32 - train: epoch 0023, iter [00240, 01251], lr: 0.000573, loss: 0.4170
2022-10-02 22:38:51 - train: epoch 0023, iter [00250, 01251], lr: 0.000573, loss: 0.4350
2022-10-02 22:39:11 - train: epoch 0023, iter [00260, 01251], lr: 0.000573, loss: 0.4419
2022-10-02 22:39:31 - train: epoch 0023, iter [00270, 01251], lr: 0.000573, loss: 0.4342
2022-10-02 22:39:50 - train: epoch 0023, iter [00280, 01251], lr: 0.000573, loss: 0.4190
2022-10-02 22:40:09 - train: epoch 0023, iter [00290, 01251], lr: 0.000573, loss: 0.4337
2022-10-02 22:40:27 - train: epoch 0023, iter [00300, 01251], lr: 0.000573, loss: 0.4144
2022-10-02 22:40:46 - train: epoch 0023, iter [00310, 01251], lr: 0.000573, loss: 0.4240
2022-10-02 22:41:05 - train: epoch 0023, iter [00320, 01251], lr: 0.000573, loss: 0.4275
2022-10-02 22:41:24 - train: epoch 0023, iter [00330, 01251], lr: 0.000573, loss: 0.4152
2022-10-02 22:41:42 - train: epoch 0023, iter [00340, 01251], lr: 0.000573, loss: 0.4246
2022-10-02 22:42:01 - train: epoch 0023, iter [00350, 01251], lr: 0.000573, loss: 0.4214
2022-10-02 22:42:20 - train: epoch 0023, iter [00360, 01251], lr: 0.000573, loss: 0.4441
2022-10-02 22:42:39 - train: epoch 0023, iter [00370, 01251], lr: 0.000573, loss: 0.4051
2022-10-02 22:42:58 - train: epoch 0023, iter [00380, 01251], lr: 0.000573, loss: 0.4188
2022-10-02 22:43:16 - train: epoch 0023, iter [00390, 01251], lr: 0.000573, loss: 0.4234
2022-10-02 22:43:35 - train: epoch 0023, iter [00400, 01251], lr: 0.000573, loss: 0.4306
2022-10-02 22:43:54 - train: epoch 0023, iter [00410, 01251], lr: 0.000573, loss: 0.4374
2022-10-02 22:44:12 - train: epoch 0023, iter [00420, 01251], lr: 0.000573, loss: 0.4045
2022-10-02 22:44:30 - train: epoch 0023, iter [00430, 01251], lr: 0.000573, loss: 0.4327
2022-10-02 22:44:49 - train: epoch 0023, iter [00440, 01251], lr: 0.000573, loss: 0.4246
2022-10-02 22:45:08 - train: epoch 0023, iter [00450, 01251], lr: 0.000573, loss: 0.4081
2022-10-02 22:45:27 - train: epoch 0023, iter [00460, 01251], lr: 0.000572, loss: 0.4275
2022-10-02 22:45:46 - train: epoch 0023, iter [00470, 01251], lr: 0.000572, loss: 0.4086
2022-10-02 22:46:04 - train: epoch 0023, iter [00480, 01251], lr: 0.000572, loss: 0.4167
2022-10-02 22:46:23 - train: epoch 0023, iter [00490, 01251], lr: 0.000572, loss: 0.3909
2022-10-02 22:46:42 - train: epoch 0023, iter [00500, 01251], lr: 0.000572, loss: 0.4309
2022-10-02 22:47:00 - train: epoch 0023, iter [00510, 01251], lr: 0.000572, loss: 0.4438
2022-10-02 22:47:20 - train: epoch 0023, iter [00520, 01251], lr: 0.000572, loss: 0.4073
2022-10-02 22:47:38 - train: epoch 0023, iter [00530, 01251], lr: 0.000572, loss: 0.4217
2022-10-02 22:47:56 - train: epoch 0023, iter [00540, 01251], lr: 0.000572, loss: 0.4216
2022-10-02 22:48:15 - train: epoch 0023, iter [00550, 01251], lr: 0.000572, loss: 0.4163
2022-10-02 22:48:34 - train: epoch 0023, iter [00560, 01251], lr: 0.000572, loss: 0.4196
2022-10-02 22:48:52 - train: epoch 0023, iter [00570, 01251], lr: 0.000572, loss: 0.4266
2022-10-02 22:49:11 - train: epoch 0023, iter [00580, 01251], lr: 0.000572, loss: 0.4203
2022-10-02 22:49:29 - train: epoch 0023, iter [00590, 01251], lr: 0.000572, loss: 0.4271
2022-10-02 22:49:48 - train: epoch 0023, iter [00600, 01251], lr: 0.000572, loss: 0.4304
2022-10-02 22:50:06 - train: epoch 0023, iter [00610, 01251], lr: 0.000572, loss: 0.4019
2022-10-02 22:50:25 - train: epoch 0023, iter [00620, 01251], lr: 0.000572, loss: 0.4350
2022-10-02 22:50:44 - train: epoch 0023, iter [00630, 01251], lr: 0.000572, loss: 0.4417
2022-10-02 22:51:03 - train: epoch 0023, iter [00640, 01251], lr: 0.000572, loss: 0.4444
2022-10-02 22:51:22 - train: epoch 0023, iter [00650, 01251], lr: 0.000572, loss: 0.4238
2022-10-02 22:51:41 - train: epoch 0023, iter [00660, 01251], lr: 0.000572, loss: 0.4263
2022-10-02 22:52:00 - train: epoch 0023, iter [00670, 01251], lr: 0.000572, loss: 0.4425
2022-10-02 22:52:19 - train: epoch 0023, iter [00680, 01251], lr: 0.000572, loss: 0.4231
2022-10-02 22:52:37 - train: epoch 0023, iter [00690, 01251], lr: 0.000572, loss: 0.4308
2022-10-02 22:52:56 - train: epoch 0023, iter [00700, 01251], lr: 0.000572, loss: 0.4221
2022-10-02 22:53:15 - train: epoch 0023, iter [00710, 01251], lr: 0.000572, loss: 0.4159
2022-10-02 22:53:34 - train: epoch 0023, iter [00720, 01251], lr: 0.000572, loss: 0.4174
2022-10-02 22:53:52 - train: epoch 0023, iter [00730, 01251], lr: 0.000572, loss: 0.4116
2022-10-02 22:54:11 - train: epoch 0023, iter [00740, 01251], lr: 0.000571, loss: 0.4121
2022-10-02 22:54:30 - train: epoch 0023, iter [00750, 01251], lr: 0.000571, loss: 0.4021
2022-10-02 22:54:49 - train: epoch 0023, iter [00760, 01251], lr: 0.000571, loss: 0.4193
2022-10-02 22:55:07 - train: epoch 0023, iter [00770, 01251], lr: 0.000571, loss: 0.4114
2022-10-02 22:55:26 - train: epoch 0023, iter [00780, 01251], lr: 0.000571, loss: 0.4192
2022-10-02 22:55:45 - train: epoch 0023, iter [00790, 01251], lr: 0.000571, loss: 0.4313
2022-10-02 22:56:03 - train: epoch 0023, iter [00800, 01251], lr: 0.000571, loss: 0.4185
2022-10-02 22:56:22 - train: epoch 0023, iter [00810, 01251], lr: 0.000571, loss: 0.4091
2022-10-02 22:56:40 - train: epoch 0023, iter [00820, 01251], lr: 0.000571, loss: 0.3961
2022-10-02 22:56:59 - train: epoch 0023, iter [00830, 01251], lr: 0.000571, loss: 0.4311
2022-10-02 22:57:18 - train: epoch 0023, iter [00840, 01251], lr: 0.000571, loss: 0.4179
2022-10-02 22:57:38 - train: epoch 0023, iter [00850, 01251], lr: 0.000571, loss: 0.4059
2022-10-02 22:57:57 - train: epoch 0023, iter [00860, 01251], lr: 0.000571, loss: 0.4417
2022-10-02 22:58:16 - train: epoch 0023, iter [00870, 01251], lr: 0.000571, loss: 0.4330
2022-10-02 22:58:35 - train: epoch 0023, iter [00880, 01251], lr: 0.000571, loss: 0.4410
2022-10-02 22:58:54 - train: epoch 0023, iter [00890, 01251], lr: 0.000571, loss: 0.4293
2022-10-02 22:59:14 - train: epoch 0023, iter [00900, 01251], lr: 0.000571, loss: 0.4440
2022-10-02 22:59:33 - train: epoch 0023, iter [00910, 01251], lr: 0.000571, loss: 0.4120
2022-10-02 22:59:52 - train: epoch 0023, iter [00920, 01251], lr: 0.000571, loss: 0.4139
2022-10-02 23:00:12 - train: epoch 0023, iter [00930, 01251], lr: 0.000571, loss: 0.4245
2022-10-02 23:00:31 - train: epoch 0023, iter [00940, 01251], lr: 0.000571, loss: 0.4268
2022-10-02 23:00:51 - train: epoch 0023, iter [00950, 01251], lr: 0.000571, loss: 0.4209
2022-10-02 23:01:11 - train: epoch 0023, iter [00960, 01251], lr: 0.000571, loss: 0.4087
2022-10-02 23:01:29 - train: epoch 0023, iter [00970, 01251], lr: 0.000571, loss: 0.4314
2022-10-02 23:01:49 - train: epoch 0023, iter [00980, 01251], lr: 0.000571, loss: 0.4030
2022-10-02 23:02:08 - train: epoch 0023, iter [00990, 01251], lr: 0.000571, loss: 0.4161
2022-10-02 23:02:27 - train: epoch 0023, iter [01000, 01251], lr: 0.000571, loss: 0.4431
2022-10-02 23:02:47 - train: epoch 0023, iter [01010, 01251], lr: 0.000571, loss: 0.4112
2022-10-02 23:03:06 - train: epoch 0023, iter [01020, 01251], lr: 0.000570, loss: 0.4238
2022-10-02 23:03:25 - train: epoch 0023, iter [01030, 01251], lr: 0.000570, loss: 0.4240
2022-10-02 23:03:45 - train: epoch 0023, iter [01040, 01251], lr: 0.000570, loss: 0.4232
2022-10-02 23:04:04 - train: epoch 0023, iter [01050, 01251], lr: 0.000570, loss: 0.4188
2022-10-02 23:04:24 - train: epoch 0023, iter [01060, 01251], lr: 0.000570, loss: 0.4186
2022-10-02 23:04:43 - train: epoch 0023, iter [01070, 01251], lr: 0.000570, loss: 0.4139
2022-10-02 23:05:02 - train: epoch 0023, iter [01080, 01251], lr: 0.000570, loss: 0.4395
2022-10-02 23:05:22 - train: epoch 0023, iter [01090, 01251], lr: 0.000570, loss: 0.4201
2022-10-02 23:05:40 - train: epoch 0023, iter [01100, 01251], lr: 0.000570, loss: 0.4364
2022-10-02 23:06:00 - train: epoch 0023, iter [01110, 01251], lr: 0.000570, loss: 0.4226
2022-10-02 23:06:19 - train: epoch 0023, iter [01120, 01251], lr: 0.000570, loss: 0.4121
2022-10-02 23:06:38 - train: epoch 0023, iter [01130, 01251], lr: 0.000570, loss: 0.4500
2022-10-02 23:06:56 - train: epoch 0023, iter [01140, 01251], lr: 0.000570, loss: 0.4314
2022-10-02 23:07:15 - train: epoch 0023, iter [01150, 01251], lr: 0.000570, loss: 0.4272
2022-10-02 23:07:35 - train: epoch 0023, iter [01160, 01251], lr: 0.000570, loss: 0.4215
2022-10-02 23:07:54 - train: epoch 0023, iter [01170, 01251], lr: 0.000570, loss: 0.4085
2022-10-02 23:08:13 - train: epoch 0023, iter [01180, 01251], lr: 0.000570, loss: 0.4181
2022-10-02 23:08:31 - train: epoch 0023, iter [01190, 01251], lr: 0.000570, loss: 0.4227
2022-10-02 23:08:51 - train: epoch 0023, iter [01200, 01251], lr: 0.000570, loss: 0.4239
2022-10-02 23:09:09 - train: epoch 0023, iter [01210, 01251], lr: 0.000570, loss: 0.4302
2022-10-02 23:09:28 - train: epoch 0023, iter [01220, 01251], lr: 0.000570, loss: 0.3963
2022-10-02 23:09:48 - train: epoch 0023, iter [01230, 01251], lr: 0.000570, loss: 0.4039
2022-10-02 23:10:07 - train: epoch 0023, iter [01240, 01251], lr: 0.000570, loss: 0.4062
2022-10-02 23:10:25 - train: epoch 0023, iter [01250, 01251], lr: 0.000570, loss: 0.4265
2022-10-02 23:10:28 - train: epoch 023, train_loss: 0.4226
2022-10-02 23:10:32 - until epoch: 023, best_loss: 0.4226
2022-10-02 23:10:32 - epoch 024 lr: 0.000570
2022-10-02 23:10:58 - train: epoch 0024, iter [00010, 01251], lr: 0.000570, loss: 0.4113
2022-10-02 23:11:16 - train: epoch 0024, iter [00020, 01251], lr: 0.000570, loss: 0.4370
2022-10-02 23:11:36 - train: epoch 0024, iter [00030, 01251], lr: 0.000570, loss: 0.4220
2022-10-02 23:11:55 - train: epoch 0024, iter [00040, 01251], lr: 0.000569, loss: 0.4209
2022-10-02 23:12:14 - train: epoch 0024, iter [00050, 01251], lr: 0.000569, loss: 0.4335
2022-10-02 23:12:32 - train: epoch 0024, iter [00060, 01251], lr: 0.000569, loss: 0.4226
2022-10-02 23:12:52 - train: epoch 0024, iter [00070, 01251], lr: 0.000569, loss: 0.4106
2022-10-02 23:13:11 - train: epoch 0024, iter [00080, 01251], lr: 0.000569, loss: 0.4267
2022-10-02 23:13:31 - train: epoch 0024, iter [00090, 01251], lr: 0.000569, loss: 0.4028
2022-10-02 23:13:49 - train: epoch 0024, iter [00100, 01251], lr: 0.000569, loss: 0.4291
2022-10-02 23:14:08 - train: epoch 0024, iter [00110, 01251], lr: 0.000569, loss: 0.3995
2022-10-02 23:14:27 - train: epoch 0024, iter [00120, 01251], lr: 0.000569, loss: 0.4182
2022-10-02 23:14:46 - train: epoch 0024, iter [00130, 01251], lr: 0.000569, loss: 0.4398
2022-10-02 23:15:04 - train: epoch 0024, iter [00140, 01251], lr: 0.000569, loss: 0.4268
2022-10-02 23:15:23 - train: epoch 0024, iter [00150, 01251], lr: 0.000569, loss: 0.4322
2022-10-02 23:15:42 - train: epoch 0024, iter [00160, 01251], lr: 0.000569, loss: 0.4261
2022-10-02 23:16:02 - train: epoch 0024, iter [00170, 01251], lr: 0.000569, loss: 0.4372
2022-10-02 23:16:20 - train: epoch 0024, iter [00180, 01251], lr: 0.000569, loss: 0.4371
2022-10-02 23:16:39 - train: epoch 0024, iter [00190, 01251], lr: 0.000569, loss: 0.4031
2022-10-02 23:16:58 - train: epoch 0024, iter [00200, 01251], lr: 0.000569, loss: 0.4432
2022-10-02 23:17:17 - train: epoch 0024, iter [00210, 01251], lr: 0.000569, loss: 0.4321
2022-10-02 23:17:35 - train: epoch 0024, iter [00220, 01251], lr: 0.000569, loss: 0.4112
2022-10-02 23:17:54 - train: epoch 0024, iter [00230, 01251], lr: 0.000569, loss: 0.4110
2022-10-02 23:18:13 - train: epoch 0024, iter [00240, 01251], lr: 0.000569, loss: 0.4289
2022-10-02 23:18:32 - train: epoch 0024, iter [00250, 01251], lr: 0.000569, loss: 0.4278
2022-10-02 23:18:51 - train: epoch 0024, iter [00260, 01251], lr: 0.000569, loss: 0.4417
2022-10-02 23:19:10 - train: epoch 0024, iter [00270, 01251], lr: 0.000569, loss: 0.4130
2022-10-02 23:19:29 - train: epoch 0024, iter [00280, 01251], lr: 0.000569, loss: 0.4158
2022-10-02 23:19:48 - train: epoch 0024, iter [00290, 01251], lr: 0.000569, loss: 0.4374
2022-10-02 23:20:07 - train: epoch 0024, iter [00300, 01251], lr: 0.000569, loss: 0.4206
2022-10-02 23:20:26 - train: epoch 0024, iter [00310, 01251], lr: 0.000568, loss: 0.4395
2022-10-02 23:20:45 - train: epoch 0024, iter [00320, 01251], lr: 0.000568, loss: 0.4070
2022-10-02 23:21:04 - train: epoch 0024, iter [00330, 01251], lr: 0.000568, loss: 0.4227
2022-10-02 23:21:24 - train: epoch 0024, iter [00340, 01251], lr: 0.000568, loss: 0.4381
2022-10-02 23:21:44 - train: epoch 0024, iter [00350, 01251], lr: 0.000568, loss: 0.4333
2022-10-02 23:22:03 - train: epoch 0024, iter [00360, 01251], lr: 0.000568, loss: 0.4137
2022-10-02 23:22:23 - train: epoch 0024, iter [00370, 01251], lr: 0.000568, loss: 0.4228
2022-10-02 23:22:42 - train: epoch 0024, iter [00380, 01251], lr: 0.000568, loss: 0.4313
2022-10-02 23:23:02 - train: epoch 0024, iter [00390, 01251], lr: 0.000568, loss: 0.4138
2022-10-02 23:23:21 - train: epoch 0024, iter [00400, 01251], lr: 0.000568, loss: 0.4198
2022-10-02 23:23:41 - train: epoch 0024, iter [00410, 01251], lr: 0.000568, loss: 0.4229
2022-10-02 23:24:01 - train: epoch 0024, iter [00420, 01251], lr: 0.000568, loss: 0.4254
2022-10-02 23:24:20 - train: epoch 0024, iter [00430, 01251], lr: 0.000568, loss: 0.4286
2022-10-02 23:24:40 - train: epoch 0024, iter [00440, 01251], lr: 0.000568, loss: 0.4247
2022-10-02 23:24:59 - train: epoch 0024, iter [00450, 01251], lr: 0.000568, loss: 0.4250
2022-10-02 23:25:19 - train: epoch 0024, iter [00460, 01251], lr: 0.000568, loss: 0.4162
2022-10-02 23:25:38 - train: epoch 0024, iter [00470, 01251], lr: 0.000568, loss: 0.4269
2022-10-02 23:25:58 - train: epoch 0024, iter [00480, 01251], lr: 0.000568, loss: 0.4158
2022-10-02 23:26:18 - train: epoch 0024, iter [00490, 01251], lr: 0.000568, loss: 0.4083
2022-10-02 23:26:37 - train: epoch 0024, iter [00500, 01251], lr: 0.000568, loss: 0.4025
2022-10-02 23:26:57 - train: epoch 0024, iter [00510, 01251], lr: 0.000568, loss: 0.4183
2022-10-02 23:27:16 - train: epoch 0024, iter [00520, 01251], lr: 0.000568, loss: 0.4074
2022-10-02 23:27:36 - train: epoch 0024, iter [00530, 01251], lr: 0.000568, loss: 0.4278
2022-10-02 23:27:56 - train: epoch 0024, iter [00540, 01251], lr: 0.000568, loss: 0.4237
2022-10-02 23:28:15 - train: epoch 0024, iter [00550, 01251], lr: 0.000568, loss: 0.4209
2022-10-02 23:28:35 - train: epoch 0024, iter [00560, 01251], lr: 0.000568, loss: 0.4114
2022-10-02 23:28:54 - train: epoch 0024, iter [00570, 01251], lr: 0.000568, loss: 0.4118
2022-10-02 23:29:14 - train: epoch 0024, iter [00580, 01251], lr: 0.000567, loss: 0.4140
2022-10-02 23:29:33 - train: epoch 0024, iter [00590, 01251], lr: 0.000567, loss: 0.4461
2022-10-02 23:29:53 - train: epoch 0024, iter [00600, 01251], lr: 0.000567, loss: 0.4190
2022-10-02 23:30:13 - train: epoch 0024, iter [00610, 01251], lr: 0.000567, loss: 0.4150
2022-10-02 23:30:32 - train: epoch 0024, iter [00620, 01251], lr: 0.000567, loss: 0.4128
2022-10-02 23:30:52 - train: epoch 0024, iter [00630, 01251], lr: 0.000567, loss: 0.4188
2022-10-02 23:31:12 - train: epoch 0024, iter [00640, 01251], lr: 0.000567, loss: 0.4243
2022-10-02 23:31:32 - train: epoch 0024, iter [00650, 01251], lr: 0.000567, loss: 0.4124
2022-10-02 23:31:50 - train: epoch 0024, iter [00660, 01251], lr: 0.000567, loss: 0.4207
2022-10-02 23:32:09 - train: epoch 0024, iter [00670, 01251], lr: 0.000567, loss: 0.4393
2022-10-02 23:32:28 - train: epoch 0024, iter [00680, 01251], lr: 0.000567, loss: 0.4184
2022-10-02 23:32:46 - train: epoch 0024, iter [00690, 01251], lr: 0.000567, loss: 0.4171
2022-10-02 23:33:05 - train: epoch 0024, iter [00700, 01251], lr: 0.000567, loss: 0.4184
2022-10-02 23:33:24 - train: epoch 0024, iter [00710, 01251], lr: 0.000567, loss: 0.4160
2022-10-02 23:33:42 - train: epoch 0024, iter [00720, 01251], lr: 0.000567, loss: 0.4228
2022-10-02 23:34:01 - train: epoch 0024, iter [00730, 01251], lr: 0.000567, loss: 0.4199
2022-10-02 23:34:19 - train: epoch 0024, iter [00740, 01251], lr: 0.000567, loss: 0.4273
2022-10-02 23:34:37 - train: epoch 0024, iter [00750, 01251], lr: 0.000567, loss: 0.4015
2022-10-02 23:34:56 - train: epoch 0024, iter [00760, 01251], lr: 0.000567, loss: 0.4151
2022-10-02 23:35:15 - train: epoch 0024, iter [00770, 01251], lr: 0.000567, loss: 0.4304
2022-10-02 23:35:34 - train: epoch 0024, iter [00780, 01251], lr: 0.000567, loss: 0.4095
2022-10-02 23:35:52 - train: epoch 0024, iter [00790, 01251], lr: 0.000567, loss: 0.4317
2022-10-02 23:36:11 - train: epoch 0024, iter [00800, 01251], lr: 0.000567, loss: 0.4335
2022-10-02 23:36:29 - train: epoch 0024, iter [00810, 01251], lr: 0.000567, loss: 0.4163
2022-10-02 23:36:48 - train: epoch 0024, iter [00820, 01251], lr: 0.000567, loss: 0.4242
2022-10-02 23:37:07 - train: epoch 0024, iter [00830, 01251], lr: 0.000567, loss: 0.4145
2022-10-02 23:37:25 - train: epoch 0024, iter [00840, 01251], lr: 0.000566, loss: 0.4234
2022-10-02 23:37:44 - train: epoch 0024, iter [00850, 01251], lr: 0.000566, loss: 0.4097
2022-10-02 23:38:02 - train: epoch 0024, iter [00860, 01251], lr: 0.000566, loss: 0.4241
2022-10-02 23:38:21 - train: epoch 0024, iter [00870, 01251], lr: 0.000566, loss: 0.4103
2022-10-02 23:38:39 - train: epoch 0024, iter [00880, 01251], lr: 0.000566, loss: 0.4381
2022-10-02 23:38:58 - train: epoch 0024, iter [00890, 01251], lr: 0.000566, loss: 0.4117
2022-10-02 23:39:16 - train: epoch 0024, iter [00900, 01251], lr: 0.000566, loss: 0.4345
2022-10-02 23:39:35 - train: epoch 0024, iter [00910, 01251], lr: 0.000566, loss: 0.4297
2022-10-02 23:39:53 - train: epoch 0024, iter [00920, 01251], lr: 0.000566, loss: 0.4264
2022-10-02 23:40:12 - train: epoch 0024, iter [00930, 01251], lr: 0.000566, loss: 0.4336
2022-10-02 23:40:30 - train: epoch 0024, iter [00940, 01251], lr: 0.000566, loss: 0.4317
2022-10-02 23:40:49 - train: epoch 0024, iter [00950, 01251], lr: 0.000566, loss: 0.3958
2022-10-02 23:41:08 - train: epoch 0024, iter [00960, 01251], lr: 0.000566, loss: 0.4298
2022-10-02 23:41:26 - train: epoch 0024, iter [00970, 01251], lr: 0.000566, loss: 0.4172
2022-10-02 23:41:45 - train: epoch 0024, iter [00980, 01251], lr: 0.000566, loss: 0.4236
2022-10-02 23:42:03 - train: epoch 0024, iter [00990, 01251], lr: 0.000566, loss: 0.4153
2022-10-02 23:42:22 - train: epoch 0024, iter [01000, 01251], lr: 0.000566, loss: 0.4227
2022-10-02 23:42:40 - train: epoch 0024, iter [01010, 01251], lr: 0.000566, loss: 0.4245
2022-10-02 23:43:00 - train: epoch 0024, iter [01020, 01251], lr: 0.000566, loss: 0.4253
2022-10-02 23:43:19 - train: epoch 0024, iter [01030, 01251], lr: 0.000566, loss: 0.4208
2022-10-02 23:43:37 - train: epoch 0024, iter [01040, 01251], lr: 0.000566, loss: 0.4238
2022-10-02 23:43:56 - train: epoch 0024, iter [01050, 01251], lr: 0.000566, loss: 0.3984
2022-10-02 23:44:14 - train: epoch 0024, iter [01060, 01251], lr: 0.000566, loss: 0.4197
2022-10-02 23:44:33 - train: epoch 0024, iter [01070, 01251], lr: 0.000566, loss: 0.4037
2022-10-02 23:44:52 - train: epoch 0024, iter [01080, 01251], lr: 0.000566, loss: 0.4190
2022-10-02 23:45:10 - train: epoch 0024, iter [01090, 01251], lr: 0.000566, loss: 0.4164
2022-10-02 23:45:29 - train: epoch 0024, iter [01100, 01251], lr: 0.000565, loss: 0.4361
2022-10-02 23:45:47 - train: epoch 0024, iter [01110, 01251], lr: 0.000565, loss: 0.4313
2022-10-02 23:46:06 - train: epoch 0024, iter [01120, 01251], lr: 0.000565, loss: 0.4421
2022-10-02 23:46:25 - train: epoch 0024, iter [01130, 01251], lr: 0.000565, loss: 0.4239
2022-10-02 23:46:44 - train: epoch 0024, iter [01140, 01251], lr: 0.000565, loss: 0.4365
2022-10-02 23:47:02 - train: epoch 0024, iter [01150, 01251], lr: 0.000565, loss: 0.4268
2022-10-02 23:47:21 - train: epoch 0024, iter [01160, 01251], lr: 0.000565, loss: 0.4166
2022-10-02 23:47:40 - train: epoch 0024, iter [01170, 01251], lr: 0.000565, loss: 0.4017
2022-10-02 23:47:59 - train: epoch 0024, iter [01180, 01251], lr: 0.000565, loss: 0.4062
2022-10-02 23:48:19 - train: epoch 0024, iter [01190, 01251], lr: 0.000565, loss: 0.4238
2022-10-02 23:48:38 - train: epoch 0024, iter [01200, 01251], lr: 0.000565, loss: 0.4334
2022-10-02 23:48:57 - train: epoch 0024, iter [01210, 01251], lr: 0.000565, loss: 0.4228
2022-10-02 23:49:16 - train: epoch 0024, iter [01220, 01251], lr: 0.000565, loss: 0.4182
2022-10-02 23:49:35 - train: epoch 0024, iter [01230, 01251], lr: 0.000565, loss: 0.4170
2022-10-02 23:49:54 - train: epoch 0024, iter [01240, 01251], lr: 0.000565, loss: 0.4198
2022-10-02 23:50:12 - train: epoch 0024, iter [01250, 01251], lr: 0.000565, loss: 0.4318
2022-10-02 23:50:15 - train: epoch 024, train_loss: 0.4220
2022-10-02 23:50:19 - until epoch: 024, best_loss: 0.4220
2022-10-02 23:50:19 - epoch 025 lr: 0.000565
2022-10-02 23:50:45 - train: epoch 0025, iter [00010, 01251], lr: 0.000565, loss: 0.4067
2022-10-02 23:51:04 - train: epoch 0025, iter [00020, 01251], lr: 0.000565, loss: 0.4159
2022-10-02 23:51:24 - train: epoch 0025, iter [00030, 01251], lr: 0.000565, loss: 0.4184
2022-10-02 23:51:43 - train: epoch 0025, iter [00040, 01251], lr: 0.000565, loss: 0.4118
2022-10-02 23:52:02 - train: epoch 0025, iter [00050, 01251], lr: 0.000565, loss: 0.4299
2022-10-02 23:52:22 - train: epoch 0025, iter [00060, 01251], lr: 0.000565, loss: 0.4288
2022-10-02 23:52:41 - train: epoch 0025, iter [00070, 01251], lr: 0.000565, loss: 0.4114
2022-10-02 23:53:00 - train: epoch 0025, iter [00080, 01251], lr: 0.000565, loss: 0.4094
2022-10-02 23:53:19 - train: epoch 0025, iter [00090, 01251], lr: 0.000565, loss: 0.4067
2022-10-02 23:53:38 - train: epoch 0025, iter [00100, 01251], lr: 0.000564, loss: 0.4343
2022-10-02 23:53:57 - train: epoch 0025, iter [00110, 01251], lr: 0.000564, loss: 0.4129
2022-10-02 23:54:16 - train: epoch 0025, iter [00120, 01251], lr: 0.000564, loss: 0.4122
2022-10-02 23:54:35 - train: epoch 0025, iter [00130, 01251], lr: 0.000564, loss: 0.4200
2022-10-02 23:54:54 - train: epoch 0025, iter [00140, 01251], lr: 0.000564, loss: 0.4231
2022-10-02 23:55:13 - train: epoch 0025, iter [00150, 01251], lr: 0.000564, loss: 0.3987
2022-10-02 23:55:32 - train: epoch 0025, iter [00160, 01251], lr: 0.000564, loss: 0.4025
2022-10-02 23:55:51 - train: epoch 0025, iter [00170, 01251], lr: 0.000564, loss: 0.4169
2022-10-02 23:56:10 - train: epoch 0025, iter [00180, 01251], lr: 0.000564, loss: 0.4011
2022-10-02 23:56:30 - train: epoch 0025, iter [00190, 01251], lr: 0.000564, loss: 0.4149
2022-10-02 23:56:49 - train: epoch 0025, iter [00200, 01251], lr: 0.000564, loss: 0.4235
2022-10-02 23:57:08 - train: epoch 0025, iter [00210, 01251], lr: 0.000564, loss: 0.4469
2022-10-02 23:57:27 - train: epoch 0025, iter [00220, 01251], lr: 0.000564, loss: 0.4072
2022-10-02 23:57:46 - train: epoch 0025, iter [00230, 01251], lr: 0.000564, loss: 0.4278
2022-10-02 23:58:05 - train: epoch 0025, iter [00240, 01251], lr: 0.000564, loss: 0.4207
2022-10-02 23:58:24 - train: epoch 0025, iter [00250, 01251], lr: 0.000564, loss: 0.4133
2022-10-02 23:58:43 - train: epoch 0025, iter [00260, 01251], lr: 0.000564, loss: 0.4434
2022-10-02 23:59:02 - train: epoch 0025, iter [00270, 01251], lr: 0.000564, loss: 0.4094
2022-10-02 23:59:21 - train: epoch 0025, iter [00280, 01251], lr: 0.000564, loss: 0.4291
2022-10-02 23:59:40 - train: epoch 0025, iter [00290, 01251], lr: 0.000564, loss: 0.4147
2022-10-02 23:59:59 - train: epoch 0025, iter [00300, 01251], lr: 0.000564, loss: 0.4208
2022-10-03 00:00:18 - train: epoch 0025, iter [00310, 01251], lr: 0.000564, loss: 0.3985
2022-10-03 00:00:37 - train: epoch 0025, iter [00320, 01251], lr: 0.000564, loss: 0.4187
2022-10-03 00:00:56 - train: epoch 0025, iter [00330, 01251], lr: 0.000564, loss: 0.4362
2022-10-03 00:01:15 - train: epoch 0025, iter [00340, 01251], lr: 0.000564, loss: 0.4152
2022-10-03 00:01:34 - train: epoch 0025, iter [00350, 01251], lr: 0.000563, loss: 0.4193
2022-10-03 00:01:54 - train: epoch 0025, iter [00360, 01251], lr: 0.000563, loss: 0.4168
2022-10-03 00:02:13 - train: epoch 0025, iter [00370, 01251], lr: 0.000563, loss: 0.4242
2022-10-03 00:02:33 - train: epoch 0025, iter [00380, 01251], lr: 0.000563, loss: 0.4119
2022-10-03 00:02:52 - train: epoch 0025, iter [00390, 01251], lr: 0.000563, loss: 0.4197
2022-10-03 00:03:11 - train: epoch 0025, iter [00400, 01251], lr: 0.000563, loss: 0.4211
2022-10-03 00:03:31 - train: epoch 0025, iter [00410, 01251], lr: 0.000563, loss: 0.4238
2022-10-03 00:03:49 - train: epoch 0025, iter [00420, 01251], lr: 0.000563, loss: 0.4120
2022-10-03 00:04:09 - train: epoch 0025, iter [00430, 01251], lr: 0.000563, loss: 0.4288
2022-10-03 00:04:28 - train: epoch 0025, iter [00440, 01251], lr: 0.000563, loss: 0.4114
2022-10-03 00:04:47 - train: epoch 0025, iter [00450, 01251], lr: 0.000563, loss: 0.4204
2022-10-03 00:05:06 - train: epoch 0025, iter [00460, 01251], lr: 0.000563, loss: 0.4215
2022-10-03 00:05:26 - train: epoch 0025, iter [00470, 01251], lr: 0.000563, loss: 0.4111
2022-10-03 00:05:45 - train: epoch 0025, iter [00480, 01251], lr: 0.000563, loss: 0.4240
2022-10-03 00:06:04 - train: epoch 0025, iter [00490, 01251], lr: 0.000563, loss: 0.4203
2022-10-03 00:06:23 - train: epoch 0025, iter [00500, 01251], lr: 0.000563, loss: 0.4228
2022-10-03 00:06:42 - train: epoch 0025, iter [00510, 01251], lr: 0.000563, loss: 0.4188
2022-10-03 00:07:01 - train: epoch 0025, iter [00520, 01251], lr: 0.000563, loss: 0.4447
2022-10-03 00:07:20 - train: epoch 0025, iter [00530, 01251], lr: 0.000563, loss: 0.4210
2022-10-03 00:07:39 - train: epoch 0025, iter [00540, 01251], lr: 0.000563, loss: 0.4226
2022-10-03 00:07:59 - train: epoch 0025, iter [00550, 01251], lr: 0.000563, loss: 0.4158
2022-10-03 00:08:18 - train: epoch 0025, iter [00560, 01251], lr: 0.000563, loss: 0.4082
2022-10-03 00:08:37 - train: epoch 0025, iter [00570, 01251], lr: 0.000563, loss: 0.4270
2022-10-03 00:08:57 - train: epoch 0025, iter [00580, 01251], lr: 0.000563, loss: 0.4238
2022-10-03 00:09:16 - train: epoch 0025, iter [00590, 01251], lr: 0.000563, loss: 0.4326
2022-10-03 00:09:35 - train: epoch 0025, iter [00600, 01251], lr: 0.000562, loss: 0.4362
2022-10-03 00:09:54 - train: epoch 0025, iter [00610, 01251], lr: 0.000562, loss: 0.4115
2022-10-03 00:10:14 - train: epoch 0025, iter [00620, 01251], lr: 0.000562, loss: 0.4273
2022-10-03 00:10:33 - train: epoch 0025, iter [00630, 01251], lr: 0.000562, loss: 0.4186
2022-10-03 00:10:52 - train: epoch 0025, iter [00640, 01251], lr: 0.000562, loss: 0.4255
2022-10-03 00:11:12 - train: epoch 0025, iter [00650, 01251], lr: 0.000562, loss: 0.4306
2022-10-03 00:11:31 - train: epoch 0025, iter [00660, 01251], lr: 0.000562, loss: 0.4339
2022-10-03 00:11:50 - train: epoch 0025, iter [00670, 01251], lr: 0.000562, loss: 0.4312
2022-10-03 00:12:10 - train: epoch 0025, iter [00680, 01251], lr: 0.000562, loss: 0.4310
2022-10-03 00:12:29 - train: epoch 0025, iter [00690, 01251], lr: 0.000562, loss: 0.4008
2022-10-03 00:12:49 - train: epoch 0025, iter [00700, 01251], lr: 0.000562, loss: 0.4202
2022-10-03 00:13:09 - train: epoch 0025, iter [00710, 01251], lr: 0.000562, loss: 0.4095
2022-10-03 00:13:28 - train: epoch 0025, iter [00720, 01251], lr: 0.000562, loss: 0.4272
2022-10-03 00:13:48 - train: epoch 0025, iter [00730, 01251], lr: 0.000562, loss: 0.4168
2022-10-03 00:14:08 - train: epoch 0025, iter [00740, 01251], lr: 0.000562, loss: 0.3928
2022-10-03 00:14:27 - train: epoch 0025, iter [00750, 01251], lr: 0.000562, loss: 0.4117
2022-10-03 00:14:46 - train: epoch 0025, iter [00760, 01251], lr: 0.000562, loss: 0.4431
2022-10-03 00:15:06 - train: epoch 0025, iter [00770, 01251], lr: 0.000562, loss: 0.4175
2022-10-03 00:15:25 - train: epoch 0025, iter [00780, 01251], lr: 0.000562, loss: 0.4327
2022-10-03 00:15:45 - train: epoch 0025, iter [00790, 01251], lr: 0.000562, loss: 0.4276
2022-10-03 00:16:05 - train: epoch 0025, iter [00800, 01251], lr: 0.000562, loss: 0.4330
2022-10-03 00:16:24 - train: epoch 0025, iter [00810, 01251], lr: 0.000562, loss: 0.4303
2022-10-03 00:16:43 - train: epoch 0025, iter [00820, 01251], lr: 0.000562, loss: 0.4152
2022-10-03 00:17:03 - train: epoch 0025, iter [00830, 01251], lr: 0.000562, loss: 0.4324
2022-10-03 00:17:23 - train: epoch 0025, iter [00840, 01251], lr: 0.000562, loss: 0.4402
2022-10-03 00:17:42 - train: epoch 0025, iter [00850, 01251], lr: 0.000561, loss: 0.4142
2022-10-03 00:18:01 - train: epoch 0025, iter [00860, 01251], lr: 0.000561, loss: 0.4279
2022-10-03 00:18:20 - train: epoch 0025, iter [00870, 01251], lr: 0.000561, loss: 0.4383
2022-10-03 00:18:38 - train: epoch 0025, iter [00880, 01251], lr: 0.000561, loss: 0.4290
2022-10-03 00:18:57 - train: epoch 0025, iter [00890, 01251], lr: 0.000561, loss: 0.4128
2022-10-03 00:19:16 - train: epoch 0025, iter [00900, 01251], lr: 0.000561, loss: 0.4318
2022-10-03 00:19:35 - train: epoch 0025, iter [00910, 01251], lr: 0.000561, loss: 0.4308
2022-10-03 00:19:54 - train: epoch 0025, iter [00920, 01251], lr: 0.000561, loss: 0.4123
2022-10-03 00:20:13 - train: epoch 0025, iter [00930, 01251], lr: 0.000561, loss: 0.4135
2022-10-03 00:20:32 - train: epoch 0025, iter [00940, 01251], lr: 0.000561, loss: 0.4242
2022-10-03 00:20:50 - train: epoch 0025, iter [00950, 01251], lr: 0.000561, loss: 0.4259
2022-10-03 00:21:09 - train: epoch 0025, iter [00960, 01251], lr: 0.000561, loss: 0.4267
2022-10-03 00:21:28 - train: epoch 0025, iter [00970, 01251], lr: 0.000561, loss: 0.4278
2022-10-03 00:21:47 - train: epoch 0025, iter [00980, 01251], lr: 0.000561, loss: 0.4225
2022-10-03 00:22:06 - train: epoch 0025, iter [00990, 01251], lr: 0.000561, loss: 0.4304
2022-10-03 00:22:25 - train: epoch 0025, iter [01000, 01251], lr: 0.000561, loss: 0.4180
2022-10-03 00:22:45 - train: epoch 0025, iter [01010, 01251], lr: 0.000561, loss: 0.4034
2022-10-03 00:23:04 - train: epoch 0025, iter [01020, 01251], lr: 0.000561, loss: 0.4130
2022-10-03 00:23:23 - train: epoch 0025, iter [01030, 01251], lr: 0.000561, loss: 0.4296
2022-10-03 00:23:42 - train: epoch 0025, iter [01040, 01251], lr: 0.000561, loss: 0.4331
2022-10-03 00:24:01 - train: epoch 0025, iter [01050, 01251], lr: 0.000561, loss: 0.4186
2022-10-03 00:24:20 - train: epoch 0025, iter [01060, 01251], lr: 0.000561, loss: 0.4073
2022-10-03 00:24:40 - train: epoch 0025, iter [01070, 01251], lr: 0.000561, loss: 0.4176
2022-10-03 00:24:59 - train: epoch 0025, iter [01080, 01251], lr: 0.000561, loss: 0.4108
2022-10-03 00:25:18 - train: epoch 0025, iter [01090, 01251], lr: 0.000560, loss: 0.4240
2022-10-03 00:25:37 - train: epoch 0025, iter [01100, 01251], lr: 0.000560, loss: 0.4267
2022-10-03 00:25:57 - train: epoch 0025, iter [01110, 01251], lr: 0.000560, loss: 0.3981
2022-10-03 00:26:16 - train: epoch 0025, iter [01120, 01251], lr: 0.000560, loss: 0.4094
2022-10-03 00:26:34 - train: epoch 0025, iter [01130, 01251], lr: 0.000560, loss: 0.4305
2022-10-03 00:26:54 - train: epoch 0025, iter [01140, 01251], lr: 0.000560, loss: 0.4138
2022-10-03 00:27:13 - train: epoch 0025, iter [01150, 01251], lr: 0.000560, loss: 0.4380
2022-10-03 00:27:32 - train: epoch 0025, iter [01160, 01251], lr: 0.000560, loss: 0.4115
2022-10-03 00:27:50 - train: epoch 0025, iter [01170, 01251], lr: 0.000560, loss: 0.4225
2022-10-03 00:28:10 - train: epoch 0025, iter [01180, 01251], lr: 0.000560, loss: 0.4135
2022-10-03 00:28:29 - train: epoch 0025, iter [01190, 01251], lr: 0.000560, loss: 0.4056
2022-10-03 00:28:48 - train: epoch 0025, iter [01200, 01251], lr: 0.000560, loss: 0.4200
2022-10-03 00:29:07 - train: epoch 0025, iter [01210, 01251], lr: 0.000560, loss: 0.4156
2022-10-03 00:29:26 - train: epoch 0025, iter [01220, 01251], lr: 0.000560, loss: 0.4336
2022-10-03 00:29:44 - train: epoch 0025, iter [01230, 01251], lr: 0.000560, loss: 0.4013
2022-10-03 00:30:03 - train: epoch 0025, iter [01240, 01251], lr: 0.000560, loss: 0.3992
2022-10-03 00:30:20 - train: epoch 0025, iter [01250, 01251], lr: 0.000560, loss: 0.4341
2022-10-03 00:30:24 - train: epoch 025, train_loss: 0.4213
2022-10-03 00:30:27 - until epoch: 025, best_loss: 0.4213
2022-10-03 00:30:27 - epoch 026 lr: 0.000560
2022-10-03 00:30:53 - train: epoch 0026, iter [00010, 01251], lr: 0.000560, loss: 0.4170
2022-10-03 00:31:13 - train: epoch 0026, iter [00020, 01251], lr: 0.000560, loss: 0.4288
2022-10-03 00:31:32 - train: epoch 0026, iter [00030, 01251], lr: 0.000560, loss: 0.4268
2022-10-03 00:31:51 - train: epoch 0026, iter [00040, 01251], lr: 0.000560, loss: 0.4135
2022-10-03 00:32:09 - train: epoch 0026, iter [00050, 01251], lr: 0.000560, loss: 0.4382
2022-10-03 00:32:29 - train: epoch 0026, iter [00060, 01251], lr: 0.000560, loss: 0.4362
2022-10-03 00:32:48 - train: epoch 0026, iter [00070, 01251], lr: 0.000560, loss: 0.4202
2022-10-03 00:33:07 - train: epoch 0026, iter [00080, 01251], lr: 0.000559, loss: 0.4132
2022-10-03 00:33:26 - train: epoch 0026, iter [00090, 01251], lr: 0.000559, loss: 0.4109
2022-10-03 00:33:45 - train: epoch 0026, iter [00100, 01251], lr: 0.000559, loss: 0.4289
2022-10-03 00:34:04 - train: epoch 0026, iter [00110, 01251], lr: 0.000559, loss: 0.4033
2022-10-03 00:34:23 - train: epoch 0026, iter [00120, 01251], lr: 0.000559, loss: 0.4161
2022-10-03 00:34:42 - train: epoch 0026, iter [00130, 01251], lr: 0.000559, loss: 0.4196
2022-10-03 00:35:01 - train: epoch 0026, iter [00140, 01251], lr: 0.000559, loss: 0.4407
2022-10-03 00:35:20 - train: epoch 0026, iter [00150, 01251], lr: 0.000559, loss: 0.4202
2022-10-03 00:35:39 - train: epoch 0026, iter [00160, 01251], lr: 0.000559, loss: 0.4184
2022-10-03 00:35:59 - train: epoch 0026, iter [00170, 01251], lr: 0.000559, loss: 0.4183
2022-10-03 00:36:18 - train: epoch 0026, iter [00180, 01251], lr: 0.000559, loss: 0.4202
2022-10-03 00:36:37 - train: epoch 0026, iter [00190, 01251], lr: 0.000559, loss: 0.4082
2022-10-03 00:36:55 - train: epoch 0026, iter [00200, 01251], lr: 0.000559, loss: 0.4266
2022-10-03 00:37:13 - train: epoch 0026, iter [00210, 01251], lr: 0.000559, loss: 0.4226
2022-10-03 00:37:32 - train: epoch 0026, iter [00220, 01251], lr: 0.000559, loss: 0.4313
2022-10-03 00:37:51 - train: epoch 0026, iter [00230, 01251], lr: 0.000559, loss: 0.4183
2022-10-03 00:38:09 - train: epoch 0026, iter [00240, 01251], lr: 0.000559, loss: 0.4085
2022-10-03 00:38:28 - train: epoch 0026, iter [00250, 01251], lr: 0.000559, loss: 0.4324
2022-10-03 00:38:47 - train: epoch 0026, iter [00260, 01251], lr: 0.000559, loss: 0.4096
2022-10-03 00:39:05 - train: epoch 0026, iter [00270, 01251], lr: 0.000559, loss: 0.4040
2022-10-03 00:39:24 - train: epoch 0026, iter [00280, 01251], lr: 0.000559, loss: 0.4164
2022-10-03 00:39:43 - train: epoch 0026, iter [00290, 01251], lr: 0.000559, loss: 0.4152
2022-10-03 00:40:02 - train: epoch 0026, iter [00300, 01251], lr: 0.000559, loss: 0.4280
2022-10-03 00:40:20 - train: epoch 0026, iter [00310, 01251], lr: 0.000559, loss: 0.4160
2022-10-03 00:40:39 - train: epoch 0026, iter [00320, 01251], lr: 0.000558, loss: 0.4198
2022-10-03 00:40:58 - train: epoch 0026, iter [00330, 01251], lr: 0.000558, loss: 0.4491
2022-10-03 00:41:18 - train: epoch 0026, iter [00340, 01251], lr: 0.000558, loss: 0.4333
2022-10-03 00:41:37 - train: epoch 0026, iter [00350, 01251], lr: 0.000558, loss: 0.4169
2022-10-03 00:41:56 - train: epoch 0026, iter [00360, 01251], lr: 0.000558, loss: 0.4084
2022-10-03 00:42:14 - train: epoch 0026, iter [00370, 01251], lr: 0.000558, loss: 0.4244
2022-10-03 00:42:33 - train: epoch 0026, iter [00380, 01251], lr: 0.000558, loss: 0.4239
2022-10-03 00:42:52 - train: epoch 0026, iter [00390, 01251], lr: 0.000558, loss: 0.4258
2022-10-03 00:43:11 - train: epoch 0026, iter [00400, 01251], lr: 0.000558, loss: 0.4199
2022-10-03 00:43:29 - train: epoch 0026, iter [00410, 01251], lr: 0.000558, loss: 0.4317
2022-10-03 00:43:48 - train: epoch 0026, iter [00420, 01251], lr: 0.000558, loss: 0.4266
2022-10-03 00:44:07 - train: epoch 0026, iter [00430, 01251], lr: 0.000558, loss: 0.4302
2022-10-03 00:44:25 - train: epoch 0026, iter [00440, 01251], lr: 0.000558, loss: 0.4082
2022-10-03 00:44:44 - train: epoch 0026, iter [00450, 01251], lr: 0.000558, loss: 0.4282
2022-10-03 00:45:03 - train: epoch 0026, iter [00460, 01251], lr: 0.000558, loss: 0.4119
2022-10-03 00:45:21 - train: epoch 0026, iter [00470, 01251], lr: 0.000558, loss: 0.4227
2022-10-03 00:45:40 - train: epoch 0026, iter [00480, 01251], lr: 0.000558, loss: 0.4183
2022-10-03 00:45:59 - train: epoch 0026, iter [00490, 01251], lr: 0.000558, loss: 0.4426
2022-10-03 00:46:17 - train: epoch 0026, iter [00500, 01251], lr: 0.000558, loss: 0.4193
2022-10-03 00:46:36 - train: epoch 0026, iter [00510, 01251], lr: 0.000558, loss: 0.4091
2022-10-03 00:46:54 - train: epoch 0026, iter [00520, 01251], lr: 0.000558, loss: 0.4303
2022-10-03 00:47:13 - train: epoch 0026, iter [00530, 01251], lr: 0.000558, loss: 0.4170
2022-10-03 00:47:32 - train: epoch 0026, iter [00540, 01251], lr: 0.000558, loss: 0.4296
2022-10-03 00:47:51 - train: epoch 0026, iter [00550, 01251], lr: 0.000557, loss: 0.4161
2022-10-03 00:48:09 - train: epoch 0026, iter [00560, 01251], lr: 0.000557, loss: 0.4028
2022-10-03 00:48:28 - train: epoch 0026, iter [00570, 01251], lr: 0.000557, loss: 0.4093
2022-10-03 00:48:47 - train: epoch 0026, iter [00580, 01251], lr: 0.000557, loss: 0.4077
2022-10-03 00:49:06 - train: epoch 0026, iter [00590, 01251], lr: 0.000557, loss: 0.4193
2022-10-03 00:49:24 - train: epoch 0026, iter [00600, 01251], lr: 0.000557, loss: 0.4289
2022-10-03 00:49:43 - train: epoch 0026, iter [00610, 01251], lr: 0.000557, loss: 0.4283
2022-10-03 00:50:01 - train: epoch 0026, iter [00620, 01251], lr: 0.000557, loss: 0.4063
2022-10-03 00:50:20 - train: epoch 0026, iter [00630, 01251], lr: 0.000557, loss: 0.4365
2022-10-03 00:50:38 - train: epoch 0026, iter [00640, 01251], lr: 0.000557, loss: 0.4305
2022-10-03 00:50:57 - train: epoch 0026, iter [00650, 01251], lr: 0.000557, loss: 0.4107
2022-10-03 00:51:15 - train: epoch 0026, iter [00660, 01251], lr: 0.000557, loss: 0.4114
2022-10-03 00:51:34 - train: epoch 0026, iter [00670, 01251], lr: 0.000557, loss: 0.4162
2022-10-03 00:51:52 - train: epoch 0026, iter [00680, 01251], lr: 0.000557, loss: 0.4339
2022-10-03 00:52:11 - train: epoch 0026, iter [00690, 01251], lr: 0.000557, loss: 0.4036
2022-10-03 00:52:29 - train: epoch 0026, iter [00700, 01251], lr: 0.000557, loss: 0.4031
2022-10-03 00:52:49 - train: epoch 0026, iter [00710, 01251], lr: 0.000557, loss: 0.4345
2022-10-03 00:53:07 - train: epoch 0026, iter [00720, 01251], lr: 0.000557, loss: 0.4061
2022-10-03 00:53:25 - train: epoch 0026, iter [00730, 01251], lr: 0.000557, loss: 0.4194
2022-10-03 00:53:44 - train: epoch 0026, iter [00740, 01251], lr: 0.000557, loss: 0.4251
2022-10-03 00:54:03 - train: epoch 0026, iter [00750, 01251], lr: 0.000557, loss: 0.4050
2022-10-03 00:54:21 - train: epoch 0026, iter [00760, 01251], lr: 0.000557, loss: 0.4218
2022-10-03 00:54:40 - train: epoch 0026, iter [00770, 01251], lr: 0.000557, loss: 0.4327
2022-10-03 00:54:58 - train: epoch 0026, iter [00780, 01251], lr: 0.000556, loss: 0.4218
2022-10-03 00:55:17 - train: epoch 0026, iter [00790, 01251], lr: 0.000556, loss: 0.4268
2022-10-03 00:55:35 - train: epoch 0026, iter [00800, 01251], lr: 0.000556, loss: 0.4344
2022-10-03 00:55:54 - train: epoch 0026, iter [00810, 01251], lr: 0.000556, loss: 0.4274
2022-10-03 00:56:12 - train: epoch 0026, iter [00820, 01251], lr: 0.000556, loss: 0.4385
2022-10-03 00:56:30 - train: epoch 0026, iter [00830, 01251], lr: 0.000556, loss: 0.4254
2022-10-03 00:56:49 - train: epoch 0026, iter [00840, 01251], lr: 0.000556, loss: 0.4040
2022-10-03 00:57:07 - train: epoch 0026, iter [00850, 01251], lr: 0.000556, loss: 0.4268
2022-10-03 00:57:25 - train: epoch 0026, iter [00860, 01251], lr: 0.000556, loss: 0.4289
2022-10-03 00:57:44 - train: epoch 0026, iter [00870, 01251], lr: 0.000556, loss: 0.4072
2022-10-03 00:58:02 - train: epoch 0026, iter [00880, 01251], lr: 0.000556, loss: 0.4223
2022-10-03 00:58:21 - train: epoch 0026, iter [00890, 01251], lr: 0.000556, loss: 0.4202
2022-10-03 00:58:39 - train: epoch 0026, iter [00900, 01251], lr: 0.000556, loss: 0.4176
2022-10-03 00:58:57 - train: epoch 0026, iter [00910, 01251], lr: 0.000556, loss: 0.4194
2022-10-03 00:59:16 - train: epoch 0026, iter [00920, 01251], lr: 0.000556, loss: 0.4181
2022-10-03 00:59:34 - train: epoch 0026, iter [00930, 01251], lr: 0.000556, loss: 0.4084
2022-10-03 00:59:53 - train: epoch 0026, iter [00940, 01251], lr: 0.000556, loss: 0.4277
2022-10-03 01:00:11 - train: epoch 0026, iter [00950, 01251], lr: 0.000556, loss: 0.4229
2022-10-03 01:00:30 - train: epoch 0026, iter [00960, 01251], lr: 0.000556, loss: 0.4095
2022-10-03 01:00:48 - train: epoch 0026, iter [00970, 01251], lr: 0.000556, loss: 0.3965
2022-10-03 01:01:06 - train: epoch 0026, iter [00980, 01251], lr: 0.000556, loss: 0.4271
2022-10-03 01:01:25 - train: epoch 0026, iter [00990, 01251], lr: 0.000556, loss: 0.4202
2022-10-03 01:01:43 - train: epoch 0026, iter [01000, 01251], lr: 0.000556, loss: 0.4125
2022-10-03 01:02:01 - train: epoch 0026, iter [01010, 01251], lr: 0.000555, loss: 0.4383
2022-10-03 01:02:20 - train: epoch 0026, iter [01020, 01251], lr: 0.000555, loss: 0.4132
2022-10-03 01:02:39 - train: epoch 0026, iter [01030, 01251], lr: 0.000555, loss: 0.4092
2022-10-03 01:02:57 - train: epoch 0026, iter [01040, 01251], lr: 0.000555, loss: 0.4297
2022-10-03 01:03:15 - train: epoch 0026, iter [01050, 01251], lr: 0.000555, loss: 0.4343
2022-10-03 01:03:34 - train: epoch 0026, iter [01060, 01251], lr: 0.000555, loss: 0.4136
2022-10-03 01:03:52 - train: epoch 0026, iter [01070, 01251], lr: 0.000555, loss: 0.4309
2022-10-03 01:04:11 - train: epoch 0026, iter [01080, 01251], lr: 0.000555, loss: 0.4104
2022-10-03 01:04:29 - train: epoch 0026, iter [01090, 01251], lr: 0.000555, loss: 0.4275
2022-10-03 01:04:47 - train: epoch 0026, iter [01100, 01251], lr: 0.000555, loss: 0.4371
2022-10-03 01:05:06 - train: epoch 0026, iter [01110, 01251], lr: 0.000555, loss: 0.4060
2022-10-03 01:05:24 - train: epoch 0026, iter [01120, 01251], lr: 0.000555, loss: 0.4073
2022-10-03 01:05:42 - train: epoch 0026, iter [01130, 01251], lr: 0.000555, loss: 0.4243
2022-10-03 01:06:00 - train: epoch 0026, iter [01140, 01251], lr: 0.000555, loss: 0.4254
2022-10-03 01:06:19 - train: epoch 0026, iter [01150, 01251], lr: 0.000555, loss: 0.4262
2022-10-03 01:06:37 - train: epoch 0026, iter [01160, 01251], lr: 0.000555, loss: 0.4175
2022-10-03 01:06:55 - train: epoch 0026, iter [01170, 01251], lr: 0.000555, loss: 0.4237
2022-10-03 01:07:14 - train: epoch 0026, iter [01180, 01251], lr: 0.000555, loss: 0.4164
2022-10-03 01:07:32 - train: epoch 0026, iter [01190, 01251], lr: 0.000555, loss: 0.4077
2022-10-03 01:07:51 - train: epoch 0026, iter [01200, 01251], lr: 0.000555, loss: 0.4426
2022-10-03 01:08:09 - train: epoch 0026, iter [01210, 01251], lr: 0.000555, loss: 0.4378
2022-10-03 01:08:28 - train: epoch 0026, iter [01220, 01251], lr: 0.000555, loss: 0.4183
2022-10-03 01:08:46 - train: epoch 0026, iter [01230, 01251], lr: 0.000555, loss: 0.4219
2022-10-03 01:09:05 - train: epoch 0026, iter [01240, 01251], lr: 0.000554, loss: 0.4246
2022-10-03 01:09:23 - train: epoch 0026, iter [01250, 01251], lr: 0.000554, loss: 0.4035
2022-10-03 01:09:27 - train: epoch 026, train_loss: 0.4206
2022-10-03 01:09:30 - until epoch: 026, best_loss: 0.4206
2022-10-03 01:09:30 - epoch 027 lr: 0.000554
2022-10-03 01:09:55 - train: epoch 0027, iter [00010, 01251], lr: 0.000554, loss: 0.4087
2022-10-03 01:10:13 - train: epoch 0027, iter [00020, 01251], lr: 0.000554, loss: 0.4095
2022-10-03 01:10:31 - train: epoch 0027, iter [00030, 01251], lr: 0.000554, loss: 0.4425
2022-10-03 01:10:49 - train: epoch 0027, iter [00040, 01251], lr: 0.000554, loss: 0.4240
2022-10-03 01:11:08 - train: epoch 0027, iter [00050, 01251], lr: 0.000554, loss: 0.4199
2022-10-03 01:11:26 - train: epoch 0027, iter [00060, 01251], lr: 0.000554, loss: 0.4003
2022-10-03 01:11:44 - train: epoch 0027, iter [00070, 01251], lr: 0.000554, loss: 0.4225
2022-10-03 01:12:02 - train: epoch 0027, iter [00080, 01251], lr: 0.000554, loss: 0.4202
2022-10-03 01:12:21 - train: epoch 0027, iter [00090, 01251], lr: 0.000554, loss: 0.4186
2022-10-03 01:12:39 - train: epoch 0027, iter [00100, 01251], lr: 0.000554, loss: 0.4347
2022-10-03 01:12:57 - train: epoch 0027, iter [00110, 01251], lr: 0.000554, loss: 0.4214
2022-10-03 01:13:16 - train: epoch 0027, iter [00120, 01251], lr: 0.000554, loss: 0.4237
2022-10-03 01:13:34 - train: epoch 0027, iter [00130, 01251], lr: 0.000554, loss: 0.4231
2022-10-03 01:13:52 - train: epoch 0027, iter [00140, 01251], lr: 0.000554, loss: 0.4279
2022-10-03 01:14:10 - train: epoch 0027, iter [00150, 01251], lr: 0.000554, loss: 0.4301
2022-10-03 01:14:29 - train: epoch 0027, iter [00160, 01251], lr: 0.000554, loss: 0.4315
2022-10-03 01:14:47 - train: epoch 0027, iter [00170, 01251], lr: 0.000554, loss: 0.4050
2022-10-03 01:15:06 - train: epoch 0027, iter [00180, 01251], lr: 0.000554, loss: 0.4209
2022-10-03 01:15:24 - train: epoch 0027, iter [00190, 01251], lr: 0.000554, loss: 0.4448
2022-10-03 01:15:43 - train: epoch 0027, iter [00200, 01251], lr: 0.000554, loss: 0.4164
2022-10-03 01:16:01 - train: epoch 0027, iter [00210, 01251], lr: 0.000553, loss: 0.4318
2022-10-03 01:16:20 - train: epoch 0027, iter [00220, 01251], lr: 0.000553, loss: 0.4122
2022-10-03 01:16:38 - train: epoch 0027, iter [00230, 01251], lr: 0.000553, loss: 0.4291
2022-10-03 01:16:56 - train: epoch 0027, iter [00240, 01251], lr: 0.000553, loss: 0.4107
2022-10-03 01:17:15 - train: epoch 0027, iter [00250, 01251], lr: 0.000553, loss: 0.4268
2022-10-03 01:17:33 - train: epoch 0027, iter [00260, 01251], lr: 0.000553, loss: 0.4137
2022-10-03 01:17:52 - train: epoch 0027, iter [00270, 01251], lr: 0.000553, loss: 0.4304
2022-10-03 01:18:10 - train: epoch 0027, iter [00280, 01251], lr: 0.000553, loss: 0.4222
2022-10-03 01:18:29 - train: epoch 0027, iter [00290, 01251], lr: 0.000553, loss: 0.4069
2022-10-03 01:18:47 - train: epoch 0027, iter [00300, 01251], lr: 0.000553, loss: 0.4163
2022-10-03 01:19:06 - train: epoch 0027, iter [00310, 01251], lr: 0.000553, loss: 0.4297
2022-10-03 01:19:24 - train: epoch 0027, iter [00320, 01251], lr: 0.000553, loss: 0.4194
2022-10-03 01:19:43 - train: epoch 0027, iter [00330, 01251], lr: 0.000553, loss: 0.4150
2022-10-03 01:20:02 - train: epoch 0027, iter [00340, 01251], lr: 0.000553, loss: 0.4185
2022-10-03 01:20:21 - train: epoch 0027, iter [00350, 01251], lr: 0.000553, loss: 0.4374
2022-10-03 01:20:40 - train: epoch 0027, iter [00360, 01251], lr: 0.000553, loss: 0.4364
2022-10-03 01:20:59 - train: epoch 0027, iter [00370, 01251], lr: 0.000553, loss: 0.4324
2022-10-03 01:21:18 - train: epoch 0027, iter [00380, 01251], lr: 0.000553, loss: 0.4170
2022-10-03 01:21:37 - train: epoch 0027, iter [00390, 01251], lr: 0.000553, loss: 0.4166
2022-10-03 01:21:56 - train: epoch 0027, iter [00400, 01251], lr: 0.000553, loss: 0.4293
2022-10-03 01:22:15 - train: epoch 0027, iter [00410, 01251], lr: 0.000553, loss: 0.4174
2022-10-03 01:22:34 - train: epoch 0027, iter [00420, 01251], lr: 0.000553, loss: 0.4374
2022-10-03 01:22:53 - train: epoch 0027, iter [00430, 01251], lr: 0.000552, loss: 0.4132
2022-10-03 01:23:12 - train: epoch 0027, iter [00440, 01251], lr: 0.000552, loss: 0.4149
2022-10-03 01:23:31 - train: epoch 0027, iter [00450, 01251], lr: 0.000552, loss: 0.4103
2022-10-03 01:23:50 - train: epoch 0027, iter [00460, 01251], lr: 0.000552, loss: 0.4094
2022-10-03 01:24:09 - train: epoch 0027, iter [00470, 01251], lr: 0.000552, loss: 0.4080
2022-10-03 01:24:29 - train: epoch 0027, iter [00480, 01251], lr: 0.000552, loss: 0.4016
2022-10-03 01:24:48 - train: epoch 0027, iter [00490, 01251], lr: 0.000552, loss: 0.4311
2022-10-03 01:25:07 - train: epoch 0027, iter [00500, 01251], lr: 0.000552, loss: 0.4284
2022-10-03 01:25:27 - train: epoch 0027, iter [00510, 01251], lr: 0.000552, loss: 0.3890
2022-10-03 01:25:46 - train: epoch 0027, iter [00520, 01251], lr: 0.000552, loss: 0.4296
2022-10-03 01:26:05 - train: epoch 0027, iter [00530, 01251], lr: 0.000552, loss: 0.4388
2022-10-03 01:26:24 - train: epoch 0027, iter [00540, 01251], lr: 0.000552, loss: 0.4218
2022-10-03 01:26:43 - train: epoch 0027, iter [00550, 01251], lr: 0.000552, loss: 0.4243
2022-10-03 01:27:03 - train: epoch 0027, iter [00560, 01251], lr: 0.000552, loss: 0.4286
2022-10-03 01:27:22 - train: epoch 0027, iter [00570, 01251], lr: 0.000552, loss: 0.4302
2022-10-03 01:27:42 - train: epoch 0027, iter [00580, 01251], lr: 0.000552, loss: 0.4321
2022-10-03 01:28:01 - train: epoch 0027, iter [00590, 01251], lr: 0.000552, loss: 0.4152
2022-10-03 01:28:20 - train: epoch 0027, iter [00600, 01251], lr: 0.000552, loss: 0.4204
2022-10-03 01:28:39 - train: epoch 0027, iter [00610, 01251], lr: 0.000552, loss: 0.3935
2022-10-03 01:28:59 - train: epoch 0027, iter [00620, 01251], lr: 0.000552, loss: 0.4142
2022-10-03 01:29:19 - train: epoch 0027, iter [00630, 01251], lr: 0.000552, loss: 0.4354
2022-10-03 01:29:38 - train: epoch 0027, iter [00640, 01251], lr: 0.000552, loss: 0.4349
2022-10-03 01:29:57 - train: epoch 0027, iter [00650, 01251], lr: 0.000551, loss: 0.4228
2022-10-03 01:30:17 - train: epoch 0027, iter [00660, 01251], lr: 0.000551, loss: 0.4217
2022-10-03 01:30:37 - train: epoch 0027, iter [00670, 01251], lr: 0.000551, loss: 0.4070
2022-10-03 01:30:55 - train: epoch 0027, iter [00680, 01251], lr: 0.000551, loss: 0.4208
2022-10-03 01:31:14 - train: epoch 0027, iter [00690, 01251], lr: 0.000551, loss: 0.4257
2022-10-03 01:31:34 - train: epoch 0027, iter [00700, 01251], lr: 0.000551, loss: 0.4197
2022-10-03 01:31:53 - train: epoch 0027, iter [00710, 01251], lr: 0.000551, loss: 0.4367
2022-10-03 01:32:13 - train: epoch 0027, iter [00720, 01251], lr: 0.000551, loss: 0.4280
2022-10-03 01:32:32 - train: epoch 0027, iter [00730, 01251], lr: 0.000551, loss: 0.4337
2022-10-03 01:32:51 - train: epoch 0027, iter [00740, 01251], lr: 0.000551, loss: 0.4223
2022-10-03 01:33:10 - train: epoch 0027, iter [00750, 01251], lr: 0.000551, loss: 0.4228
2022-10-03 01:33:28 - train: epoch 0027, iter [00760, 01251], lr: 0.000551, loss: 0.4447
2022-10-03 01:33:47 - train: epoch 0027, iter [00770, 01251], lr: 0.000551, loss: 0.4331
2022-10-03 01:34:05 - train: epoch 0027, iter [00780, 01251], lr: 0.000551, loss: 0.4310
2022-10-03 01:34:24 - train: epoch 0027, iter [00790, 01251], lr: 0.000551, loss: 0.3970
2022-10-03 01:34:42 - train: epoch 0027, iter [00800, 01251], lr: 0.000551, loss: 0.4079
2022-10-03 01:35:01 - train: epoch 0027, iter [00810, 01251], lr: 0.000551, loss: 0.4272
2022-10-03 01:35:20 - train: epoch 0027, iter [00820, 01251], lr: 0.000551, loss: 0.4416
2022-10-03 01:35:39 - train: epoch 0027, iter [00830, 01251], lr: 0.000551, loss: 0.3970
2022-10-03 01:35:58 - train: epoch 0027, iter [00840, 01251], lr: 0.000551, loss: 0.4463
2022-10-03 01:36:16 - train: epoch 0027, iter [00850, 01251], lr: 0.000551, loss: 0.3970
2022-10-03 01:36:35 - train: epoch 0027, iter [00860, 01251], lr: 0.000551, loss: 0.4388
2022-10-03 01:36:54 - train: epoch 0027, iter [00870, 01251], lr: 0.000550, loss: 0.4408
2022-10-03 01:37:13 - train: epoch 0027, iter [00880, 01251], lr: 0.000550, loss: 0.4150
2022-10-03 01:37:32 - train: epoch 0027, iter [00890, 01251], lr: 0.000550, loss: 0.4310
2022-10-03 01:37:51 - train: epoch 0027, iter [00900, 01251], lr: 0.000550, loss: 0.4232
2022-10-03 01:38:10 - train: epoch 0027, iter [00910, 01251], lr: 0.000550, loss: 0.4060
2022-10-03 01:38:29 - train: epoch 0027, iter [00920, 01251], lr: 0.000550, loss: 0.4182
2022-10-03 01:38:48 - train: epoch 0027, iter [00930, 01251], lr: 0.000550, loss: 0.4081
2022-10-03 01:39:08 - train: epoch 0027, iter [00940, 01251], lr: 0.000550, loss: 0.4383
2022-10-03 01:39:27 - train: epoch 0027, iter [00950, 01251], lr: 0.000550, loss: 0.4084
2022-10-03 01:39:47 - train: epoch 0027, iter [00960, 01251], lr: 0.000550, loss: 0.4066
2022-10-03 01:40:07 - train: epoch 0027, iter [00970, 01251], lr: 0.000550, loss: 0.4111
2022-10-03 01:40:26 - train: epoch 0027, iter [00980, 01251], lr: 0.000550, loss: 0.4287
2022-10-03 01:40:46 - train: epoch 0027, iter [00990, 01251], lr: 0.000550, loss: 0.4225
2022-10-03 01:41:06 - train: epoch 0027, iter [01000, 01251], lr: 0.000550, loss: 0.4188
2022-10-03 01:41:26 - train: epoch 0027, iter [01010, 01251], lr: 0.000550, loss: 0.4262
2022-10-03 01:41:46 - train: epoch 0027, iter [01020, 01251], lr: 0.000550, loss: 0.4166
2022-10-03 01:42:06 - train: epoch 0027, iter [01030, 01251], lr: 0.000550, loss: 0.4115
2022-10-03 01:42:26 - train: epoch 0027, iter [01040, 01251], lr: 0.000550, loss: 0.4018
2022-10-03 01:42:46 - train: epoch 0027, iter [01050, 01251], lr: 0.000550, loss: 0.4150
2022-10-03 01:43:05 - train: epoch 0027, iter [01060, 01251], lr: 0.000550, loss: 0.4395
2022-10-03 01:43:25 - train: epoch 0027, iter [01070, 01251], lr: 0.000550, loss: 0.4275
2022-10-03 01:43:44 - train: epoch 0027, iter [01080, 01251], lr: 0.000550, loss: 0.4149
2022-10-03 01:44:04 - train: epoch 0027, iter [01090, 01251], lr: 0.000549, loss: 0.4104
2022-10-03 01:44:24 - train: epoch 0027, iter [01100, 01251], lr: 0.000549, loss: 0.4307
2022-10-03 01:44:43 - train: epoch 0027, iter [01110, 01251], lr: 0.000549, loss: 0.4180
2022-10-03 01:45:02 - train: epoch 0027, iter [01120, 01251], lr: 0.000549, loss: 0.4111
2022-10-03 01:45:22 - train: epoch 0027, iter [01130, 01251], lr: 0.000549, loss: 0.4190
2022-10-03 01:45:42 - train: epoch 0027, iter [01140, 01251], lr: 0.000549, loss: 0.4337
2022-10-03 01:46:01 - train: epoch 0027, iter [01150, 01251], lr: 0.000549, loss: 0.4221
2022-10-03 01:46:21 - train: epoch 0027, iter [01160, 01251], lr: 0.000549, loss: 0.4119
2022-10-03 01:46:40 - train: epoch 0027, iter [01170, 01251], lr: 0.000549, loss: 0.4288
2022-10-03 01:47:00 - train: epoch 0027, iter [01180, 01251], lr: 0.000549, loss: 0.4400
2022-10-03 01:47:20 - train: epoch 0027, iter [01190, 01251], lr: 0.000549, loss: 0.3975
2022-10-03 01:47:39 - train: epoch 0027, iter [01200, 01251], lr: 0.000549, loss: 0.4240
2022-10-03 01:47:59 - train: epoch 0027, iter [01210, 01251], lr: 0.000549, loss: 0.4254
2022-10-03 01:48:18 - train: epoch 0027, iter [01220, 01251], lr: 0.000549, loss: 0.4208
2022-10-03 01:48:38 - train: epoch 0027, iter [01230, 01251], lr: 0.000549, loss: 0.4172
2022-10-03 01:48:57 - train: epoch 0027, iter [01240, 01251], lr: 0.000549, loss: 0.4101
2022-10-03 01:49:16 - train: epoch 0027, iter [01250, 01251], lr: 0.000549, loss: 0.4281
2022-10-03 01:49:19 - train: epoch 027, train_loss: 0.4201
2022-10-03 01:49:23 - until epoch: 027, best_loss: 0.4201
2022-10-03 01:49:23 - epoch 028 lr: 0.000549
2022-10-03 01:49:49 - train: epoch 0028, iter [00010, 01251], lr: 0.000549, loss: 0.4302
2022-10-03 01:50:08 - train: epoch 0028, iter [00020, 01251], lr: 0.000549, loss: 0.4152
2022-10-03 01:50:28 - train: epoch 0028, iter [00030, 01251], lr: 0.000549, loss: 0.4369
2022-10-03 01:50:47 - train: epoch 0028, iter [00040, 01251], lr: 0.000549, loss: 0.4182
2022-10-03 01:51:06 - train: epoch 0028, iter [00050, 01251], lr: 0.000548, loss: 0.4052
2022-10-03 01:51:25 - train: epoch 0028, iter [00060, 01251], lr: 0.000548, loss: 0.4142
2022-10-03 01:51:44 - train: epoch 0028, iter [00070, 01251], lr: 0.000548, loss: 0.4045
2022-10-03 01:52:03 - train: epoch 0028, iter [00080, 01251], lr: 0.000548, loss: 0.4233
2022-10-03 01:52:22 - train: epoch 0028, iter [00090, 01251], lr: 0.000548, loss: 0.4103
2022-10-03 01:52:42 - train: epoch 0028, iter [00100, 01251], lr: 0.000548, loss: 0.4191
2022-10-03 01:53:01 - train: epoch 0028, iter [00110, 01251], lr: 0.000548, loss: 0.4306
2022-10-03 01:53:21 - train: epoch 0028, iter [00120, 01251], lr: 0.000548, loss: 0.4151
2022-10-03 01:53:40 - train: epoch 0028, iter [00130, 01251], lr: 0.000548, loss: 0.4003
2022-10-03 01:53:59 - train: epoch 0028, iter [00140, 01251], lr: 0.000548, loss: 0.4118
2022-10-03 01:54:17 - train: epoch 0028, iter [00150, 01251], lr: 0.000548, loss: 0.4059
2022-10-03 01:54:36 - train: epoch 0028, iter [00160, 01251], lr: 0.000548, loss: 0.4240
2022-10-03 01:54:55 - train: epoch 0028, iter [00170, 01251], lr: 0.000548, loss: 0.4135
2022-10-03 01:55:14 - train: epoch 0028, iter [00180, 01251], lr: 0.000548, loss: 0.4108
2022-10-03 01:55:33 - train: epoch 0028, iter [00190, 01251], lr: 0.000548, loss: 0.4250
2022-10-03 01:55:52 - train: epoch 0028, iter [00200, 01251], lr: 0.000548, loss: 0.4326
2022-10-03 01:56:11 - train: epoch 0028, iter [00210, 01251], lr: 0.000548, loss: 0.4212
2022-10-03 01:56:30 - train: epoch 0028, iter [00220, 01251], lr: 0.000548, loss: 0.4208
2022-10-03 01:56:49 - train: epoch 0028, iter [00230, 01251], lr: 0.000548, loss: 0.4307
2022-10-03 01:57:07 - train: epoch 0028, iter [00240, 01251], lr: 0.000548, loss: 0.4194
2022-10-03 01:57:26 - train: epoch 0028, iter [00250, 01251], lr: 0.000548, loss: 0.4266
2022-10-03 01:57:44 - train: epoch 0028, iter [00260, 01251], lr: 0.000547, loss: 0.4365
2022-10-03 01:58:03 - train: epoch 0028, iter [00270, 01251], lr: 0.000547, loss: 0.4132
2022-10-03 01:58:22 - train: epoch 0028, iter [00280, 01251], lr: 0.000547, loss: 0.4137
2022-10-03 01:58:41 - train: epoch 0028, iter [00290, 01251], lr: 0.000547, loss: 0.4334
2022-10-03 01:59:00 - train: epoch 0028, iter [00300, 01251], lr: 0.000547, loss: 0.4310
2022-10-03 01:59:19 - train: epoch 0028, iter [00310, 01251], lr: 0.000547, loss: 0.4155
2022-10-03 01:59:37 - train: epoch 0028, iter [00320, 01251], lr: 0.000547, loss: 0.4186
2022-10-03 01:59:56 - train: epoch 0028, iter [00330, 01251], lr: 0.000547, loss: 0.4212
2022-10-03 02:00:15 - train: epoch 0028, iter [00340, 01251], lr: 0.000547, loss: 0.4057
2022-10-03 02:00:34 - train: epoch 0028, iter [00350, 01251], lr: 0.000547, loss: 0.4099
2022-10-03 02:00:52 - train: epoch 0028, iter [00360, 01251], lr: 0.000547, loss: 0.4251
2022-10-03 02:01:11 - train: epoch 0028, iter [00370, 01251], lr: 0.000547, loss: 0.4175
2022-10-03 02:01:30 - train: epoch 0028, iter [00380, 01251], lr: 0.000547, loss: 0.4091
2022-10-03 02:01:49 - train: epoch 0028, iter [00390, 01251], lr: 0.000547, loss: 0.4180
2022-10-03 02:02:08 - train: epoch 0028, iter [00400, 01251], lr: 0.000547, loss: 0.4159
2022-10-03 02:02:27 - train: epoch 0028, iter [00410, 01251], lr: 0.000547, loss: 0.4144
2022-10-03 02:02:46 - train: epoch 0028, iter [00420, 01251], lr: 0.000547, loss: 0.4258
2022-10-03 02:03:06 - train: epoch 0028, iter [00430, 01251], lr: 0.000547, loss: 0.4163
2022-10-03 02:03:25 - train: epoch 0028, iter [00440, 01251], lr: 0.000547, loss: 0.4054
2022-10-03 02:03:45 - train: epoch 0028, iter [00450, 01251], lr: 0.000547, loss: 0.4381
2022-10-03 02:04:03 - train: epoch 0028, iter [00460, 01251], lr: 0.000547, loss: 0.4304
2022-10-03 02:04:22 - train: epoch 0028, iter [00470, 01251], lr: 0.000546, loss: 0.4094
2022-10-03 02:04:41 - train: epoch 0028, iter [00480, 01251], lr: 0.000546, loss: 0.4304
2022-10-03 02:05:00 - train: epoch 0028, iter [00490, 01251], lr: 0.000546, loss: 0.4299
2022-10-03 02:05:19 - train: epoch 0028, iter [00500, 01251], lr: 0.000546, loss: 0.4229
2022-10-03 02:05:38 - train: epoch 0028, iter [00510, 01251], lr: 0.000546, loss: 0.4235
2022-10-03 02:05:58 - train: epoch 0028, iter [00520, 01251], lr: 0.000546, loss: 0.4285
2022-10-03 02:06:18 - train: epoch 0028, iter [00530, 01251], lr: 0.000546, loss: 0.4186
2022-10-03 02:06:37 - train: epoch 0028, iter [00540, 01251], lr: 0.000546, loss: 0.4254
2022-10-03 02:06:56 - train: epoch 0028, iter [00550, 01251], lr: 0.000546, loss: 0.4217
2022-10-03 02:07:15 - train: epoch 0028, iter [00560, 01251], lr: 0.000546, loss: 0.4243
2022-10-03 02:07:34 - train: epoch 0028, iter [00570, 01251], lr: 0.000546, loss: 0.4250
2022-10-03 02:07:54 - train: epoch 0028, iter [00580, 01251], lr: 0.000546, loss: 0.4123
2022-10-03 02:08:14 - train: epoch 0028, iter [00590, 01251], lr: 0.000546, loss: 0.4293
2022-10-03 02:08:33 - train: epoch 0028, iter [00600, 01251], lr: 0.000546, loss: 0.4280
2022-10-03 02:08:53 - train: epoch 0028, iter [00610, 01251], lr: 0.000546, loss: 0.4282
2022-10-03 02:09:12 - train: epoch 0028, iter [00620, 01251], lr: 0.000546, loss: 0.4223
2022-10-03 02:09:31 - train: epoch 0028, iter [00630, 01251], lr: 0.000546, loss: 0.4284
2022-10-03 02:09:50 - train: epoch 0028, iter [00640, 01251], lr: 0.000546, loss: 0.4102
2022-10-03 02:10:09 - train: epoch 0028, iter [00650, 01251], lr: 0.000546, loss: 0.4191
2022-10-03 02:10:29 - train: epoch 0028, iter [00660, 01251], lr: 0.000546, loss: 0.4247
2022-10-03 02:10:49 - train: epoch 0028, iter [00670, 01251], lr: 0.000546, loss: 0.3970
2022-10-03 02:11:08 - train: epoch 0028, iter [00680, 01251], lr: 0.000545, loss: 0.4026
2022-10-03 02:11:27 - train: epoch 0028, iter [00690, 01251], lr: 0.000545, loss: 0.4152
2022-10-03 02:11:47 - train: epoch 0028, iter [00700, 01251], lr: 0.000545, loss: 0.4253
2022-10-03 02:12:06 - train: epoch 0028, iter [00710, 01251], lr: 0.000545, loss: 0.4285
2022-10-03 02:12:26 - train: epoch 0028, iter [00720, 01251], lr: 0.000545, loss: 0.4092
2022-10-03 02:12:45 - train: epoch 0028, iter [00730, 01251], lr: 0.000545, loss: 0.4014
2022-10-03 02:13:04 - train: epoch 0028, iter [00740, 01251], lr: 0.000545, loss: 0.4090
2022-10-03 02:13:24 - train: epoch 0028, iter [00750, 01251], lr: 0.000545, loss: 0.4184
2022-10-03 02:13:43 - train: epoch 0028, iter [00760, 01251], lr: 0.000545, loss: 0.4354
2022-10-03 02:14:02 - train: epoch 0028, iter [00770, 01251], lr: 0.000545, loss: 0.4194
2022-10-03 02:14:22 - train: epoch 0028, iter [00780, 01251], lr: 0.000545, loss: 0.4373
2022-10-03 02:14:42 - train: epoch 0028, iter [00790, 01251], lr: 0.000545, loss: 0.4233
2022-10-03 02:15:01 - train: epoch 0028, iter [00800, 01251], lr: 0.000545, loss: 0.4139
2022-10-03 02:15:20 - train: epoch 0028, iter [00810, 01251], lr: 0.000545, loss: 0.4243
2022-10-03 02:15:39 - train: epoch 0028, iter [00820, 01251], lr: 0.000545, loss: 0.4206
2022-10-03 02:15:58 - train: epoch 0028, iter [00830, 01251], lr: 0.000545, loss: 0.4102
2022-10-03 02:16:16 - train: epoch 0028, iter [00840, 01251], lr: 0.000545, loss: 0.4117
2022-10-03 02:16:35 - train: epoch 0028, iter [00850, 01251], lr: 0.000545, loss: 0.4062
2022-10-03 02:16:54 - train: epoch 0028, iter [00860, 01251], lr: 0.000545, loss: 0.4029
2022-10-03 02:17:13 - train: epoch 0028, iter [00870, 01251], lr: 0.000545, loss: 0.4316
2022-10-03 02:17:31 - train: epoch 0028, iter [00880, 01251], lr: 0.000545, loss: 0.4244
2022-10-03 02:17:50 - train: epoch 0028, iter [00890, 01251], lr: 0.000544, loss: 0.4092
2022-10-03 02:18:09 - train: epoch 0028, iter [00900, 01251], lr: 0.000544, loss: 0.4232
2022-10-03 02:18:28 - train: epoch 0028, iter [00910, 01251], lr: 0.000544, loss: 0.4322
2022-10-03 02:18:46 - train: epoch 0028, iter [00920, 01251], lr: 0.000544, loss: 0.4241
2022-10-03 02:19:05 - train: epoch 0028, iter [00930, 01251], lr: 0.000544, loss: 0.4088
2022-10-03 02:19:24 - train: epoch 0028, iter [00940, 01251], lr: 0.000544, loss: 0.4141
2022-10-03 02:19:43 - train: epoch 0028, iter [00950, 01251], lr: 0.000544, loss: 0.4102
2022-10-03 02:20:02 - train: epoch 0028, iter [00960, 01251], lr: 0.000544, loss: 0.4176
2022-10-03 02:20:21 - train: epoch 0028, iter [00970, 01251], lr: 0.000544, loss: 0.4143
2022-10-03 02:20:39 - train: epoch 0028, iter [00980, 01251], lr: 0.000544, loss: 0.4188
2022-10-03 02:20:59 - train: epoch 0028, iter [00990, 01251], lr: 0.000544, loss: 0.4277
2022-10-03 02:21:18 - train: epoch 0028, iter [01000, 01251], lr: 0.000544, loss: 0.4064
2022-10-03 02:21:37 - train: epoch 0028, iter [01010, 01251], lr: 0.000544, loss: 0.4046
2022-10-03 02:21:55 - train: epoch 0028, iter [01020, 01251], lr: 0.000544, loss: 0.4093
2022-10-03 02:22:15 - train: epoch 0028, iter [01030, 01251], lr: 0.000544, loss: 0.4262
2022-10-03 02:22:35 - train: epoch 0028, iter [01040, 01251], lr: 0.000544, loss: 0.3983
2022-10-03 02:22:54 - train: epoch 0028, iter [01050, 01251], lr: 0.000544, loss: 0.4166
2022-10-03 02:23:13 - train: epoch 0028, iter [01060, 01251], lr: 0.000544, loss: 0.4351
2022-10-03 02:23:33 - train: epoch 0028, iter [01070, 01251], lr: 0.000544, loss: 0.4199
2022-10-03 02:23:52 - train: epoch 0028, iter [01080, 01251], lr: 0.000544, loss: 0.4000
2022-10-03 02:24:12 - train: epoch 0028, iter [01090, 01251], lr: 0.000543, loss: 0.4035
2022-10-03 02:24:31 - train: epoch 0028, iter [01100, 01251], lr: 0.000543, loss: 0.4245
2022-10-03 02:24:51 - train: epoch 0028, iter [01110, 01251], lr: 0.000543, loss: 0.4269
2022-10-03 02:25:10 - train: epoch 0028, iter [01120, 01251], lr: 0.000543, loss: 0.4203
2022-10-03 02:25:30 - train: epoch 0028, iter [01130, 01251], lr: 0.000543, loss: 0.4157
2022-10-03 02:25:49 - train: epoch 0028, iter [01140, 01251], lr: 0.000543, loss: 0.4373
2022-10-03 02:26:08 - train: epoch 0028, iter [01150, 01251], lr: 0.000543, loss: 0.4139
2022-10-03 02:26:28 - train: epoch 0028, iter [01160, 01251], lr: 0.000543, loss: 0.4129
2022-10-03 02:26:47 - train: epoch 0028, iter [01170, 01251], lr: 0.000543, loss: 0.4077
2022-10-03 02:27:06 - train: epoch 0028, iter [01180, 01251], lr: 0.000543, loss: 0.4005
2022-10-03 02:27:25 - train: epoch 0028, iter [01190, 01251], lr: 0.000543, loss: 0.4204
2022-10-03 02:27:45 - train: epoch 0028, iter [01200, 01251], lr: 0.000543, loss: 0.4338
2022-10-03 02:28:03 - train: epoch 0028, iter [01210, 01251], lr: 0.000543, loss: 0.4100
2022-10-03 02:28:22 - train: epoch 0028, iter [01220, 01251], lr: 0.000543, loss: 0.4332
2022-10-03 02:28:41 - train: epoch 0028, iter [01230, 01251], lr: 0.000543, loss: 0.4199
2022-10-03 02:29:00 - train: epoch 0028, iter [01240, 01251], lr: 0.000543, loss: 0.4247
2022-10-03 02:29:18 - train: epoch 0028, iter [01250, 01251], lr: 0.000543, loss: 0.4312
2022-10-03 02:29:23 - train: epoch 028, train_loss: 0.4195
2022-10-03 02:29:28 - until epoch: 028, best_loss: 0.4195
2022-10-03 02:29:28 - epoch 029 lr: 0.000543
2022-10-03 02:29:55 - train: epoch 0029, iter [00010, 01251], lr: 0.000543, loss: 0.4175
2022-10-03 02:30:15 - train: epoch 0029, iter [00020, 01251], lr: 0.000543, loss: 0.4024
2022-10-03 02:30:35 - train: epoch 0029, iter [00030, 01251], lr: 0.000543, loss: 0.4335
2022-10-03 02:30:54 - train: epoch 0029, iter [00040, 01251], lr: 0.000543, loss: 0.4334
2022-10-03 02:31:13 - train: epoch 0029, iter [00050, 01251], lr: 0.000542, loss: 0.4071
2022-10-03 02:31:33 - train: epoch 0029, iter [00060, 01251], lr: 0.000542, loss: 0.4310
2022-10-03 02:31:53 - train: epoch 0029, iter [00070, 01251], lr: 0.000542, loss: 0.4019
2022-10-03 02:32:12 - train: epoch 0029, iter [00080, 01251], lr: 0.000542, loss: 0.4286
2022-10-03 02:32:31 - train: epoch 0029, iter [00090, 01251], lr: 0.000542, loss: 0.4306
2022-10-03 02:32:50 - train: epoch 0029, iter [00100, 01251], lr: 0.000542, loss: 0.4295
2022-10-03 02:33:10 - train: epoch 0029, iter [00110, 01251], lr: 0.000542, loss: 0.4341
2022-10-03 02:33:28 - train: epoch 0029, iter [00120, 01251], lr: 0.000542, loss: 0.4186
2022-10-03 02:33:48 - train: epoch 0029, iter [00130, 01251], lr: 0.000542, loss: 0.4262
2022-10-03 02:34:07 - train: epoch 0029, iter [00140, 01251], lr: 0.000542, loss: 0.4327
2022-10-03 02:34:27 - train: epoch 0029, iter [00150, 01251], lr: 0.000542, loss: 0.4122
2022-10-03 02:34:46 - train: epoch 0029, iter [00160, 01251], lr: 0.000542, loss: 0.4153
2022-10-03 02:35:05 - train: epoch 0029, iter [00170, 01251], lr: 0.000542, loss: 0.4190
2022-10-03 02:35:23 - train: epoch 0029, iter [00180, 01251], lr: 0.000542, loss: 0.4226
2022-10-03 02:35:43 - train: epoch 0029, iter [00190, 01251], lr: 0.000542, loss: 0.4182
2022-10-03 02:36:02 - train: epoch 0029, iter [00200, 01251], lr: 0.000542, loss: 0.4206
2022-10-03 02:36:21 - train: epoch 0029, iter [00210, 01251], lr: 0.000542, loss: 0.4306
2022-10-03 02:36:39 - train: epoch 0029, iter [00220, 01251], lr: 0.000542, loss: 0.4112
2022-10-03 02:36:59 - train: epoch 0029, iter [00230, 01251], lr: 0.000542, loss: 0.4393
2022-10-03 02:37:18 - train: epoch 0029, iter [00240, 01251], lr: 0.000542, loss: 0.4074
2022-10-03 02:37:37 - train: epoch 0029, iter [00250, 01251], lr: 0.000541, loss: 0.4290
2022-10-03 02:37:56 - train: epoch 0029, iter [00260, 01251], lr: 0.000541, loss: 0.4170
2022-10-03 02:38:15 - train: epoch 0029, iter [00270, 01251], lr: 0.000541, loss: 0.4128
2022-10-03 02:38:34 - train: epoch 0029, iter [00280, 01251], lr: 0.000541, loss: 0.4279
2022-10-03 02:38:53 - train: epoch 0029, iter [00290, 01251], lr: 0.000541, loss: 0.4271
2022-10-03 02:39:12 - train: epoch 0029, iter [00300, 01251], lr: 0.000541, loss: 0.4116
2022-10-03 02:39:31 - train: epoch 0029, iter [00310, 01251], lr: 0.000541, loss: 0.4036
2022-10-03 02:39:50 - train: epoch 0029, iter [00320, 01251], lr: 0.000541, loss: 0.4166
2022-10-03 02:40:09 - train: epoch 0029, iter [00330, 01251], lr: 0.000541, loss: 0.4027
2022-10-03 02:40:28 - train: epoch 0029, iter [00340, 01251], lr: 0.000541, loss: 0.4198
2022-10-03 02:40:47 - train: epoch 0029, iter [00350, 01251], lr: 0.000541, loss: 0.4056
2022-10-03 02:41:06 - train: epoch 0029, iter [00360, 01251], lr: 0.000541, loss: 0.4343
2022-10-03 02:41:25 - train: epoch 0029, iter [00370, 01251], lr: 0.000541, loss: 0.4183
2022-10-03 02:41:44 - train: epoch 0029, iter [00380, 01251], lr: 0.000541, loss: 0.4400
2022-10-03 02:42:03 - train: epoch 0029, iter [00390, 01251], lr: 0.000541, loss: 0.4300
2022-10-03 02:42:22 - train: epoch 0029, iter [00400, 01251], lr: 0.000541, loss: 0.4191
2022-10-03 02:42:41 - train: epoch 0029, iter [00410, 01251], lr: 0.000541, loss: 0.4322
2022-10-03 02:43:00 - train: epoch 0029, iter [00420, 01251], lr: 0.000541, loss: 0.4325
2022-10-03 02:43:19 - train: epoch 0029, iter [00430, 01251], lr: 0.000541, loss: 0.4243
2022-10-03 02:43:38 - train: epoch 0029, iter [00440, 01251], lr: 0.000541, loss: 0.4256
2022-10-03 02:43:56 - train: epoch 0029, iter [00450, 01251], lr: 0.000540, loss: 0.4163
2022-10-03 02:44:15 - train: epoch 0029, iter [00460, 01251], lr: 0.000540, loss: 0.4161
2022-10-03 02:44:34 - train: epoch 0029, iter [00470, 01251], lr: 0.000540, loss: 0.4290
2022-10-03 02:44:53 - train: epoch 0029, iter [00480, 01251], lr: 0.000540, loss: 0.4405
2022-10-03 02:45:12 - train: epoch 0029, iter [00490, 01251], lr: 0.000540, loss: 0.4209
2022-10-03 02:45:31 - train: epoch 0029, iter [00500, 01251], lr: 0.000540, loss: 0.4175
2022-10-03 02:45:50 - train: epoch 0029, iter [00510, 01251], lr: 0.000540, loss: 0.4263
2022-10-03 02:46:09 - train: epoch 0029, iter [00520, 01251], lr: 0.000540, loss: 0.4236
2022-10-03 02:46:28 - train: epoch 0029, iter [00530, 01251], lr: 0.000540, loss: 0.4374
2022-10-03 02:46:47 - train: epoch 0029, iter [00540, 01251], lr: 0.000540, loss: 0.4464
2022-10-03 02:47:06 - train: epoch 0029, iter [00550, 01251], lr: 0.000540, loss: 0.4224
2022-10-03 02:47:26 - train: epoch 0029, iter [00560, 01251], lr: 0.000540, loss: 0.4266
2022-10-03 02:47:45 - train: epoch 0029, iter [00570, 01251], lr: 0.000540, loss: 0.4154
2022-10-03 02:48:04 - train: epoch 0029, iter [00580, 01251], lr: 0.000540, loss: 0.4032
2022-10-03 02:48:23 - train: epoch 0029, iter [00590, 01251], lr: 0.000540, loss: 0.4347
2022-10-03 02:48:42 - train: epoch 0029, iter [00600, 01251], lr: 0.000540, loss: 0.4163
2022-10-03 02:49:01 - train: epoch 0029, iter [00610, 01251], lr: 0.000540, loss: 0.4100
2022-10-03 02:49:20 - train: epoch 0029, iter [00620, 01251], lr: 0.000540, loss: 0.4274
2022-10-03 02:49:39 - train: epoch 0029, iter [00630, 01251], lr: 0.000540, loss: 0.4205
2022-10-03 02:49:58 - train: epoch 0029, iter [00640, 01251], lr: 0.000540, loss: 0.4165
2022-10-03 02:50:17 - train: epoch 0029, iter [00650, 01251], lr: 0.000539, loss: 0.4071
2022-10-03 02:50:36 - train: epoch 0029, iter [00660, 01251], lr: 0.000539, loss: 0.3964
2022-10-03 02:50:54 - train: epoch 0029, iter [00670, 01251], lr: 0.000539, loss: 0.4097
2022-10-03 02:51:13 - train: epoch 0029, iter [00680, 01251], lr: 0.000539, loss: 0.4135
2022-10-03 02:51:32 - train: epoch 0029, iter [00690, 01251], lr: 0.000539, loss: 0.4176
2022-10-03 02:51:51 - train: epoch 0029, iter [00700, 01251], lr: 0.000539, loss: 0.4140
2022-10-03 02:52:11 - train: epoch 0029, iter [00710, 01251], lr: 0.000539, loss: 0.4329
2022-10-03 02:52:29 - train: epoch 0029, iter [00720, 01251], lr: 0.000539, loss: 0.4225
2022-10-03 02:52:48 - train: epoch 0029, iter [00730, 01251], lr: 0.000539, loss: 0.4100
2022-10-03 02:53:07 - train: epoch 0029, iter [00740, 01251], lr: 0.000539, loss: 0.4108
2022-10-03 02:53:26 - train: epoch 0029, iter [00750, 01251], lr: 0.000539, loss: 0.4118
2022-10-03 02:53:45 - train: epoch 0029, iter [00760, 01251], lr: 0.000539, loss: 0.4090
2022-10-03 02:54:04 - train: epoch 0029, iter [00770, 01251], lr: 0.000539, loss: 0.4205
2022-10-03 02:54:23 - train: epoch 0029, iter [00780, 01251], lr: 0.000539, loss: 0.4459
2022-10-03 02:54:42 - train: epoch 0029, iter [00790, 01251], lr: 0.000539, loss: 0.4242
2022-10-03 02:55:01 - train: epoch 0029, iter [00800, 01251], lr: 0.000539, loss: 0.4230
2022-10-03 02:55:19 - train: epoch 0029, iter [00810, 01251], lr: 0.000539, loss: 0.4398
2022-10-03 02:55:39 - train: epoch 0029, iter [00820, 01251], lr: 0.000539, loss: 0.4300
2022-10-03 02:55:58 - train: epoch 0029, iter [00830, 01251], lr: 0.000539, loss: 0.4281
2022-10-03 02:56:17 - train: epoch 0029, iter [00840, 01251], lr: 0.000539, loss: 0.3958
2022-10-03 02:56:36 - train: epoch 0029, iter [00850, 01251], lr: 0.000538, loss: 0.4313
2022-10-03 02:56:55 - train: epoch 0029, iter [00860, 01251], lr: 0.000538, loss: 0.4173
2022-10-03 02:57:14 - train: epoch 0029, iter [00870, 01251], lr: 0.000538, loss: 0.4364
2022-10-03 02:57:33 - train: epoch 0029, iter [00880, 01251], lr: 0.000538, loss: 0.4125
2022-10-03 02:57:52 - train: epoch 0029, iter [00890, 01251], lr: 0.000538, loss: 0.4248
2022-10-03 02:58:10 - train: epoch 0029, iter [00900, 01251], lr: 0.000538, loss: 0.4192
2022-10-03 02:58:30 - train: epoch 0029, iter [00910, 01251], lr: 0.000538, loss: 0.4190
2022-10-03 02:58:48 - train: epoch 0029, iter [00920, 01251], lr: 0.000538, loss: 0.4233
2022-10-03 02:59:08 - train: epoch 0029, iter [00930, 01251], lr: 0.000538, loss: 0.4332
2022-10-03 02:59:26 - train: epoch 0029, iter [00940, 01251], lr: 0.000538, loss: 0.4138
2022-10-03 02:59:45 - train: epoch 0029, iter [00950, 01251], lr: 0.000538, loss: 0.4274
2022-10-03 03:00:04 - train: epoch 0029, iter [00960, 01251], lr: 0.000538, loss: 0.4157
2022-10-03 03:00:23 - train: epoch 0029, iter [00970, 01251], lr: 0.000538, loss: 0.4315
2022-10-03 03:00:42 - train: epoch 0029, iter [00980, 01251], lr: 0.000538, loss: 0.4024
2022-10-03 03:01:01 - train: epoch 0029, iter [00990, 01251], lr: 0.000538, loss: 0.4161
2022-10-03 03:01:20 - train: epoch 0029, iter [01000, 01251], lr: 0.000538, loss: 0.4132
2022-10-03 03:01:39 - train: epoch 0029, iter [01010, 01251], lr: 0.000538, loss: 0.4024
2022-10-03 03:01:58 - train: epoch 0029, iter [01020, 01251], lr: 0.000538, loss: 0.4366
2022-10-03 03:02:17 - train: epoch 0029, iter [01030, 01251], lr: 0.000538, loss: 0.4215
2022-10-03 03:02:36 - train: epoch 0029, iter [01040, 01251], lr: 0.000537, loss: 0.4301
2022-10-03 03:02:55 - train: epoch 0029, iter [01050, 01251], lr: 0.000537, loss: 0.4296
2022-10-03 03:03:14 - train: epoch 0029, iter [01060, 01251], lr: 0.000537, loss: 0.4118
2022-10-03 03:03:33 - train: epoch 0029, iter [01070, 01251], lr: 0.000537, loss: 0.4258
2022-10-03 03:03:52 - train: epoch 0029, iter [01080, 01251], lr: 0.000537, loss: 0.4214
2022-10-03 03:04:11 - train: epoch 0029, iter [01090, 01251], lr: 0.000537, loss: 0.4183
2022-10-03 03:04:29 - train: epoch 0029, iter [01100, 01251], lr: 0.000537, loss: 0.4244
2022-10-03 03:04:48 - train: epoch 0029, iter [01110, 01251], lr: 0.000537, loss: 0.4236
2022-10-03 03:05:07 - train: epoch 0029, iter [01120, 01251], lr: 0.000537, loss: 0.4260
2022-10-03 03:05:26 - train: epoch 0029, iter [01130, 01251], lr: 0.000537, loss: 0.4203
2022-10-03 03:05:45 - train: epoch 0029, iter [01140, 01251], lr: 0.000537, loss: 0.4306
2022-10-03 03:06:04 - train: epoch 0029, iter [01150, 01251], lr: 0.000537, loss: 0.4437
2022-10-03 03:06:23 - train: epoch 0029, iter [01160, 01251], lr: 0.000537, loss: 0.4247
2022-10-03 03:06:42 - train: epoch 0029, iter [01170, 01251], lr: 0.000537, loss: 0.4125
2022-10-03 03:07:00 - train: epoch 0029, iter [01180, 01251], lr: 0.000537, loss: 0.4146
2022-10-03 03:07:19 - train: epoch 0029, iter [01190, 01251], lr: 0.000537, loss: 0.4269
2022-10-03 03:07:38 - train: epoch 0029, iter [01200, 01251], lr: 0.000537, loss: 0.4168
2022-10-03 03:07:57 - train: epoch 0029, iter [01210, 01251], lr: 0.000537, loss: 0.4130
2022-10-03 03:08:16 - train: epoch 0029, iter [01220, 01251], lr: 0.000537, loss: 0.4381
2022-10-03 03:08:35 - train: epoch 0029, iter [01230, 01251], lr: 0.000537, loss: 0.4327
2022-10-03 03:08:54 - train: epoch 0029, iter [01240, 01251], lr: 0.000536, loss: 0.4234
2022-10-03 03:09:13 - train: epoch 0029, iter [01250, 01251], lr: 0.000536, loss: 0.4306
2022-10-03 03:09:17 - train: epoch 029, train_loss: 0.4190
2022-10-03 03:09:21 - until epoch: 029, best_loss: 0.4190
2022-10-03 03:09:21 - epoch 030 lr: 0.000536
2022-10-03 03:09:48 - train: epoch 0030, iter [00010, 01251], lr: 0.000536, loss: 0.4015
2022-10-03 03:10:08 - train: epoch 0030, iter [00020, 01251], lr: 0.000536, loss: 0.4050
2022-10-03 03:10:27 - train: epoch 0030, iter [00030, 01251], lr: 0.000536, loss: 0.4026
2022-10-03 03:10:47 - train: epoch 0030, iter [00040, 01251], lr: 0.000536, loss: 0.4330
2022-10-03 03:11:06 - train: epoch 0030, iter [00050, 01251], lr: 0.000536, loss: 0.4064
2022-10-03 03:11:26 - train: epoch 0030, iter [00060, 01251], lr: 0.000536, loss: 0.4209
2022-10-03 03:11:46 - train: epoch 0030, iter [00070, 01251], lr: 0.000536, loss: 0.4160
2022-10-03 03:12:05 - train: epoch 0030, iter [00080, 01251], lr: 0.000536, loss: 0.4192
2022-10-03 03:12:25 - train: epoch 0030, iter [00090, 01251], lr: 0.000536, loss: 0.4125
2022-10-03 03:12:45 - train: epoch 0030, iter [00100, 01251], lr: 0.000536, loss: 0.4210
2022-10-03 03:13:04 - train: epoch 0030, iter [00110, 01251], lr: 0.000536, loss: 0.4133
2022-10-03 03:13:24 - train: epoch 0030, iter [00120, 01251], lr: 0.000536, loss: 0.4210
2022-10-03 03:13:44 - train: epoch 0030, iter [00130, 01251], lr: 0.000536, loss: 0.4359
2022-10-03 03:14:04 - train: epoch 0030, iter [00140, 01251], lr: 0.000536, loss: 0.4231
2022-10-03 03:14:23 - train: epoch 0030, iter [00150, 01251], lr: 0.000536, loss: 0.4340
2022-10-03 03:14:43 - train: epoch 0030, iter [00160, 01251], lr: 0.000536, loss: 0.4315
2022-10-03 03:15:03 - train: epoch 0030, iter [00170, 01251], lr: 0.000536, loss: 0.4246
2022-10-03 03:15:23 - train: epoch 0030, iter [00180, 01251], lr: 0.000535, loss: 0.4073
2022-10-03 03:15:42 - train: epoch 0030, iter [00190, 01251], lr: 0.000535, loss: 0.4219
2022-10-03 03:16:02 - train: epoch 0030, iter [00200, 01251], lr: 0.000535, loss: 0.4169
2022-10-03 03:16:22 - train: epoch 0030, iter [00210, 01251], lr: 0.000535, loss: 0.4386
2022-10-03 03:16:42 - train: epoch 0030, iter [00220, 01251], lr: 0.000535, loss: 0.4080
2022-10-03 03:17:01 - train: epoch 0030, iter [00230, 01251], lr: 0.000535, loss: 0.4210
2022-10-03 03:17:21 - train: epoch 0030, iter [00240, 01251], lr: 0.000535, loss: 0.4322
2022-10-03 03:17:41 - train: epoch 0030, iter [00250, 01251], lr: 0.000535, loss: 0.4101
2022-10-03 03:18:01 - train: epoch 0030, iter [00260, 01251], lr: 0.000535, loss: 0.4147
2022-10-03 03:18:21 - train: epoch 0030, iter [00270, 01251], lr: 0.000535, loss: 0.4096
2022-10-03 03:18:40 - train: epoch 0030, iter [00280, 01251], lr: 0.000535, loss: 0.4083
2022-10-03 03:19:00 - train: epoch 0030, iter [00290, 01251], lr: 0.000535, loss: 0.4111
2022-10-03 03:19:20 - train: epoch 0030, iter [00300, 01251], lr: 0.000535, loss: 0.3954
2022-10-03 03:19:40 - train: epoch 0030, iter [00310, 01251], lr: 0.000535, loss: 0.4204
2022-10-03 03:20:00 - train: epoch 0030, iter [00320, 01251], lr: 0.000535, loss: 0.4156
2022-10-03 03:20:20 - train: epoch 0030, iter [00330, 01251], lr: 0.000535, loss: 0.4256
2022-10-03 03:20:39 - train: epoch 0030, iter [00340, 01251], lr: 0.000535, loss: 0.4062
2022-10-03 03:20:59 - train: epoch 0030, iter [00350, 01251], lr: 0.000535, loss: 0.4215
2022-10-03 03:21:19 - train: epoch 0030, iter [00360, 01251], lr: 0.000535, loss: 0.4114
2022-10-03 03:21:39 - train: epoch 0030, iter [00370, 01251], lr: 0.000534, loss: 0.4120
2022-10-03 03:21:58 - train: epoch 0030, iter [00380, 01251], lr: 0.000534, loss: 0.4207
2022-10-03 03:22:18 - train: epoch 0030, iter [00390, 01251], lr: 0.000534, loss: 0.4215
2022-10-03 03:22:38 - train: epoch 0030, iter [00400, 01251], lr: 0.000534, loss: 0.4230
2022-10-03 03:22:58 - train: epoch 0030, iter [00410, 01251], lr: 0.000534, loss: 0.4243
2022-10-03 03:23:18 - train: epoch 0030, iter [00420, 01251], lr: 0.000534, loss: 0.4168
2022-10-03 03:23:38 - train: epoch 0030, iter [00430, 01251], lr: 0.000534, loss: 0.4141
2022-10-03 03:23:57 - train: epoch 0030, iter [00440, 01251], lr: 0.000534, loss: 0.4036
2022-10-03 03:24:17 - train: epoch 0030, iter [00450, 01251], lr: 0.000534, loss: 0.3790
2022-10-03 03:24:37 - train: epoch 0030, iter [00460, 01251], lr: 0.000534, loss: 0.4184
2022-10-03 03:24:56 - train: epoch 0030, iter [00470, 01251], lr: 0.000534, loss: 0.4162
2022-10-03 03:25:16 - train: epoch 0030, iter [00480, 01251], lr: 0.000534, loss: 0.4063
2022-10-03 03:25:36 - train: epoch 0030, iter [00490, 01251], lr: 0.000534, loss: 0.4192
2022-10-03 03:25:55 - train: epoch 0030, iter [00500, 01251], lr: 0.000534, loss: 0.4127
2022-10-03 03:26:15 - train: epoch 0030, iter [00510, 01251], lr: 0.000534, loss: 0.4402
2022-10-03 03:26:35 - train: epoch 0030, iter [00520, 01251], lr: 0.000534, loss: 0.4139
2022-10-03 03:26:55 - train: epoch 0030, iter [00530, 01251], lr: 0.000534, loss: 0.4135
2022-10-03 03:27:15 - train: epoch 0030, iter [00540, 01251], lr: 0.000534, loss: 0.4183
2022-10-03 03:27:34 - train: epoch 0030, iter [00550, 01251], lr: 0.000534, loss: 0.4212
2022-10-03 03:27:54 - train: epoch 0030, iter [00560, 01251], lr: 0.000533, loss: 0.3957
2022-10-03 03:28:14 - train: epoch 0030, iter [00570, 01251], lr: 0.000533, loss: 0.4024
2022-10-03 03:28:33 - train: epoch 0030, iter [00580, 01251], lr: 0.000533, loss: 0.4245
2022-10-03 03:28:53 - train: epoch 0030, iter [00590, 01251], lr: 0.000533, loss: 0.4214
2022-10-03 03:29:13 - train: epoch 0030, iter [00600, 01251], lr: 0.000533, loss: 0.3868
2022-10-03 03:29:32 - train: epoch 0030, iter [00610, 01251], lr: 0.000533, loss: 0.4364
2022-10-03 03:29:52 - train: epoch 0030, iter [00620, 01251], lr: 0.000533, loss: 0.4354
2022-10-03 03:30:12 - train: epoch 0030, iter [00630, 01251], lr: 0.000533, loss: 0.4394
2022-10-03 03:30:32 - train: epoch 0030, iter [00640, 01251], lr: 0.000533, loss: 0.4280
2022-10-03 03:30:51 - train: epoch 0030, iter [00650, 01251], lr: 0.000533, loss: 0.4150
2022-10-03 03:31:11 - train: epoch 0030, iter [00660, 01251], lr: 0.000533, loss: 0.4075
2022-10-03 03:31:31 - train: epoch 0030, iter [00670, 01251], lr: 0.000533, loss: 0.4163
2022-10-03 03:31:51 - train: epoch 0030, iter [00680, 01251], lr: 0.000533, loss: 0.4276
2022-10-03 03:32:10 - train: epoch 0030, iter [00690, 01251], lr: 0.000533, loss: 0.4166
2022-10-03 03:32:30 - train: epoch 0030, iter [00700, 01251], lr: 0.000533, loss: 0.3943
2022-10-03 03:32:50 - train: epoch 0030, iter [00710, 01251], lr: 0.000533, loss: 0.4221
2022-10-03 03:33:09 - train: epoch 0030, iter [00720, 01251], lr: 0.000533, loss: 0.4137
2022-10-03 03:33:29 - train: epoch 0030, iter [00730, 01251], lr: 0.000533, loss: 0.4220
2022-10-03 03:33:49 - train: epoch 0030, iter [00740, 01251], lr: 0.000533, loss: 0.4059
2022-10-03 03:34:09 - train: epoch 0030, iter [00750, 01251], lr: 0.000532, loss: 0.4134
2022-10-03 03:34:28 - train: epoch 0030, iter [00760, 01251], lr: 0.000532, loss: 0.4209
2022-10-03 03:34:48 - train: epoch 0030, iter [00770, 01251], lr: 0.000532, loss: 0.4195
2022-10-03 03:35:08 - train: epoch 0030, iter [00780, 01251], lr: 0.000532, loss: 0.4095
2022-10-03 03:35:27 - train: epoch 0030, iter [00790, 01251], lr: 0.000532, loss: 0.4196
2022-10-03 03:35:47 - train: epoch 0030, iter [00800, 01251], lr: 0.000532, loss: 0.4086
2022-10-03 03:36:07 - train: epoch 0030, iter [00810, 01251], lr: 0.000532, loss: 0.4210
2022-10-03 03:36:27 - train: epoch 0030, iter [00820, 01251], lr: 0.000532, loss: 0.4213
2022-10-03 03:36:46 - train: epoch 0030, iter [00830, 01251], lr: 0.000532, loss: 0.4362
2022-10-03 03:37:06 - train: epoch 0030, iter [00840, 01251], lr: 0.000532, loss: 0.4189
2022-10-03 03:37:26 - train: epoch 0030, iter [00850, 01251], lr: 0.000532, loss: 0.4170
2022-10-03 03:37:45 - train: epoch 0030, iter [00860, 01251], lr: 0.000532, loss: 0.4417
2022-10-03 03:38:05 - train: epoch 0030, iter [00870, 01251], lr: 0.000532, loss: 0.4144
2022-10-03 03:38:25 - train: epoch 0030, iter [00880, 01251], lr: 0.000532, loss: 0.4348
2022-10-03 03:38:45 - train: epoch 0030, iter [00890, 01251], lr: 0.000532, loss: 0.4251
2022-10-03 03:39:05 - train: epoch 0030, iter [00900, 01251], lr: 0.000532, loss: 0.4194
2022-10-03 03:39:24 - train: epoch 0030, iter [00910, 01251], lr: 0.000532, loss: 0.4173
2022-10-03 03:39:44 - train: epoch 0030, iter [00920, 01251], lr: 0.000532, loss: 0.4230
2022-10-03 03:40:04 - train: epoch 0030, iter [00930, 01251], lr: 0.000532, loss: 0.3872
2022-10-03 03:40:23 - train: epoch 0030, iter [00940, 01251], lr: 0.000531, loss: 0.4192
2022-10-03 03:40:43 - train: epoch 0030, iter [00950, 01251], lr: 0.000531, loss: 0.4020
2022-10-03 03:41:03 - train: epoch 0030, iter [00960, 01251], lr: 0.000531, loss: 0.4291
2022-10-03 03:41:23 - train: epoch 0030, iter [00970, 01251], lr: 0.000531, loss: 0.4277
2022-10-03 03:41:42 - train: epoch 0030, iter [00980, 01251], lr: 0.000531, loss: 0.4396
2022-10-03 03:42:02 - train: epoch 0030, iter [00990, 01251], lr: 0.000531, loss: 0.4323
2022-10-03 03:42:22 - train: epoch 0030, iter [01000, 01251], lr: 0.000531, loss: 0.4264
2022-10-03 03:42:42 - train: epoch 0030, iter [01010, 01251], lr: 0.000531, loss: 0.4261
2022-10-03 03:43:01 - train: epoch 0030, iter [01020, 01251], lr: 0.000531, loss: 0.4410
2022-10-03 03:43:21 - train: epoch 0030, iter [01030, 01251], lr: 0.000531, loss: 0.4235
2022-10-03 03:43:41 - train: epoch 0030, iter [01040, 01251], lr: 0.000531, loss: 0.4168
2022-10-03 03:44:01 - train: epoch 0030, iter [01050, 01251], lr: 0.000531, loss: 0.4246
2022-10-03 03:44:21 - train: epoch 0030, iter [01060, 01251], lr: 0.000531, loss: 0.4069
2022-10-03 03:44:40 - train: epoch 0030, iter [01070, 01251], lr: 0.000531, loss: 0.4062
2022-10-03 03:45:00 - train: epoch 0030, iter [01080, 01251], lr: 0.000531, loss: 0.4387
2022-10-03 03:45:20 - train: epoch 0030, iter [01090, 01251], lr: 0.000531, loss: 0.4430
2022-10-03 03:45:40 - train: epoch 0030, iter [01100, 01251], lr: 0.000531, loss: 0.4208
2022-10-03 03:45:59 - train: epoch 0030, iter [01110, 01251], lr: 0.000531, loss: 0.4031
2022-10-03 03:46:19 - train: epoch 0030, iter [01120, 01251], lr: 0.000531, loss: 0.4157
2022-10-03 03:46:39 - train: epoch 0030, iter [01130, 01251], lr: 0.000530, loss: 0.4115
2022-10-03 03:46:58 - train: epoch 0030, iter [01140, 01251], lr: 0.000530, loss: 0.4237
2022-10-03 03:47:18 - train: epoch 0030, iter [01150, 01251], lr: 0.000530, loss: 0.4229
2022-10-03 03:47:38 - train: epoch 0030, iter [01160, 01251], lr: 0.000530, loss: 0.3993
2022-10-03 03:47:57 - train: epoch 0030, iter [01170, 01251], lr: 0.000530, loss: 0.4263
2022-10-03 03:48:17 - train: epoch 0030, iter [01180, 01251], lr: 0.000530, loss: 0.4008
2022-10-03 03:48:37 - train: epoch 0030, iter [01190, 01251], lr: 0.000530, loss: 0.4302
2022-10-03 03:48:57 - train: epoch 0030, iter [01200, 01251], lr: 0.000530, loss: 0.4199
2022-10-03 03:49:16 - train: epoch 0030, iter [01210, 01251], lr: 0.000530, loss: 0.4240
2022-10-03 03:49:36 - train: epoch 0030, iter [01220, 01251], lr: 0.000530, loss: 0.4059
2022-10-03 03:49:56 - train: epoch 0030, iter [01230, 01251], lr: 0.000530, loss: 0.4383
2022-10-03 03:50:15 - train: epoch 0030, iter [01240, 01251], lr: 0.000530, loss: 0.4330
2022-10-03 03:50:34 - train: epoch 0030, iter [01250, 01251], lr: 0.000530, loss: 0.4167
2022-10-03 03:50:38 - train: epoch 030, train_loss: 0.4186
2022-10-03 03:50:42 - until epoch: 030, best_loss: 0.4186
2022-10-03 03:50:42 - epoch 031 lr: 0.000530
2022-10-03 03:51:09 - train: epoch 0031, iter [00010, 01251], lr: 0.000530, loss: 0.4057
2022-10-03 03:51:28 - train: epoch 0031, iter [00020, 01251], lr: 0.000530, loss: 0.4226
2022-10-03 03:51:48 - train: epoch 0031, iter [00030, 01251], lr: 0.000530, loss: 0.4085
2022-10-03 03:52:07 - train: epoch 0031, iter [00040, 01251], lr: 0.000530, loss: 0.4187
2022-10-03 03:52:27 - train: epoch 0031, iter [00050, 01251], lr: 0.000530, loss: 0.4062
2022-10-03 03:52:47 - train: epoch 0031, iter [00060, 01251], lr: 0.000529, loss: 0.3994
2022-10-03 03:53:07 - train: epoch 0031, iter [00070, 01251], lr: 0.000529, loss: 0.4181
2022-10-03 03:53:26 - train: epoch 0031, iter [00080, 01251], lr: 0.000529, loss: 0.4189
2022-10-03 03:53:46 - train: epoch 0031, iter [00090, 01251], lr: 0.000529, loss: 0.4216
2022-10-03 03:54:06 - train: epoch 0031, iter [00100, 01251], lr: 0.000529, loss: 0.4165
2022-10-03 03:54:26 - train: epoch 0031, iter [00110, 01251], lr: 0.000529, loss: 0.4286
2022-10-03 03:54:45 - train: epoch 0031, iter [00120, 01251], lr: 0.000529, loss: 0.4036
2022-10-03 03:55:05 - train: epoch 0031, iter [00130, 01251], lr: 0.000529, loss: 0.4266
2022-10-03 03:55:25 - train: epoch 0031, iter [00140, 01251], lr: 0.000529, loss: 0.4259
2022-10-03 03:55:44 - train: epoch 0031, iter [00150, 01251], lr: 0.000529, loss: 0.3988
2022-10-03 03:56:04 - train: epoch 0031, iter [00160, 01251], lr: 0.000529, loss: 0.4244
2022-10-03 03:56:24 - train: epoch 0031, iter [00170, 01251], lr: 0.000529, loss: 0.4033
2022-10-03 03:56:44 - train: epoch 0031, iter [00180, 01251], lr: 0.000529, loss: 0.4158
2022-10-03 03:57:03 - train: epoch 0031, iter [00190, 01251], lr: 0.000529, loss: 0.4382
2022-10-03 03:57:23 - train: epoch 0031, iter [00200, 01251], lr: 0.000529, loss: 0.4211
2022-10-03 03:57:43 - train: epoch 0031, iter [00210, 01251], lr: 0.000529, loss: 0.3999
2022-10-03 03:58:02 - train: epoch 0031, iter [00220, 01251], lr: 0.000529, loss: 0.4212
2022-10-03 03:58:22 - train: epoch 0031, iter [00230, 01251], lr: 0.000529, loss: 0.4326
2022-10-03 03:58:42 - train: epoch 0031, iter [00240, 01251], lr: 0.000529, loss: 0.4362
2022-10-03 03:59:02 - train: epoch 0031, iter [00250, 01251], lr: 0.000528, loss: 0.4135
2022-10-03 03:59:21 - train: epoch 0031, iter [00260, 01251], lr: 0.000528, loss: 0.4187
2022-10-03 03:59:41 - train: epoch 0031, iter [00270, 01251], lr: 0.000528, loss: 0.4159
2022-10-03 04:00:01 - train: epoch 0031, iter [00280, 01251], lr: 0.000528, loss: 0.4118
2022-10-03 04:00:20 - train: epoch 0031, iter [00290, 01251], lr: 0.000528, loss: 0.4105
2022-10-03 04:00:40 - train: epoch 0031, iter [00300, 01251], lr: 0.000528, loss: 0.4117
2022-10-03 04:01:00 - train: epoch 0031, iter [00310, 01251], lr: 0.000528, loss: 0.4354
2022-10-03 04:01:20 - train: epoch 0031, iter [00320, 01251], lr: 0.000528, loss: 0.4319
2022-10-03 04:01:39 - train: epoch 0031, iter [00330, 01251], lr: 0.000528, loss: 0.4330
2022-10-03 04:01:59 - train: epoch 0031, iter [00340, 01251], lr: 0.000528, loss: 0.4181
2022-10-03 04:02:19 - train: epoch 0031, iter [00350, 01251], lr: 0.000528, loss: 0.4128
2022-10-03 04:02:38 - train: epoch 0031, iter [00360, 01251], lr: 0.000528, loss: 0.4231
2022-10-03 04:02:58 - train: epoch 0031, iter [00370, 01251], lr: 0.000528, loss: 0.4027
2022-10-03 04:03:18 - train: epoch 0031, iter [00380, 01251], lr: 0.000528, loss: 0.4302
2022-10-03 04:03:37 - train: epoch 0031, iter [00390, 01251], lr: 0.000528, loss: 0.4120
2022-10-03 04:03:57 - train: epoch 0031, iter [00400, 01251], lr: 0.000528, loss: 0.4349
2022-10-03 04:04:17 - train: epoch 0031, iter [00410, 01251], lr: 0.000528, loss: 0.4150
2022-10-03 04:04:36 - train: epoch 0031, iter [00420, 01251], lr: 0.000528, loss: 0.4287
2022-10-03 04:04:56 - train: epoch 0031, iter [00430, 01251], lr: 0.000527, loss: 0.4232
2022-10-03 04:05:15 - train: epoch 0031, iter [00440, 01251], lr: 0.000527, loss: 0.4232
2022-10-03 04:05:35 - train: epoch 0031, iter [00450, 01251], lr: 0.000527, loss: 0.4126
2022-10-03 04:05:55 - train: epoch 0031, iter [00460, 01251], lr: 0.000527, loss: 0.4145
2022-10-03 04:06:15 - train: epoch 0031, iter [00470, 01251], lr: 0.000527, loss: 0.4189
2022-10-03 04:06:34 - train: epoch 0031, iter [00480, 01251], lr: 0.000527, loss: 0.4410
2022-10-03 04:06:54 - train: epoch 0031, iter [00490, 01251], lr: 0.000527, loss: 0.4073
2022-10-03 04:07:14 - train: epoch 0031, iter [00500, 01251], lr: 0.000527, loss: 0.3972
2022-10-03 04:07:34 - train: epoch 0031, iter [00510, 01251], lr: 0.000527, loss: 0.4190
2022-10-03 04:07:53 - train: epoch 0031, iter [00520, 01251], lr: 0.000527, loss: 0.4300
2022-10-03 04:08:13 - train: epoch 0031, iter [00530, 01251], lr: 0.000527, loss: 0.4062
2022-10-03 04:08:33 - train: epoch 0031, iter [00540, 01251], lr: 0.000527, loss: 0.4164
2022-10-03 04:08:52 - train: epoch 0031, iter [00550, 01251], lr: 0.000527, loss: 0.3962
2022-10-03 04:09:12 - train: epoch 0031, iter [00560, 01251], lr: 0.000527, loss: 0.4175
2022-10-03 04:09:32 - train: epoch 0031, iter [00570, 01251], lr: 0.000527, loss: 0.4215
2022-10-03 04:09:51 - train: epoch 0031, iter [00580, 01251], lr: 0.000527, loss: 0.4245
2022-10-03 04:10:11 - train: epoch 0031, iter [00590, 01251], lr: 0.000527, loss: 0.4181
2022-10-03 04:10:31 - train: epoch 0031, iter [00600, 01251], lr: 0.000527, loss: 0.4283
2022-10-03 04:10:51 - train: epoch 0031, iter [00610, 01251], lr: 0.000526, loss: 0.3919
2022-10-03 04:11:10 - train: epoch 0031, iter [00620, 01251], lr: 0.000526, loss: 0.4336
2022-10-03 04:11:30 - train: epoch 0031, iter [00630, 01251], lr: 0.000526, loss: 0.4127
2022-10-03 04:11:49 - train: epoch 0031, iter [00640, 01251], lr: 0.000526, loss: 0.4268
2022-10-03 04:12:09 - train: epoch 0031, iter [00650, 01251], lr: 0.000526, loss: 0.4119
2022-10-03 04:12:29 - train: epoch 0031, iter [00660, 01251], lr: 0.000526, loss: 0.4237
2022-10-03 04:12:48 - train: epoch 0031, iter [00670, 01251], lr: 0.000526, loss: 0.4134
2022-10-03 04:13:08 - train: epoch 0031, iter [00680, 01251], lr: 0.000526, loss: 0.4184
2022-10-03 04:13:28 - train: epoch 0031, iter [00690, 01251], lr: 0.000526, loss: 0.4245
2022-10-03 04:13:48 - train: epoch 0031, iter [00700, 01251], lr: 0.000526, loss: 0.4044
2022-10-03 04:14:07 - train: epoch 0031, iter [00710, 01251], lr: 0.000526, loss: 0.4020
2022-10-03 04:14:27 - train: epoch 0031, iter [00720, 01251], lr: 0.000526, loss: 0.4281
2022-10-03 04:14:47 - train: epoch 0031, iter [00730, 01251], lr: 0.000526, loss: 0.4263
2022-10-03 04:15:06 - train: epoch 0031, iter [00740, 01251], lr: 0.000526, loss: 0.3972
2022-10-03 04:15:26 - train: epoch 0031, iter [00750, 01251], lr: 0.000526, loss: 0.4217
2022-10-03 04:15:46 - train: epoch 0031, iter [00760, 01251], lr: 0.000526, loss: 0.4009
2022-10-03 04:16:05 - train: epoch 0031, iter [00770, 01251], lr: 0.000526, loss: 0.4098
2022-10-03 04:16:25 - train: epoch 0031, iter [00780, 01251], lr: 0.000526, loss: 0.4286
2022-10-03 04:16:45 - train: epoch 0031, iter [00790, 01251], lr: 0.000526, loss: 0.4027
2022-10-03 04:17:04 - train: epoch 0031, iter [00800, 01251], lr: 0.000525, loss: 0.4215
2022-10-03 04:17:24 - train: epoch 0031, iter [00810, 01251], lr: 0.000525, loss: 0.4307
2022-10-03 04:17:44 - train: epoch 0031, iter [00820, 01251], lr: 0.000525, loss: 0.4227
2022-10-03 04:18:04 - train: epoch 0031, iter [00830, 01251], lr: 0.000525, loss: 0.4279
2022-10-03 04:18:23 - train: epoch 0031, iter [00840, 01251], lr: 0.000525, loss: 0.4125
2022-10-03 04:18:43 - train: epoch 0031, iter [00850, 01251], lr: 0.000525, loss: 0.4226
2022-10-03 04:19:03 - train: epoch 0031, iter [00860, 01251], lr: 0.000525, loss: 0.3969
2022-10-03 04:19:22 - train: epoch 0031, iter [00870, 01251], lr: 0.000525, loss: 0.4145
2022-10-03 04:19:42 - train: epoch 0031, iter [00880, 01251], lr: 0.000525, loss: 0.4356
2022-10-03 04:20:02 - train: epoch 0031, iter [00890, 01251], lr: 0.000525, loss: 0.4370
2022-10-03 04:20:21 - train: epoch 0031, iter [00900, 01251], lr: 0.000525, loss: 0.4350
2022-10-03 04:20:41 - train: epoch 0031, iter [00910, 01251], lr: 0.000525, loss: 0.4312
2022-10-03 04:21:01 - train: epoch 0031, iter [00920, 01251], lr: 0.000525, loss: 0.4281
2022-10-03 04:21:21 - train: epoch 0031, iter [00930, 01251], lr: 0.000525, loss: 0.4097
2022-10-03 04:21:40 - train: epoch 0031, iter [00940, 01251], lr: 0.000525, loss: 0.4159
2022-10-03 04:22:00 - train: epoch 0031, iter [00950, 01251], lr: 0.000525, loss: 0.4180
2022-10-03 04:22:20 - train: epoch 0031, iter [00960, 01251], lr: 0.000525, loss: 0.4141
2022-10-03 04:22:39 - train: epoch 0031, iter [00970, 01251], lr: 0.000525, loss: 0.4167
2022-10-03 04:22:59 - train: epoch 0031, iter [00980, 01251], lr: 0.000524, loss: 0.4224
2022-10-03 04:23:19 - train: epoch 0031, iter [00990, 01251], lr: 0.000524, loss: 0.4235
2022-10-03 04:23:38 - train: epoch 0031, iter [01000, 01251], lr: 0.000524, loss: 0.4109
2022-10-03 04:23:58 - train: epoch 0031, iter [01010, 01251], lr: 0.000524, loss: 0.4286
2022-10-03 04:24:18 - train: epoch 0031, iter [01020, 01251], lr: 0.000524, loss: 0.4326
2022-10-03 04:24:37 - train: epoch 0031, iter [01030, 01251], lr: 0.000524, loss: 0.4359
2022-10-03 04:24:57 - train: epoch 0031, iter [01040, 01251], lr: 0.000524, loss: 0.4145
2022-10-03 04:25:17 - train: epoch 0031, iter [01050, 01251], lr: 0.000524, loss: 0.4135
2022-10-03 04:25:36 - train: epoch 0031, iter [01060, 01251], lr: 0.000524, loss: 0.4064
2022-10-03 04:25:56 - train: epoch 0031, iter [01070, 01251], lr: 0.000524, loss: 0.3969
2022-10-03 04:26:16 - train: epoch 0031, iter [01080, 01251], lr: 0.000524, loss: 0.4207
2022-10-03 04:26:36 - train: epoch 0031, iter [01090, 01251], lr: 0.000524, loss: 0.4312
2022-10-03 04:26:56 - train: epoch 0031, iter [01100, 01251], lr: 0.000524, loss: 0.4216
2022-10-03 04:27:15 - train: epoch 0031, iter [01110, 01251], lr: 0.000524, loss: 0.4172
2022-10-03 04:27:35 - train: epoch 0031, iter [01120, 01251], lr: 0.000524, loss: 0.4203
2022-10-03 04:27:55 - train: epoch 0031, iter [01130, 01251], lr: 0.000524, loss: 0.4014
2022-10-03 04:28:14 - train: epoch 0031, iter [01140, 01251], lr: 0.000524, loss: 0.3919
2022-10-03 04:28:34 - train: epoch 0031, iter [01150, 01251], lr: 0.000524, loss: 0.4254
2022-10-03 04:28:54 - train: epoch 0031, iter [01160, 01251], lr: 0.000523, loss: 0.3976
2022-10-03 04:29:13 - train: epoch 0031, iter [01170, 01251], lr: 0.000523, loss: 0.4110
2022-10-03 04:29:33 - train: epoch 0031, iter [01180, 01251], lr: 0.000523, loss: 0.4147
2022-10-03 04:29:53 - train: epoch 0031, iter [01190, 01251], lr: 0.000523, loss: 0.4153
2022-10-03 04:30:13 - train: epoch 0031, iter [01200, 01251], lr: 0.000523, loss: 0.4290
2022-10-03 04:30:32 - train: epoch 0031, iter [01210, 01251], lr: 0.000523, loss: 0.4041
2022-10-03 04:30:52 - train: epoch 0031, iter [01220, 01251], lr: 0.000523, loss: 0.4229
2022-10-03 04:31:12 - train: epoch 0031, iter [01230, 01251], lr: 0.000523, loss: 0.4017
2022-10-03 04:31:31 - train: epoch 0031, iter [01240, 01251], lr: 0.000523, loss: 0.4332
2022-10-03 04:31:50 - train: epoch 0031, iter [01250, 01251], lr: 0.000523, loss: 0.4147
2022-10-03 04:31:55 - train: epoch 031, train_loss: 0.4180
2022-10-03 04:32:00 - until epoch: 031, best_loss: 0.4180
2022-10-03 04:32:00 - epoch 032 lr: 0.000523
2022-10-03 04:32:26 - train: epoch 0032, iter [00010, 01251], lr: 0.000523, loss: 0.4226
2022-10-03 04:32:46 - train: epoch 0032, iter [00020, 01251], lr: 0.000523, loss: 0.4246
2022-10-03 04:33:05 - train: epoch 0032, iter [00030, 01251], lr: 0.000523, loss: 0.4045
2022-10-03 04:33:25 - train: epoch 0032, iter [00040, 01251], lr: 0.000523, loss: 0.4099
2022-10-03 04:33:44 - train: epoch 0032, iter [00050, 01251], lr: 0.000523, loss: 0.4287
2022-10-03 04:34:04 - train: epoch 0032, iter [00060, 01251], lr: 0.000523, loss: 0.4134
2022-10-03 04:34:24 - train: epoch 0032, iter [00070, 01251], lr: 0.000523, loss: 0.4150
2022-10-03 04:34:43 - train: epoch 0032, iter [00080, 01251], lr: 0.000522, loss: 0.4143
2022-10-03 04:35:03 - train: epoch 0032, iter [00090, 01251], lr: 0.000522, loss: 0.4114
2022-10-03 04:35:23 - train: epoch 0032, iter [00100, 01251], lr: 0.000522, loss: 0.4337
2022-10-03 04:35:42 - train: epoch 0032, iter [00110, 01251], lr: 0.000522, loss: 0.4126
2022-10-03 04:36:02 - train: epoch 0032, iter [00120, 01251], lr: 0.000522, loss: 0.4414
2022-10-03 04:36:22 - train: epoch 0032, iter [00130, 01251], lr: 0.000522, loss: 0.4039
2022-10-03 04:36:41 - train: epoch 0032, iter [00140, 01251], lr: 0.000522, loss: 0.4201
2022-10-03 04:37:01 - train: epoch 0032, iter [00150, 01251], lr: 0.000522, loss: 0.4260
2022-10-03 04:37:21 - train: epoch 0032, iter [00160, 01251], lr: 0.000522, loss: 0.4226
2022-10-03 04:37:40 - train: epoch 0032, iter [00170, 01251], lr: 0.000522, loss: 0.4246
2022-10-03 04:38:00 - train: epoch 0032, iter [00180, 01251], lr: 0.000522, loss: 0.4160
2022-10-03 04:38:20 - train: epoch 0032, iter [00190, 01251], lr: 0.000522, loss: 0.4176
2022-10-03 04:38:39 - train: epoch 0032, iter [00200, 01251], lr: 0.000522, loss: 0.4226
2022-10-03 04:38:59 - train: epoch 0032, iter [00210, 01251], lr: 0.000522, loss: 0.4190
2022-10-03 04:39:19 - train: epoch 0032, iter [00220, 01251], lr: 0.000522, loss: 0.4246
2022-10-03 04:39:38 - train: epoch 0032, iter [00230, 01251], lr: 0.000522, loss: 0.4250
2022-10-03 04:39:58 - train: epoch 0032, iter [00240, 01251], lr: 0.000522, loss: 0.4221
2022-10-03 04:40:18 - train: epoch 0032, iter [00250, 01251], lr: 0.000522, loss: 0.4143
2022-10-03 04:40:38 - train: epoch 0032, iter [00260, 01251], lr: 0.000521, loss: 0.4210
2022-10-03 04:40:57 - train: epoch 0032, iter [00270, 01251], lr: 0.000521, loss: 0.4220
2022-10-03 04:41:17 - train: epoch 0032, iter [00280, 01251], lr: 0.000521, loss: 0.4153
2022-10-03 04:41:37 - train: epoch 0032, iter [00290, 01251], lr: 0.000521, loss: 0.4137
2022-10-03 04:41:57 - train: epoch 0032, iter [00300, 01251], lr: 0.000521, loss: 0.4225
2022-10-03 04:42:16 - train: epoch 0032, iter [00310, 01251], lr: 0.000521, loss: 0.4372
2022-10-03 04:42:36 - train: epoch 0032, iter [00320, 01251], lr: 0.000521, loss: 0.4273
2022-10-03 04:42:55 - train: epoch 0032, iter [00330, 01251], lr: 0.000521, loss: 0.4123
2022-10-03 04:43:15 - train: epoch 0032, iter [00340, 01251], lr: 0.000521, loss: 0.4299
2022-10-03 04:43:35 - train: epoch 0032, iter [00350, 01251], lr: 0.000521, loss: 0.4165
2022-10-03 04:43:55 - train: epoch 0032, iter [00360, 01251], lr: 0.000521, loss: 0.4303
2022-10-03 04:44:14 - train: epoch 0032, iter [00370, 01251], lr: 0.000521, loss: 0.4442
2022-10-03 04:44:34 - train: epoch 0032, iter [00380, 01251], lr: 0.000521, loss: 0.4172
2022-10-03 04:44:53 - train: epoch 0032, iter [00390, 01251], lr: 0.000521, loss: 0.4279
2022-10-03 04:45:13 - train: epoch 0032, iter [00400, 01251], lr: 0.000521, loss: 0.4231
2022-10-03 04:45:33 - train: epoch 0032, iter [00410, 01251], lr: 0.000521, loss: 0.4237
2022-10-03 04:45:53 - train: epoch 0032, iter [00420, 01251], lr: 0.000521, loss: 0.4242
2022-10-03 04:46:13 - train: epoch 0032, iter [00430, 01251], lr: 0.000521, loss: 0.4050
2022-10-03 04:46:32 - train: epoch 0032, iter [00440, 01251], lr: 0.000520, loss: 0.4012
2022-10-03 04:46:52 - train: epoch 0032, iter [00450, 01251], lr: 0.000520, loss: 0.4100
2022-10-03 04:47:12 - train: epoch 0032, iter [00460, 01251], lr: 0.000520, loss: 0.4168
2022-10-03 04:47:31 - train: epoch 0032, iter [00470, 01251], lr: 0.000520, loss: 0.4085
2022-10-03 04:47:51 - train: epoch 0032, iter [00480, 01251], lr: 0.000520, loss: 0.4299
2022-10-03 04:48:11 - train: epoch 0032, iter [00490, 01251], lr: 0.000520, loss: 0.4036
2022-10-03 04:48:30 - train: epoch 0032, iter [00500, 01251], lr: 0.000520, loss: 0.4272
2022-10-03 04:48:50 - train: epoch 0032, iter [00510, 01251], lr: 0.000520, loss: 0.4117
2022-10-03 04:49:10 - train: epoch 0032, iter [00520, 01251], lr: 0.000520, loss: 0.4200
2022-10-03 04:49:30 - train: epoch 0032, iter [00530, 01251], lr: 0.000520, loss: 0.3951
2022-10-03 04:49:49 - train: epoch 0032, iter [00540, 01251], lr: 0.000520, loss: 0.4024
2022-10-03 04:50:09 - train: epoch 0032, iter [00550, 01251], lr: 0.000520, loss: 0.4034
2022-10-03 04:50:29 - train: epoch 0032, iter [00560, 01251], lr: 0.000520, loss: 0.4392
2022-10-03 04:50:48 - train: epoch 0032, iter [00570, 01251], lr: 0.000520, loss: 0.4039
2022-10-03 04:51:08 - train: epoch 0032, iter [00580, 01251], lr: 0.000520, loss: 0.4205
2022-10-03 04:51:27 - train: epoch 0032, iter [00590, 01251], lr: 0.000520, loss: 0.4177
2022-10-03 04:51:47 - train: epoch 0032, iter [00600, 01251], lr: 0.000520, loss: 0.4070
2022-10-03 04:52:07 - train: epoch 0032, iter [00610, 01251], lr: 0.000519, loss: 0.4210
2022-10-03 04:52:27 - train: epoch 0032, iter [00620, 01251], lr: 0.000519, loss: 0.4146
2022-10-03 04:52:46 - train: epoch 0032, iter [00630, 01251], lr: 0.000519, loss: 0.4216
2022-10-03 04:53:06 - train: epoch 0032, iter [00640, 01251], lr: 0.000519, loss: 0.3906
2022-10-03 04:53:26 - train: epoch 0032, iter [00650, 01251], lr: 0.000519, loss: 0.4217
2022-10-03 04:53:46 - train: epoch 0032, iter [00660, 01251], lr: 0.000519, loss: 0.4194
2022-10-03 04:54:05 - train: epoch 0032, iter [00670, 01251], lr: 0.000519, loss: 0.4251
2022-10-03 04:54:25 - train: epoch 0032, iter [00680, 01251], lr: 0.000519, loss: 0.4075
2022-10-03 04:54:45 - train: epoch 0032, iter [00690, 01251], lr: 0.000519, loss: 0.4117
2022-10-03 04:55:04 - train: epoch 0032, iter [00700, 01251], lr: 0.000519, loss: 0.4061
2022-10-03 04:55:24 - train: epoch 0032, iter [00710, 01251], lr: 0.000519, loss: 0.4265
2022-10-03 04:55:44 - train: epoch 0032, iter [00720, 01251], lr: 0.000519, loss: 0.4071
2022-10-03 04:56:04 - train: epoch 0032, iter [00730, 01251], lr: 0.000519, loss: 0.4184
2022-10-03 04:56:24 - train: epoch 0032, iter [00740, 01251], lr: 0.000519, loss: 0.4239
2022-10-03 04:56:43 - train: epoch 0032, iter [00750, 01251], lr: 0.000519, loss: 0.3988
2022-10-03 04:57:03 - train: epoch 0032, iter [00760, 01251], lr: 0.000519, loss: 0.4309
2022-10-03 04:57:23 - train: epoch 0032, iter [00770, 01251], lr: 0.000519, loss: 0.4342
2022-10-03 04:57:42 - train: epoch 0032, iter [00780, 01251], lr: 0.000519, loss: 0.4105
2022-10-03 04:58:02 - train: epoch 0032, iter [00790, 01251], lr: 0.000518, loss: 0.4031
2022-10-03 04:58:22 - train: epoch 0032, iter [00800, 01251], lr: 0.000518, loss: 0.4193
2022-10-03 04:58:41 - train: epoch 0032, iter [00810, 01251], lr: 0.000518, loss: 0.4353
2022-10-03 04:59:01 - train: epoch 0032, iter [00820, 01251], lr: 0.000518, loss: 0.4027
2022-10-03 04:59:21 - train: epoch 0032, iter [00830, 01251], lr: 0.000518, loss: 0.4135
2022-10-03 04:59:41 - train: epoch 0032, iter [00840, 01251], lr: 0.000518, loss: 0.3913
2022-10-03 05:00:00 - train: epoch 0032, iter [00850, 01251], lr: 0.000518, loss: 0.4304
2022-10-03 05:00:20 - train: epoch 0032, iter [00860, 01251], lr: 0.000518, loss: 0.4067
2022-10-03 05:00:40 - train: epoch 0032, iter [00870, 01251], lr: 0.000518, loss: 0.4054
2022-10-03 05:00:59 - train: epoch 0032, iter [00880, 01251], lr: 0.000518, loss: 0.4151
2022-10-03 05:01:19 - train: epoch 0032, iter [00890, 01251], lr: 0.000518, loss: 0.4156
2022-10-03 05:01:39 - train: epoch 0032, iter [00900, 01251], lr: 0.000518, loss: 0.4178
2022-10-03 05:01:59 - train: epoch 0032, iter [00910, 01251], lr: 0.000518, loss: 0.4127
2022-10-03 05:02:19 - train: epoch 0032, iter [00920, 01251], lr: 0.000518, loss: 0.4161
2022-10-03 05:02:38 - train: epoch 0032, iter [00930, 01251], lr: 0.000518, loss: 0.4050
2022-10-03 05:02:58 - train: epoch 0032, iter [00940, 01251], lr: 0.000518, loss: 0.4382
2022-10-03 05:03:18 - train: epoch 0032, iter [00950, 01251], lr: 0.000518, loss: 0.3981
2022-10-03 05:03:37 - train: epoch 0032, iter [00960, 01251], lr: 0.000517, loss: 0.4204
2022-10-03 05:03:57 - train: epoch 0032, iter [00970, 01251], lr: 0.000517, loss: 0.4133
2022-10-03 05:04:17 - train: epoch 0032, iter [00980, 01251], lr: 0.000517, loss: 0.4217
2022-10-03 05:04:37 - train: epoch 0032, iter [00990, 01251], lr: 0.000517, loss: 0.4253
2022-10-03 05:04:56 - train: epoch 0032, iter [01000, 01251], lr: 0.000517, loss: 0.4119
2022-10-03 05:05:16 - train: epoch 0032, iter [01010, 01251], lr: 0.000517, loss: 0.4310
2022-10-03 05:05:36 - train: epoch 0032, iter [01020, 01251], lr: 0.000517, loss: 0.4154
2022-10-03 05:05:55 - train: epoch 0032, iter [01030, 01251], lr: 0.000517, loss: 0.4244
2022-10-03 05:06:15 - train: epoch 0032, iter [01040, 01251], lr: 0.000517, loss: 0.4038
2022-10-03 05:06:35 - train: epoch 0032, iter [01050, 01251], lr: 0.000517, loss: 0.4076
2022-10-03 05:06:55 - train: epoch 0032, iter [01060, 01251], lr: 0.000517, loss: 0.4397
2022-10-03 05:07:14 - train: epoch 0032, iter [01070, 01251], lr: 0.000517, loss: 0.4152
2022-10-03 05:07:34 - train: epoch 0032, iter [01080, 01251], lr: 0.000517, loss: 0.4395
2022-10-03 05:07:53 - train: epoch 0032, iter [01090, 01251], lr: 0.000517, loss: 0.4137
2022-10-03 05:08:13 - train: epoch 0032, iter [01100, 01251], lr: 0.000517, loss: 0.4264
2022-10-03 05:08:33 - train: epoch 0032, iter [01110, 01251], lr: 0.000517, loss: 0.4096
2022-10-03 05:08:52 - train: epoch 0032, iter [01120, 01251], lr: 0.000517, loss: 0.4202
2022-10-03 05:09:12 - train: epoch 0032, iter [01130, 01251], lr: 0.000517, loss: 0.4120
2022-10-03 05:09:32 - train: epoch 0032, iter [01140, 01251], lr: 0.000516, loss: 0.4141
2022-10-03 05:09:52 - train: epoch 0032, iter [01150, 01251], lr: 0.000516, loss: 0.4176
2022-10-03 05:10:11 - train: epoch 0032, iter [01160, 01251], lr: 0.000516, loss: 0.4210
2022-10-03 05:10:31 - train: epoch 0032, iter [01170, 01251], lr: 0.000516, loss: 0.4248
2022-10-03 05:10:51 - train: epoch 0032, iter [01180, 01251], lr: 0.000516, loss: 0.4134
2022-10-03 05:11:11 - train: epoch 0032, iter [01190, 01251], lr: 0.000516, loss: 0.4206
2022-10-03 05:11:30 - train: epoch 0032, iter [01200, 01251], lr: 0.000516, loss: 0.4143
2022-10-03 05:11:50 - train: epoch 0032, iter [01210, 01251], lr: 0.000516, loss: 0.4169
2022-10-03 05:12:09 - train: epoch 0032, iter [01220, 01251], lr: 0.000516, loss: 0.4186
2022-10-03 05:12:29 - train: epoch 0032, iter [01230, 01251], lr: 0.000516, loss: 0.4180
2022-10-03 05:12:49 - train: epoch 0032, iter [01240, 01251], lr: 0.000516, loss: 0.4404
2022-10-03 05:13:08 - train: epoch 0032, iter [01250, 01251], lr: 0.000516, loss: 0.4176
2022-10-03 05:13:12 - train: epoch 032, train_loss: 0.4176
2022-10-03 05:13:16 - until epoch: 032, best_loss: 0.4176
2022-10-03 05:13:16 - epoch 033 lr: 0.000516
2022-10-03 05:13:43 - train: epoch 0033, iter [00010, 01251], lr: 0.000516, loss: 0.4159
2022-10-03 05:14:03 - train: epoch 0033, iter [00020, 01251], lr: 0.000516, loss: 0.4164
2022-10-03 05:14:22 - train: epoch 0033, iter [00030, 01251], lr: 0.000516, loss: 0.4289
2022-10-03 05:14:42 - train: epoch 0033, iter [00040, 01251], lr: 0.000516, loss: 0.3985
2022-10-03 05:15:02 - train: epoch 0033, iter [00050, 01251], lr: 0.000516, loss: 0.4388
2022-10-03 05:15:21 - train: epoch 0033, iter [00060, 01251], lr: 0.000515, loss: 0.4134
2022-10-03 05:15:41 - train: epoch 0033, iter [00070, 01251], lr: 0.000515, loss: 0.3966
2022-10-03 05:16:01 - train: epoch 0033, iter [00080, 01251], lr: 0.000515, loss: 0.3922
2022-10-03 05:16:21 - train: epoch 0033, iter [00090, 01251], lr: 0.000515, loss: 0.4436
2022-10-03 05:16:40 - train: epoch 0033, iter [00100, 01251], lr: 0.000515, loss: 0.4023
2022-10-03 05:17:00 - train: epoch 0033, iter [00110, 01251], lr: 0.000515, loss: 0.3905
2022-10-03 05:17:20 - train: epoch 0033, iter [00120, 01251], lr: 0.000515, loss: 0.4061
2022-10-03 05:17:39 - train: epoch 0033, iter [00130, 01251], lr: 0.000515, loss: 0.4128
2022-10-03 05:17:59 - train: epoch 0033, iter [00140, 01251], lr: 0.000515, loss: 0.4272
2022-10-03 05:18:19 - train: epoch 0033, iter [00150, 01251], lr: 0.000515, loss: 0.4494
2022-10-03 05:18:38 - train: epoch 0033, iter [00160, 01251], lr: 0.000515, loss: 0.4164
2022-10-03 05:18:58 - train: epoch 0033, iter [00170, 01251], lr: 0.000515, loss: 0.4101
2022-10-03 05:19:18 - train: epoch 0033, iter [00180, 01251], lr: 0.000515, loss: 0.3916
2022-10-03 05:19:38 - train: epoch 0033, iter [00190, 01251], lr: 0.000515, loss: 0.4164
2022-10-03 05:19:58 - train: epoch 0033, iter [00200, 01251], lr: 0.000515, loss: 0.4384
2022-10-03 05:20:17 - train: epoch 0033, iter [00210, 01251], lr: 0.000515, loss: 0.4275
2022-10-03 05:20:37 - train: epoch 0033, iter [00220, 01251], lr: 0.000515, loss: 0.4054
2022-10-03 05:20:57 - train: epoch 0033, iter [00230, 01251], lr: 0.000514, loss: 0.4069
2022-10-03 05:21:16 - train: epoch 0033, iter [00240, 01251], lr: 0.000514, loss: 0.4181
2022-10-03 05:21:36 - train: epoch 0033, iter [00250, 01251], lr: 0.000514, loss: 0.3892
2022-10-03 05:21:56 - train: epoch 0033, iter [00260, 01251], lr: 0.000514, loss: 0.4140
2022-10-03 05:22:16 - train: epoch 0033, iter [00270, 01251], lr: 0.000514, loss: 0.4140
2022-10-03 05:22:36 - train: epoch 0033, iter [00280, 01251], lr: 0.000514, loss: 0.4095
2022-10-03 05:22:56 - train: epoch 0033, iter [00290, 01251], lr: 0.000514, loss: 0.4203
2022-10-03 05:23:15 - train: epoch 0033, iter [00300, 01251], lr: 0.000514, loss: 0.4167
2022-10-03 05:23:35 - train: epoch 0033, iter [00310, 01251], lr: 0.000514, loss: 0.4359
2022-10-03 05:23:55 - train: epoch 0033, iter [00320, 01251], lr: 0.000514, loss: 0.4284
2022-10-03 05:24:15 - train: epoch 0033, iter [00330, 01251], lr: 0.000514, loss: 0.4233
2022-10-03 05:24:34 - train: epoch 0033, iter [00340, 01251], lr: 0.000514, loss: 0.4109
2022-10-03 05:24:54 - train: epoch 0033, iter [00350, 01251], lr: 0.000514, loss: 0.4183
2022-10-03 05:25:14 - train: epoch 0033, iter [00360, 01251], lr: 0.000514, loss: 0.4176
2022-10-03 05:25:33 - train: epoch 0033, iter [00370, 01251], lr: 0.000514, loss: 0.4389
2022-10-03 05:25:53 - train: epoch 0033, iter [00380, 01251], lr: 0.000514, loss: 0.4007
2022-10-03 05:26:13 - train: epoch 0033, iter [00390, 01251], lr: 0.000514, loss: 0.4127
2022-10-03 05:26:32 - train: epoch 0033, iter [00400, 01251], lr: 0.000513, loss: 0.4055
2022-10-03 05:26:52 - train: epoch 0033, iter [00410, 01251], lr: 0.000513, loss: 0.4101
2022-10-03 05:27:12 - train: epoch 0033, iter [00420, 01251], lr: 0.000513, loss: 0.4209
2022-10-03 05:27:32 - train: epoch 0033, iter [00430, 01251], lr: 0.000513, loss: 0.4417
2022-10-03 05:27:51 - train: epoch 0033, iter [00440, 01251], lr: 0.000513, loss: 0.4146
2022-10-03 05:28:11 - train: epoch 0033, iter [00450, 01251], lr: 0.000513, loss: 0.4065
2022-10-03 05:28:31 - train: epoch 0033, iter [00460, 01251], lr: 0.000513, loss: 0.4189
2022-10-03 05:28:51 - train: epoch 0033, iter [00470, 01251], lr: 0.000513, loss: 0.4492
2022-10-03 05:29:10 - train: epoch 0033, iter [00480, 01251], lr: 0.000513, loss: 0.3966
2022-10-03 05:29:30 - train: epoch 0033, iter [00490, 01251], lr: 0.000513, loss: 0.4138
2022-10-03 05:29:49 - train: epoch 0033, iter [00500, 01251], lr: 0.000513, loss: 0.4294
2022-10-03 05:30:09 - train: epoch 0033, iter [00510, 01251], lr: 0.000513, loss: 0.4435
2022-10-03 05:30:29 - train: epoch 0033, iter [00520, 01251], lr: 0.000513, loss: 0.4378
2022-10-03 05:30:48 - train: epoch 0033, iter [00530, 01251], lr: 0.000513, loss: 0.4179
2022-10-03 05:31:08 - train: epoch 0033, iter [00540, 01251], lr: 0.000513, loss: 0.4207
2022-10-03 05:31:28 - train: epoch 0033, iter [00550, 01251], lr: 0.000513, loss: 0.4323
2022-10-03 05:31:48 - train: epoch 0033, iter [00560, 01251], lr: 0.000513, loss: 0.4009
2022-10-03 05:32:08 - train: epoch 0033, iter [00570, 01251], lr: 0.000512, loss: 0.4066
2022-10-03 05:32:27 - train: epoch 0033, iter [00580, 01251], lr: 0.000512, loss: 0.4234
2022-10-03 05:32:47 - train: epoch 0033, iter [00590, 01251], lr: 0.000512, loss: 0.4151
2022-10-03 05:33:07 - train: epoch 0033, iter [00600, 01251], lr: 0.000512, loss: 0.4218
2022-10-03 05:33:27 - train: epoch 0033, iter [00610, 01251], lr: 0.000512, loss: 0.3902
2022-10-03 05:33:46 - train: epoch 0033, iter [00620, 01251], lr: 0.000512, loss: 0.4119
2022-10-03 05:34:06 - train: epoch 0033, iter [00630, 01251], lr: 0.000512, loss: 0.4062
2022-10-03 05:34:26 - train: epoch 0033, iter [00640, 01251], lr: 0.000512, loss: 0.4276
2022-10-03 05:34:46 - train: epoch 0033, iter [00650, 01251], lr: 0.000512, loss: 0.4092
2022-10-03 05:35:06 - train: epoch 0033, iter [00660, 01251], lr: 0.000512, loss: 0.4328
2022-10-03 05:35:25 - train: epoch 0033, iter [00670, 01251], lr: 0.000512, loss: 0.4170
2022-10-03 05:35:45 - train: epoch 0033, iter [00680, 01251], lr: 0.000512, loss: 0.3991
2022-10-03 05:36:05 - train: epoch 0033, iter [00690, 01251], lr: 0.000512, loss: 0.4288
2022-10-03 05:36:25 - train: epoch 0033, iter [00700, 01251], lr: 0.000512, loss: 0.4460
2022-10-03 05:36:44 - train: epoch 0033, iter [00710, 01251], lr: 0.000512, loss: 0.4056
2022-10-03 05:37:04 - train: epoch 0033, iter [00720, 01251], lr: 0.000512, loss: 0.4260
2022-10-03 05:37:24 - train: epoch 0033, iter [00730, 01251], lr: 0.000512, loss: 0.4160
2022-10-03 05:37:44 - train: epoch 0033, iter [00740, 01251], lr: 0.000511, loss: 0.4187
2022-10-03 05:38:03 - train: epoch 0033, iter [00750, 01251], lr: 0.000511, loss: 0.4294
2022-10-03 05:38:23 - train: epoch 0033, iter [00760, 01251], lr: 0.000511, loss: 0.4452
2022-10-03 05:38:43 - train: epoch 0033, iter [00770, 01251], lr: 0.000511, loss: 0.4228
2022-10-03 05:39:03 - train: epoch 0033, iter [00780, 01251], lr: 0.000511, loss: 0.4401
2022-10-03 05:39:22 - train: epoch 0033, iter [00790, 01251], lr: 0.000511, loss: 0.4127
2022-10-03 05:39:42 - train: epoch 0033, iter [00800, 01251], lr: 0.000511, loss: 0.4356
2022-10-03 05:40:02 - train: epoch 0033, iter [00810, 01251], lr: 0.000511, loss: 0.4392
2022-10-03 05:40:22 - train: epoch 0033, iter [00820, 01251], lr: 0.000511, loss: 0.4446
2022-10-03 05:40:41 - train: epoch 0033, iter [00830, 01251], lr: 0.000511, loss: 0.4252
2022-10-03 05:41:01 - train: epoch 0033, iter [00840, 01251], lr: 0.000511, loss: 0.4276
2022-10-03 05:41:21 - train: epoch 0033, iter [00850, 01251], lr: 0.000511, loss: 0.4299
2022-10-03 05:41:40 - train: epoch 0033, iter [00860, 01251], lr: 0.000511, loss: 0.4177
2022-10-03 05:42:00 - train: epoch 0033, iter [00870, 01251], lr: 0.000511, loss: 0.4192
2022-10-03 05:42:20 - train: epoch 0033, iter [00880, 01251], lr: 0.000511, loss: 0.4151
2022-10-03 05:42:39 - train: epoch 0033, iter [00890, 01251], lr: 0.000511, loss: 0.4103
2022-10-03 05:42:59 - train: epoch 0033, iter [00900, 01251], lr: 0.000511, loss: 0.4107
2022-10-03 05:43:19 - train: epoch 0033, iter [00910, 01251], lr: 0.000510, loss: 0.4097
2022-10-03 05:43:39 - train: epoch 0033, iter [00920, 01251], lr: 0.000510, loss: 0.4230
2022-10-03 05:43:58 - train: epoch 0033, iter [00930, 01251], lr: 0.000510, loss: 0.4122
2022-10-03 05:44:18 - train: epoch 0033, iter [00940, 01251], lr: 0.000510, loss: 0.4505
2022-10-03 05:44:38 - train: epoch 0033, iter [00950, 01251], lr: 0.000510, loss: 0.4162
2022-10-03 05:44:58 - train: epoch 0033, iter [00960, 01251], lr: 0.000510, loss: 0.4211
2022-10-03 05:45:17 - train: epoch 0033, iter [00970, 01251], lr: 0.000510, loss: 0.4169
2022-10-03 05:45:37 - train: epoch 0033, iter [00980, 01251], lr: 0.000510, loss: 0.4080
2022-10-03 05:45:57 - train: epoch 0033, iter [00990, 01251], lr: 0.000510, loss: 0.4172
2022-10-03 05:46:17 - train: epoch 0033, iter [01000, 01251], lr: 0.000510, loss: 0.4173
2022-10-03 05:46:36 - train: epoch 0033, iter [01010, 01251], lr: 0.000510, loss: 0.4142
2022-10-03 05:46:56 - train: epoch 0033, iter [01020, 01251], lr: 0.000510, loss: 0.3938
2022-10-03 05:47:16 - train: epoch 0033, iter [01030, 01251], lr: 0.000510, loss: 0.4072
2022-10-03 05:47:36 - train: epoch 0033, iter [01040, 01251], lr: 0.000510, loss: 0.4131
2022-10-03 05:47:56 - train: epoch 0033, iter [01050, 01251], lr: 0.000510, loss: 0.4299
2022-10-03 05:48:15 - train: epoch 0033, iter [01060, 01251], lr: 0.000510, loss: 0.4217
2022-10-03 05:48:35 - train: epoch 0033, iter [01070, 01251], lr: 0.000509, loss: 0.4205
2022-10-03 05:48:54 - train: epoch 0033, iter [01080, 01251], lr: 0.000509, loss: 0.4294
2022-10-03 05:49:14 - train: epoch 0033, iter [01090, 01251], lr: 0.000509, loss: 0.4034
2022-10-03 05:49:34 - train: epoch 0033, iter [01100, 01251], lr: 0.000509, loss: 0.4231
2022-10-03 05:49:54 - train: epoch 0033, iter [01110, 01251], lr: 0.000509, loss: 0.4261
2022-10-03 05:50:13 - train: epoch 0033, iter [01120, 01251], lr: 0.000509, loss: 0.4097
2022-10-03 05:50:33 - train: epoch 0033, iter [01130, 01251], lr: 0.000509, loss: 0.3961
2022-10-03 05:50:52 - train: epoch 0033, iter [01140, 01251], lr: 0.000509, loss: 0.4059
2022-10-03 05:51:12 - train: epoch 0033, iter [01150, 01251], lr: 0.000509, loss: 0.4128
2022-10-03 05:51:32 - train: epoch 0033, iter [01160, 01251], lr: 0.000509, loss: 0.4113
2022-10-03 05:51:52 - train: epoch 0033, iter [01170, 01251], lr: 0.000509, loss: 0.3994
2022-10-03 05:52:12 - train: epoch 0033, iter [01180, 01251], lr: 0.000509, loss: 0.4147
2022-10-03 05:52:31 - train: epoch 0033, iter [01190, 01251], lr: 0.000509, loss: 0.4101
2022-10-03 05:52:51 - train: epoch 0033, iter [01200, 01251], lr: 0.000509, loss: 0.4138
2022-10-03 05:53:11 - train: epoch 0033, iter [01210, 01251], lr: 0.000509, loss: 0.4195
2022-10-03 05:53:31 - train: epoch 0033, iter [01220, 01251], lr: 0.000509, loss: 0.4139
2022-10-03 05:53:50 - train: epoch 0033, iter [01230, 01251], lr: 0.000509, loss: 0.4128
2022-10-03 05:54:10 - train: epoch 0033, iter [01240, 01251], lr: 0.000508, loss: 0.3825
2022-10-03 05:54:29 - train: epoch 0033, iter [01250, 01251], lr: 0.000508, loss: 0.4346
2022-10-03 05:54:33 - train: epoch 033, train_loss: 0.4171
2022-10-03 05:54:37 - until epoch: 033, best_loss: 0.4171
2022-10-03 05:54:37 - epoch 034 lr: 0.000508
2022-10-03 05:55:04 - train: epoch 0034, iter [00010, 01251], lr: 0.000508, loss: 0.4260
2022-10-03 05:55:23 - train: epoch 0034, iter [00020, 01251], lr: 0.000508, loss: 0.4187
2022-10-03 05:55:43 - train: epoch 0034, iter [00030, 01251], lr: 0.000508, loss: 0.4226
2022-10-03 05:56:03 - train: epoch 0034, iter [00040, 01251], lr: 0.000508, loss: 0.4140
2022-10-03 05:56:22 - train: epoch 0034, iter [00050, 01251], lr: 0.000508, loss: 0.4018
2022-10-03 05:56:42 - train: epoch 0034, iter [00060, 01251], lr: 0.000508, loss: 0.4256
2022-10-03 05:57:01 - train: epoch 0034, iter [00070, 01251], lr: 0.000508, loss: 0.4231
2022-10-03 05:57:21 - train: epoch 0034, iter [00080, 01251], lr: 0.000508, loss: 0.4284
2022-10-03 05:57:41 - train: epoch 0034, iter [00090, 01251], lr: 0.000508, loss: 0.4132
2022-10-03 05:58:00 - train: epoch 0034, iter [00100, 01251], lr: 0.000508, loss: 0.4137
2022-10-03 05:58:20 - train: epoch 0034, iter [00110, 01251], lr: 0.000508, loss: 0.4466
2022-10-03 05:58:40 - train: epoch 0034, iter [00120, 01251], lr: 0.000508, loss: 0.4007
2022-10-03 05:58:59 - train: epoch 0034, iter [00130, 01251], lr: 0.000508, loss: 0.4228
2022-10-03 05:59:19 - train: epoch 0034, iter [00140, 01251], lr: 0.000508, loss: 0.4146
2022-10-03 05:59:39 - train: epoch 0034, iter [00150, 01251], lr: 0.000507, loss: 0.3986
2022-10-03 05:59:58 - train: epoch 0034, iter [00160, 01251], lr: 0.000507, loss: 0.4279
2022-10-03 06:00:18 - train: epoch 0034, iter [00170, 01251], lr: 0.000507, loss: 0.4378
2022-10-03 06:00:38 - train: epoch 0034, iter [00180, 01251], lr: 0.000507, loss: 0.4070
2022-10-03 06:00:57 - train: epoch 0034, iter [00190, 01251], lr: 0.000507, loss: 0.4194
2022-10-03 06:01:17 - train: epoch 0034, iter [00200, 01251], lr: 0.000507, loss: 0.4270
2022-10-03 06:01:36 - train: epoch 0034, iter [00210, 01251], lr: 0.000507, loss: 0.4308
2022-10-03 06:01:56 - train: epoch 0034, iter [00220, 01251], lr: 0.000507, loss: 0.4134
2022-10-03 06:02:16 - train: epoch 0034, iter [00230, 01251], lr: 0.000507, loss: 0.4201
2022-10-03 06:02:36 - train: epoch 0034, iter [00240, 01251], lr: 0.000507, loss: 0.4182
2022-10-03 06:02:55 - train: epoch 0034, iter [00250, 01251], lr: 0.000507, loss: 0.4184
2022-10-03 06:03:15 - train: epoch 0034, iter [00260, 01251], lr: 0.000507, loss: 0.4014
2022-10-03 06:03:34 - train: epoch 0034, iter [00270, 01251], lr: 0.000507, loss: 0.4085
2022-10-03 06:03:54 - train: epoch 0034, iter [00280, 01251], lr: 0.000507, loss: 0.4234
2022-10-03 06:04:14 - train: epoch 0034, iter [00290, 01251], lr: 0.000507, loss: 0.4052
2022-10-03 06:04:34 - train: epoch 0034, iter [00300, 01251], lr: 0.000507, loss: 0.3989
2022-10-03 06:04:53 - train: epoch 0034, iter [00310, 01251], lr: 0.000507, loss: 0.4058
2022-10-03 06:05:13 - train: epoch 0034, iter [00320, 01251], lr: 0.000506, loss: 0.4162
2022-10-03 06:05:33 - train: epoch 0034, iter [00330, 01251], lr: 0.000506, loss: 0.4031
2022-10-03 06:05:52 - train: epoch 0034, iter [00340, 01251], lr: 0.000506, loss: 0.4198
2022-10-03 06:06:12 - train: epoch 0034, iter [00350, 01251], lr: 0.000506, loss: 0.4095
2022-10-03 06:06:32 - train: epoch 0034, iter [00360, 01251], lr: 0.000506, loss: 0.4197
2022-10-03 06:06:52 - train: epoch 0034, iter [00370, 01251], lr: 0.000506, loss: 0.4159
2022-10-03 06:07:12 - train: epoch 0034, iter [00380, 01251], lr: 0.000506, loss: 0.4324
2022-10-03 06:07:31 - train: epoch 0034, iter [00390, 01251], lr: 0.000506, loss: 0.4113
2022-10-03 06:07:51 - train: epoch 0034, iter [00400, 01251], lr: 0.000506, loss: 0.4143
2022-10-03 06:08:11 - train: epoch 0034, iter [00410, 01251], lr: 0.000506, loss: 0.4036
2022-10-03 06:08:30 - train: epoch 0034, iter [00420, 01251], lr: 0.000506, loss: 0.4109
2022-10-03 06:08:50 - train: epoch 0034, iter [00430, 01251], lr: 0.000506, loss: 0.4123
2022-10-03 06:09:10 - train: epoch 0034, iter [00440, 01251], lr: 0.000506, loss: 0.4062
2022-10-03 06:09:30 - train: epoch 0034, iter [00450, 01251], lr: 0.000506, loss: 0.4185
2022-10-03 06:09:50 - train: epoch 0034, iter [00460, 01251], lr: 0.000506, loss: 0.4163
2022-10-03 06:10:09 - train: epoch 0034, iter [00470, 01251], lr: 0.000506, loss: 0.4098
2022-10-03 06:10:29 - train: epoch 0034, iter [00480, 01251], lr: 0.000505, loss: 0.4078
2022-10-03 06:10:49 - train: epoch 0034, iter [00490, 01251], lr: 0.000505, loss: 0.4292
2022-10-03 06:11:09 - train: epoch 0034, iter [00500, 01251], lr: 0.000505, loss: 0.4122
2022-10-03 06:11:28 - train: epoch 0034, iter [00510, 01251], lr: 0.000505, loss: 0.4211
2022-10-03 06:11:48 - train: epoch 0034, iter [00520, 01251], lr: 0.000505, loss: 0.3951
2022-10-03 06:12:08 - train: epoch 0034, iter [00530, 01251], lr: 0.000505, loss: 0.4084
2022-10-03 06:12:28 - train: epoch 0034, iter [00540, 01251], lr: 0.000505, loss: 0.3892
2022-10-03 06:12:47 - train: epoch 0034, iter [00550, 01251], lr: 0.000505, loss: 0.4324
2022-10-03 06:13:07 - train: epoch 0034, iter [00560, 01251], lr: 0.000505, loss: 0.4097
2022-10-03 06:13:27 - train: epoch 0034, iter [00570, 01251], lr: 0.000505, loss: 0.4169
2022-10-03 06:13:46 - train: epoch 0034, iter [00580, 01251], lr: 0.000505, loss: 0.4329
2022-10-03 06:14:06 - train: epoch 0034, iter [00590, 01251], lr: 0.000505, loss: 0.4286
2022-10-03 06:14:26 - train: epoch 0034, iter [00600, 01251], lr: 0.000505, loss: 0.4171
2022-10-03 06:14:46 - train: epoch 0034, iter [00610, 01251], lr: 0.000505, loss: 0.3891
2022-10-03 06:15:05 - train: epoch 0034, iter [00620, 01251], lr: 0.000505, loss: 0.4189
2022-10-03 06:15:25 - train: epoch 0034, iter [00630, 01251], lr: 0.000505, loss: 0.3967
2022-10-03 06:15:45 - train: epoch 0034, iter [00640, 01251], lr: 0.000505, loss: 0.4138
2022-10-03 06:16:05 - train: epoch 0034, iter [00650, 01251], lr: 0.000504, loss: 0.4191
2022-10-03 06:16:25 - train: epoch 0034, iter [00660, 01251], lr: 0.000504, loss: 0.4270
2022-10-03 06:16:44 - train: epoch 0034, iter [00670, 01251], lr: 0.000504, loss: 0.4251
2022-10-03 06:17:04 - train: epoch 0034, iter [00680, 01251], lr: 0.000504, loss: 0.4275
2022-10-03 06:17:23 - train: epoch 0034, iter [00690, 01251], lr: 0.000504, loss: 0.4015
2022-10-03 06:17:43 - train: epoch 0034, iter [00700, 01251], lr: 0.000504, loss: 0.4250
2022-10-03 06:18:03 - train: epoch 0034, iter [00710, 01251], lr: 0.000504, loss: 0.4223
2022-10-03 06:18:23 - train: epoch 0034, iter [00720, 01251], lr: 0.000504, loss: 0.4139
2022-10-03 06:18:42 - train: epoch 0034, iter [00730, 01251], lr: 0.000504, loss: 0.4188
2022-10-03 06:19:02 - train: epoch 0034, iter [00740, 01251], lr: 0.000504, loss: 0.4019
2022-10-03 06:19:22 - train: epoch 0034, iter [00750, 01251], lr: 0.000504, loss: 0.4173
2022-10-03 06:19:42 - train: epoch 0034, iter [00760, 01251], lr: 0.000504, loss: 0.4226
2022-10-03 06:20:01 - train: epoch 0034, iter [00770, 01251], lr: 0.000504, loss: 0.4064
2022-10-03 06:20:21 - train: epoch 0034, iter [00780, 01251], lr: 0.000504, loss: 0.4129
2022-10-03 06:20:41 - train: epoch 0034, iter [00790, 01251], lr: 0.000504, loss: 0.4142
2022-10-03 06:21:01 - train: epoch 0034, iter [00800, 01251], lr: 0.000504, loss: 0.4190
2022-10-03 06:21:20 - train: epoch 0034, iter [00810, 01251], lr: 0.000503, loss: 0.4049
2022-10-03 06:21:40 - train: epoch 0034, iter [00820, 01251], lr: 0.000503, loss: 0.4306
2022-10-03 06:22:00 - train: epoch 0034, iter [00830, 01251], lr: 0.000503, loss: 0.3798
2022-10-03 06:22:20 - train: epoch 0034, iter [00840, 01251], lr: 0.000503, loss: 0.4130
2022-10-03 06:22:40 - train: epoch 0034, iter [00850, 01251], lr: 0.000503, loss: 0.4323
2022-10-03 06:22:59 - train: epoch 0034, iter [00860, 01251], lr: 0.000503, loss: 0.4256
2022-10-03 06:23:19 - train: epoch 0034, iter [00870, 01251], lr: 0.000503, loss: 0.4125
2022-10-03 06:23:39 - train: epoch 0034, iter [00880, 01251], lr: 0.000503, loss: 0.4020
2022-10-03 06:23:59 - train: epoch 0034, iter [00890, 01251], lr: 0.000503, loss: 0.4194
2022-10-03 06:24:19 - train: epoch 0034, iter [00900, 01251], lr: 0.000503, loss: 0.4323
2022-10-03 06:24:38 - train: epoch 0034, iter [00910, 01251], lr: 0.000503, loss: 0.4257
2022-10-03 06:24:58 - train: epoch 0034, iter [00920, 01251], lr: 0.000503, loss: 0.4224
2022-10-03 06:25:18 - train: epoch 0034, iter [00930, 01251], lr: 0.000503, loss: 0.4439
2022-10-03 06:25:37 - train: epoch 0034, iter [00940, 01251], lr: 0.000503, loss: 0.4183
2022-10-03 06:25:57 - train: epoch 0034, iter [00950, 01251], lr: 0.000503, loss: 0.4195
2022-10-03 06:26:17 - train: epoch 0034, iter [00960, 01251], lr: 0.000503, loss: 0.4159
2022-10-03 06:26:36 - train: epoch 0034, iter [00970, 01251], lr: 0.000502, loss: 0.4134
2022-10-03 06:26:56 - train: epoch 0034, iter [00980, 01251], lr: 0.000502, loss: 0.4081
2022-10-03 06:27:16 - train: epoch 0034, iter [00990, 01251], lr: 0.000502, loss: 0.4221
2022-10-03 06:27:36 - train: epoch 0034, iter [01000, 01251], lr: 0.000502, loss: 0.4051
2022-10-03 06:27:55 - train: epoch 0034, iter [01010, 01251], lr: 0.000502, loss: 0.4108
2022-10-03 06:28:15 - train: epoch 0034, iter [01020, 01251], lr: 0.000502, loss: 0.4126
2022-10-03 06:28:35 - train: epoch 0034, iter [01030, 01251], lr: 0.000502, loss: 0.3948
2022-10-03 06:28:55 - train: epoch 0034, iter [01040, 01251], lr: 0.000502, loss: 0.4334
2022-10-03 06:29:14 - train: epoch 0034, iter [01050, 01251], lr: 0.000502, loss: 0.4140
2022-10-03 06:29:34 - train: epoch 0034, iter [01060, 01251], lr: 0.000502, loss: 0.4293
2022-10-03 06:29:54 - train: epoch 0034, iter [01070, 01251], lr: 0.000502, loss: 0.4073
2022-10-03 06:30:13 - train: epoch 0034, iter [01080, 01251], lr: 0.000502, loss: 0.4262
2022-10-03 06:30:33 - train: epoch 0034, iter [01090, 01251], lr: 0.000502, loss: 0.4417
2022-10-03 06:30:53 - train: epoch 0034, iter [01100, 01251], lr: 0.000502, loss: 0.4289
2022-10-03 06:31:13 - train: epoch 0034, iter [01110, 01251], lr: 0.000502, loss: 0.4063
2022-10-03 06:31:33 - train: epoch 0034, iter [01120, 01251], lr: 0.000502, loss: 0.4123
2022-10-03 06:31:53 - train: epoch 0034, iter [01130, 01251], lr: 0.000501, loss: 0.4209
2022-10-03 06:32:12 - train: epoch 0034, iter [01140, 01251], lr: 0.000501, loss: 0.4080
2022-10-03 06:32:32 - train: epoch 0034, iter [01150, 01251], lr: 0.000501, loss: 0.4156
2022-10-03 06:32:52 - train: epoch 0034, iter [01160, 01251], lr: 0.000501, loss: 0.4009
2022-10-03 06:33:11 - train: epoch 0034, iter [01170, 01251], lr: 0.000501, loss: 0.4164
2022-10-03 06:33:31 - train: epoch 0034, iter [01180, 01251], lr: 0.000501, loss: 0.4222
2022-10-03 06:33:51 - train: epoch 0034, iter [01190, 01251], lr: 0.000501, loss: 0.4168
2022-10-03 06:34:10 - train: epoch 0034, iter [01200, 01251], lr: 0.000501, loss: 0.4065
2022-10-03 06:34:30 - train: epoch 0034, iter [01210, 01251], lr: 0.000501, loss: 0.4436
2022-10-03 06:34:50 - train: epoch 0034, iter [01220, 01251], lr: 0.000501, loss: 0.4150
2022-10-03 06:35:10 - train: epoch 0034, iter [01230, 01251], lr: 0.000501, loss: 0.4209
2022-10-03 06:35:29 - train: epoch 0034, iter [01240, 01251], lr: 0.000501, loss: 0.4207
2022-10-03 06:35:48 - train: epoch 0034, iter [01250, 01251], lr: 0.000501, loss: 0.4120
2022-10-03 06:35:54 - train: epoch 034, train_loss: 0.4167
2022-10-03 06:35:59 - until epoch: 034, best_loss: 0.4167
2022-10-03 06:35:59 - epoch 035 lr: 0.000501
2022-10-03 06:36:24 - train: epoch 0035, iter [00010, 01251], lr: 0.000501, loss: 0.4298
2022-10-03 06:36:44 - train: epoch 0035, iter [00020, 01251], lr: 0.000501, loss: 0.4118
2022-10-03 06:37:03 - train: epoch 0035, iter [00030, 01251], lr: 0.000501, loss: 0.4084
2022-10-03 06:37:23 - train: epoch 0035, iter [00040, 01251], lr: 0.000500, loss: 0.4195
2022-10-03 06:37:43 - train: epoch 0035, iter [00050, 01251], lr: 0.000500, loss: 0.4029
2022-10-03 06:38:02 - train: epoch 0035, iter [00060, 01251], lr: 0.000500, loss: 0.4313
2022-10-03 06:38:22 - train: epoch 0035, iter [00070, 01251], lr: 0.000500, loss: 0.4006
2022-10-03 06:38:42 - train: epoch 0035, iter [00080, 01251], lr: 0.000500, loss: 0.4101
2022-10-03 06:39:02 - train: epoch 0035, iter [00090, 01251], lr: 0.000500, loss: 0.4214
2022-10-03 06:39:22 - train: epoch 0035, iter [00100, 01251], lr: 0.000500, loss: 0.4140
2022-10-03 06:39:41 - train: epoch 0035, iter [00110, 01251], lr: 0.000500, loss: 0.4021
2022-10-03 06:40:01 - train: epoch 0035, iter [00120, 01251], lr: 0.000500, loss: 0.4062
2022-10-03 06:40:21 - train: epoch 0035, iter [00130, 01251], lr: 0.000500, loss: 0.4192
2022-10-03 06:40:41 - train: epoch 0035, iter [00140, 01251], lr: 0.000500, loss: 0.4267
2022-10-03 06:41:01 - train: epoch 0035, iter [00150, 01251], lr: 0.000500, loss: 0.4010
2022-10-03 06:41:21 - train: epoch 0035, iter [00160, 01251], lr: 0.000500, loss: 0.4058
2022-10-03 06:41:41 - train: epoch 0035, iter [00170, 01251], lr: 0.000500, loss: 0.4291
2022-10-03 06:42:00 - train: epoch 0035, iter [00180, 01251], lr: 0.000500, loss: 0.4182
2022-10-03 06:42:20 - train: epoch 0035, iter [00190, 01251], lr: 0.000500, loss: 0.4164
2022-10-03 06:42:40 - train: epoch 0035, iter [00200, 01251], lr: 0.000499, loss: 0.4362
2022-10-03 06:43:00 - train: epoch 0035, iter [00210, 01251], lr: 0.000499, loss: 0.4138
2022-10-03 06:43:19 - train: epoch 0035, iter [00220, 01251], lr: 0.000499, loss: 0.4027
2022-10-03 06:43:40 - train: epoch 0035, iter [00230, 01251], lr: 0.000499, loss: 0.3946
2022-10-03 06:43:59 - train: epoch 0035, iter [00240, 01251], lr: 0.000499, loss: 0.4113
2022-10-03 06:44:19 - train: epoch 0035, iter [00250, 01251], lr: 0.000499, loss: 0.4154
2022-10-03 06:44:39 - train: epoch 0035, iter [00260, 01251], lr: 0.000499, loss: 0.3974
2022-10-03 06:44:59 - train: epoch 0035, iter [00270, 01251], lr: 0.000499, loss: 0.4176
2022-10-03 06:45:18 - train: epoch 0035, iter [00280, 01251], lr: 0.000499, loss: 0.4094
2022-10-03 06:45:38 - train: epoch 0035, iter [00290, 01251], lr: 0.000499, loss: 0.4201
2022-10-03 06:45:58 - train: epoch 0035, iter [00300, 01251], lr: 0.000499, loss: 0.4157
2022-10-03 06:46:18 - train: epoch 0035, iter [00310, 01251], lr: 0.000499, loss: 0.4110
2022-10-03 06:46:38 - train: epoch 0035, iter [00320, 01251], lr: 0.000499, loss: 0.4144
2022-10-03 06:46:57 - train: epoch 0035, iter [00330, 01251], lr: 0.000499, loss: 0.4124
2022-10-03 06:47:17 - train: epoch 0035, iter [00340, 01251], lr: 0.000499, loss: 0.3988
2022-10-03 06:47:37 - train: epoch 0035, iter [00350, 01251], lr: 0.000499, loss: 0.4309
2022-10-03 06:47:57 - train: epoch 0035, iter [00360, 01251], lr: 0.000498, loss: 0.4321
2022-10-03 06:48:16 - train: epoch 0035, iter [00370, 01251], lr: 0.000498, loss: 0.4242
2022-10-03 06:48:36 - train: epoch 0035, iter [00380, 01251], lr: 0.000498, loss: 0.4064
2022-10-03 06:48:56 - train: epoch 0035, iter [00390, 01251], lr: 0.000498, loss: 0.4011
2022-10-03 06:49:16 - train: epoch 0035, iter [00400, 01251], lr: 0.000498, loss: 0.4221
2022-10-03 06:49:36 - train: epoch 0035, iter [00410, 01251], lr: 0.000498, loss: 0.4114
2022-10-03 06:49:55 - train: epoch 0035, iter [00420, 01251], lr: 0.000498, loss: 0.4146
2022-10-03 06:50:15 - train: epoch 0035, iter [00430, 01251], lr: 0.000498, loss: 0.4184
2022-10-03 06:50:35 - train: epoch 0035, iter [00440, 01251], lr: 0.000498, loss: 0.4122
2022-10-03 06:50:55 - train: epoch 0035, iter [00450, 01251], lr: 0.000498, loss: 0.4078
2022-10-03 06:51:15 - train: epoch 0035, iter [00460, 01251], lr: 0.000498, loss: 0.4157
2022-10-03 06:51:35 - train: epoch 0035, iter [00470, 01251], lr: 0.000498, loss: 0.4082
2022-10-03 06:51:54 - train: epoch 0035, iter [00480, 01251], lr: 0.000498, loss: 0.4122
2022-10-03 06:52:14 - train: epoch 0035, iter [00490, 01251], lr: 0.000498, loss: 0.4057
2022-10-03 06:52:34 - train: epoch 0035, iter [00500, 01251], lr: 0.000498, loss: 0.4198
2022-10-03 06:52:54 - train: epoch 0035, iter [00510, 01251], lr: 0.000498, loss: 0.4235
2022-10-03 06:53:13 - train: epoch 0035, iter [00520, 01251], lr: 0.000497, loss: 0.4101
2022-10-03 06:53:33 - train: epoch 0035, iter [00530, 01251], lr: 0.000497, loss: 0.4273
2022-10-03 06:53:53 - train: epoch 0035, iter [00540, 01251], lr: 0.000497, loss: 0.4146
2022-10-03 06:54:13 - train: epoch 0035, iter [00550, 01251], lr: 0.000497, loss: 0.4329
2022-10-03 06:54:32 - train: epoch 0035, iter [00560, 01251], lr: 0.000497, loss: 0.4054
2022-10-03 06:54:52 - train: epoch 0035, iter [00570, 01251], lr: 0.000497, loss: 0.4130
2022-10-03 06:55:12 - train: epoch 0035, iter [00580, 01251], lr: 0.000497, loss: 0.4140
2022-10-03 06:55:31 - train: epoch 0035, iter [00590, 01251], lr: 0.000497, loss: 0.4079
2022-10-03 06:55:51 - train: epoch 0035, iter [00600, 01251], lr: 0.000497, loss: 0.4113
2022-10-03 06:56:11 - train: epoch 0035, iter [00610, 01251], lr: 0.000497, loss: 0.4118
2022-10-03 06:56:31 - train: epoch 0035, iter [00620, 01251], lr: 0.000497, loss: 0.4224
2022-10-03 06:56:51 - train: epoch 0035, iter [00630, 01251], lr: 0.000497, loss: 0.4111
2022-10-03 06:57:10 - train: epoch 0035, iter [00640, 01251], lr: 0.000497, loss: 0.4160
2022-10-03 06:57:30 - train: epoch 0035, iter [00650, 01251], lr: 0.000497, loss: 0.4347
2022-10-03 06:57:50 - train: epoch 0035, iter [00660, 01251], lr: 0.000497, loss: 0.4204
2022-10-03 06:58:10 - train: epoch 0035, iter [00670, 01251], lr: 0.000497, loss: 0.4188
2022-10-03 06:58:29 - train: epoch 0035, iter [00680, 01251], lr: 0.000496, loss: 0.4078
2022-10-03 06:58:49 - train: epoch 0035, iter [00690, 01251], lr: 0.000496, loss: 0.4145
2022-10-03 06:59:09 - train: epoch 0035, iter [00700, 01251], lr: 0.000496, loss: 0.4095
2022-10-03 06:59:29 - train: epoch 0035, iter [00710, 01251], lr: 0.000496, loss: 0.4296
2022-10-03 06:59:48 - train: epoch 0035, iter [00720, 01251], lr: 0.000496, loss: 0.4182
2022-10-03 07:00:08 - train: epoch 0035, iter [00730, 01251], lr: 0.000496, loss: 0.4098
2022-10-03 07:00:28 - train: epoch 0035, iter [00740, 01251], lr: 0.000496, loss: 0.4053
2022-10-03 07:00:47 - train: epoch 0035, iter [00750, 01251], lr: 0.000496, loss: 0.4205
2022-10-03 07:01:07 - train: epoch 0035, iter [00760, 01251], lr: 0.000496, loss: 0.4252
2022-10-03 07:01:27 - train: epoch 0035, iter [00770, 01251], lr: 0.000496, loss: 0.4014
2022-10-03 07:01:47 - train: epoch 0035, iter [00780, 01251], lr: 0.000496, loss: 0.4303
2022-10-03 07:02:06 - train: epoch 0035, iter [00790, 01251], lr: 0.000496, loss: 0.4085
2022-10-03 07:02:26 - train: epoch 0035, iter [00800, 01251], lr: 0.000496, loss: 0.4070
2022-10-03 07:02:45 - train: epoch 0035, iter [00810, 01251], lr: 0.000496, loss: 0.4248
2022-10-03 07:03:05 - train: epoch 0035, iter [00820, 01251], lr: 0.000496, loss: 0.4134
2022-10-03 07:03:25 - train: epoch 0035, iter [00830, 01251], lr: 0.000496, loss: 0.4163
2022-10-03 07:03:45 - train: epoch 0035, iter [00840, 01251], lr: 0.000495, loss: 0.4080
2022-10-03 07:04:04 - train: epoch 0035, iter [00850, 01251], lr: 0.000495, loss: 0.4162
2022-10-03 07:04:24 - train: epoch 0035, iter [00860, 01251], lr: 0.000495, loss: 0.4215
2022-10-03 07:04:44 - train: epoch 0035, iter [00870, 01251], lr: 0.000495, loss: 0.4094
2022-10-03 07:05:04 - train: epoch 0035, iter [00880, 01251], lr: 0.000495, loss: 0.4446
2022-10-03 07:05:23 - train: epoch 0035, iter [00890, 01251], lr: 0.000495, loss: 0.4082
2022-10-03 07:05:43 - train: epoch 0035, iter [00900, 01251], lr: 0.000495, loss: 0.4178
2022-10-03 07:06:03 - train: epoch 0035, iter [00910, 01251], lr: 0.000495, loss: 0.4003
2022-10-03 07:06:23 - train: epoch 0035, iter [00920, 01251], lr: 0.000495, loss: 0.4108
2022-10-03 07:06:42 - train: epoch 0035, iter [00930, 01251], lr: 0.000495, loss: 0.4326
2022-10-03 07:07:02 - train: epoch 0035, iter [00940, 01251], lr: 0.000495, loss: 0.4208
2022-10-03 07:07:22 - train: epoch 0035, iter [00950, 01251], lr: 0.000495, loss: 0.4049
2022-10-03 07:07:42 - train: epoch 0035, iter [00960, 01251], lr: 0.000495, loss: 0.4084
2022-10-03 07:08:01 - train: epoch 0035, iter [00970, 01251], lr: 0.000495, loss: 0.4384
2022-10-03 07:08:21 - train: epoch 0035, iter [00980, 01251], lr: 0.000495, loss: 0.4062
2022-10-03 07:08:41 - train: epoch 0035, iter [00990, 01251], lr: 0.000495, loss: 0.4089
2022-10-03 07:09:00 - train: epoch 0035, iter [01000, 01251], lr: 0.000494, loss: 0.4370
2022-10-03 07:09:20 - train: epoch 0035, iter [01010, 01251], lr: 0.000494, loss: 0.4138
2022-10-03 07:09:40 - train: epoch 0035, iter [01020, 01251], lr: 0.000494, loss: 0.4176
2022-10-03 07:10:00 - train: epoch 0035, iter [01030, 01251], lr: 0.000494, loss: 0.4213
2022-10-03 07:10:19 - train: epoch 0035, iter [01040, 01251], lr: 0.000494, loss: 0.4192
2022-10-03 07:10:39 - train: epoch 0035, iter [01050, 01251], lr: 0.000494, loss: 0.4218
2022-10-03 07:10:59 - train: epoch 0035, iter [01060, 01251], lr: 0.000494, loss: 0.4093
2022-10-03 07:11:18 - train: epoch 0035, iter [01070, 01251], lr: 0.000494, loss: 0.4163
2022-10-03 07:11:38 - train: epoch 0035, iter [01080, 01251], lr: 0.000494, loss: 0.4116
2022-10-03 07:11:58 - train: epoch 0035, iter [01090, 01251], lr: 0.000494, loss: 0.4128
2022-10-03 07:12:18 - train: epoch 0035, iter [01100, 01251], lr: 0.000494, loss: 0.4158
2022-10-03 07:12:37 - train: epoch 0035, iter [01110, 01251], lr: 0.000494, loss: 0.4039
2022-10-03 07:12:57 - train: epoch 0035, iter [01120, 01251], lr: 0.000494, loss: 0.4164
2022-10-03 07:13:17 - train: epoch 0035, iter [01130, 01251], lr: 0.000494, loss: 0.4280
2022-10-03 07:13:36 - train: epoch 0035, iter [01140, 01251], lr: 0.000494, loss: 0.4210
2022-10-03 07:13:56 - train: epoch 0035, iter [01150, 01251], lr: 0.000493, loss: 0.4409
2022-10-03 07:14:16 - train: epoch 0035, iter [01160, 01251], lr: 0.000493, loss: 0.4135
2022-10-03 07:14:35 - train: epoch 0035, iter [01170, 01251], lr: 0.000493, loss: 0.4063
2022-10-03 07:14:55 - train: epoch 0035, iter [01180, 01251], lr: 0.000493, loss: 0.4164
2022-10-03 07:15:15 - train: epoch 0035, iter [01190, 01251], lr: 0.000493, loss: 0.4285
2022-10-03 07:15:35 - train: epoch 0035, iter [01200, 01251], lr: 0.000493, loss: 0.4287
2022-10-03 07:15:54 - train: epoch 0035, iter [01210, 01251], lr: 0.000493, loss: 0.4226
2022-10-03 07:16:14 - train: epoch 0035, iter [01220, 01251], lr: 0.000493, loss: 0.4055
2022-10-03 07:16:34 - train: epoch 0035, iter [01230, 01251], lr: 0.000493, loss: 0.4169
2022-10-03 07:16:53 - train: epoch 0035, iter [01240, 01251], lr: 0.000493, loss: 0.4228
2022-10-03 07:17:12 - train: epoch 0035, iter [01250, 01251], lr: 0.000493, loss: 0.4304
2022-10-03 07:17:17 - train: epoch 035, train_loss: 0.4162
2022-10-03 07:17:22 - until epoch: 035, best_loss: 0.4162
2022-10-03 07:17:22 - epoch 036 lr: 0.000493
2022-10-03 07:17:49 - train: epoch 0036, iter [00010, 01251], lr: 0.000493, loss: 0.4239
2022-10-03 07:18:08 - train: epoch 0036, iter [00020, 01251], lr: 0.000493, loss: 0.4236
2022-10-03 07:18:28 - train: epoch 0036, iter [00030, 01251], lr: 0.000493, loss: 0.4097
2022-10-03 07:18:48 - train: epoch 0036, iter [00040, 01251], lr: 0.000493, loss: 0.4413
2022-10-03 07:19:08 - train: epoch 0036, iter [00050, 01251], lr: 0.000493, loss: 0.4061
2022-10-03 07:19:28 - train: epoch 0036, iter [00060, 01251], lr: 0.000492, loss: 0.4211
2022-10-03 07:19:48 - train: epoch 0036, iter [00070, 01251], lr: 0.000492, loss: 0.4026
2022-10-03 07:20:08 - train: epoch 0036, iter [00080, 01251], lr: 0.000492, loss: 0.3876
2022-10-03 07:20:28 - train: epoch 0036, iter [00090, 01251], lr: 0.000492, loss: 0.4112
2022-10-03 07:20:48 - train: epoch 0036, iter [00100, 01251], lr: 0.000492, loss: 0.4052
2022-10-03 07:21:08 - train: epoch 0036, iter [00110, 01251], lr: 0.000492, loss: 0.4126
2022-10-03 07:21:28 - train: epoch 0036, iter [00120, 01251], lr: 0.000492, loss: 0.4071
2022-10-03 07:21:48 - train: epoch 0036, iter [00130, 01251], lr: 0.000492, loss: 0.4154
2022-10-03 07:22:08 - train: epoch 0036, iter [00140, 01251], lr: 0.000492, loss: 0.4149
2022-10-03 07:22:28 - train: epoch 0036, iter [00150, 01251], lr: 0.000492, loss: 0.4335
2022-10-03 07:22:48 - train: epoch 0036, iter [00160, 01251], lr: 0.000492, loss: 0.4388
2022-10-03 07:23:08 - train: epoch 0036, iter [00170, 01251], lr: 0.000492, loss: 0.4199
2022-10-03 07:23:28 - train: epoch 0036, iter [00180, 01251], lr: 0.000492, loss: 0.4269
2022-10-03 07:23:48 - train: epoch 0036, iter [00190, 01251], lr: 0.000492, loss: 0.4193
2022-10-03 07:24:08 - train: epoch 0036, iter [00200, 01251], lr: 0.000492, loss: 0.4314
2022-10-03 07:24:28 - train: epoch 0036, iter [00210, 01251], lr: 0.000491, loss: 0.4189
2022-10-03 07:24:48 - train: epoch 0036, iter [00220, 01251], lr: 0.000491, loss: 0.4242
2022-10-03 07:25:08 - train: epoch 0036, iter [00230, 01251], lr: 0.000491, loss: 0.4191
2022-10-03 07:25:27 - train: epoch 0036, iter [00240, 01251], lr: 0.000491, loss: 0.4266
2022-10-03 07:25:47 - train: epoch 0036, iter [00250, 01251], lr: 0.000491, loss: 0.3980
2022-10-03 07:26:07 - train: epoch 0036, iter [00260, 01251], lr: 0.000491, loss: 0.4252
2022-10-03 07:26:27 - train: epoch 0036, iter [00270, 01251], lr: 0.000491, loss: 0.4164
2022-10-03 07:26:47 - train: epoch 0036, iter [00280, 01251], lr: 0.000491, loss: 0.4100
2022-10-03 07:27:07 - train: epoch 0036, iter [00290, 01251], lr: 0.000491, loss: 0.4197
2022-10-03 07:27:27 - train: epoch 0036, iter [00300, 01251], lr: 0.000491, loss: 0.4195
2022-10-03 07:27:47 - train: epoch 0036, iter [00310, 01251], lr: 0.000491, loss: 0.4216
2022-10-03 07:28:07 - train: epoch 0036, iter [00320, 01251], lr: 0.000491, loss: 0.3976
2022-10-03 07:28:26 - train: epoch 0036, iter [00330, 01251], lr: 0.000491, loss: 0.4044
2022-10-03 07:28:46 - train: epoch 0036, iter [00340, 01251], lr: 0.000491, loss: 0.4242
2022-10-03 07:29:06 - train: epoch 0036, iter [00350, 01251], lr: 0.000491, loss: 0.4035
2022-10-03 07:29:26 - train: epoch 0036, iter [00360, 01251], lr: 0.000491, loss: 0.4324
2022-10-03 07:29:46 - train: epoch 0036, iter [00370, 01251], lr: 0.000490, loss: 0.4179
2022-10-03 07:30:06 - train: epoch 0036, iter [00380, 01251], lr: 0.000490, loss: 0.4058
2022-10-03 07:30:26 - train: epoch 0036, iter [00390, 01251], lr: 0.000490, loss: 0.4328
2022-10-03 07:30:46 - train: epoch 0036, iter [00400, 01251], lr: 0.000490, loss: 0.4171
2022-10-03 07:31:06 - train: epoch 0036, iter [00410, 01251], lr: 0.000490, loss: 0.4221
2022-10-03 07:31:26 - train: epoch 0036, iter [00420, 01251], lr: 0.000490, loss: 0.4218
2022-10-03 07:31:45 - train: epoch 0036, iter [00430, 01251], lr: 0.000490, loss: 0.3980
2022-10-03 07:32:05 - train: epoch 0036, iter [00440, 01251], lr: 0.000490, loss: 0.4287
2022-10-03 07:32:25 - train: epoch 0036, iter [00450, 01251], lr: 0.000490, loss: 0.4181
2022-10-03 07:32:45 - train: epoch 0036, iter [00460, 01251], lr: 0.000490, loss: 0.4124
2022-10-03 07:33:05 - train: epoch 0036, iter [00470, 01251], lr: 0.000490, loss: 0.4060
2022-10-03 07:33:24 - train: epoch 0036, iter [00480, 01251], lr: 0.000490, loss: 0.4068
2022-10-03 07:33:44 - train: epoch 0036, iter [00490, 01251], lr: 0.000490, loss: 0.4334
2022-10-03 07:34:04 - train: epoch 0036, iter [00500, 01251], lr: 0.000490, loss: 0.4264
2022-10-03 07:34:24 - train: epoch 0036, iter [00510, 01251], lr: 0.000490, loss: 0.4164
2022-10-03 07:34:44 - train: epoch 0036, iter [00520, 01251], lr: 0.000489, loss: 0.4180
2022-10-03 07:35:04 - train: epoch 0036, iter [00530, 01251], lr: 0.000489, loss: 0.4153
2022-10-03 07:35:23 - train: epoch 0036, iter [00540, 01251], lr: 0.000489, loss: 0.4109
2022-10-03 07:35:44 - train: epoch 0036, iter [00550, 01251], lr: 0.000489, loss: 0.3953
2022-10-03 07:36:04 - train: epoch 0036, iter [00560, 01251], lr: 0.000489, loss: 0.4219
2022-10-03 07:36:23 - train: epoch 0036, iter [00570, 01251], lr: 0.000489, loss: 0.4273
2022-10-03 07:36:43 - train: epoch 0036, iter [00580, 01251], lr: 0.000489, loss: 0.4310
2022-10-03 07:37:03 - train: epoch 0036, iter [00590, 01251], lr: 0.000489, loss: 0.4217
2022-10-03 07:37:23 - train: epoch 0036, iter [00600, 01251], lr: 0.000489, loss: 0.4190
2022-10-03 07:37:43 - train: epoch 0036, iter [00610, 01251], lr: 0.000489, loss: 0.4026
2022-10-03 07:38:03 - train: epoch 0036, iter [00620, 01251], lr: 0.000489, loss: 0.4053
2022-10-03 07:38:23 - train: epoch 0036, iter [00630, 01251], lr: 0.000489, loss: 0.4051
2022-10-03 07:38:42 - train: epoch 0036, iter [00640, 01251], lr: 0.000489, loss: 0.4145
2022-10-03 07:39:02 - train: epoch 0036, iter [00650, 01251], lr: 0.000489, loss: 0.4041
2022-10-03 07:39:22 - train: epoch 0036, iter [00660, 01251], lr: 0.000489, loss: 0.4195
2022-10-03 07:39:42 - train: epoch 0036, iter [00670, 01251], lr: 0.000489, loss: 0.4179
2022-10-03 07:40:01 - train: epoch 0036, iter [00680, 01251], lr: 0.000488, loss: 0.4257
2022-10-03 07:40:21 - train: epoch 0036, iter [00690, 01251], lr: 0.000488, loss: 0.4397
2022-10-03 07:40:41 - train: epoch 0036, iter [00700, 01251], lr: 0.000488, loss: 0.4068
2022-10-03 07:41:01 - train: epoch 0036, iter [00710, 01251], lr: 0.000488, loss: 0.4071
2022-10-03 07:41:21 - train: epoch 0036, iter [00720, 01251], lr: 0.000488, loss: 0.4069
2022-10-03 07:41:41 - train: epoch 0036, iter [00730, 01251], lr: 0.000488, loss: 0.4164
2022-10-03 07:42:00 - train: epoch 0036, iter [00740, 01251], lr: 0.000488, loss: 0.4278
2022-10-03 07:42:20 - train: epoch 0036, iter [00750, 01251], lr: 0.000488, loss: 0.4384
2022-10-03 07:42:40 - train: epoch 0036, iter [00760, 01251], lr: 0.000488, loss: 0.4053
2022-10-03 07:43:00 - train: epoch 0036, iter [00770, 01251], lr: 0.000488, loss: 0.4107
2022-10-03 07:43:20 - train: epoch 0036, iter [00780, 01251], lr: 0.000488, loss: 0.4102
2022-10-03 07:43:40 - train: epoch 0036, iter [00790, 01251], lr: 0.000488, loss: 0.4018
2022-10-03 07:43:59 - train: epoch 0036, iter [00800, 01251], lr: 0.000488, loss: 0.3960
2022-10-03 07:44:19 - train: epoch 0036, iter [00810, 01251], lr: 0.000488, loss: 0.4193
2022-10-03 07:44:39 - train: epoch 0036, iter [00820, 01251], lr: 0.000488, loss: 0.4068
2022-10-03 07:44:59 - train: epoch 0036, iter [00830, 01251], lr: 0.000487, loss: 0.3978
2022-10-03 07:45:19 - train: epoch 0036, iter [00840, 01251], lr: 0.000487, loss: 0.4270
2022-10-03 07:45:39 - train: epoch 0036, iter [00850, 01251], lr: 0.000487, loss: 0.4290
2022-10-03 07:45:59 - train: epoch 0036, iter [00860, 01251], lr: 0.000487, loss: 0.4110
2022-10-03 07:46:19 - train: epoch 0036, iter [00870, 01251], lr: 0.000487, loss: 0.4275
2022-10-03 07:46:38 - train: epoch 0036, iter [00880, 01251], lr: 0.000487, loss: 0.4305
2022-10-03 07:46:58 - train: epoch 0036, iter [00890, 01251], lr: 0.000487, loss: 0.4152
2022-10-03 07:47:18 - train: epoch 0036, iter [00900, 01251], lr: 0.000487, loss: 0.4191
2022-10-03 07:47:38 - train: epoch 0036, iter [00910, 01251], lr: 0.000487, loss: 0.4274
2022-10-03 07:47:58 - train: epoch 0036, iter [00920, 01251], lr: 0.000487, loss: 0.3870
2022-10-03 07:48:18 - train: epoch 0036, iter [00930, 01251], lr: 0.000487, loss: 0.4232
2022-10-03 07:48:38 - train: epoch 0036, iter [00940, 01251], lr: 0.000487, loss: 0.4148
2022-10-03 07:48:58 - train: epoch 0036, iter [00950, 01251], lr: 0.000487, loss: 0.4319
2022-10-03 07:49:17 - train: epoch 0036, iter [00960, 01251], lr: 0.000487, loss: 0.4097
2022-10-03 07:49:37 - train: epoch 0036, iter [00970, 01251], lr: 0.000487, loss: 0.3994
2022-10-03 07:49:57 - train: epoch 0036, iter [00980, 01251], lr: 0.000486, loss: 0.4138
2022-10-03 07:50:17 - train: epoch 0036, iter [00990, 01251], lr: 0.000486, loss: 0.4208
2022-10-03 07:50:37 - train: epoch 0036, iter [01000, 01251], lr: 0.000486, loss: 0.4011
2022-10-03 07:50:57 - train: epoch 0036, iter [01010, 01251], lr: 0.000486, loss: 0.4212
2022-10-03 07:51:17 - train: epoch 0036, iter [01020, 01251], lr: 0.000486, loss: 0.3978
2022-10-03 07:51:36 - train: epoch 0036, iter [01030, 01251], lr: 0.000486, loss: 0.4034
2022-10-03 07:51:56 - train: epoch 0036, iter [01040, 01251], lr: 0.000486, loss: 0.4259
2022-10-03 07:52:16 - train: epoch 0036, iter [01050, 01251], lr: 0.000486, loss: 0.4028
2022-10-03 07:52:36 - train: epoch 0036, iter [01060, 01251], lr: 0.000486, loss: 0.4042
2022-10-03 07:52:56 - train: epoch 0036, iter [01070, 01251], lr: 0.000486, loss: 0.4235
2022-10-03 07:53:15 - train: epoch 0036, iter [01080, 01251], lr: 0.000486, loss: 0.4176
2022-10-03 07:53:35 - train: epoch 0036, iter [01090, 01251], lr: 0.000486, loss: 0.4232
2022-10-03 07:53:55 - train: epoch 0036, iter [01100, 01251], lr: 0.000486, loss: 0.4049
2022-10-03 07:54:14 - train: epoch 0036, iter [01110, 01251], lr: 0.000486, loss: 0.4162
2022-10-03 07:54:34 - train: epoch 0036, iter [01120, 01251], lr: 0.000486, loss: 0.4194
2022-10-03 07:54:54 - train: epoch 0036, iter [01130, 01251], lr: 0.000485, loss: 0.3862
2022-10-03 07:55:14 - train: epoch 0036, iter [01140, 01251], lr: 0.000485, loss: 0.4212
2022-10-03 07:55:34 - train: epoch 0036, iter [01150, 01251], lr: 0.000485, loss: 0.4276
2022-10-03 07:55:54 - train: epoch 0036, iter [01160, 01251], lr: 0.000485, loss: 0.3860
2022-10-03 07:56:14 - train: epoch 0036, iter [01170, 01251], lr: 0.000485, loss: 0.4307
2022-10-03 07:56:33 - train: epoch 0036, iter [01180, 01251], lr: 0.000485, loss: 0.4113
2022-10-03 07:56:53 - train: epoch 0036, iter [01190, 01251], lr: 0.000485, loss: 0.4112
2022-10-03 07:57:13 - train: epoch 0036, iter [01200, 01251], lr: 0.000485, loss: 0.4161
2022-10-03 07:57:33 - train: epoch 0036, iter [01210, 01251], lr: 0.000485, loss: 0.4077
2022-10-03 07:57:52 - train: epoch 0036, iter [01220, 01251], lr: 0.000485, loss: 0.3976
2022-10-03 07:58:12 - train: epoch 0036, iter [01230, 01251], lr: 0.000485, loss: 0.4249
2022-10-03 07:58:32 - train: epoch 0036, iter [01240, 01251], lr: 0.000485, loss: 0.4291
2022-10-03 07:58:51 - train: epoch 0036, iter [01250, 01251], lr: 0.000485, loss: 0.4111
2022-10-03 07:58:57 - train: epoch 036, train_loss: 0.4157
2022-10-03 07:59:01 - until epoch: 036, best_loss: 0.4157
2022-10-03 07:59:01 - epoch 037 lr: 0.000485
2022-10-03 07:59:28 - train: epoch 0037, iter [00010, 01251], lr: 0.000485, loss: 0.4322
2022-10-03 07:59:48 - train: epoch 0037, iter [00020, 01251], lr: 0.000485, loss: 0.4080
2022-10-03 08:00:08 - train: epoch 0037, iter [00030, 01251], lr: 0.000485, loss: 0.4198
2022-10-03 08:00:28 - train: epoch 0037, iter [00040, 01251], lr: 0.000484, loss: 0.4271
2022-10-03 08:00:48 - train: epoch 0037, iter [00050, 01251], lr: 0.000484, loss: 0.4174
2022-10-03 08:01:08 - train: epoch 0037, iter [00060, 01251], lr: 0.000484, loss: 0.4054
2022-10-03 08:01:28 - train: epoch 0037, iter [00070, 01251], lr: 0.000484, loss: 0.4070
2022-10-03 08:01:48 - train: epoch 0037, iter [00080, 01251], lr: 0.000484, loss: 0.4364
2022-10-03 08:02:08 - train: epoch 0037, iter [00090, 01251], lr: 0.000484, loss: 0.4171
2022-10-03 08:02:28 - train: epoch 0037, iter [00100, 01251], lr: 0.000484, loss: 0.4112
2022-10-03 08:02:48 - train: epoch 0037, iter [00110, 01251], lr: 0.000484, loss: 0.4196
2022-10-03 08:03:08 - train: epoch 0037, iter [00120, 01251], lr: 0.000484, loss: 0.4264
2022-10-03 08:03:28 - train: epoch 0037, iter [00130, 01251], lr: 0.000484, loss: 0.4214
2022-10-03 08:03:48 - train: epoch 0037, iter [00140, 01251], lr: 0.000484, loss: 0.4283
2022-10-03 08:04:08 - train: epoch 0037, iter [00150, 01251], lr: 0.000484, loss: 0.4278
2022-10-03 08:04:28 - train: epoch 0037, iter [00160, 01251], lr: 0.000484, loss: 0.4356
2022-10-03 08:04:48 - train: epoch 0037, iter [00170, 01251], lr: 0.000484, loss: 0.4229
2022-10-03 08:05:08 - train: epoch 0037, iter [00180, 01251], lr: 0.000484, loss: 0.4079
2022-10-03 08:05:28 - train: epoch 0037, iter [00190, 01251], lr: 0.000483, loss: 0.4180
2022-10-03 08:05:48 - train: epoch 0037, iter [00200, 01251], lr: 0.000483, loss: 0.4267
2022-10-03 08:06:08 - train: epoch 0037, iter [00210, 01251], lr: 0.000483, loss: 0.4142
2022-10-03 08:06:28 - train: epoch 0037, iter [00220, 01251], lr: 0.000483, loss: 0.4256
2022-10-03 08:06:49 - train: epoch 0037, iter [00230, 01251], lr: 0.000483, loss: 0.4029
2022-10-03 08:07:09 - train: epoch 0037, iter [00240, 01251], lr: 0.000483, loss: 0.3893
2022-10-03 08:07:29 - train: epoch 0037, iter [00250, 01251], lr: 0.000483, loss: 0.4303
2022-10-03 08:07:49 - train: epoch 0037, iter [00260, 01251], lr: 0.000483, loss: 0.4128
2022-10-03 08:08:09 - train: epoch 0037, iter [00270, 01251], lr: 0.000483, loss: 0.4088
2022-10-03 08:08:29 - train: epoch 0037, iter [00280, 01251], lr: 0.000483, loss: 0.4426
2022-10-03 08:08:49 - train: epoch 0037, iter [00290, 01251], lr: 0.000483, loss: 0.4022
2022-10-03 08:09:09 - train: epoch 0037, iter [00300, 01251], lr: 0.000483, loss: 0.4163
2022-10-03 08:09:29 - train: epoch 0037, iter [00310, 01251], lr: 0.000483, loss: 0.4028
2022-10-03 08:09:49 - train: epoch 0037, iter [00320, 01251], lr: 0.000483, loss: 0.4330
2022-10-03 08:10:09 - train: epoch 0037, iter [00330, 01251], lr: 0.000483, loss: 0.4132
2022-10-03 08:10:29 - train: epoch 0037, iter [00340, 01251], lr: 0.000482, loss: 0.4167
2022-10-03 08:10:49 - train: epoch 0037, iter [00350, 01251], lr: 0.000482, loss: 0.4195
2022-10-03 08:11:09 - train: epoch 0037, iter [00360, 01251], lr: 0.000482, loss: 0.4193
2022-10-03 08:11:28 - train: epoch 0037, iter [00370, 01251], lr: 0.000482, loss: 0.4104
2022-10-03 08:11:48 - train: epoch 0037, iter [00380, 01251], lr: 0.000482, loss: 0.4258
2022-10-03 08:12:08 - train: epoch 0037, iter [00390, 01251], lr: 0.000482, loss: 0.3889
2022-10-03 08:12:28 - train: epoch 0037, iter [00400, 01251], lr: 0.000482, loss: 0.4245
2022-10-03 08:12:48 - train: epoch 0037, iter [00410, 01251], lr: 0.000482, loss: 0.4181
2022-10-03 08:13:08 - train: epoch 0037, iter [00420, 01251], lr: 0.000482, loss: 0.4177
2022-10-03 08:13:28 - train: epoch 0037, iter [00430, 01251], lr: 0.000482, loss: 0.4163
2022-10-03 08:13:48 - train: epoch 0037, iter [00440, 01251], lr: 0.000482, loss: 0.4163
2022-10-03 08:14:08 - train: epoch 0037, iter [00450, 01251], lr: 0.000482, loss: 0.4016
2022-10-03 08:14:27 - train: epoch 0037, iter [00460, 01251], lr: 0.000482, loss: 0.4118
2022-10-03 08:14:47 - train: epoch 0037, iter [00470, 01251], lr: 0.000482, loss: 0.4237
2022-10-03 08:15:07 - train: epoch 0037, iter [00480, 01251], lr: 0.000482, loss: 0.4118
2022-10-03 08:15:27 - train: epoch 0037, iter [00490, 01251], lr: 0.000481, loss: 0.4172
2022-10-03 08:15:47 - train: epoch 0037, iter [00500, 01251], lr: 0.000481, loss: 0.4166
2022-10-03 08:16:07 - train: epoch 0037, iter [00510, 01251], lr: 0.000481, loss: 0.4269
2022-10-03 08:16:27 - train: epoch 0037, iter [00520, 01251], lr: 0.000481, loss: 0.4305
2022-10-03 08:16:47 - train: epoch 0037, iter [00530, 01251], lr: 0.000481, loss: 0.4192
2022-10-03 08:17:07 - train: epoch 0037, iter [00540, 01251], lr: 0.000481, loss: 0.4047
2022-10-03 08:17:26 - train: epoch 0037, iter [00550, 01251], lr: 0.000481, loss: 0.4147
2022-10-03 08:17:46 - train: epoch 0037, iter [00560, 01251], lr: 0.000481, loss: 0.3996
2022-10-03 08:18:06 - train: epoch 0037, iter [00570, 01251], lr: 0.000481, loss: 0.4189
2022-10-03 08:18:26 - train: epoch 0037, iter [00580, 01251], lr: 0.000481, loss: 0.4122
2022-10-03 08:18:45 - train: epoch 0037, iter [00590, 01251], lr: 0.000481, loss: 0.4209
2022-10-03 08:19:05 - train: epoch 0037, iter [00600, 01251], lr: 0.000481, loss: 0.4099
2022-10-03 08:19:25 - train: epoch 0037, iter [00610, 01251], lr: 0.000481, loss: 0.3968
2022-10-03 08:19:45 - train: epoch 0037, iter [00620, 01251], lr: 0.000481, loss: 0.4124
2022-10-03 08:20:05 - train: epoch 0037, iter [00630, 01251], lr: 0.000481, loss: 0.4059
2022-10-03 08:20:25 - train: epoch 0037, iter [00640, 01251], lr: 0.000480, loss: 0.3971
2022-10-03 08:20:45 - train: epoch 0037, iter [00650, 01251], lr: 0.000480, loss: 0.4233
2022-10-03 08:21:05 - train: epoch 0037, iter [00660, 01251], lr: 0.000480, loss: 0.4058
2022-10-03 08:21:25 - train: epoch 0037, iter [00670, 01251], lr: 0.000480, loss: 0.4051
2022-10-03 08:21:44 - train: epoch 0037, iter [00680, 01251], lr: 0.000480, loss: 0.4055
2022-10-03 08:22:04 - train: epoch 0037, iter [00690, 01251], lr: 0.000480, loss: 0.4321
2022-10-03 08:22:24 - train: epoch 0037, iter [00700, 01251], lr: 0.000480, loss: 0.4134
2022-10-03 08:22:44 - train: epoch 0037, iter [00710, 01251], lr: 0.000480, loss: 0.4099
2022-10-03 08:23:04 - train: epoch 0037, iter [00720, 01251], lr: 0.000480, loss: 0.4111
2022-10-03 08:23:23 - train: epoch 0037, iter [00730, 01251], lr: 0.000480, loss: 0.4324
2022-10-03 08:23:43 - train: epoch 0037, iter [00740, 01251], lr: 0.000480, loss: 0.4145
2022-10-03 08:24:03 - train: epoch 0037, iter [00750, 01251], lr: 0.000480, loss: 0.4048
2022-10-03 08:24:23 - train: epoch 0037, iter [00760, 01251], lr: 0.000480, loss: 0.4217
2022-10-03 08:24:43 - train: epoch 0037, iter [00770, 01251], lr: 0.000480, loss: 0.4151
2022-10-03 08:25:03 - train: epoch 0037, iter [00780, 01251], lr: 0.000480, loss: 0.4298
2022-10-03 08:25:23 - train: epoch 0037, iter [00790, 01251], lr: 0.000479, loss: 0.4093
2022-10-03 08:25:43 - train: epoch 0037, iter [00800, 01251], lr: 0.000479, loss: 0.4065
2022-10-03 08:26:02 - train: epoch 0037, iter [00810, 01251], lr: 0.000479, loss: 0.4075
2022-10-03 08:26:23 - train: epoch 0037, iter [00820, 01251], lr: 0.000479, loss: 0.4047
2022-10-03 08:26:42 - train: epoch 0037, iter [00830, 01251], lr: 0.000479, loss: 0.4354
2022-10-03 08:27:02 - train: epoch 0037, iter [00840, 01251], lr: 0.000479, loss: 0.4116
2022-10-03 08:27:22 - train: epoch 0037, iter [00850, 01251], lr: 0.000479, loss: 0.4011
2022-10-03 08:27:42 - train: epoch 0037, iter [00860, 01251], lr: 0.000479, loss: 0.4016
2022-10-03 08:28:02 - train: epoch 0037, iter [00870, 01251], lr: 0.000479, loss: 0.4206
2022-10-03 08:28:22 - train: epoch 0037, iter [00880, 01251], lr: 0.000479, loss: 0.4145
2022-10-03 08:28:42 - train: epoch 0037, iter [00890, 01251], lr: 0.000479, loss: 0.3963
2022-10-03 08:29:02 - train: epoch 0037, iter [00900, 01251], lr: 0.000479, loss: 0.4100
2022-10-03 08:29:22 - train: epoch 0037, iter [00910, 01251], lr: 0.000479, loss: 0.4221
2022-10-03 08:29:41 - train: epoch 0037, iter [00920, 01251], lr: 0.000479, loss: 0.3922
2022-10-03 08:30:01 - train: epoch 0037, iter [00930, 01251], lr: 0.000479, loss: 0.3960
2022-10-03 08:30:21 - train: epoch 0037, iter [00940, 01251], lr: 0.000478, loss: 0.4186
2022-10-03 08:30:41 - train: epoch 0037, iter [00950, 01251], lr: 0.000478, loss: 0.4285
2022-10-03 08:31:01 - train: epoch 0037, iter [00960, 01251], lr: 0.000478, loss: 0.4100
2022-10-03 08:31:20 - train: epoch 0037, iter [00970, 01251], lr: 0.000478, loss: 0.4087
2022-10-03 08:31:40 - train: epoch 0037, iter [00980, 01251], lr: 0.000478, loss: 0.4113
2022-10-03 08:32:00 - train: epoch 0037, iter [00990, 01251], lr: 0.000478, loss: 0.4004
2022-10-03 08:32:20 - train: epoch 0037, iter [01000, 01251], lr: 0.000478, loss: 0.4153
2022-10-03 08:32:39 - train: epoch 0037, iter [01010, 01251], lr: 0.000478, loss: 0.4204
2022-10-03 08:32:59 - train: epoch 0037, iter [01020, 01251], lr: 0.000478, loss: 0.4042
2022-10-03 08:33:19 - train: epoch 0037, iter [01030, 01251], lr: 0.000478, loss: 0.4225
2022-10-03 08:33:39 - train: epoch 0037, iter [01040, 01251], lr: 0.000478, loss: 0.3960
2022-10-03 08:33:59 - train: epoch 0037, iter [01050, 01251], lr: 0.000478, loss: 0.3898
2022-10-03 08:34:18 - train: epoch 0037, iter [01060, 01251], lr: 0.000478, loss: 0.4355
2022-10-03 08:34:38 - train: epoch 0037, iter [01070, 01251], lr: 0.000478, loss: 0.3946
2022-10-03 08:34:58 - train: epoch 0037, iter [01080, 01251], lr: 0.000477, loss: 0.4172
2022-10-03 08:35:18 - train: epoch 0037, iter [01090, 01251], lr: 0.000477, loss: 0.4159
2022-10-03 08:35:38 - train: epoch 0037, iter [01100, 01251], lr: 0.000477, loss: 0.4221
2022-10-03 08:35:57 - train: epoch 0037, iter [01110, 01251], lr: 0.000477, loss: 0.4329
2022-10-03 08:36:17 - train: epoch 0037, iter [01120, 01251], lr: 0.000477, loss: 0.4229
2022-10-03 08:36:37 - train: epoch 0037, iter [01130, 01251], lr: 0.000477, loss: 0.4214
2022-10-03 08:36:56 - train: epoch 0037, iter [01140, 01251], lr: 0.000477, loss: 0.4117
2022-10-03 08:37:16 - train: epoch 0037, iter [01150, 01251], lr: 0.000477, loss: 0.4179
2022-10-03 08:37:36 - train: epoch 0037, iter [01160, 01251], lr: 0.000477, loss: 0.4112
2022-10-03 08:37:56 - train: epoch 0037, iter [01170, 01251], lr: 0.000477, loss: 0.4073
2022-10-03 08:38:16 - train: epoch 0037, iter [01180, 01251], lr: 0.000477, loss: 0.4201
2022-10-03 08:38:36 - train: epoch 0037, iter [01190, 01251], lr: 0.000477, loss: 0.4171
2022-10-03 08:38:55 - train: epoch 0037, iter [01200, 01251], lr: 0.000477, loss: 0.4185
2022-10-03 08:39:15 - train: epoch 0037, iter [01210, 01251], lr: 0.000477, loss: 0.4027
2022-10-03 08:39:35 - train: epoch 0037, iter [01220, 01251], lr: 0.000477, loss: 0.4339
2022-10-03 08:39:55 - train: epoch 0037, iter [01230, 01251], lr: 0.000476, loss: 0.4248
2022-10-03 08:40:14 - train: epoch 0037, iter [01240, 01251], lr: 0.000476, loss: 0.3861
2022-10-03 08:40:33 - train: epoch 0037, iter [01250, 01251], lr: 0.000476, loss: 0.4068
2022-10-03 08:40:40 - train: epoch 037, train_loss: 0.4154
2022-10-03 08:40:45 - until epoch: 037, best_loss: 0.4154
2022-10-03 08:40:45 - epoch 038 lr: 0.000476
2022-10-03 08:41:11 - train: epoch 0038, iter [00010, 01251], lr: 0.000476, loss: 0.4157
2022-10-03 08:41:30 - train: epoch 0038, iter [00020, 01251], lr: 0.000476, loss: 0.4043
2022-10-03 08:41:50 - train: epoch 0038, iter [00030, 01251], lr: 0.000476, loss: 0.3992
2022-10-03 08:42:10 - train: epoch 0038, iter [00040, 01251], lr: 0.000476, loss: 0.4082
2022-10-03 08:42:30 - train: epoch 0038, iter [00050, 01251], lr: 0.000476, loss: 0.4324
2022-10-03 08:42:49 - train: epoch 0038, iter [00060, 01251], lr: 0.000476, loss: 0.4125
2022-10-03 08:43:09 - train: epoch 0038, iter [00070, 01251], lr: 0.000476, loss: 0.4414
2022-10-03 08:43:29 - train: epoch 0038, iter [00080, 01251], lr: 0.000476, loss: 0.4292
2022-10-03 08:43:49 - train: epoch 0038, iter [00090, 01251], lr: 0.000476, loss: 0.4269
2022-10-03 08:44:09 - train: epoch 0038, iter [00100, 01251], lr: 0.000476, loss: 0.4334
2022-10-03 08:44:29 - train: epoch 0038, iter [00110, 01251], lr: 0.000476, loss: 0.4219
2022-10-03 08:44:49 - train: epoch 0038, iter [00120, 01251], lr: 0.000476, loss: 0.4232
2022-10-03 08:45:09 - train: epoch 0038, iter [00130, 01251], lr: 0.000475, loss: 0.4150
2022-10-03 08:45:29 - train: epoch 0038, iter [00140, 01251], lr: 0.000475, loss: 0.4094
2022-10-03 08:45:49 - train: epoch 0038, iter [00150, 01251], lr: 0.000475, loss: 0.3963
2022-10-03 08:46:09 - train: epoch 0038, iter [00160, 01251], lr: 0.000475, loss: 0.4022
2022-10-03 08:46:29 - train: epoch 0038, iter [00170, 01251], lr: 0.000475, loss: 0.4141
2022-10-03 08:46:49 - train: epoch 0038, iter [00180, 01251], lr: 0.000475, loss: 0.4213
2022-10-03 08:47:09 - train: epoch 0038, iter [00190, 01251], lr: 0.000475, loss: 0.4054
2022-10-03 08:47:29 - train: epoch 0038, iter [00200, 01251], lr: 0.000475, loss: 0.4057
2022-10-03 08:47:48 - train: epoch 0038, iter [00210, 01251], lr: 0.000475, loss: 0.4106
2022-10-03 08:48:09 - train: epoch 0038, iter [00220, 01251], lr: 0.000475, loss: 0.4093
2022-10-03 08:48:28 - train: epoch 0038, iter [00230, 01251], lr: 0.000475, loss: 0.4172
2022-10-03 08:48:49 - train: epoch 0038, iter [00240, 01251], lr: 0.000475, loss: 0.4234
2022-10-03 08:49:08 - train: epoch 0038, iter [00250, 01251], lr: 0.000475, loss: 0.4223
2022-10-03 08:49:28 - train: epoch 0038, iter [00260, 01251], lr: 0.000475, loss: 0.4126
2022-10-03 08:49:48 - train: epoch 0038, iter [00270, 01251], lr: 0.000475, loss: 0.4220
2022-10-03 08:50:08 - train: epoch 0038, iter [00280, 01251], lr: 0.000474, loss: 0.4082
2022-10-03 08:50:28 - train: epoch 0038, iter [00290, 01251], lr: 0.000474, loss: 0.4081
2022-10-03 08:50:48 - train: epoch 0038, iter [00300, 01251], lr: 0.000474, loss: 0.4282
2022-10-03 08:51:08 - train: epoch 0038, iter [00310, 01251], lr: 0.000474, loss: 0.4364
2022-10-03 08:51:28 - train: epoch 0038, iter [00320, 01251], lr: 0.000474, loss: 0.4092
2022-10-03 08:51:48 - train: epoch 0038, iter [00330, 01251], lr: 0.000474, loss: 0.4209
2022-10-03 08:52:08 - train: epoch 0038, iter [00340, 01251], lr: 0.000474, loss: 0.4264
2022-10-03 08:52:28 - train: epoch 0038, iter [00350, 01251], lr: 0.000474, loss: 0.4245
2022-10-03 08:52:48 - train: epoch 0038, iter [00360, 01251], lr: 0.000474, loss: 0.4180
2022-10-03 08:53:08 - train: epoch 0038, iter [00370, 01251], lr: 0.000474, loss: 0.4037
2022-10-03 08:53:28 - train: epoch 0038, iter [00380, 01251], lr: 0.000474, loss: 0.4115
2022-10-03 08:53:48 - train: epoch 0038, iter [00390, 01251], lr: 0.000474, loss: 0.3884
2022-10-03 08:54:08 - train: epoch 0038, iter [00400, 01251], lr: 0.000474, loss: 0.4066
2022-10-03 08:54:28 - train: epoch 0038, iter [00410, 01251], lr: 0.000474, loss: 0.4261
2022-10-03 08:54:48 - train: epoch 0038, iter [00420, 01251], lr: 0.000473, loss: 0.4158
2022-10-03 08:55:08 - train: epoch 0038, iter [00430, 01251], lr: 0.000473, loss: 0.4039
2022-10-03 08:55:28 - train: epoch 0038, iter [00440, 01251], lr: 0.000473, loss: 0.4178
2022-10-03 08:55:48 - train: epoch 0038, iter [00450, 01251], lr: 0.000473, loss: 0.4230
2022-10-03 08:56:08 - train: epoch 0038, iter [00460, 01251], lr: 0.000473, loss: 0.4117
2022-10-03 08:56:28 - train: epoch 0038, iter [00470, 01251], lr: 0.000473, loss: 0.4099
2022-10-03 08:56:48 - train: epoch 0038, iter [00480, 01251], lr: 0.000473, loss: 0.4003
2022-10-03 08:57:08 - train: epoch 0038, iter [00490, 01251], lr: 0.000473, loss: 0.4169
2022-10-03 08:57:28 - train: epoch 0038, iter [00500, 01251], lr: 0.000473, loss: 0.4034
2022-10-03 08:57:48 - train: epoch 0038, iter [00510, 01251], lr: 0.000473, loss: 0.4249
2022-10-03 08:58:08 - train: epoch 0038, iter [00520, 01251], lr: 0.000473, loss: 0.4148
2022-10-03 08:58:28 - train: epoch 0038, iter [00530, 01251], lr: 0.000473, loss: 0.4250
2022-10-03 08:58:47 - train: epoch 0038, iter [00540, 01251], lr: 0.000473, loss: 0.4258
2022-10-03 08:59:08 - train: epoch 0038, iter [00550, 01251], lr: 0.000473, loss: 0.4142
2022-10-03 08:59:28 - train: epoch 0038, iter [00560, 01251], lr: 0.000473, loss: 0.4156
2022-10-03 08:59:47 - train: epoch 0038, iter [00570, 01251], lr: 0.000472, loss: 0.4145
2022-10-03 09:00:07 - train: epoch 0038, iter [00580, 01251], lr: 0.000472, loss: 0.4120
2022-10-03 09:00:27 - train: epoch 0038, iter [00590, 01251], lr: 0.000472, loss: 0.4059
2022-10-03 09:00:47 - train: epoch 0038, iter [00600, 01251], lr: 0.000472, loss: 0.4243
2022-10-03 09:01:06 - train: epoch 0038, iter [00610, 01251], lr: 0.000472, loss: 0.3997
2022-10-03 09:01:26 - train: epoch 0038, iter [00620, 01251], lr: 0.000472, loss: 0.4351
2022-10-03 09:01:46 - train: epoch 0038, iter [00630, 01251], lr: 0.000472, loss: 0.4206
2022-10-03 09:02:05 - train: epoch 0038, iter [00640, 01251], lr: 0.000472, loss: 0.4218
2022-10-03 09:02:25 - train: epoch 0038, iter [00650, 01251], lr: 0.000472, loss: 0.4295
2022-10-03 09:02:45 - train: epoch 0038, iter [00660, 01251], lr: 0.000472, loss: 0.4177
2022-10-03 09:03:05 - train: epoch 0038, iter [00670, 01251], lr: 0.000472, loss: 0.4117
2022-10-03 09:03:25 - train: epoch 0038, iter [00680, 01251], lr: 0.000472, loss: 0.4088
2022-10-03 09:03:44 - train: epoch 0038, iter [00690, 01251], lr: 0.000472, loss: 0.4270
2022-10-03 09:04:04 - train: epoch 0038, iter [00700, 01251], lr: 0.000472, loss: 0.4106
2022-10-03 09:04:24 - train: epoch 0038, iter [00710, 01251], lr: 0.000471, loss: 0.4113
2022-10-03 09:04:44 - train: epoch 0038, iter [00720, 01251], lr: 0.000471, loss: 0.4291
2022-10-03 09:05:03 - train: epoch 0038, iter [00730, 01251], lr: 0.000471, loss: 0.4169
2022-10-03 09:05:23 - train: epoch 0038, iter [00740, 01251], lr: 0.000471, loss: 0.4108
2022-10-03 09:05:43 - train: epoch 0038, iter [00750, 01251], lr: 0.000471, loss: 0.4120
2022-10-03 09:06:02 - train: epoch 0038, iter [00760, 01251], lr: 0.000471, loss: 0.4286
2022-10-03 09:06:22 - train: epoch 0038, iter [00770, 01251], lr: 0.000471, loss: 0.4052
2022-10-03 09:06:42 - train: epoch 0038, iter [00780, 01251], lr: 0.000471, loss: 0.4226
2022-10-03 09:07:02 - train: epoch 0038, iter [00790, 01251], lr: 0.000471, loss: 0.4329
2022-10-03 09:07:21 - train: epoch 0038, iter [00800, 01251], lr: 0.000471, loss: 0.4199
2022-10-03 09:07:41 - train: epoch 0038, iter [00810, 01251], lr: 0.000471, loss: 0.4121
2022-10-03 09:08:01 - train: epoch 0038, iter [00820, 01251], lr: 0.000471, loss: 0.4373
2022-10-03 09:08:20 - train: epoch 0038, iter [00830, 01251], lr: 0.000471, loss: 0.4025
2022-10-03 09:08:40 - train: epoch 0038, iter [00840, 01251], lr: 0.000471, loss: 0.4055
2022-10-03 09:09:00 - train: epoch 0038, iter [00850, 01251], lr: 0.000471, loss: 0.4274
2022-10-03 09:09:20 - train: epoch 0038, iter [00860, 01251], lr: 0.000470, loss: 0.4181
2022-10-03 09:09:39 - train: epoch 0038, iter [00870, 01251], lr: 0.000470, loss: 0.4033
2022-10-03 09:09:59 - train: epoch 0038, iter [00880, 01251], lr: 0.000470, loss: 0.4185
2022-10-03 09:10:19 - train: epoch 0038, iter [00890, 01251], lr: 0.000470, loss: 0.4192
2022-10-03 09:10:39 - train: epoch 0038, iter [00900, 01251], lr: 0.000470, loss: 0.4144
2022-10-03 09:10:58 - train: epoch 0038, iter [00910, 01251], lr: 0.000470, loss: 0.4168
2022-10-03 09:11:18 - train: epoch 0038, iter [00920, 01251], lr: 0.000470, loss: 0.4233
2022-10-03 09:11:37 - train: epoch 0038, iter [00930, 01251], lr: 0.000470, loss: 0.4197
2022-10-03 09:11:57 - train: epoch 0038, iter [00940, 01251], lr: 0.000470, loss: 0.4158
2022-10-03 09:12:17 - train: epoch 0038, iter [00950, 01251], lr: 0.000470, loss: 0.4264
2022-10-03 09:12:37 - train: epoch 0038, iter [00960, 01251], lr: 0.000470, loss: 0.4302
2022-10-03 09:12:56 - train: epoch 0038, iter [00970, 01251], lr: 0.000470, loss: 0.4162
2022-10-03 09:13:16 - train: epoch 0038, iter [00980, 01251], lr: 0.000470, loss: 0.4330
2022-10-03 09:13:36 - train: epoch 0038, iter [00990, 01251], lr: 0.000470, loss: 0.4327
2022-10-03 09:13:55 - train: epoch 0038, iter [01000, 01251], lr: 0.000469, loss: 0.4234
2022-10-03 09:14:15 - train: epoch 0038, iter [01010, 01251], lr: 0.000469, loss: 0.4141
2022-10-03 09:14:35 - train: epoch 0038, iter [01020, 01251], lr: 0.000469, loss: 0.4262
2022-10-03 09:14:55 - train: epoch 0038, iter [01030, 01251], lr: 0.000469, loss: 0.4068
2022-10-03 09:15:15 - train: epoch 0038, iter [01040, 01251], lr: 0.000469, loss: 0.4061
2022-10-03 09:15:34 - train: epoch 0038, iter [01050, 01251], lr: 0.000469, loss: 0.3946
2022-10-03 09:15:54 - train: epoch 0038, iter [01060, 01251], lr: 0.000469, loss: 0.4148
2022-10-03 09:16:14 - train: epoch 0038, iter [01070, 01251], lr: 0.000469, loss: 0.4112
2022-10-03 09:16:34 - train: epoch 0038, iter [01080, 01251], lr: 0.000469, loss: 0.4090
2022-10-03 09:16:53 - train: epoch 0038, iter [01090, 01251], lr: 0.000469, loss: 0.4025
2022-10-03 09:17:13 - train: epoch 0038, iter [01100, 01251], lr: 0.000469, loss: 0.4340
2022-10-03 09:17:33 - train: epoch 0038, iter [01110, 01251], lr: 0.000469, loss: 0.4288
2022-10-03 09:17:52 - train: epoch 0038, iter [01120, 01251], lr: 0.000469, loss: 0.4280
2022-10-03 09:18:12 - train: epoch 0038, iter [01130, 01251], lr: 0.000469, loss: 0.4286
2022-10-03 09:18:32 - train: epoch 0038, iter [01140, 01251], lr: 0.000469, loss: 0.4047
2022-10-03 09:18:52 - train: epoch 0038, iter [01150, 01251], lr: 0.000468, loss: 0.3962
2022-10-03 09:19:12 - train: epoch 0038, iter [01160, 01251], lr: 0.000468, loss: 0.4181
2022-10-03 09:19:31 - train: epoch 0038, iter [01170, 01251], lr: 0.000468, loss: 0.3997
2022-10-03 09:19:51 - train: epoch 0038, iter [01180, 01251], lr: 0.000468, loss: 0.4298
2022-10-03 09:20:10 - train: epoch 0038, iter [01190, 01251], lr: 0.000468, loss: 0.3991
2022-10-03 09:20:30 - train: epoch 0038, iter [01200, 01251], lr: 0.000468, loss: 0.4051
2022-10-03 09:20:50 - train: epoch 0038, iter [01210, 01251], lr: 0.000468, loss: 0.4047
2022-10-03 09:21:10 - train: epoch 0038, iter [01220, 01251], lr: 0.000468, loss: 0.4054
2022-10-03 09:21:29 - train: epoch 0038, iter [01230, 01251], lr: 0.000468, loss: 0.4121
2022-10-03 09:21:49 - train: epoch 0038, iter [01240, 01251], lr: 0.000468, loss: 0.4165
2022-10-03 09:22:08 - train: epoch 0038, iter [01250, 01251], lr: 0.000468, loss: 0.4324
2022-10-03 09:22:14 - train: epoch 038, train_loss: 0.4151
2022-10-03 09:22:18 - until epoch: 038, best_loss: 0.4151
2022-10-03 09:22:18 - epoch 039 lr: 0.000468
2022-10-03 09:22:45 - train: epoch 0039, iter [00010, 01251], lr: 0.000468, loss: 0.4056
2022-10-03 09:23:05 - train: epoch 0039, iter [00020, 01251], lr: 0.000468, loss: 0.4105
2022-10-03 09:23:25 - train: epoch 0039, iter [00030, 01251], lr: 0.000468, loss: 0.4030
2022-10-03 09:23:45 - train: epoch 0039, iter [00040, 01251], lr: 0.000467, loss: 0.4216
2022-10-03 09:24:05 - train: epoch 0039, iter [00050, 01251], lr: 0.000467, loss: 0.4257
2022-10-03 09:24:25 - train: epoch 0039, iter [00060, 01251], lr: 0.000467, loss: 0.4111
2022-10-03 09:24:45 - train: epoch 0039, iter [00070, 01251], lr: 0.000467, loss: 0.4049
2022-10-03 09:25:05 - train: epoch 0039, iter [00080, 01251], lr: 0.000467, loss: 0.4250
2022-10-03 09:25:25 - train: epoch 0039, iter [00090, 01251], lr: 0.000467, loss: 0.4011
2022-10-03 09:25:44 - train: epoch 0039, iter [00100, 01251], lr: 0.000467, loss: 0.4259
2022-10-03 09:26:04 - train: epoch 0039, iter [00110, 01251], lr: 0.000467, loss: 0.4306
2022-10-03 09:26:24 - train: epoch 0039, iter [00120, 01251], lr: 0.000467, loss: 0.4186
2022-10-03 09:26:44 - train: epoch 0039, iter [00130, 01251], lr: 0.000467, loss: 0.4046
2022-10-03 09:27:04 - train: epoch 0039, iter [00140, 01251], lr: 0.000467, loss: 0.4107
2022-10-03 09:27:24 - train: epoch 0039, iter [00150, 01251], lr: 0.000467, loss: 0.4109
2022-10-03 09:27:44 - train: epoch 0039, iter [00160, 01251], lr: 0.000467, loss: 0.4247
2022-10-03 09:28:04 - train: epoch 0039, iter [00170, 01251], lr: 0.000467, loss: 0.3959
2022-10-03 09:28:24 - train: epoch 0039, iter [00180, 01251], lr: 0.000467, loss: 0.4123
2022-10-03 09:28:44 - train: epoch 0039, iter [00190, 01251], lr: 0.000466, loss: 0.4088
2022-10-03 09:29:04 - train: epoch 0039, iter [00200, 01251], lr: 0.000466, loss: 0.4002
2022-10-03 09:29:24 - train: epoch 0039, iter [00210, 01251], lr: 0.000466, loss: 0.4212
2022-10-03 09:29:44 - train: epoch 0039, iter [00220, 01251], lr: 0.000466, loss: 0.4134
2022-10-03 09:30:04 - train: epoch 0039, iter [00230, 01251], lr: 0.000466, loss: 0.4177
2022-10-03 09:30:24 - train: epoch 0039, iter [00240, 01251], lr: 0.000466, loss: 0.4059
2022-10-03 09:30:44 - train: epoch 0039, iter [00250, 01251], lr: 0.000466, loss: 0.4099
2022-10-03 09:31:04 - train: epoch 0039, iter [00260, 01251], lr: 0.000466, loss: 0.4212
2022-10-03 09:31:24 - train: epoch 0039, iter [00270, 01251], lr: 0.000466, loss: 0.4246
2022-10-03 09:31:44 - train: epoch 0039, iter [00280, 01251], lr: 0.000466, loss: 0.4081
2022-10-03 09:32:04 - train: epoch 0039, iter [00290, 01251], lr: 0.000466, loss: 0.4172
2022-10-03 09:32:23 - train: epoch 0039, iter [00300, 01251], lr: 0.000466, loss: 0.4169
2022-10-03 09:32:43 - train: epoch 0039, iter [00310, 01251], lr: 0.000466, loss: 0.4226
2022-10-03 09:33:03 - train: epoch 0039, iter [00320, 01251], lr: 0.000466, loss: 0.4105
2022-10-03 09:33:23 - train: epoch 0039, iter [00330, 01251], lr: 0.000465, loss: 0.4201
2022-10-03 09:33:43 - train: epoch 0039, iter [00340, 01251], lr: 0.000465, loss: 0.4210
2022-10-03 09:34:02 - train: epoch 0039, iter [00350, 01251], lr: 0.000465, loss: 0.4135
2022-10-03 09:34:22 - train: epoch 0039, iter [00360, 01251], lr: 0.000465, loss: 0.4083
2022-10-03 09:34:42 - train: epoch 0039, iter [00370, 01251], lr: 0.000465, loss: 0.4105
2022-10-03 09:35:02 - train: epoch 0039, iter [00380, 01251], lr: 0.000465, loss: 0.3952
2022-10-03 09:35:21 - train: epoch 0039, iter [00390, 01251], lr: 0.000465, loss: 0.4178
2022-10-03 09:35:41 - train: epoch 0039, iter [00400, 01251], lr: 0.000465, loss: 0.4153
2022-10-03 09:36:01 - train: epoch 0039, iter [00410, 01251], lr: 0.000465, loss: 0.4147
2022-10-03 09:36:21 - train: epoch 0039, iter [00420, 01251], lr: 0.000465, loss: 0.3881
2022-10-03 09:36:41 - train: epoch 0039, iter [00430, 01251], lr: 0.000465, loss: 0.4241
2022-10-03 09:37:01 - train: epoch 0039, iter [00440, 01251], lr: 0.000465, loss: 0.4218
2022-10-03 09:37:20 - train: epoch 0039, iter [00450, 01251], lr: 0.000465, loss: 0.4092
2022-10-03 09:37:40 - train: epoch 0039, iter [00460, 01251], lr: 0.000465, loss: 0.4097
2022-10-03 09:38:00 - train: epoch 0039, iter [00470, 01251], lr: 0.000464, loss: 0.3996
2022-10-03 09:38:20 - train: epoch 0039, iter [00480, 01251], lr: 0.000464, loss: 0.3999
2022-10-03 09:38:40 - train: epoch 0039, iter [00490, 01251], lr: 0.000464, loss: 0.4154
2022-10-03 09:38:59 - train: epoch 0039, iter [00500, 01251], lr: 0.000464, loss: 0.3865
2022-10-03 09:39:19 - train: epoch 0039, iter [00510, 01251], lr: 0.000464, loss: 0.4107
2022-10-03 09:39:39 - train: epoch 0039, iter [00520, 01251], lr: 0.000464, loss: 0.4032
2022-10-03 09:39:58 - train: epoch 0039, iter [00530, 01251], lr: 0.000464, loss: 0.4213
2022-10-03 09:40:18 - train: epoch 0039, iter [00540, 01251], lr: 0.000464, loss: 0.4159
2022-10-03 09:40:37 - train: epoch 0039, iter [00550, 01251], lr: 0.000464, loss: 0.4099
2022-10-03 09:40:57 - train: epoch 0039, iter [00560, 01251], lr: 0.000464, loss: 0.4184
2022-10-03 09:41:17 - train: epoch 0039, iter [00570, 01251], lr: 0.000464, loss: 0.4091
2022-10-03 09:41:37 - train: epoch 0039, iter [00580, 01251], lr: 0.000464, loss: 0.4283
2022-10-03 09:41:57 - train: epoch 0039, iter [00590, 01251], lr: 0.000464, loss: 0.4306
2022-10-03 09:42:17 - train: epoch 0039, iter [00600, 01251], lr: 0.000464, loss: 0.4042
2022-10-03 09:42:37 - train: epoch 0039, iter [00610, 01251], lr: 0.000464, loss: 0.3962
2022-10-03 09:42:57 - train: epoch 0039, iter [00620, 01251], lr: 0.000463, loss: 0.4277
2022-10-03 09:43:16 - train: epoch 0039, iter [00630, 01251], lr: 0.000463, loss: 0.4302
2022-10-03 09:43:36 - train: epoch 0039, iter [00640, 01251], lr: 0.000463, loss: 0.4134
2022-10-03 09:43:56 - train: epoch 0039, iter [00650, 01251], lr: 0.000463, loss: 0.4177
2022-10-03 09:44:16 - train: epoch 0039, iter [00660, 01251], lr: 0.000463, loss: 0.4088
2022-10-03 09:44:36 - train: epoch 0039, iter [00670, 01251], lr: 0.000463, loss: 0.4183
2022-10-03 09:44:56 - train: epoch 0039, iter [00680, 01251], lr: 0.000463, loss: 0.4252
2022-10-03 09:45:16 - train: epoch 0039, iter [00690, 01251], lr: 0.000463, loss: 0.4309
2022-10-03 09:45:36 - train: epoch 0039, iter [00700, 01251], lr: 0.000463, loss: 0.4305
2022-10-03 09:45:56 - train: epoch 0039, iter [00710, 01251], lr: 0.000463, loss: 0.4344
2022-10-03 09:46:16 - train: epoch 0039, iter [00720, 01251], lr: 0.000463, loss: 0.4127
2022-10-03 09:46:35 - train: epoch 0039, iter [00730, 01251], lr: 0.000463, loss: 0.4252
2022-10-03 09:46:55 - train: epoch 0039, iter [00740, 01251], lr: 0.000463, loss: 0.4179
2022-10-03 09:47:16 - train: epoch 0039, iter [00750, 01251], lr: 0.000463, loss: 0.4149
2022-10-03 09:47:36 - train: epoch 0039, iter [00760, 01251], lr: 0.000462, loss: 0.4130
2022-10-03 09:47:56 - train: epoch 0039, iter [00770, 01251], lr: 0.000462, loss: 0.4210
2022-10-03 09:48:16 - train: epoch 0039, iter [00780, 01251], lr: 0.000462, loss: 0.4068
2022-10-03 09:48:36 - train: epoch 0039, iter [00790, 01251], lr: 0.000462, loss: 0.4228
2022-10-03 09:48:55 - train: epoch 0039, iter [00800, 01251], lr: 0.000462, loss: 0.4388
2022-10-03 09:49:15 - train: epoch 0039, iter [00810, 01251], lr: 0.000462, loss: 0.4181
2022-10-03 09:49:35 - train: epoch 0039, iter [00820, 01251], lr: 0.000462, loss: 0.4115
2022-10-03 09:49:55 - train: epoch 0039, iter [00830, 01251], lr: 0.000462, loss: 0.4103
2022-10-03 09:50:14 - train: epoch 0039, iter [00840, 01251], lr: 0.000462, loss: 0.4105
2022-10-03 09:50:34 - train: epoch 0039, iter [00850, 01251], lr: 0.000462, loss: 0.4313
2022-10-03 09:50:54 - train: epoch 0039, iter [00860, 01251], lr: 0.000462, loss: 0.4281
2022-10-03 09:51:14 - train: epoch 0039, iter [00870, 01251], lr: 0.000462, loss: 0.4123
2022-10-03 09:51:34 - train: epoch 0039, iter [00880, 01251], lr: 0.000462, loss: 0.4193
2022-10-03 09:51:54 - train: epoch 0039, iter [00890, 01251], lr: 0.000462, loss: 0.4220
2022-10-03 09:52:14 - train: epoch 0039, iter [00900, 01251], lr: 0.000461, loss: 0.4065
2022-10-03 09:52:33 - train: epoch 0039, iter [00910, 01251], lr: 0.000461, loss: 0.4036
2022-10-03 09:52:53 - train: epoch 0039, iter [00920, 01251], lr: 0.000461, loss: 0.4208
2022-10-03 09:53:13 - train: epoch 0039, iter [00930, 01251], lr: 0.000461, loss: 0.4188
2022-10-03 09:53:33 - train: epoch 0039, iter [00940, 01251], lr: 0.000461, loss: 0.4294
2022-10-03 09:53:53 - train: epoch 0039, iter [00950, 01251], lr: 0.000461, loss: 0.4067
2022-10-03 09:54:13 - train: epoch 0039, iter [00960, 01251], lr: 0.000461, loss: 0.3904
2022-10-03 09:54:32 - train: epoch 0039, iter [00970, 01251], lr: 0.000461, loss: 0.4168
2022-10-03 09:54:52 - train: epoch 0039, iter [00980, 01251], lr: 0.000461, loss: 0.4209
2022-10-03 09:55:12 - train: epoch 0039, iter [00990, 01251], lr: 0.000461, loss: 0.4308
2022-10-03 09:55:32 - train: epoch 0039, iter [01000, 01251], lr: 0.000461, loss: 0.4220
2022-10-03 09:55:52 - train: epoch 0039, iter [01010, 01251], lr: 0.000461, loss: 0.4052
2022-10-03 09:56:12 - train: epoch 0039, iter [01020, 01251], lr: 0.000461, loss: 0.4271
2022-10-03 09:56:31 - train: epoch 0039, iter [01030, 01251], lr: 0.000461, loss: 0.4255
2022-10-03 09:56:51 - train: epoch 0039, iter [01040, 01251], lr: 0.000460, loss: 0.4311
2022-10-03 09:57:11 - train: epoch 0039, iter [01050, 01251], lr: 0.000460, loss: 0.4109
2022-10-03 09:57:31 - train: epoch 0039, iter [01060, 01251], lr: 0.000460, loss: 0.4043
2022-10-03 09:57:51 - train: epoch 0039, iter [01070, 01251], lr: 0.000460, loss: 0.4049
2022-10-03 09:58:10 - train: epoch 0039, iter [01080, 01251], lr: 0.000460, loss: 0.4315
2022-10-03 09:58:30 - train: epoch 0039, iter [01090, 01251], lr: 0.000460, loss: 0.4104
2022-10-03 09:58:50 - train: epoch 0039, iter [01100, 01251], lr: 0.000460, loss: 0.4406
2022-10-03 09:59:10 - train: epoch 0039, iter [01110, 01251], lr: 0.000460, loss: 0.4146
2022-10-03 09:59:29 - train: epoch 0039, iter [01120, 01251], lr: 0.000460, loss: 0.4209
2022-10-03 09:59:49 - train: epoch 0039, iter [01130, 01251], lr: 0.000460, loss: 0.3966
2022-10-03 10:00:09 - train: epoch 0039, iter [01140, 01251], lr: 0.000460, loss: 0.4121
2022-10-03 10:00:29 - train: epoch 0039, iter [01150, 01251], lr: 0.000460, loss: 0.4183
2022-10-03 10:00:49 - train: epoch 0039, iter [01160, 01251], lr: 0.000460, loss: 0.4051
2022-10-03 10:01:08 - train: epoch 0039, iter [01170, 01251], lr: 0.000460, loss: 0.4175
2022-10-03 10:01:28 - train: epoch 0039, iter [01180, 01251], lr: 0.000459, loss: 0.4162
2022-10-03 10:01:48 - train: epoch 0039, iter [01190, 01251], lr: 0.000459, loss: 0.3993
2022-10-03 10:02:08 - train: epoch 0039, iter [01200, 01251], lr: 0.000459, loss: 0.4154
2022-10-03 10:02:28 - train: epoch 0039, iter [01210, 01251], lr: 0.000459, loss: 0.4156
2022-10-03 10:02:48 - train: epoch 0039, iter [01220, 01251], lr: 0.000459, loss: 0.4250
2022-10-03 10:03:07 - train: epoch 0039, iter [01230, 01251], lr: 0.000459, loss: 0.4170
2022-10-03 10:03:27 - train: epoch 0039, iter [01240, 01251], lr: 0.000459, loss: 0.4205
2022-10-03 10:03:46 - train: epoch 0039, iter [01250, 01251], lr: 0.000459, loss: 0.4172
2022-10-03 10:03:51 - train: epoch 039, train_loss: 0.4147
2022-10-03 10:03:55 - until epoch: 039, best_loss: 0.4147
2022-10-03 10:03:55 - epoch 040 lr: 0.000459
2022-10-03 10:04:21 - train: epoch 0040, iter [00010, 01251], lr: 0.000459, loss: 0.3998
2022-10-03 10:04:41 - train: epoch 0040, iter [00020, 01251], lr: 0.000459, loss: 0.4072
2022-10-03 10:05:01 - train: epoch 0040, iter [00030, 01251], lr: 0.000459, loss: 0.4294
2022-10-03 10:05:21 - train: epoch 0040, iter [00040, 01251], lr: 0.000459, loss: 0.4170
2022-10-03 10:05:41 - train: epoch 0040, iter [00050, 01251], lr: 0.000459, loss: 0.4172
2022-10-03 10:06:01 - train: epoch 0040, iter [00060, 01251], lr: 0.000459, loss: 0.4207
2022-10-03 10:06:21 - train: epoch 0040, iter [00070, 01251], lr: 0.000458, loss: 0.4060
2022-10-03 10:06:41 - train: epoch 0040, iter [00080, 01251], lr: 0.000458, loss: 0.4103
2022-10-03 10:07:01 - train: epoch 0040, iter [00090, 01251], lr: 0.000458, loss: 0.4018
2022-10-03 10:07:21 - train: epoch 0040, iter [00100, 01251], lr: 0.000458, loss: 0.4162
2022-10-03 10:07:41 - train: epoch 0040, iter [00110, 01251], lr: 0.000458, loss: 0.4185
2022-10-03 10:08:01 - train: epoch 0040, iter [00120, 01251], lr: 0.000458, loss: 0.4078
2022-10-03 10:08:21 - train: epoch 0040, iter [00130, 01251], lr: 0.000458, loss: 0.4134
2022-10-03 10:08:41 - train: epoch 0040, iter [00140, 01251], lr: 0.000458, loss: 0.4154
2022-10-03 10:09:01 - train: epoch 0040, iter [00150, 01251], lr: 0.000458, loss: 0.4094
2022-10-03 10:09:22 - train: epoch 0040, iter [00160, 01251], lr: 0.000458, loss: 0.4147
2022-10-03 10:09:41 - train: epoch 0040, iter [00170, 01251], lr: 0.000458, loss: 0.4033
2022-10-03 10:10:02 - train: epoch 0040, iter [00180, 01251], lr: 0.000458, loss: 0.4201
2022-10-03 10:10:22 - train: epoch 0040, iter [00190, 01251], lr: 0.000458, loss: 0.4239
2022-10-03 10:10:42 - train: epoch 0040, iter [00200, 01251], lr: 0.000458, loss: 0.4143
2022-10-03 10:11:02 - train: epoch 0040, iter [00210, 01251], lr: 0.000457, loss: 0.4114
2022-10-03 10:11:22 - train: epoch 0040, iter [00220, 01251], lr: 0.000457, loss: 0.4164
2022-10-03 10:11:42 - train: epoch 0040, iter [00230, 01251], lr: 0.000457, loss: 0.4197
2022-10-03 10:12:01 - train: epoch 0040, iter [00240, 01251], lr: 0.000457, loss: 0.4155
2022-10-03 10:12:21 - train: epoch 0040, iter [00250, 01251], lr: 0.000457, loss: 0.4219
2022-10-03 10:12:42 - train: epoch 0040, iter [00260, 01251], lr: 0.000457, loss: 0.4353
2022-10-03 10:13:02 - train: epoch 0040, iter [00270, 01251], lr: 0.000457, loss: 0.4085
2022-10-03 10:13:22 - train: epoch 0040, iter [00280, 01251], lr: 0.000457, loss: 0.4056
2022-10-03 10:13:41 - train: epoch 0040, iter [00290, 01251], lr: 0.000457, loss: 0.3907
2022-10-03 10:14:01 - train: epoch 0040, iter [00300, 01251], lr: 0.000457, loss: 0.3911
2022-10-03 10:14:21 - train: epoch 0040, iter [00310, 01251], lr: 0.000457, loss: 0.4135
2022-10-03 10:14:42 - train: epoch 0040, iter [00320, 01251], lr: 0.000457, loss: 0.4089
2022-10-03 10:15:02 - train: epoch 0040, iter [00330, 01251], lr: 0.000457, loss: 0.4023
2022-10-03 10:15:22 - train: epoch 0040, iter [00340, 01251], lr: 0.000457, loss: 0.4205
2022-10-03 10:15:42 - train: epoch 0040, iter [00350, 01251], lr: 0.000456, loss: 0.4067
2022-10-03 10:16:01 - train: epoch 0040, iter [00360, 01251], lr: 0.000456, loss: 0.4153
2022-10-03 10:16:20 - train: epoch 0040, iter [00370, 01251], lr: 0.000456, loss: 0.4158
2022-10-03 10:16:39 - train: epoch 0040, iter [00380, 01251], lr: 0.000456, loss: 0.4163
2022-10-03 10:16:57 - train: epoch 0040, iter [00390, 01251], lr: 0.000456, loss: 0.4274
2022-10-03 10:17:16 - train: epoch 0040, iter [00400, 01251], lr: 0.000456, loss: 0.4098
2022-10-03 10:17:36 - train: epoch 0040, iter [00410, 01251], lr: 0.000456, loss: 0.4332
2022-10-03 10:17:55 - train: epoch 0040, iter [00420, 01251], lr: 0.000456, loss: 0.4372
2022-10-03 10:18:14 - train: epoch 0040, iter [00430, 01251], lr: 0.000456, loss: 0.4228
2022-10-03 10:18:33 - train: epoch 0040, iter [00440, 01251], lr: 0.000456, loss: 0.4105
2022-10-03 10:18:52 - train: epoch 0040, iter [00450, 01251], lr: 0.000456, loss: 0.4099
2022-10-03 10:19:11 - train: epoch 0040, iter [00460, 01251], lr: 0.000456, loss: 0.4105
2022-10-03 10:19:30 - train: epoch 0040, iter [00470, 01251], lr: 0.000456, loss: 0.4080
2022-10-03 10:19:50 - train: epoch 0040, iter [00480, 01251], lr: 0.000456, loss: 0.4193
2022-10-03 10:20:09 - train: epoch 0040, iter [00490, 01251], lr: 0.000455, loss: 0.4118
2022-10-03 10:20:28 - train: epoch 0040, iter [00500, 01251], lr: 0.000455, loss: 0.4198
2022-10-03 10:20:47 - train: epoch 0040, iter [00510, 01251], lr: 0.000455, loss: 0.3994
2022-10-03 10:21:06 - train: epoch 0040, iter [00520, 01251], lr: 0.000455, loss: 0.4077
2022-10-03 10:21:26 - train: epoch 0040, iter [00530, 01251], lr: 0.000455, loss: 0.4547
2022-10-03 10:21:45 - train: epoch 0040, iter [00540, 01251], lr: 0.000455, loss: 0.4275
2022-10-03 10:22:05 - train: epoch 0040, iter [00550, 01251], lr: 0.000455, loss: 0.4192
2022-10-03 10:22:24 - train: epoch 0040, iter [00560, 01251], lr: 0.000455, loss: 0.3957
2022-10-03 10:22:43 - train: epoch 0040, iter [00570, 01251], lr: 0.000455, loss: 0.4110
2022-10-03 10:23:02 - train: epoch 0040, iter [00580, 01251], lr: 0.000455, loss: 0.4044
2022-10-03 10:23:21 - train: epoch 0040, iter [00590, 01251], lr: 0.000455, loss: 0.4187
2022-10-03 10:23:41 - train: epoch 0040, iter [00600, 01251], lr: 0.000455, loss: 0.4039
2022-10-03 10:24:00 - train: epoch 0040, iter [00610, 01251], lr: 0.000455, loss: 0.4050
2022-10-03 10:24:19 - train: epoch 0040, iter [00620, 01251], lr: 0.000455, loss: 0.4185
2022-10-03 10:24:38 - train: epoch 0040, iter [00630, 01251], lr: 0.000454, loss: 0.4274
2022-10-03 10:24:57 - train: epoch 0040, iter [00640, 01251], lr: 0.000454, loss: 0.3993
2022-10-03 10:25:16 - train: epoch 0040, iter [00650, 01251], lr: 0.000454, loss: 0.4142
2022-10-03 10:25:35 - train: epoch 0040, iter [00660, 01251], lr: 0.000454, loss: 0.4165
2022-10-03 10:25:54 - train: epoch 0040, iter [00670, 01251], lr: 0.000454, loss: 0.4061
2022-10-03 10:26:13 - train: epoch 0040, iter [00680, 01251], lr: 0.000454, loss: 0.4128
2022-10-03 10:26:32 - train: epoch 0040, iter [00690, 01251], lr: 0.000454, loss: 0.4231
2022-10-03 10:26:51 - train: epoch 0040, iter [00700, 01251], lr: 0.000454, loss: 0.4070
2022-10-03 10:27:10 - train: epoch 0040, iter [00710, 01251], lr: 0.000454, loss: 0.4100
2022-10-03 10:27:29 - train: epoch 0040, iter [00720, 01251], lr: 0.000454, loss: 0.4086
2022-10-03 10:27:48 - train: epoch 0040, iter [00730, 01251], lr: 0.000454, loss: 0.4153
2022-10-03 10:28:07 - train: epoch 0040, iter [00740, 01251], lr: 0.000454, loss: 0.3851
2022-10-03 10:28:26 - train: epoch 0040, iter [00750, 01251], lr: 0.000454, loss: 0.4104
2022-10-03 10:28:45 - train: epoch 0040, iter [00760, 01251], lr: 0.000454, loss: 0.4015
2022-10-03 10:29:04 - train: epoch 0040, iter [00770, 01251], lr: 0.000453, loss: 0.4302
2022-10-03 10:29:23 - train: epoch 0040, iter [00780, 01251], lr: 0.000453, loss: 0.3920
2022-10-03 10:29:42 - train: epoch 0040, iter [00790, 01251], lr: 0.000453, loss: 0.4281
2022-10-03 10:30:01 - train: epoch 0040, iter [00800, 01251], lr: 0.000453, loss: 0.4295
2022-10-03 10:30:21 - train: epoch 0040, iter [00810, 01251], lr: 0.000453, loss: 0.4389
2022-10-03 10:30:39 - train: epoch 0040, iter [00820, 01251], lr: 0.000453, loss: 0.3977
2022-10-03 10:30:58 - train: epoch 0040, iter [00830, 01251], lr: 0.000453, loss: 0.4048
2022-10-03 10:31:17 - train: epoch 0040, iter [00840, 01251], lr: 0.000453, loss: 0.4373
2022-10-03 10:31:36 - train: epoch 0040, iter [00850, 01251], lr: 0.000453, loss: 0.4349
2022-10-03 10:31:55 - train: epoch 0040, iter [00860, 01251], lr: 0.000453, loss: 0.4230
2022-10-03 10:32:14 - train: epoch 0040, iter [00870, 01251], lr: 0.000453, loss: 0.4097
2022-10-03 10:32:34 - train: epoch 0040, iter [00880, 01251], lr: 0.000453, loss: 0.3950
2022-10-03 10:32:53 - train: epoch 0040, iter [00890, 01251], lr: 0.000453, loss: 0.4072
2022-10-03 10:33:12 - train: epoch 0040, iter [00900, 01251], lr: 0.000453, loss: 0.4128
2022-10-03 10:33:32 - train: epoch 0040, iter [00910, 01251], lr: 0.000452, loss: 0.4151
2022-10-03 10:33:51 - train: epoch 0040, iter [00920, 01251], lr: 0.000452, loss: 0.4207
2022-10-03 10:34:10 - train: epoch 0040, iter [00930, 01251], lr: 0.000452, loss: 0.4079
2022-10-03 10:34:30 - train: epoch 0040, iter [00940, 01251], lr: 0.000452, loss: 0.4257
2022-10-03 10:34:49 - train: epoch 0040, iter [00950, 01251], lr: 0.000452, loss: 0.4094
2022-10-03 10:35:08 - train: epoch 0040, iter [00960, 01251], lr: 0.000452, loss: 0.4168
2022-10-03 10:35:27 - train: epoch 0040, iter [00970, 01251], lr: 0.000452, loss: 0.4178
2022-10-03 10:35:46 - train: epoch 0040, iter [00980, 01251], lr: 0.000452, loss: 0.4088
2022-10-03 10:36:05 - train: epoch 0040, iter [00990, 01251], lr: 0.000452, loss: 0.4252
2022-10-03 10:36:24 - train: epoch 0040, iter [01000, 01251], lr: 0.000452, loss: 0.3904
2022-10-03 10:36:43 - train: epoch 0040, iter [01010, 01251], lr: 0.000452, loss: 0.4094
2022-10-03 10:37:03 - train: epoch 0040, iter [01020, 01251], lr: 0.000452, loss: 0.4113
2022-10-03 10:37:22 - train: epoch 0040, iter [01030, 01251], lr: 0.000452, loss: 0.4448
2022-10-03 10:37:41 - train: epoch 0040, iter [01040, 01251], lr: 0.000452, loss: 0.4339
2022-10-03 10:38:00 - train: epoch 0040, iter [01050, 01251], lr: 0.000451, loss: 0.4456
2022-10-03 10:38:19 - train: epoch 0040, iter [01060, 01251], lr: 0.000451, loss: 0.4210
2022-10-03 10:38:38 - train: epoch 0040, iter [01070, 01251], lr: 0.000451, loss: 0.4184
2022-10-03 10:38:57 - train: epoch 0040, iter [01080, 01251], lr: 0.000451, loss: 0.4101
2022-10-03 10:39:15 - train: epoch 0040, iter [01090, 01251], lr: 0.000451, loss: 0.4125
2022-10-03 10:39:34 - train: epoch 0040, iter [01100, 01251], lr: 0.000451, loss: 0.3959
2022-10-03 10:39:53 - train: epoch 0040, iter [01110, 01251], lr: 0.000451, loss: 0.3955
2022-10-03 10:40:12 - train: epoch 0040, iter [01120, 01251], lr: 0.000451, loss: 0.4160
2022-10-03 10:40:31 - train: epoch 0040, iter [01130, 01251], lr: 0.000451, loss: 0.4024
2022-10-03 10:40:51 - train: epoch 0040, iter [01140, 01251], lr: 0.000451, loss: 0.4241
2022-10-03 10:41:09 - train: epoch 0040, iter [01150, 01251], lr: 0.000451, loss: 0.4137
2022-10-03 10:41:28 - train: epoch 0040, iter [01160, 01251], lr: 0.000451, loss: 0.4273
2022-10-03 10:41:47 - train: epoch 0040, iter [01170, 01251], lr: 0.000451, loss: 0.4299
2022-10-03 10:42:06 - train: epoch 0040, iter [01180, 01251], lr: 0.000451, loss: 0.4033
2022-10-03 10:42:25 - train: epoch 0040, iter [01190, 01251], lr: 0.000450, loss: 0.4252
2022-10-03 10:42:44 - train: epoch 0040, iter [01200, 01251], lr: 0.000450, loss: 0.4112
2022-10-03 10:43:03 - train: epoch 0040, iter [01210, 01251], lr: 0.000450, loss: 0.4280
2022-10-03 10:43:21 - train: epoch 0040, iter [01220, 01251], lr: 0.000450, loss: 0.4211
2022-10-03 10:43:40 - train: epoch 0040, iter [01230, 01251], lr: 0.000450, loss: 0.4001
2022-10-03 10:43:59 - train: epoch 0040, iter [01240, 01251], lr: 0.000450, loss: 0.4200
2022-10-03 10:44:17 - train: epoch 0040, iter [01250, 01251], lr: 0.000450, loss: 0.4275
2022-10-03 10:44:22 - train: epoch 040, train_loss: 0.4143
2022-10-03 10:44:26 - until epoch: 040, best_loss: 0.4143
2022-10-03 10:44:26 - epoch 041 lr: 0.000450
2022-10-03 10:44:52 - train: epoch 0041, iter [00010, 01251], lr: 0.000450, loss: 0.4098
2022-10-03 10:45:11 - train: epoch 0041, iter [00020, 01251], lr: 0.000450, loss: 0.4073
2022-10-03 10:45:30 - train: epoch 0041, iter [00030, 01251], lr: 0.000450, loss: 0.4271
2022-10-03 10:45:49 - train: epoch 0041, iter [00040, 01251], lr: 0.000450, loss: 0.4190
2022-10-03 10:46:07 - train: epoch 0041, iter [00050, 01251], lr: 0.000450, loss: 0.4200
2022-10-03 10:46:26 - train: epoch 0041, iter [00060, 01251], lr: 0.000450, loss: 0.4191
2022-10-03 10:46:45 - train: epoch 0041, iter [00070, 01251], lr: 0.000449, loss: 0.4095
2022-10-03 10:47:04 - train: epoch 0041, iter [00080, 01251], lr: 0.000449, loss: 0.4359
2022-10-03 10:47:24 - train: epoch 0041, iter [00090, 01251], lr: 0.000449, loss: 0.4095
2022-10-03 10:47:43 - train: epoch 0041, iter [00100, 01251], lr: 0.000449, loss: 0.3979
2022-10-03 10:48:03 - train: epoch 0041, iter [00110, 01251], lr: 0.000449, loss: 0.3996
2022-10-03 10:48:22 - train: epoch 0041, iter [00120, 01251], lr: 0.000449, loss: 0.4301
2022-10-03 10:48:42 - train: epoch 0041, iter [00130, 01251], lr: 0.000449, loss: 0.4041
2022-10-03 10:49:01 - train: epoch 0041, iter [00140, 01251], lr: 0.000449, loss: 0.4138
2022-10-03 10:49:21 - train: epoch 0041, iter [00150, 01251], lr: 0.000449, loss: 0.4130
2022-10-03 10:49:40 - train: epoch 0041, iter [00160, 01251], lr: 0.000449, loss: 0.4142
2022-10-03 10:49:59 - train: epoch 0041, iter [00170, 01251], lr: 0.000449, loss: 0.4233
2022-10-03 10:50:19 - train: epoch 0041, iter [00180, 01251], lr: 0.000449, loss: 0.4383
2022-10-03 10:50:38 - train: epoch 0041, iter [00190, 01251], lr: 0.000449, loss: 0.4161
2022-10-03 10:50:57 - train: epoch 0041, iter [00200, 01251], lr: 0.000449, loss: 0.4014
2022-10-03 10:51:17 - train: epoch 0041, iter [00210, 01251], lr: 0.000448, loss: 0.4284
2022-10-03 10:51:36 - train: epoch 0041, iter [00220, 01251], lr: 0.000448, loss: 0.4298
2022-10-03 10:51:55 - train: epoch 0041, iter [00230, 01251], lr: 0.000448, loss: 0.3931
2022-10-03 10:52:14 - train: epoch 0041, iter [00240, 01251], lr: 0.000448, loss: 0.4149
2022-10-03 10:52:33 - train: epoch 0041, iter [00250, 01251], lr: 0.000448, loss: 0.4235
2022-10-03 10:52:52 - train: epoch 0041, iter [00260, 01251], lr: 0.000448, loss: 0.3956
2022-10-03 10:53:11 - train: epoch 0041, iter [00270, 01251], lr: 0.000448, loss: 0.4289
2022-10-03 10:53:29 - train: epoch 0041, iter [00280, 01251], lr: 0.000448, loss: 0.4066
2022-10-03 10:53:48 - train: epoch 0041, iter [00290, 01251], lr: 0.000448, loss: 0.4263
2022-10-03 10:54:07 - train: epoch 0041, iter [00300, 01251], lr: 0.000448, loss: 0.4227
2022-10-03 10:54:26 - train: epoch 0041, iter [00310, 01251], lr: 0.000448, loss: 0.4199
2022-10-03 10:54:45 - train: epoch 0041, iter [00320, 01251], lr: 0.000448, loss: 0.4078
2022-10-03 10:55:04 - train: epoch 0041, iter [00330, 01251], lr: 0.000448, loss: 0.4184
2022-10-03 10:55:23 - train: epoch 0041, iter [00340, 01251], lr: 0.000448, loss: 0.4160
2022-10-03 10:55:42 - train: epoch 0041, iter [00350, 01251], lr: 0.000447, loss: 0.4042
2022-10-03 10:56:01 - train: epoch 0041, iter [00360, 01251], lr: 0.000447, loss: 0.4082
2022-10-03 10:56:19 - train: epoch 0041, iter [00370, 01251], lr: 0.000447, loss: 0.4345
2022-10-03 10:56:38 - train: epoch 0041, iter [00380, 01251], lr: 0.000447, loss: 0.4014
2022-10-03 10:56:57 - train: epoch 0041, iter [00390, 01251], lr: 0.000447, loss: 0.4092
2022-10-03 10:57:16 - train: epoch 0041, iter [00400, 01251], lr: 0.000447, loss: 0.4076
2022-10-03 10:57:35 - train: epoch 0041, iter [00410, 01251], lr: 0.000447, loss: 0.4156
2022-10-03 10:57:54 - train: epoch 0041, iter [00420, 01251], lr: 0.000447, loss: 0.4017
2022-10-03 10:58:13 - train: epoch 0041, iter [00430, 01251], lr: 0.000447, loss: 0.4080
2022-10-03 10:58:32 - train: epoch 0041, iter [00440, 01251], lr: 0.000447, loss: 0.4085
2022-10-03 10:58:51 - train: epoch 0041, iter [00450, 01251], lr: 0.000447, loss: 0.4130
2022-10-03 10:59:10 - train: epoch 0041, iter [00460, 01251], lr: 0.000447, loss: 0.3945
2022-10-03 10:59:29 - train: epoch 0041, iter [00470, 01251], lr: 0.000447, loss: 0.4113
2022-10-03 10:59:47 - train: epoch 0041, iter [00480, 01251], lr: 0.000447, loss: 0.4276
2022-10-03 11:00:06 - train: epoch 0041, iter [00490, 01251], lr: 0.000446, loss: 0.3773
2022-10-03 11:00:25 - train: epoch 0041, iter [00500, 01251], lr: 0.000446, loss: 0.4297
2022-10-03 11:00:44 - train: epoch 0041, iter [00510, 01251], lr: 0.000446, loss: 0.4238
2022-10-03 11:01:03 - train: epoch 0041, iter [00520, 01251], lr: 0.000446, loss: 0.4154
2022-10-03 11:01:22 - train: epoch 0041, iter [00530, 01251], lr: 0.000446, loss: 0.4251
2022-10-03 11:01:41 - train: epoch 0041, iter [00540, 01251], lr: 0.000446, loss: 0.4179
2022-10-03 11:02:00 - train: epoch 0041, iter [00550, 01251], lr: 0.000446, loss: 0.4212
2022-10-03 11:02:19 - train: epoch 0041, iter [00560, 01251], lr: 0.000446, loss: 0.4141
2022-10-03 11:02:38 - train: epoch 0041, iter [00570, 01251], lr: 0.000446, loss: 0.4316
2022-10-03 11:02:56 - train: epoch 0041, iter [00580, 01251], lr: 0.000446, loss: 0.4220
2022-10-03 11:03:15 - train: epoch 0041, iter [00590, 01251], lr: 0.000446, loss: 0.4099
2022-10-03 11:03:34 - train: epoch 0041, iter [00600, 01251], lr: 0.000446, loss: 0.4237
2022-10-03 11:03:53 - train: epoch 0041, iter [00610, 01251], lr: 0.000446, loss: 0.3882
2022-10-03 11:04:12 - train: epoch 0041, iter [00620, 01251], lr: 0.000445, loss: 0.4247
2022-10-03 11:04:31 - train: epoch 0041, iter [00630, 01251], lr: 0.000445, loss: 0.4185
2022-10-03 11:04:50 - train: epoch 0041, iter [00640, 01251], lr: 0.000445, loss: 0.4122
2022-10-03 11:05:09 - train: epoch 0041, iter [00650, 01251], lr: 0.000445, loss: 0.4065
2022-10-03 11:05:28 - train: epoch 0041, iter [00660, 01251], lr: 0.000445, loss: 0.4160
2022-10-03 11:05:46 - train: epoch 0041, iter [00670, 01251], lr: 0.000445, loss: 0.4088
2022-10-03 11:06:05 - train: epoch 0041, iter [00680, 01251], lr: 0.000445, loss: 0.4040
2022-10-03 11:06:24 - train: epoch 0041, iter [00690, 01251], lr: 0.000445, loss: 0.4194
2022-10-03 11:06:43 - train: epoch 0041, iter [00700, 01251], lr: 0.000445, loss: 0.3949
2022-10-03 11:07:02 - train: epoch 0041, iter [00710, 01251], lr: 0.000445, loss: 0.4199
2022-10-03 11:07:21 - train: epoch 0041, iter [00720, 01251], lr: 0.000445, loss: 0.4129
2022-10-03 11:07:40 - train: epoch 0041, iter [00730, 01251], lr: 0.000445, loss: 0.4231
2022-10-03 11:07:59 - train: epoch 0041, iter [00740, 01251], lr: 0.000445, loss: 0.4096
2022-10-03 11:08:18 - train: epoch 0041, iter [00750, 01251], lr: 0.000445, loss: 0.4173
2022-10-03 11:08:37 - train: epoch 0041, iter [00760, 01251], lr: 0.000444, loss: 0.4106
2022-10-03 11:08:56 - train: epoch 0041, iter [00770, 01251], lr: 0.000444, loss: 0.3930
2022-10-03 11:09:15 - train: epoch 0041, iter [00780, 01251], lr: 0.000444, loss: 0.4193
2022-10-03 11:09:34 - train: epoch 0041, iter [00790, 01251], lr: 0.000444, loss: 0.4128
2022-10-03 11:09:53 - train: epoch 0041, iter [00800, 01251], lr: 0.000444, loss: 0.4057
2022-10-03 11:10:12 - train: epoch 0041, iter [00810, 01251], lr: 0.000444, loss: 0.4105
2022-10-03 11:10:31 - train: epoch 0041, iter [00820, 01251], lr: 0.000444, loss: 0.4017
2022-10-03 11:10:50 - train: epoch 0041, iter [00830, 01251], lr: 0.000444, loss: 0.4218
2022-10-03 11:11:09 - train: epoch 0041, iter [00840, 01251], lr: 0.000444, loss: 0.4266
2022-10-03 11:11:28 - train: epoch 0041, iter [00850, 01251], lr: 0.000444, loss: 0.4159
2022-10-03 11:11:47 - train: epoch 0041, iter [00860, 01251], lr: 0.000444, loss: 0.4210
2022-10-03 11:12:06 - train: epoch 0041, iter [00870, 01251], lr: 0.000444, loss: 0.4068
2022-10-03 11:12:25 - train: epoch 0041, iter [00880, 01251], lr: 0.000444, loss: 0.4289
2022-10-03 11:12:44 - train: epoch 0041, iter [00890, 01251], lr: 0.000444, loss: 0.4203
2022-10-03 11:13:03 - train: epoch 0041, iter [00900, 01251], lr: 0.000443, loss: 0.4190
2022-10-03 11:13:22 - train: epoch 0041, iter [00910, 01251], lr: 0.000443, loss: 0.4097
2022-10-03 11:13:40 - train: epoch 0041, iter [00920, 01251], lr: 0.000443, loss: 0.4105
2022-10-03 11:13:59 - train: epoch 0041, iter [00930, 01251], lr: 0.000443, loss: 0.3889
2022-10-03 11:14:18 - train: epoch 0041, iter [00940, 01251], lr: 0.000443, loss: 0.4218
2022-10-03 11:14:37 - train: epoch 0041, iter [00950, 01251], lr: 0.000443, loss: 0.3998
2022-10-03 11:14:56 - train: epoch 0041, iter [00960, 01251], lr: 0.000443, loss: 0.4124
2022-10-03 11:15:15 - train: epoch 0041, iter [00970, 01251], lr: 0.000443, loss: 0.4232
2022-10-03 11:15:34 - train: epoch 0041, iter [00980, 01251], lr: 0.000443, loss: 0.4398
2022-10-03 11:15:53 - train: epoch 0041, iter [00990, 01251], lr: 0.000443, loss: 0.4121
2022-10-03 11:16:12 - train: epoch 0041, iter [01000, 01251], lr: 0.000443, loss: 0.4060
2022-10-03 11:16:32 - train: epoch 0041, iter [01010, 01251], lr: 0.000443, loss: 0.4152
2022-10-03 11:16:51 - train: epoch 0041, iter [01020, 01251], lr: 0.000443, loss: 0.4337
2022-10-03 11:17:10 - train: epoch 0041, iter [01030, 01251], lr: 0.000442, loss: 0.4226
2022-10-03 11:17:28 - train: epoch 0041, iter [01040, 01251], lr: 0.000442, loss: 0.4223
2022-10-03 11:17:48 - train: epoch 0041, iter [01050, 01251], lr: 0.000442, loss: 0.3925
2022-10-03 11:18:07 - train: epoch 0041, iter [01060, 01251], lr: 0.000442, loss: 0.4091
2022-10-03 11:18:26 - train: epoch 0041, iter [01070, 01251], lr: 0.000442, loss: 0.4060
2022-10-03 11:18:45 - train: epoch 0041, iter [01080, 01251], lr: 0.000442, loss: 0.4186
2022-10-03 11:19:04 - train: epoch 0041, iter [01090, 01251], lr: 0.000442, loss: 0.4226
2022-10-03 11:19:23 - train: epoch 0041, iter [01100, 01251], lr: 0.000442, loss: 0.3952
2022-10-03 11:19:41 - train: epoch 0041, iter [01110, 01251], lr: 0.000442, loss: 0.4045
2022-10-03 11:20:00 - train: epoch 0041, iter [01120, 01251], lr: 0.000442, loss: 0.4193
2022-10-03 11:20:19 - train: epoch 0041, iter [01130, 01251], lr: 0.000442, loss: 0.4239
2022-10-03 11:20:38 - train: epoch 0041, iter [01140, 01251], lr: 0.000442, loss: 0.4238
2022-10-03 11:20:57 - train: epoch 0041, iter [01150, 01251], lr: 0.000442, loss: 0.4090
2022-10-03 11:21:16 - train: epoch 0041, iter [01160, 01251], lr: 0.000442, loss: 0.4180
2022-10-03 11:21:35 - train: epoch 0041, iter [01170, 01251], lr: 0.000441, loss: 0.3994
2022-10-03 11:21:54 - train: epoch 0041, iter [01180, 01251], lr: 0.000441, loss: 0.4292
2022-10-03 11:22:13 - train: epoch 0041, iter [01190, 01251], lr: 0.000441, loss: 0.4165
2022-10-03 11:22:32 - train: epoch 0041, iter [01200, 01251], lr: 0.000441, loss: 0.4166
2022-10-03 11:22:52 - train: epoch 0041, iter [01210, 01251], lr: 0.000441, loss: 0.4110
2022-10-03 11:23:11 - train: epoch 0041, iter [01220, 01251], lr: 0.000441, loss: 0.4263
2022-10-03 11:23:30 - train: epoch 0041, iter [01230, 01251], lr: 0.000441, loss: 0.4129
2022-10-03 11:23:49 - train: epoch 0041, iter [01240, 01251], lr: 0.000441, loss: 0.3940
2022-10-03 11:24:07 - train: epoch 0041, iter [01250, 01251], lr: 0.000441, loss: 0.4221
2022-10-03 11:24:12 - train: epoch 041, train_loss: 0.4139
2022-10-03 11:24:16 - until epoch: 041, best_loss: 0.4139
2022-10-03 11:24:16 - epoch 042 lr: 0.000441
2022-10-03 11:24:40 - train: epoch 0042, iter [00010, 01251], lr: 0.000441, loss: 0.4080
2022-10-03 11:24:59 - train: epoch 0042, iter [00020, 01251], lr: 0.000441, loss: 0.4120
2022-10-03 11:25:18 - train: epoch 0042, iter [00030, 01251], lr: 0.000441, loss: 0.4041
2022-10-03 11:25:37 - train: epoch 0042, iter [00040, 01251], lr: 0.000441, loss: 0.3953
2022-10-03 11:25:55 - train: epoch 0042, iter [00050, 01251], lr: 0.000440, loss: 0.4048
2022-10-03 11:26:14 - train: epoch 0042, iter [00060, 01251], lr: 0.000440, loss: 0.4104
2022-10-03 11:26:33 - train: epoch 0042, iter [00070, 01251], lr: 0.000440, loss: 0.4021
2022-10-03 11:26:51 - train: epoch 0042, iter [00080, 01251], lr: 0.000440, loss: 0.4051
2022-10-03 11:27:10 - train: epoch 0042, iter [00090, 01251], lr: 0.000440, loss: 0.3949
2022-10-03 11:27:29 - train: epoch 0042, iter [00100, 01251], lr: 0.000440, loss: 0.3963
2022-10-03 11:27:48 - train: epoch 0042, iter [00110, 01251], lr: 0.000440, loss: 0.3989
2022-10-03 11:28:06 - train: epoch 0042, iter [00120, 01251], lr: 0.000440, loss: 0.4190
2022-10-03 11:28:25 - train: epoch 0042, iter [00130, 01251], lr: 0.000440, loss: 0.4227
2022-10-03 11:28:44 - train: epoch 0042, iter [00140, 01251], lr: 0.000440, loss: 0.4096
2022-10-03 11:29:03 - train: epoch 0042, iter [00150, 01251], lr: 0.000440, loss: 0.4103
2022-10-03 11:29:22 - train: epoch 0042, iter [00160, 01251], lr: 0.000440, loss: 0.4033
2022-10-03 11:29:41 - train: epoch 0042, iter [00170, 01251], lr: 0.000440, loss: 0.4242
2022-10-03 11:30:00 - train: epoch 0042, iter [00180, 01251], lr: 0.000440, loss: 0.4098
2022-10-03 11:30:19 - train: epoch 0042, iter [00190, 01251], lr: 0.000439, loss: 0.4285
2022-10-03 11:30:38 - train: epoch 0042, iter [00200, 01251], lr: 0.000439, loss: 0.4146
2022-10-03 11:30:56 - train: epoch 0042, iter [00210, 01251], lr: 0.000439, loss: 0.4084
2022-10-03 11:31:15 - train: epoch 0042, iter [00220, 01251], lr: 0.000439, loss: 0.4162
2022-10-03 11:31:34 - train: epoch 0042, iter [00230, 01251], lr: 0.000439, loss: 0.4156
2022-10-03 11:31:52 - train: epoch 0042, iter [00240, 01251], lr: 0.000439, loss: 0.4071
2022-10-03 11:32:11 - train: epoch 0042, iter [00250, 01251], lr: 0.000439, loss: 0.4208
2022-10-03 11:32:30 - train: epoch 0042, iter [00260, 01251], lr: 0.000439, loss: 0.4030
2022-10-03 11:32:49 - train: epoch 0042, iter [00270, 01251], lr: 0.000439, loss: 0.4176
2022-10-03 11:33:08 - train: epoch 0042, iter [00280, 01251], lr: 0.000439, loss: 0.4277
2022-10-03 11:33:26 - train: epoch 0042, iter [00290, 01251], lr: 0.000439, loss: 0.4160
2022-10-03 11:33:45 - train: epoch 0042, iter [00300, 01251], lr: 0.000439, loss: 0.4027
2022-10-03 11:34:04 - train: epoch 0042, iter [00310, 01251], lr: 0.000439, loss: 0.3940
2022-10-03 11:34:23 - train: epoch 0042, iter [00320, 01251], lr: 0.000438, loss: 0.4027
2022-10-03 11:34:42 - train: epoch 0042, iter [00330, 01251], lr: 0.000438, loss: 0.4434
2022-10-03 11:35:00 - train: epoch 0042, iter [00340, 01251], lr: 0.000438, loss: 0.4012
2022-10-03 11:35:19 - train: epoch 0042, iter [00350, 01251], lr: 0.000438, loss: 0.4235
2022-10-03 11:35:37 - train: epoch 0042, iter [00360, 01251], lr: 0.000438, loss: 0.4204
2022-10-03 11:35:56 - train: epoch 0042, iter [00370, 01251], lr: 0.000438, loss: 0.4133
2022-10-03 11:36:14 - train: epoch 0042, iter [00380, 01251], lr: 0.000438, loss: 0.4177
2022-10-03 11:36:33 - train: epoch 0042, iter [00390, 01251], lr: 0.000438, loss: 0.4057
2022-10-03 11:36:51 - train: epoch 0042, iter [00400, 01251], lr: 0.000438, loss: 0.4284
2022-10-03 11:37:10 - train: epoch 0042, iter [00410, 01251], lr: 0.000438, loss: 0.4046
2022-10-03 11:37:29 - train: epoch 0042, iter [00420, 01251], lr: 0.000438, loss: 0.4127
2022-10-03 11:37:47 - train: epoch 0042, iter [00430, 01251], lr: 0.000438, loss: 0.4128
2022-10-03 11:38:06 - train: epoch 0042, iter [00440, 01251], lr: 0.000438, loss: 0.4272
2022-10-03 11:38:25 - train: epoch 0042, iter [00450, 01251], lr: 0.000438, loss: 0.4130
2022-10-03 11:38:43 - train: epoch 0042, iter [00460, 01251], lr: 0.000437, loss: 0.3974
2022-10-03 11:39:03 - train: epoch 0042, iter [00470, 01251], lr: 0.000437, loss: 0.4124
2022-10-03 11:39:21 - train: epoch 0042, iter [00480, 01251], lr: 0.000437, loss: 0.4137
2022-10-03 11:39:40 - train: epoch 0042, iter [00490, 01251], lr: 0.000437, loss: 0.4095
2022-10-03 11:39:59 - train: epoch 0042, iter [00500, 01251], lr: 0.000437, loss: 0.4010
2022-10-03 11:40:17 - train: epoch 0042, iter [00510, 01251], lr: 0.000437, loss: 0.4065
2022-10-03 11:40:35 - train: epoch 0042, iter [00520, 01251], lr: 0.000437, loss: 0.4122
2022-10-03 11:40:54 - train: epoch 0042, iter [00530, 01251], lr: 0.000437, loss: 0.4066
2022-10-03 11:41:13 - train: epoch 0042, iter [00540, 01251], lr: 0.000437, loss: 0.4025
2022-10-03 11:41:32 - train: epoch 0042, iter [00550, 01251], lr: 0.000437, loss: 0.4090
2022-10-03 11:41:51 - train: epoch 0042, iter [00560, 01251], lr: 0.000437, loss: 0.4093
2022-10-03 11:42:10 - train: epoch 0042, iter [00570, 01251], lr: 0.000437, loss: 0.4183
2022-10-03 11:42:29 - train: epoch 0042, iter [00580, 01251], lr: 0.000437, loss: 0.4054
2022-10-03 11:42:47 - train: epoch 0042, iter [00590, 01251], lr: 0.000436, loss: 0.4095
2022-10-03 11:43:06 - train: epoch 0042, iter [00600, 01251], lr: 0.000436, loss: 0.4028
2022-10-03 11:43:25 - train: epoch 0042, iter [00610, 01251], lr: 0.000436, loss: 0.4110
2022-10-03 11:43:44 - train: epoch 0042, iter [00620, 01251], lr: 0.000436, loss: 0.4159
2022-10-03 11:44:02 - train: epoch 0042, iter [00630, 01251], lr: 0.000436, loss: 0.4093
2022-10-03 11:44:21 - train: epoch 0042, iter [00640, 01251], lr: 0.000436, loss: 0.4069
2022-10-03 11:44:39 - train: epoch 0042, iter [00650, 01251], lr: 0.000436, loss: 0.3992
2022-10-03 11:44:58 - train: epoch 0042, iter [00660, 01251], lr: 0.000436, loss: 0.4024
2022-10-03 11:45:17 - train: epoch 0042, iter [00670, 01251], lr: 0.000436, loss: 0.4123
2022-10-03 11:45:36 - train: epoch 0042, iter [00680, 01251], lr: 0.000436, loss: 0.4172
2022-10-03 11:45:55 - train: epoch 0042, iter [00690, 01251], lr: 0.000436, loss: 0.4319
2022-10-03 11:46:14 - train: epoch 0042, iter [00700, 01251], lr: 0.000436, loss: 0.4405
2022-10-03 11:46:33 - train: epoch 0042, iter [00710, 01251], lr: 0.000436, loss: 0.4067
2022-10-03 11:46:52 - train: epoch 0042, iter [00720, 01251], lr: 0.000435, loss: 0.4092
2022-10-03 11:47:11 - train: epoch 0042, iter [00730, 01251], lr: 0.000435, loss: 0.4148
2022-10-03 11:47:29 - train: epoch 0042, iter [00740, 01251], lr: 0.000435, loss: 0.3995
2022-10-03 11:47:49 - train: epoch 0042, iter [00750, 01251], lr: 0.000435, loss: 0.4173
2022-10-03 11:48:07 - train: epoch 0042, iter [00760, 01251], lr: 0.000435, loss: 0.4150
2022-10-03 11:48:26 - train: epoch 0042, iter [00770, 01251], lr: 0.000435, loss: 0.4205
2022-10-03 11:48:44 - train: epoch 0042, iter [00780, 01251], lr: 0.000435, loss: 0.4243
2022-10-03 11:49:02 - train: epoch 0042, iter [00790, 01251], lr: 0.000435, loss: 0.4189
2022-10-03 11:49:21 - train: epoch 0042, iter [00800, 01251], lr: 0.000435, loss: 0.4031
2022-10-03 11:49:40 - train: epoch 0042, iter [00810, 01251], lr: 0.000435, loss: 0.3982
2022-10-03 11:49:59 - train: epoch 0042, iter [00820, 01251], lr: 0.000435, loss: 0.4119
2022-10-03 11:50:18 - train: epoch 0042, iter [00830, 01251], lr: 0.000435, loss: 0.4163
2022-10-03 11:50:37 - train: epoch 0042, iter [00840, 01251], lr: 0.000435, loss: 0.4045
2022-10-03 11:50:55 - train: epoch 0042, iter [00850, 01251], lr: 0.000435, loss: 0.4318
2022-10-03 11:51:14 - train: epoch 0042, iter [00860, 01251], lr: 0.000434, loss: 0.3890
2022-10-03 11:51:32 - train: epoch 0042, iter [00870, 01251], lr: 0.000434, loss: 0.4320
2022-10-03 11:51:51 - train: epoch 0042, iter [00880, 01251], lr: 0.000434, loss: 0.4086
2022-10-03 11:52:09 - train: epoch 0042, iter [00890, 01251], lr: 0.000434, loss: 0.4178
2022-10-03 11:52:28 - train: epoch 0042, iter [00900, 01251], lr: 0.000434, loss: 0.4228
2022-10-03 11:52:47 - train: epoch 0042, iter [00910, 01251], lr: 0.000434, loss: 0.4056
2022-10-03 11:53:06 - train: epoch 0042, iter [00920, 01251], lr: 0.000434, loss: 0.4113
2022-10-03 11:53:25 - train: epoch 0042, iter [00930, 01251], lr: 0.000434, loss: 0.4174
2022-10-03 11:53:43 - train: epoch 0042, iter [00940, 01251], lr: 0.000434, loss: 0.4199
2022-10-03 11:54:02 - train: epoch 0042, iter [00950, 01251], lr: 0.000434, loss: 0.3995
2022-10-03 11:54:21 - train: epoch 0042, iter [00960, 01251], lr: 0.000434, loss: 0.4066
2022-10-03 11:54:40 - train: epoch 0042, iter [00970, 01251], lr: 0.000434, loss: 0.4154
2022-10-03 11:54:59 - train: epoch 0042, iter [00980, 01251], lr: 0.000434, loss: 0.4049
2022-10-03 11:55:17 - train: epoch 0042, iter [00990, 01251], lr: 0.000433, loss: 0.4015
2022-10-03 11:55:36 - train: epoch 0042, iter [01000, 01251], lr: 0.000433, loss: 0.4269
2022-10-03 11:55:55 - train: epoch 0042, iter [01010, 01251], lr: 0.000433, loss: 0.3990
2022-10-03 11:56:14 - train: epoch 0042, iter [01020, 01251], lr: 0.000433, loss: 0.3941
2022-10-03 11:56:32 - train: epoch 0042, iter [01030, 01251], lr: 0.000433, loss: 0.4147
2022-10-03 11:56:51 - train: epoch 0042, iter [01040, 01251], lr: 0.000433, loss: 0.4204
2022-10-03 11:57:10 - train: epoch 0042, iter [01050, 01251], lr: 0.000433, loss: 0.4088
2022-10-03 11:57:28 - train: epoch 0042, iter [01060, 01251], lr: 0.000433, loss: 0.4106
2022-10-03 11:57:47 - train: epoch 0042, iter [01070, 01251], lr: 0.000433, loss: 0.4198
2022-10-03 11:58:05 - train: epoch 0042, iter [01080, 01251], lr: 0.000433, loss: 0.4249
2022-10-03 11:58:24 - train: epoch 0042, iter [01090, 01251], lr: 0.000433, loss: 0.3984
2022-10-03 11:58:43 - train: epoch 0042, iter [01100, 01251], lr: 0.000433, loss: 0.4116
2022-10-03 11:59:01 - train: epoch 0042, iter [01110, 01251], lr: 0.000433, loss: 0.4172
2022-10-03 11:59:20 - train: epoch 0042, iter [01120, 01251], lr: 0.000432, loss: 0.4097
2022-10-03 11:59:39 - train: epoch 0042, iter [01130, 01251], lr: 0.000432, loss: 0.3974
2022-10-03 11:59:57 - train: epoch 0042, iter [01140, 01251], lr: 0.000432, loss: 0.4161
2022-10-03 12:00:16 - train: epoch 0042, iter [01150, 01251], lr: 0.000432, loss: 0.4081
2022-10-03 12:00:35 - train: epoch 0042, iter [01160, 01251], lr: 0.000432, loss: 0.4146
2022-10-03 12:00:54 - train: epoch 0042, iter [01170, 01251], lr: 0.000432, loss: 0.4098
2022-10-03 12:01:12 - train: epoch 0042, iter [01180, 01251], lr: 0.000432, loss: 0.4004
2022-10-03 12:01:31 - train: epoch 0042, iter [01190, 01251], lr: 0.000432, loss: 0.4013
2022-10-03 12:01:50 - train: epoch 0042, iter [01200, 01251], lr: 0.000432, loss: 0.4127
2022-10-03 12:02:08 - train: epoch 0042, iter [01210, 01251], lr: 0.000432, loss: 0.4084
2022-10-03 12:02:27 - train: epoch 0042, iter [01220, 01251], lr: 0.000432, loss: 0.4218
2022-10-03 12:02:46 - train: epoch 0042, iter [01230, 01251], lr: 0.000432, loss: 0.4467
2022-10-03 12:03:05 - train: epoch 0042, iter [01240, 01251], lr: 0.000432, loss: 0.4153
2022-10-03 12:03:22 - train: epoch 0042, iter [01250, 01251], lr: 0.000432, loss: 0.4013
2022-10-03 12:03:27 - train: epoch 042, train_loss: 0.4136
2022-10-03 12:03:31 - until epoch: 042, best_loss: 0.4136
2022-10-03 12:03:31 - epoch 043 lr: 0.000432
2022-10-03 12:03:55 - train: epoch 0043, iter [00010, 01251], lr: 0.000431, loss: 0.4180
2022-10-03 12:04:14 - train: epoch 0043, iter [00020, 01251], lr: 0.000431, loss: 0.3935
2022-10-03 12:04:33 - train: epoch 0043, iter [00030, 01251], lr: 0.000431, loss: 0.4127
2022-10-03 12:04:51 - train: epoch 0043, iter [00040, 01251], lr: 0.000431, loss: 0.4272
2022-10-03 12:05:10 - train: epoch 0043, iter [00050, 01251], lr: 0.000431, loss: 0.4046
2022-10-03 12:05:29 - train: epoch 0043, iter [00060, 01251], lr: 0.000431, loss: 0.4067
2022-10-03 12:05:47 - train: epoch 0043, iter [00070, 01251], lr: 0.000431, loss: 0.4366
2022-10-03 12:06:06 - train: epoch 0043, iter [00080, 01251], lr: 0.000431, loss: 0.4045
2022-10-03 12:06:25 - train: epoch 0043, iter [00090, 01251], lr: 0.000431, loss: 0.3996
2022-10-03 12:06:44 - train: epoch 0043, iter [00100, 01251], lr: 0.000431, loss: 0.4233
2022-10-03 12:07:02 - train: epoch 0043, iter [00110, 01251], lr: 0.000431, loss: 0.4330
2022-10-03 12:07:21 - train: epoch 0043, iter [00120, 01251], lr: 0.000431, loss: 0.4124
2022-10-03 12:07:40 - train: epoch 0043, iter [00130, 01251], lr: 0.000431, loss: 0.4259
2022-10-03 12:07:58 - train: epoch 0043, iter [00140, 01251], lr: 0.000430, loss: 0.4234
2022-10-03 12:08:17 - train: epoch 0043, iter [00150, 01251], lr: 0.000430, loss: 0.4271
2022-10-03 12:08:35 - train: epoch 0043, iter [00160, 01251], lr: 0.000430, loss: 0.4220
2022-10-03 12:08:54 - train: epoch 0043, iter [00170, 01251], lr: 0.000430, loss: 0.3999
2022-10-03 12:09:13 - train: epoch 0043, iter [00180, 01251], lr: 0.000430, loss: 0.4040
2022-10-03 12:09:32 - train: epoch 0043, iter [00190, 01251], lr: 0.000430, loss: 0.4056
2022-10-03 12:09:51 - train: epoch 0043, iter [00200, 01251], lr: 0.000430, loss: 0.3985
2022-10-03 12:10:10 - train: epoch 0043, iter [00210, 01251], lr: 0.000430, loss: 0.4306
2022-10-03 12:10:28 - train: epoch 0043, iter [00220, 01251], lr: 0.000430, loss: 0.4197
2022-10-03 12:10:47 - train: epoch 0043, iter [00230, 01251], lr: 0.000430, loss: 0.4047
2022-10-03 12:11:06 - train: epoch 0043, iter [00240, 01251], lr: 0.000430, loss: 0.4211
2022-10-03 12:11:25 - train: epoch 0043, iter [00250, 01251], lr: 0.000430, loss: 0.4178
2022-10-03 12:11:43 - train: epoch 0043, iter [00260, 01251], lr: 0.000430, loss: 0.4122
2022-10-03 12:12:02 - train: epoch 0043, iter [00270, 01251], lr: 0.000429, loss: 0.4278
2022-10-03 12:12:20 - train: epoch 0043, iter [00280, 01251], lr: 0.000429, loss: 0.4229
2022-10-03 12:12:39 - train: epoch 0043, iter [00290, 01251], lr: 0.000429, loss: 0.4340
2022-10-03 12:12:58 - train: epoch 0043, iter [00300, 01251], lr: 0.000429, loss: 0.4027
2022-10-03 12:13:17 - train: epoch 0043, iter [00310, 01251], lr: 0.000429, loss: 0.4157
2022-10-03 12:13:35 - train: epoch 0043, iter [00320, 01251], lr: 0.000429, loss: 0.4191
2022-10-03 12:13:54 - train: epoch 0043, iter [00330, 01251], lr: 0.000429, loss: 0.4099
2022-10-03 12:14:13 - train: epoch 0043, iter [00340, 01251], lr: 0.000429, loss: 0.4252
2022-10-03 12:14:32 - train: epoch 0043, iter [00350, 01251], lr: 0.000429, loss: 0.4152
2022-10-03 12:14:51 - train: epoch 0043, iter [00360, 01251], lr: 0.000429, loss: 0.4139
2022-10-03 12:15:09 - train: epoch 0043, iter [00370, 01251], lr: 0.000429, loss: 0.4088
2022-10-03 12:15:28 - train: epoch 0043, iter [00380, 01251], lr: 0.000429, loss: 0.4100
2022-10-03 12:15:47 - train: epoch 0043, iter [00390, 01251], lr: 0.000429, loss: 0.4091
2022-10-03 12:16:05 - train: epoch 0043, iter [00400, 01251], lr: 0.000428, loss: 0.4200
2022-10-03 12:16:24 - train: epoch 0043, iter [00410, 01251], lr: 0.000428, loss: 0.3929
2022-10-03 12:16:42 - train: epoch 0043, iter [00420, 01251], lr: 0.000428, loss: 0.4159
2022-10-03 12:17:01 - train: epoch 0043, iter [00430, 01251], lr: 0.000428, loss: 0.4019
2022-10-03 12:17:20 - train: epoch 0043, iter [00440, 01251], lr: 0.000428, loss: 0.4164
2022-10-03 12:17:38 - train: epoch 0043, iter [00450, 01251], lr: 0.000428, loss: 0.4137
2022-10-03 12:17:57 - train: epoch 0043, iter [00460, 01251], lr: 0.000428, loss: 0.4212
2022-10-03 12:18:15 - train: epoch 0043, iter [00470, 01251], lr: 0.000428, loss: 0.4117
2022-10-03 12:18:34 - train: epoch 0043, iter [00480, 01251], lr: 0.000428, loss: 0.4075
2022-10-03 12:18:53 - train: epoch 0043, iter [00490, 01251], lr: 0.000428, loss: 0.4191
2022-10-03 12:19:12 - train: epoch 0043, iter [00500, 01251], lr: 0.000428, loss: 0.4330
2022-10-03 12:19:30 - train: epoch 0043, iter [00510, 01251], lr: 0.000428, loss: 0.4175
2022-10-03 12:19:49 - train: epoch 0043, iter [00520, 01251], lr: 0.000428, loss: 0.3967
2022-10-03 12:20:07 - train: epoch 0043, iter [00530, 01251], lr: 0.000428, loss: 0.4120
2022-10-03 12:20:26 - train: epoch 0043, iter [00540, 01251], lr: 0.000427, loss: 0.4036
2022-10-03 12:20:44 - train: epoch 0043, iter [00550, 01251], lr: 0.000427, loss: 0.4142
2022-10-03 12:21:03 - train: epoch 0043, iter [00560, 01251], lr: 0.000427, loss: 0.4243
2022-10-03 12:21:21 - train: epoch 0043, iter [00570, 01251], lr: 0.000427, loss: 0.4117
2022-10-03 12:21:40 - train: epoch 0043, iter [00580, 01251], lr: 0.000427, loss: 0.4098
2022-10-03 12:21:58 - train: epoch 0043, iter [00590, 01251], lr: 0.000427, loss: 0.4171
2022-10-03 12:22:17 - train: epoch 0043, iter [00600, 01251], lr: 0.000427, loss: 0.3998
2022-10-03 12:22:36 - train: epoch 0043, iter [00610, 01251], lr: 0.000427, loss: 0.4083
2022-10-03 12:22:54 - train: epoch 0043, iter [00620, 01251], lr: 0.000427, loss: 0.4265
2022-10-03 12:23:13 - train: epoch 0043, iter [00630, 01251], lr: 0.000427, loss: 0.3994
2022-10-03 12:23:32 - train: epoch 0043, iter [00640, 01251], lr: 0.000427, loss: 0.4217
2022-10-03 12:23:51 - train: epoch 0043, iter [00650, 01251], lr: 0.000427, loss: 0.4180
2022-10-03 12:24:10 - train: epoch 0043, iter [00660, 01251], lr: 0.000427, loss: 0.4167
2022-10-03 12:24:28 - train: epoch 0043, iter [00670, 01251], lr: 0.000426, loss: 0.4196
2022-10-03 12:24:47 - train: epoch 0043, iter [00680, 01251], lr: 0.000426, loss: 0.4163
2022-10-03 12:25:06 - train: epoch 0043, iter [00690, 01251], lr: 0.000426, loss: 0.4043
2022-10-03 12:25:24 - train: epoch 0043, iter [00700, 01251], lr: 0.000426, loss: 0.4042
2022-10-03 12:25:43 - train: epoch 0043, iter [00710, 01251], lr: 0.000426, loss: 0.4114
2022-10-03 12:26:01 - train: epoch 0043, iter [00720, 01251], lr: 0.000426, loss: 0.4160
2022-10-03 12:26:20 - train: epoch 0043, iter [00730, 01251], lr: 0.000426, loss: 0.4225
2022-10-03 12:26:39 - train: epoch 0043, iter [00740, 01251], lr: 0.000426, loss: 0.4135
2022-10-03 12:26:58 - train: epoch 0043, iter [00750, 01251], lr: 0.000426, loss: 0.4117
2022-10-03 12:27:17 - train: epoch 0043, iter [00760, 01251], lr: 0.000426, loss: 0.4074
2022-10-03 12:27:35 - train: epoch 0043, iter [00770, 01251], lr: 0.000426, loss: 0.3990
2022-10-03 12:27:55 - train: epoch 0043, iter [00780, 01251], lr: 0.000426, loss: 0.4193
2022-10-03 12:28:14 - train: epoch 0043, iter [00790, 01251], lr: 0.000426, loss: 0.4002
2022-10-03 12:28:32 - train: epoch 0043, iter [00800, 01251], lr: 0.000425, loss: 0.4074
2022-10-03 12:28:51 - train: epoch 0043, iter [00810, 01251], lr: 0.000425, loss: 0.4157
2022-10-03 12:29:10 - train: epoch 0043, iter [00820, 01251], lr: 0.000425, loss: 0.4138
2022-10-03 12:29:29 - train: epoch 0043, iter [00830, 01251], lr: 0.000425, loss: 0.4003
2022-10-03 12:29:47 - train: epoch 0043, iter [00840, 01251], lr: 0.000425, loss: 0.3858
2022-10-03 12:30:06 - train: epoch 0043, iter [00850, 01251], lr: 0.000425, loss: 0.3949
2022-10-03 12:30:24 - train: epoch 0043, iter [00860, 01251], lr: 0.000425, loss: 0.4215
2022-10-03 12:30:43 - train: epoch 0043, iter [00870, 01251], lr: 0.000425, loss: 0.3892
2022-10-03 12:31:01 - train: epoch 0043, iter [00880, 01251], lr: 0.000425, loss: 0.4108
2022-10-03 12:31:21 - train: epoch 0043, iter [00890, 01251], lr: 0.000425, loss: 0.4160
2022-10-03 12:31:39 - train: epoch 0043, iter [00900, 01251], lr: 0.000425, loss: 0.3926
2022-10-03 12:31:58 - train: epoch 0043, iter [00910, 01251], lr: 0.000425, loss: 0.4033
2022-10-03 12:32:17 - train: epoch 0043, iter [00920, 01251], lr: 0.000425, loss: 0.4096
2022-10-03 12:32:35 - train: epoch 0043, iter [00930, 01251], lr: 0.000424, loss: 0.4052
2022-10-03 12:32:54 - train: epoch 0043, iter [00940, 01251], lr: 0.000424, loss: 0.4071
2022-10-03 12:33:13 - train: epoch 0043, iter [00950, 01251], lr: 0.000424, loss: 0.3974
2022-10-03 12:33:31 - train: epoch 0043, iter [00960, 01251], lr: 0.000424, loss: 0.4097
2022-10-03 12:33:50 - train: epoch 0043, iter [00970, 01251], lr: 0.000424, loss: 0.4282
2022-10-03 12:34:09 - train: epoch 0043, iter [00980, 01251], lr: 0.000424, loss: 0.4341
2022-10-03 12:34:28 - train: epoch 0043, iter [00990, 01251], lr: 0.000424, loss: 0.3960
2022-10-03 12:34:47 - train: epoch 0043, iter [01000, 01251], lr: 0.000424, loss: 0.4143
2022-10-03 12:35:05 - train: epoch 0043, iter [01010, 01251], lr: 0.000424, loss: 0.4357
2022-10-03 12:35:24 - train: epoch 0043, iter [01020, 01251], lr: 0.000424, loss: 0.3922
2022-10-03 12:35:42 - train: epoch 0043, iter [01030, 01251], lr: 0.000424, loss: 0.4161
2022-10-03 12:36:01 - train: epoch 0043, iter [01040, 01251], lr: 0.000424, loss: 0.4205
2022-10-03 12:36:20 - train: epoch 0043, iter [01050, 01251], lr: 0.000424, loss: 0.4327
2022-10-03 12:36:39 - train: epoch 0043, iter [01060, 01251], lr: 0.000423, loss: 0.4221
2022-10-03 12:36:57 - train: epoch 0043, iter [01070, 01251], lr: 0.000423, loss: 0.4027
2022-10-03 12:37:16 - train: epoch 0043, iter [01080, 01251], lr: 0.000423, loss: 0.4181
2022-10-03 12:37:35 - train: epoch 0043, iter [01090, 01251], lr: 0.000423, loss: 0.4089
2022-10-03 12:37:53 - train: epoch 0043, iter [01100, 01251], lr: 0.000423, loss: 0.4101
2022-10-03 12:38:12 - train: epoch 0043, iter [01110, 01251], lr: 0.000423, loss: 0.4023
2022-10-03 12:38:30 - train: epoch 0043, iter [01120, 01251], lr: 0.000423, loss: 0.4252
2022-10-03 12:38:49 - train: epoch 0043, iter [01130, 01251], lr: 0.000423, loss: 0.3935
2022-10-03 12:39:08 - train: epoch 0043, iter [01140, 01251], lr: 0.000423, loss: 0.4209
2022-10-03 12:39:26 - train: epoch 0043, iter [01150, 01251], lr: 0.000423, loss: 0.4134
2022-10-03 12:39:45 - train: epoch 0043, iter [01160, 01251], lr: 0.000423, loss: 0.4044
2022-10-03 12:40:04 - train: epoch 0043, iter [01170, 01251], lr: 0.000423, loss: 0.4038
2022-10-03 12:40:22 - train: epoch 0043, iter [01180, 01251], lr: 0.000423, loss: 0.4049
2022-10-03 12:40:41 - train: epoch 0043, iter [01190, 01251], lr: 0.000422, loss: 0.4009
2022-10-03 12:41:00 - train: epoch 0043, iter [01200, 01251], lr: 0.000422, loss: 0.4313
2022-10-03 12:41:18 - train: epoch 0043, iter [01210, 01251], lr: 0.000422, loss: 0.4163
2022-10-03 12:41:37 - train: epoch 0043, iter [01220, 01251], lr: 0.000422, loss: 0.4114
2022-10-03 12:41:56 - train: epoch 0043, iter [01230, 01251], lr: 0.000422, loss: 0.4195
2022-10-03 12:42:15 - train: epoch 0043, iter [01240, 01251], lr: 0.000422, loss: 0.4165
2022-10-03 12:42:33 - train: epoch 0043, iter [01250, 01251], lr: 0.000422, loss: 0.4097
2022-10-03 12:42:38 - train: epoch 043, train_loss: 0.4132
2022-10-03 12:42:42 - until epoch: 043, best_loss: 0.4132
2022-10-03 12:42:42 - epoch 044 lr: 0.000422
2022-10-03 12:43:07 - train: epoch 0044, iter [00010, 01251], lr: 0.000422, loss: 0.4295
2022-10-03 12:43:25 - train: epoch 0044, iter [00020, 01251], lr: 0.000422, loss: 0.4052
2022-10-03 12:43:44 - train: epoch 0044, iter [00030, 01251], lr: 0.000422, loss: 0.4192
2022-10-03 12:44:03 - train: epoch 0044, iter [00040, 01251], lr: 0.000422, loss: 0.4110
2022-10-03 12:44:22 - train: epoch 0044, iter [00050, 01251], lr: 0.000422, loss: 0.3965
2022-10-03 12:44:41 - train: epoch 0044, iter [00060, 01251], lr: 0.000422, loss: 0.4087
2022-10-03 12:44:59 - train: epoch 0044, iter [00070, 01251], lr: 0.000421, loss: 0.4153
2022-10-03 12:45:18 - train: epoch 0044, iter [00080, 01251], lr: 0.000421, loss: 0.4145
2022-10-03 12:45:38 - train: epoch 0044, iter [00090, 01251], lr: 0.000421, loss: 0.4348
2022-10-03 12:45:57 - train: epoch 0044, iter [00100, 01251], lr: 0.000421, loss: 0.4069
2022-10-03 12:46:16 - train: epoch 0044, iter [00110, 01251], lr: 0.000421, loss: 0.4001
2022-10-03 12:46:35 - train: epoch 0044, iter [00120, 01251], lr: 0.000421, loss: 0.3909
2022-10-03 12:46:54 - train: epoch 0044, iter [00130, 01251], lr: 0.000421, loss: 0.4080
2022-10-03 12:47:13 - train: epoch 0044, iter [00140, 01251], lr: 0.000421, loss: 0.4058
2022-10-03 12:47:32 - train: epoch 0044, iter [00150, 01251], lr: 0.000421, loss: 0.4314
2022-10-03 12:47:51 - train: epoch 0044, iter [00160, 01251], lr: 0.000421, loss: 0.4056
2022-10-03 12:48:10 - train: epoch 0044, iter [00170, 01251], lr: 0.000421, loss: 0.4187
2022-10-03 12:48:30 - train: epoch 0044, iter [00180, 01251], lr: 0.000421, loss: 0.4253
2022-10-03 12:48:49 - train: epoch 0044, iter [00190, 01251], lr: 0.000421, loss: 0.4125
2022-10-03 12:49:08 - train: epoch 0044, iter [00200, 01251], lr: 0.000420, loss: 0.4037
2022-10-03 12:49:27 - train: epoch 0044, iter [00210, 01251], lr: 0.000420, loss: 0.4098
2022-10-03 12:49:46 - train: epoch 0044, iter [00220, 01251], lr: 0.000420, loss: 0.4231
2022-10-03 12:50:05 - train: epoch 0044, iter [00230, 01251], lr: 0.000420, loss: 0.4197
2022-10-03 12:50:24 - train: epoch 0044, iter [00240, 01251], lr: 0.000420, loss: 0.4065
2022-10-03 12:50:43 - train: epoch 0044, iter [00250, 01251], lr: 0.000420, loss: 0.4089
2022-10-03 12:51:02 - train: epoch 0044, iter [00260, 01251], lr: 0.000420, loss: 0.4054
2022-10-03 12:51:21 - train: epoch 0044, iter [00270, 01251], lr: 0.000420, loss: 0.4185
2022-10-03 12:51:40 - train: epoch 0044, iter [00280, 01251], lr: 0.000420, loss: 0.4322
2022-10-03 12:51:59 - train: epoch 0044, iter [00290, 01251], lr: 0.000420, loss: 0.4303
2022-10-03 12:52:18 - train: epoch 0044, iter [00300, 01251], lr: 0.000420, loss: 0.4024
2022-10-03 12:52:37 - train: epoch 0044, iter [00310, 01251], lr: 0.000420, loss: 0.3935
2022-10-03 12:52:57 - train: epoch 0044, iter [00320, 01251], lr: 0.000420, loss: 0.4105
2022-10-03 12:53:15 - train: epoch 0044, iter [00330, 01251], lr: 0.000419, loss: 0.4220
2022-10-03 12:53:34 - train: epoch 0044, iter [00340, 01251], lr: 0.000419, loss: 0.3923
2022-10-03 12:53:53 - train: epoch 0044, iter [00350, 01251], lr: 0.000419, loss: 0.4019
2022-10-03 12:54:12 - train: epoch 0044, iter [00360, 01251], lr: 0.000419, loss: 0.4291
2022-10-03 12:54:31 - train: epoch 0044, iter [00370, 01251], lr: 0.000419, loss: 0.4099
2022-10-03 12:54:50 - train: epoch 0044, iter [00380, 01251], lr: 0.000419, loss: 0.4262
2022-10-03 12:55:09 - train: epoch 0044, iter [00390, 01251], lr: 0.000419, loss: 0.4248
2022-10-03 12:55:28 - train: epoch 0044, iter [00400, 01251], lr: 0.000419, loss: 0.4243
2022-10-03 12:55:47 - train: epoch 0044, iter [00410, 01251], lr: 0.000419, loss: 0.4153
2022-10-03 12:56:06 - train: epoch 0044, iter [00420, 01251], lr: 0.000419, loss: 0.4134
2022-10-03 12:56:25 - train: epoch 0044, iter [00430, 01251], lr: 0.000419, loss: 0.4065
2022-10-03 12:56:44 - train: epoch 0044, iter [00440, 01251], lr: 0.000419, loss: 0.4193
2022-10-03 12:57:03 - train: epoch 0044, iter [00450, 01251], lr: 0.000419, loss: 0.4084
2022-10-03 12:57:22 - train: epoch 0044, iter [00460, 01251], lr: 0.000418, loss: 0.4100
2022-10-03 12:57:41 - train: epoch 0044, iter [00470, 01251], lr: 0.000418, loss: 0.3926
2022-10-03 12:57:59 - train: epoch 0044, iter [00480, 01251], lr: 0.000418, loss: 0.3936
2022-10-03 12:58:18 - train: epoch 0044, iter [00490, 01251], lr: 0.000418, loss: 0.3937
2022-10-03 12:58:37 - train: epoch 0044, iter [00500, 01251], lr: 0.000418, loss: 0.4152
2022-10-03 12:58:56 - train: epoch 0044, iter [00510, 01251], lr: 0.000418, loss: 0.4144
2022-10-03 12:59:15 - train: epoch 0044, iter [00520, 01251], lr: 0.000418, loss: 0.4119
2022-10-03 12:59:34 - train: epoch 0044, iter [00530, 01251], lr: 0.000418, loss: 0.4213
2022-10-03 12:59:54 - train: epoch 0044, iter [00540, 01251], lr: 0.000418, loss: 0.4157
2022-10-03 13:00:13 - train: epoch 0044, iter [00550, 01251], lr: 0.000418, loss: 0.4200
2022-10-03 13:00:31 - train: epoch 0044, iter [00560, 01251], lr: 0.000418, loss: 0.4030
2022-10-03 13:00:50 - train: epoch 0044, iter [00570, 01251], lr: 0.000418, loss: 0.4188
2022-10-03 13:01:09 - train: epoch 0044, iter [00580, 01251], lr: 0.000418, loss: 0.4200
2022-10-03 13:01:28 - train: epoch 0044, iter [00590, 01251], lr: 0.000417, loss: 0.4107
2022-10-03 13:01:47 - train: epoch 0044, iter [00600, 01251], lr: 0.000417, loss: 0.4081
2022-10-03 13:02:06 - train: epoch 0044, iter [00610, 01251], lr: 0.000417, loss: 0.4082
2022-10-03 13:02:25 - train: epoch 0044, iter [00620, 01251], lr: 0.000417, loss: 0.4301
2022-10-03 13:02:44 - train: epoch 0044, iter [00630, 01251], lr: 0.000417, loss: 0.4209
2022-10-03 13:03:02 - train: epoch 0044, iter [00640, 01251], lr: 0.000417, loss: 0.4290
2022-10-03 13:03:21 - train: epoch 0044, iter [00650, 01251], lr: 0.000417, loss: 0.3962
2022-10-03 13:03:40 - train: epoch 0044, iter [00660, 01251], lr: 0.000417, loss: 0.4168
2022-10-03 13:03:59 - train: epoch 0044, iter [00670, 01251], lr: 0.000417, loss: 0.4203
2022-10-03 13:04:18 - train: epoch 0044, iter [00680, 01251], lr: 0.000417, loss: 0.4295
2022-10-03 13:04:37 - train: epoch 0044, iter [00690, 01251], lr: 0.000417, loss: 0.4225
2022-10-03 13:04:56 - train: epoch 0044, iter [00700, 01251], lr: 0.000417, loss: 0.4199
2022-10-03 13:05:15 - train: epoch 0044, iter [00710, 01251], lr: 0.000417, loss: 0.4053
2022-10-03 13:05:34 - train: epoch 0044, iter [00720, 01251], lr: 0.000416, loss: 0.4201
2022-10-03 13:05:52 - train: epoch 0044, iter [00730, 01251], lr: 0.000416, loss: 0.4027
2022-10-03 13:06:11 - train: epoch 0044, iter [00740, 01251], lr: 0.000416, loss: 0.4254
2022-10-03 13:06:30 - train: epoch 0044, iter [00750, 01251], lr: 0.000416, loss: 0.4299
2022-10-03 13:06:49 - train: epoch 0044, iter [00760, 01251], lr: 0.000416, loss: 0.4092
2022-10-03 13:07:08 - train: epoch 0044, iter [00770, 01251], lr: 0.000416, loss: 0.4072
2022-10-03 13:07:27 - train: epoch 0044, iter [00780, 01251], lr: 0.000416, loss: 0.3959
2022-10-03 13:07:45 - train: epoch 0044, iter [00790, 01251], lr: 0.000416, loss: 0.4292
2022-10-03 13:08:04 - train: epoch 0044, iter [00800, 01251], lr: 0.000416, loss: 0.4262
2022-10-03 13:08:23 - train: epoch 0044, iter [00810, 01251], lr: 0.000416, loss: 0.4123
2022-10-03 13:08:42 - train: epoch 0044, iter [00820, 01251], lr: 0.000416, loss: 0.3944
2022-10-03 13:09:01 - train: epoch 0044, iter [00830, 01251], lr: 0.000416, loss: 0.4079
2022-10-03 13:09:20 - train: epoch 0044, iter [00840, 01251], lr: 0.000416, loss: 0.4069
2022-10-03 13:09:39 - train: epoch 0044, iter [00850, 01251], lr: 0.000415, loss: 0.4090
2022-10-03 13:09:58 - train: epoch 0044, iter [00860, 01251], lr: 0.000415, loss: 0.3818
2022-10-03 13:10:16 - train: epoch 0044, iter [00870, 01251], lr: 0.000415, loss: 0.3986
2022-10-03 13:10:35 - train: epoch 0044, iter [00880, 01251], lr: 0.000415, loss: 0.4203
2022-10-03 13:10:54 - train: epoch 0044, iter [00890, 01251], lr: 0.000415, loss: 0.4115
2022-10-03 13:11:13 - train: epoch 0044, iter [00900, 01251], lr: 0.000415, loss: 0.3991
2022-10-03 13:11:32 - train: epoch 0044, iter [00910, 01251], lr: 0.000415, loss: 0.4155
2022-10-03 13:11:51 - train: epoch 0044, iter [00920, 01251], lr: 0.000415, loss: 0.4052
2022-10-03 13:12:09 - train: epoch 0044, iter [00930, 01251], lr: 0.000415, loss: 0.4145
2022-10-03 13:12:28 - train: epoch 0044, iter [00940, 01251], lr: 0.000415, loss: 0.4052
2022-10-03 13:12:47 - train: epoch 0044, iter [00950, 01251], lr: 0.000415, loss: 0.4110
2022-10-03 13:13:06 - train: epoch 0044, iter [00960, 01251], lr: 0.000415, loss: 0.4178
2022-10-03 13:13:25 - train: epoch 0044, iter [00970, 01251], lr: 0.000415, loss: 0.4093
2022-10-03 13:13:43 - train: epoch 0044, iter [00980, 01251], lr: 0.000414, loss: 0.4100
2022-10-03 13:14:02 - train: epoch 0044, iter [00990, 01251], lr: 0.000414, loss: 0.4194
2022-10-03 13:14:20 - train: epoch 0044, iter [01000, 01251], lr: 0.000414, loss: 0.4067
2022-10-03 13:14:39 - train: epoch 0044, iter [01010, 01251], lr: 0.000414, loss: 0.4311
2022-10-03 13:14:58 - train: epoch 0044, iter [01020, 01251], lr: 0.000414, loss: 0.4098
2022-10-03 13:15:17 - train: epoch 0044, iter [01030, 01251], lr: 0.000414, loss: 0.4147
2022-10-03 13:15:35 - train: epoch 0044, iter [01040, 01251], lr: 0.000414, loss: 0.4034
2022-10-03 13:15:55 - train: epoch 0044, iter [01050, 01251], lr: 0.000414, loss: 0.4046
2022-10-03 13:16:14 - train: epoch 0044, iter [01060, 01251], lr: 0.000414, loss: 0.4266
2022-10-03 13:16:33 - train: epoch 0044, iter [01070, 01251], lr: 0.000414, loss: 0.3983
2022-10-03 13:16:52 - train: epoch 0044, iter [01080, 01251], lr: 0.000414, loss: 0.4084
2022-10-03 13:17:11 - train: epoch 0044, iter [01090, 01251], lr: 0.000414, loss: 0.4014
2022-10-03 13:17:30 - train: epoch 0044, iter [01100, 01251], lr: 0.000414, loss: 0.4255
2022-10-03 13:17:49 - train: epoch 0044, iter [01110, 01251], lr: 0.000413, loss: 0.4262
2022-10-03 13:18:09 - train: epoch 0044, iter [01120, 01251], lr: 0.000413, loss: 0.4172
2022-10-03 13:18:28 - train: epoch 0044, iter [01130, 01251], lr: 0.000413, loss: 0.4203
2022-10-03 13:18:47 - train: epoch 0044, iter [01140, 01251], lr: 0.000413, loss: 0.4274
2022-10-03 13:19:06 - train: epoch 0044, iter [01150, 01251], lr: 0.000413, loss: 0.4181
2022-10-03 13:19:25 - train: epoch 0044, iter [01160, 01251], lr: 0.000413, loss: 0.4304
2022-10-03 13:19:45 - train: epoch 0044, iter [01170, 01251], lr: 0.000413, loss: 0.4047
2022-10-03 13:20:04 - train: epoch 0044, iter [01180, 01251], lr: 0.000413, loss: 0.4349
2022-10-03 13:20:22 - train: epoch 0044, iter [01190, 01251], lr: 0.000413, loss: 0.4067
2022-10-03 13:20:42 - train: epoch 0044, iter [01200, 01251], lr: 0.000413, loss: 0.3866
2022-10-03 13:21:00 - train: epoch 0044, iter [01210, 01251], lr: 0.000413, loss: 0.4106
2022-10-03 13:21:20 - train: epoch 0044, iter [01220, 01251], lr: 0.000413, loss: 0.4243
2022-10-03 13:21:39 - train: epoch 0044, iter [01230, 01251], lr: 0.000413, loss: 0.4218
2022-10-03 13:21:59 - train: epoch 0044, iter [01240, 01251], lr: 0.000412, loss: 0.3998
2022-10-03 13:22:17 - train: epoch 0044, iter [01250, 01251], lr: 0.000412, loss: 0.4099
2022-10-03 13:22:22 - train: epoch 044, train_loss: 0.4128
2022-10-03 13:22:26 - until epoch: 044, best_loss: 0.4128
2022-10-03 13:22:26 - epoch 045 lr: 0.000412
2022-10-03 13:22:52 - train: epoch 0045, iter [00010, 01251], lr: 0.000412, loss: 0.4261
2022-10-03 13:23:12 - train: epoch 0045, iter [00020, 01251], lr: 0.000412, loss: 0.3966
2022-10-03 13:23:31 - train: epoch 0045, iter [00030, 01251], lr: 0.000412, loss: 0.4225
2022-10-03 13:23:50 - train: epoch 0045, iter [00040, 01251], lr: 0.000412, loss: 0.4247
2022-10-03 13:24:10 - train: epoch 0045, iter [00050, 01251], lr: 0.000412, loss: 0.3983
2022-10-03 13:24:29 - train: epoch 0045, iter [00060, 01251], lr: 0.000412, loss: 0.4065
2022-10-03 13:24:49 - train: epoch 0045, iter [00070, 01251], lr: 0.000412, loss: 0.4204
2022-10-03 13:25:08 - train: epoch 0045, iter [00080, 01251], lr: 0.000412, loss: 0.4058
2022-10-03 13:25:27 - train: epoch 0045, iter [00090, 01251], lr: 0.000412, loss: 0.4218
2022-10-03 13:25:46 - train: epoch 0045, iter [00100, 01251], lr: 0.000412, loss: 0.4151
2022-10-03 13:26:06 - train: epoch 0045, iter [00110, 01251], lr: 0.000412, loss: 0.4182
2022-10-03 13:26:25 - train: epoch 0045, iter [00120, 01251], lr: 0.000411, loss: 0.4328
2022-10-03 13:26:44 - train: epoch 0045, iter [00130, 01251], lr: 0.000411, loss: 0.4260
2022-10-03 13:27:03 - train: epoch 0045, iter [00140, 01251], lr: 0.000411, loss: 0.3856
2022-10-03 13:27:22 - train: epoch 0045, iter [00150, 01251], lr: 0.000411, loss: 0.3969
2022-10-03 13:27:42 - train: epoch 0045, iter [00160, 01251], lr: 0.000411, loss: 0.4015
2022-10-03 13:28:01 - train: epoch 0045, iter [00170, 01251], lr: 0.000411, loss: 0.4161
2022-10-03 13:28:21 - train: epoch 0045, iter [00180, 01251], lr: 0.000411, loss: 0.4091
2022-10-03 13:28:40 - train: epoch 0045, iter [00190, 01251], lr: 0.000411, loss: 0.3986
2022-10-03 13:28:59 - train: epoch 0045, iter [00200, 01251], lr: 0.000411, loss: 0.4110
2022-10-03 13:29:18 - train: epoch 0045, iter [00210, 01251], lr: 0.000411, loss: 0.4142
2022-10-03 13:29:37 - train: epoch 0045, iter [00220, 01251], lr: 0.000411, loss: 0.4188
2022-10-03 13:29:56 - train: epoch 0045, iter [00230, 01251], lr: 0.000411, loss: 0.4074
2022-10-03 13:30:15 - train: epoch 0045, iter [00240, 01251], lr: 0.000411, loss: 0.4278
2022-10-03 13:30:34 - train: epoch 0045, iter [00250, 01251], lr: 0.000410, loss: 0.4294
2022-10-03 13:30:54 - train: epoch 0045, iter [00260, 01251], lr: 0.000410, loss: 0.4086
2022-10-03 13:31:13 - train: epoch 0045, iter [00270, 01251], lr: 0.000410, loss: 0.4144
2022-10-03 13:31:33 - train: epoch 0045, iter [00280, 01251], lr: 0.000410, loss: 0.3961
2022-10-03 13:31:52 - train: epoch 0045, iter [00290, 01251], lr: 0.000410, loss: 0.4254
2022-10-03 13:32:11 - train: epoch 0045, iter [00300, 01251], lr: 0.000410, loss: 0.4249
2022-10-03 13:32:30 - train: epoch 0045, iter [00310, 01251], lr: 0.000410, loss: 0.4056
2022-10-03 13:32:49 - train: epoch 0045, iter [00320, 01251], lr: 0.000410, loss: 0.4096
2022-10-03 13:33:08 - train: epoch 0045, iter [00330, 01251], lr: 0.000410, loss: 0.3966
2022-10-03 13:33:28 - train: epoch 0045, iter [00340, 01251], lr: 0.000410, loss: 0.4215
2022-10-03 13:33:47 - train: epoch 0045, iter [00350, 01251], lr: 0.000410, loss: 0.4041
2022-10-03 13:34:06 - train: epoch 0045, iter [00360, 01251], lr: 0.000410, loss: 0.4304
2022-10-03 13:34:25 - train: epoch 0045, iter [00370, 01251], lr: 0.000410, loss: 0.4077
2022-10-03 13:34:45 - train: epoch 0045, iter [00380, 01251], lr: 0.000409, loss: 0.4210
2022-10-03 13:35:04 - train: epoch 0045, iter [00390, 01251], lr: 0.000409, loss: 0.4091
2022-10-03 13:35:23 - train: epoch 0045, iter [00400, 01251], lr: 0.000409, loss: 0.4030
2022-10-03 13:35:42 - train: epoch 0045, iter [00410, 01251], lr: 0.000409, loss: 0.3936
2022-10-03 13:36:01 - train: epoch 0045, iter [00420, 01251], lr: 0.000409, loss: 0.4255
2022-10-03 13:36:20 - train: epoch 0045, iter [00430, 01251], lr: 0.000409, loss: 0.4028
2022-10-03 13:36:40 - train: epoch 0045, iter [00440, 01251], lr: 0.000409, loss: 0.4214
2022-10-03 13:36:59 - train: epoch 0045, iter [00450, 01251], lr: 0.000409, loss: 0.4139
2022-10-03 13:37:18 - train: epoch 0045, iter [00460, 01251], lr: 0.000409, loss: 0.4240
2022-10-03 13:37:37 - train: epoch 0045, iter [00470, 01251], lr: 0.000409, loss: 0.4268
2022-10-03 13:37:57 - train: epoch 0045, iter [00480, 01251], lr: 0.000409, loss: 0.4160
2022-10-03 13:38:16 - train: epoch 0045, iter [00490, 01251], lr: 0.000409, loss: 0.4251
2022-10-03 13:38:35 - train: epoch 0045, iter [00500, 01251], lr: 0.000408, loss: 0.4240
2022-10-03 13:38:55 - train: epoch 0045, iter [00510, 01251], lr: 0.000408, loss: 0.4119
2022-10-03 13:39:14 - train: epoch 0045, iter [00520, 01251], lr: 0.000408, loss: 0.3906
2022-10-03 13:39:32 - train: epoch 0045, iter [00530, 01251], lr: 0.000408, loss: 0.3889
2022-10-03 13:39:52 - train: epoch 0045, iter [00540, 01251], lr: 0.000408, loss: 0.4097
2022-10-03 13:40:11 - train: epoch 0045, iter [00550, 01251], lr: 0.000408, loss: 0.3975
2022-10-03 13:40:30 - train: epoch 0045, iter [00560, 01251], lr: 0.000408, loss: 0.4009
2022-10-03 13:40:49 - train: epoch 0045, iter [00570, 01251], lr: 0.000408, loss: 0.4091
2022-10-03 13:41:08 - train: epoch 0045, iter [00580, 01251], lr: 0.000408, loss: 0.4211
2022-10-03 13:41:28 - train: epoch 0045, iter [00590, 01251], lr: 0.000408, loss: 0.4057
2022-10-03 13:41:47 - train: epoch 0045, iter [00600, 01251], lr: 0.000408, loss: 0.4217
2022-10-03 13:42:06 - train: epoch 0045, iter [00610, 01251], lr: 0.000408, loss: 0.4098
2022-10-03 13:42:25 - train: epoch 0045, iter [00620, 01251], lr: 0.000408, loss: 0.4167
2022-10-03 13:42:44 - train: epoch 0045, iter [00630, 01251], lr: 0.000407, loss: 0.4242
2022-10-03 13:43:04 - train: epoch 0045, iter [00640, 01251], lr: 0.000407, loss: 0.4177
2022-10-03 13:43:23 - train: epoch 0045, iter [00650, 01251], lr: 0.000407, loss: 0.4016
2022-10-03 13:43:42 - train: epoch 0045, iter [00660, 01251], lr: 0.000407, loss: 0.3998
2022-10-03 13:44:02 - train: epoch 0045, iter [00670, 01251], lr: 0.000407, loss: 0.4076
2022-10-03 13:44:21 - train: epoch 0045, iter [00680, 01251], lr: 0.000407, loss: 0.4213
2022-10-03 13:44:40 - train: epoch 0045, iter [00690, 01251], lr: 0.000407, loss: 0.4004
2022-10-03 13:44:59 - train: epoch 0045, iter [00700, 01251], lr: 0.000407, loss: 0.4100
2022-10-03 13:45:18 - train: epoch 0045, iter [00710, 01251], lr: 0.000407, loss: 0.4016
2022-10-03 13:45:37 - train: epoch 0045, iter [00720, 01251], lr: 0.000407, loss: 0.4132
2022-10-03 13:45:56 - train: epoch 0045, iter [00730, 01251], lr: 0.000407, loss: 0.4165
2022-10-03 13:46:15 - train: epoch 0045, iter [00740, 01251], lr: 0.000407, loss: 0.4213
2022-10-03 13:46:34 - train: epoch 0045, iter [00750, 01251], lr: 0.000407, loss: 0.4004
2022-10-03 13:46:54 - train: epoch 0045, iter [00760, 01251], lr: 0.000406, loss: 0.4142
2022-10-03 13:47:13 - train: epoch 0045, iter [00770, 01251], lr: 0.000406, loss: 0.3945
2022-10-03 13:47:32 - train: epoch 0045, iter [00780, 01251], lr: 0.000406, loss: 0.4126
2022-10-03 13:47:51 - train: epoch 0045, iter [00790, 01251], lr: 0.000406, loss: 0.4016
2022-10-03 13:48:10 - train: epoch 0045, iter [00800, 01251], lr: 0.000406, loss: 0.4226
2022-10-03 13:48:29 - train: epoch 0045, iter [00810, 01251], lr: 0.000406, loss: 0.3963
2022-10-03 13:48:48 - train: epoch 0045, iter [00820, 01251], lr: 0.000406, loss: 0.4197
2022-10-03 13:49:07 - train: epoch 0045, iter [00830, 01251], lr: 0.000406, loss: 0.4185
2022-10-03 13:49:27 - train: epoch 0045, iter [00840, 01251], lr: 0.000406, loss: 0.4286
2022-10-03 13:49:46 - train: epoch 0045, iter [00850, 01251], lr: 0.000406, loss: 0.4065
2022-10-03 13:50:04 - train: epoch 0045, iter [00860, 01251], lr: 0.000406, loss: 0.3921
2022-10-03 13:50:23 - train: epoch 0045, iter [00870, 01251], lr: 0.000406, loss: 0.4349
2022-10-03 13:50:43 - train: epoch 0045, iter [00880, 01251], lr: 0.000406, loss: 0.3907
2022-10-03 13:51:02 - train: epoch 0045, iter [00890, 01251], lr: 0.000405, loss: 0.4205
2022-10-03 13:51:21 - train: epoch 0045, iter [00900, 01251], lr: 0.000405, loss: 0.4052
2022-10-03 13:51:40 - train: epoch 0045, iter [00910, 01251], lr: 0.000405, loss: 0.4022
2022-10-03 13:51:58 - train: epoch 0045, iter [00920, 01251], lr: 0.000405, loss: 0.4353
2022-10-03 13:52:18 - train: epoch 0045, iter [00930, 01251], lr: 0.000405, loss: 0.4090
2022-10-03 13:52:37 - train: epoch 0045, iter [00940, 01251], lr: 0.000405, loss: 0.4248
2022-10-03 13:52:56 - train: epoch 0045, iter [00950, 01251], lr: 0.000405, loss: 0.3962
2022-10-03 13:53:15 - train: epoch 0045, iter [00960, 01251], lr: 0.000405, loss: 0.3849
2022-10-03 13:53:34 - train: epoch 0045, iter [00970, 01251], lr: 0.000405, loss: 0.4194
2022-10-03 13:53:53 - train: epoch 0045, iter [00980, 01251], lr: 0.000405, loss: 0.4099
2022-10-03 13:54:13 - train: epoch 0045, iter [00990, 01251], lr: 0.000405, loss: 0.4191
2022-10-03 13:54:32 - train: epoch 0045, iter [01000, 01251], lr: 0.000405, loss: 0.4084
2022-10-03 13:54:51 - train: epoch 0045, iter [01010, 01251], lr: 0.000404, loss: 0.4208
2022-10-03 13:55:10 - train: epoch 0045, iter [01020, 01251], lr: 0.000404, loss: 0.4037
2022-10-03 13:55:29 - train: epoch 0045, iter [01030, 01251], lr: 0.000404, loss: 0.4069
2022-10-03 13:55:48 - train: epoch 0045, iter [01040, 01251], lr: 0.000404, loss: 0.4338
2022-10-03 13:56:07 - train: epoch 0045, iter [01050, 01251], lr: 0.000404, loss: 0.4209
2022-10-03 13:56:26 - train: epoch 0045, iter [01060, 01251], lr: 0.000404, loss: 0.4328
2022-10-03 13:56:45 - train: epoch 0045, iter [01070, 01251], lr: 0.000404, loss: 0.4005
2022-10-03 13:57:05 - train: epoch 0045, iter [01080, 01251], lr: 0.000404, loss: 0.3930
2022-10-03 13:57:24 - train: epoch 0045, iter [01090, 01251], lr: 0.000404, loss: 0.4358
2022-10-03 13:57:43 - train: epoch 0045, iter [01100, 01251], lr: 0.000404, loss: 0.4078
2022-10-03 13:58:02 - train: epoch 0045, iter [01110, 01251], lr: 0.000404, loss: 0.4239
2022-10-03 13:58:21 - train: epoch 0045, iter [01120, 01251], lr: 0.000404, loss: 0.4025
2022-10-03 13:58:41 - train: epoch 0045, iter [01130, 01251], lr: 0.000404, loss: 0.4081
2022-10-03 13:59:00 - train: epoch 0045, iter [01140, 01251], lr: 0.000403, loss: 0.4359
2022-10-03 13:59:19 - train: epoch 0045, iter [01150, 01251], lr: 0.000403, loss: 0.4128
2022-10-03 13:59:38 - train: epoch 0045, iter [01160, 01251], lr: 0.000403, loss: 0.3998
2022-10-03 13:59:57 - train: epoch 0045, iter [01170, 01251], lr: 0.000403, loss: 0.4262
2022-10-03 14:00:16 - train: epoch 0045, iter [01180, 01251], lr: 0.000403, loss: 0.4228
2022-10-03 14:00:35 - train: epoch 0045, iter [01190, 01251], lr: 0.000403, loss: 0.4105
2022-10-03 14:00:54 - train: epoch 0045, iter [01200, 01251], lr: 0.000403, loss: 0.4355
2022-10-03 14:01:13 - train: epoch 0045, iter [01210, 01251], lr: 0.000403, loss: 0.4077
2022-10-03 14:01:32 - train: epoch 0045, iter [01220, 01251], lr: 0.000403, loss: 0.4066
2022-10-03 14:01:51 - train: epoch 0045, iter [01230, 01251], lr: 0.000403, loss: 0.4354
2022-10-03 14:02:10 - train: epoch 0045, iter [01240, 01251], lr: 0.000403, loss: 0.4129
2022-10-03 14:02:28 - train: epoch 0045, iter [01250, 01251], lr: 0.000403, loss: 0.4070
2022-10-03 14:02:33 - train: epoch 045, train_loss: 0.4126
2022-10-03 14:02:37 - until epoch: 045, best_loss: 0.4126
2022-10-03 14:02:37 - epoch 046 lr: 0.000403
2022-10-03 14:03:03 - train: epoch 0046, iter [00010, 01251], lr: 0.000403, loss: 0.4139
2022-10-03 14:03:22 - train: epoch 0046, iter [00020, 01251], lr: 0.000402, loss: 0.4162
2022-10-03 14:03:41 - train: epoch 0046, iter [00030, 01251], lr: 0.000402, loss: 0.4057
2022-10-03 14:04:00 - train: epoch 0046, iter [00040, 01251], lr: 0.000402, loss: 0.4140
2022-10-03 14:04:20 - train: epoch 0046, iter [00050, 01251], lr: 0.000402, loss: 0.4245
2022-10-03 14:04:39 - train: epoch 0046, iter [00060, 01251], lr: 0.000402, loss: 0.3981
2022-10-03 14:04:58 - train: epoch 0046, iter [00070, 01251], lr: 0.000402, loss: 0.4075
2022-10-03 14:05:17 - train: epoch 0046, iter [00080, 01251], lr: 0.000402, loss: 0.4269
2022-10-03 14:05:37 - train: epoch 0046, iter [00090, 01251], lr: 0.000402, loss: 0.4189
2022-10-03 14:05:56 - train: epoch 0046, iter [00100, 01251], lr: 0.000402, loss: 0.4078
2022-10-03 14:06:15 - train: epoch 0046, iter [00110, 01251], lr: 0.000402, loss: 0.4196
2022-10-03 14:06:34 - train: epoch 0046, iter [00120, 01251], lr: 0.000402, loss: 0.3906
2022-10-03 14:06:53 - train: epoch 0046, iter [00130, 01251], lr: 0.000402, loss: 0.4064
2022-10-03 14:07:12 - train: epoch 0046, iter [00140, 01251], lr: 0.000402, loss: 0.4153
2022-10-03 14:07:31 - train: epoch 0046, iter [00150, 01251], lr: 0.000401, loss: 0.4118
2022-10-03 14:07:50 - train: epoch 0046, iter [00160, 01251], lr: 0.000401, loss: 0.4126
2022-10-03 14:08:09 - train: epoch 0046, iter [00170, 01251], lr: 0.000401, loss: 0.4186
2022-10-03 14:08:28 - train: epoch 0046, iter [00180, 01251], lr: 0.000401, loss: 0.3983
2022-10-03 14:08:47 - train: epoch 0046, iter [00190, 01251], lr: 0.000401, loss: 0.3969
2022-10-03 14:09:07 - train: epoch 0046, iter [00200, 01251], lr: 0.000401, loss: 0.4052
2022-10-03 14:09:26 - train: epoch 0046, iter [00210, 01251], lr: 0.000401, loss: 0.4293
2022-10-03 14:09:46 - train: epoch 0046, iter [00220, 01251], lr: 0.000401, loss: 0.4180
2022-10-03 14:10:04 - train: epoch 0046, iter [00230, 01251], lr: 0.000401, loss: 0.4176
2022-10-03 14:10:24 - train: epoch 0046, iter [00240, 01251], lr: 0.000401, loss: 0.3934
2022-10-03 14:10:43 - train: epoch 0046, iter [00250, 01251], lr: 0.000401, loss: 0.3856
2022-10-03 14:11:02 - train: epoch 0046, iter [00260, 01251], lr: 0.000401, loss: 0.4225
2022-10-03 14:11:21 - train: epoch 0046, iter [00270, 01251], lr: 0.000400, loss: 0.4189
2022-10-03 14:11:41 - train: epoch 0046, iter [00280, 01251], lr: 0.000400, loss: 0.4245
2022-10-03 14:12:00 - train: epoch 0046, iter [00290, 01251], lr: 0.000400, loss: 0.4154
2022-10-03 14:12:19 - train: epoch 0046, iter [00300, 01251], lr: 0.000400, loss: 0.4156
2022-10-03 14:12:38 - train: epoch 0046, iter [00310, 01251], lr: 0.000400, loss: 0.4309
2022-10-03 14:12:58 - train: epoch 0046, iter [00320, 01251], lr: 0.000400, loss: 0.4149
2022-10-03 14:13:17 - train: epoch 0046, iter [00330, 01251], lr: 0.000400, loss: 0.4194
2022-10-03 14:13:36 - train: epoch 0046, iter [00340, 01251], lr: 0.000400, loss: 0.4073
2022-10-03 14:13:55 - train: epoch 0046, iter [00350, 01251], lr: 0.000400, loss: 0.4115
2022-10-03 14:14:14 - train: epoch 0046, iter [00360, 01251], lr: 0.000400, loss: 0.4093
2022-10-03 14:14:34 - train: epoch 0046, iter [00370, 01251], lr: 0.000400, loss: 0.4274
2022-10-03 14:14:53 - train: epoch 0046, iter [00380, 01251], lr: 0.000400, loss: 0.4182
2022-10-03 14:15:11 - train: epoch 0046, iter [00390, 01251], lr: 0.000400, loss: 0.4246
2022-10-03 14:15:30 - train: epoch 0046, iter [00400, 01251], lr: 0.000399, loss: 0.3934
2022-10-03 14:15:49 - train: epoch 0046, iter [00410, 01251], lr: 0.000399, loss: 0.4280
2022-10-03 14:16:09 - train: epoch 0046, iter [00420, 01251], lr: 0.000399, loss: 0.4085
2022-10-03 14:16:28 - train: epoch 0046, iter [00430, 01251], lr: 0.000399, loss: 0.4252
2022-10-03 14:16:47 - train: epoch 0046, iter [00440, 01251], lr: 0.000399, loss: 0.4162
2022-10-03 14:17:06 - train: epoch 0046, iter [00450, 01251], lr: 0.000399, loss: 0.4152
2022-10-03 14:17:26 - train: epoch 0046, iter [00460, 01251], lr: 0.000399, loss: 0.4045
2022-10-03 14:17:44 - train: epoch 0046, iter [00470, 01251], lr: 0.000399, loss: 0.4177
2022-10-03 14:18:03 - train: epoch 0046, iter [00480, 01251], lr: 0.000399, loss: 0.4054
2022-10-03 14:18:22 - train: epoch 0046, iter [00490, 01251], lr: 0.000399, loss: 0.4225
2022-10-03 14:18:41 - train: epoch 0046, iter [00500, 01251], lr: 0.000399, loss: 0.3994
2022-10-03 14:19:00 - train: epoch 0046, iter [00510, 01251], lr: 0.000399, loss: 0.4141
2022-10-03 14:19:19 - train: epoch 0046, iter [00520, 01251], lr: 0.000399, loss: 0.4214
2022-10-03 14:19:39 - train: epoch 0046, iter [00530, 01251], lr: 0.000398, loss: 0.4216
2022-10-03 14:19:57 - train: epoch 0046, iter [00540, 01251], lr: 0.000398, loss: 0.4119
2022-10-03 14:20:17 - train: epoch 0046, iter [00550, 01251], lr: 0.000398, loss: 0.4138
2022-10-03 14:20:36 - train: epoch 0046, iter [00560, 01251], lr: 0.000398, loss: 0.4005
2022-10-03 14:20:55 - train: epoch 0046, iter [00570, 01251], lr: 0.000398, loss: 0.4085
2022-10-03 14:21:13 - train: epoch 0046, iter [00580, 01251], lr: 0.000398, loss: 0.4063
2022-10-03 14:21:32 - train: epoch 0046, iter [00590, 01251], lr: 0.000398, loss: 0.4004
2022-10-03 14:21:52 - train: epoch 0046, iter [00600, 01251], lr: 0.000398, loss: 0.4452
2022-10-03 14:22:11 - train: epoch 0046, iter [00610, 01251], lr: 0.000398, loss: 0.4255
2022-10-03 14:22:30 - train: epoch 0046, iter [00620, 01251], lr: 0.000398, loss: 0.3966
2022-10-03 14:22:49 - train: epoch 0046, iter [00630, 01251], lr: 0.000398, loss: 0.4241
2022-10-03 14:23:08 - train: epoch 0046, iter [00640, 01251], lr: 0.000398, loss: 0.4245
2022-10-03 14:23:27 - train: epoch 0046, iter [00650, 01251], lr: 0.000397, loss: 0.3955
2022-10-03 14:23:46 - train: epoch 0046, iter [00660, 01251], lr: 0.000397, loss: 0.4360
2022-10-03 14:24:06 - train: epoch 0046, iter [00670, 01251], lr: 0.000397, loss: 0.4411
2022-10-03 14:24:25 - train: epoch 0046, iter [00680, 01251], lr: 0.000397, loss: 0.4087
2022-10-03 14:24:44 - train: epoch 0046, iter [00690, 01251], lr: 0.000397, loss: 0.4115
2022-10-03 14:25:03 - train: epoch 0046, iter [00700, 01251], lr: 0.000397, loss: 0.4097
2022-10-03 14:25:22 - train: epoch 0046, iter [00710, 01251], lr: 0.000397, loss: 0.4242
2022-10-03 14:25:41 - train: epoch 0046, iter [00720, 01251], lr: 0.000397, loss: 0.3914
2022-10-03 14:26:01 - train: epoch 0046, iter [00730, 01251], lr: 0.000397, loss: 0.3980
2022-10-03 14:26:20 - train: epoch 0046, iter [00740, 01251], lr: 0.000397, loss: 0.3999
2022-10-03 14:26:40 - train: epoch 0046, iter [00750, 01251], lr: 0.000397, loss: 0.4106
2022-10-03 14:26:59 - train: epoch 0046, iter [00760, 01251], lr: 0.000397, loss: 0.4175
2022-10-03 14:27:18 - train: epoch 0046, iter [00770, 01251], lr: 0.000397, loss: 0.4125
2022-10-03 14:27:37 - train: epoch 0046, iter [00780, 01251], lr: 0.000396, loss: 0.4052
2022-10-03 14:27:57 - train: epoch 0046, iter [00790, 01251], lr: 0.000396, loss: 0.4222
2022-10-03 14:28:16 - train: epoch 0046, iter [00800, 01251], lr: 0.000396, loss: 0.4141
2022-10-03 14:28:35 - train: epoch 0046, iter [00810, 01251], lr: 0.000396, loss: 0.4031
2022-10-03 14:28:55 - train: epoch 0046, iter [00820, 01251], lr: 0.000396, loss: 0.4076
2022-10-03 14:29:14 - train: epoch 0046, iter [00830, 01251], lr: 0.000396, loss: 0.3946
2022-10-03 14:29:33 - train: epoch 0046, iter [00840, 01251], lr: 0.000396, loss: 0.4083
2022-10-03 14:29:52 - train: epoch 0046, iter [00850, 01251], lr: 0.000396, loss: 0.4144
2022-10-03 14:30:11 - train: epoch 0046, iter [00860, 01251], lr: 0.000396, loss: 0.4276
2022-10-03 14:30:30 - train: epoch 0046, iter [00870, 01251], lr: 0.000396, loss: 0.4335
2022-10-03 14:30:49 - train: epoch 0046, iter [00880, 01251], lr: 0.000396, loss: 0.4117
2022-10-03 14:31:08 - train: epoch 0046, iter [00890, 01251], lr: 0.000396, loss: 0.4043
2022-10-03 14:31:27 - train: epoch 0046, iter [00900, 01251], lr: 0.000395, loss: 0.4197
2022-10-03 14:31:47 - train: epoch 0046, iter [00910, 01251], lr: 0.000395, loss: 0.4042
2022-10-03 14:32:06 - train: epoch 0046, iter [00920, 01251], lr: 0.000395, loss: 0.4138
2022-10-03 14:32:25 - train: epoch 0046, iter [00930, 01251], lr: 0.000395, loss: 0.4303
2022-10-03 14:32:44 - train: epoch 0046, iter [00940, 01251], lr: 0.000395, loss: 0.4036
2022-10-03 14:33:03 - train: epoch 0046, iter [00950, 01251], lr: 0.000395, loss: 0.4133
2022-10-03 14:33:22 - train: epoch 0046, iter [00960, 01251], lr: 0.000395, loss: 0.4047
2022-10-03 14:33:41 - train: epoch 0046, iter [00970, 01251], lr: 0.000395, loss: 0.4087
2022-10-03 14:34:00 - train: epoch 0046, iter [00980, 01251], lr: 0.000395, loss: 0.4220
2022-10-03 14:34:19 - train: epoch 0046, iter [00990, 01251], lr: 0.000395, loss: 0.4164
2022-10-03 14:34:38 - train: epoch 0046, iter [01000, 01251], lr: 0.000395, loss: 0.4124
2022-10-03 14:34:57 - train: epoch 0046, iter [01010, 01251], lr: 0.000395, loss: 0.4173
2022-10-03 14:35:16 - train: epoch 0046, iter [01020, 01251], lr: 0.000395, loss: 0.4055
2022-10-03 14:35:35 - train: epoch 0046, iter [01030, 01251], lr: 0.000394, loss: 0.4223
2022-10-03 14:35:54 - train: epoch 0046, iter [01040, 01251], lr: 0.000394, loss: 0.4247
2022-10-03 14:36:12 - train: epoch 0046, iter [01050, 01251], lr: 0.000394, loss: 0.4176
2022-10-03 14:36:32 - train: epoch 0046, iter [01060, 01251], lr: 0.000394, loss: 0.4137
2022-10-03 14:36:51 - train: epoch 0046, iter [01070, 01251], lr: 0.000394, loss: 0.4161
2022-10-03 14:37:09 - train: epoch 0046, iter [01080, 01251], lr: 0.000394, loss: 0.3932
2022-10-03 14:37:28 - train: epoch 0046, iter [01090, 01251], lr: 0.000394, loss: 0.4267
2022-10-03 14:37:47 - train: epoch 0046, iter [01100, 01251], lr: 0.000394, loss: 0.3928
2022-10-03 14:38:06 - train: epoch 0046, iter [01110, 01251], lr: 0.000394, loss: 0.4350
2022-10-03 14:38:25 - train: epoch 0046, iter [01120, 01251], lr: 0.000394, loss: 0.4079
2022-10-03 14:38:44 - train: epoch 0046, iter [01130, 01251], lr: 0.000394, loss: 0.4307
2022-10-03 14:39:03 - train: epoch 0046, iter [01140, 01251], lr: 0.000394, loss: 0.3938
2022-10-03 14:39:22 - train: epoch 0046, iter [01150, 01251], lr: 0.000394, loss: 0.4078
2022-10-03 14:39:42 - train: epoch 0046, iter [01160, 01251], lr: 0.000393, loss: 0.4190
2022-10-03 14:40:01 - train: epoch 0046, iter [01170, 01251], lr: 0.000393, loss: 0.4122
2022-10-03 14:40:21 - train: epoch 0046, iter [01180, 01251], lr: 0.000393, loss: 0.4031
2022-10-03 14:40:39 - train: epoch 0046, iter [01190, 01251], lr: 0.000393, loss: 0.4403
2022-10-03 14:40:59 - train: epoch 0046, iter [01200, 01251], lr: 0.000393, loss: 0.4080
2022-10-03 14:41:18 - train: epoch 0046, iter [01210, 01251], lr: 0.000393, loss: 0.3848
2022-10-03 14:41:37 - train: epoch 0046, iter [01220, 01251], lr: 0.000393, loss: 0.4301
2022-10-03 14:41:56 - train: epoch 0046, iter [01230, 01251], lr: 0.000393, loss: 0.4233
2022-10-03 14:42:15 - train: epoch 0046, iter [01240, 01251], lr: 0.000393, loss: 0.4369
2022-10-03 14:42:33 - train: epoch 0046, iter [01250, 01251], lr: 0.000393, loss: 0.4014
2022-10-03 14:42:38 - train: epoch 046, train_loss: 0.4122
2022-10-03 14:42:42 - until epoch: 046, best_loss: 0.4122
2022-10-03 14:42:42 - epoch 047 lr: 0.000393
2022-10-03 14:43:08 - train: epoch 0047, iter [00010, 01251], lr: 0.000393, loss: 0.4246
2022-10-03 14:43:27 - train: epoch 0047, iter [00020, 01251], lr: 0.000393, loss: 0.4305
2022-10-03 14:43:46 - train: epoch 0047, iter [00030, 01251], lr: 0.000392, loss: 0.4005
2022-10-03 14:44:05 - train: epoch 0047, iter [00040, 01251], lr: 0.000392, loss: 0.4135
2022-10-03 14:44:24 - train: epoch 0047, iter [00050, 01251], lr: 0.000392, loss: 0.4086
2022-10-03 14:44:44 - train: epoch 0047, iter [00060, 01251], lr: 0.000392, loss: 0.4188
2022-10-03 14:45:02 - train: epoch 0047, iter [00070, 01251], lr: 0.000392, loss: 0.4085
2022-10-03 14:45:21 - train: epoch 0047, iter [00080, 01251], lr: 0.000392, loss: 0.4362
2022-10-03 14:45:41 - train: epoch 0047, iter [00090, 01251], lr: 0.000392, loss: 0.4150
2022-10-03 14:46:00 - train: epoch 0047, iter [00100, 01251], lr: 0.000392, loss: 0.4279
2022-10-03 14:46:19 - train: epoch 0047, iter [00110, 01251], lr: 0.000392, loss: 0.3933
2022-10-03 14:46:38 - train: epoch 0047, iter [00120, 01251], lr: 0.000392, loss: 0.4135
2022-10-03 14:46:58 - train: epoch 0047, iter [00130, 01251], lr: 0.000392, loss: 0.4202
2022-10-03 14:47:17 - train: epoch 0047, iter [00140, 01251], lr: 0.000392, loss: 0.4340
2022-10-03 14:47:36 - train: epoch 0047, iter [00150, 01251], lr: 0.000392, loss: 0.4149
2022-10-03 14:47:55 - train: epoch 0047, iter [00160, 01251], lr: 0.000391, loss: 0.4326
2022-10-03 14:48:15 - train: epoch 0047, iter [00170, 01251], lr: 0.000391, loss: 0.4024
2022-10-03 14:48:34 - train: epoch 0047, iter [00180, 01251], lr: 0.000391, loss: 0.4235
2022-10-03 14:48:53 - train: epoch 0047, iter [00190, 01251], lr: 0.000391, loss: 0.4293
2022-10-03 14:49:12 - train: epoch 0047, iter [00200, 01251], lr: 0.000391, loss: 0.3885
2022-10-03 14:49:32 - train: epoch 0047, iter [00210, 01251], lr: 0.000391, loss: 0.4219
2022-10-03 14:49:51 - train: epoch 0047, iter [00220, 01251], lr: 0.000391, loss: 0.4172
2022-10-03 14:50:10 - train: epoch 0047, iter [00230, 01251], lr: 0.000391, loss: 0.4227
2022-10-03 14:50:29 - train: epoch 0047, iter [00240, 01251], lr: 0.000391, loss: 0.4396
2022-10-03 14:50:48 - train: epoch 0047, iter [00250, 01251], lr: 0.000391, loss: 0.4303
2022-10-03 14:51:08 - train: epoch 0047, iter [00260, 01251], lr: 0.000391, loss: 0.4107
2022-10-03 14:51:27 - train: epoch 0047, iter [00270, 01251], lr: 0.000391, loss: 0.4028
2022-10-03 14:51:46 - train: epoch 0047, iter [00280, 01251], lr: 0.000390, loss: 0.4136
2022-10-03 14:52:05 - train: epoch 0047, iter [00290, 01251], lr: 0.000390, loss: 0.4099
2022-10-03 14:52:24 - train: epoch 0047, iter [00300, 01251], lr: 0.000390, loss: 0.4163
2022-10-03 14:52:43 - train: epoch 0047, iter [00310, 01251], lr: 0.000390, loss: 0.4320
2022-10-03 14:53:02 - train: epoch 0047, iter [00320, 01251], lr: 0.000390, loss: 0.4174
2022-10-03 14:53:22 - train: epoch 0047, iter [00330, 01251], lr: 0.000390, loss: 0.4220
2022-10-03 14:53:41 - train: epoch 0047, iter [00340, 01251], lr: 0.000390, loss: 0.4047
2022-10-03 14:54:01 - train: epoch 0047, iter [00350, 01251], lr: 0.000390, loss: 0.4192
2022-10-03 14:54:20 - train: epoch 0047, iter [00360, 01251], lr: 0.000390, loss: 0.4004
2022-10-03 14:54:39 - train: epoch 0047, iter [00370, 01251], lr: 0.000390, loss: 0.4144
2022-10-03 14:54:59 - train: epoch 0047, iter [00380, 01251], lr: 0.000390, loss: 0.4109
2022-10-03 14:55:18 - train: epoch 0047, iter [00390, 01251], lr: 0.000390, loss: 0.3910
2022-10-03 14:55:37 - train: epoch 0047, iter [00400, 01251], lr: 0.000390, loss: 0.4146
2022-10-03 14:55:57 - train: epoch 0047, iter [00410, 01251], lr: 0.000389, loss: 0.4440
2022-10-03 14:56:16 - train: epoch 0047, iter [00420, 01251], lr: 0.000389, loss: 0.4103
2022-10-03 14:56:35 - train: epoch 0047, iter [00430, 01251], lr: 0.000389, loss: 0.4124
2022-10-03 14:56:54 - train: epoch 0047, iter [00440, 01251], lr: 0.000389, loss: 0.4316
2022-10-03 14:57:14 - train: epoch 0047, iter [00450, 01251], lr: 0.000389, loss: 0.4121
2022-10-03 14:57:33 - train: epoch 0047, iter [00460, 01251], lr: 0.000389, loss: 0.4102
2022-10-03 14:57:52 - train: epoch 0047, iter [00470, 01251], lr: 0.000389, loss: 0.3985
2022-10-03 14:58:11 - train: epoch 0047, iter [00480, 01251], lr: 0.000389, loss: 0.4270
2022-10-03 14:58:31 - train: epoch 0047, iter [00490, 01251], lr: 0.000389, loss: 0.3927
2022-10-03 14:58:50 - train: epoch 0047, iter [00500, 01251], lr: 0.000389, loss: 0.4247
2022-10-03 14:59:09 - train: epoch 0047, iter [00510, 01251], lr: 0.000389, loss: 0.4108
2022-10-03 14:59:27 - train: epoch 0047, iter [00520, 01251], lr: 0.000389, loss: 0.4165
2022-10-03 14:59:46 - train: epoch 0047, iter [00530, 01251], lr: 0.000388, loss: 0.4178
2022-10-03 15:00:05 - train: epoch 0047, iter [00540, 01251], lr: 0.000388, loss: 0.4235
2022-10-03 15:00:24 - train: epoch 0047, iter [00550, 01251], lr: 0.000388, loss: 0.4178
2022-10-03 15:00:42 - train: epoch 0047, iter [00560, 01251], lr: 0.000388, loss: 0.4184
2022-10-03 15:01:01 - train: epoch 0047, iter [00570, 01251], lr: 0.000388, loss: 0.4110
2022-10-03 15:01:20 - train: epoch 0047, iter [00580, 01251], lr: 0.000388, loss: 0.4255
2022-10-03 15:01:38 - train: epoch 0047, iter [00590, 01251], lr: 0.000388, loss: 0.3965
2022-10-03 15:01:57 - train: epoch 0047, iter [00600, 01251], lr: 0.000388, loss: 0.4222
2022-10-03 15:02:16 - train: epoch 0047, iter [00610, 01251], lr: 0.000388, loss: 0.4162
2022-10-03 15:02:34 - train: epoch 0047, iter [00620, 01251], lr: 0.000388, loss: 0.4029
2022-10-03 15:02:53 - train: epoch 0047, iter [00630, 01251], lr: 0.000388, loss: 0.4130
2022-10-03 15:03:12 - train: epoch 0047, iter [00640, 01251], lr: 0.000388, loss: 0.4163
2022-10-03 15:03:30 - train: epoch 0047, iter [00650, 01251], lr: 0.000388, loss: 0.4046
2022-10-03 15:03:49 - train: epoch 0047, iter [00660, 01251], lr: 0.000387, loss: 0.3913
2022-10-03 15:04:08 - train: epoch 0047, iter [00670, 01251], lr: 0.000387, loss: 0.4101
2022-10-03 15:04:27 - train: epoch 0047, iter [00680, 01251], lr: 0.000387, loss: 0.4031
2022-10-03 15:04:46 - train: epoch 0047, iter [00690, 01251], lr: 0.000387, loss: 0.4038
2022-10-03 15:05:04 - train: epoch 0047, iter [00700, 01251], lr: 0.000387, loss: 0.4182
2022-10-03 15:05:23 - train: epoch 0047, iter [00710, 01251], lr: 0.000387, loss: 0.4250
2022-10-03 15:05:41 - train: epoch 0047, iter [00720, 01251], lr: 0.000387, loss: 0.4107
2022-10-03 15:06:00 - train: epoch 0047, iter [00730, 01251], lr: 0.000387, loss: 0.4098
2022-10-03 15:06:19 - train: epoch 0047, iter [00740, 01251], lr: 0.000387, loss: 0.4470
2022-10-03 15:06:37 - train: epoch 0047, iter [00750, 01251], lr: 0.000387, loss: 0.4039
2022-10-03 15:06:57 - train: epoch 0047, iter [00760, 01251], lr: 0.000387, loss: 0.4247
2022-10-03 15:07:15 - train: epoch 0047, iter [00770, 01251], lr: 0.000387, loss: 0.4050
2022-10-03 15:07:34 - train: epoch 0047, iter [00780, 01251], lr: 0.000386, loss: 0.4206
2022-10-03 15:07:53 - train: epoch 0047, iter [00790, 01251], lr: 0.000386, loss: 0.4169
2022-10-03 15:08:12 - train: epoch 0047, iter [00800, 01251], lr: 0.000386, loss: 0.4124
2022-10-03 15:08:30 - train: epoch 0047, iter [00810, 01251], lr: 0.000386, loss: 0.4159
2022-10-03 15:08:49 - train: epoch 0047, iter [00820, 01251], lr: 0.000386, loss: 0.4221
2022-10-03 15:09:07 - train: epoch 0047, iter [00830, 01251], lr: 0.000386, loss: 0.4402
2022-10-03 15:09:26 - train: epoch 0047, iter [00840, 01251], lr: 0.000386, loss: 0.4066
2022-10-03 15:09:45 - train: epoch 0047, iter [00850, 01251], lr: 0.000386, loss: 0.4226
2022-10-03 15:10:03 - train: epoch 0047, iter [00860, 01251], lr: 0.000386, loss: 0.4020
2022-10-03 15:10:22 - train: epoch 0047, iter [00870, 01251], lr: 0.000386, loss: 0.4083
2022-10-03 15:10:41 - train: epoch 0047, iter [00880, 01251], lr: 0.000386, loss: 0.4008
2022-10-03 15:10:59 - train: epoch 0047, iter [00890, 01251], lr: 0.000386, loss: 0.4102
2022-10-03 15:11:18 - train: epoch 0047, iter [00900, 01251], lr: 0.000386, loss: 0.4243
2022-10-03 15:11:37 - train: epoch 0047, iter [00910, 01251], lr: 0.000385, loss: 0.4113
2022-10-03 15:11:56 - train: epoch 0047, iter [00920, 01251], lr: 0.000385, loss: 0.4480
2022-10-03 15:12:14 - train: epoch 0047, iter [00930, 01251], lr: 0.000385, loss: 0.4020
2022-10-03 15:12:33 - train: epoch 0047, iter [00940, 01251], lr: 0.000385, loss: 0.4149
2022-10-03 15:12:51 - train: epoch 0047, iter [00950, 01251], lr: 0.000385, loss: 0.4231
2022-10-03 15:13:10 - train: epoch 0047, iter [00960, 01251], lr: 0.000385, loss: 0.3907
2022-10-03 15:13:28 - train: epoch 0047, iter [00970, 01251], lr: 0.000385, loss: 0.4180
2022-10-03 15:13:47 - train: epoch 0047, iter [00980, 01251], lr: 0.000385, loss: 0.3991
2022-10-03 15:14:06 - train: epoch 0047, iter [00990, 01251], lr: 0.000385, loss: 0.4245
2022-10-03 15:14:25 - train: epoch 0047, iter [01000, 01251], lr: 0.000385, loss: 0.4212
2022-10-03 15:14:43 - train: epoch 0047, iter [01010, 01251], lr: 0.000385, loss: 0.4015
2022-10-03 15:15:02 - train: epoch 0047, iter [01020, 01251], lr: 0.000385, loss: 0.4397
2022-10-03 15:15:20 - train: epoch 0047, iter [01030, 01251], lr: 0.000384, loss: 0.4106
2022-10-03 15:15:39 - train: epoch 0047, iter [01040, 01251], lr: 0.000384, loss: 0.4101
2022-10-03 15:15:58 - train: epoch 0047, iter [01050, 01251], lr: 0.000384, loss: 0.3860
2022-10-03 15:16:16 - train: epoch 0047, iter [01060, 01251], lr: 0.000384, loss: 0.4092
2022-10-03 15:16:35 - train: epoch 0047, iter [01070, 01251], lr: 0.000384, loss: 0.4067
2022-10-03 15:16:53 - train: epoch 0047, iter [01080, 01251], lr: 0.000384, loss: 0.4201
2022-10-03 15:17:12 - train: epoch 0047, iter [01090, 01251], lr: 0.000384, loss: 0.4170
2022-10-03 15:17:31 - train: epoch 0047, iter [01100, 01251], lr: 0.000384, loss: 0.4226
2022-10-03 15:17:49 - train: epoch 0047, iter [01110, 01251], lr: 0.000384, loss: 0.4087
2022-10-03 15:18:08 - train: epoch 0047, iter [01120, 01251], lr: 0.000384, loss: 0.3964
2022-10-03 15:18:26 - train: epoch 0047, iter [01130, 01251], lr: 0.000384, loss: 0.4117
2022-10-03 15:18:45 - train: epoch 0047, iter [01140, 01251], lr: 0.000384, loss: 0.4097
2022-10-03 15:19:03 - train: epoch 0047, iter [01150, 01251], lr: 0.000384, loss: 0.4114
2022-10-03 15:19:22 - train: epoch 0047, iter [01160, 01251], lr: 0.000383, loss: 0.4141
2022-10-03 15:19:40 - train: epoch 0047, iter [01170, 01251], lr: 0.000383, loss: 0.4161
2022-10-03 15:19:59 - train: epoch 0047, iter [01180, 01251], lr: 0.000383, loss: 0.4140
2022-10-03 15:20:19 - train: epoch 0047, iter [01190, 01251], lr: 0.000383, loss: 0.3834
2022-10-03 15:20:37 - train: epoch 0047, iter [01200, 01251], lr: 0.000383, loss: 0.4126
2022-10-03 15:20:57 - train: epoch 0047, iter [01210, 01251], lr: 0.000383, loss: 0.4139
2022-10-03 15:21:16 - train: epoch 0047, iter [01220, 01251], lr: 0.000383, loss: 0.3992
2022-10-03 15:21:35 - train: epoch 0047, iter [01230, 01251], lr: 0.000383, loss: 0.4354
2022-10-03 15:21:54 - train: epoch 0047, iter [01240, 01251], lr: 0.000383, loss: 0.4086
2022-10-03 15:22:12 - train: epoch 0047, iter [01250, 01251], lr: 0.000383, loss: 0.4065
2022-10-03 15:22:17 - train: epoch 047, train_loss: 0.4119
2022-10-03 15:22:21 - until epoch: 047, best_loss: 0.4119
2022-10-03 15:22:21 - epoch 048 lr: 0.000383
2022-10-03 15:22:45 - train: epoch 0048, iter [00010, 01251], lr: 0.000383, loss: 0.4153
2022-10-03 15:23:04 - train: epoch 0048, iter [00020, 01251], lr: 0.000383, loss: 0.4087
2022-10-03 15:23:23 - train: epoch 0048, iter [00030, 01251], lr: 0.000382, loss: 0.4184
2022-10-03 15:23:43 - train: epoch 0048, iter [00040, 01251], lr: 0.000382, loss: 0.4052
2022-10-03 15:24:02 - train: epoch 0048, iter [00050, 01251], lr: 0.000382, loss: 0.4131
2022-10-03 15:24:20 - train: epoch 0048, iter [00060, 01251], lr: 0.000382, loss: 0.4140
2022-10-03 15:24:39 - train: epoch 0048, iter [00070, 01251], lr: 0.000382, loss: 0.3999
2022-10-03 15:24:58 - train: epoch 0048, iter [00080, 01251], lr: 0.000382, loss: 0.4179
2022-10-03 15:25:17 - train: epoch 0048, iter [00090, 01251], lr: 0.000382, loss: 0.4274
2022-10-03 15:25:36 - train: epoch 0048, iter [00100, 01251], lr: 0.000382, loss: 0.4044
2022-10-03 15:25:55 - train: epoch 0048, iter [00110, 01251], lr: 0.000382, loss: 0.4149
2022-10-03 15:26:14 - train: epoch 0048, iter [00120, 01251], lr: 0.000382, loss: 0.4094
2022-10-03 15:26:33 - train: epoch 0048, iter [00130, 01251], lr: 0.000382, loss: 0.4040
2022-10-03 15:26:52 - train: epoch 0048, iter [00140, 01251], lr: 0.000382, loss: 0.3960
2022-10-03 15:27:11 - train: epoch 0048, iter [00150, 01251], lr: 0.000381, loss: 0.4127
2022-10-03 15:27:30 - train: epoch 0048, iter [00160, 01251], lr: 0.000381, loss: 0.4225
2022-10-03 15:27:49 - train: epoch 0048, iter [00170, 01251], lr: 0.000381, loss: 0.4099
2022-10-03 15:28:08 - train: epoch 0048, iter [00180, 01251], lr: 0.000381, loss: 0.4232
2022-10-03 15:28:28 - train: epoch 0048, iter [00190, 01251], lr: 0.000381, loss: 0.3901
2022-10-03 15:28:47 - train: epoch 0048, iter [00200, 01251], lr: 0.000381, loss: 0.4160
2022-10-03 15:29:06 - train: epoch 0048, iter [00210, 01251], lr: 0.000381, loss: 0.4094
2022-10-03 15:29:25 - train: epoch 0048, iter [00220, 01251], lr: 0.000381, loss: 0.4011
2022-10-03 15:29:44 - train: epoch 0048, iter [00230, 01251], lr: 0.000381, loss: 0.4151
2022-10-03 15:30:03 - train: epoch 0048, iter [00240, 01251], lr: 0.000381, loss: 0.4118
2022-10-03 15:30:22 - train: epoch 0048, iter [00250, 01251], lr: 0.000381, loss: 0.4133
2022-10-03 15:30:41 - train: epoch 0048, iter [00260, 01251], lr: 0.000381, loss: 0.4336
2022-10-03 15:31:00 - train: epoch 0048, iter [00270, 01251], lr: 0.000381, loss: 0.4151
2022-10-03 15:31:19 - train: epoch 0048, iter [00280, 01251], lr: 0.000380, loss: 0.4201
2022-10-03 15:31:38 - train: epoch 0048, iter [00290, 01251], lr: 0.000380, loss: 0.4152
2022-10-03 15:31:57 - train: epoch 0048, iter [00300, 01251], lr: 0.000380, loss: 0.4189
2022-10-03 15:32:16 - train: epoch 0048, iter [00310, 01251], lr: 0.000380, loss: 0.4377
2022-10-03 15:32:35 - train: epoch 0048, iter [00320, 01251], lr: 0.000380, loss: 0.4203
2022-10-03 15:32:53 - train: epoch 0048, iter [00330, 01251], lr: 0.000380, loss: 0.4171
2022-10-03 15:33:12 - train: epoch 0048, iter [00340, 01251], lr: 0.000380, loss: 0.4051
2022-10-03 15:33:31 - train: epoch 0048, iter [00350, 01251], lr: 0.000380, loss: 0.4243
2022-10-03 15:33:49 - train: epoch 0048, iter [00360, 01251], lr: 0.000380, loss: 0.4151
2022-10-03 15:34:08 - train: epoch 0048, iter [00370, 01251], lr: 0.000380, loss: 0.4097
2022-10-03 15:34:27 - train: epoch 0048, iter [00380, 01251], lr: 0.000380, loss: 0.4137
2022-10-03 15:34:47 - train: epoch 0048, iter [00390, 01251], lr: 0.000380, loss: 0.4147
2022-10-03 15:35:05 - train: epoch 0048, iter [00400, 01251], lr: 0.000379, loss: 0.4198
2022-10-03 15:35:24 - train: epoch 0048, iter [00410, 01251], lr: 0.000379, loss: 0.4310
2022-10-03 15:35:43 - train: epoch 0048, iter [00420, 01251], lr: 0.000379, loss: 0.4150
2022-10-03 15:36:01 - train: epoch 0048, iter [00430, 01251], lr: 0.000379, loss: 0.4142
2022-10-03 15:36:20 - train: epoch 0048, iter [00440, 01251], lr: 0.000379, loss: 0.4107
2022-10-03 15:36:38 - train: epoch 0048, iter [00450, 01251], lr: 0.000379, loss: 0.4083
2022-10-03 15:36:58 - train: epoch 0048, iter [00460, 01251], lr: 0.000379, loss: 0.4071
2022-10-03 15:37:17 - train: epoch 0048, iter [00470, 01251], lr: 0.000379, loss: 0.4076
2022-10-03 15:37:36 - train: epoch 0048, iter [00480, 01251], lr: 0.000379, loss: 0.3914
2022-10-03 15:37:55 - train: epoch 0048, iter [00490, 01251], lr: 0.000379, loss: 0.4276
2022-10-03 15:38:13 - train: epoch 0048, iter [00500, 01251], lr: 0.000379, loss: 0.4181
2022-10-03 15:38:32 - train: epoch 0048, iter [00510, 01251], lr: 0.000379, loss: 0.4032
2022-10-03 15:38:51 - train: epoch 0048, iter [00520, 01251], lr: 0.000378, loss: 0.3972
2022-10-03 15:39:10 - train: epoch 0048, iter [00530, 01251], lr: 0.000378, loss: 0.4260
2022-10-03 15:39:29 - train: epoch 0048, iter [00540, 01251], lr: 0.000378, loss: 0.4198
2022-10-03 15:39:48 - train: epoch 0048, iter [00550, 01251], lr: 0.000378, loss: 0.4050
2022-10-03 15:40:06 - train: epoch 0048, iter [00560, 01251], lr: 0.000378, loss: 0.3974
2022-10-03 15:40:25 - train: epoch 0048, iter [00570, 01251], lr: 0.000378, loss: 0.4035
2022-10-03 15:40:44 - train: epoch 0048, iter [00580, 01251], lr: 0.000378, loss: 0.4003
2022-10-03 15:41:03 - train: epoch 0048, iter [00590, 01251], lr: 0.000378, loss: 0.4033
2022-10-03 15:41:21 - train: epoch 0048, iter [00600, 01251], lr: 0.000378, loss: 0.4056
2022-10-03 15:41:40 - train: epoch 0048, iter [00610, 01251], lr: 0.000378, loss: 0.4044
2022-10-03 15:41:59 - train: epoch 0048, iter [00620, 01251], lr: 0.000378, loss: 0.4120
2022-10-03 15:42:18 - train: epoch 0048, iter [00630, 01251], lr: 0.000378, loss: 0.4172
2022-10-03 15:42:37 - train: epoch 0048, iter [00640, 01251], lr: 0.000378, loss: 0.3924
2022-10-03 15:42:56 - train: epoch 0048, iter [00650, 01251], lr: 0.000377, loss: 0.4261
2022-10-03 15:43:15 - train: epoch 0048, iter [00660, 01251], lr: 0.000377, loss: 0.4221
2022-10-03 15:43:34 - train: epoch 0048, iter [00670, 01251], lr: 0.000377, loss: 0.3886
2022-10-03 15:43:53 - train: epoch 0048, iter [00680, 01251], lr: 0.000377, loss: 0.4199
2022-10-03 15:44:12 - train: epoch 0048, iter [00690, 01251], lr: 0.000377, loss: 0.4280
2022-10-03 15:44:30 - train: epoch 0048, iter [00700, 01251], lr: 0.000377, loss: 0.4200
2022-10-03 15:44:49 - train: epoch 0048, iter [00710, 01251], lr: 0.000377, loss: 0.4190
2022-10-03 15:45:08 - train: epoch 0048, iter [00720, 01251], lr: 0.000377, loss: 0.4100
2022-10-03 15:45:27 - train: epoch 0048, iter [00730, 01251], lr: 0.000377, loss: 0.4093
2022-10-03 15:45:46 - train: epoch 0048, iter [00740, 01251], lr: 0.000377, loss: 0.3971
2022-10-03 15:46:05 - train: epoch 0048, iter [00750, 01251], lr: 0.000377, loss: 0.4144
2022-10-03 15:46:23 - train: epoch 0048, iter [00760, 01251], lr: 0.000377, loss: 0.4402
2022-10-03 15:46:42 - train: epoch 0048, iter [00770, 01251], lr: 0.000376, loss: 0.4235
2022-10-03 15:47:00 - train: epoch 0048, iter [00780, 01251], lr: 0.000376, loss: 0.4172
2022-10-03 15:47:19 - train: epoch 0048, iter [00790, 01251], lr: 0.000376, loss: 0.4126
2022-10-03 15:47:38 - train: epoch 0048, iter [00800, 01251], lr: 0.000376, loss: 0.4066
2022-10-03 15:47:56 - train: epoch 0048, iter [00810, 01251], lr: 0.000376, loss: 0.4238
2022-10-03 15:48:15 - train: epoch 0048, iter [00820, 01251], lr: 0.000376, loss: 0.4252
2022-10-03 15:48:33 - train: epoch 0048, iter [00830, 01251], lr: 0.000376, loss: 0.4087
2022-10-03 15:48:53 - train: epoch 0048, iter [00840, 01251], lr: 0.000376, loss: 0.3973
2022-10-03 15:49:12 - train: epoch 0048, iter [00850, 01251], lr: 0.000376, loss: 0.3990
2022-10-03 15:49:30 - train: epoch 0048, iter [00860, 01251], lr: 0.000376, loss: 0.4290
2022-10-03 15:49:49 - train: epoch 0048, iter [00870, 01251], lr: 0.000376, loss: 0.4277
2022-10-03 15:50:08 - train: epoch 0048, iter [00880, 01251], lr: 0.000376, loss: 0.4115
2022-10-03 15:50:27 - train: epoch 0048, iter [00890, 01251], lr: 0.000376, loss: 0.4168
2022-10-03 15:50:45 - train: epoch 0048, iter [00900, 01251], lr: 0.000375, loss: 0.4126
2022-10-03 15:51:04 - train: epoch 0048, iter [00910, 01251], lr: 0.000375, loss: 0.4056
2022-10-03 15:51:23 - train: epoch 0048, iter [00920, 01251], lr: 0.000375, loss: 0.4013
2022-10-03 15:51:42 - train: epoch 0048, iter [00930, 01251], lr: 0.000375, loss: 0.3880
2022-10-03 15:52:01 - train: epoch 0048, iter [00940, 01251], lr: 0.000375, loss: 0.4109
2022-10-03 15:52:20 - train: epoch 0048, iter [00950, 01251], lr: 0.000375, loss: 0.4270
2022-10-03 15:52:40 - train: epoch 0048, iter [00960, 01251], lr: 0.000375, loss: 0.4014
2022-10-03 15:52:58 - train: epoch 0048, iter [00970, 01251], lr: 0.000375, loss: 0.4265
2022-10-03 15:53:17 - train: epoch 0048, iter [00980, 01251], lr: 0.000375, loss: 0.4256
2022-10-03 15:53:37 - train: epoch 0048, iter [00990, 01251], lr: 0.000375, loss: 0.4121
2022-10-03 15:53:55 - train: epoch 0048, iter [01000, 01251], lr: 0.000375, loss: 0.4246
2022-10-03 15:54:14 - train: epoch 0048, iter [01010, 01251], lr: 0.000375, loss: 0.4097
2022-10-03 15:54:33 - train: epoch 0048, iter [01020, 01251], lr: 0.000374, loss: 0.3971
2022-10-03 15:54:52 - train: epoch 0048, iter [01030, 01251], lr: 0.000374, loss: 0.4039
2022-10-03 15:55:11 - train: epoch 0048, iter [01040, 01251], lr: 0.000374, loss: 0.4114
2022-10-03 15:55:29 - train: epoch 0048, iter [01050, 01251], lr: 0.000374, loss: 0.4390
2022-10-03 15:55:48 - train: epoch 0048, iter [01060, 01251], lr: 0.000374, loss: 0.4271
2022-10-03 15:56:07 - train: epoch 0048, iter [01070, 01251], lr: 0.000374, loss: 0.4007
2022-10-03 15:56:26 - train: epoch 0048, iter [01080, 01251], lr: 0.000374, loss: 0.4281
2022-10-03 15:56:45 - train: epoch 0048, iter [01090, 01251], lr: 0.000374, loss: 0.4116
2022-10-03 15:57:04 - train: epoch 0048, iter [01100, 01251], lr: 0.000374, loss: 0.4180
2022-10-03 15:57:23 - train: epoch 0048, iter [01110, 01251], lr: 0.000374, loss: 0.4195
2022-10-03 15:57:41 - train: epoch 0048, iter [01120, 01251], lr: 0.000374, loss: 0.4076
2022-10-03 15:58:00 - train: epoch 0048, iter [01130, 01251], lr: 0.000374, loss: 0.4287
2022-10-03 15:58:19 - train: epoch 0048, iter [01140, 01251], lr: 0.000373, loss: 0.4149
2022-10-03 15:58:39 - train: epoch 0048, iter [01150, 01251], lr: 0.000373, loss: 0.4143
2022-10-03 15:58:58 - train: epoch 0048, iter [01160, 01251], lr: 0.000373, loss: 0.3993
2022-10-03 15:59:17 - train: epoch 0048, iter [01170, 01251], lr: 0.000373, loss: 0.4135
2022-10-03 15:59:36 - train: epoch 0048, iter [01180, 01251], lr: 0.000373, loss: 0.4227
2022-10-03 15:59:54 - train: epoch 0048, iter [01190, 01251], lr: 0.000373, loss: 0.3882
2022-10-03 16:00:14 - train: epoch 0048, iter [01200, 01251], lr: 0.000373, loss: 0.4269
2022-10-03 16:00:32 - train: epoch 0048, iter [01210, 01251], lr: 0.000373, loss: 0.4161
2022-10-03 16:00:51 - train: epoch 0048, iter [01220, 01251], lr: 0.000373, loss: 0.4108
2022-10-03 16:01:10 - train: epoch 0048, iter [01230, 01251], lr: 0.000373, loss: 0.4004
2022-10-03 16:01:29 - train: epoch 0048, iter [01240, 01251], lr: 0.000373, loss: 0.3978
2022-10-03 16:01:47 - train: epoch 0048, iter [01250, 01251], lr: 0.000373, loss: 0.4189
2022-10-03 16:01:52 - train: epoch 048, train_loss: 0.4114
2022-10-03 16:01:57 - until epoch: 048, best_loss: 0.4114
2022-10-03 16:01:57 - epoch 049 lr: 0.000373
2022-10-03 16:02:21 - train: epoch 0049, iter [00010, 01251], lr: 0.000372, loss: 0.4164
2022-10-03 16:02:40 - train: epoch 0049, iter [00020, 01251], lr: 0.000372, loss: 0.3889
2022-10-03 16:02:59 - train: epoch 0049, iter [00030, 01251], lr: 0.000372, loss: 0.3893
2022-10-03 16:03:17 - train: epoch 0049, iter [00040, 01251], lr: 0.000372, loss: 0.3979
2022-10-03 16:03:35 - train: epoch 0049, iter [00050, 01251], lr: 0.000372, loss: 0.4100
2022-10-03 16:03:54 - train: epoch 0049, iter [00060, 01251], lr: 0.000372, loss: 0.3997
2022-10-03 16:04:13 - train: epoch 0049, iter [00070, 01251], lr: 0.000372, loss: 0.4201
2022-10-03 16:04:31 - train: epoch 0049, iter [00080, 01251], lr: 0.000372, loss: 0.4011
2022-10-03 16:04:50 - train: epoch 0049, iter [00090, 01251], lr: 0.000372, loss: 0.4128
2022-10-03 16:05:08 - train: epoch 0049, iter [00100, 01251], lr: 0.000372, loss: 0.4067
2022-10-03 16:05:27 - train: epoch 0049, iter [00110, 01251], lr: 0.000372, loss: 0.4195
2022-10-03 16:05:46 - train: epoch 0049, iter [00120, 01251], lr: 0.000372, loss: 0.4226
2022-10-03 16:06:05 - train: epoch 0049, iter [00130, 01251], lr: 0.000372, loss: 0.4162
2022-10-03 16:06:23 - train: epoch 0049, iter [00140, 01251], lr: 0.000371, loss: 0.4212
2022-10-03 16:06:42 - train: epoch 0049, iter [00150, 01251], lr: 0.000371, loss: 0.4009
2022-10-03 16:07:01 - train: epoch 0049, iter [00160, 01251], lr: 0.000371, loss: 0.4025
2022-10-03 16:07:19 - train: epoch 0049, iter [00170, 01251], lr: 0.000371, loss: 0.4172
2022-10-03 16:07:38 - train: epoch 0049, iter [00180, 01251], lr: 0.000371, loss: 0.4089
2022-10-03 16:07:57 - train: epoch 0049, iter [00190, 01251], lr: 0.000371, loss: 0.4021
2022-10-03 16:08:15 - train: epoch 0049, iter [00200, 01251], lr: 0.000371, loss: 0.4168
2022-10-03 16:08:34 - train: epoch 0049, iter [00210, 01251], lr: 0.000371, loss: 0.3895
2022-10-03 16:08:53 - train: epoch 0049, iter [00220, 01251], lr: 0.000371, loss: 0.4073
2022-10-03 16:09:12 - train: epoch 0049, iter [00230, 01251], lr: 0.000371, loss: 0.4187
2022-10-03 16:09:31 - train: epoch 0049, iter [00240, 01251], lr: 0.000371, loss: 0.4170
2022-10-03 16:09:49 - train: epoch 0049, iter [00250, 01251], lr: 0.000371, loss: 0.3989
2022-10-03 16:10:08 - train: epoch 0049, iter [00260, 01251], lr: 0.000370, loss: 0.3923
2022-10-03 16:10:27 - train: epoch 0049, iter [00270, 01251], lr: 0.000370, loss: 0.4057
2022-10-03 16:10:46 - train: epoch 0049, iter [00280, 01251], lr: 0.000370, loss: 0.4218
2022-10-03 16:11:05 - train: epoch 0049, iter [00290, 01251], lr: 0.000370, loss: 0.4401
2022-10-03 16:11:24 - train: epoch 0049, iter [00300, 01251], lr: 0.000370, loss: 0.4161
2022-10-03 16:11:43 - train: epoch 0049, iter [00310, 01251], lr: 0.000370, loss: 0.4033
2022-10-03 16:12:01 - train: epoch 0049, iter [00320, 01251], lr: 0.000370, loss: 0.4014
2022-10-03 16:12:20 - train: epoch 0049, iter [00330, 01251], lr: 0.000370, loss: 0.3875
2022-10-03 16:12:39 - train: epoch 0049, iter [00340, 01251], lr: 0.000370, loss: 0.4140
2022-10-03 16:12:58 - train: epoch 0049, iter [00350, 01251], lr: 0.000370, loss: 0.4168
2022-10-03 16:13:17 - train: epoch 0049, iter [00360, 01251], lr: 0.000370, loss: 0.4152
2022-10-03 16:13:36 - train: epoch 0049, iter [00370, 01251], lr: 0.000370, loss: 0.4073
2022-10-03 16:13:54 - train: epoch 0049, iter [00380, 01251], lr: 0.000369, loss: 0.4145
2022-10-03 16:14:13 - train: epoch 0049, iter [00390, 01251], lr: 0.000369, loss: 0.4113
2022-10-03 16:14:32 - train: epoch 0049, iter [00400, 01251], lr: 0.000369, loss: 0.4051
2022-10-03 16:14:51 - train: epoch 0049, iter [00410, 01251], lr: 0.000369, loss: 0.4040
2022-10-03 16:15:10 - train: epoch 0049, iter [00420, 01251], lr: 0.000369, loss: 0.4174
2022-10-03 16:15:29 - train: epoch 0049, iter [00430, 01251], lr: 0.000369, loss: 0.3898
2022-10-03 16:15:48 - train: epoch 0049, iter [00440, 01251], lr: 0.000369, loss: 0.3993
2022-10-03 16:16:07 - train: epoch 0049, iter [00450, 01251], lr: 0.000369, loss: 0.4354
2022-10-03 16:16:27 - train: epoch 0049, iter [00460, 01251], lr: 0.000369, loss: 0.4204
2022-10-03 16:16:46 - train: epoch 0049, iter [00470, 01251], lr: 0.000369, loss: 0.4037
2022-10-03 16:17:05 - train: epoch 0049, iter [00480, 01251], lr: 0.000369, loss: 0.4144
2022-10-03 16:17:24 - train: epoch 0049, iter [00490, 01251], lr: 0.000369, loss: 0.4217
2022-10-03 16:17:43 - train: epoch 0049, iter [00500, 01251], lr: 0.000369, loss: 0.4045
2022-10-03 16:18:02 - train: epoch 0049, iter [00510, 01251], lr: 0.000368, loss: 0.4278
2022-10-03 16:18:21 - train: epoch 0049, iter [00520, 01251], lr: 0.000368, loss: 0.4204
2022-10-03 16:18:40 - train: epoch 0049, iter [00530, 01251], lr: 0.000368, loss: 0.4228
2022-10-03 16:18:59 - train: epoch 0049, iter [00540, 01251], lr: 0.000368, loss: 0.4159
2022-10-03 16:19:17 - train: epoch 0049, iter [00550, 01251], lr: 0.000368, loss: 0.4039
2022-10-03 16:19:36 - train: epoch 0049, iter [00560, 01251], lr: 0.000368, loss: 0.4035
2022-10-03 16:19:54 - train: epoch 0049, iter [00570, 01251], lr: 0.000368, loss: 0.4027
2022-10-03 16:20:13 - train: epoch 0049, iter [00580, 01251], lr: 0.000368, loss: 0.4248
2022-10-03 16:20:32 - train: epoch 0049, iter [00590, 01251], lr: 0.000368, loss: 0.4090
2022-10-03 16:20:51 - train: epoch 0049, iter [00600, 01251], lr: 0.000368, loss: 0.4124
2022-10-03 16:21:10 - train: epoch 0049, iter [00610, 01251], lr: 0.000368, loss: 0.3970
2022-10-03 16:21:28 - train: epoch 0049, iter [00620, 01251], lr: 0.000368, loss: 0.4206
2022-10-03 16:21:47 - train: epoch 0049, iter [00630, 01251], lr: 0.000367, loss: 0.3972
2022-10-03 16:22:06 - train: epoch 0049, iter [00640, 01251], lr: 0.000367, loss: 0.4345
2022-10-03 16:22:25 - train: epoch 0049, iter [00650, 01251], lr: 0.000367, loss: 0.4142
2022-10-03 16:22:44 - train: epoch 0049, iter [00660, 01251], lr: 0.000367, loss: 0.4275
2022-10-03 16:23:03 - train: epoch 0049, iter [00670, 01251], lr: 0.000367, loss: 0.4141
2022-10-03 16:23:22 - train: epoch 0049, iter [00680, 01251], lr: 0.000367, loss: 0.4245
2022-10-03 16:23:41 - train: epoch 0049, iter [00690, 01251], lr: 0.000367, loss: 0.4086
2022-10-03 16:23:59 - train: epoch 0049, iter [00700, 01251], lr: 0.000367, loss: 0.4098
2022-10-03 16:24:18 - train: epoch 0049, iter [00710, 01251], lr: 0.000367, loss: 0.4244
2022-10-03 16:24:37 - train: epoch 0049, iter [00720, 01251], lr: 0.000367, loss: 0.4032
2022-10-03 16:24:56 - train: epoch 0049, iter [00730, 01251], lr: 0.000367, loss: 0.3991
2022-10-03 16:25:16 - train: epoch 0049, iter [00740, 01251], lr: 0.000367, loss: 0.4069
2022-10-03 16:25:35 - train: epoch 0049, iter [00750, 01251], lr: 0.000366, loss: 0.4116
2022-10-03 16:25:54 - train: epoch 0049, iter [00760, 01251], lr: 0.000366, loss: 0.4115
2022-10-03 16:26:13 - train: epoch 0049, iter [00770, 01251], lr: 0.000366, loss: 0.4066
2022-10-03 16:26:32 - train: epoch 0049, iter [00780, 01251], lr: 0.000366, loss: 0.4077
2022-10-03 16:26:51 - train: epoch 0049, iter [00790, 01251], lr: 0.000366, loss: 0.4198
2022-10-03 16:27:09 - train: epoch 0049, iter [00800, 01251], lr: 0.000366, loss: 0.4161
2022-10-03 16:27:28 - train: epoch 0049, iter [00810, 01251], lr: 0.000366, loss: 0.3923
2022-10-03 16:27:48 - train: epoch 0049, iter [00820, 01251], lr: 0.000366, loss: 0.3943
2022-10-03 16:28:06 - train: epoch 0049, iter [00830, 01251], lr: 0.000366, loss: 0.4095
2022-10-03 16:28:25 - train: epoch 0049, iter [00840, 01251], lr: 0.000366, loss: 0.4055
2022-10-03 16:28:44 - train: epoch 0049, iter [00850, 01251], lr: 0.000366, loss: 0.3971
2022-10-03 16:29:02 - train: epoch 0049, iter [00860, 01251], lr: 0.000366, loss: 0.4159
2022-10-03 16:29:21 - train: epoch 0049, iter [00870, 01251], lr: 0.000365, loss: 0.4033
2022-10-03 16:29:40 - train: epoch 0049, iter [00880, 01251], lr: 0.000365, loss: 0.4345
2022-10-03 16:29:59 - train: epoch 0049, iter [00890, 01251], lr: 0.000365, loss: 0.4319
2022-10-03 16:30:18 - train: epoch 0049, iter [00900, 01251], lr: 0.000365, loss: 0.4078
2022-10-03 16:30:37 - train: epoch 0049, iter [00910, 01251], lr: 0.000365, loss: 0.4019
2022-10-03 16:30:56 - train: epoch 0049, iter [00920, 01251], lr: 0.000365, loss: 0.4342
2022-10-03 16:31:14 - train: epoch 0049, iter [00930, 01251], lr: 0.000365, loss: 0.3985
2022-10-03 16:31:33 - train: epoch 0049, iter [00940, 01251], lr: 0.000365, loss: 0.3994
2022-10-03 16:31:52 - train: epoch 0049, iter [00950, 01251], lr: 0.000365, loss: 0.4168
2022-10-03 16:32:11 - train: epoch 0049, iter [00960, 01251], lr: 0.000365, loss: 0.3893
2022-10-03 16:32:30 - train: epoch 0049, iter [00970, 01251], lr: 0.000365, loss: 0.4023
2022-10-03 16:32:48 - train: epoch 0049, iter [00980, 01251], lr: 0.000365, loss: 0.4151
2022-10-03 16:33:07 - train: epoch 0049, iter [00990, 01251], lr: 0.000365, loss: 0.4180
2022-10-03 16:33:25 - train: epoch 0049, iter [01000, 01251], lr: 0.000364, loss: 0.4078
2022-10-03 16:33:44 - train: epoch 0049, iter [01010, 01251], lr: 0.000364, loss: 0.4148
2022-10-03 16:34:03 - train: epoch 0049, iter [01020, 01251], lr: 0.000364, loss: 0.4327
2022-10-03 16:34:22 - train: epoch 0049, iter [01030, 01251], lr: 0.000364, loss: 0.4175
2022-10-03 16:34:40 - train: epoch 0049, iter [01040, 01251], lr: 0.000364, loss: 0.3866
2022-10-03 16:34:59 - train: epoch 0049, iter [01050, 01251], lr: 0.000364, loss: 0.4302
2022-10-03 16:35:18 - train: epoch 0049, iter [01060, 01251], lr: 0.000364, loss: 0.4137
2022-10-03 16:35:37 - train: epoch 0049, iter [01070, 01251], lr: 0.000364, loss: 0.4085
2022-10-03 16:35:56 - train: epoch 0049, iter [01080, 01251], lr: 0.000364, loss: 0.4098
2022-10-03 16:36:14 - train: epoch 0049, iter [01090, 01251], lr: 0.000364, loss: 0.4263
2022-10-03 16:36:33 - train: epoch 0049, iter [01100, 01251], lr: 0.000364, loss: 0.4204
2022-10-03 16:36:52 - train: epoch 0049, iter [01110, 01251], lr: 0.000364, loss: 0.4188
2022-10-03 16:37:11 - train: epoch 0049, iter [01120, 01251], lr: 0.000363, loss: 0.3945
2022-10-03 16:37:30 - train: epoch 0049, iter [01130, 01251], lr: 0.000363, loss: 0.4231
2022-10-03 16:37:48 - train: epoch 0049, iter [01140, 01251], lr: 0.000363, loss: 0.4201
2022-10-03 16:38:07 - train: epoch 0049, iter [01150, 01251], lr: 0.000363, loss: 0.4012
2022-10-03 16:38:27 - train: epoch 0049, iter [01160, 01251], lr: 0.000363, loss: 0.4157
2022-10-03 16:38:46 - train: epoch 0049, iter [01170, 01251], lr: 0.000363, loss: 0.4132
2022-10-03 16:39:05 - train: epoch 0049, iter [01180, 01251], lr: 0.000363, loss: 0.4098
2022-10-03 16:39:24 - train: epoch 0049, iter [01190, 01251], lr: 0.000363, loss: 0.4070
2022-10-03 16:39:43 - train: epoch 0049, iter [01200, 01251], lr: 0.000363, loss: 0.4219
2022-10-03 16:40:01 - train: epoch 0049, iter [01210, 01251], lr: 0.000363, loss: 0.4220
2022-10-03 16:40:20 - train: epoch 0049, iter [01220, 01251], lr: 0.000363, loss: 0.4330
2022-10-03 16:40:39 - train: epoch 0049, iter [01230, 01251], lr: 0.000363, loss: 0.4168
2022-10-03 16:40:58 - train: epoch 0049, iter [01240, 01251], lr: 0.000362, loss: 0.4056
2022-10-03 16:41:16 - train: epoch 0049, iter [01250, 01251], lr: 0.000362, loss: 0.4088
2022-10-03 16:41:21 - train: epoch 049, train_loss: 0.4112
2022-10-03 16:41:26 - until epoch: 049, best_loss: 0.4112
2022-10-03 16:41:26 - epoch 050 lr: 0.000362
2022-10-03 16:41:50 - train: epoch 0050, iter [00010, 01251], lr: 0.000362, loss: 0.3967
2022-10-03 16:42:09 - train: epoch 0050, iter [00020, 01251], lr: 0.000362, loss: 0.4097
2022-10-03 16:42:28 - train: epoch 0050, iter [00030, 01251], lr: 0.000362, loss: 0.4126
2022-10-03 16:42:47 - train: epoch 0050, iter [00040, 01251], lr: 0.000362, loss: 0.4018
2022-10-03 16:43:06 - train: epoch 0050, iter [00050, 01251], lr: 0.000362, loss: 0.4270
2022-10-03 16:43:24 - train: epoch 0050, iter [00060, 01251], lr: 0.000362, loss: 0.4016
2022-10-03 16:43:43 - train: epoch 0050, iter [00070, 01251], lr: 0.000362, loss: 0.4191
2022-10-03 16:44:01 - train: epoch 0050, iter [00080, 01251], lr: 0.000362, loss: 0.4162
2022-10-03 16:44:20 - train: epoch 0050, iter [00090, 01251], lr: 0.000362, loss: 0.4067
2022-10-03 16:44:39 - train: epoch 0050, iter [00100, 01251], lr: 0.000362, loss: 0.3997
2022-10-03 16:44:58 - train: epoch 0050, iter [00110, 01251], lr: 0.000361, loss: 0.4234
2022-10-03 16:45:17 - train: epoch 0050, iter [00120, 01251], lr: 0.000361, loss: 0.3971
2022-10-03 16:45:36 - train: epoch 0050, iter [00130, 01251], lr: 0.000361, loss: 0.4131
2022-10-03 16:45:55 - train: epoch 0050, iter [00140, 01251], lr: 0.000361, loss: 0.4297
2022-10-03 16:46:14 - train: epoch 0050, iter [00150, 01251], lr: 0.000361, loss: 0.4264
2022-10-03 16:46:33 - train: epoch 0050, iter [00160, 01251], lr: 0.000361, loss: 0.4100
2022-10-03 16:46:52 - train: epoch 0050, iter [00170, 01251], lr: 0.000361, loss: 0.4196
2022-10-03 16:47:11 - train: epoch 0050, iter [00180, 01251], lr: 0.000361, loss: 0.3893
2022-10-03 16:47:31 - train: epoch 0050, iter [00190, 01251], lr: 0.000361, loss: 0.4084
2022-10-03 16:47:50 - train: epoch 0050, iter [00200, 01251], lr: 0.000361, loss: 0.4236
2022-10-03 16:48:08 - train: epoch 0050, iter [00210, 01251], lr: 0.000361, loss: 0.3921
2022-10-03 16:48:27 - train: epoch 0050, iter [00220, 01251], lr: 0.000361, loss: 0.4052
2022-10-03 16:48:46 - train: epoch 0050, iter [00230, 01251], lr: 0.000360, loss: 0.4171
2022-10-03 16:49:04 - train: epoch 0050, iter [00240, 01251], lr: 0.000360, loss: 0.4041
2022-10-03 16:49:23 - train: epoch 0050, iter [00250, 01251], lr: 0.000360, loss: 0.4062
2022-10-03 16:49:41 - train: epoch 0050, iter [00260, 01251], lr: 0.000360, loss: 0.4141
2022-10-03 16:50:00 - train: epoch 0050, iter [00270, 01251], lr: 0.000360, loss: 0.4170
2022-10-03 16:50:19 - train: epoch 0050, iter [00280, 01251], lr: 0.000360, loss: 0.4109
2022-10-03 16:50:37 - train: epoch 0050, iter [00290, 01251], lr: 0.000360, loss: 0.4252
2022-10-03 16:50:56 - train: epoch 0050, iter [00300, 01251], lr: 0.000360, loss: 0.3899
2022-10-03 16:51:14 - train: epoch 0050, iter [00310, 01251], lr: 0.000360, loss: 0.4032
2022-10-03 16:51:33 - train: epoch 0050, iter [00320, 01251], lr: 0.000360, loss: 0.3949
2022-10-03 16:51:51 - train: epoch 0050, iter [00330, 01251], lr: 0.000360, loss: 0.4285
2022-10-03 16:52:10 - train: epoch 0050, iter [00340, 01251], lr: 0.000360, loss: 0.4070
2022-10-03 16:52:29 - train: epoch 0050, iter [00350, 01251], lr: 0.000360, loss: 0.4150
2022-10-03 16:52:47 - train: epoch 0050, iter [00360, 01251], lr: 0.000359, loss: 0.4255
2022-10-03 16:53:06 - train: epoch 0050, iter [00370, 01251], lr: 0.000359, loss: 0.4246
2022-10-03 16:53:25 - train: epoch 0050, iter [00380, 01251], lr: 0.000359, loss: 0.4132
2022-10-03 16:53:44 - train: epoch 0050, iter [00390, 01251], lr: 0.000359, loss: 0.4136
2022-10-03 16:54:02 - train: epoch 0050, iter [00400, 01251], lr: 0.000359, loss: 0.4165
2022-10-03 16:54:21 - train: epoch 0050, iter [00410, 01251], lr: 0.000359, loss: 0.4043
2022-10-03 16:54:40 - train: epoch 0050, iter [00420, 01251], lr: 0.000359, loss: 0.4266
2022-10-03 16:54:59 - train: epoch 0050, iter [00430, 01251], lr: 0.000359, loss: 0.4001
2022-10-03 16:55:17 - train: epoch 0050, iter [00440, 01251], lr: 0.000359, loss: 0.4220
2022-10-03 16:55:36 - train: epoch 0050, iter [00450, 01251], lr: 0.000359, loss: 0.4073
2022-10-03 16:55:55 - train: epoch 0050, iter [00460, 01251], lr: 0.000359, loss: 0.4021
2022-10-03 16:56:13 - train: epoch 0050, iter [00470, 01251], lr: 0.000359, loss: 0.4172
2022-10-03 16:56:32 - train: epoch 0050, iter [00480, 01251], lr: 0.000358, loss: 0.4038
2022-10-03 16:56:51 - train: epoch 0050, iter [00490, 01251], lr: 0.000358, loss: 0.4134
2022-10-03 16:57:09 - train: epoch 0050, iter [00500, 01251], lr: 0.000358, loss: 0.4122
2022-10-03 16:57:28 - train: epoch 0050, iter [00510, 01251], lr: 0.000358, loss: 0.4023
2022-10-03 16:57:46 - train: epoch 0050, iter [00520, 01251], lr: 0.000358, loss: 0.4294
2022-10-03 16:58:05 - train: epoch 0050, iter [00530, 01251], lr: 0.000358, loss: 0.4130
2022-10-03 16:58:24 - train: epoch 0050, iter [00540, 01251], lr: 0.000358, loss: 0.4060
2022-10-03 16:58:43 - train: epoch 0050, iter [00550, 01251], lr: 0.000358, loss: 0.4082
2022-10-03 16:59:02 - train: epoch 0050, iter [00560, 01251], lr: 0.000358, loss: 0.4103
2022-10-03 16:59:21 - train: epoch 0050, iter [00570, 01251], lr: 0.000358, loss: 0.4276
2022-10-03 16:59:39 - train: epoch 0050, iter [00580, 01251], lr: 0.000358, loss: 0.4228
2022-10-03 16:59:58 - train: epoch 0050, iter [00590, 01251], lr: 0.000358, loss: 0.4091
2022-10-03 17:00:18 - train: epoch 0050, iter [00600, 01251], lr: 0.000357, loss: 0.4243
2022-10-03 17:00:36 - train: epoch 0050, iter [00610, 01251], lr: 0.000357, loss: 0.4312
2022-10-03 17:00:55 - train: epoch 0050, iter [00620, 01251], lr: 0.000357, loss: 0.4088
2022-10-03 17:01:14 - train: epoch 0050, iter [00630, 01251], lr: 0.000357, loss: 0.4330
2022-10-03 17:01:33 - train: epoch 0050, iter [00640, 01251], lr: 0.000357, loss: 0.4186
2022-10-03 17:01:51 - train: epoch 0050, iter [00650, 01251], lr: 0.000357, loss: 0.4032
2022-10-03 17:02:10 - train: epoch 0050, iter [00660, 01251], lr: 0.000357, loss: 0.3912
2022-10-03 17:02:29 - train: epoch 0050, iter [00670, 01251], lr: 0.000357, loss: 0.4014
2022-10-03 17:02:48 - train: epoch 0050, iter [00680, 01251], lr: 0.000357, loss: 0.3953
2022-10-03 17:03:06 - train: epoch 0050, iter [00690, 01251], lr: 0.000357, loss: 0.4126
2022-10-03 17:03:25 - train: epoch 0050, iter [00700, 01251], lr: 0.000357, loss: 0.3978
2022-10-03 17:03:44 - train: epoch 0050, iter [00710, 01251], lr: 0.000357, loss: 0.4134
2022-10-03 17:04:03 - train: epoch 0050, iter [00720, 01251], lr: 0.000356, loss: 0.4072
2022-10-03 17:04:22 - train: epoch 0050, iter [00730, 01251], lr: 0.000356, loss: 0.4133
2022-10-03 17:04:41 - train: epoch 0050, iter [00740, 01251], lr: 0.000356, loss: 0.3912
2022-10-03 17:05:00 - train: epoch 0050, iter [00750, 01251], lr: 0.000356, loss: 0.4190
2022-10-03 17:05:19 - train: epoch 0050, iter [00760, 01251], lr: 0.000356, loss: 0.4010
2022-10-03 17:05:38 - train: epoch 0050, iter [00770, 01251], lr: 0.000356, loss: 0.4145
2022-10-03 17:05:57 - train: epoch 0050, iter [00780, 01251], lr: 0.000356, loss: 0.4300
2022-10-03 17:06:16 - train: epoch 0050, iter [00790, 01251], lr: 0.000356, loss: 0.4156
2022-10-03 17:06:35 - train: epoch 0050, iter [00800, 01251], lr: 0.000356, loss: 0.4061
2022-10-03 17:06:54 - train: epoch 0050, iter [00810, 01251], lr: 0.000356, loss: 0.4030
2022-10-03 17:07:13 - train: epoch 0050, iter [00820, 01251], lr: 0.000356, loss: 0.3940
2022-10-03 17:07:32 - train: epoch 0050, iter [00830, 01251], lr: 0.000356, loss: 0.4069
2022-10-03 17:07:51 - train: epoch 0050, iter [00840, 01251], lr: 0.000355, loss: 0.3830
2022-10-03 17:08:09 - train: epoch 0050, iter [00850, 01251], lr: 0.000355, loss: 0.4091
2022-10-03 17:08:28 - train: epoch 0050, iter [00860, 01251], lr: 0.000355, loss: 0.3976
2022-10-03 17:08:47 - train: epoch 0050, iter [00870, 01251], lr: 0.000355, loss: 0.4253
2022-10-03 17:09:06 - train: epoch 0050, iter [00880, 01251], lr: 0.000355, loss: 0.4210
2022-10-03 17:09:25 - train: epoch 0050, iter [00890, 01251], lr: 0.000355, loss: 0.4205
2022-10-03 17:09:44 - train: epoch 0050, iter [00900, 01251], lr: 0.000355, loss: 0.3967
2022-10-03 17:10:03 - train: epoch 0050, iter [00910, 01251], lr: 0.000355, loss: 0.4010
2022-10-03 17:10:22 - train: epoch 0050, iter [00920, 01251], lr: 0.000355, loss: 0.4137
2022-10-03 17:10:41 - train: epoch 0050, iter [00930, 01251], lr: 0.000355, loss: 0.4100
2022-10-03 17:11:00 - train: epoch 0050, iter [00940, 01251], lr: 0.000355, loss: 0.4024
2022-10-03 17:11:19 - train: epoch 0050, iter [00950, 01251], lr: 0.000355, loss: 0.4164
2022-10-03 17:11:38 - train: epoch 0050, iter [00960, 01251], lr: 0.000354, loss: 0.3982
2022-10-03 17:11:58 - train: epoch 0050, iter [00970, 01251], lr: 0.000354, loss: 0.4229
2022-10-03 17:12:17 - train: epoch 0050, iter [00980, 01251], lr: 0.000354, loss: 0.3873
2022-10-03 17:12:36 - train: epoch 0050, iter [00990, 01251], lr: 0.000354, loss: 0.3933
2022-10-03 17:12:55 - train: epoch 0050, iter [01000, 01251], lr: 0.000354, loss: 0.4019
2022-10-03 17:13:14 - train: epoch 0050, iter [01010, 01251], lr: 0.000354, loss: 0.4177
2022-10-03 17:13:33 - train: epoch 0050, iter [01020, 01251], lr: 0.000354, loss: 0.4352
2022-10-03 17:13:52 - train: epoch 0050, iter [01030, 01251], lr: 0.000354, loss: 0.4026
2022-10-03 17:14:11 - train: epoch 0050, iter [01040, 01251], lr: 0.000354, loss: 0.4092
2022-10-03 17:14:30 - train: epoch 0050, iter [01050, 01251], lr: 0.000354, loss: 0.4162
2022-10-03 17:14:49 - train: epoch 0050, iter [01060, 01251], lr: 0.000354, loss: 0.4030
2022-10-03 17:15:07 - train: epoch 0050, iter [01070, 01251], lr: 0.000354, loss: 0.3922
2022-10-03 17:15:26 - train: epoch 0050, iter [01080, 01251], lr: 0.000354, loss: 0.4272
2022-10-03 17:15:44 - train: epoch 0050, iter [01090, 01251], lr: 0.000353, loss: 0.4326
2022-10-03 17:16:03 - train: epoch 0050, iter [01100, 01251], lr: 0.000353, loss: 0.3962
2022-10-03 17:16:22 - train: epoch 0050, iter [01110, 01251], lr: 0.000353, loss: 0.4291
2022-10-03 17:16:40 - train: epoch 0050, iter [01120, 01251], lr: 0.000353, loss: 0.4223
2022-10-03 17:16:59 - train: epoch 0050, iter [01130, 01251], lr: 0.000353, loss: 0.4006
2022-10-03 17:17:18 - train: epoch 0050, iter [01140, 01251], lr: 0.000353, loss: 0.4040
2022-10-03 17:17:37 - train: epoch 0050, iter [01150, 01251], lr: 0.000353, loss: 0.4090
2022-10-03 17:17:55 - train: epoch 0050, iter [01160, 01251], lr: 0.000353, loss: 0.4079
2022-10-03 17:18:14 - train: epoch 0050, iter [01170, 01251], lr: 0.000353, loss: 0.4029
2022-10-03 17:18:33 - train: epoch 0050, iter [01180, 01251], lr: 0.000353, loss: 0.4256
2022-10-03 17:18:51 - train: epoch 0050, iter [01190, 01251], lr: 0.000353, loss: 0.4156
2022-10-03 17:19:10 - train: epoch 0050, iter [01200, 01251], lr: 0.000353, loss: 0.4139
2022-10-03 17:19:29 - train: epoch 0050, iter [01210, 01251], lr: 0.000352, loss: 0.4033
2022-10-03 17:19:48 - train: epoch 0050, iter [01220, 01251], lr: 0.000352, loss: 0.4144
2022-10-03 17:20:06 - train: epoch 0050, iter [01230, 01251], lr: 0.000352, loss: 0.3931
2022-10-03 17:20:25 - train: epoch 0050, iter [01240, 01251], lr: 0.000352, loss: 0.4139
2022-10-03 17:20:43 - train: epoch 0050, iter [01250, 01251], lr: 0.000352, loss: 0.4096
2022-10-03 17:20:48 - train: epoch 050, train_loss: 0.4109
2022-10-03 17:20:53 - until epoch: 050, best_loss: 0.4109
2022-10-03 17:20:53 - epoch 051 lr: 0.000352
2022-10-03 17:21:17 - train: epoch 0051, iter [00010, 01251], lr: 0.000352, loss: 0.3959
2022-10-03 17:21:35 - train: epoch 0051, iter [00020, 01251], lr: 0.000352, loss: 0.4003
2022-10-03 17:21:54 - train: epoch 0051, iter [00030, 01251], lr: 0.000352, loss: 0.4034
2022-10-03 17:22:12 - train: epoch 0051, iter [00040, 01251], lr: 0.000352, loss: 0.4205
2022-10-03 17:22:30 - train: epoch 0051, iter [00050, 01251], lr: 0.000352, loss: 0.4356
2022-10-03 17:22:49 - train: epoch 0051, iter [00060, 01251], lr: 0.000352, loss: 0.4107
2022-10-03 17:23:07 - train: epoch 0051, iter [00070, 01251], lr: 0.000352, loss: 0.4106
2022-10-03 17:23:26 - train: epoch 0051, iter [00080, 01251], lr: 0.000351, loss: 0.4159
2022-10-03 17:23:45 - train: epoch 0051, iter [00090, 01251], lr: 0.000351, loss: 0.4034
2022-10-03 17:24:04 - train: epoch 0051, iter [00100, 01251], lr: 0.000351, loss: 0.4334
2022-10-03 17:24:22 - train: epoch 0051, iter [00110, 01251], lr: 0.000351, loss: 0.4022
2022-10-03 17:24:41 - train: epoch 0051, iter [00120, 01251], lr: 0.000351, loss: 0.4009
2022-10-03 17:24:59 - train: epoch 0051, iter [00130, 01251], lr: 0.000351, loss: 0.4130
2022-10-03 17:25:18 - train: epoch 0051, iter [00140, 01251], lr: 0.000351, loss: 0.4028
2022-10-03 17:25:36 - train: epoch 0051, iter [00150, 01251], lr: 0.000351, loss: 0.4166
2022-10-03 17:25:55 - train: epoch 0051, iter [00160, 01251], lr: 0.000351, loss: 0.4262
2022-10-03 17:26:14 - train: epoch 0051, iter [00170, 01251], lr: 0.000351, loss: 0.4222
2022-10-03 17:26:32 - train: epoch 0051, iter [00180, 01251], lr: 0.000351, loss: 0.4164
2022-10-03 17:26:51 - train: epoch 0051, iter [00190, 01251], lr: 0.000351, loss: 0.3991
2022-10-03 17:27:10 - train: epoch 0051, iter [00200, 01251], lr: 0.000350, loss: 0.4026
2022-10-03 17:27:29 - train: epoch 0051, iter [00210, 01251], lr: 0.000350, loss: 0.3832
2022-10-03 17:27:48 - train: epoch 0051, iter [00220, 01251], lr: 0.000350, loss: 0.4035
2022-10-03 17:28:07 - train: epoch 0051, iter [00230, 01251], lr: 0.000350, loss: 0.4097
2022-10-03 17:28:26 - train: epoch 0051, iter [00240, 01251], lr: 0.000350, loss: 0.4142
2022-10-03 17:28:44 - train: epoch 0051, iter [00250, 01251], lr: 0.000350, loss: 0.4113
2022-10-03 17:29:03 - train: epoch 0051, iter [00260, 01251], lr: 0.000350, loss: 0.3966
2022-10-03 17:29:21 - train: epoch 0051, iter [00270, 01251], lr: 0.000350, loss: 0.4298
2022-10-03 17:29:40 - train: epoch 0051, iter [00280, 01251], lr: 0.000350, loss: 0.4200
2022-10-03 17:29:59 - train: epoch 0051, iter [00290, 01251], lr: 0.000350, loss: 0.4300
2022-10-03 17:30:18 - train: epoch 0051, iter [00300, 01251], lr: 0.000350, loss: 0.4109
2022-10-03 17:30:36 - train: epoch 0051, iter [00310, 01251], lr: 0.000350, loss: 0.4070
2022-10-03 17:30:55 - train: epoch 0051, iter [00320, 01251], lr: 0.000349, loss: 0.3995
2022-10-03 17:31:14 - train: epoch 0051, iter [00330, 01251], lr: 0.000349, loss: 0.4092
2022-10-03 17:31:33 - train: epoch 0051, iter [00340, 01251], lr: 0.000349, loss: 0.4123
2022-10-03 17:31:51 - train: epoch 0051, iter [00350, 01251], lr: 0.000349, loss: 0.4077
2022-10-03 17:32:10 - train: epoch 0051, iter [00360, 01251], lr: 0.000349, loss: 0.4096
2022-10-03 17:32:29 - train: epoch 0051, iter [00370, 01251], lr: 0.000349, loss: 0.4189
2022-10-03 17:32:48 - train: epoch 0051, iter [00380, 01251], lr: 0.000349, loss: 0.4041
2022-10-03 17:33:07 - train: epoch 0051, iter [00390, 01251], lr: 0.000349, loss: 0.4208
2022-10-03 17:33:26 - train: epoch 0051, iter [00400, 01251], lr: 0.000349, loss: 0.4264
2022-10-03 17:33:45 - train: epoch 0051, iter [00410, 01251], lr: 0.000349, loss: 0.4303
2022-10-03 17:34:03 - train: epoch 0051, iter [00420, 01251], lr: 0.000349, loss: 0.3978
2022-10-03 17:34:22 - train: epoch 0051, iter [00430, 01251], lr: 0.000349, loss: 0.4063
2022-10-03 17:34:41 - train: epoch 0051, iter [00440, 01251], lr: 0.000348, loss: 0.4192
2022-10-03 17:35:00 - train: epoch 0051, iter [00450, 01251], lr: 0.000348, loss: 0.4084
2022-10-03 17:35:18 - train: epoch 0051, iter [00460, 01251], lr: 0.000348, loss: 0.3996
2022-10-03 17:35:36 - train: epoch 0051, iter [00470, 01251], lr: 0.000348, loss: 0.4085
2022-10-03 17:35:55 - train: epoch 0051, iter [00480, 01251], lr: 0.000348, loss: 0.4100
2022-10-03 17:36:14 - train: epoch 0051, iter [00490, 01251], lr: 0.000348, loss: 0.4045
2022-10-03 17:36:33 - train: epoch 0051, iter [00500, 01251], lr: 0.000348, loss: 0.4008
2022-10-03 17:36:52 - train: epoch 0051, iter [00510, 01251], lr: 0.000348, loss: 0.4028
2022-10-03 17:37:11 - train: epoch 0051, iter [00520, 01251], lr: 0.000348, loss: 0.4129
2022-10-03 17:37:30 - train: epoch 0051, iter [00530, 01251], lr: 0.000348, loss: 0.4119
2022-10-03 17:37:49 - train: epoch 0051, iter [00540, 01251], lr: 0.000348, loss: 0.4122
2022-10-03 17:38:08 - train: epoch 0051, iter [00550, 01251], lr: 0.000348, loss: 0.4240
2022-10-03 17:38:26 - train: epoch 0051, iter [00560, 01251], lr: 0.000347, loss: 0.4248
2022-10-03 17:38:45 - train: epoch 0051, iter [00570, 01251], lr: 0.000347, loss: 0.4058
2022-10-03 17:39:04 - train: epoch 0051, iter [00580, 01251], lr: 0.000347, loss: 0.4208
2022-10-03 17:39:23 - train: epoch 0051, iter [00590, 01251], lr: 0.000347, loss: 0.4046
2022-10-03 17:39:41 - train: epoch 0051, iter [00600, 01251], lr: 0.000347, loss: 0.4022
2022-10-03 17:40:00 - train: epoch 0051, iter [00610, 01251], lr: 0.000347, loss: 0.4074
2022-10-03 17:40:19 - train: epoch 0051, iter [00620, 01251], lr: 0.000347, loss: 0.4034
2022-10-03 17:40:38 - train: epoch 0051, iter [00630, 01251], lr: 0.000347, loss: 0.4273
2022-10-03 17:40:57 - train: epoch 0051, iter [00640, 01251], lr: 0.000347, loss: 0.3991
2022-10-03 17:41:16 - train: epoch 0051, iter [00650, 01251], lr: 0.000347, loss: 0.4024
2022-10-03 17:41:34 - train: epoch 0051, iter [00660, 01251], lr: 0.000347, loss: 0.3950
2022-10-03 17:41:53 - train: epoch 0051, iter [00670, 01251], lr: 0.000347, loss: 0.3964
2022-10-03 17:42:11 - train: epoch 0051, iter [00680, 01251], lr: 0.000346, loss: 0.3995
2022-10-03 17:42:30 - train: epoch 0051, iter [00690, 01251], lr: 0.000346, loss: 0.4033
2022-10-03 17:42:49 - train: epoch 0051, iter [00700, 01251], lr: 0.000346, loss: 0.4190
2022-10-03 17:43:08 - train: epoch 0051, iter [00710, 01251], lr: 0.000346, loss: 0.4109
2022-10-03 17:43:27 - train: epoch 0051, iter [00720, 01251], lr: 0.000346, loss: 0.4148
2022-10-03 17:43:46 - train: epoch 0051, iter [00730, 01251], lr: 0.000346, loss: 0.4069
2022-10-03 17:44:04 - train: epoch 0051, iter [00740, 01251], lr: 0.000346, loss: 0.3864
2022-10-03 17:44:23 - train: epoch 0051, iter [00750, 01251], lr: 0.000346, loss: 0.4230
2022-10-03 17:44:42 - train: epoch 0051, iter [00760, 01251], lr: 0.000346, loss: 0.4075
2022-10-03 17:45:00 - train: epoch 0051, iter [00770, 01251], lr: 0.000346, loss: 0.4025
2022-10-03 17:45:19 - train: epoch 0051, iter [00780, 01251], lr: 0.000346, loss: 0.4124
2022-10-03 17:45:37 - train: epoch 0051, iter [00790, 01251], lr: 0.000346, loss: 0.4058
2022-10-03 17:45:56 - train: epoch 0051, iter [00800, 01251], lr: 0.000345, loss: 0.4106
2022-10-03 17:46:15 - train: epoch 0051, iter [00810, 01251], lr: 0.000345, loss: 0.4108
2022-10-03 17:46:34 - train: epoch 0051, iter [00820, 01251], lr: 0.000345, loss: 0.4100
2022-10-03 17:46:53 - train: epoch 0051, iter [00830, 01251], lr: 0.000345, loss: 0.4019
2022-10-03 17:47:11 - train: epoch 0051, iter [00840, 01251], lr: 0.000345, loss: 0.4230
2022-10-03 17:47:31 - train: epoch 0051, iter [00850, 01251], lr: 0.000345, loss: 0.3957
2022-10-03 17:47:50 - train: epoch 0051, iter [00860, 01251], lr: 0.000345, loss: 0.4265
2022-10-03 17:48:08 - train: epoch 0051, iter [00870, 01251], lr: 0.000345, loss: 0.4089
2022-10-03 17:48:27 - train: epoch 0051, iter [00880, 01251], lr: 0.000345, loss: 0.4049
2022-10-03 17:48:46 - train: epoch 0051, iter [00890, 01251], lr: 0.000345, loss: 0.4185
2022-10-03 17:49:04 - train: epoch 0051, iter [00900, 01251], lr: 0.000345, loss: 0.4291
2022-10-03 17:49:23 - train: epoch 0051, iter [00910, 01251], lr: 0.000345, loss: 0.4072
2022-10-03 17:49:42 - train: epoch 0051, iter [00920, 01251], lr: 0.000344, loss: 0.4075
2022-10-03 17:50:01 - train: epoch 0051, iter [00930, 01251], lr: 0.000344, loss: 0.4102
2022-10-03 17:50:20 - train: epoch 0051, iter [00940, 01251], lr: 0.000344, loss: 0.4123
2022-10-03 17:50:39 - train: epoch 0051, iter [00950, 01251], lr: 0.000344, loss: 0.3992
2022-10-03 17:50:58 - train: epoch 0051, iter [00960, 01251], lr: 0.000344, loss: 0.4207
2022-10-03 17:51:17 - train: epoch 0051, iter [00970, 01251], lr: 0.000344, loss: 0.4264
2022-10-03 17:51:36 - train: epoch 0051, iter [00980, 01251], lr: 0.000344, loss: 0.3972
2022-10-03 17:51:54 - train: epoch 0051, iter [00990, 01251], lr: 0.000344, loss: 0.3809
2022-10-03 17:52:13 - train: epoch 0051, iter [01000, 01251], lr: 0.000344, loss: 0.4084
2022-10-03 17:52:32 - train: epoch 0051, iter [01010, 01251], lr: 0.000344, loss: 0.4140
2022-10-03 17:52:50 - train: epoch 0051, iter [01020, 01251], lr: 0.000344, loss: 0.3984
2022-10-03 17:53:09 - train: epoch 0051, iter [01030, 01251], lr: 0.000344, loss: 0.4079
2022-10-03 17:53:27 - train: epoch 0051, iter [01040, 01251], lr: 0.000344, loss: 0.3991
2022-10-03 17:53:46 - train: epoch 0051, iter [01050, 01251], lr: 0.000343, loss: 0.4060
2022-10-03 17:54:05 - train: epoch 0051, iter [01060, 01251], lr: 0.000343, loss: 0.4248
2022-10-03 17:54:23 - train: epoch 0051, iter [01070, 01251], lr: 0.000343, loss: 0.4200
2022-10-03 17:54:42 - train: epoch 0051, iter [01080, 01251], lr: 0.000343, loss: 0.4302
2022-10-03 17:55:01 - train: epoch 0051, iter [01090, 01251], lr: 0.000343, loss: 0.4155
2022-10-03 17:55:19 - train: epoch 0051, iter [01100, 01251], lr: 0.000343, loss: 0.4150
2022-10-03 17:55:38 - train: epoch 0051, iter [01110, 01251], lr: 0.000343, loss: 0.4042
2022-10-03 17:55:57 - train: epoch 0051, iter [01120, 01251], lr: 0.000343, loss: 0.4013
2022-10-03 17:56:15 - train: epoch 0051, iter [01130, 01251], lr: 0.000343, loss: 0.4096
2022-10-03 17:56:34 - train: epoch 0051, iter [01140, 01251], lr: 0.000343, loss: 0.4014
2022-10-03 17:56:53 - train: epoch 0051, iter [01150, 01251], lr: 0.000343, loss: 0.4097
2022-10-03 17:57:12 - train: epoch 0051, iter [01160, 01251], lr: 0.000343, loss: 0.4105
2022-10-03 17:57:31 - train: epoch 0051, iter [01170, 01251], lr: 0.000342, loss: 0.4149
2022-10-03 17:57:50 - train: epoch 0051, iter [01180, 01251], lr: 0.000342, loss: 0.3922
2022-10-03 17:58:09 - train: epoch 0051, iter [01190, 01251], lr: 0.000342, loss: 0.4361
2022-10-03 17:58:28 - train: epoch 0051, iter [01200, 01251], lr: 0.000342, loss: 0.4042
2022-10-03 17:58:46 - train: epoch 0051, iter [01210, 01251], lr: 0.000342, loss: 0.4269
2022-10-03 17:59:05 - train: epoch 0051, iter [01220, 01251], lr: 0.000342, loss: 0.4107
2022-10-03 17:59:23 - train: epoch 0051, iter [01230, 01251], lr: 0.000342, loss: 0.3974
2022-10-03 17:59:42 - train: epoch 0051, iter [01240, 01251], lr: 0.000342, loss: 0.3994
2022-10-03 18:00:00 - train: epoch 0051, iter [01250, 01251], lr: 0.000342, loss: 0.4045
2022-10-03 18:00:04 - train: epoch 051, train_loss: 0.4105
2022-10-03 18:00:09 - until epoch: 051, best_loss: 0.4105
2022-10-03 18:00:09 - epoch 052 lr: 0.000342
2022-10-03 18:00:33 - train: epoch 0052, iter [00010, 01251], lr: 0.000342, loss: 0.4290
2022-10-03 18:00:52 - train: epoch 0052, iter [00020, 01251], lr: 0.000342, loss: 0.4233
2022-10-03 18:01:10 - train: epoch 0052, iter [00030, 01251], lr: 0.000342, loss: 0.4092
2022-10-03 18:01:29 - train: epoch 0052, iter [00040, 01251], lr: 0.000341, loss: 0.4136
2022-10-03 18:01:47 - train: epoch 0052, iter [00050, 01251], lr: 0.000341, loss: 0.4176
2022-10-03 18:02:06 - train: epoch 0052, iter [00060, 01251], lr: 0.000341, loss: 0.4053
2022-10-03 18:02:24 - train: epoch 0052, iter [00070, 01251], lr: 0.000341, loss: 0.4242
2022-10-03 18:02:43 - train: epoch 0052, iter [00080, 01251], lr: 0.000341, loss: 0.4368
2022-10-03 18:03:01 - train: epoch 0052, iter [00090, 01251], lr: 0.000341, loss: 0.3993
2022-10-03 18:03:20 - train: epoch 0052, iter [00100, 01251], lr: 0.000341, loss: 0.3920
2022-10-03 18:03:38 - train: epoch 0052, iter [00110, 01251], lr: 0.000341, loss: 0.3943
2022-10-03 18:03:57 - train: epoch 0052, iter [00120, 01251], lr: 0.000341, loss: 0.4325
2022-10-03 18:04:16 - train: epoch 0052, iter [00130, 01251], lr: 0.000341, loss: 0.4230
2022-10-03 18:04:35 - train: epoch 0052, iter [00140, 01251], lr: 0.000341, loss: 0.3963
2022-10-03 18:04:53 - train: epoch 0052, iter [00150, 01251], lr: 0.000341, loss: 0.4167
2022-10-03 18:05:12 - train: epoch 0052, iter [00160, 01251], lr: 0.000340, loss: 0.3773
2022-10-03 18:05:31 - train: epoch 0052, iter [00170, 01251], lr: 0.000340, loss: 0.3954
2022-10-03 18:05:49 - train: epoch 0052, iter [00180, 01251], lr: 0.000340, loss: 0.4102
2022-10-03 18:06:08 - train: epoch 0052, iter [00190, 01251], lr: 0.000340, loss: 0.3930
2022-10-03 18:06:28 - train: epoch 0052, iter [00200, 01251], lr: 0.000340, loss: 0.3931
2022-10-03 18:06:47 - train: epoch 0052, iter [00210, 01251], lr: 0.000340, loss: 0.4147
2022-10-03 18:07:06 - train: epoch 0052, iter [00220, 01251], lr: 0.000340, loss: 0.3892
2022-10-03 18:07:25 - train: epoch 0052, iter [00230, 01251], lr: 0.000340, loss: 0.4092
2022-10-03 18:07:44 - train: epoch 0052, iter [00240, 01251], lr: 0.000340, loss: 0.4289
2022-10-03 18:08:03 - train: epoch 0052, iter [00250, 01251], lr: 0.000340, loss: 0.3985
2022-10-03 18:08:22 - train: epoch 0052, iter [00260, 01251], lr: 0.000340, loss: 0.4007
2022-10-03 18:08:42 - train: epoch 0052, iter [00270, 01251], lr: 0.000340, loss: 0.4167
2022-10-03 18:09:00 - train: epoch 0052, iter [00280, 01251], lr: 0.000339, loss: 0.3936
2022-10-03 18:09:20 - train: epoch 0052, iter [00290, 01251], lr: 0.000339, loss: 0.4161
2022-10-03 18:09:39 - train: epoch 0052, iter [00300, 01251], lr: 0.000339, loss: 0.4093
2022-10-03 18:09:58 - train: epoch 0052, iter [00310, 01251], lr: 0.000339, loss: 0.4191
2022-10-03 18:10:17 - train: epoch 0052, iter [00320, 01251], lr: 0.000339, loss: 0.4142
2022-10-03 18:10:36 - train: epoch 0052, iter [00330, 01251], lr: 0.000339, loss: 0.4356
2022-10-03 18:10:55 - train: epoch 0052, iter [00340, 01251], lr: 0.000339, loss: 0.4087
2022-10-03 18:11:14 - train: epoch 0052, iter [00350, 01251], lr: 0.000339, loss: 0.3912
2022-10-03 18:11:33 - train: epoch 0052, iter [00360, 01251], lr: 0.000339, loss: 0.3982
2022-10-03 18:11:52 - train: epoch 0052, iter [00370, 01251], lr: 0.000339, loss: 0.3946
2022-10-03 18:12:12 - train: epoch 0052, iter [00380, 01251], lr: 0.000339, loss: 0.4279
2022-10-03 18:12:31 - train: epoch 0052, iter [00390, 01251], lr: 0.000339, loss: 0.4007
2022-10-03 18:12:49 - train: epoch 0052, iter [00400, 01251], lr: 0.000338, loss: 0.4057
2022-10-03 18:13:08 - train: epoch 0052, iter [00410, 01251], lr: 0.000338, loss: 0.4138
2022-10-03 18:13:28 - train: epoch 0052, iter [00420, 01251], lr: 0.000338, loss: 0.4212
2022-10-03 18:13:47 - train: epoch 0052, iter [00430, 01251], lr: 0.000338, loss: 0.3978
2022-10-03 18:14:06 - train: epoch 0052, iter [00440, 01251], lr: 0.000338, loss: 0.3886
2022-10-03 18:14:25 - train: epoch 0052, iter [00450, 01251], lr: 0.000338, loss: 0.4267
2022-10-03 18:14:44 - train: epoch 0052, iter [00460, 01251], lr: 0.000338, loss: 0.4000
2022-10-03 18:15:03 - train: epoch 0052, iter [00470, 01251], lr: 0.000338, loss: 0.4017
2022-10-03 18:15:22 - train: epoch 0052, iter [00480, 01251], lr: 0.000338, loss: 0.3967
2022-10-03 18:15:40 - train: epoch 0052, iter [00490, 01251], lr: 0.000338, loss: 0.4216
2022-10-03 18:16:00 - train: epoch 0052, iter [00500, 01251], lr: 0.000338, loss: 0.4230
2022-10-03 18:16:19 - train: epoch 0052, iter [00510, 01251], lr: 0.000338, loss: 0.3957
2022-10-03 18:16:38 - train: epoch 0052, iter [00520, 01251], lr: 0.000337, loss: 0.4312
2022-10-03 18:16:57 - train: epoch 0052, iter [00530, 01251], lr: 0.000337, loss: 0.4097
2022-10-03 18:17:16 - train: epoch 0052, iter [00540, 01251], lr: 0.000337, loss: 0.4121
2022-10-03 18:17:34 - train: epoch 0052, iter [00550, 01251], lr: 0.000337, loss: 0.4169
2022-10-03 18:17:53 - train: epoch 0052, iter [00560, 01251], lr: 0.000337, loss: 0.4340
2022-10-03 18:18:12 - train: epoch 0052, iter [00570, 01251], lr: 0.000337, loss: 0.4112
2022-10-03 18:18:31 - train: epoch 0052, iter [00580, 01251], lr: 0.000337, loss: 0.4078
2022-10-03 18:18:51 - train: epoch 0052, iter [00590, 01251], lr: 0.000337, loss: 0.4029
2022-10-03 18:19:09 - train: epoch 0052, iter [00600, 01251], lr: 0.000337, loss: 0.4079
2022-10-03 18:19:28 - train: epoch 0052, iter [00610, 01251], lr: 0.000337, loss: 0.4070
2022-10-03 18:19:47 - train: epoch 0052, iter [00620, 01251], lr: 0.000337, loss: 0.4078
2022-10-03 18:20:06 - train: epoch 0052, iter [00630, 01251], lr: 0.000337, loss: 0.3894
2022-10-03 18:20:25 - train: epoch 0052, iter [00640, 01251], lr: 0.000336, loss: 0.3892
2022-10-03 18:20:44 - train: epoch 0052, iter [00650, 01251], lr: 0.000336, loss: 0.3840
2022-10-03 18:21:03 - train: epoch 0052, iter [00660, 01251], lr: 0.000336, loss: 0.4009
2022-10-03 18:21:22 - train: epoch 0052, iter [00670, 01251], lr: 0.000336, loss: 0.4205
2022-10-03 18:21:40 - train: epoch 0052, iter [00680, 01251], lr: 0.000336, loss: 0.4340
2022-10-03 18:21:59 - train: epoch 0052, iter [00690, 01251], lr: 0.000336, loss: 0.4050
2022-10-03 18:22:18 - train: epoch 0052, iter [00700, 01251], lr: 0.000336, loss: 0.4205
2022-10-03 18:22:37 - train: epoch 0052, iter [00710, 01251], lr: 0.000336, loss: 0.4109
2022-10-03 18:22:56 - train: epoch 0052, iter [00720, 01251], lr: 0.000336, loss: 0.3904
2022-10-03 18:23:15 - train: epoch 0052, iter [00730, 01251], lr: 0.000336, loss: 0.3993
2022-10-03 18:23:33 - train: epoch 0052, iter [00740, 01251], lr: 0.000336, loss: 0.3997
2022-10-03 18:23:52 - train: epoch 0052, iter [00750, 01251], lr: 0.000336, loss: 0.4148
2022-10-03 18:24:11 - train: epoch 0052, iter [00760, 01251], lr: 0.000335, loss: 0.4246
2022-10-03 18:24:31 - train: epoch 0052, iter [00770, 01251], lr: 0.000335, loss: 0.4229
2022-10-03 18:24:50 - train: epoch 0052, iter [00780, 01251], lr: 0.000335, loss: 0.4117
2022-10-03 18:25:09 - train: epoch 0052, iter [00790, 01251], lr: 0.000335, loss: 0.4162
2022-10-03 18:25:28 - train: epoch 0052, iter [00800, 01251], lr: 0.000335, loss: 0.4178
2022-10-03 18:25:48 - train: epoch 0052, iter [00810, 01251], lr: 0.000335, loss: 0.4041
2022-10-03 18:26:08 - train: epoch 0052, iter [00820, 01251], lr: 0.000335, loss: 0.3998
2022-10-03 18:26:27 - train: epoch 0052, iter [00830, 01251], lr: 0.000335, loss: 0.4078
2022-10-03 18:26:45 - train: epoch 0052, iter [00840, 01251], lr: 0.000335, loss: 0.4103
2022-10-03 18:27:04 - train: epoch 0052, iter [00850, 01251], lr: 0.000335, loss: 0.4176
2022-10-03 18:27:23 - train: epoch 0052, iter [00860, 01251], lr: 0.000335, loss: 0.4080
2022-10-03 18:27:42 - train: epoch 0052, iter [00870, 01251], lr: 0.000335, loss: 0.4068
2022-10-03 18:28:01 - train: epoch 0052, iter [00880, 01251], lr: 0.000334, loss: 0.4165
2022-10-03 18:28:20 - train: epoch 0052, iter [00890, 01251], lr: 0.000334, loss: 0.4099
2022-10-03 18:28:38 - train: epoch 0052, iter [00900, 01251], lr: 0.000334, loss: 0.4010
2022-10-03 18:28:58 - train: epoch 0052, iter [00910, 01251], lr: 0.000334, loss: 0.4020
2022-10-03 18:29:17 - train: epoch 0052, iter [00920, 01251], lr: 0.000334, loss: 0.3972
2022-10-03 18:29:35 - train: epoch 0052, iter [00930, 01251], lr: 0.000334, loss: 0.4147
2022-10-03 18:29:54 - train: epoch 0052, iter [00940, 01251], lr: 0.000334, loss: 0.4210
2022-10-03 18:30:13 - train: epoch 0052, iter [00950, 01251], lr: 0.000334, loss: 0.4155
2022-10-03 18:30:32 - train: epoch 0052, iter [00960, 01251], lr: 0.000334, loss: 0.4126
2022-10-03 18:30:51 - train: epoch 0052, iter [00970, 01251], lr: 0.000334, loss: 0.4046
2022-10-03 18:31:10 - train: epoch 0052, iter [00980, 01251], lr: 0.000334, loss: 0.3983
2022-10-03 18:31:28 - train: epoch 0052, iter [00990, 01251], lr: 0.000334, loss: 0.4119
2022-10-03 18:31:48 - train: epoch 0052, iter [01000, 01251], lr: 0.000333, loss: 0.4032
2022-10-03 18:32:07 - train: epoch 0052, iter [01010, 01251], lr: 0.000333, loss: 0.3989
2022-10-03 18:32:25 - train: epoch 0052, iter [01020, 01251], lr: 0.000333, loss: 0.4340
2022-10-03 18:32:44 - train: epoch 0052, iter [01030, 01251], lr: 0.000333, loss: 0.4033
2022-10-03 18:33:03 - train: epoch 0052, iter [01040, 01251], lr: 0.000333, loss: 0.4245
2022-10-03 18:33:22 - train: epoch 0052, iter [01050, 01251], lr: 0.000333, loss: 0.3793
2022-10-03 18:33:41 - train: epoch 0052, iter [01060, 01251], lr: 0.000333, loss: 0.4246
2022-10-03 18:34:00 - train: epoch 0052, iter [01070, 01251], lr: 0.000333, loss: 0.4169
2022-10-03 18:34:19 - train: epoch 0052, iter [01080, 01251], lr: 0.000333, loss: 0.4175
2022-10-03 18:34:39 - train: epoch 0052, iter [01090, 01251], lr: 0.000333, loss: 0.4149
2022-10-03 18:34:57 - train: epoch 0052, iter [01100, 01251], lr: 0.000333, loss: 0.3894
2022-10-03 18:35:16 - train: epoch 0052, iter [01110, 01251], lr: 0.000333, loss: 0.4124
2022-10-03 18:35:35 - train: epoch 0052, iter [01120, 01251], lr: 0.000332, loss: 0.4120
2022-10-03 18:35:54 - train: epoch 0052, iter [01130, 01251], lr: 0.000332, loss: 0.3986
2022-10-03 18:36:13 - train: epoch 0052, iter [01140, 01251], lr: 0.000332, loss: 0.4281
2022-10-03 18:36:32 - train: epoch 0052, iter [01150, 01251], lr: 0.000332, loss: 0.4070
2022-10-03 18:36:51 - train: epoch 0052, iter [01160, 01251], lr: 0.000332, loss: 0.3973
2022-10-03 18:37:09 - train: epoch 0052, iter [01170, 01251], lr: 0.000332, loss: 0.4173
2022-10-03 18:37:28 - train: epoch 0052, iter [01180, 01251], lr: 0.000332, loss: 0.4255
2022-10-03 18:37:47 - train: epoch 0052, iter [01190, 01251], lr: 0.000332, loss: 0.4089
2022-10-03 18:38:07 - train: epoch 0052, iter [01200, 01251], lr: 0.000332, loss: 0.4028
2022-10-03 18:38:26 - train: epoch 0052, iter [01210, 01251], lr: 0.000332, loss: 0.3960
2022-10-03 18:38:45 - train: epoch 0052, iter [01220, 01251], lr: 0.000332, loss: 0.4070
2022-10-03 18:39:04 - train: epoch 0052, iter [01230, 01251], lr: 0.000332, loss: 0.4028
2022-10-03 18:39:22 - train: epoch 0052, iter [01240, 01251], lr: 0.000331, loss: 0.4166
2022-10-03 18:39:41 - train: epoch 0052, iter [01250, 01251], lr: 0.000331, loss: 0.4098
2022-10-03 18:39:46 - train: epoch 052, train_loss: 0.4101
2022-10-03 18:39:50 - until epoch: 052, best_loss: 0.4101
2022-10-03 18:39:50 - epoch 053 lr: 0.000331
2022-10-03 18:40:15 - train: epoch 0053, iter [00010, 01251], lr: 0.000331, loss: 0.4101
2022-10-03 18:40:34 - train: epoch 0053, iter [00020, 01251], lr: 0.000331, loss: 0.4183
2022-10-03 18:40:52 - train: epoch 0053, iter [00030, 01251], lr: 0.000331, loss: 0.3930
2022-10-03 18:41:11 - train: epoch 0053, iter [00040, 01251], lr: 0.000331, loss: 0.4244
2022-10-03 18:41:30 - train: epoch 0053, iter [00050, 01251], lr: 0.000331, loss: 0.4007
2022-10-03 18:41:49 - train: epoch 0053, iter [00060, 01251], lr: 0.000331, loss: 0.4184
2022-10-03 18:42:08 - train: epoch 0053, iter [00070, 01251], lr: 0.000331, loss: 0.4035
2022-10-03 18:42:26 - train: epoch 0053, iter [00080, 01251], lr: 0.000331, loss: 0.4081
2022-10-03 18:42:45 - train: epoch 0053, iter [00090, 01251], lr: 0.000331, loss: 0.3981
2022-10-03 18:43:04 - train: epoch 0053, iter [00100, 01251], lr: 0.000331, loss: 0.4101
2022-10-03 18:43:22 - train: epoch 0053, iter [00110, 01251], lr: 0.000330, loss: 0.4052
2022-10-03 18:43:41 - train: epoch 0053, iter [00120, 01251], lr: 0.000330, loss: 0.4202
2022-10-03 18:44:00 - train: epoch 0053, iter [00130, 01251], lr: 0.000330, loss: 0.4123
2022-10-03 18:44:18 - train: epoch 0053, iter [00140, 01251], lr: 0.000330, loss: 0.3985
2022-10-03 18:44:37 - train: epoch 0053, iter [00150, 01251], lr: 0.000330, loss: 0.4187
2022-10-03 18:44:56 - train: epoch 0053, iter [00160, 01251], lr: 0.000330, loss: 0.4229
2022-10-03 18:45:15 - train: epoch 0053, iter [00170, 01251], lr: 0.000330, loss: 0.4031
2022-10-03 18:45:34 - train: epoch 0053, iter [00180, 01251], lr: 0.000330, loss: 0.3819
2022-10-03 18:45:53 - train: epoch 0053, iter [00190, 01251], lr: 0.000330, loss: 0.4153
2022-10-03 18:46:11 - train: epoch 0053, iter [00200, 01251], lr: 0.000330, loss: 0.4175
2022-10-03 18:46:30 - train: epoch 0053, iter [00210, 01251], lr: 0.000330, loss: 0.4294
2022-10-03 18:46:49 - train: epoch 0053, iter [00220, 01251], lr: 0.000330, loss: 0.4181
2022-10-03 18:47:08 - train: epoch 0053, iter [00230, 01251], lr: 0.000329, loss: 0.4203
2022-10-03 18:47:27 - train: epoch 0053, iter [00240, 01251], lr: 0.000329, loss: 0.4175
2022-10-03 18:47:45 - train: epoch 0053, iter [00250, 01251], lr: 0.000329, loss: 0.4155
2022-10-03 18:48:03 - train: epoch 0053, iter [00260, 01251], lr: 0.000329, loss: 0.4200
2022-10-03 18:48:22 - train: epoch 0053, iter [00270, 01251], lr: 0.000329, loss: 0.4262
2022-10-03 18:48:41 - train: epoch 0053, iter [00280, 01251], lr: 0.000329, loss: 0.4191
2022-10-03 18:49:00 - train: epoch 0053, iter [00290, 01251], lr: 0.000329, loss: 0.4021
2022-10-03 18:49:19 - train: epoch 0053, iter [00300, 01251], lr: 0.000329, loss: 0.4218
2022-10-03 18:49:37 - train: epoch 0053, iter [00310, 01251], lr: 0.000329, loss: 0.4184
2022-10-03 18:49:56 - train: epoch 0053, iter [00320, 01251], lr: 0.000329, loss: 0.4094
2022-10-03 18:50:15 - train: epoch 0053, iter [00330, 01251], lr: 0.000329, loss: 0.4221
2022-10-03 18:50:34 - train: epoch 0053, iter [00340, 01251], lr: 0.000329, loss: 0.4206
2022-10-03 18:50:53 - train: epoch 0053, iter [00350, 01251], lr: 0.000328, loss: 0.3882
2022-10-03 18:51:11 - train: epoch 0053, iter [00360, 01251], lr: 0.000328, loss: 0.4225
2022-10-03 18:51:30 - train: epoch 0053, iter [00370, 01251], lr: 0.000328, loss: 0.4181
2022-10-03 18:51:49 - train: epoch 0053, iter [00380, 01251], lr: 0.000328, loss: 0.4182
2022-10-03 18:52:08 - train: epoch 0053, iter [00390, 01251], lr: 0.000328, loss: 0.4030
2022-10-03 18:52:26 - train: epoch 0053, iter [00400, 01251], lr: 0.000328, loss: 0.4107
2022-10-03 18:52:46 - train: epoch 0053, iter [00410, 01251], lr: 0.000328, loss: 0.4004
2022-10-03 18:53:05 - train: epoch 0053, iter [00420, 01251], lr: 0.000328, loss: 0.4156
2022-10-03 18:53:23 - train: epoch 0053, iter [00430, 01251], lr: 0.000328, loss: 0.4182
2022-10-03 18:53:42 - train: epoch 0053, iter [00440, 01251], lr: 0.000328, loss: 0.4154
2022-10-03 18:54:01 - train: epoch 0053, iter [00450, 01251], lr: 0.000328, loss: 0.4105
2022-10-03 18:54:20 - train: epoch 0053, iter [00460, 01251], lr: 0.000328, loss: 0.4233
2022-10-03 18:54:39 - train: epoch 0053, iter [00470, 01251], lr: 0.000327, loss: 0.4006
2022-10-03 18:54:58 - train: epoch 0053, iter [00480, 01251], lr: 0.000327, loss: 0.4206
2022-10-03 18:55:17 - train: epoch 0053, iter [00490, 01251], lr: 0.000327, loss: 0.4065
2022-10-03 18:55:35 - train: epoch 0053, iter [00500, 01251], lr: 0.000327, loss: 0.4209
2022-10-03 18:55:55 - train: epoch 0053, iter [00510, 01251], lr: 0.000327, loss: 0.4342
2022-10-03 18:56:13 - train: epoch 0053, iter [00520, 01251], lr: 0.000327, loss: 0.4096
2022-10-03 18:56:32 - train: epoch 0053, iter [00530, 01251], lr: 0.000327, loss: 0.4256
2022-10-03 18:56:51 - train: epoch 0053, iter [00540, 01251], lr: 0.000327, loss: 0.3887
2022-10-03 18:57:10 - train: epoch 0053, iter [00550, 01251], lr: 0.000327, loss: 0.4360
2022-10-03 18:57:30 - train: epoch 0053, iter [00560, 01251], lr: 0.000327, loss: 0.4165
2022-10-03 18:57:48 - train: epoch 0053, iter [00570, 01251], lr: 0.000327, loss: 0.4185
2022-10-03 18:58:07 - train: epoch 0053, iter [00580, 01251], lr: 0.000327, loss: 0.4004
2022-10-03 18:58:25 - train: epoch 0053, iter [00590, 01251], lr: 0.000326, loss: 0.4038
2022-10-03 18:58:44 - train: epoch 0053, iter [00600, 01251], lr: 0.000326, loss: 0.3903
2022-10-03 18:59:03 - train: epoch 0053, iter [00610, 01251], lr: 0.000326, loss: 0.4087
2022-10-03 18:59:22 - train: epoch 0053, iter [00620, 01251], lr: 0.000326, loss: 0.4067
2022-10-03 18:59:41 - train: epoch 0053, iter [00630, 01251], lr: 0.000326, loss: 0.4104
2022-10-03 18:59:59 - train: epoch 0053, iter [00640, 01251], lr: 0.000326, loss: 0.4088
2022-10-03 19:00:18 - train: epoch 0053, iter [00650, 01251], lr: 0.000326, loss: 0.4051
2022-10-03 19:00:37 - train: epoch 0053, iter [00660, 01251], lr: 0.000326, loss: 0.4176
2022-10-03 19:00:56 - train: epoch 0053, iter [00670, 01251], lr: 0.000326, loss: 0.4241
2022-10-03 19:01:14 - train: epoch 0053, iter [00680, 01251], lr: 0.000326, loss: 0.4007
2022-10-03 19:01:33 - train: epoch 0053, iter [00690, 01251], lr: 0.000326, loss: 0.4066
2022-10-03 19:01:52 - train: epoch 0053, iter [00700, 01251], lr: 0.000326, loss: 0.4229
2022-10-03 19:02:10 - train: epoch 0053, iter [00710, 01251], lr: 0.000325, loss: 0.4205
2022-10-03 19:02:29 - train: epoch 0053, iter [00720, 01251], lr: 0.000325, loss: 0.4177
2022-10-03 19:02:48 - train: epoch 0053, iter [00730, 01251], lr: 0.000325, loss: 0.4061
2022-10-03 19:03:06 - train: epoch 0053, iter [00740, 01251], lr: 0.000325, loss: 0.3867
2022-10-03 19:03:25 - train: epoch 0053, iter [00750, 01251], lr: 0.000325, loss: 0.4126
2022-10-03 19:03:44 - train: epoch 0053, iter [00760, 01251], lr: 0.000325, loss: 0.3981
2022-10-03 19:04:03 - train: epoch 0053, iter [00770, 01251], lr: 0.000325, loss: 0.3971
2022-10-03 19:04:22 - train: epoch 0053, iter [00780, 01251], lr: 0.000325, loss: 0.4070
2022-10-03 19:04:41 - train: epoch 0053, iter [00790, 01251], lr: 0.000325, loss: 0.3925
2022-10-03 19:05:00 - train: epoch 0053, iter [00800, 01251], lr: 0.000325, loss: 0.3974
2022-10-03 19:05:19 - train: epoch 0053, iter [00810, 01251], lr: 0.000325, loss: 0.4065
2022-10-03 19:05:38 - train: epoch 0053, iter [00820, 01251], lr: 0.000325, loss: 0.4242
2022-10-03 19:05:57 - train: epoch 0053, iter [00830, 01251], lr: 0.000324, loss: 0.4210
2022-10-03 19:06:16 - train: epoch 0053, iter [00840, 01251], lr: 0.000324, loss: 0.4055
2022-10-03 19:06:34 - train: epoch 0053, iter [00850, 01251], lr: 0.000324, loss: 0.4357
2022-10-03 19:06:53 - train: epoch 0053, iter [00860, 01251], lr: 0.000324, loss: 0.4118
2022-10-03 19:07:12 - train: epoch 0053, iter [00870, 01251], lr: 0.000324, loss: 0.4175
2022-10-03 19:07:31 - train: epoch 0053, iter [00880, 01251], lr: 0.000324, loss: 0.3960
2022-10-03 19:07:51 - train: epoch 0053, iter [00890, 01251], lr: 0.000324, loss: 0.4011
2022-10-03 19:08:09 - train: epoch 0053, iter [00900, 01251], lr: 0.000324, loss: 0.4197
2022-10-03 19:08:28 - train: epoch 0053, iter [00910, 01251], lr: 0.000324, loss: 0.3846
2022-10-03 19:08:46 - train: epoch 0053, iter [00920, 01251], lr: 0.000324, loss: 0.4078
2022-10-03 19:09:05 - train: epoch 0053, iter [00930, 01251], lr: 0.000324, loss: 0.4119
2022-10-03 19:09:24 - train: epoch 0053, iter [00940, 01251], lr: 0.000324, loss: 0.3972
2022-10-03 19:09:43 - train: epoch 0053, iter [00950, 01251], lr: 0.000323, loss: 0.3999
2022-10-03 19:10:02 - train: epoch 0053, iter [00960, 01251], lr: 0.000323, loss: 0.4088
2022-10-03 19:10:20 - train: epoch 0053, iter [00970, 01251], lr: 0.000323, loss: 0.3936
2022-10-03 19:10:39 - train: epoch 0053, iter [00980, 01251], lr: 0.000323, loss: 0.4014
2022-10-03 19:10:58 - train: epoch 0053, iter [00990, 01251], lr: 0.000323, loss: 0.3910
2022-10-03 19:11:17 - train: epoch 0053, iter [01000, 01251], lr: 0.000323, loss: 0.4080
2022-10-03 19:11:35 - train: epoch 0053, iter [01010, 01251], lr: 0.000323, loss: 0.3913
2022-10-03 19:11:54 - train: epoch 0053, iter [01020, 01251], lr: 0.000323, loss: 0.4205
2022-10-03 19:12:13 - train: epoch 0053, iter [01030, 01251], lr: 0.000323, loss: 0.3804
2022-10-03 19:12:32 - train: epoch 0053, iter [01040, 01251], lr: 0.000323, loss: 0.4032
2022-10-03 19:12:51 - train: epoch 0053, iter [01050, 01251], lr: 0.000323, loss: 0.4193
2022-10-03 19:13:10 - train: epoch 0053, iter [01060, 01251], lr: 0.000323, loss: 0.4192
2022-10-03 19:13:29 - train: epoch 0053, iter [01070, 01251], lr: 0.000322, loss: 0.4099
2022-10-03 19:13:48 - train: epoch 0053, iter [01080, 01251], lr: 0.000322, loss: 0.3973
2022-10-03 19:14:07 - train: epoch 0053, iter [01090, 01251], lr: 0.000322, loss: 0.4192
2022-10-03 19:14:25 - train: epoch 0053, iter [01100, 01251], lr: 0.000322, loss: 0.4213
2022-10-03 19:14:45 - train: epoch 0053, iter [01110, 01251], lr: 0.000322, loss: 0.4205
2022-10-03 19:15:03 - train: epoch 0053, iter [01120, 01251], lr: 0.000322, loss: 0.4042
2022-10-03 19:15:22 - train: epoch 0053, iter [01130, 01251], lr: 0.000322, loss: 0.4040
2022-10-03 19:15:41 - train: epoch 0053, iter [01140, 01251], lr: 0.000322, loss: 0.4081
2022-10-03 19:16:00 - train: epoch 0053, iter [01150, 01251], lr: 0.000322, loss: 0.3979
2022-10-03 19:16:19 - train: epoch 0053, iter [01160, 01251], lr: 0.000322, loss: 0.4180
2022-10-03 19:16:38 - train: epoch 0053, iter [01170, 01251], lr: 0.000322, loss: 0.4146
2022-10-03 19:16:58 - train: epoch 0053, iter [01180, 01251], lr: 0.000322, loss: 0.4061
2022-10-03 19:17:17 - train: epoch 0053, iter [01190, 01251], lr: 0.000321, loss: 0.3903
2022-10-03 19:17:36 - train: epoch 0053, iter [01200, 01251], lr: 0.000321, loss: 0.3945
2022-10-03 19:17:55 - train: epoch 0053, iter [01210, 01251], lr: 0.000321, loss: 0.4220
2022-10-03 19:18:14 - train: epoch 0053, iter [01220, 01251], lr: 0.000321, loss: 0.4131
2022-10-03 19:18:33 - train: epoch 0053, iter [01230, 01251], lr: 0.000321, loss: 0.4003
2022-10-03 19:18:52 - train: epoch 0053, iter [01240, 01251], lr: 0.000321, loss: 0.3935
2022-10-03 19:19:10 - train: epoch 0053, iter [01250, 01251], lr: 0.000321, loss: 0.4071
2022-10-03 19:19:15 - train: epoch 053, train_loss: 0.4098
2022-10-03 19:19:20 - until epoch: 053, best_loss: 0.4098
2022-10-03 19:19:20 - epoch 054 lr: 0.000321
2022-10-03 19:19:44 - train: epoch 0054, iter [00010, 01251], lr: 0.000321, loss: 0.3993
2022-10-03 19:20:03 - train: epoch 0054, iter [00020, 01251], lr: 0.000321, loss: 0.4187
2022-10-03 19:20:23 - train: epoch 0054, iter [00030, 01251], lr: 0.000321, loss: 0.4222
2022-10-03 19:20:41 - train: epoch 0054, iter [00040, 01251], lr: 0.000321, loss: 0.3963
2022-10-03 19:21:00 - train: epoch 0054, iter [00050, 01251], lr: 0.000321, loss: 0.4132
2022-10-03 19:21:19 - train: epoch 0054, iter [00060, 01251], lr: 0.000320, loss: 0.3934
2022-10-03 19:21:38 - train: epoch 0054, iter [00070, 01251], lr: 0.000320, loss: 0.3930
2022-10-03 19:21:57 - train: epoch 0054, iter [00080, 01251], lr: 0.000320, loss: 0.4206
2022-10-03 19:22:16 - train: epoch 0054, iter [00090, 01251], lr: 0.000320, loss: 0.4127
2022-10-03 19:22:35 - train: epoch 0054, iter [00100, 01251], lr: 0.000320, loss: 0.4058
2022-10-03 19:22:54 - train: epoch 0054, iter [00110, 01251], lr: 0.000320, loss: 0.3998
2022-10-03 19:23:13 - train: epoch 0054, iter [00120, 01251], lr: 0.000320, loss: 0.4066
2022-10-03 19:23:32 - train: epoch 0054, iter [00130, 01251], lr: 0.000320, loss: 0.3992
2022-10-03 19:23:51 - train: epoch 0054, iter [00140, 01251], lr: 0.000320, loss: 0.4039
2022-10-03 19:24:10 - train: epoch 0054, iter [00150, 01251], lr: 0.000320, loss: 0.3995
2022-10-03 19:24:29 - train: epoch 0054, iter [00160, 01251], lr: 0.000320, loss: 0.4029
2022-10-03 19:24:48 - train: epoch 0054, iter [00170, 01251], lr: 0.000320, loss: 0.4047
2022-10-03 19:25:07 - train: epoch 0054, iter [00180, 01251], lr: 0.000319, loss: 0.4071
2022-10-03 19:25:26 - train: epoch 0054, iter [00190, 01251], lr: 0.000319, loss: 0.4094
2022-10-03 19:25:44 - train: epoch 0054, iter [00200, 01251], lr: 0.000319, loss: 0.4035
2022-10-03 19:26:04 - train: epoch 0054, iter [00210, 01251], lr: 0.000319, loss: 0.3914
2022-10-03 19:26:23 - train: epoch 0054, iter [00220, 01251], lr: 0.000319, loss: 0.4028
2022-10-03 19:26:42 - train: epoch 0054, iter [00230, 01251], lr: 0.000319, loss: 0.3841
2022-10-03 19:27:01 - train: epoch 0054, iter [00240, 01251], lr: 0.000319, loss: 0.4138
2022-10-03 19:27:20 - train: epoch 0054, iter [00250, 01251], lr: 0.000319, loss: 0.3834
2022-10-03 19:27:38 - train: epoch 0054, iter [00260, 01251], lr: 0.000319, loss: 0.4048
2022-10-03 19:27:57 - train: epoch 0054, iter [00270, 01251], lr: 0.000319, loss: 0.3948
2022-10-03 19:28:17 - train: epoch 0054, iter [00280, 01251], lr: 0.000319, loss: 0.4265
2022-10-03 19:28:35 - train: epoch 0054, iter [00290, 01251], lr: 0.000319, loss: 0.4125
2022-10-03 19:28:54 - train: epoch 0054, iter [00300, 01251], lr: 0.000318, loss: 0.3883
2022-10-03 19:29:14 - train: epoch 0054, iter [00310, 01251], lr: 0.000318, loss: 0.4325
2022-10-03 19:29:33 - train: epoch 0054, iter [00320, 01251], lr: 0.000318, loss: 0.4257
2022-10-03 19:29:52 - train: epoch 0054, iter [00330, 01251], lr: 0.000318, loss: 0.4010
2022-10-03 19:30:11 - train: epoch 0054, iter [00340, 01251], lr: 0.000318, loss: 0.3894
2022-10-03 19:30:30 - train: epoch 0054, iter [00350, 01251], lr: 0.000318, loss: 0.3915
2022-10-03 19:30:49 - train: epoch 0054, iter [00360, 01251], lr: 0.000318, loss: 0.3919
2022-10-03 19:31:08 - train: epoch 0054, iter [00370, 01251], lr: 0.000318, loss: 0.4195
2022-10-03 19:31:27 - train: epoch 0054, iter [00380, 01251], lr: 0.000318, loss: 0.4067
2022-10-03 19:31:46 - train: epoch 0054, iter [00390, 01251], lr: 0.000318, loss: 0.3996
2022-10-03 19:32:05 - train: epoch 0054, iter [00400, 01251], lr: 0.000318, loss: 0.4117
2022-10-03 19:32:24 - train: epoch 0054, iter [00410, 01251], lr: 0.000318, loss: 0.4076
2022-10-03 19:32:43 - train: epoch 0054, iter [00420, 01251], lr: 0.000317, loss: 0.3969
2022-10-03 19:33:01 - train: epoch 0054, iter [00430, 01251], lr: 0.000317, loss: 0.4074
2022-10-03 19:33:20 - train: epoch 0054, iter [00440, 01251], lr: 0.000317, loss: 0.4259
2022-10-03 19:33:39 - train: epoch 0054, iter [00450, 01251], lr: 0.000317, loss: 0.3918
2022-10-03 19:33:58 - train: epoch 0054, iter [00460, 01251], lr: 0.000317, loss: 0.3951
2022-10-03 19:34:17 - train: epoch 0054, iter [00470, 01251], lr: 0.000317, loss: 0.4174
2022-10-03 19:34:37 - train: epoch 0054, iter [00480, 01251], lr: 0.000317, loss: 0.4026
2022-10-03 19:34:55 - train: epoch 0054, iter [00490, 01251], lr: 0.000317, loss: 0.4129
2022-10-03 19:35:14 - train: epoch 0054, iter [00500, 01251], lr: 0.000317, loss: 0.4020
2022-10-03 19:35:33 - train: epoch 0054, iter [00510, 01251], lr: 0.000317, loss: 0.4144
2022-10-03 19:35:52 - train: epoch 0054, iter [00520, 01251], lr: 0.000317, loss: 0.4035
2022-10-03 19:36:11 - train: epoch 0054, iter [00530, 01251], lr: 0.000316, loss: 0.4012
2022-10-03 19:36:30 - train: epoch 0054, iter [00540, 01251], lr: 0.000316, loss: 0.4257
2022-10-03 19:36:49 - train: epoch 0054, iter [00550, 01251], lr: 0.000316, loss: 0.3959
2022-10-03 19:37:08 - train: epoch 0054, iter [00560, 01251], lr: 0.000316, loss: 0.4311
2022-10-03 19:37:27 - train: epoch 0054, iter [00570, 01251], lr: 0.000316, loss: 0.4217
2022-10-03 19:37:46 - train: epoch 0054, iter [00580, 01251], lr: 0.000316, loss: 0.4219
2022-10-03 19:38:04 - train: epoch 0054, iter [00590, 01251], lr: 0.000316, loss: 0.3854
2022-10-03 19:38:23 - train: epoch 0054, iter [00600, 01251], lr: 0.000316, loss: 0.4126
2022-10-03 19:38:42 - train: epoch 0054, iter [00610, 01251], lr: 0.000316, loss: 0.4362
2022-10-03 19:39:01 - train: epoch 0054, iter [00620, 01251], lr: 0.000316, loss: 0.4078
2022-10-03 19:39:20 - train: epoch 0054, iter [00630, 01251], lr: 0.000316, loss: 0.4206
2022-10-03 19:39:40 - train: epoch 0054, iter [00640, 01251], lr: 0.000316, loss: 0.4218
2022-10-03 19:39:59 - train: epoch 0054, iter [00650, 01251], lr: 0.000315, loss: 0.4137
2022-10-03 19:40:17 - train: epoch 0054, iter [00660, 01251], lr: 0.000315, loss: 0.3998
2022-10-03 19:40:36 - train: epoch 0054, iter [00670, 01251], lr: 0.000315, loss: 0.4017
2022-10-03 19:40:54 - train: epoch 0054, iter [00680, 01251], lr: 0.000315, loss: 0.4211
2022-10-03 19:41:13 - train: epoch 0054, iter [00690, 01251], lr: 0.000315, loss: 0.3954
2022-10-03 19:41:32 - train: epoch 0054, iter [00700, 01251], lr: 0.000315, loss: 0.4285
2022-10-03 19:41:51 - train: epoch 0054, iter [00710, 01251], lr: 0.000315, loss: 0.4198
2022-10-03 19:42:10 - train: epoch 0054, iter [00720, 01251], lr: 0.000315, loss: 0.4052
2022-10-03 19:42:28 - train: epoch 0054, iter [00730, 01251], lr: 0.000315, loss: 0.4080
2022-10-03 19:42:47 - train: epoch 0054, iter [00740, 01251], lr: 0.000315, loss: 0.4059
2022-10-03 19:43:06 - train: epoch 0054, iter [00750, 01251], lr: 0.000315, loss: 0.4172
2022-10-03 19:43:24 - train: epoch 0054, iter [00760, 01251], lr: 0.000315, loss: 0.4149
2022-10-03 19:43:43 - train: epoch 0054, iter [00770, 01251], lr: 0.000314, loss: 0.3946
2022-10-03 19:44:02 - train: epoch 0054, iter [00780, 01251], lr: 0.000314, loss: 0.4025
2022-10-03 19:44:20 - train: epoch 0054, iter [00790, 01251], lr: 0.000314, loss: 0.3833
2022-10-03 19:44:39 - train: epoch 0054, iter [00800, 01251], lr: 0.000314, loss: 0.4125
2022-10-03 19:44:59 - train: epoch 0054, iter [00810, 01251], lr: 0.000314, loss: 0.4112
2022-10-03 19:45:17 - train: epoch 0054, iter [00820, 01251], lr: 0.000314, loss: 0.3897
2022-10-03 19:45:36 - train: epoch 0054, iter [00830, 01251], lr: 0.000314, loss: 0.4122
2022-10-03 19:45:54 - train: epoch 0054, iter [00840, 01251], lr: 0.000314, loss: 0.4240
2022-10-03 19:46:13 - train: epoch 0054, iter [00850, 01251], lr: 0.000314, loss: 0.4056
2022-10-03 19:46:32 - train: epoch 0054, iter [00860, 01251], lr: 0.000314, loss: 0.4142
2022-10-03 19:46:50 - train: epoch 0054, iter [00870, 01251], lr: 0.000314, loss: 0.4264
2022-10-03 19:47:09 - train: epoch 0054, iter [00880, 01251], lr: 0.000314, loss: 0.4178
2022-10-03 19:47:28 - train: epoch 0054, iter [00890, 01251], lr: 0.000313, loss: 0.4420
2022-10-03 19:47:46 - train: epoch 0054, iter [00900, 01251], lr: 0.000313, loss: 0.4010
2022-10-03 19:48:05 - train: epoch 0054, iter [00910, 01251], lr: 0.000313, loss: 0.4119
2022-10-03 19:48:23 - train: epoch 0054, iter [00920, 01251], lr: 0.000313, loss: 0.3994
2022-10-03 19:48:42 - train: epoch 0054, iter [00930, 01251], lr: 0.000313, loss: 0.4185
2022-10-03 19:49:01 - train: epoch 0054, iter [00940, 01251], lr: 0.000313, loss: 0.3959
2022-10-03 19:49:20 - train: epoch 0054, iter [00950, 01251], lr: 0.000313, loss: 0.3963
2022-10-03 19:49:39 - train: epoch 0054, iter [00960, 01251], lr: 0.000313, loss: 0.4217
2022-10-03 19:49:57 - train: epoch 0054, iter [00970, 01251], lr: 0.000313, loss: 0.4156
2022-10-03 19:50:16 - train: epoch 0054, iter [00980, 01251], lr: 0.000313, loss: 0.4122
2022-10-03 19:50:35 - train: epoch 0054, iter [00990, 01251], lr: 0.000313, loss: 0.3834
2022-10-03 19:50:54 - train: epoch 0054, iter [01000, 01251], lr: 0.000313, loss: 0.4196
2022-10-03 19:51:13 - train: epoch 0054, iter [01010, 01251], lr: 0.000312, loss: 0.4048
2022-10-03 19:51:31 - train: epoch 0054, iter [01020, 01251], lr: 0.000312, loss: 0.4254
2022-10-03 19:51:50 - train: epoch 0054, iter [01030, 01251], lr: 0.000312, loss: 0.4137
2022-10-03 19:52:09 - train: epoch 0054, iter [01040, 01251], lr: 0.000312, loss: 0.4205
2022-10-03 19:52:28 - train: epoch 0054, iter [01050, 01251], lr: 0.000312, loss: 0.4079
2022-10-03 19:52:47 - train: epoch 0054, iter [01060, 01251], lr: 0.000312, loss: 0.4003
2022-10-03 19:53:06 - train: epoch 0054, iter [01070, 01251], lr: 0.000312, loss: 0.4036
2022-10-03 19:53:24 - train: epoch 0054, iter [01080, 01251], lr: 0.000312, loss: 0.3958
2022-10-03 19:53:43 - train: epoch 0054, iter [01090, 01251], lr: 0.000312, loss: 0.4064
2022-10-03 19:54:01 - train: epoch 0054, iter [01100, 01251], lr: 0.000312, loss: 0.4004
2022-10-03 19:54:20 - train: epoch 0054, iter [01110, 01251], lr: 0.000312, loss: 0.4072
2022-10-03 19:54:39 - train: epoch 0054, iter [01120, 01251], lr: 0.000312, loss: 0.4045
2022-10-03 19:54:58 - train: epoch 0054, iter [01130, 01251], lr: 0.000311, loss: 0.4197
2022-10-03 19:55:17 - train: epoch 0054, iter [01140, 01251], lr: 0.000311, loss: 0.4048
2022-10-03 19:55:35 - train: epoch 0054, iter [01150, 01251], lr: 0.000311, loss: 0.4090
2022-10-03 19:55:54 - train: epoch 0054, iter [01160, 01251], lr: 0.000311, loss: 0.3844
2022-10-03 19:56:13 - train: epoch 0054, iter [01170, 01251], lr: 0.000311, loss: 0.4101
2022-10-03 19:56:32 - train: epoch 0054, iter [01180, 01251], lr: 0.000311, loss: 0.4119
2022-10-03 19:56:51 - train: epoch 0054, iter [01190, 01251], lr: 0.000311, loss: 0.4147
2022-10-03 19:57:09 - train: epoch 0054, iter [01200, 01251], lr: 0.000311, loss: 0.4254
2022-10-03 19:57:28 - train: epoch 0054, iter [01210, 01251], lr: 0.000311, loss: 0.4034
2022-10-03 19:57:47 - train: epoch 0054, iter [01220, 01251], lr: 0.000311, loss: 0.4261
2022-10-03 19:58:06 - train: epoch 0054, iter [01230, 01251], lr: 0.000311, loss: 0.4041
2022-10-03 19:58:25 - train: epoch 0054, iter [01240, 01251], lr: 0.000311, loss: 0.3995
2022-10-03 19:58:43 - train: epoch 0054, iter [01250, 01251], lr: 0.000310, loss: 0.4107
2022-10-03 19:58:48 - train: epoch 054, train_loss: 0.4094
2022-10-03 19:58:53 - until epoch: 054, best_loss: 0.4094
2022-10-03 19:58:53 - epoch 055 lr: 0.000310
2022-10-03 19:59:17 - train: epoch 0055, iter [00010, 01251], lr: 0.000310, loss: 0.4229
2022-10-03 19:59:36 - train: epoch 0055, iter [00020, 01251], lr: 0.000310, loss: 0.4063
2022-10-03 19:59:55 - train: epoch 0055, iter [00030, 01251], lr: 0.000310, loss: 0.4221
2022-10-03 20:00:13 - train: epoch 0055, iter [00040, 01251], lr: 0.000310, loss: 0.4060
2022-10-03 20:00:31 - train: epoch 0055, iter [00050, 01251], lr: 0.000310, loss: 0.4222
2022-10-03 20:00:50 - train: epoch 0055, iter [00060, 01251], lr: 0.000310, loss: 0.4029
2022-10-03 20:01:09 - train: epoch 0055, iter [00070, 01251], lr: 0.000310, loss: 0.3999
2022-10-03 20:01:27 - train: epoch 0055, iter [00080, 01251], lr: 0.000310, loss: 0.3976
2022-10-03 20:01:46 - train: epoch 0055, iter [00090, 01251], lr: 0.000310, loss: 0.4064
2022-10-03 20:02:04 - train: epoch 0055, iter [00100, 01251], lr: 0.000310, loss: 0.4129
2022-10-03 20:02:23 - train: epoch 0055, iter [00110, 01251], lr: 0.000310, loss: 0.4168
2022-10-03 20:02:42 - train: epoch 0055, iter [00120, 01251], lr: 0.000309, loss: 0.4077
2022-10-03 20:03:00 - train: epoch 0055, iter [00130, 01251], lr: 0.000309, loss: 0.4048
2022-10-03 20:03:19 - train: epoch 0055, iter [00140, 01251], lr: 0.000309, loss: 0.3960
2022-10-03 20:03:38 - train: epoch 0055, iter [00150, 01251], lr: 0.000309, loss: 0.4149
2022-10-03 20:03:57 - train: epoch 0055, iter [00160, 01251], lr: 0.000309, loss: 0.4057
2022-10-03 20:04:15 - train: epoch 0055, iter [00170, 01251], lr: 0.000309, loss: 0.4068
2022-10-03 20:04:33 - train: epoch 0055, iter [00180, 01251], lr: 0.000309, loss: 0.4164
2022-10-03 20:04:52 - train: epoch 0055, iter [00190, 01251], lr: 0.000309, loss: 0.4216
2022-10-03 20:05:10 - train: epoch 0055, iter [00200, 01251], lr: 0.000309, loss: 0.3895
2022-10-03 20:05:28 - train: epoch 0055, iter [00210, 01251], lr: 0.000309, loss: 0.3969
2022-10-03 20:05:47 - train: epoch 0055, iter [00220, 01251], lr: 0.000309, loss: 0.4144
2022-10-03 20:06:06 - train: epoch 0055, iter [00230, 01251], lr: 0.000309, loss: 0.3961
2022-10-03 20:06:26 - train: epoch 0055, iter [00240, 01251], lr: 0.000308, loss: 0.4067
2022-10-03 20:06:44 - train: epoch 0055, iter [00250, 01251], lr: 0.000308, loss: 0.4114
2022-10-03 20:07:03 - train: epoch 0055, iter [00260, 01251], lr: 0.000308, loss: 0.4074
2022-10-03 20:07:22 - train: epoch 0055, iter [00270, 01251], lr: 0.000308, loss: 0.4109
2022-10-03 20:07:40 - train: epoch 0055, iter [00280, 01251], lr: 0.000308, loss: 0.4391
2022-10-03 20:07:59 - train: epoch 0055, iter [00290, 01251], lr: 0.000308, loss: 0.4343
2022-10-03 20:08:18 - train: epoch 0055, iter [00300, 01251], lr: 0.000308, loss: 0.4214
2022-10-03 20:08:37 - train: epoch 0055, iter [00310, 01251], lr: 0.000308, loss: 0.4089
2022-10-03 20:08:56 - train: epoch 0055, iter [00320, 01251], lr: 0.000308, loss: 0.4162
2022-10-03 20:09:15 - train: epoch 0055, iter [00330, 01251], lr: 0.000308, loss: 0.4102
2022-10-03 20:09:34 - train: epoch 0055, iter [00340, 01251], lr: 0.000308, loss: 0.4089
2022-10-03 20:09:52 - train: epoch 0055, iter [00350, 01251], lr: 0.000308, loss: 0.4102
2022-10-03 20:10:11 - train: epoch 0055, iter [00360, 01251], lr: 0.000307, loss: 0.4170
2022-10-03 20:10:30 - train: epoch 0055, iter [00370, 01251], lr: 0.000307, loss: 0.4129
2022-10-03 20:10:48 - train: epoch 0055, iter [00380, 01251], lr: 0.000307, loss: 0.4006
2022-10-03 20:11:07 - train: epoch 0055, iter [00390, 01251], lr: 0.000307, loss: 0.3908
2022-10-03 20:11:25 - train: epoch 0055, iter [00400, 01251], lr: 0.000307, loss: 0.4072
2022-10-03 20:11:44 - train: epoch 0055, iter [00410, 01251], lr: 0.000307, loss: 0.4013
2022-10-03 20:12:03 - train: epoch 0055, iter [00420, 01251], lr: 0.000307, loss: 0.4057
2022-10-03 20:12:22 - train: epoch 0055, iter [00430, 01251], lr: 0.000307, loss: 0.4243
2022-10-03 20:12:41 - train: epoch 0055, iter [00440, 01251], lr: 0.000307, loss: 0.4036
2022-10-03 20:13:00 - train: epoch 0055, iter [00450, 01251], lr: 0.000307, loss: 0.4066
2022-10-03 20:13:19 - train: epoch 0055, iter [00460, 01251], lr: 0.000307, loss: 0.4103
2022-10-03 20:13:38 - train: epoch 0055, iter [00470, 01251], lr: 0.000307, loss: 0.3930
2022-10-03 20:13:57 - train: epoch 0055, iter [00480, 01251], lr: 0.000306, loss: 0.4032
2022-10-03 20:14:16 - train: epoch 0055, iter [00490, 01251], lr: 0.000306, loss: 0.4083
2022-10-03 20:14:34 - train: epoch 0055, iter [00500, 01251], lr: 0.000306, loss: 0.4025
2022-10-03 20:14:53 - train: epoch 0055, iter [00510, 01251], lr: 0.000306, loss: 0.4144
2022-10-03 20:15:12 - train: epoch 0055, iter [00520, 01251], lr: 0.000306, loss: 0.4054
2022-10-03 20:15:32 - train: epoch 0055, iter [00530, 01251], lr: 0.000306, loss: 0.3898
2022-10-03 20:15:51 - train: epoch 0055, iter [00540, 01251], lr: 0.000306, loss: 0.3908
2022-10-03 20:16:09 - train: epoch 0055, iter [00550, 01251], lr: 0.000306, loss: 0.3931
2022-10-03 20:16:28 - train: epoch 0055, iter [00560, 01251], lr: 0.000306, loss: 0.4172
2022-10-03 20:16:47 - train: epoch 0055, iter [00570, 01251], lr: 0.000306, loss: 0.4126
2022-10-03 20:17:06 - train: epoch 0055, iter [00580, 01251], lr: 0.000306, loss: 0.4307
2022-10-03 20:17:25 - train: epoch 0055, iter [00590, 01251], lr: 0.000306, loss: 0.4145
2022-10-03 20:17:43 - train: epoch 0055, iter [00600, 01251], lr: 0.000305, loss: 0.3894
2022-10-03 20:18:02 - train: epoch 0055, iter [00610, 01251], lr: 0.000305, loss: 0.3983
2022-10-03 20:18:22 - train: epoch 0055, iter [00620, 01251], lr: 0.000305, loss: 0.4307
2022-10-03 20:18:40 - train: epoch 0055, iter [00630, 01251], lr: 0.000305, loss: 0.4216
2022-10-03 20:18:59 - train: epoch 0055, iter [00640, 01251], lr: 0.000305, loss: 0.4178
2022-10-03 20:19:18 - train: epoch 0055, iter [00650, 01251], lr: 0.000305, loss: 0.4068
2022-10-03 20:19:36 - train: epoch 0055, iter [00660, 01251], lr: 0.000305, loss: 0.4189
2022-10-03 20:19:55 - train: epoch 0055, iter [00670, 01251], lr: 0.000305, loss: 0.4183
2022-10-03 20:20:14 - train: epoch 0055, iter [00680, 01251], lr: 0.000305, loss: 0.4123
2022-10-03 20:20:33 - train: epoch 0055, iter [00690, 01251], lr: 0.000305, loss: 0.4034
2022-10-03 20:20:51 - train: epoch 0055, iter [00700, 01251], lr: 0.000305, loss: 0.4132
2022-10-03 20:21:10 - train: epoch 0055, iter [00710, 01251], lr: 0.000305, loss: 0.4083
2022-10-03 20:21:29 - train: epoch 0055, iter [00720, 01251], lr: 0.000304, loss: 0.4042
2022-10-03 20:21:49 - train: epoch 0055, iter [00730, 01251], lr: 0.000304, loss: 0.4098
2022-10-03 20:22:07 - train: epoch 0055, iter [00740, 01251], lr: 0.000304, loss: 0.4204
2022-10-03 20:22:26 - train: epoch 0055, iter [00750, 01251], lr: 0.000304, loss: 0.4101
2022-10-03 20:22:45 - train: epoch 0055, iter [00760, 01251], lr: 0.000304, loss: 0.3933
2022-10-03 20:23:04 - train: epoch 0055, iter [00770, 01251], lr: 0.000304, loss: 0.4119
2022-10-03 20:23:22 - train: epoch 0055, iter [00780, 01251], lr: 0.000304, loss: 0.4125
2022-10-03 20:23:41 - train: epoch 0055, iter [00790, 01251], lr: 0.000304, loss: 0.3985
2022-10-03 20:24:00 - train: epoch 0055, iter [00800, 01251], lr: 0.000304, loss: 0.3994
2022-10-03 20:24:18 - train: epoch 0055, iter [00810, 01251], lr: 0.000304, loss: 0.4004
2022-10-03 20:24:38 - train: epoch 0055, iter [00820, 01251], lr: 0.000304, loss: 0.3998
2022-10-03 20:24:57 - train: epoch 0055, iter [00830, 01251], lr: 0.000304, loss: 0.4211
2022-10-03 20:25:16 - train: epoch 0055, iter [00840, 01251], lr: 0.000303, loss: 0.4127
2022-10-03 20:25:35 - train: epoch 0055, iter [00850, 01251], lr: 0.000303, loss: 0.4059
2022-10-03 20:25:54 - train: epoch 0055, iter [00860, 01251], lr: 0.000303, loss: 0.4097
2022-10-03 20:26:12 - train: epoch 0055, iter [00870, 01251], lr: 0.000303, loss: 0.4165
2022-10-03 20:26:31 - train: epoch 0055, iter [00880, 01251], lr: 0.000303, loss: 0.4089
2022-10-03 20:26:50 - train: epoch 0055, iter [00890, 01251], lr: 0.000303, loss: 0.4051
2022-10-03 20:27:08 - train: epoch 0055, iter [00900, 01251], lr: 0.000303, loss: 0.4022
2022-10-03 20:27:27 - train: epoch 0055, iter [00910, 01251], lr: 0.000303, loss: 0.4075
2022-10-03 20:27:46 - train: epoch 0055, iter [00920, 01251], lr: 0.000303, loss: 0.4056
2022-10-03 20:28:04 - train: epoch 0055, iter [00930, 01251], lr: 0.000303, loss: 0.4164
2022-10-03 20:28:23 - train: epoch 0055, iter [00940, 01251], lr: 0.000303, loss: 0.4115
2022-10-03 20:28:42 - train: epoch 0055, iter [00950, 01251], lr: 0.000303, loss: 0.4168
2022-10-03 20:29:01 - train: epoch 0055, iter [00960, 01251], lr: 0.000302, loss: 0.3992
2022-10-03 20:29:20 - train: epoch 0055, iter [00970, 01251], lr: 0.000302, loss: 0.4211
2022-10-03 20:29:38 - train: epoch 0055, iter [00980, 01251], lr: 0.000302, loss: 0.4160
2022-10-03 20:29:57 - train: epoch 0055, iter [00990, 01251], lr: 0.000302, loss: 0.4156
2022-10-03 20:30:16 - train: epoch 0055, iter [01000, 01251], lr: 0.000302, loss: 0.4117
2022-10-03 20:30:34 - train: epoch 0055, iter [01010, 01251], lr: 0.000302, loss: 0.4075
2022-10-03 20:30:53 - train: epoch 0055, iter [01020, 01251], lr: 0.000302, loss: 0.3908
2022-10-03 20:31:12 - train: epoch 0055, iter [01030, 01251], lr: 0.000302, loss: 0.4145
2022-10-03 20:31:31 - train: epoch 0055, iter [01040, 01251], lr: 0.000302, loss: 0.4306
2022-10-03 20:31:50 - train: epoch 0055, iter [01050, 01251], lr: 0.000302, loss: 0.3920
2022-10-03 20:32:09 - train: epoch 0055, iter [01060, 01251], lr: 0.000302, loss: 0.4014
2022-10-03 20:32:27 - train: epoch 0055, iter [01070, 01251], lr: 0.000302, loss: 0.4332
2022-10-03 20:32:46 - train: epoch 0055, iter [01080, 01251], lr: 0.000301, loss: 0.4083
2022-10-03 20:33:05 - train: epoch 0055, iter [01090, 01251], lr: 0.000301, loss: 0.3906
2022-10-03 20:33:23 - train: epoch 0055, iter [01100, 01251], lr: 0.000301, loss: 0.4213
2022-10-03 20:33:42 - train: epoch 0055, iter [01110, 01251], lr: 0.000301, loss: 0.4044
2022-10-03 20:34:01 - train: epoch 0055, iter [01120, 01251], lr: 0.000301, loss: 0.4025
2022-10-03 20:34:20 - train: epoch 0055, iter [01130, 01251], lr: 0.000301, loss: 0.4115
2022-10-03 20:34:39 - train: epoch 0055, iter [01140, 01251], lr: 0.000301, loss: 0.4193
2022-10-03 20:34:58 - train: epoch 0055, iter [01150, 01251], lr: 0.000301, loss: 0.4331
2022-10-03 20:35:17 - train: epoch 0055, iter [01160, 01251], lr: 0.000301, loss: 0.4043
2022-10-03 20:35:36 - train: epoch 0055, iter [01170, 01251], lr: 0.000301, loss: 0.3960
2022-10-03 20:35:55 - train: epoch 0055, iter [01180, 01251], lr: 0.000301, loss: 0.4173
2022-10-03 20:36:14 - train: epoch 0055, iter [01190, 01251], lr: 0.000301, loss: 0.3978
2022-10-03 20:36:32 - train: epoch 0055, iter [01200, 01251], lr: 0.000300, loss: 0.3948
2022-10-03 20:36:52 - train: epoch 0055, iter [01210, 01251], lr: 0.000300, loss: 0.4286
2022-10-03 20:37:10 - train: epoch 0055, iter [01220, 01251], lr: 0.000300, loss: 0.4114
2022-10-03 20:37:29 - train: epoch 0055, iter [01230, 01251], lr: 0.000300, loss: 0.4199
2022-10-03 20:37:48 - train: epoch 0055, iter [01240, 01251], lr: 0.000300, loss: 0.4232
2022-10-03 20:38:06 - train: epoch 0055, iter [01250, 01251], lr: 0.000300, loss: 0.4216
2022-10-03 20:38:11 - train: epoch 055, train_loss: 0.4091
2022-10-03 20:38:16 - until epoch: 055, best_loss: 0.4091
2022-10-03 20:38:16 - epoch 056 lr: 0.000300
2022-10-03 20:38:40 - train: epoch 0056, iter [00010, 01251], lr: 0.000300, loss: 0.4033
2022-10-03 20:38:59 - train: epoch 0056, iter [00020, 01251], lr: 0.000300, loss: 0.4133
2022-10-03 20:39:17 - train: epoch 0056, iter [00030, 01251], lr: 0.000300, loss: 0.4069
2022-10-03 20:39:36 - train: epoch 0056, iter [00040, 01251], lr: 0.000300, loss: 0.4246
2022-10-03 20:39:56 - train: epoch 0056, iter [00050, 01251], lr: 0.000300, loss: 0.4013
2022-10-03 20:40:14 - train: epoch 0056, iter [00060, 01251], lr: 0.000299, loss: 0.4076
2022-10-03 20:40:33 - train: epoch 0056, iter [00070, 01251], lr: 0.000299, loss: 0.4236
2022-10-03 20:40:52 - train: epoch 0056, iter [00080, 01251], lr: 0.000299, loss: 0.4001
2022-10-03 20:41:11 - train: epoch 0056, iter [00090, 01251], lr: 0.000299, loss: 0.4043
2022-10-03 20:41:30 - train: epoch 0056, iter [00100, 01251], lr: 0.000299, loss: 0.3887
2022-10-03 20:41:49 - train: epoch 0056, iter [00110, 01251], lr: 0.000299, loss: 0.3947
2022-10-03 20:42:08 - train: epoch 0056, iter [00120, 01251], lr: 0.000299, loss: 0.3884
2022-10-03 20:42:27 - train: epoch 0056, iter [00130, 01251], lr: 0.000299, loss: 0.3894
2022-10-03 20:42:46 - train: epoch 0056, iter [00140, 01251], lr: 0.000299, loss: 0.3939
2022-10-03 20:43:05 - train: epoch 0056, iter [00150, 01251], lr: 0.000299, loss: 0.4325
2022-10-03 20:43:24 - train: epoch 0056, iter [00160, 01251], lr: 0.000299, loss: 0.4135
2022-10-03 20:43:43 - train: epoch 0056, iter [00170, 01251], lr: 0.000299, loss: 0.4208
2022-10-03 20:44:02 - train: epoch 0056, iter [00180, 01251], lr: 0.000298, loss: 0.4110
2022-10-03 20:44:22 - train: epoch 0056, iter [00190, 01251], lr: 0.000298, loss: 0.3984
2022-10-03 20:44:40 - train: epoch 0056, iter [00200, 01251], lr: 0.000298, loss: 0.4129
2022-10-03 20:44:59 - train: epoch 0056, iter [00210, 01251], lr: 0.000298, loss: 0.4163
2022-10-03 20:45:18 - train: epoch 0056, iter [00220, 01251], lr: 0.000298, loss: 0.4065
2022-10-03 20:45:37 - train: epoch 0056, iter [00230, 01251], lr: 0.000298, loss: 0.3882
2022-10-03 20:45:57 - train: epoch 0056, iter [00240, 01251], lr: 0.000298, loss: 0.3928
2022-10-03 20:46:15 - train: epoch 0056, iter [00250, 01251], lr: 0.000298, loss: 0.4030
2022-10-03 20:46:34 - train: epoch 0056, iter [00260, 01251], lr: 0.000298, loss: 0.4123
2022-10-03 20:46:53 - train: epoch 0056, iter [00270, 01251], lr: 0.000298, loss: 0.3961
2022-10-03 20:47:12 - train: epoch 0056, iter [00280, 01251], lr: 0.000298, loss: 0.3951
2022-10-03 20:47:31 - train: epoch 0056, iter [00290, 01251], lr: 0.000298, loss: 0.4066
2022-10-03 20:47:51 - train: epoch 0056, iter [00300, 01251], lr: 0.000297, loss: 0.3969
2022-10-03 20:48:10 - train: epoch 0056, iter [00310, 01251], lr: 0.000297, loss: 0.4318
2022-10-03 20:48:28 - train: epoch 0056, iter [00320, 01251], lr: 0.000297, loss: 0.4214
2022-10-03 20:48:47 - train: epoch 0056, iter [00330, 01251], lr: 0.000297, loss: 0.4025
2022-10-03 20:49:06 - train: epoch 0056, iter [00340, 01251], lr: 0.000297, loss: 0.3998
2022-10-03 20:49:24 - train: epoch 0056, iter [00350, 01251], lr: 0.000297, loss: 0.4067
2022-10-03 20:49:44 - train: epoch 0056, iter [00360, 01251], lr: 0.000297, loss: 0.4217
2022-10-03 20:50:03 - train: epoch 0056, iter [00370, 01251], lr: 0.000297, loss: 0.4126
2022-10-03 20:50:22 - train: epoch 0056, iter [00380, 01251], lr: 0.000297, loss: 0.4219
2022-10-03 20:50:41 - train: epoch 0056, iter [00390, 01251], lr: 0.000297, loss: 0.4087
2022-10-03 20:51:00 - train: epoch 0056, iter [00400, 01251], lr: 0.000297, loss: 0.4085
2022-10-03 20:51:19 - train: epoch 0056, iter [00410, 01251], lr: 0.000297, loss: 0.4115
2022-10-03 20:51:38 - train: epoch 0056, iter [00420, 01251], lr: 0.000296, loss: 0.4145
2022-10-03 20:51:57 - train: epoch 0056, iter [00430, 01251], lr: 0.000296, loss: 0.4145
2022-10-03 20:52:16 - train: epoch 0056, iter [00440, 01251], lr: 0.000296, loss: 0.4107
2022-10-03 20:52:35 - train: epoch 0056, iter [00450, 01251], lr: 0.000296, loss: 0.4095
2022-10-03 20:52:54 - train: epoch 0056, iter [00460, 01251], lr: 0.000296, loss: 0.3933
2022-10-03 20:53:13 - train: epoch 0056, iter [00470, 01251], lr: 0.000296, loss: 0.4271
2022-10-03 20:53:31 - train: epoch 0056, iter [00480, 01251], lr: 0.000296, loss: 0.4215
2022-10-03 20:53:50 - train: epoch 0056, iter [00490, 01251], lr: 0.000296, loss: 0.3977
2022-10-03 20:54:09 - train: epoch 0056, iter [00500, 01251], lr: 0.000296, loss: 0.3915
2022-10-03 20:54:28 - train: epoch 0056, iter [00510, 01251], lr: 0.000296, loss: 0.3929
2022-10-03 20:54:47 - train: epoch 0056, iter [00520, 01251], lr: 0.000296, loss: 0.4111
2022-10-03 20:55:06 - train: epoch 0056, iter [00530, 01251], lr: 0.000296, loss: 0.4223
2022-10-03 20:55:25 - train: epoch 0056, iter [00540, 01251], lr: 0.000295, loss: 0.4227
2022-10-03 20:55:44 - train: epoch 0056, iter [00550, 01251], lr: 0.000295, loss: 0.4144
2022-10-03 20:56:02 - train: epoch 0056, iter [00560, 01251], lr: 0.000295, loss: 0.3889
2022-10-03 20:56:21 - train: epoch 0056, iter [00570, 01251], lr: 0.000295, loss: 0.4312
2022-10-03 20:56:40 - train: epoch 0056, iter [00580, 01251], lr: 0.000295, loss: 0.4179
2022-10-03 20:56:59 - train: epoch 0056, iter [00590, 01251], lr: 0.000295, loss: 0.4167
2022-10-03 20:57:18 - train: epoch 0056, iter [00600, 01251], lr: 0.000295, loss: 0.4047
2022-10-03 20:57:37 - train: epoch 0056, iter [00610, 01251], lr: 0.000295, loss: 0.4216
2022-10-03 20:57:56 - train: epoch 0056, iter [00620, 01251], lr: 0.000295, loss: 0.4053
2022-10-03 20:58:14 - train: epoch 0056, iter [00630, 01251], lr: 0.000295, loss: 0.4149
2022-10-03 20:58:33 - train: epoch 0056, iter [00640, 01251], lr: 0.000295, loss: 0.4246
2022-10-03 20:58:52 - train: epoch 0056, iter [00650, 01251], lr: 0.000295, loss: 0.4022
2022-10-03 20:59:11 - train: epoch 0056, iter [00660, 01251], lr: 0.000294, loss: 0.3902
2022-10-03 20:59:30 - train: epoch 0056, iter [00670, 01251], lr: 0.000294, loss: 0.4332
2022-10-03 20:59:49 - train: epoch 0056, iter [00680, 01251], lr: 0.000294, loss: 0.4071
2022-10-03 21:00:08 - train: epoch 0056, iter [00690, 01251], lr: 0.000294, loss: 0.4145
2022-10-03 21:00:27 - train: epoch 0056, iter [00700, 01251], lr: 0.000294, loss: 0.3960
2022-10-03 21:00:46 - train: epoch 0056, iter [00710, 01251], lr: 0.000294, loss: 0.3997
2022-10-03 21:01:05 - train: epoch 0056, iter [00720, 01251], lr: 0.000294, loss: 0.3894
2022-10-03 21:01:24 - train: epoch 0056, iter [00730, 01251], lr: 0.000294, loss: 0.4173
2022-10-03 21:01:42 - train: epoch 0056, iter [00740, 01251], lr: 0.000294, loss: 0.4116
2022-10-03 21:02:01 - train: epoch 0056, iter [00750, 01251], lr: 0.000294, loss: 0.3928
2022-10-03 21:02:20 - train: epoch 0056, iter [00760, 01251], lr: 0.000294, loss: 0.4073
2022-10-03 21:02:39 - train: epoch 0056, iter [00770, 01251], lr: 0.000294, loss: 0.4111
2022-10-03 21:02:58 - train: epoch 0056, iter [00780, 01251], lr: 0.000293, loss: 0.4044
2022-10-03 21:03:17 - train: epoch 0056, iter [00790, 01251], lr: 0.000293, loss: 0.4136
2022-10-03 21:03:37 - train: epoch 0056, iter [00800, 01251], lr: 0.000293, loss: 0.4025
2022-10-03 21:03:56 - train: epoch 0056, iter [00810, 01251], lr: 0.000293, loss: 0.4138
2022-10-03 21:04:15 - train: epoch 0056, iter [00820, 01251], lr: 0.000293, loss: 0.4100
2022-10-03 21:04:34 - train: epoch 0056, iter [00830, 01251], lr: 0.000293, loss: 0.4089
2022-10-03 21:04:53 - train: epoch 0056, iter [00840, 01251], lr: 0.000293, loss: 0.3990
2022-10-03 21:05:12 - train: epoch 0056, iter [00850, 01251], lr: 0.000293, loss: 0.3729
2022-10-03 21:05:31 - train: epoch 0056, iter [00860, 01251], lr: 0.000293, loss: 0.4143
2022-10-03 21:05:50 - train: epoch 0056, iter [00870, 01251], lr: 0.000293, loss: 0.4155
2022-10-03 21:06:08 - train: epoch 0056, iter [00880, 01251], lr: 0.000293, loss: 0.4160
2022-10-03 21:06:28 - train: epoch 0056, iter [00890, 01251], lr: 0.000293, loss: 0.3862
2022-10-03 21:06:47 - train: epoch 0056, iter [00900, 01251], lr: 0.000292, loss: 0.4134
2022-10-03 21:07:06 - train: epoch 0056, iter [00910, 01251], lr: 0.000292, loss: 0.4026
2022-10-03 21:07:25 - train: epoch 0056, iter [00920, 01251], lr: 0.000292, loss: 0.4173
2022-10-03 21:07:44 - train: epoch 0056, iter [00930, 01251], lr: 0.000292, loss: 0.4341
2022-10-03 21:08:03 - train: epoch 0056, iter [00940, 01251], lr: 0.000292, loss: 0.4139
2022-10-03 21:08:22 - train: epoch 0056, iter [00950, 01251], lr: 0.000292, loss: 0.4105
2022-10-03 21:08:41 - train: epoch 0056, iter [00960, 01251], lr: 0.000292, loss: 0.3894
2022-10-03 21:09:01 - train: epoch 0056, iter [00970, 01251], lr: 0.000292, loss: 0.4213
2022-10-03 21:09:20 - train: epoch 0056, iter [00980, 01251], lr: 0.000292, loss: 0.3995
2022-10-03 21:09:39 - train: epoch 0056, iter [00990, 01251], lr: 0.000292, loss: 0.4237
2022-10-03 21:09:58 - train: epoch 0056, iter [01000, 01251], lr: 0.000292, loss: 0.4102
2022-10-03 21:10:17 - train: epoch 0056, iter [01010, 01251], lr: 0.000292, loss: 0.4268
2022-10-03 21:10:36 - train: epoch 0056, iter [01020, 01251], lr: 0.000291, loss: 0.4016
2022-10-03 21:10:55 - train: epoch 0056, iter [01030, 01251], lr: 0.000291, loss: 0.4061
2022-10-03 21:11:14 - train: epoch 0056, iter [01040, 01251], lr: 0.000291, loss: 0.4157
2022-10-03 21:11:33 - train: epoch 0056, iter [01050, 01251], lr: 0.000291, loss: 0.3852
2022-10-03 21:11:52 - train: epoch 0056, iter [01060, 01251], lr: 0.000291, loss: 0.4051
2022-10-03 21:12:11 - train: epoch 0056, iter [01070, 01251], lr: 0.000291, loss: 0.4200
2022-10-03 21:12:30 - train: epoch 0056, iter [01080, 01251], lr: 0.000291, loss: 0.4140
2022-10-03 21:12:49 - train: epoch 0056, iter [01090, 01251], lr: 0.000291, loss: 0.3920
2022-10-03 21:13:08 - train: epoch 0056, iter [01100, 01251], lr: 0.000291, loss: 0.4279
2022-10-03 21:13:27 - train: epoch 0056, iter [01110, 01251], lr: 0.000291, loss: 0.4056
2022-10-03 21:13:46 - train: epoch 0056, iter [01120, 01251], lr: 0.000291, loss: 0.4175
2022-10-03 21:14:05 - train: epoch 0056, iter [01130, 01251], lr: 0.000291, loss: 0.3989
2022-10-03 21:14:24 - train: epoch 0056, iter [01140, 01251], lr: 0.000290, loss: 0.3841
2022-10-03 21:14:43 - train: epoch 0056, iter [01150, 01251], lr: 0.000290, loss: 0.4222
2022-10-03 21:15:02 - train: epoch 0056, iter [01160, 01251], lr: 0.000290, loss: 0.3901
2022-10-03 21:15:21 - train: epoch 0056, iter [01170, 01251], lr: 0.000290, loss: 0.4047
2022-10-03 21:15:39 - train: epoch 0056, iter [01180, 01251], lr: 0.000290, loss: 0.4044
2022-10-03 21:15:59 - train: epoch 0056, iter [01190, 01251], lr: 0.000290, loss: 0.4101
2022-10-03 21:16:18 - train: epoch 0056, iter [01200, 01251], lr: 0.000290, loss: 0.3753
2022-10-03 21:16:37 - train: epoch 0056, iter [01210, 01251], lr: 0.000290, loss: 0.4098
2022-10-03 21:16:56 - train: epoch 0056, iter [01220, 01251], lr: 0.000290, loss: 0.4207
2022-10-03 21:17:15 - train: epoch 0056, iter [01230, 01251], lr: 0.000290, loss: 0.4027
2022-10-03 21:17:35 - train: epoch 0056, iter [01240, 01251], lr: 0.000290, loss: 0.4093
2022-10-03 21:17:53 - train: epoch 0056, iter [01250, 01251], lr: 0.000290, loss: 0.3919
2022-10-03 21:17:58 - train: epoch 056, train_loss: 0.4088
2022-10-03 21:18:03 - until epoch: 056, best_loss: 0.4088
2022-10-03 21:18:03 - epoch 057 lr: 0.000290
2022-10-03 21:18:28 - train: epoch 0057, iter [00010, 01251], lr: 0.000289, loss: 0.3868
2022-10-03 21:18:46 - train: epoch 0057, iter [00020, 01251], lr: 0.000289, loss: 0.4090
2022-10-03 21:19:05 - train: epoch 0057, iter [00030, 01251], lr: 0.000289, loss: 0.4127
2022-10-03 21:19:23 - train: epoch 0057, iter [00040, 01251], lr: 0.000289, loss: 0.4188
2022-10-03 21:19:42 - train: epoch 0057, iter [00050, 01251], lr: 0.000289, loss: 0.4208
2022-10-03 21:20:01 - train: epoch 0057, iter [00060, 01251], lr: 0.000289, loss: 0.4055
2022-10-03 21:20:20 - train: epoch 0057, iter [00070, 01251], lr: 0.000289, loss: 0.4035
2022-10-03 21:20:39 - train: epoch 0057, iter [00080, 01251], lr: 0.000289, loss: 0.4428
2022-10-03 21:20:58 - train: epoch 0057, iter [00090, 01251], lr: 0.000289, loss: 0.3918
2022-10-03 21:21:17 - train: epoch 0057, iter [00100, 01251], lr: 0.000289, loss: 0.4292
2022-10-03 21:21:36 - train: epoch 0057, iter [00110, 01251], lr: 0.000289, loss: 0.4031
2022-10-03 21:21:55 - train: epoch 0057, iter [00120, 01251], lr: 0.000289, loss: 0.4033
2022-10-03 21:22:14 - train: epoch 0057, iter [00130, 01251], lr: 0.000288, loss: 0.4112
2022-10-03 21:22:33 - train: epoch 0057, iter [00140, 01251], lr: 0.000288, loss: 0.4465
2022-10-03 21:22:52 - train: epoch 0057, iter [00150, 01251], lr: 0.000288, loss: 0.4148
2022-10-03 21:23:10 - train: epoch 0057, iter [00160, 01251], lr: 0.000288, loss: 0.4212
2022-10-03 21:23:30 - train: epoch 0057, iter [00170, 01251], lr: 0.000288, loss: 0.4150
2022-10-03 21:23:48 - train: epoch 0057, iter [00180, 01251], lr: 0.000288, loss: 0.4215
2022-10-03 21:24:08 - train: epoch 0057, iter [00190, 01251], lr: 0.000288, loss: 0.4015
2022-10-03 21:24:26 - train: epoch 0057, iter [00200, 01251], lr: 0.000288, loss: 0.4011
2022-10-03 21:24:45 - train: epoch 0057, iter [00210, 01251], lr: 0.000288, loss: 0.4212
2022-10-03 21:25:04 - train: epoch 0057, iter [00220, 01251], lr: 0.000288, loss: 0.4059
2022-10-03 21:25:23 - train: epoch 0057, iter [00230, 01251], lr: 0.000288, loss: 0.4089
2022-10-03 21:25:41 - train: epoch 0057, iter [00240, 01251], lr: 0.000288, loss: 0.4168
2022-10-03 21:26:00 - train: epoch 0057, iter [00250, 01251], lr: 0.000287, loss: 0.4062
2022-10-03 21:26:19 - train: epoch 0057, iter [00260, 01251], lr: 0.000287, loss: 0.4044
2022-10-03 21:26:38 - train: epoch 0057, iter [00270, 01251], lr: 0.000287, loss: 0.4020
2022-10-03 21:26:57 - train: epoch 0057, iter [00280, 01251], lr: 0.000287, loss: 0.4174
2022-10-03 21:27:16 - train: epoch 0057, iter [00290, 01251], lr: 0.000287, loss: 0.4142
2022-10-03 21:27:35 - train: epoch 0057, iter [00300, 01251], lr: 0.000287, loss: 0.4091
2022-10-03 21:27:54 - train: epoch 0057, iter [00310, 01251], lr: 0.000287, loss: 0.4048
2022-10-03 21:28:13 - train: epoch 0057, iter [00320, 01251], lr: 0.000287, loss: 0.4106
2022-10-03 21:28:32 - train: epoch 0057, iter [00330, 01251], lr: 0.000287, loss: 0.4058
2022-10-03 21:28:51 - train: epoch 0057, iter [00340, 01251], lr: 0.000287, loss: 0.4074
2022-10-03 21:29:10 - train: epoch 0057, iter [00350, 01251], lr: 0.000287, loss: 0.4098
2022-10-03 21:29:29 - train: epoch 0057, iter [00360, 01251], lr: 0.000287, loss: 0.4042
2022-10-03 21:29:48 - train: epoch 0057, iter [00370, 01251], lr: 0.000286, loss: 0.4221
2022-10-03 21:30:06 - train: epoch 0057, iter [00380, 01251], lr: 0.000286, loss: 0.4060
2022-10-03 21:30:25 - train: epoch 0057, iter [00390, 01251], lr: 0.000286, loss: 0.4119
2022-10-03 21:30:43 - train: epoch 0057, iter [00400, 01251], lr: 0.000286, loss: 0.3962
2022-10-03 21:31:03 - train: epoch 0057, iter [00410, 01251], lr: 0.000286, loss: 0.4079
2022-10-03 21:31:21 - train: epoch 0057, iter [00420, 01251], lr: 0.000286, loss: 0.4108
2022-10-03 21:31:40 - train: epoch 0057, iter [00430, 01251], lr: 0.000286, loss: 0.4165
2022-10-03 21:31:58 - train: epoch 0057, iter [00440, 01251], lr: 0.000286, loss: 0.3970
2022-10-03 21:32:17 - train: epoch 0057, iter [00450, 01251], lr: 0.000286, loss: 0.3914
2022-10-03 21:32:36 - train: epoch 0057, iter [00460, 01251], lr: 0.000286, loss: 0.4042
2022-10-03 21:32:54 - train: epoch 0057, iter [00470, 01251], lr: 0.000286, loss: 0.4009
2022-10-03 21:33:13 - train: epoch 0057, iter [00480, 01251], lr: 0.000286, loss: 0.4194
2022-10-03 21:33:32 - train: epoch 0057, iter [00490, 01251], lr: 0.000285, loss: 0.4093
2022-10-03 21:33:51 - train: epoch 0057, iter [00500, 01251], lr: 0.000285, loss: 0.4081
2022-10-03 21:34:10 - train: epoch 0057, iter [00510, 01251], lr: 0.000285, loss: 0.3976
2022-10-03 21:34:28 - train: epoch 0057, iter [00520, 01251], lr: 0.000285, loss: 0.4109
2022-10-03 21:34:47 - train: epoch 0057, iter [00530, 01251], lr: 0.000285, loss: 0.4154
2022-10-03 21:35:06 - train: epoch 0057, iter [00540, 01251], lr: 0.000285, loss: 0.4065
2022-10-03 21:35:26 - train: epoch 0057, iter [00550, 01251], lr: 0.000285, loss: 0.4138
2022-10-03 21:35:45 - train: epoch 0057, iter [00560, 01251], lr: 0.000285, loss: 0.4145
2022-10-03 21:36:04 - train: epoch 0057, iter [00570, 01251], lr: 0.000285, loss: 0.4162
2022-10-03 21:36:23 - train: epoch 0057, iter [00580, 01251], lr: 0.000285, loss: 0.4150
2022-10-03 21:36:42 - train: epoch 0057, iter [00590, 01251], lr: 0.000285, loss: 0.4081
2022-10-03 21:37:01 - train: epoch 0057, iter [00600, 01251], lr: 0.000285, loss: 0.4098
2022-10-03 21:37:20 - train: epoch 0057, iter [00610, 01251], lr: 0.000284, loss: 0.4036
2022-10-03 21:37:39 - train: epoch 0057, iter [00620, 01251], lr: 0.000284, loss: 0.4133
2022-10-03 21:37:58 - train: epoch 0057, iter [00630, 01251], lr: 0.000284, loss: 0.4231
2022-10-03 21:38:18 - train: epoch 0057, iter [00640, 01251], lr: 0.000284, loss: 0.4158
2022-10-03 21:38:37 - train: epoch 0057, iter [00650, 01251], lr: 0.000284, loss: 0.4189
2022-10-03 21:38:55 - train: epoch 0057, iter [00660, 01251], lr: 0.000284, loss: 0.3966
2022-10-03 21:39:14 - train: epoch 0057, iter [00670, 01251], lr: 0.000284, loss: 0.3982
2022-10-03 21:39:33 - train: epoch 0057, iter [00680, 01251], lr: 0.000284, loss: 0.3999
2022-10-03 21:39:52 - train: epoch 0057, iter [00690, 01251], lr: 0.000284, loss: 0.4076
2022-10-03 21:40:11 - train: epoch 0057, iter [00700, 01251], lr: 0.000284, loss: 0.4074
2022-10-03 21:40:30 - train: epoch 0057, iter [00710, 01251], lr: 0.000284, loss: 0.4191
2022-10-03 21:40:48 - train: epoch 0057, iter [00720, 01251], lr: 0.000284, loss: 0.3990
2022-10-03 21:41:07 - train: epoch 0057, iter [00730, 01251], lr: 0.000283, loss: 0.4058
2022-10-03 21:41:26 - train: epoch 0057, iter [00740, 01251], lr: 0.000283, loss: 0.3954
2022-10-03 21:41:45 - train: epoch 0057, iter [00750, 01251], lr: 0.000283, loss: 0.3974
2022-10-03 21:42:04 - train: epoch 0057, iter [00760, 01251], lr: 0.000283, loss: 0.4170
2022-10-03 21:42:22 - train: epoch 0057, iter [00770, 01251], lr: 0.000283, loss: 0.4226
2022-10-03 21:42:41 - train: epoch 0057, iter [00780, 01251], lr: 0.000283, loss: 0.4185
2022-10-03 21:43:00 - train: epoch 0057, iter [00790, 01251], lr: 0.000283, loss: 0.3879
2022-10-03 21:43:19 - train: epoch 0057, iter [00800, 01251], lr: 0.000283, loss: 0.4071
2022-10-03 21:43:38 - train: epoch 0057, iter [00810, 01251], lr: 0.000283, loss: 0.3983
2022-10-03 21:43:57 - train: epoch 0057, iter [00820, 01251], lr: 0.000283, loss: 0.4034
2022-10-03 21:44:15 - train: epoch 0057, iter [00830, 01251], lr: 0.000283, loss: 0.3945
2022-10-03 21:44:34 - train: epoch 0057, iter [00840, 01251], lr: 0.000283, loss: 0.3809
2022-10-03 21:44:53 - train: epoch 0057, iter [00850, 01251], lr: 0.000282, loss: 0.4205
2022-10-03 21:45:12 - train: epoch 0057, iter [00860, 01251], lr: 0.000282, loss: 0.4024
2022-10-03 21:45:32 - train: epoch 0057, iter [00870, 01251], lr: 0.000282, loss: 0.4082
2022-10-03 21:45:51 - train: epoch 0057, iter [00880, 01251], lr: 0.000282, loss: 0.4166
2022-10-03 21:46:10 - train: epoch 0057, iter [00890, 01251], lr: 0.000282, loss: 0.4272
2022-10-03 21:46:28 - train: epoch 0057, iter [00900, 01251], lr: 0.000282, loss: 0.4076
2022-10-03 21:46:47 - train: epoch 0057, iter [00910, 01251], lr: 0.000282, loss: 0.4151
2022-10-03 21:47:06 - train: epoch 0057, iter [00920, 01251], lr: 0.000282, loss: 0.4151
2022-10-03 21:47:25 - train: epoch 0057, iter [00930, 01251], lr: 0.000282, loss: 0.4291
2022-10-03 21:47:44 - train: epoch 0057, iter [00940, 01251], lr: 0.000282, loss: 0.4038
2022-10-03 21:48:04 - train: epoch 0057, iter [00950, 01251], lr: 0.000282, loss: 0.4138
2022-10-03 21:48:23 - train: epoch 0057, iter [00960, 01251], lr: 0.000282, loss: 0.4139
2022-10-03 21:48:42 - train: epoch 0057, iter [00970, 01251], lr: 0.000281, loss: 0.4295
2022-10-03 21:49:01 - train: epoch 0057, iter [00980, 01251], lr: 0.000281, loss: 0.4003
2022-10-03 21:49:21 - train: epoch 0057, iter [00990, 01251], lr: 0.000281, loss: 0.3938
2022-10-03 21:49:40 - train: epoch 0057, iter [01000, 01251], lr: 0.000281, loss: 0.3922
2022-10-03 21:49:59 - train: epoch 0057, iter [01010, 01251], lr: 0.000281, loss: 0.4148
2022-10-03 21:50:17 - train: epoch 0057, iter [01020, 01251], lr: 0.000281, loss: 0.4108
2022-10-03 21:50:37 - train: epoch 0057, iter [01030, 01251], lr: 0.000281, loss: 0.4140
2022-10-03 21:50:56 - train: epoch 0057, iter [01040, 01251], lr: 0.000281, loss: 0.4162
2022-10-03 21:51:15 - train: epoch 0057, iter [01050, 01251], lr: 0.000281, loss: 0.3860
2022-10-03 21:51:34 - train: epoch 0057, iter [01060, 01251], lr: 0.000281, loss: 0.4230
2022-10-03 21:51:53 - train: epoch 0057, iter [01070, 01251], lr: 0.000281, loss: 0.4031
2022-10-03 21:52:11 - train: epoch 0057, iter [01080, 01251], lr: 0.000281, loss: 0.4162
2022-10-03 21:52:30 - train: epoch 0057, iter [01090, 01251], lr: 0.000280, loss: 0.4043
2022-10-03 21:52:49 - train: epoch 0057, iter [01100, 01251], lr: 0.000280, loss: 0.4017
2022-10-03 21:53:08 - train: epoch 0057, iter [01110, 01251], lr: 0.000280, loss: 0.4220
2022-10-03 21:53:27 - train: epoch 0057, iter [01120, 01251], lr: 0.000280, loss: 0.4146
2022-10-03 21:53:46 - train: epoch 0057, iter [01130, 01251], lr: 0.000280, loss: 0.3912
2022-10-03 21:54:05 - train: epoch 0057, iter [01140, 01251], lr: 0.000280, loss: 0.3993
2022-10-03 21:54:23 - train: epoch 0057, iter [01150, 01251], lr: 0.000280, loss: 0.4064
2022-10-03 21:54:42 - train: epoch 0057, iter [01160, 01251], lr: 0.000280, loss: 0.3886
2022-10-03 21:55:02 - train: epoch 0057, iter [01170, 01251], lr: 0.000280, loss: 0.3872
2022-10-03 21:55:21 - train: epoch 0057, iter [01180, 01251], lr: 0.000280, loss: 0.4099
2022-10-03 21:55:40 - train: epoch 0057, iter [01190, 01251], lr: 0.000280, loss: 0.4128
2022-10-03 21:55:59 - train: epoch 0057, iter [01200, 01251], lr: 0.000279, loss: 0.4094
2022-10-03 21:56:18 - train: epoch 0057, iter [01210, 01251], lr: 0.000279, loss: 0.4275
2022-10-03 21:56:37 - train: epoch 0057, iter [01220, 01251], lr: 0.000279, loss: 0.4232
2022-10-03 21:56:56 - train: epoch 0057, iter [01230, 01251], lr: 0.000279, loss: 0.4022
2022-10-03 21:57:14 - train: epoch 0057, iter [01240, 01251], lr: 0.000279, loss: 0.4093
2022-10-03 21:57:32 - train: epoch 0057, iter [01250, 01251], lr: 0.000279, loss: 0.4014
2022-10-03 21:57:37 - train: epoch 057, train_loss: 0.4085
2022-10-03 21:57:42 - until epoch: 057, best_loss: 0.4085
2022-10-03 21:57:42 - epoch 058 lr: 0.000279
2022-10-03 21:58:06 - train: epoch 0058, iter [00010, 01251], lr: 0.000279, loss: 0.3964
2022-10-03 21:58:25 - train: epoch 0058, iter [00020, 01251], lr: 0.000279, loss: 0.4245
2022-10-03 21:58:44 - train: epoch 0058, iter [00030, 01251], lr: 0.000279, loss: 0.4093
2022-10-03 21:59:03 - train: epoch 0058, iter [00040, 01251], lr: 0.000279, loss: 0.4335
2022-10-03 21:59:21 - train: epoch 0058, iter [00050, 01251], lr: 0.000279, loss: 0.4125
2022-10-03 21:59:40 - train: epoch 0058, iter [00060, 01251], lr: 0.000279, loss: 0.4224
2022-10-03 21:59:59 - train: epoch 0058, iter [00070, 01251], lr: 0.000278, loss: 0.4091
2022-10-03 22:00:18 - train: epoch 0058, iter [00080, 01251], lr: 0.000278, loss: 0.4172
2022-10-03 22:00:37 - train: epoch 0058, iter [00090, 01251], lr: 0.000278, loss: 0.4159
2022-10-03 22:00:55 - train: epoch 0058, iter [00100, 01251], lr: 0.000278, loss: 0.4036
2022-10-03 22:01:14 - train: epoch 0058, iter [00110, 01251], lr: 0.000278, loss: 0.4110
2022-10-03 22:01:33 - train: epoch 0058, iter [00120, 01251], lr: 0.000278, loss: 0.4106
2022-10-03 22:01:51 - train: epoch 0058, iter [00130, 01251], lr: 0.000278, loss: 0.4055
2022-10-03 22:02:10 - train: epoch 0058, iter [00140, 01251], lr: 0.000278, loss: 0.4078
2022-10-03 22:02:29 - train: epoch 0058, iter [00150, 01251], lr: 0.000278, loss: 0.4015
2022-10-03 22:02:48 - train: epoch 0058, iter [00160, 01251], lr: 0.000278, loss: 0.4187
2022-10-03 22:03:07 - train: epoch 0058, iter [00170, 01251], lr: 0.000278, loss: 0.4207
2022-10-03 22:03:25 - train: epoch 0058, iter [00180, 01251], lr: 0.000278, loss: 0.3979
2022-10-03 22:03:44 - train: epoch 0058, iter [00190, 01251], lr: 0.000277, loss: 0.3937
2022-10-03 22:04:03 - train: epoch 0058, iter [00200, 01251], lr: 0.000277, loss: 0.4112
2022-10-03 22:04:22 - train: epoch 0058, iter [00210, 01251], lr: 0.000277, loss: 0.4232
2022-10-03 22:04:41 - train: epoch 0058, iter [00220, 01251], lr: 0.000277, loss: 0.4057
2022-10-03 22:05:00 - train: epoch 0058, iter [00230, 01251], lr: 0.000277, loss: 0.3992
2022-10-03 22:05:19 - train: epoch 0058, iter [00240, 01251], lr: 0.000277, loss: 0.4304
2022-10-03 22:05:38 - train: epoch 0058, iter [00250, 01251], lr: 0.000277, loss: 0.4102
2022-10-03 22:05:57 - train: epoch 0058, iter [00260, 01251], lr: 0.000277, loss: 0.4300
2022-10-03 22:06:16 - train: epoch 0058, iter [00270, 01251], lr: 0.000277, loss: 0.4194
2022-10-03 22:06:35 - train: epoch 0058, iter [00280, 01251], lr: 0.000277, loss: 0.4172
2022-10-03 22:06:53 - train: epoch 0058, iter [00290, 01251], lr: 0.000277, loss: 0.4022
2022-10-03 22:07:12 - train: epoch 0058, iter [00300, 01251], lr: 0.000277, loss: 0.4074
2022-10-03 22:07:31 - train: epoch 0058, iter [00310, 01251], lr: 0.000276, loss: 0.4108
2022-10-03 22:07:50 - train: epoch 0058, iter [00320, 01251], lr: 0.000276, loss: 0.4126
2022-10-03 22:08:09 - train: epoch 0058, iter [00330, 01251], lr: 0.000276, loss: 0.3995
2022-10-03 22:08:28 - train: epoch 0058, iter [00340, 01251], lr: 0.000276, loss: 0.4130
2022-10-03 22:08:47 - train: epoch 0058, iter [00350, 01251], lr: 0.000276, loss: 0.4160
2022-10-03 22:09:05 - train: epoch 0058, iter [00360, 01251], lr: 0.000276, loss: 0.3950
2022-10-03 22:09:24 - train: epoch 0058, iter [00370, 01251], lr: 0.000276, loss: 0.3957
2022-10-03 22:09:43 - train: epoch 0058, iter [00380, 01251], lr: 0.000276, loss: 0.4163
2022-10-03 22:10:01 - train: epoch 0058, iter [00390, 01251], lr: 0.000276, loss: 0.3983
2022-10-03 22:10:20 - train: epoch 0058, iter [00400, 01251], lr: 0.000276, loss: 0.4194
2022-10-03 22:10:39 - train: epoch 0058, iter [00410, 01251], lr: 0.000276, loss: 0.4006
2022-10-03 22:10:58 - train: epoch 0058, iter [00420, 01251], lr: 0.000276, loss: 0.4004
2022-10-03 22:11:17 - train: epoch 0058, iter [00430, 01251], lr: 0.000275, loss: 0.4138
2022-10-03 22:11:35 - train: epoch 0058, iter [00440, 01251], lr: 0.000275, loss: 0.4197
2022-10-03 22:11:55 - train: epoch 0058, iter [00450, 01251], lr: 0.000275, loss: 0.4036
2022-10-03 22:12:13 - train: epoch 0058, iter [00460, 01251], lr: 0.000275, loss: 0.3863
2022-10-03 22:12:32 - train: epoch 0058, iter [00470, 01251], lr: 0.000275, loss: 0.4115
2022-10-03 22:12:51 - train: epoch 0058, iter [00480, 01251], lr: 0.000275, loss: 0.3985
2022-10-03 22:13:09 - train: epoch 0058, iter [00490, 01251], lr: 0.000275, loss: 0.4060
2022-10-03 22:13:28 - train: epoch 0058, iter [00500, 01251], lr: 0.000275, loss: 0.4034
2022-10-03 22:13:47 - train: epoch 0058, iter [00510, 01251], lr: 0.000275, loss: 0.4067
2022-10-03 22:14:06 - train: epoch 0058, iter [00520, 01251], lr: 0.000275, loss: 0.4210
2022-10-03 22:14:25 - train: epoch 0058, iter [00530, 01251], lr: 0.000275, loss: 0.4010
2022-10-03 22:14:43 - train: epoch 0058, iter [00540, 01251], lr: 0.000275, loss: 0.3967
2022-10-03 22:15:02 - train: epoch 0058, iter [00550, 01251], lr: 0.000274, loss: 0.4012
2022-10-03 22:15:21 - train: epoch 0058, iter [00560, 01251], lr: 0.000274, loss: 0.4046
2022-10-03 22:15:40 - train: epoch 0058, iter [00570, 01251], lr: 0.000274, loss: 0.4328
2022-10-03 22:15:59 - train: epoch 0058, iter [00580, 01251], lr: 0.000274, loss: 0.4010
2022-10-03 22:16:18 - train: epoch 0058, iter [00590, 01251], lr: 0.000274, loss: 0.4068
2022-10-03 22:16:36 - train: epoch 0058, iter [00600, 01251], lr: 0.000274, loss: 0.4072
2022-10-03 22:16:55 - train: epoch 0058, iter [00610, 01251], lr: 0.000274, loss: 0.3997
2022-10-03 22:17:15 - train: epoch 0058, iter [00620, 01251], lr: 0.000274, loss: 0.3776
2022-10-03 22:17:33 - train: epoch 0058, iter [00630, 01251], lr: 0.000274, loss: 0.4153
2022-10-03 22:17:53 - train: epoch 0058, iter [00640, 01251], lr: 0.000274, loss: 0.4023
2022-10-03 22:18:11 - train: epoch 0058, iter [00650, 01251], lr: 0.000274, loss: 0.4200
2022-10-03 22:18:30 - train: epoch 0058, iter [00660, 01251], lr: 0.000274, loss: 0.4055
2022-10-03 22:18:49 - train: epoch 0058, iter [00670, 01251], lr: 0.000273, loss: 0.4069
2022-10-03 22:19:07 - train: epoch 0058, iter [00680, 01251], lr: 0.000273, loss: 0.4233
2022-10-03 22:19:26 - train: epoch 0058, iter [00690, 01251], lr: 0.000273, loss: 0.4105
2022-10-03 22:19:45 - train: epoch 0058, iter [00700, 01251], lr: 0.000273, loss: 0.4008
2022-10-03 22:20:04 - train: epoch 0058, iter [00710, 01251], lr: 0.000273, loss: 0.4260
2022-10-03 22:20:23 - train: epoch 0058, iter [00720, 01251], lr: 0.000273, loss: 0.3975
2022-10-03 22:20:42 - train: epoch 0058, iter [00730, 01251], lr: 0.000273, loss: 0.4056
2022-10-03 22:21:01 - train: epoch 0058, iter [00740, 01251], lr: 0.000273, loss: 0.4061
2022-10-03 22:21:20 - train: epoch 0058, iter [00750, 01251], lr: 0.000273, loss: 0.4131
2022-10-03 22:21:39 - train: epoch 0058, iter [00760, 01251], lr: 0.000273, loss: 0.4002
2022-10-03 22:21:58 - train: epoch 0058, iter [00770, 01251], lr: 0.000273, loss: 0.3866
2022-10-03 22:22:17 - train: epoch 0058, iter [00780, 01251], lr: 0.000273, loss: 0.4025
2022-10-03 22:22:36 - train: epoch 0058, iter [00790, 01251], lr: 0.000272, loss: 0.4159
2022-10-03 22:22:55 - train: epoch 0058, iter [00800, 01251], lr: 0.000272, loss: 0.4111
2022-10-03 22:23:14 - train: epoch 0058, iter [00810, 01251], lr: 0.000272, loss: 0.3930
2022-10-03 22:23:32 - train: epoch 0058, iter [00820, 01251], lr: 0.000272, loss: 0.3914
2022-10-03 22:23:51 - train: epoch 0058, iter [00830, 01251], lr: 0.000272, loss: 0.4057
2022-10-03 22:24:10 - train: epoch 0058, iter [00840, 01251], lr: 0.000272, loss: 0.3942
2022-10-03 22:24:29 - train: epoch 0058, iter [00850, 01251], lr: 0.000272, loss: 0.3979
2022-10-03 22:24:47 - train: epoch 0058, iter [00860, 01251], lr: 0.000272, loss: 0.4152
2022-10-03 22:25:06 - train: epoch 0058, iter [00870, 01251], lr: 0.000272, loss: 0.4180
2022-10-03 22:25:25 - train: epoch 0058, iter [00880, 01251], lr: 0.000272, loss: 0.3963
2022-10-03 22:25:44 - train: epoch 0058, iter [00890, 01251], lr: 0.000272, loss: 0.4140
2022-10-03 22:26:02 - train: epoch 0058, iter [00900, 01251], lr: 0.000272, loss: 0.4014
2022-10-03 22:26:21 - train: epoch 0058, iter [00910, 01251], lr: 0.000271, loss: 0.4010
2022-10-03 22:26:40 - train: epoch 0058, iter [00920, 01251], lr: 0.000271, loss: 0.4089
2022-10-03 22:26:59 - train: epoch 0058, iter [00930, 01251], lr: 0.000271, loss: 0.4098
2022-10-03 22:27:18 - train: epoch 0058, iter [00940, 01251], lr: 0.000271, loss: 0.3984
2022-10-03 22:27:37 - train: epoch 0058, iter [00950, 01251], lr: 0.000271, loss: 0.3991
2022-10-03 22:27:56 - train: epoch 0058, iter [00960, 01251], lr: 0.000271, loss: 0.4147
2022-10-03 22:28:15 - train: epoch 0058, iter [00970, 01251], lr: 0.000271, loss: 0.4367
2022-10-03 22:28:34 - train: epoch 0058, iter [00980, 01251], lr: 0.000271, loss: 0.4095
2022-10-03 22:28:53 - train: epoch 0058, iter [00990, 01251], lr: 0.000271, loss: 0.3997
2022-10-03 22:29:12 - train: epoch 0058, iter [01000, 01251], lr: 0.000271, loss: 0.4161
2022-10-03 22:29:30 - train: epoch 0058, iter [01010, 01251], lr: 0.000271, loss: 0.4259
2022-10-03 22:29:49 - train: epoch 0058, iter [01020, 01251], lr: 0.000271, loss: 0.4080
2022-10-03 22:30:08 - train: epoch 0058, iter [01030, 01251], lr: 0.000270, loss: 0.4073
2022-10-03 22:30:27 - train: epoch 0058, iter [01040, 01251], lr: 0.000270, loss: 0.4007
2022-10-03 22:30:46 - train: epoch 0058, iter [01050, 01251], lr: 0.000270, loss: 0.4101
2022-10-03 22:31:04 - train: epoch 0058, iter [01060, 01251], lr: 0.000270, loss: 0.4064
2022-10-03 22:31:23 - train: epoch 0058, iter [01070, 01251], lr: 0.000270, loss: 0.3944
2022-10-03 22:31:42 - train: epoch 0058, iter [01080, 01251], lr: 0.000270, loss: 0.4079
2022-10-03 22:32:01 - train: epoch 0058, iter [01090, 01251], lr: 0.000270, loss: 0.3960
2022-10-03 22:32:20 - train: epoch 0058, iter [01100, 01251], lr: 0.000270, loss: 0.4018
2022-10-03 22:32:39 - train: epoch 0058, iter [01110, 01251], lr: 0.000270, loss: 0.4071
2022-10-03 22:32:58 - train: epoch 0058, iter [01120, 01251], lr: 0.000270, loss: 0.4066
2022-10-03 22:33:17 - train: epoch 0058, iter [01130, 01251], lr: 0.000270, loss: 0.4051
2022-10-03 22:33:36 - train: epoch 0058, iter [01140, 01251], lr: 0.000270, loss: 0.4016
2022-10-03 22:33:55 - train: epoch 0058, iter [01150, 01251], lr: 0.000269, loss: 0.4014
2022-10-03 22:34:14 - train: epoch 0058, iter [01160, 01251], lr: 0.000269, loss: 0.3993
2022-10-03 22:34:33 - train: epoch 0058, iter [01170, 01251], lr: 0.000269, loss: 0.4048
2022-10-03 22:34:51 - train: epoch 0058, iter [01180, 01251], lr: 0.000269, loss: 0.3941
2022-10-03 22:35:10 - train: epoch 0058, iter [01190, 01251], lr: 0.000269, loss: 0.4108
2022-10-03 22:35:29 - train: epoch 0058, iter [01200, 01251], lr: 0.000269, loss: 0.4082
2022-10-03 22:35:48 - train: epoch 0058, iter [01210, 01251], lr: 0.000269, loss: 0.4115
2022-10-03 22:36:06 - train: epoch 0058, iter [01220, 01251], lr: 0.000269, loss: 0.3932
2022-10-03 22:36:25 - train: epoch 0058, iter [01230, 01251], lr: 0.000269, loss: 0.4071
2022-10-03 22:36:44 - train: epoch 0058, iter [01240, 01251], lr: 0.000269, loss: 0.4090
2022-10-03 22:37:02 - train: epoch 0058, iter [01250, 01251], lr: 0.000269, loss: 0.3972
2022-10-03 22:37:07 - train: epoch 058, train_loss: 0.4082
2022-10-03 22:37:12 - until epoch: 058, best_loss: 0.4082
2022-10-03 22:37:12 - epoch 059 lr: 0.000269
2022-10-03 22:37:36 - train: epoch 0059, iter [00010, 01251], lr: 0.000269, loss: 0.4064
2022-10-03 22:37:55 - train: epoch 0059, iter [00020, 01251], lr: 0.000268, loss: 0.4105
2022-10-03 22:38:14 - train: epoch 0059, iter [00030, 01251], lr: 0.000268, loss: 0.3683
2022-10-03 22:38:33 - train: epoch 0059, iter [00040, 01251], lr: 0.000268, loss: 0.3983
2022-10-03 22:38:51 - train: epoch 0059, iter [00050, 01251], lr: 0.000268, loss: 0.4154
2022-10-03 22:39:10 - train: epoch 0059, iter [00060, 01251], lr: 0.000268, loss: 0.4321
2022-10-03 22:39:30 - train: epoch 0059, iter [00070, 01251], lr: 0.000268, loss: 0.4072
2022-10-03 22:39:49 - train: epoch 0059, iter [00080, 01251], lr: 0.000268, loss: 0.4131
2022-10-03 22:40:08 - train: epoch 0059, iter [00090, 01251], lr: 0.000268, loss: 0.4019
2022-10-03 22:40:27 - train: epoch 0059, iter [00100, 01251], lr: 0.000268, loss: 0.3914
2022-10-03 22:40:45 - train: epoch 0059, iter [00110, 01251], lr: 0.000268, loss: 0.3983
2022-10-03 22:41:04 - train: epoch 0059, iter [00120, 01251], lr: 0.000268, loss: 0.3935
2022-10-03 22:41:23 - train: epoch 0059, iter [00130, 01251], lr: 0.000268, loss: 0.4153
2022-10-03 22:41:41 - train: epoch 0059, iter [00140, 01251], lr: 0.000267, loss: 0.4307
2022-10-03 22:42:00 - train: epoch 0059, iter [00150, 01251], lr: 0.000267, loss: 0.3985
2022-10-03 22:42:18 - train: epoch 0059, iter [00160, 01251], lr: 0.000267, loss: 0.4198
2022-10-03 22:42:37 - train: epoch 0059, iter [00170, 01251], lr: 0.000267, loss: 0.4248
2022-10-03 22:42:56 - train: epoch 0059, iter [00180, 01251], lr: 0.000267, loss: 0.3951
2022-10-03 22:43:15 - train: epoch 0059, iter [00190, 01251], lr: 0.000267, loss: 0.3987
2022-10-03 22:43:34 - train: epoch 0059, iter [00200, 01251], lr: 0.000267, loss: 0.4177
2022-10-03 22:43:52 - train: epoch 0059, iter [00210, 01251], lr: 0.000267, loss: 0.4075
2022-10-03 22:44:10 - train: epoch 0059, iter [00220, 01251], lr: 0.000267, loss: 0.4037
2022-10-03 22:44:29 - train: epoch 0059, iter [00230, 01251], lr: 0.000267, loss: 0.4137
2022-10-03 22:44:48 - train: epoch 0059, iter [00240, 01251], lr: 0.000267, loss: 0.4168
2022-10-03 22:45:07 - train: epoch 0059, iter [00250, 01251], lr: 0.000267, loss: 0.4075
2022-10-03 22:45:26 - train: epoch 0059, iter [00260, 01251], lr: 0.000266, loss: 0.4036
2022-10-03 22:45:46 - train: epoch 0059, iter [00270, 01251], lr: 0.000266, loss: 0.3932
2022-10-03 22:46:05 - train: epoch 0059, iter [00280, 01251], lr: 0.000266, loss: 0.4145
2022-10-03 22:46:24 - train: epoch 0059, iter [00290, 01251], lr: 0.000266, loss: 0.4002
2022-10-03 22:46:43 - train: epoch 0059, iter [00300, 01251], lr: 0.000266, loss: 0.4101
2022-10-03 22:47:01 - train: epoch 0059, iter [00310, 01251], lr: 0.000266, loss: 0.4174
2022-10-03 22:47:20 - train: epoch 0059, iter [00320, 01251], lr: 0.000266, loss: 0.3796
2022-10-03 22:47:39 - train: epoch 0059, iter [00330, 01251], lr: 0.000266, loss: 0.3979
2022-10-03 22:47:57 - train: epoch 0059, iter [00340, 01251], lr: 0.000266, loss: 0.3942
2022-10-03 22:48:16 - train: epoch 0059, iter [00350, 01251], lr: 0.000266, loss: 0.3953
2022-10-03 22:48:35 - train: epoch 0059, iter [00360, 01251], lr: 0.000266, loss: 0.4246
2022-10-03 22:48:54 - train: epoch 0059, iter [00370, 01251], lr: 0.000266, loss: 0.4144
2022-10-03 22:49:13 - train: epoch 0059, iter [00380, 01251], lr: 0.000265, loss: 0.4212
2022-10-03 22:49:31 - train: epoch 0059, iter [00390, 01251], lr: 0.000265, loss: 0.4301
2022-10-03 22:49:50 - train: epoch 0059, iter [00400, 01251], lr: 0.000265, loss: 0.4040
2022-10-03 22:50:09 - train: epoch 0059, iter [00410, 01251], lr: 0.000265, loss: 0.4147
2022-10-03 22:50:28 - train: epoch 0059, iter [00420, 01251], lr: 0.000265, loss: 0.3990
2022-10-03 22:50:47 - train: epoch 0059, iter [00430, 01251], lr: 0.000265, loss: 0.4077
2022-10-03 22:51:06 - train: epoch 0059, iter [00440, 01251], lr: 0.000265, loss: 0.4072
2022-10-03 22:51:24 - train: epoch 0059, iter [00450, 01251], lr: 0.000265, loss: 0.4131
2022-10-03 22:51:43 - train: epoch 0059, iter [00460, 01251], lr: 0.000265, loss: 0.4024
2022-10-03 22:52:02 - train: epoch 0059, iter [00470, 01251], lr: 0.000265, loss: 0.4031
2022-10-03 22:52:21 - train: epoch 0059, iter [00480, 01251], lr: 0.000265, loss: 0.4233
2022-10-03 22:52:39 - train: epoch 0059, iter [00490, 01251], lr: 0.000265, loss: 0.3972
2022-10-03 22:52:58 - train: epoch 0059, iter [00500, 01251], lr: 0.000264, loss: 0.4109
2022-10-03 22:53:17 - train: epoch 0059, iter [00510, 01251], lr: 0.000264, loss: 0.4011
2022-10-03 22:53:36 - train: epoch 0059, iter [00520, 01251], lr: 0.000264, loss: 0.4155
2022-10-03 22:53:55 - train: epoch 0059, iter [00530, 01251], lr: 0.000264, loss: 0.4064
2022-10-03 22:54:14 - train: epoch 0059, iter [00540, 01251], lr: 0.000264, loss: 0.4152
2022-10-03 22:54:33 - train: epoch 0059, iter [00550, 01251], lr: 0.000264, loss: 0.4144
2022-10-03 22:54:51 - train: epoch 0059, iter [00560, 01251], lr: 0.000264, loss: 0.4167
2022-10-03 22:55:10 - train: epoch 0059, iter [00570, 01251], lr: 0.000264, loss: 0.3923
2022-10-03 22:55:29 - train: epoch 0059, iter [00580, 01251], lr: 0.000264, loss: 0.4039
2022-10-03 22:55:49 - train: epoch 0059, iter [00590, 01251], lr: 0.000264, loss: 0.4057
2022-10-03 22:56:07 - train: epoch 0059, iter [00600, 01251], lr: 0.000264, loss: 0.4174
2022-10-03 22:56:26 - train: epoch 0059, iter [00610, 01251], lr: 0.000264, loss: 0.3995
2022-10-03 22:56:45 - train: epoch 0059, iter [00620, 01251], lr: 0.000263, loss: 0.4112
2022-10-03 22:57:04 - train: epoch 0059, iter [00630, 01251], lr: 0.000263, loss: 0.4012
2022-10-03 22:57:22 - train: epoch 0059, iter [00640, 01251], lr: 0.000263, loss: 0.4098
2022-10-03 22:57:41 - train: epoch 0059, iter [00650, 01251], lr: 0.000263, loss: 0.4064
2022-10-03 22:58:00 - train: epoch 0059, iter [00660, 01251], lr: 0.000263, loss: 0.4092
2022-10-03 22:58:19 - train: epoch 0059, iter [00670, 01251], lr: 0.000263, loss: 0.3893
2022-10-03 22:58:38 - train: epoch 0059, iter [00680, 01251], lr: 0.000263, loss: 0.3991
2022-10-03 22:58:57 - train: epoch 0059, iter [00690, 01251], lr: 0.000263, loss: 0.4098
2022-10-03 22:59:16 - train: epoch 0059, iter [00700, 01251], lr: 0.000263, loss: 0.4018
2022-10-03 22:59:35 - train: epoch 0059, iter [00710, 01251], lr: 0.000263, loss: 0.4085
2022-10-03 22:59:54 - train: epoch 0059, iter [00720, 01251], lr: 0.000263, loss: 0.4222
2022-10-03 23:00:13 - train: epoch 0059, iter [00730, 01251], lr: 0.000263, loss: 0.4086
2022-10-03 23:00:31 - train: epoch 0059, iter [00740, 01251], lr: 0.000262, loss: 0.4036
2022-10-03 23:00:50 - train: epoch 0059, iter [00750, 01251], lr: 0.000262, loss: 0.4329
2022-10-03 23:01:08 - train: epoch 0059, iter [00760, 01251], lr: 0.000262, loss: 0.4206
2022-10-03 23:01:27 - train: epoch 0059, iter [00770, 01251], lr: 0.000262, loss: 0.4214
2022-10-03 23:01:46 - train: epoch 0059, iter [00780, 01251], lr: 0.000262, loss: 0.4145
2022-10-03 23:02:05 - train: epoch 0059, iter [00790, 01251], lr: 0.000262, loss: 0.4072
2022-10-03 23:02:23 - train: epoch 0059, iter [00800, 01251], lr: 0.000262, loss: 0.4067
2022-10-03 23:02:43 - train: epoch 0059, iter [00810, 01251], lr: 0.000262, loss: 0.3991
2022-10-03 23:03:02 - train: epoch 0059, iter [00820, 01251], lr: 0.000262, loss: 0.3939
2022-10-03 23:03:20 - train: epoch 0059, iter [00830, 01251], lr: 0.000262, loss: 0.4148
2022-10-03 23:03:39 - train: epoch 0059, iter [00840, 01251], lr: 0.000262, loss: 0.4149
2022-10-03 23:03:58 - train: epoch 0059, iter [00850, 01251], lr: 0.000262, loss: 0.4158
2022-10-03 23:04:16 - train: epoch 0059, iter [00860, 01251], lr: 0.000261, loss: 0.4245
2022-10-03 23:04:35 - train: epoch 0059, iter [00870, 01251], lr: 0.000261, loss: 0.4162
2022-10-03 23:04:54 - train: epoch 0059, iter [00880, 01251], lr: 0.000261, loss: 0.4060
2022-10-03 23:05:13 - train: epoch 0059, iter [00890, 01251], lr: 0.000261, loss: 0.4032
2022-10-03 23:05:32 - train: epoch 0059, iter [00900, 01251], lr: 0.000261, loss: 0.3928
2022-10-03 23:05:51 - train: epoch 0059, iter [00910, 01251], lr: 0.000261, loss: 0.4207
2022-10-03 23:06:09 - train: epoch 0059, iter [00920, 01251], lr: 0.000261, loss: 0.4250
2022-10-03 23:06:28 - train: epoch 0059, iter [00930, 01251], lr: 0.000261, loss: 0.4013
2022-10-03 23:06:47 - train: epoch 0059, iter [00940, 01251], lr: 0.000261, loss: 0.4136
2022-10-03 23:07:06 - train: epoch 0059, iter [00950, 01251], lr: 0.000261, loss: 0.3919
2022-10-03 23:07:25 - train: epoch 0059, iter [00960, 01251], lr: 0.000261, loss: 0.3972
2022-10-03 23:07:45 - train: epoch 0059, iter [00970, 01251], lr: 0.000261, loss: 0.4001
2022-10-03 23:08:04 - train: epoch 0059, iter [00980, 01251], lr: 0.000260, loss: 0.3998
2022-10-03 23:08:23 - train: epoch 0059, iter [00990, 01251], lr: 0.000260, loss: 0.3991
2022-10-03 23:08:41 - train: epoch 0059, iter [01000, 01251], lr: 0.000260, loss: 0.4153
2022-10-03 23:09:00 - train: epoch 0059, iter [01010, 01251], lr: 0.000260, loss: 0.4233
2022-10-03 23:09:19 - train: epoch 0059, iter [01020, 01251], lr: 0.000260, loss: 0.4241
2022-10-03 23:09:38 - train: epoch 0059, iter [01030, 01251], lr: 0.000260, loss: 0.3996
2022-10-03 23:09:57 - train: epoch 0059, iter [01040, 01251], lr: 0.000260, loss: 0.4418
2022-10-03 23:10:17 - train: epoch 0059, iter [01050, 01251], lr: 0.000260, loss: 0.4087
2022-10-03 23:10:35 - train: epoch 0059, iter [01060, 01251], lr: 0.000260, loss: 0.4110
2022-10-03 23:10:54 - train: epoch 0059, iter [01070, 01251], lr: 0.000260, loss: 0.4035
2022-10-03 23:11:13 - train: epoch 0059, iter [01080, 01251], lr: 0.000260, loss: 0.3953
2022-10-03 23:11:33 - train: epoch 0059, iter [01090, 01251], lr: 0.000260, loss: 0.4121
2022-10-03 23:11:51 - train: epoch 0059, iter [01100, 01251], lr: 0.000260, loss: 0.4137
2022-10-03 23:12:10 - train: epoch 0059, iter [01110, 01251], lr: 0.000259, loss: 0.4100
2022-10-03 23:12:28 - train: epoch 0059, iter [01120, 01251], lr: 0.000259, loss: 0.4146
2022-10-03 23:12:47 - train: epoch 0059, iter [01130, 01251], lr: 0.000259, loss: 0.4103
2022-10-03 23:13:06 - train: epoch 0059, iter [01140, 01251], lr: 0.000259, loss: 0.3896
2022-10-03 23:13:24 - train: epoch 0059, iter [01150, 01251], lr: 0.000259, loss: 0.4244
2022-10-03 23:13:43 - train: epoch 0059, iter [01160, 01251], lr: 0.000259, loss: 0.4128
2022-10-03 23:14:02 - train: epoch 0059, iter [01170, 01251], lr: 0.000259, loss: 0.4125
2022-10-03 23:14:20 - train: epoch 0059, iter [01180, 01251], lr: 0.000259, loss: 0.4151
2022-10-03 23:14:39 - train: epoch 0059, iter [01190, 01251], lr: 0.000259, loss: 0.3859
2022-10-03 23:14:58 - train: epoch 0059, iter [01200, 01251], lr: 0.000259, loss: 0.4034
2022-10-03 23:15:17 - train: epoch 0059, iter [01210, 01251], lr: 0.000259, loss: 0.4108
2022-10-03 23:15:35 - train: epoch 0059, iter [01220, 01251], lr: 0.000259, loss: 0.4218
2022-10-03 23:15:53 - train: epoch 0059, iter [01230, 01251], lr: 0.000258, loss: 0.4132
2022-10-03 23:16:12 - train: epoch 0059, iter [01240, 01251], lr: 0.000258, loss: 0.4119
2022-10-03 23:16:30 - train: epoch 0059, iter [01250, 01251], lr: 0.000258, loss: 0.3935
2022-10-03 23:16:35 - train: epoch 059, train_loss: 0.4079
2022-10-03 23:16:40 - until epoch: 059, best_loss: 0.4079
2022-10-03 23:16:40 - epoch 060 lr: 0.000258
2022-10-03 23:17:04 - train: epoch 0060, iter [00010, 01251], lr: 0.000258, loss: 0.4198
2022-10-03 23:17:23 - train: epoch 0060, iter [00020, 01251], lr: 0.000258, loss: 0.4015
2022-10-03 23:17:42 - train: epoch 0060, iter [00030, 01251], lr: 0.000258, loss: 0.4157
2022-10-03 23:18:00 - train: epoch 0060, iter [00040, 01251], lr: 0.000258, loss: 0.4028
2022-10-03 23:18:19 - train: epoch 0060, iter [00050, 01251], lr: 0.000258, loss: 0.4081
2022-10-03 23:18:38 - train: epoch 0060, iter [00060, 01251], lr: 0.000258, loss: 0.4253
2022-10-03 23:18:57 - train: epoch 0060, iter [00070, 01251], lr: 0.000258, loss: 0.4147
2022-10-03 23:19:16 - train: epoch 0060, iter [00080, 01251], lr: 0.000258, loss: 0.4227
2022-10-03 23:19:34 - train: epoch 0060, iter [00090, 01251], lr: 0.000258, loss: 0.4007
2022-10-03 23:19:53 - train: epoch 0060, iter [00100, 01251], lr: 0.000257, loss: 0.4301
2022-10-03 23:20:12 - train: epoch 0060, iter [00110, 01251], lr: 0.000257, loss: 0.3951
2022-10-03 23:20:31 - train: epoch 0060, iter [00120, 01251], lr: 0.000257, loss: 0.3978
2022-10-03 23:20:50 - train: epoch 0060, iter [00130, 01251], lr: 0.000257, loss: 0.4274
2022-10-03 23:21:09 - train: epoch 0060, iter [00140, 01251], lr: 0.000257, loss: 0.4063
2022-10-03 23:21:27 - train: epoch 0060, iter [00150, 01251], lr: 0.000257, loss: 0.4160
2022-10-03 23:21:46 - train: epoch 0060, iter [00160, 01251], lr: 0.000257, loss: 0.4157
2022-10-03 23:22:05 - train: epoch 0060, iter [00170, 01251], lr: 0.000257, loss: 0.3906
2022-10-03 23:22:24 - train: epoch 0060, iter [00180, 01251], lr: 0.000257, loss: 0.4037
2022-10-03 23:22:43 - train: epoch 0060, iter [00190, 01251], lr: 0.000257, loss: 0.3930
2022-10-03 23:23:02 - train: epoch 0060, iter [00200, 01251], lr: 0.000257, loss: 0.3946
2022-10-03 23:23:21 - train: epoch 0060, iter [00210, 01251], lr: 0.000257, loss: 0.3828
2022-10-03 23:23:39 - train: epoch 0060, iter [00220, 01251], lr: 0.000256, loss: 0.4102
2022-10-03 23:23:59 - train: epoch 0060, iter [00230, 01251], lr: 0.000256, loss: 0.3925
2022-10-03 23:24:18 - train: epoch 0060, iter [00240, 01251], lr: 0.000256, loss: 0.4096
2022-10-03 23:24:36 - train: epoch 0060, iter [00250, 01251], lr: 0.000256, loss: 0.4109
2022-10-03 23:24:55 - train: epoch 0060, iter [00260, 01251], lr: 0.000256, loss: 0.4275
2022-10-03 23:25:14 - train: epoch 0060, iter [00270, 01251], lr: 0.000256, loss: 0.4095
2022-10-03 23:25:33 - train: epoch 0060, iter [00280, 01251], lr: 0.000256, loss: 0.3791
2022-10-03 23:25:52 - train: epoch 0060, iter [00290, 01251], lr: 0.000256, loss: 0.4133
2022-10-03 23:26:11 - train: epoch 0060, iter [00300, 01251], lr: 0.000256, loss: 0.4169
2022-10-03 23:26:30 - train: epoch 0060, iter [00310, 01251], lr: 0.000256, loss: 0.4257
2022-10-03 23:26:48 - train: epoch 0060, iter [00320, 01251], lr: 0.000256, loss: 0.4088
2022-10-03 23:27:07 - train: epoch 0060, iter [00330, 01251], lr: 0.000256, loss: 0.3881
2022-10-03 23:27:26 - train: epoch 0060, iter [00340, 01251], lr: 0.000255, loss: 0.4047
2022-10-03 23:27:45 - train: epoch 0060, iter [00350, 01251], lr: 0.000255, loss: 0.4009
2022-10-03 23:28:04 - train: epoch 0060, iter [00360, 01251], lr: 0.000255, loss: 0.4158
2022-10-03 23:28:23 - train: epoch 0060, iter [00370, 01251], lr: 0.000255, loss: 0.4003
2022-10-03 23:28:42 - train: epoch 0060, iter [00380, 01251], lr: 0.000255, loss: 0.3969
2022-10-03 23:29:00 - train: epoch 0060, iter [00390, 01251], lr: 0.000255, loss: 0.4228
2022-10-03 23:29:19 - train: epoch 0060, iter [00400, 01251], lr: 0.000255, loss: 0.4024
2022-10-03 23:29:38 - train: epoch 0060, iter [00410, 01251], lr: 0.000255, loss: 0.4197
2022-10-03 23:29:58 - train: epoch 0060, iter [00420, 01251], lr: 0.000255, loss: 0.4071
2022-10-03 23:30:17 - train: epoch 0060, iter [00430, 01251], lr: 0.000255, loss: 0.4115
2022-10-03 23:30:36 - train: epoch 0060, iter [00440, 01251], lr: 0.000255, loss: 0.4130
2022-10-03 23:30:55 - train: epoch 0060, iter [00450, 01251], lr: 0.000255, loss: 0.3951
2022-10-03 23:31:14 - train: epoch 0060, iter [00460, 01251], lr: 0.000254, loss: 0.4074
2022-10-03 23:31:33 - train: epoch 0060, iter [00470, 01251], lr: 0.000254, loss: 0.3902
2022-10-03 23:31:52 - train: epoch 0060, iter [00480, 01251], lr: 0.000254, loss: 0.4362
2022-10-03 23:32:11 - train: epoch 0060, iter [00490, 01251], lr: 0.000254, loss: 0.4112
2022-10-03 23:32:30 - train: epoch 0060, iter [00500, 01251], lr: 0.000254, loss: 0.4016
2022-10-03 23:32:48 - train: epoch 0060, iter [00510, 01251], lr: 0.000254, loss: 0.4365
2022-10-03 23:33:07 - train: epoch 0060, iter [00520, 01251], lr: 0.000254, loss: 0.4095
2022-10-03 23:33:26 - train: epoch 0060, iter [00530, 01251], lr: 0.000254, loss: 0.4165
2022-10-03 23:33:45 - train: epoch 0060, iter [00540, 01251], lr: 0.000254, loss: 0.4154
2022-10-03 23:34:04 - train: epoch 0060, iter [00550, 01251], lr: 0.000254, loss: 0.4142
2022-10-03 23:34:23 - train: epoch 0060, iter [00560, 01251], lr: 0.000254, loss: 0.4047
2022-10-03 23:34:42 - train: epoch 0060, iter [00570, 01251], lr: 0.000254, loss: 0.4148
2022-10-03 23:35:00 - train: epoch 0060, iter [00580, 01251], lr: 0.000253, loss: 0.4045
2022-10-03 23:35:19 - train: epoch 0060, iter [00590, 01251], lr: 0.000253, loss: 0.4073
2022-10-03 23:35:38 - train: epoch 0060, iter [00600, 01251], lr: 0.000253, loss: 0.4072
2022-10-03 23:35:57 - train: epoch 0060, iter [00610, 01251], lr: 0.000253, loss: 0.4288
2022-10-03 23:36:15 - train: epoch 0060, iter [00620, 01251], lr: 0.000253, loss: 0.4025
2022-10-03 23:36:34 - train: epoch 0060, iter [00630, 01251], lr: 0.000253, loss: 0.4009
2022-10-03 23:36:53 - train: epoch 0060, iter [00640, 01251], lr: 0.000253, loss: 0.4175
2022-10-03 23:37:12 - train: epoch 0060, iter [00650, 01251], lr: 0.000253, loss: 0.4000
2022-10-03 23:37:31 - train: epoch 0060, iter [00660, 01251], lr: 0.000253, loss: 0.4159
2022-10-03 23:37:50 - train: epoch 0060, iter [00670, 01251], lr: 0.000253, loss: 0.4274
2022-10-03 23:38:09 - train: epoch 0060, iter [00680, 01251], lr: 0.000253, loss: 0.4099
2022-10-03 23:38:28 - train: epoch 0060, iter [00690, 01251], lr: 0.000253, loss: 0.4098
2022-10-03 23:38:47 - train: epoch 0060, iter [00700, 01251], lr: 0.000252, loss: 0.3927
2022-10-03 23:39:05 - train: epoch 0060, iter [00710, 01251], lr: 0.000252, loss: 0.4222
2022-10-03 23:39:25 - train: epoch 0060, iter [00720, 01251], lr: 0.000252, loss: 0.4175
2022-10-03 23:39:44 - train: epoch 0060, iter [00730, 01251], lr: 0.000252, loss: 0.3983
2022-10-03 23:40:02 - train: epoch 0060, iter [00740, 01251], lr: 0.000252, loss: 0.4001
2022-10-03 23:40:21 - train: epoch 0060, iter [00750, 01251], lr: 0.000252, loss: 0.4049
2022-10-03 23:40:40 - train: epoch 0060, iter [00760, 01251], lr: 0.000252, loss: 0.4248
2022-10-03 23:40:59 - train: epoch 0060, iter [00770, 01251], lr: 0.000252, loss: 0.4215
2022-10-03 23:41:17 - train: epoch 0060, iter [00780, 01251], lr: 0.000252, loss: 0.3860
2022-10-03 23:41:36 - train: epoch 0060, iter [00790, 01251], lr: 0.000252, loss: 0.4022
2022-10-03 23:41:55 - train: epoch 0060, iter [00800, 01251], lr: 0.000252, loss: 0.4062
2022-10-03 23:42:14 - train: epoch 0060, iter [00810, 01251], lr: 0.000252, loss: 0.4230
2022-10-03 23:42:33 - train: epoch 0060, iter [00820, 01251], lr: 0.000251, loss: 0.3952
2022-10-03 23:42:52 - train: epoch 0060, iter [00830, 01251], lr: 0.000251, loss: 0.4214
2022-10-03 23:43:11 - train: epoch 0060, iter [00840, 01251], lr: 0.000251, loss: 0.3994
2022-10-03 23:43:30 - train: epoch 0060, iter [00850, 01251], lr: 0.000251, loss: 0.3958
2022-10-03 23:43:49 - train: epoch 0060, iter [00860, 01251], lr: 0.000251, loss: 0.4042
2022-10-03 23:44:07 - train: epoch 0060, iter [00870, 01251], lr: 0.000251, loss: 0.4153
2022-10-03 23:44:27 - train: epoch 0060, iter [00880, 01251], lr: 0.000251, loss: 0.4078
2022-10-03 23:44:45 - train: epoch 0060, iter [00890, 01251], lr: 0.000251, loss: 0.4050
2022-10-03 23:45:04 - train: epoch 0060, iter [00900, 01251], lr: 0.000251, loss: 0.4113
2022-10-03 23:45:23 - train: epoch 0060, iter [00910, 01251], lr: 0.000251, loss: 0.4365
2022-10-03 23:45:42 - train: epoch 0060, iter [00920, 01251], lr: 0.000251, loss: 0.3998
2022-10-03 23:46:00 - train: epoch 0060, iter [00930, 01251], lr: 0.000251, loss: 0.4003
2022-10-03 23:46:19 - train: epoch 0060, iter [00940, 01251], lr: 0.000250, loss: 0.4114
2022-10-03 23:46:38 - train: epoch 0060, iter [00950, 01251], lr: 0.000250, loss: 0.4183
2022-10-03 23:46:57 - train: epoch 0060, iter [00960, 01251], lr: 0.000250, loss: 0.3933
2022-10-03 23:47:16 - train: epoch 0060, iter [00970, 01251], lr: 0.000250, loss: 0.4017
2022-10-03 23:47:34 - train: epoch 0060, iter [00980, 01251], lr: 0.000250, loss: 0.4069
2022-10-03 23:47:53 - train: epoch 0060, iter [00990, 01251], lr: 0.000250, loss: 0.4146
2022-10-03 23:48:12 - train: epoch 0060, iter [01000, 01251], lr: 0.000250, loss: 0.4236
2022-10-03 23:48:31 - train: epoch 0060, iter [01010, 01251], lr: 0.000250, loss: 0.3936
2022-10-03 23:48:50 - train: epoch 0060, iter [01020, 01251], lr: 0.000250, loss: 0.4178
2022-10-03 23:49:09 - train: epoch 0060, iter [01030, 01251], lr: 0.000250, loss: 0.4030
2022-10-03 23:49:27 - train: epoch 0060, iter [01040, 01251], lr: 0.000250, loss: 0.3950
2022-10-03 23:49:46 - train: epoch 0060, iter [01050, 01251], lr: 0.000250, loss: 0.4055
2022-10-03 23:50:05 - train: epoch 0060, iter [01060, 01251], lr: 0.000249, loss: 0.3975
2022-10-03 23:50:24 - train: epoch 0060, iter [01070, 01251], lr: 0.000249, loss: 0.4205
2022-10-03 23:50:43 - train: epoch 0060, iter [01080, 01251], lr: 0.000249, loss: 0.4084
2022-10-03 23:51:02 - train: epoch 0060, iter [01090, 01251], lr: 0.000249, loss: 0.3897
2022-10-03 23:51:21 - train: epoch 0060, iter [01100, 01251], lr: 0.000249, loss: 0.4064
2022-10-03 23:51:39 - train: epoch 0060, iter [01110, 01251], lr: 0.000249, loss: 0.3956
2022-10-03 23:51:58 - train: epoch 0060, iter [01120, 01251], lr: 0.000249, loss: 0.3946
2022-10-03 23:52:17 - train: epoch 0060, iter [01130, 01251], lr: 0.000249, loss: 0.4074
2022-10-03 23:52:36 - train: epoch 0060, iter [01140, 01251], lr: 0.000249, loss: 0.4225
2022-10-03 23:52:54 - train: epoch 0060, iter [01150, 01251], lr: 0.000249, loss: 0.4142
2022-10-03 23:53:13 - train: epoch 0060, iter [01160, 01251], lr: 0.000249, loss: 0.4298
2022-10-03 23:53:32 - train: epoch 0060, iter [01170, 01251], lr: 0.000249, loss: 0.4128
2022-10-03 23:53:51 - train: epoch 0060, iter [01180, 01251], lr: 0.000248, loss: 0.4142
2022-10-03 23:54:09 - train: epoch 0060, iter [01190, 01251], lr: 0.000248, loss: 0.3983
2022-10-03 23:54:28 - train: epoch 0060, iter [01200, 01251], lr: 0.000248, loss: 0.4058
2022-10-03 23:54:47 - train: epoch 0060, iter [01210, 01251], lr: 0.000248, loss: 0.3986
2022-10-03 23:55:06 - train: epoch 0060, iter [01220, 01251], lr: 0.000248, loss: 0.4046
2022-10-03 23:55:25 - train: epoch 0060, iter [01230, 01251], lr: 0.000248, loss: 0.4049
2022-10-03 23:55:44 - train: epoch 0060, iter [01240, 01251], lr: 0.000248, loss: 0.4075
2022-10-03 23:56:02 - train: epoch 0060, iter [01250, 01251], lr: 0.000248, loss: 0.3915
2022-10-03 23:56:07 - train: epoch 060, train_loss: 0.4076
2022-10-03 23:56:12 - until epoch: 060, best_loss: 0.4076
2022-10-03 23:56:12 - epoch 061 lr: 0.000248
2022-10-03 23:56:37 - train: epoch 0061, iter [00010, 01251], lr: 0.000248, loss: 0.3971
2022-10-03 23:56:55 - train: epoch 0061, iter [00020, 01251], lr: 0.000248, loss: 0.3955
2022-10-03 23:57:14 - train: epoch 0061, iter [00030, 01251], lr: 0.000248, loss: 0.3944
2022-10-03 23:57:33 - train: epoch 0061, iter [00040, 01251], lr: 0.000248, loss: 0.3993
2022-10-03 23:57:52 - train: epoch 0061, iter [00050, 01251], lr: 0.000247, loss: 0.4100
2022-10-03 23:58:10 - train: epoch 0061, iter [00060, 01251], lr: 0.000247, loss: 0.4063
2022-10-03 23:58:29 - train: epoch 0061, iter [00070, 01251], lr: 0.000247, loss: 0.4155
2022-10-03 23:58:48 - train: epoch 0061, iter [00080, 01251], lr: 0.000247, loss: 0.4004
2022-10-03 23:59:07 - train: epoch 0061, iter [00090, 01251], lr: 0.000247, loss: 0.4352
2022-10-03 23:59:26 - train: epoch 0061, iter [00100, 01251], lr: 0.000247, loss: 0.4107
2022-10-03 23:59:45 - train: epoch 0061, iter [00110, 01251], lr: 0.000247, loss: 0.4237
