2022-10-11 00:00:16 - train: epoch 0022, iter [01900, 05004], lr: 0.089140, loss: 4.1299
2022-10-11 00:01:08 - train: epoch 0022, iter [02000, 05004], lr: 0.089121, loss: 4.4641
2022-10-11 00:01:58 - train: epoch 0022, iter [02100, 05004], lr: 0.089101, loss: 5.1150
2022-10-11 00:02:47 - train: epoch 0022, iter [02200, 05004], lr: 0.089082, loss: 3.9921
2022-10-11 00:03:36 - train: epoch 0022, iter [02300, 05004], lr: 0.089062, loss: 4.8941
2022-10-11 00:04:26 - train: epoch 0022, iter [02400, 05004], lr: 0.089043, loss: 4.6373
2022-10-11 00:05:17 - train: epoch 0022, iter [02500, 05004], lr: 0.089023, loss: 3.7484
2022-10-11 00:06:05 - train: epoch 0022, iter [02600, 05004], lr: 0.089003, loss: 4.1498
2022-10-11 00:06:56 - train: epoch 0022, iter [02700, 05004], lr: 0.088984, loss: 4.6871
2022-10-11 00:07:45 - train: epoch 0022, iter [02800, 05004], lr: 0.088964, loss: 4.8634
2022-10-11 00:08:35 - train: epoch 0022, iter [02900, 05004], lr: 0.088944, loss: 3.7627
2022-10-11 00:09:26 - train: epoch 0022, iter [03000, 05004], lr: 0.088925, loss: 4.1037
2022-10-11 00:10:17 - train: epoch 0022, iter [03100, 05004], lr: 0.088905, loss: 4.0746
2022-10-11 00:11:06 - train: epoch 0022, iter [03200, 05004], lr: 0.088885, loss: 3.9399
2022-10-11 00:11:54 - train: epoch 0022, iter [03300, 05004], lr: 0.088866, loss: 4.3292
2022-10-11 00:12:45 - train: epoch 0022, iter [03400, 05004], lr: 0.088846, loss: 4.9551
2022-10-11 00:13:35 - train: epoch 0022, iter [03500, 05004], lr: 0.088826, loss: 4.6159
2022-10-11 00:14:26 - train: epoch 0022, iter [03600, 05004], lr: 0.088806, loss: 4.4127
2022-10-11 00:15:16 - train: epoch 0022, iter [03700, 05004], lr: 0.088786, loss: 5.2461
2022-10-11 00:16:06 - train: epoch 0022, iter [03800, 05004], lr: 0.088767, loss: 4.7031
2022-10-11 00:16:54 - train: epoch 0022, iter [03900, 05004], lr: 0.088747, loss: 4.1542
2022-10-11 00:17:44 - train: epoch 0022, iter [04000, 05004], lr: 0.088727, loss: 4.5840
2022-10-11 00:18:33 - train: epoch 0022, iter [04100, 05004], lr: 0.088707, loss: 4.1714
2022-10-11 00:19:23 - train: epoch 0022, iter [04200, 05004], lr: 0.088687, loss: 4.6516
2022-10-11 00:20:14 - train: epoch 0022, iter [04300, 05004], lr: 0.088667, loss: 4.9991
2022-10-11 00:21:02 - train: epoch 0022, iter [04400, 05004], lr: 0.088647, loss: 3.9821
2022-10-11 00:21:51 - train: epoch 0022, iter [04500, 05004], lr: 0.088627, loss: 4.7429
2022-10-11 00:22:42 - train: epoch 0022, iter [04600, 05004], lr: 0.088608, loss: 5.1181
2022-10-11 00:23:32 - train: epoch 0022, iter [04700, 05004], lr: 0.088588, loss: 4.5443
2022-10-11 00:24:20 - train: epoch 0022, iter [04800, 05004], lr: 0.088568, loss: 4.4106
2022-10-11 00:25:10 - train: epoch 0022, iter [04900, 05004], lr: 0.088548, loss: 4.9211
2022-10-11 00:25:57 - train: epoch 0022, iter [05000, 05004], lr: 0.088528, loss: 4.4199
2022-10-11 00:25:59 - train: epoch 022, train_loss: 4.5222
2022-10-11 00:27:44 - eval: epoch: 022, acc1: 54.280%, acc5: 79.532%, test_loss: 2.1845, per_image_load_time: 3.118ms, per_image_inference_time: 0.514ms
2022-10-11 00:27:44 - until epoch: 022, best_acc1: 54.286%
2022-10-11 00:27:44 - epoch 023 lr: 0.088527
2022-10-11 00:28:41 - train: epoch 0023, iter [00100, 05004], lr: 0.088507, loss: 4.3786
2022-10-11 00:29:30 - train: epoch 0023, iter [00200, 05004], lr: 0.088487, loss: 4.6477
2022-10-11 00:30:20 - train: epoch 0023, iter [00300, 05004], lr: 0.088467, loss: 4.9509
2022-10-11 00:31:10 - train: epoch 0023, iter [00400, 05004], lr: 0.088447, loss: 5.1295
2022-10-11 00:31:59 - train: epoch 0023, iter [00500, 05004], lr: 0.088427, loss: 5.0100
2022-10-11 00:32:49 - train: epoch 0023, iter [00600, 05004], lr: 0.088406, loss: 4.7284
2022-10-11 00:33:38 - train: epoch 0023, iter [00700, 05004], lr: 0.088386, loss: 4.9663
2022-10-11 00:34:28 - train: epoch 0023, iter [00800, 05004], lr: 0.088366, loss: 4.5749
2022-10-11 00:35:18 - train: epoch 0023, iter [00900, 05004], lr: 0.088346, loss: 4.3755
2022-10-11 00:36:07 - train: epoch 0023, iter [01000, 05004], lr: 0.088326, loss: 4.1293
2022-10-11 00:36:58 - train: epoch 0023, iter [01100, 05004], lr: 0.088306, loss: 4.5895
2022-10-11 00:37:48 - train: epoch 0023, iter [01200, 05004], lr: 0.088286, loss: 4.7657
2022-10-11 00:38:38 - train: epoch 0023, iter [01300, 05004], lr: 0.088265, loss: 5.0189
2022-10-11 00:39:28 - train: epoch 0023, iter [01400, 05004], lr: 0.088245, loss: 4.8009
2022-10-11 00:40:19 - train: epoch 0023, iter [01500, 05004], lr: 0.088225, loss: 4.7063
2022-10-11 00:41:08 - train: epoch 0023, iter [01600, 05004], lr: 0.088205, loss: 4.0634
2022-10-11 00:41:56 - train: epoch 0023, iter [01700, 05004], lr: 0.088184, loss: 4.2390
2022-10-11 00:42:47 - train: epoch 0023, iter [01800, 05004], lr: 0.088164, loss: 4.5152
2022-10-11 00:43:35 - train: epoch 0023, iter [01900, 05004], lr: 0.088144, loss: 4.2291
2022-10-11 00:44:25 - train: epoch 0023, iter [02000, 05004], lr: 0.088124, loss: 3.2984
2022-10-11 00:45:16 - train: epoch 0023, iter [02100, 05004], lr: 0.088103, loss: 4.1127
2022-10-11 00:46:05 - train: epoch 0023, iter [02200, 05004], lr: 0.088083, loss: 4.6427
2022-10-11 00:46:55 - train: epoch 0023, iter [02300, 05004], lr: 0.088063, loss: 4.6064
2022-10-11 00:47:45 - train: epoch 0023, iter [02400, 05004], lr: 0.088042, loss: 4.2477
2022-10-11 00:48:34 - train: epoch 0023, iter [02500, 05004], lr: 0.088022, loss: 5.0215
2022-10-11 00:49:25 - train: epoch 0023, iter [02600, 05004], lr: 0.088002, loss: 4.4151
2022-10-11 00:50:15 - train: epoch 0023, iter [02700, 05004], lr: 0.087981, loss: 4.0713
2022-10-11 00:51:05 - train: epoch 0023, iter [02800, 05004], lr: 0.087961, loss: 3.9457
2022-10-11 00:51:55 - train: epoch 0023, iter [02900, 05004], lr: 0.087940, loss: 4.0573
2022-10-11 00:52:45 - train: epoch 0023, iter [03000, 05004], lr: 0.087920, loss: 3.7514
2022-10-11 00:53:33 - train: epoch 0023, iter [03100, 05004], lr: 0.087899, loss: 4.7165
2022-10-11 00:54:22 - train: epoch 0023, iter [03200, 05004], lr: 0.087879, loss: 4.4529
2022-10-11 00:55:11 - train: epoch 0023, iter [03300, 05004], lr: 0.087858, loss: 4.2793
2022-10-11 00:56:01 - train: epoch 0023, iter [03400, 05004], lr: 0.087838, loss: 4.4128
2022-10-11 00:56:51 - train: epoch 0023, iter [03500, 05004], lr: 0.087817, loss: 4.7100
2022-10-11 00:57:41 - train: epoch 0023, iter [03600, 05004], lr: 0.087797, loss: 4.3899
2022-10-11 00:58:31 - train: epoch 0023, iter [03700, 05004], lr: 0.087776, loss: 4.3003
2022-10-11 00:59:20 - train: epoch 0023, iter [03800, 05004], lr: 0.087756, loss: 4.0819
2022-10-11 01:00:10 - train: epoch 0023, iter [03900, 05004], lr: 0.087735, loss: 4.0481
2022-10-11 01:01:00 - train: epoch 0023, iter [04000, 05004], lr: 0.087714, loss: 4.0779
2022-10-11 01:01:49 - train: epoch 0023, iter [04100, 05004], lr: 0.087694, loss: 4.6460
2022-10-11 01:02:40 - train: epoch 0023, iter [04200, 05004], lr: 0.087673, loss: 4.5106
2022-10-11 01:03:28 - train: epoch 0023, iter [04300, 05004], lr: 0.087653, loss: 4.8925
2022-10-11 01:04:18 - train: epoch 0023, iter [04400, 05004], lr: 0.087632, loss: 4.1813
2022-10-11 01:05:08 - train: epoch 0023, iter [04500, 05004], lr: 0.087611, loss: 4.5346
2022-10-11 01:05:57 - train: epoch 0023, iter [04600, 05004], lr: 0.087591, loss: 4.9452
2022-10-11 01:06:47 - train: epoch 0023, iter [04700, 05004], lr: 0.087570, loss: 4.4967
2022-10-11 01:07:38 - train: epoch 0023, iter [04800, 05004], lr: 0.087549, loss: 4.7133
2022-10-11 01:08:27 - train: epoch 0023, iter [04900, 05004], lr: 0.087528, loss: 4.0498
2022-10-11 01:09:13 - train: epoch 0023, iter [05000, 05004], lr: 0.087508, loss: 4.8280
2022-10-11 01:09:16 - train: epoch 023, train_loss: 4.5134
2022-10-11 01:11:02 - eval: epoch: 023, acc1: 53.972%, acc5: 79.296%, test_loss: 2.1781, per_image_load_time: 3.429ms, per_image_inference_time: 0.499ms
2022-10-11 01:11:02 - until epoch: 023, best_acc1: 54.286%
2022-10-11 01:11:02 - epoch 024 lr: 0.087507
2022-10-11 01:11:59 - train: epoch 0024, iter [00100, 05004], lr: 0.087486, loss: 5.1321
2022-10-11 01:12:49 - train: epoch 0024, iter [00200, 05004], lr: 0.087465, loss: 3.6035
2022-10-11 01:13:40 - train: epoch 0024, iter [00300, 05004], lr: 0.087444, loss: 4.9888
2022-10-11 01:14:29 - train: epoch 0024, iter [00400, 05004], lr: 0.087424, loss: 4.0592
2022-10-11 01:15:19 - train: epoch 0024, iter [00500, 05004], lr: 0.087403, loss: 3.8941
2022-10-11 01:16:09 - train: epoch 0024, iter [00600, 05004], lr: 0.087382, loss: 5.0308
2022-10-11 01:16:59 - train: epoch 0024, iter [00700, 05004], lr: 0.087361, loss: 4.5575
2022-10-11 01:17:48 - train: epoch 0024, iter [00800, 05004], lr: 0.087340, loss: 4.1529
2022-10-11 01:18:38 - train: epoch 0024, iter [00900, 05004], lr: 0.087319, loss: 4.1382
2022-10-11 01:19:28 - train: epoch 0024, iter [01000, 05004], lr: 0.087298, loss: 3.8170
2022-10-11 01:20:17 - train: epoch 0024, iter [01100, 05004], lr: 0.087278, loss: 4.3771
2022-10-11 01:21:08 - train: epoch 0024, iter [01200, 05004], lr: 0.087257, loss: 4.3924
2022-10-11 01:21:56 - train: epoch 0024, iter [01300, 05004], lr: 0.087236, loss: 4.4927
2022-10-11 01:22:44 - train: epoch 0024, iter [01400, 05004], lr: 0.087215, loss: 4.9154
2022-10-11 01:23:33 - train: epoch 0024, iter [01500, 05004], lr: 0.087194, loss: 4.4555
2022-10-11 01:24:21 - train: epoch 0024, iter [01600, 05004], lr: 0.087173, loss: 4.3386
2022-10-11 01:25:10 - train: epoch 0024, iter [01700, 05004], lr: 0.087152, loss: 4.2174
2022-10-11 01:25:58 - train: epoch 0024, iter [01800, 05004], lr: 0.087131, loss: 4.5751
2022-10-11 01:26:47 - train: epoch 0024, iter [01900, 05004], lr: 0.087110, loss: 3.9655
2022-10-11 01:27:36 - train: epoch 0024, iter [02000, 05004], lr: 0.087089, loss: 4.2013
2022-10-11 01:28:23 - train: epoch 0024, iter [02100, 05004], lr: 0.087068, loss: 4.3007
2022-10-11 01:29:12 - train: epoch 0024, iter [02200, 05004], lr: 0.087047, loss: 4.2632
2022-10-11 01:30:01 - train: epoch 0024, iter [02300, 05004], lr: 0.087025, loss: 4.1525
2022-10-11 01:30:51 - train: epoch 0024, iter [02400, 05004], lr: 0.087004, loss: 4.6742
2022-10-11 01:31:39 - train: epoch 0024, iter [02500, 05004], lr: 0.086983, loss: 5.2639
2022-10-11 01:32:28 - train: epoch 0024, iter [02600, 05004], lr: 0.086962, loss: 4.4244
2022-10-11 01:33:16 - train: epoch 0024, iter [02700, 05004], lr: 0.086941, loss: 4.3068
2022-10-11 01:34:04 - train: epoch 0024, iter [02800, 05004], lr: 0.086920, loss: 4.8514
2022-10-11 01:34:52 - train: epoch 0024, iter [02900, 05004], lr: 0.086899, loss: 4.4575
2022-10-11 01:35:41 - train: epoch 0024, iter [03000, 05004], lr: 0.086877, loss: 4.1999
2022-10-11 01:36:29 - train: epoch 0024, iter [03100, 05004], lr: 0.086856, loss: 4.6128
2022-10-11 01:37:18 - train: epoch 0024, iter [03200, 05004], lr: 0.086835, loss: 4.5522
2022-10-11 01:38:06 - train: epoch 0024, iter [03300, 05004], lr: 0.086814, loss: 4.0256
2022-10-11 01:38:54 - train: epoch 0024, iter [03400, 05004], lr: 0.086793, loss: 4.5616
2022-10-11 01:39:42 - train: epoch 0024, iter [03500, 05004], lr: 0.086771, loss: 5.4261
2022-10-11 01:40:32 - train: epoch 0024, iter [03600, 05004], lr: 0.086750, loss: 4.0616
2022-10-11 01:41:20 - train: epoch 0024, iter [03700, 05004], lr: 0.086729, loss: 3.9395
2022-10-11 01:42:09 - train: epoch 0024, iter [03800, 05004], lr: 0.086707, loss: 4.3234
2022-10-11 01:42:57 - train: epoch 0024, iter [03900, 05004], lr: 0.086686, loss: 4.8418
2022-10-11 01:43:45 - train: epoch 0024, iter [04000, 05004], lr: 0.086665, loss: 4.0383
2022-10-11 01:44:35 - train: epoch 0024, iter [04100, 05004], lr: 0.086643, loss: 4.1551
2022-10-11 01:45:23 - train: epoch 0024, iter [04200, 05004], lr: 0.086622, loss: 4.7345
2022-10-11 01:46:12 - train: epoch 0024, iter [04300, 05004], lr: 0.086601, loss: 3.6373
2022-10-11 01:47:00 - train: epoch 0024, iter [04400, 05004], lr: 0.086579, loss: 4.2320
2022-10-11 01:47:49 - train: epoch 0024, iter [04500, 05004], lr: 0.086558, loss: 4.1299
2022-10-11 01:48:37 - train: epoch 0024, iter [04600, 05004], lr: 0.086536, loss: 4.0534
2022-10-11 01:49:25 - train: epoch 0024, iter [04700, 05004], lr: 0.086515, loss: 5.0552
2022-10-11 01:50:12 - train: epoch 0024, iter [04800, 05004], lr: 0.086494, loss: 4.3197
2022-10-11 01:51:01 - train: epoch 0024, iter [04900, 05004], lr: 0.086472, loss: 4.7814
2022-10-11 01:51:47 - train: epoch 0024, iter [05000, 05004], lr: 0.086451, loss: 4.9630
2022-10-11 01:51:49 - train: epoch 024, train_loss: 4.5160
2022-10-11 01:53:34 - eval: epoch: 024, acc1: 52.568%, acc5: 77.950%, test_loss: 2.2832, per_image_load_time: 3.202ms, per_image_inference_time: 0.503ms
2022-10-11 01:53:34 - until epoch: 024, best_acc1: 54.286%
2022-10-11 01:53:34 - epoch 025 lr: 0.086450
2022-10-11 01:54:31 - train: epoch 0025, iter [00100, 05004], lr: 0.086428, loss: 4.5090
2022-10-11 01:55:22 - train: epoch 0025, iter [00200, 05004], lr: 0.086407, loss: 4.1011
2022-10-11 01:56:11 - train: epoch 0025, iter [00300, 05004], lr: 0.086385, loss: 5.0713
2022-10-11 01:57:01 - train: epoch 0025, iter [00400, 05004], lr: 0.086364, loss: 4.4963
2022-10-11 01:57:51 - train: epoch 0025, iter [00500, 05004], lr: 0.086342, loss: 4.1834
2022-10-11 01:58:42 - train: epoch 0025, iter [00600, 05004], lr: 0.086321, loss: 5.0073
2022-10-11 01:59:31 - train: epoch 0025, iter [00700, 05004], lr: 0.086299, loss: 4.3157
2022-10-11 02:00:22 - train: epoch 0025, iter [00800, 05004], lr: 0.086277, loss: 4.1391
2022-10-11 02:01:12 - train: epoch 0025, iter [00900, 05004], lr: 0.086256, loss: 3.7639
2022-10-11 02:02:01 - train: epoch 0025, iter [01000, 05004], lr: 0.086234, loss: 5.2051
2022-10-11 02:02:50 - train: epoch 0025, iter [01100, 05004], lr: 0.086213, loss: 4.5806
2022-10-11 02:03:40 - train: epoch 0025, iter [01200, 05004], lr: 0.086191, loss: 4.0723
2022-10-11 02:04:29 - train: epoch 0025, iter [01300, 05004], lr: 0.086169, loss: 4.7598
2022-10-11 02:05:20 - train: epoch 0025, iter [01400, 05004], lr: 0.086148, loss: 5.2637
2022-10-11 02:06:10 - train: epoch 0025, iter [01500, 05004], lr: 0.086126, loss: 3.7228
2022-10-11 02:06:59 - train: epoch 0025, iter [01600, 05004], lr: 0.086104, loss: 4.1916
2022-10-11 02:07:48 - train: epoch 0025, iter [01700, 05004], lr: 0.086082, loss: 4.4499
2022-10-11 02:08:39 - train: epoch 0025, iter [01800, 05004], lr: 0.086061, loss: 4.1871
2022-10-11 02:09:28 - train: epoch 0025, iter [01900, 05004], lr: 0.086039, loss: 5.0762
2022-10-11 02:10:18 - train: epoch 0025, iter [02000, 05004], lr: 0.086017, loss: 4.6453
2022-10-11 02:11:08 - train: epoch 0025, iter [02100, 05004], lr: 0.085995, loss: 3.8071
2022-10-11 02:11:59 - train: epoch 0025, iter [02200, 05004], lr: 0.085974, loss: 4.7578
2022-10-11 02:12:48 - train: epoch 0025, iter [02300, 05004], lr: 0.085952, loss: 4.4823
2022-10-11 02:13:39 - train: epoch 0025, iter [02400, 05004], lr: 0.085930, loss: 4.0242
2022-10-11 02:14:29 - train: epoch 0025, iter [02500, 05004], lr: 0.085908, loss: 4.8725
2022-10-11 02:15:20 - train: epoch 0025, iter [02600, 05004], lr: 0.085886, loss: 4.7843
2022-10-11 02:16:09 - train: epoch 0025, iter [02700, 05004], lr: 0.085864, loss: 4.8076
2022-10-11 02:16:59 - train: epoch 0025, iter [02800, 05004], lr: 0.085843, loss: 4.4383
2022-10-11 02:17:49 - train: epoch 0025, iter [02900, 05004], lr: 0.085821, loss: 4.1122
2022-10-11 02:18:38 - train: epoch 0025, iter [03000, 05004], lr: 0.085799, loss: 4.5222
2022-10-11 02:19:28 - train: epoch 0025, iter [03100, 05004], lr: 0.085777, loss: 4.5996
2022-10-11 02:20:18 - train: epoch 0025, iter [03200, 05004], lr: 0.085755, loss: 4.9287
2022-10-11 02:21:07 - train: epoch 0025, iter [03300, 05004], lr: 0.085733, loss: 4.5818
2022-10-11 02:21:56 - train: epoch 0025, iter [03400, 05004], lr: 0.085711, loss: 4.0986
2022-10-11 02:22:47 - train: epoch 0025, iter [03500, 05004], lr: 0.085689, loss: 4.1947
2022-10-11 02:23:37 - train: epoch 0025, iter [03600, 05004], lr: 0.085667, loss: 3.8781
2022-10-11 02:24:27 - train: epoch 0025, iter [03700, 05004], lr: 0.085645, loss: 5.2013
2022-10-11 02:25:17 - train: epoch 0025, iter [03800, 05004], lr: 0.085623, loss: 4.0149
2022-10-11 02:26:05 - train: epoch 0025, iter [03900, 05004], lr: 0.085601, loss: 5.1742
2022-10-11 02:26:56 - train: epoch 0025, iter [04000, 05004], lr: 0.085579, loss: 4.5776
2022-10-11 02:27:45 - train: epoch 0025, iter [04100, 05004], lr: 0.085557, loss: 5.0412
2022-10-11 02:28:34 - train: epoch 0025, iter [04200, 05004], lr: 0.085535, loss: 4.2402
2022-10-11 02:29:24 - train: epoch 0025, iter [04300, 05004], lr: 0.085513, loss: 4.6821
2022-10-11 02:30:13 - train: epoch 0025, iter [04400, 05004], lr: 0.085491, loss: 4.9858
2022-10-11 02:31:02 - train: epoch 0025, iter [04500, 05004], lr: 0.085468, loss: 5.0729
2022-10-11 02:31:51 - train: epoch 0025, iter [04600, 05004], lr: 0.085446, loss: 4.3025
2022-10-11 02:32:42 - train: epoch 0025, iter [04700, 05004], lr: 0.085424, loss: 4.0746
2022-10-11 02:33:32 - train: epoch 0025, iter [04800, 05004], lr: 0.085402, loss: 4.8731
2022-10-11 02:34:22 - train: epoch 0025, iter [04900, 05004], lr: 0.085380, loss: 4.6340
2022-10-11 02:35:09 - train: epoch 0025, iter [05000, 05004], lr: 0.085358, loss: 4.5646
2022-10-11 02:35:11 - train: epoch 025, train_loss: 4.5028
2022-10-11 02:36:56 - eval: epoch: 025, acc1: 55.578%, acc5: 80.670%, test_loss: 2.1023, per_image_load_time: 2.841ms, per_image_inference_time: 0.529ms
2022-10-11 02:36:56 - until epoch: 025, best_acc1: 55.578%
2022-10-11 02:36:56 - epoch 026 lr: 0.085357
2022-10-11 02:37:54 - train: epoch 0026, iter [00100, 05004], lr: 0.085335, loss: 5.3752
2022-10-11 02:38:46 - train: epoch 0026, iter [00200, 05004], lr: 0.085312, loss: 4.7386
2022-10-11 02:39:35 - train: epoch 0026, iter [00300, 05004], lr: 0.085290, loss: 4.2648
2022-10-11 02:40:24 - train: epoch 0026, iter [00400, 05004], lr: 0.085268, loss: 4.4879
2022-10-11 02:41:14 - train: epoch 0026, iter [00500, 05004], lr: 0.085246, loss: 5.3273
2022-10-11 02:42:03 - train: epoch 0026, iter [00600, 05004], lr: 0.085223, loss: 3.4535
2022-10-11 02:42:53 - train: epoch 0026, iter [00700, 05004], lr: 0.085201, loss: 4.4929
2022-10-11 02:43:43 - train: epoch 0026, iter [00800, 05004], lr: 0.085179, loss: 4.4149
2022-10-11 02:44:34 - train: epoch 0026, iter [00900, 05004], lr: 0.085156, loss: 4.9634
2022-10-11 02:45:23 - train: epoch 0026, iter [01000, 05004], lr: 0.085134, loss: 4.9134
2022-10-11 02:46:13 - train: epoch 0026, iter [01100, 05004], lr: 0.085112, loss: 4.6282
2022-10-11 02:47:03 - train: epoch 0026, iter [01200, 05004], lr: 0.085089, loss: 4.6598
2022-10-11 02:47:52 - train: epoch 0026, iter [01300, 05004], lr: 0.085067, loss: 4.0172
2022-10-11 02:48:41 - train: epoch 0026, iter [01400, 05004], lr: 0.085045, loss: 4.7689
2022-10-11 02:49:31 - train: epoch 0026, iter [01500, 05004], lr: 0.085022, loss: 4.4925
2022-10-11 02:50:21 - train: epoch 0026, iter [01600, 05004], lr: 0.085000, loss: 4.5158
2022-10-11 02:51:11 - train: epoch 0026, iter [01700, 05004], lr: 0.084977, loss: 4.9017
2022-10-11 02:52:01 - train: epoch 0026, iter [01800, 05004], lr: 0.084955, loss: 4.1956
2022-10-11 02:52:51 - train: epoch 0026, iter [01900, 05004], lr: 0.084933, loss: 5.1484
2022-10-11 02:53:40 - train: epoch 0026, iter [02000, 05004], lr: 0.084910, loss: 4.9047
2022-10-11 02:54:30 - train: epoch 0026, iter [02100, 05004], lr: 0.084888, loss: 3.8811
2022-10-11 02:55:20 - train: epoch 0026, iter [02200, 05004], lr: 0.084865, loss: 4.1222
2022-10-11 02:56:10 - train: epoch 0026, iter [02300, 05004], lr: 0.084843, loss: 4.7016
2022-10-11 02:57:01 - train: epoch 0026, iter [02400, 05004], lr: 0.084820, loss: 4.2247
2022-10-11 02:57:51 - train: epoch 0026, iter [02500, 05004], lr: 0.084798, loss: 5.1193
2022-10-11 02:58:41 - train: epoch 0026, iter [02600, 05004], lr: 0.084775, loss: 5.0157
2022-10-11 02:59:30 - train: epoch 0026, iter [02700, 05004], lr: 0.084753, loss: 4.5344
2022-10-11 03:00:20 - train: epoch 0026, iter [02800, 05004], lr: 0.084730, loss: 4.0590
2022-10-11 03:01:09 - train: epoch 0026, iter [02900, 05004], lr: 0.084707, loss: 5.0822
2022-10-11 03:01:59 - train: epoch 0026, iter [03000, 05004], lr: 0.084685, loss: 4.8117
2022-10-11 03:02:50 - train: epoch 0026, iter [03100, 05004], lr: 0.084662, loss: 4.6460
2022-10-11 03:03:40 - train: epoch 0026, iter [03200, 05004], lr: 0.084639, loss: 3.7029
2022-10-11 03:04:30 - train: epoch 0026, iter [03300, 05004], lr: 0.084617, loss: 5.0427
2022-10-11 03:05:20 - train: epoch 0026, iter [03400, 05004], lr: 0.084594, loss: 5.0269
2022-10-11 03:06:09 - train: epoch 0026, iter [03500, 05004], lr: 0.084572, loss: 4.1884
2022-10-11 03:07:00 - train: epoch 0026, iter [03600, 05004], lr: 0.084549, loss: 4.1155
2022-10-11 03:07:50 - train: epoch 0026, iter [03700, 05004], lr: 0.084526, loss: 4.4585
2022-10-11 03:08:39 - train: epoch 0026, iter [03800, 05004], lr: 0.084503, loss: 3.9149
2022-10-11 03:09:28 - train: epoch 0026, iter [03900, 05004], lr: 0.084481, loss: 5.0268
2022-10-11 03:10:18 - train: epoch 0026, iter [04000, 05004], lr: 0.084458, loss: 4.1577
2022-10-11 03:11:07 - train: epoch 0026, iter [04100, 05004], lr: 0.084435, loss: 3.5718
2022-10-11 03:11:57 - train: epoch 0026, iter [04200, 05004], lr: 0.084412, loss: 4.5387
2022-10-11 03:12:46 - train: epoch 0026, iter [04300, 05004], lr: 0.084390, loss: 4.5279
2022-10-11 03:13:36 - train: epoch 0026, iter [04400, 05004], lr: 0.084367, loss: 4.7400
2022-10-11 03:14:26 - train: epoch 0026, iter [04500, 05004], lr: 0.084344, loss: 4.4108
2022-10-11 03:15:15 - train: epoch 0026, iter [04600, 05004], lr: 0.084321, loss: 4.3134
2022-10-11 03:16:05 - train: epoch 0026, iter [04700, 05004], lr: 0.084298, loss: 5.0239
2022-10-11 03:16:55 - train: epoch 0026, iter [04800, 05004], lr: 0.084276, loss: 4.3301
2022-10-11 03:17:44 - train: epoch 0026, iter [04900, 05004], lr: 0.084253, loss: 3.9813
2022-10-11 03:18:31 - train: epoch 0026, iter [05000, 05004], lr: 0.084230, loss: 4.0911
2022-10-11 03:18:33 - train: epoch 026, train_loss: 4.4850
2022-10-11 03:20:19 - eval: epoch: 026, acc1: 55.770%, acc5: 80.654%, test_loss: 2.1171, per_image_load_time: 3.349ms, per_image_inference_time: 0.506ms
2022-10-11 03:20:19 - until epoch: 026, best_acc1: 55.770%
2022-10-11 03:20:20 - epoch 027 lr: 0.084229
2022-10-11 03:21:16 - train: epoch 0027, iter [00100, 05004], lr: 0.084206, loss: 4.7349
2022-10-11 03:22:05 - train: epoch 0027, iter [00200, 05004], lr: 0.084183, loss: 4.2324
2022-10-11 03:22:55 - train: epoch 0027, iter [00300, 05004], lr: 0.084160, loss: 4.6029
2022-10-11 03:23:47 - train: epoch 0027, iter [00400, 05004], lr: 0.084137, loss: 4.6659
2022-10-11 03:24:37 - train: epoch 0027, iter [00500, 05004], lr: 0.084114, loss: 4.9075
2022-10-11 03:25:25 - train: epoch 0027, iter [00600, 05004], lr: 0.084091, loss: 3.3756
2022-10-11 03:26:15 - train: epoch 0027, iter [00700, 05004], lr: 0.084068, loss: 4.8317
2022-10-11 03:27:05 - train: epoch 0027, iter [00800, 05004], lr: 0.084045, loss: 4.4059
2022-10-11 03:27:54 - train: epoch 0027, iter [00900, 05004], lr: 0.084022, loss: 4.6403
2022-10-11 03:28:45 - train: epoch 0027, iter [01000, 05004], lr: 0.083999, loss: 5.0151
2022-10-11 03:29:36 - train: epoch 0027, iter [01100, 05004], lr: 0.083976, loss: 4.6851
2022-10-11 03:30:24 - train: epoch 0027, iter [01200, 05004], lr: 0.083953, loss: 5.2158
2022-10-11 03:31:13 - train: epoch 0027, iter [01300, 05004], lr: 0.083930, loss: 5.4198
2022-10-11 03:32:03 - train: epoch 0027, iter [01400, 05004], lr: 0.083907, loss: 4.6768
2022-10-11 03:32:53 - train: epoch 0027, iter [01500, 05004], lr: 0.083884, loss: 4.6496
2022-10-11 03:33:44 - train: epoch 0027, iter [01600, 05004], lr: 0.083861, loss: 4.1478
2022-10-11 03:34:33 - train: epoch 0027, iter [01700, 05004], lr: 0.083838, loss: 4.0802
2022-10-11 03:35:23 - train: epoch 0027, iter [01800, 05004], lr: 0.083815, loss: 4.6316
2022-10-11 03:36:13 - train: epoch 0027, iter [01900, 05004], lr: 0.083792, loss: 4.9194
2022-10-11 03:37:02 - train: epoch 0027, iter [02000, 05004], lr: 0.083769, loss: 4.1619
2022-10-11 03:37:52 - train: epoch 0027, iter [02100, 05004], lr: 0.083745, loss: 4.6170
2022-10-11 03:38:42 - train: epoch 0027, iter [02200, 05004], lr: 0.083722, loss: 3.7497
2022-10-11 03:39:32 - train: epoch 0027, iter [02300, 05004], lr: 0.083699, loss: 4.7272
2022-10-11 03:40:23 - train: epoch 0027, iter [02400, 05004], lr: 0.083676, loss: 4.5343
2022-10-11 03:41:12 - train: epoch 0027, iter [02500, 05004], lr: 0.083653, loss: 3.9241
2022-10-11 03:42:02 - train: epoch 0027, iter [02600, 05004], lr: 0.083630, loss: 4.1117
2022-10-11 03:42:51 - train: epoch 0027, iter [02700, 05004], lr: 0.083606, loss: 4.2426
2022-10-11 03:43:42 - train: epoch 0027, iter [02800, 05004], lr: 0.083583, loss: 4.3925
2022-10-11 03:44:32 - train: epoch 0027, iter [02900, 05004], lr: 0.083560, loss: 4.7054
2022-10-11 03:45:21 - train: epoch 0027, iter [03000, 05004], lr: 0.083536, loss: 4.0784
2022-10-11 03:46:10 - train: epoch 0027, iter [03100, 05004], lr: 0.083513, loss: 4.4332
2022-10-11 03:47:00 - train: epoch 0027, iter [03200, 05004], lr: 0.083490, loss: 3.2122
2022-10-11 03:47:50 - train: epoch 0027, iter [03300, 05004], lr: 0.083467, loss: 5.1581
2022-10-11 03:48:39 - train: epoch 0027, iter [03400, 05004], lr: 0.083443, loss: 3.8632
2022-10-11 03:49:30 - train: epoch 0027, iter [03500, 05004], lr: 0.083420, loss: 4.7096
2022-10-11 03:50:20 - train: epoch 0027, iter [03600, 05004], lr: 0.083397, loss: 4.8137
2022-10-11 03:51:09 - train: epoch 0027, iter [03700, 05004], lr: 0.083373, loss: 4.4460
2022-10-11 03:52:00 - train: epoch 0027, iter [03800, 05004], lr: 0.083350, loss: 4.6225
2022-10-11 03:52:49 - train: epoch 0027, iter [03900, 05004], lr: 0.083326, loss: 4.3981
2022-10-11 03:53:39 - train: epoch 0027, iter [04000, 05004], lr: 0.083303, loss: 4.4663
2022-10-11 03:54:28 - train: epoch 0027, iter [04100, 05004], lr: 0.083280, loss: 4.6724
2022-10-11 03:55:18 - train: epoch 0027, iter [04200, 05004], lr: 0.083256, loss: 5.1205
2022-10-11 03:56:08 - train: epoch 0027, iter [04300, 05004], lr: 0.083233, loss: 3.3546
2022-10-11 03:56:57 - train: epoch 0027, iter [04400, 05004], lr: 0.083209, loss: 4.3994
2022-10-11 03:57:47 - train: epoch 0027, iter [04500, 05004], lr: 0.083186, loss: 4.4061
2022-10-11 03:58:36 - train: epoch 0027, iter [04600, 05004], lr: 0.083162, loss: 5.1390
2022-10-11 03:59:25 - train: epoch 0027, iter [04700, 05004], lr: 0.083139, loss: 4.6773
2022-10-11 04:00:17 - train: epoch 0027, iter [04800, 05004], lr: 0.083115, loss: 4.3162
2022-10-11 04:01:06 - train: epoch 0027, iter [04900, 05004], lr: 0.083092, loss: 4.7962
2022-10-11 04:01:53 - train: epoch 0027, iter [05000, 05004], lr: 0.083068, loss: 4.6231
2022-10-11 04:01:55 - train: epoch 027, train_loss: 4.4820
2022-10-11 04:03:41 - eval: epoch: 027, acc1: 56.230%, acc5: 81.104%, test_loss: 2.0305, per_image_load_time: 3.452ms, per_image_inference_time: 0.490ms
2022-10-11 04:03:41 - until epoch: 027, best_acc1: 56.230%
2022-10-11 04:03:41 - epoch 028 lr: 0.083067
2022-10-11 04:04:38 - train: epoch 0028, iter [00100, 05004], lr: 0.083044, loss: 4.8943
2022-10-11 04:05:28 - train: epoch 0028, iter [00200, 05004], lr: 0.083020, loss: 4.1796
2022-10-11 04:06:20 - train: epoch 0028, iter [00300, 05004], lr: 0.082997, loss: 4.7220
2022-10-11 04:07:10 - train: epoch 0028, iter [00400, 05004], lr: 0.082973, loss: 4.7650
2022-10-11 04:07:58 - train: epoch 0028, iter [00500, 05004], lr: 0.082949, loss: 4.5522
2022-10-11 04:08:48 - train: epoch 0028, iter [00600, 05004], lr: 0.082926, loss: 4.4912
2022-10-11 04:09:37 - train: epoch 0028, iter [00700, 05004], lr: 0.082902, loss: 4.3086
2022-10-11 04:10:27 - train: epoch 0028, iter [00800, 05004], lr: 0.082879, loss: 4.9788
2022-10-11 04:11:17 - train: epoch 0028, iter [00900, 05004], lr: 0.082855, loss: 4.4315
2022-10-11 04:12:06 - train: epoch 0028, iter [01000, 05004], lr: 0.082831, loss: 4.5258
2022-10-11 04:12:57 - train: epoch 0028, iter [01100, 05004], lr: 0.082808, loss: 4.7176
2022-10-11 04:13:46 - train: epoch 0028, iter [01200, 05004], lr: 0.082784, loss: 4.2956
2022-10-11 04:14:37 - train: epoch 0028, iter [01300, 05004], lr: 0.082760, loss: 5.0415
2022-10-11 04:15:26 - train: epoch 0028, iter [01400, 05004], lr: 0.082736, loss: 3.7232
2022-10-11 04:16:16 - train: epoch 0028, iter [01500, 05004], lr: 0.082713, loss: 4.3559
2022-10-11 04:17:04 - train: epoch 0028, iter [01600, 05004], lr: 0.082689, loss: 4.8673
2022-10-11 04:17:55 - train: epoch 0028, iter [01700, 05004], lr: 0.082665, loss: 4.5230
2022-10-11 04:18:44 - train: epoch 0028, iter [01800, 05004], lr: 0.082641, loss: 4.4211
2022-10-11 04:19:34 - train: epoch 0028, iter [01900, 05004], lr: 0.082618, loss: 4.1231
2022-10-11 04:20:24 - train: epoch 0028, iter [02000, 05004], lr: 0.082594, loss: 4.2977
2022-10-11 04:21:14 - train: epoch 0028, iter [02100, 05004], lr: 0.082570, loss: 3.8639
2022-10-11 04:22:05 - train: epoch 0028, iter [02200, 05004], lr: 0.082546, loss: 4.4376
2022-10-11 04:22:56 - train: epoch 0028, iter [02300, 05004], lr: 0.082522, loss: 4.2847
2022-10-11 04:23:45 - train: epoch 0028, iter [02400, 05004], lr: 0.082498, loss: 4.9934
2022-10-11 04:24:35 - train: epoch 0028, iter [02500, 05004], lr: 0.082475, loss: 3.9026
2022-10-11 04:25:25 - train: epoch 0028, iter [02600, 05004], lr: 0.082451, loss: 4.2055
2022-10-11 04:26:15 - train: epoch 0028, iter [02700, 05004], lr: 0.082427, loss: 4.4488
2022-10-11 04:27:05 - train: epoch 0028, iter [02800, 05004], lr: 0.082403, loss: 4.2164
2022-10-11 04:27:53 - train: epoch 0028, iter [02900, 05004], lr: 0.082379, loss: 4.3403
2022-10-11 04:28:44 - train: epoch 0028, iter [03000, 05004], lr: 0.082355, loss: 4.1253
2022-10-11 04:29:33 - train: epoch 0028, iter [03100, 05004], lr: 0.082331, loss: 4.4280
2022-10-11 04:30:22 - train: epoch 0028, iter [03200, 05004], lr: 0.082307, loss: 3.9157
2022-10-11 04:31:13 - train: epoch 0028, iter [03300, 05004], lr: 0.082283, loss: 4.7727
2022-10-11 04:32:02 - train: epoch 0028, iter [03400, 05004], lr: 0.082259, loss: 4.8904
2022-10-11 04:32:51 - train: epoch 0028, iter [03500, 05004], lr: 0.082235, loss: 3.7119
2022-10-11 04:33:42 - train: epoch 0028, iter [03600, 05004], lr: 0.082211, loss: 4.2342
2022-10-11 04:34:31 - train: epoch 0028, iter [03700, 05004], lr: 0.082187, loss: 4.3675
2022-10-11 04:35:21 - train: epoch 0028, iter [03800, 05004], lr: 0.082163, loss: 4.3522
2022-10-11 04:36:11 - train: epoch 0028, iter [03900, 05004], lr: 0.082139, loss: 4.6502
2022-10-11 04:37:00 - train: epoch 0028, iter [04000, 05004], lr: 0.082115, loss: 5.1820
2022-10-11 04:37:51 - train: epoch 0028, iter [04100, 05004], lr: 0.082091, loss: 4.4004
2022-10-11 04:38:41 - train: epoch 0028, iter [04200, 05004], lr: 0.082067, loss: 4.6899
2022-10-11 04:39:30 - train: epoch 0028, iter [04300, 05004], lr: 0.082043, loss: 5.0030
2022-10-11 04:40:20 - train: epoch 0028, iter [04400, 05004], lr: 0.082019, loss: 3.7432
2022-10-11 04:41:11 - train: epoch 0028, iter [04500, 05004], lr: 0.081995, loss: 4.2038
2022-10-11 04:41:59 - train: epoch 0028, iter [04600, 05004], lr: 0.081971, loss: 4.8260
2022-10-11 04:42:49 - train: epoch 0028, iter [04700, 05004], lr: 0.081946, loss: 4.3966
2022-10-11 04:43:39 - train: epoch 0028, iter [04800, 05004], lr: 0.081922, loss: 4.5701
2022-10-11 04:44:29 - train: epoch 0028, iter [04900, 05004], lr: 0.081898, loss: 4.4135
2022-10-11 04:45:16 - train: epoch 0028, iter [05000, 05004], lr: 0.081874, loss: 4.9509
2022-10-11 04:45:18 - train: epoch 028, train_loss: 4.4735
2022-10-11 04:47:03 - eval: epoch: 028, acc1: 57.006%, acc5: 81.734%, test_loss: 2.0869, per_image_load_time: 3.325ms, per_image_inference_time: 0.501ms
2022-10-11 04:47:04 - until epoch: 028, best_acc1: 57.006%
2022-10-11 04:47:04 - epoch 029 lr: 0.081873
2022-10-11 04:48:01 - train: epoch 0029, iter [00100, 05004], lr: 0.081849, loss: 4.2330
2022-10-11 04:48:52 - train: epoch 0029, iter [00200, 05004], lr: 0.081825, loss: 4.5948
2022-10-11 04:49:42 - train: epoch 0029, iter [00300, 05004], lr: 0.081800, loss: 4.1404
2022-10-11 04:50:29 - train: epoch 0029, iter [00400, 05004], lr: 0.081776, loss: 4.1165
2022-10-11 04:51:19 - train: epoch 0029, iter [00500, 05004], lr: 0.081752, loss: 4.9375
2022-10-11 04:52:07 - train: epoch 0029, iter [00600, 05004], lr: 0.081728, loss: 4.6841
2022-10-11 04:52:55 - train: epoch 0029, iter [00700, 05004], lr: 0.081703, loss: 4.0034
2022-10-11 04:53:45 - train: epoch 0029, iter [00800, 05004], lr: 0.081679, loss: 4.6897
2022-10-11 04:54:33 - train: epoch 0029, iter [00900, 05004], lr: 0.081655, loss: 5.1135
2022-10-11 04:55:22 - train: epoch 0029, iter [01000, 05004], lr: 0.081631, loss: 4.7333
2022-10-11 04:56:10 - train: epoch 0029, iter [01100, 05004], lr: 0.081606, loss: 3.5468
2022-10-11 04:56:58 - train: epoch 0029, iter [01200, 05004], lr: 0.081582, loss: 5.0404
2022-10-11 04:57:47 - train: epoch 0029, iter [01300, 05004], lr: 0.081558, loss: 4.2577
2022-10-11 04:58:36 - train: epoch 0029, iter [01400, 05004], lr: 0.081533, loss: 4.0964
2022-10-11 04:59:26 - train: epoch 0029, iter [01500, 05004], lr: 0.081509, loss: 4.6341
2022-10-11 05:00:14 - train: epoch 0029, iter [01600, 05004], lr: 0.081484, loss: 4.0295
2022-10-11 05:01:02 - train: epoch 0029, iter [01700, 05004], lr: 0.081460, loss: 4.1712
2022-10-11 05:01:51 - train: epoch 0029, iter [01800, 05004], lr: 0.081436, loss: 4.4747
2022-10-11 05:02:38 - train: epoch 0029, iter [01900, 05004], lr: 0.081411, loss: 3.5814
2022-10-11 05:03:27 - train: epoch 0029, iter [02000, 05004], lr: 0.081387, loss: 4.6577
2022-10-11 05:04:16 - train: epoch 0029, iter [02100, 05004], lr: 0.081362, loss: 4.8058
2022-10-11 05:05:04 - train: epoch 0029, iter [02200, 05004], lr: 0.081338, loss: 4.4113
2022-10-11 05:05:53 - train: epoch 0029, iter [02300, 05004], lr: 0.081313, loss: 3.5573
2022-10-11 05:06:43 - train: epoch 0029, iter [02400, 05004], lr: 0.081289, loss: 4.2214
2022-10-11 05:07:32 - train: epoch 0029, iter [02500, 05004], lr: 0.081264, loss: 4.7568
2022-10-11 05:08:20 - train: epoch 0029, iter [02600, 05004], lr: 0.081240, loss: 4.3232
2022-10-11 05:09:07 - train: epoch 0029, iter [02700, 05004], lr: 0.081215, loss: 4.2210
2022-10-11 05:09:56 - train: epoch 0029, iter [02800, 05004], lr: 0.081191, loss: 4.7825
2022-10-11 05:10:44 - train: epoch 0029, iter [02900, 05004], lr: 0.081166, loss: 4.1386
2022-10-11 05:11:32 - train: epoch 0029, iter [03000, 05004], lr: 0.081142, loss: 4.9574
2022-10-11 05:12:21 - train: epoch 0029, iter [03100, 05004], lr: 0.081117, loss: 4.0992
2022-10-11 05:13:10 - train: epoch 0029, iter [03200, 05004], lr: 0.081093, loss: 4.6538
2022-10-11 05:13:58 - train: epoch 0029, iter [03300, 05004], lr: 0.081068, loss: 3.7407
2022-10-11 05:14:47 - train: epoch 0029, iter [03400, 05004], lr: 0.081044, loss: 4.4229
2022-10-11 05:15:35 - train: epoch 0029, iter [03500, 05004], lr: 0.081019, loss: 4.1878
2022-10-11 05:16:24 - train: epoch 0029, iter [03600, 05004], lr: 0.080994, loss: 4.3809
2022-10-11 05:17:12 - train: epoch 0029, iter [03700, 05004], lr: 0.080970, loss: 4.2527
2022-10-11 05:18:00 - train: epoch 0029, iter [03800, 05004], lr: 0.080945, loss: 4.3692
2022-10-11 05:18:50 - train: epoch 0029, iter [03900, 05004], lr: 0.080920, loss: 3.1383
2022-10-11 05:19:38 - train: epoch 0029, iter [04000, 05004], lr: 0.080896, loss: 4.9390
2022-10-11 05:20:26 - train: epoch 0029, iter [04100, 05004], lr: 0.080871, loss: 4.4556
2022-10-11 05:21:13 - train: epoch 0029, iter [04200, 05004], lr: 0.080846, loss: 4.9779
2022-10-11 05:22:02 - train: epoch 0029, iter [04300, 05004], lr: 0.080822, loss: 4.5640
2022-10-11 05:22:51 - train: epoch 0029, iter [04400, 05004], lr: 0.080797, loss: 4.3603
2022-10-11 05:23:40 - train: epoch 0029, iter [04500, 05004], lr: 0.080772, loss: 4.9302
2022-10-11 05:24:29 - train: epoch 0029, iter [04600, 05004], lr: 0.080747, loss: 4.1575
2022-10-11 05:25:19 - train: epoch 0029, iter [04700, 05004], lr: 0.080723, loss: 3.8010
2022-10-11 05:26:09 - train: epoch 0029, iter [04800, 05004], lr: 0.080698, loss: 4.2963
2022-10-11 05:27:00 - train: epoch 0029, iter [04900, 05004], lr: 0.080673, loss: 4.1820
2022-10-11 05:27:45 - train: epoch 0029, iter [05000, 05004], lr: 0.080648, loss: 4.5902
2022-10-11 05:27:48 - train: epoch 029, train_loss: 4.4634
2022-10-11 05:29:35 - eval: epoch: 029, acc1: 56.900%, acc5: 81.624%, test_loss: 2.0776, per_image_load_time: 3.449ms, per_image_inference_time: 0.514ms
2022-10-11 05:29:35 - until epoch: 029, best_acc1: 57.006%
2022-10-11 05:29:35 - epoch 030 lr: 0.080647
2022-10-11 05:30:32 - train: epoch 0030, iter [00100, 05004], lr: 0.080622, loss: 3.7409
2022-10-11 05:31:22 - train: epoch 0030, iter [00200, 05004], lr: 0.080598, loss: 4.9281
2022-10-11 05:32:12 - train: epoch 0030, iter [00300, 05004], lr: 0.080573, loss: 4.4659
2022-10-11 05:33:01 - train: epoch 0030, iter [00400, 05004], lr: 0.080548, loss: 3.8724
2022-10-11 05:33:51 - train: epoch 0030, iter [00500, 05004], lr: 0.080523, loss: 4.9686
2022-10-11 05:34:42 - train: epoch 0030, iter [00600, 05004], lr: 0.080498, loss: 5.2728
2022-10-11 05:35:31 - train: epoch 0030, iter [00700, 05004], lr: 0.080473, loss: 4.8006
2022-10-11 05:36:21 - train: epoch 0030, iter [00800, 05004], lr: 0.080448, loss: 4.1962
2022-10-11 05:37:10 - train: epoch 0030, iter [00900, 05004], lr: 0.080424, loss: 4.6185
2022-10-11 05:38:00 - train: epoch 0030, iter [01000, 05004], lr: 0.080399, loss: 5.0015
2022-10-11 05:38:49 - train: epoch 0030, iter [01100, 05004], lr: 0.080374, loss: 4.6589
2022-10-11 05:39:39 - train: epoch 0030, iter [01200, 05004], lr: 0.080349, loss: 5.1164
2022-10-11 05:40:30 - train: epoch 0030, iter [01300, 05004], lr: 0.080324, loss: 4.7786
2022-10-11 05:41:19 - train: epoch 0030, iter [01400, 05004], lr: 0.080299, loss: 3.7904
2022-10-11 05:42:10 - train: epoch 0030, iter [01500, 05004], lr: 0.080274, loss: 4.5614
2022-10-11 05:42:59 - train: epoch 0030, iter [01600, 05004], lr: 0.080249, loss: 4.4185
2022-10-11 05:43:49 - train: epoch 0030, iter [01700, 05004], lr: 0.080224, loss: 4.2750
2022-10-11 05:44:38 - train: epoch 0030, iter [01800, 05004], lr: 0.080199, loss: 4.2772
2022-10-11 05:45:28 - train: epoch 0030, iter [01900, 05004], lr: 0.080174, loss: 4.2480
2022-10-11 05:46:17 - train: epoch 0030, iter [02000, 05004], lr: 0.080149, loss: 4.5847
2022-10-11 05:47:07 - train: epoch 0030, iter [02100, 05004], lr: 0.080124, loss: 4.6288
2022-10-11 05:47:58 - train: epoch 0030, iter [02200, 05004], lr: 0.080099, loss: 4.1645
2022-10-11 05:48:47 - train: epoch 0030, iter [02300, 05004], lr: 0.080074, loss: 4.0283
2022-10-11 05:49:37 - train: epoch 0030, iter [02400, 05004], lr: 0.080049, loss: 4.0476
2022-10-11 05:50:27 - train: epoch 0030, iter [02500, 05004], lr: 0.080024, loss: 4.5099
2022-10-11 05:51:17 - train: epoch 0030, iter [02600, 05004], lr: 0.079998, loss: 4.6724
2022-10-11 05:52:06 - train: epoch 0030, iter [02700, 05004], lr: 0.079973, loss: 4.4449
2022-10-11 05:52:57 - train: epoch 0030, iter [02800, 05004], lr: 0.079948, loss: 4.5785
2022-10-11 05:53:48 - train: epoch 0030, iter [02900, 05004], lr: 0.079923, loss: 4.7956
2022-10-11 05:54:36 - train: epoch 0030, iter [03000, 05004], lr: 0.079898, loss: 3.5576
2022-10-11 05:55:26 - train: epoch 0030, iter [03100, 05004], lr: 0.079873, loss: 4.3778
2022-10-11 05:56:15 - train: epoch 0030, iter [03200, 05004], lr: 0.079848, loss: 4.9410
2022-10-11 05:57:06 - train: epoch 0030, iter [03300, 05004], lr: 0.079822, loss: 3.8601
2022-10-11 05:57:55 - train: epoch 0030, iter [03400, 05004], lr: 0.079797, loss: 4.4559
2022-10-11 05:58:45 - train: epoch 0030, iter [03500, 05004], lr: 0.079772, loss: 3.8787
2022-10-11 05:59:36 - train: epoch 0030, iter [03600, 05004], lr: 0.079747, loss: 4.1583
2022-10-11 06:00:26 - train: epoch 0030, iter [03700, 05004], lr: 0.079721, loss: 4.2835
2022-10-11 06:01:15 - train: epoch 0030, iter [03800, 05004], lr: 0.079696, loss: 3.8001
2022-10-11 06:02:06 - train: epoch 0030, iter [03900, 05004], lr: 0.079671, loss: 4.3161
2022-10-11 06:02:55 - train: epoch 0030, iter [04000, 05004], lr: 0.079646, loss: 4.8557
2022-10-11 06:03:44 - train: epoch 0030, iter [04100, 05004], lr: 0.079620, loss: 4.5383
2022-10-11 06:04:34 - train: epoch 0030, iter [04200, 05004], lr: 0.079595, loss: 4.7239
2022-10-11 06:05:25 - train: epoch 0030, iter [04300, 05004], lr: 0.079570, loss: 3.9905
2022-10-11 06:06:13 - train: epoch 0030, iter [04400, 05004], lr: 0.079544, loss: 3.6223
2022-10-11 06:07:03 - train: epoch 0030, iter [04500, 05004], lr: 0.079519, loss: 4.7844
2022-10-11 06:07:53 - train: epoch 0030, iter [04600, 05004], lr: 0.079494, loss: 3.9558
2022-10-11 06:08:43 - train: epoch 0030, iter [04700, 05004], lr: 0.079468, loss: 4.9964
2022-10-11 06:09:32 - train: epoch 0030, iter [04800, 05004], lr: 0.079443, loss: 4.4296
2022-10-11 06:10:23 - train: epoch 0030, iter [04900, 05004], lr: 0.079418, loss: 4.3965
2022-10-11 06:11:10 - train: epoch 0030, iter [05000, 05004], lr: 0.079392, loss: 4.6322
2022-10-11 06:11:12 - train: epoch 030, train_loss: 4.4579
2022-10-11 06:12:57 - eval: epoch: 030, acc1: 56.388%, acc5: 80.890%, test_loss: 2.0662, per_image_load_time: 2.347ms, per_image_inference_time: 0.518ms
2022-10-11 06:12:58 - until epoch: 030, best_acc1: 57.006%
2022-10-11 06:12:58 - epoch 031 lr: 0.079391
2022-10-11 06:13:54 - train: epoch 0031, iter [00100, 05004], lr: 0.079366, loss: 4.1901
2022-10-11 06:14:44 - train: epoch 0031, iter [00200, 05004], lr: 0.079341, loss: 4.1324
2022-10-11 06:15:35 - train: epoch 0031, iter [00300, 05004], lr: 0.079315, loss: 4.7152
2022-10-11 06:16:24 - train: epoch 0031, iter [00400, 05004], lr: 0.079290, loss: 4.4155
2022-10-11 06:17:14 - train: epoch 0031, iter [00500, 05004], lr: 0.079264, loss: 3.7447
2022-10-11 06:18:04 - train: epoch 0031, iter [00600, 05004], lr: 0.079239, loss: 4.2268
2022-10-11 06:18:54 - train: epoch 0031, iter [00700, 05004], lr: 0.079213, loss: 4.4185
2022-10-11 06:19:44 - train: epoch 0031, iter [00800, 05004], lr: 0.079188, loss: 4.2381
2022-10-11 06:20:34 - train: epoch 0031, iter [00900, 05004], lr: 0.079162, loss: 3.9078
2022-10-11 06:21:24 - train: epoch 0031, iter [01000, 05004], lr: 0.079137, loss: 4.7297
2022-10-11 06:22:13 - train: epoch 0031, iter [01100, 05004], lr: 0.079111, loss: 4.0944
2022-10-11 06:23:04 - train: epoch 0031, iter [01200, 05004], lr: 0.079086, loss: 4.5394
2022-10-11 06:23:54 - train: epoch 0031, iter [01300, 05004], lr: 0.079060, loss: 4.6598
2022-10-11 06:24:44 - train: epoch 0031, iter [01400, 05004], lr: 0.079035, loss: 5.0092
2022-10-11 06:25:34 - train: epoch 0031, iter [01500, 05004], lr: 0.079009, loss: 4.4672
2022-10-11 06:26:26 - train: epoch 0031, iter [01600, 05004], lr: 0.078984, loss: 4.2887
2022-10-11 06:27:16 - train: epoch 0031, iter [01700, 05004], lr: 0.078958, loss: 4.1226
2022-10-11 06:28:06 - train: epoch 0031, iter [01800, 05004], lr: 0.078932, loss: 4.2573
2022-10-11 06:28:56 - train: epoch 0031, iter [01900, 05004], lr: 0.078907, loss: 4.9557
2022-10-11 06:29:44 - train: epoch 0031, iter [02000, 05004], lr: 0.078881, loss: 4.3311
2022-10-11 06:30:35 - train: epoch 0031, iter [02100, 05004], lr: 0.078856, loss: 4.6597
2022-10-11 06:31:26 - train: epoch 0031, iter [02200, 05004], lr: 0.078830, loss: 4.7676
2022-10-11 06:32:15 - train: epoch 0031, iter [02300, 05004], lr: 0.078804, loss: 3.9402
2022-10-11 06:33:06 - train: epoch 0031, iter [02400, 05004], lr: 0.078779, loss: 4.5372
2022-10-11 06:33:54 - train: epoch 0031, iter [02500, 05004], lr: 0.078753, loss: 3.9716
2022-10-11 06:34:45 - train: epoch 0031, iter [02600, 05004], lr: 0.078727, loss: 4.3023
2022-10-11 06:35:34 - train: epoch 0031, iter [02700, 05004], lr: 0.078702, loss: 4.7361
2022-10-11 06:36:25 - train: epoch 0031, iter [02800, 05004], lr: 0.078676, loss: 4.4363
2022-10-11 06:37:14 - train: epoch 0031, iter [02900, 05004], lr: 0.078650, loss: 4.3432
2022-10-11 06:38:04 - train: epoch 0031, iter [03000, 05004], lr: 0.078624, loss: 4.7059
2022-10-11 06:38:54 - train: epoch 0031, iter [03100, 05004], lr: 0.078599, loss: 4.4212
2022-10-11 06:39:44 - train: epoch 0031, iter [03200, 05004], lr: 0.078573, loss: 4.5311
2022-10-11 06:40:35 - train: epoch 0031, iter [03300, 05004], lr: 0.078547, loss: 3.9549
2022-10-11 06:41:26 - train: epoch 0031, iter [03400, 05004], lr: 0.078521, loss: 5.2772
2022-10-11 06:42:14 - train: epoch 0031, iter [03500, 05004], lr: 0.078496, loss: 4.6690
2022-10-11 06:43:05 - train: epoch 0031, iter [03600, 05004], lr: 0.078470, loss: 4.7033
2022-10-11 06:43:56 - train: epoch 0031, iter [03700, 05004], lr: 0.078444, loss: 4.5631
2022-10-11 06:44:46 - train: epoch 0031, iter [03800, 05004], lr: 0.078418, loss: 3.2530
2022-10-11 06:45:37 - train: epoch 0031, iter [03900, 05004], lr: 0.078392, loss: 3.8639
2022-10-11 06:46:25 - train: epoch 0031, iter [04000, 05004], lr: 0.078366, loss: 3.9031
2022-10-11 06:47:16 - train: epoch 0031, iter [04100, 05004], lr: 0.078341, loss: 4.9704
2022-10-11 06:48:05 - train: epoch 0031, iter [04200, 05004], lr: 0.078315, loss: 3.6009
2022-10-11 06:48:54 - train: epoch 0031, iter [04300, 05004], lr: 0.078289, loss: 4.1209
2022-10-11 06:49:44 - train: epoch 0031, iter [04400, 05004], lr: 0.078263, loss: 4.5351
2022-10-11 06:50:33 - train: epoch 0031, iter [04500, 05004], lr: 0.078237, loss: 4.0701
2022-10-11 06:51:24 - train: epoch 0031, iter [04600, 05004], lr: 0.078211, loss: 4.8767
2022-10-11 06:52:15 - train: epoch 0031, iter [04700, 05004], lr: 0.078185, loss: 4.0712
2022-10-11 06:53:04 - train: epoch 0031, iter [04800, 05004], lr: 0.078159, loss: 4.7672
2022-10-11 06:53:53 - train: epoch 0031, iter [04900, 05004], lr: 0.078133, loss: 4.7560
2022-10-11 06:54:41 - train: epoch 0031, iter [05000, 05004], lr: 0.078107, loss: 4.1062
2022-10-11 06:54:43 - train: epoch 031, train_loss: 4.4440
2022-10-11 06:56:29 - eval: epoch: 031, acc1: 57.900%, acc5: 82.280%, test_loss: 2.0338, per_image_load_time: 3.223ms, per_image_inference_time: 0.485ms
2022-10-11 06:56:30 - until epoch: 031, best_acc1: 57.900%
2022-10-11 06:56:30 - epoch 032 lr: 0.078106
2022-10-11 06:57:27 - train: epoch 0032, iter [00100, 05004], lr: 0.078080, loss: 4.0785
2022-10-11 06:58:18 - train: epoch 0032, iter [00200, 05004], lr: 0.078054, loss: 3.5195
2022-10-11 06:59:07 - train: epoch 0032, iter [00300, 05004], lr: 0.078028, loss: 3.4186
2022-10-11 06:59:56 - train: epoch 0032, iter [00400, 05004], lr: 0.078002, loss: 3.7737
2022-10-11 07:00:45 - train: epoch 0032, iter [00500, 05004], lr: 0.077976, loss: 4.5972
2022-10-11 07:01:34 - train: epoch 0032, iter [00600, 05004], lr: 0.077950, loss: 4.6893
2022-10-11 07:02:25 - train: epoch 0032, iter [00700, 05004], lr: 0.077924, loss: 3.9169
2022-10-11 07:03:15 - train: epoch 0032, iter [00800, 05004], lr: 0.077898, loss: 4.8636
2022-10-11 07:04:04 - train: epoch 0032, iter [00900, 05004], lr: 0.077872, loss: 3.7426
2022-10-11 07:04:55 - train: epoch 0032, iter [01000, 05004], lr: 0.077846, loss: 4.2728
2022-10-11 07:05:43 - train: epoch 0032, iter [01100, 05004], lr: 0.077820, loss: 3.2540
2022-10-11 07:06:33 - train: epoch 0032, iter [01200, 05004], lr: 0.077794, loss: 4.1773
2022-10-11 07:07:23 - train: epoch 0032, iter [01300, 05004], lr: 0.077768, loss: 4.5615
2022-10-11 07:08:13 - train: epoch 0032, iter [01400, 05004], lr: 0.077742, loss: 4.7059
2022-10-11 07:09:03 - train: epoch 0032, iter [01500, 05004], lr: 0.077716, loss: 4.8900
2022-10-11 07:09:53 - train: epoch 0032, iter [01600, 05004], lr: 0.077690, loss: 4.5966
2022-10-11 07:10:42 - train: epoch 0032, iter [01700, 05004], lr: 0.077663, loss: 3.9572
2022-10-11 07:11:32 - train: epoch 0032, iter [01800, 05004], lr: 0.077637, loss: 4.7126
2022-10-11 07:12:22 - train: epoch 0032, iter [01900, 05004], lr: 0.077611, loss: 4.6858
2022-10-11 07:13:12 - train: epoch 0032, iter [02000, 05004], lr: 0.077585, loss: 4.9673
2022-10-11 07:14:01 - train: epoch 0032, iter [02100, 05004], lr: 0.077559, loss: 4.0608
2022-10-11 07:14:50 - train: epoch 0032, iter [02200, 05004], lr: 0.077533, loss: 4.3613
2022-10-11 07:15:41 - train: epoch 0032, iter [02300, 05004], lr: 0.077506, loss: 3.7869
2022-10-11 07:16:30 - train: epoch 0032, iter [02400, 05004], lr: 0.077480, loss: 3.7035
2022-10-11 07:17:21 - train: epoch 0032, iter [02500, 05004], lr: 0.077454, loss: 5.0128
2022-10-11 07:18:10 - train: epoch 0032, iter [02600, 05004], lr: 0.077428, loss: 3.9735
2022-10-11 07:19:00 - train: epoch 0032, iter [02700, 05004], lr: 0.077401, loss: 4.6777
2022-10-11 07:19:50 - train: epoch 0032, iter [02800, 05004], lr: 0.077375, loss: 3.6387
2022-10-11 07:20:39 - train: epoch 0032, iter [02900, 05004], lr: 0.077349, loss: 4.1456
2022-10-11 07:21:29 - train: epoch 0032, iter [03000, 05004], lr: 0.077323, loss: 4.3558
2022-10-11 07:22:18 - train: epoch 0032, iter [03100, 05004], lr: 0.077296, loss: 4.7341
2022-10-11 07:23:08 - train: epoch 0032, iter [03200, 05004], lr: 0.077270, loss: 4.4211
2022-10-11 07:23:58 - train: epoch 0032, iter [03300, 05004], lr: 0.077244, loss: 4.7703
2022-10-11 07:24:48 - train: epoch 0032, iter [03400, 05004], lr: 0.077217, loss: 4.5514
2022-10-11 07:25:38 - train: epoch 0032, iter [03500, 05004], lr: 0.077191, loss: 4.4376
2022-10-11 07:26:29 - train: epoch 0032, iter [03600, 05004], lr: 0.077165, loss: 4.6107
2022-10-11 07:27:19 - train: epoch 0032, iter [03700, 05004], lr: 0.077138, loss: 3.6736
2022-10-11 07:28:08 - train: epoch 0032, iter [03800, 05004], lr: 0.077112, loss: 3.7576
2022-10-11 07:28:59 - train: epoch 0032, iter [03900, 05004], lr: 0.077086, loss: 4.3573
2022-10-11 07:29:48 - train: epoch 0032, iter [04000, 05004], lr: 0.077059, loss: 4.6117
2022-10-11 07:30:38 - train: epoch 0032, iter [04100, 05004], lr: 0.077033, loss: 4.0014
2022-10-11 07:31:28 - train: epoch 0032, iter [04200, 05004], lr: 0.077006, loss: 4.3393
2022-10-11 07:32:18 - train: epoch 0032, iter [04300, 05004], lr: 0.076980, loss: 4.0905
2022-10-11 07:33:08 - train: epoch 0032, iter [04400, 05004], lr: 0.076954, loss: 4.0618
2022-10-11 07:34:00 - train: epoch 0032, iter [04500, 05004], lr: 0.076927, loss: 4.9654
2022-10-11 07:34:50 - train: epoch 0032, iter [04600, 05004], lr: 0.076901, loss: 4.1844
2022-10-11 07:35:39 - train: epoch 0032, iter [04700, 05004], lr: 0.076874, loss: 4.3397
2022-10-11 07:36:28 - train: epoch 0032, iter [04800, 05004], lr: 0.076848, loss: 4.0530
2022-10-11 07:37:18 - train: epoch 0032, iter [04900, 05004], lr: 0.076821, loss: 4.0564
2022-10-11 07:38:03 - train: epoch 0032, iter [05000, 05004], lr: 0.076795, loss: 4.1299
2022-10-11 07:38:06 - train: epoch 032, train_loss: 4.4253
2022-10-11 07:39:50 - eval: epoch: 032, acc1: 56.740%, acc5: 81.842%, test_loss: 2.0464, per_image_load_time: 2.597ms, per_image_inference_time: 0.494ms
2022-10-11 07:39:50 - until epoch: 032, best_acc1: 57.900%
2022-10-11 07:39:50 - epoch 033 lr: 0.076794
2022-10-11 07:40:47 - train: epoch 0033, iter [00100, 05004], lr: 0.076767, loss: 3.2876
2022-10-11 07:41:37 - train: epoch 0033, iter [00200, 05004], lr: 0.076741, loss: 4.2171
2022-10-11 07:42:28 - train: epoch 0033, iter [00300, 05004], lr: 0.076714, loss: 4.8182
2022-10-11 07:43:18 - train: epoch 0033, iter [00400, 05004], lr: 0.076688, loss: 4.6345
2022-10-11 07:44:08 - train: epoch 0033, iter [00500, 05004], lr: 0.076661, loss: 4.8269
2022-10-11 07:44:57 - train: epoch 0033, iter [00600, 05004], lr: 0.076634, loss: 4.2786
2022-10-11 07:45:47 - train: epoch 0033, iter [00700, 05004], lr: 0.076608, loss: 4.9102
2022-10-11 07:46:36 - train: epoch 0033, iter [00800, 05004], lr: 0.076581, loss: 4.8871
2022-10-11 07:47:26 - train: epoch 0033, iter [00900, 05004], lr: 0.076555, loss: 3.8619
2022-10-11 07:48:16 - train: epoch 0033, iter [01000, 05004], lr: 0.076528, loss: 4.1718
2022-10-11 07:49:05 - train: epoch 0033, iter [01100, 05004], lr: 0.076502, loss: 3.2136
2022-10-11 07:49:54 - train: epoch 0033, iter [01200, 05004], lr: 0.076475, loss: 4.3406
2022-10-11 07:50:45 - train: epoch 0033, iter [01300, 05004], lr: 0.076448, loss: 4.5203
2022-10-11 07:51:35 - train: epoch 0033, iter [01400, 05004], lr: 0.076422, loss: 4.4421
2022-10-11 07:52:24 - train: epoch 0033, iter [01500, 05004], lr: 0.076395, loss: 4.3298
2022-10-11 07:53:13 - train: epoch 0033, iter [01600, 05004], lr: 0.076368, loss: 4.2436
2022-10-11 07:54:02 - train: epoch 0033, iter [01700, 05004], lr: 0.076342, loss: 3.9464
2022-10-11 07:54:52 - train: epoch 0033, iter [01800, 05004], lr: 0.076315, loss: 4.0344
2022-10-11 07:55:42 - train: epoch 0033, iter [01900, 05004], lr: 0.076288, loss: 5.1379
2022-10-11 07:56:32 - train: epoch 0033, iter [02000, 05004], lr: 0.076262, loss: 4.3704
2022-10-11 07:57:21 - train: epoch 0033, iter [02100, 05004], lr: 0.076235, loss: 4.6004
2022-10-11 07:58:11 - train: epoch 0033, iter [02200, 05004], lr: 0.076208, loss: 4.7214
2022-10-11 07:59:01 - train: epoch 0033, iter [02300, 05004], lr: 0.076181, loss: 4.8671
2022-10-11 07:59:52 - train: epoch 0033, iter [02400, 05004], lr: 0.076155, loss: 4.1364
2022-10-11 08:00:42 - train: epoch 0033, iter [02500, 05004], lr: 0.076128, loss: 5.0246
2022-10-11 08:01:32 - train: epoch 0033, iter [02600, 05004], lr: 0.076101, loss: 3.6551
2022-10-11 08:02:20 - train: epoch 0033, iter [02700, 05004], lr: 0.076074, loss: 4.9402
2022-10-11 08:03:10 - train: epoch 0033, iter [02800, 05004], lr: 0.076048, loss: 4.9889
2022-10-11 08:04:00 - train: epoch 0033, iter [02900, 05004], lr: 0.076021, loss: 4.6229
2022-10-11 08:04:50 - train: epoch 0033, iter [03000, 05004], lr: 0.075994, loss: 3.8825
2022-10-11 08:05:39 - train: epoch 0033, iter [03100, 05004], lr: 0.075967, loss: 4.7049
2022-10-11 08:06:29 - train: epoch 0033, iter [03200, 05004], lr: 0.075940, loss: 4.8802
2022-10-11 08:07:18 - train: epoch 0033, iter [03300, 05004], lr: 0.075913, loss: 4.6090
2022-10-11 08:08:07 - train: epoch 0033, iter [03400, 05004], lr: 0.075887, loss: 3.9721
2022-10-11 08:08:59 - train: epoch 0033, iter [03500, 05004], lr: 0.075860, loss: 4.6839
2022-10-11 08:09:47 - train: epoch 0033, iter [03600, 05004], lr: 0.075833, loss: 4.7979
2022-10-11 08:10:38 - train: epoch 0033, iter [03700, 05004], lr: 0.075806, loss: 4.5747
2022-10-11 08:11:26 - train: epoch 0033, iter [03800, 05004], lr: 0.075779, loss: 4.9514
2022-10-11 08:12:16 - train: epoch 0033, iter [03900, 05004], lr: 0.075752, loss: 4.7971
2022-10-11 08:13:04 - train: epoch 0033, iter [04000, 05004], lr: 0.075725, loss: 4.5847
2022-10-11 08:13:53 - train: epoch 0033, iter [04100, 05004], lr: 0.075698, loss: 4.5012
2022-10-11 08:14:44 - train: epoch 0033, iter [04200, 05004], lr: 0.075671, loss: 3.8772
2022-10-11 08:15:34 - train: epoch 0033, iter [04300, 05004], lr: 0.075644, loss: 4.1695
2022-10-11 08:16:23 - train: epoch 0033, iter [04400, 05004], lr: 0.075618, loss: 4.5686
2022-10-11 08:17:13 - train: epoch 0033, iter [04500, 05004], lr: 0.075591, loss: 4.6582
2022-10-11 08:18:02 - train: epoch 0033, iter [04600, 05004], lr: 0.075564, loss: 3.7340
2022-10-11 08:18:51 - train: epoch 0033, iter [04700, 05004], lr: 0.075537, loss: 4.3321
2022-10-11 08:19:39 - train: epoch 0033, iter [04800, 05004], lr: 0.075510, loss: 5.0554
2022-10-11 08:20:30 - train: epoch 0033, iter [04900, 05004], lr: 0.075483, loss: 5.0134
2022-10-11 08:21:15 - train: epoch 0033, iter [05000, 05004], lr: 0.075456, loss: 3.8158
2022-10-11 08:21:17 - train: epoch 033, train_loss: 4.4203
2022-10-11 08:23:03 - eval: epoch: 033, acc1: 56.728%, acc5: 81.420%, test_loss: 2.0491, per_image_load_time: 3.234ms, per_image_inference_time: 0.480ms
2022-10-11 08:23:03 - until epoch: 033, best_acc1: 57.900%
2022-10-11 08:23:03 - epoch 034 lr: 0.075455
2022-10-11 08:23:59 - train: epoch 0034, iter [00100, 05004], lr: 0.075428, loss: 4.9850
2022-10-11 08:24:50 - train: epoch 0034, iter [00200, 05004], lr: 0.075400, loss: 4.0135
2022-10-11 08:25:40 - train: epoch 0034, iter [00300, 05004], lr: 0.075373, loss: 4.9680
2022-10-11 08:26:30 - train: epoch 0034, iter [00400, 05004], lr: 0.075346, loss: 4.1678
2022-10-11 08:27:18 - train: epoch 0034, iter [00500, 05004], lr: 0.075319, loss: 4.4802
2022-10-11 08:28:07 - train: epoch 0034, iter [00600, 05004], lr: 0.075292, loss: 4.8149
2022-10-11 08:28:57 - train: epoch 0034, iter [00700, 05004], lr: 0.075265, loss: 4.7597
2022-10-11 08:29:44 - train: epoch 0034, iter [00800, 05004], lr: 0.075238, loss: 4.1764
2022-10-11 08:30:34 - train: epoch 0034, iter [00900, 05004], lr: 0.075211, loss: 5.2296
2022-10-11 08:31:22 - train: epoch 0034, iter [01000, 05004], lr: 0.075184, loss: 4.0770
2022-10-11 08:32:11 - train: epoch 0034, iter [01100, 05004], lr: 0.075157, loss: 4.8347
2022-10-11 08:33:01 - train: epoch 0034, iter [01200, 05004], lr: 0.075130, loss: 4.2340
2022-10-11 08:33:49 - train: epoch 0034, iter [01300, 05004], lr: 0.075102, loss: 4.3940
2022-10-11 08:34:37 - train: epoch 0034, iter [01400, 05004], lr: 0.075075, loss: 3.9933
2022-10-11 08:35:26 - train: epoch 0034, iter [01500, 05004], lr: 0.075048, loss: 5.1369
2022-10-11 08:36:14 - train: epoch 0034, iter [01600, 05004], lr: 0.075021, loss: 4.5096
2022-10-11 08:37:03 - train: epoch 0034, iter [01700, 05004], lr: 0.074994, loss: 4.6038
2022-10-11 08:37:52 - train: epoch 0034, iter [01800, 05004], lr: 0.074967, loss: 4.3168
2022-10-11 08:38:41 - train: epoch 0034, iter [01900, 05004], lr: 0.074939, loss: 4.1966
2022-10-11 08:39:29 - train: epoch 0034, iter [02000, 05004], lr: 0.074912, loss: 3.8856
2022-10-11 08:40:19 - train: epoch 0034, iter [02100, 05004], lr: 0.074885, loss: 5.1770
2022-10-11 08:41:07 - train: epoch 0034, iter [02200, 05004], lr: 0.074858, loss: 4.0408
2022-10-11 08:41:55 - train: epoch 0034, iter [02300, 05004], lr: 0.074831, loss: 4.4196
2022-10-11 08:42:44 - train: epoch 0034, iter [02400, 05004], lr: 0.074803, loss: 3.8954
2022-10-11 08:43:31 - train: epoch 0034, iter [02500, 05004], lr: 0.074776, loss: 4.8508
2022-10-11 08:44:20 - train: epoch 0034, iter [02600, 05004], lr: 0.074749, loss: 5.2212
2022-10-11 08:45:08 - train: epoch 0034, iter [02700, 05004], lr: 0.074721, loss: 4.8865
2022-10-11 08:45:56 - train: epoch 0034, iter [02800, 05004], lr: 0.074694, loss: 3.4401
2022-10-11 08:46:45 - train: epoch 0034, iter [02900, 05004], lr: 0.074667, loss: 4.5759
2022-10-11 08:47:34 - train: epoch 0034, iter [03000, 05004], lr: 0.074640, loss: 4.0046
2022-10-11 08:48:22 - train: epoch 0034, iter [03100, 05004], lr: 0.074612, loss: 4.4224
2022-10-11 08:49:10 - train: epoch 0034, iter [03200, 05004], lr: 0.074585, loss: 4.3529
2022-10-11 08:49:59 - train: epoch 0034, iter [03300, 05004], lr: 0.074558, loss: 4.4426
2022-10-11 08:50:48 - train: epoch 0034, iter [03400, 05004], lr: 0.074530, loss: 4.2486
2022-10-11 08:51:37 - train: epoch 0034, iter [03500, 05004], lr: 0.074503, loss: 4.1058
2022-10-11 08:52:28 - train: epoch 0034, iter [03600, 05004], lr: 0.074476, loss: 4.7721
2022-10-11 08:53:17 - train: epoch 0034, iter [03700, 05004], lr: 0.074448, loss: 3.9574
2022-10-11 08:54:05 - train: epoch 0034, iter [03800, 05004], lr: 0.074421, loss: 4.9778
2022-10-11 08:54:56 - train: epoch 0034, iter [03900, 05004], lr: 0.074393, loss: 4.6446
2022-10-11 08:55:46 - train: epoch 0034, iter [04000, 05004], lr: 0.074366, loss: 3.9054
2022-10-11 08:56:37 - train: epoch 0034, iter [04100, 05004], lr: 0.074339, loss: 5.0168
2022-10-11 08:57:27 - train: epoch 0034, iter [04200, 05004], lr: 0.074311, loss: 4.6440
2022-10-11 08:58:16 - train: epoch 0034, iter [04300, 05004], lr: 0.074284, loss: 3.8735
2022-10-11 08:59:08 - train: epoch 0034, iter [04400, 05004], lr: 0.074256, loss: 4.6976
2022-10-11 08:59:58 - train: epoch 0034, iter [04500, 05004], lr: 0.074229, loss: 4.2254
2022-10-11 09:00:47 - train: epoch 0034, iter [04600, 05004], lr: 0.074201, loss: 4.5707
2022-10-11 09:01:37 - train: epoch 0034, iter [04700, 05004], lr: 0.074174, loss: 4.8716
2022-10-11 09:02:27 - train: epoch 0034, iter [04800, 05004], lr: 0.074146, loss: 4.2480
2022-10-11 09:03:17 - train: epoch 0034, iter [04900, 05004], lr: 0.074119, loss: 4.7364
2022-10-11 09:04:05 - train: epoch 0034, iter [05000, 05004], lr: 0.074091, loss: 4.0556
2022-10-11 09:04:07 - train: epoch 034, train_loss: 4.4238
2022-10-11 09:05:52 - eval: epoch: 034, acc1: 53.200%, acc5: 78.230%, test_loss: 2.2996, per_image_load_time: 3.243ms, per_image_inference_time: 0.516ms
2022-10-11 09:05:53 - until epoch: 034, best_acc1: 57.900%
2022-10-11 09:05:53 - epoch 035 lr: 0.074090
2022-10-11 09:06:49 - train: epoch 0035, iter [00100, 05004], lr: 0.074063, loss: 4.6071
2022-10-11 09:07:40 - train: epoch 0035, iter [00200, 05004], lr: 0.074035, loss: 4.4608
2022-10-11 09:08:30 - train: epoch 0035, iter [00300, 05004], lr: 0.074008, loss: 4.3521
2022-10-11 09:09:21 - train: epoch 0035, iter [00400, 05004], lr: 0.073980, loss: 4.0401
2022-10-11 09:10:10 - train: epoch 0035, iter [00500, 05004], lr: 0.073953, loss: 4.0086
2022-10-11 09:10:58 - train: epoch 0035, iter [00600, 05004], lr: 0.073925, loss: 4.0420
2022-10-11 09:11:48 - train: epoch 0035, iter [00700, 05004], lr: 0.073898, loss: 3.3705
2022-10-11 09:12:38 - train: epoch 0035, iter [00800, 05004], lr: 0.073870, loss: 4.8015
2022-10-11 09:13:28 - train: epoch 0035, iter [00900, 05004], lr: 0.073842, loss: 4.3894
2022-10-11 09:14:17 - train: epoch 0035, iter [01000, 05004], lr: 0.073815, loss: 4.0406
2022-10-11 09:15:08 - train: epoch 0035, iter [01100, 05004], lr: 0.073787, loss: 4.4420
2022-10-11 09:15:58 - train: epoch 0035, iter [01200, 05004], lr: 0.073760, loss: 4.5475
2022-10-11 09:16:47 - train: epoch 0035, iter [01300, 05004], lr: 0.073732, loss: 4.0135
2022-10-11 09:17:38 - train: epoch 0035, iter [01400, 05004], lr: 0.073704, loss: 4.8724
2022-10-11 09:18:29 - train: epoch 0035, iter [01500, 05004], lr: 0.073677, loss: 4.1684
2022-10-11 09:19:18 - train: epoch 0035, iter [01600, 05004], lr: 0.073649, loss: 4.3206
2022-10-11 09:20:09 - train: epoch 0035, iter [01700, 05004], lr: 0.073621, loss: 4.1779
2022-10-11 09:21:00 - train: epoch 0035, iter [01800, 05004], lr: 0.073594, loss: 4.4513
2022-10-11 09:21:51 - train: epoch 0035, iter [01900, 05004], lr: 0.073566, loss: 4.5767
2022-10-11 09:22:39 - train: epoch 0035, iter [02000, 05004], lr: 0.073538, loss: 3.8058
2022-10-11 09:23:30 - train: epoch 0035, iter [02100, 05004], lr: 0.073511, loss: 4.6325
2022-10-11 09:24:20 - train: epoch 0035, iter [02200, 05004], lr: 0.073483, loss: 4.7995
2022-10-11 09:25:08 - train: epoch 0035, iter [02300, 05004], lr: 0.073455, loss: 4.3072
2022-10-11 09:25:59 - train: epoch 0035, iter [02400, 05004], lr: 0.073427, loss: 4.7910
2022-10-11 09:26:48 - train: epoch 0035, iter [02500, 05004], lr: 0.073400, loss: 4.4245
2022-10-11 09:27:39 - train: epoch 0035, iter [02600, 05004], lr: 0.073372, loss: 4.6443
2022-10-11 09:28:29 - train: epoch 0035, iter [02700, 05004], lr: 0.073344, loss: 3.2319
2022-10-11 09:29:20 - train: epoch 0035, iter [02800, 05004], lr: 0.073316, loss: 4.7946
2022-10-11 09:30:12 - train: epoch 0035, iter [02900, 05004], lr: 0.073289, loss: 4.2880
2022-10-11 09:30:59 - train: epoch 0035, iter [03000, 05004], lr: 0.073261, loss: 3.5037
2022-10-11 09:31:51 - train: epoch 0035, iter [03100, 05004], lr: 0.073233, loss: 3.7790
2022-10-11 09:32:43 - train: epoch 0035, iter [03200, 05004], lr: 0.073205, loss: 3.6467
2022-10-11 09:33:31 - train: epoch 0035, iter [03300, 05004], lr: 0.073177, loss: 4.6876
2022-10-11 09:34:20 - train: epoch 0035, iter [03400, 05004], lr: 0.073150, loss: 4.5367
2022-10-11 09:35:11 - train: epoch 0035, iter [03500, 05004], lr: 0.073122, loss: 4.3159
2022-10-11 09:36:02 - train: epoch 0035, iter [03600, 05004], lr: 0.073094, loss: 4.7805
2022-10-11 09:36:50 - train: epoch 0035, iter [03700, 05004], lr: 0.073066, loss: 4.0913
2022-10-11 09:37:40 - train: epoch 0035, iter [03800, 05004], lr: 0.073038, loss: 4.0550
2022-10-11 09:38:31 - train: epoch 0035, iter [03900, 05004], lr: 0.073010, loss: 4.7505
2022-10-11 09:39:19 - train: epoch 0035, iter [04000, 05004], lr: 0.072983, loss: 3.9230
2022-10-11 09:40:11 - train: epoch 0035, iter [04100, 05004], lr: 0.072955, loss: 4.5702
2022-10-11 09:41:01 - train: epoch 0035, iter [04200, 05004], lr: 0.072927, loss: 4.6646
2022-10-11 09:41:51 - train: epoch 0035, iter [04300, 05004], lr: 0.072899, loss: 4.2970
2022-10-11 09:42:41 - train: epoch 0035, iter [04400, 05004], lr: 0.072871, loss: 4.9903
2022-10-11 09:43:31 - train: epoch 0035, iter [04500, 05004], lr: 0.072843, loss: 4.5862
2022-10-11 09:44:23 - train: epoch 0035, iter [04600, 05004], lr: 0.072815, loss: 3.9259
2022-10-11 09:45:12 - train: epoch 0035, iter [04700, 05004], lr: 0.072787, loss: 4.3867
2022-10-11 09:46:02 - train: epoch 0035, iter [04800, 05004], lr: 0.072759, loss: 5.0421
2022-10-11 09:46:55 - train: epoch 0035, iter [04900, 05004], lr: 0.072731, loss: 4.0571
2022-10-11 09:47:42 - train: epoch 0035, iter [05000, 05004], lr: 0.072703, loss: 4.1467
2022-10-11 09:47:45 - train: epoch 035, train_loss: 4.4049
2022-10-11 09:49:31 - eval: epoch: 035, acc1: 56.410%, acc5: 80.876%, test_loss: 2.0753, per_image_load_time: 2.951ms, per_image_inference_time: 0.500ms
2022-10-11 09:49:31 - until epoch: 035, best_acc1: 57.900%
2022-10-11 09:49:31 - epoch 036 lr: 0.072702
2022-10-11 09:50:27 - train: epoch 0036, iter [00100, 05004], lr: 0.072674, loss: 3.9153
2022-10-11 09:51:19 - train: epoch 0036, iter [00200, 05004], lr: 0.072646, loss: 3.6310
2022-10-11 09:52:08 - train: epoch 0036, iter [00300, 05004], lr: 0.072618, loss: 3.7813
2022-10-11 09:52:58 - train: epoch 0036, iter [00400, 05004], lr: 0.072590, loss: 4.3414
2022-10-11 09:53:47 - train: epoch 0036, iter [00500, 05004], lr: 0.072562, loss: 4.5206
2022-10-11 09:54:38 - train: epoch 0036, iter [00600, 05004], lr: 0.072534, loss: 4.9102
2022-10-11 09:55:28 - train: epoch 0036, iter [00700, 05004], lr: 0.072506, loss: 4.2314
2022-10-11 09:56:18 - train: epoch 0036, iter [00800, 05004], lr: 0.072478, loss: 4.6141
2022-10-11 09:57:06 - train: epoch 0036, iter [00900, 05004], lr: 0.072450, loss: 4.6329
2022-10-11 09:57:56 - train: epoch 0036, iter [01000, 05004], lr: 0.072422, loss: 4.8319
2022-10-11 09:58:47 - train: epoch 0036, iter [01100, 05004], lr: 0.072394, loss: 3.7342
2022-10-11 09:59:37 - train: epoch 0036, iter [01200, 05004], lr: 0.072366, loss: 4.2363
2022-10-11 10:00:27 - train: epoch 0036, iter [01300, 05004], lr: 0.072338, loss: 4.8312
2022-10-11 10:01:16 - train: epoch 0036, iter [01400, 05004], lr: 0.072310, loss: 4.2432
2022-10-11 10:02:06 - train: epoch 0036, iter [01500, 05004], lr: 0.072282, loss: 4.6916
2022-10-11 10:02:56 - train: epoch 0036, iter [01600, 05004], lr: 0.072254, loss: 4.5397
2022-10-11 10:03:45 - train: epoch 0036, iter [01700, 05004], lr: 0.072226, loss: 3.6598
2022-10-11 10:04:36 - train: epoch 0036, iter [01800, 05004], lr: 0.072197, loss: 3.9516
2022-10-11 10:05:26 - train: epoch 0036, iter [01900, 05004], lr: 0.072169, loss: 4.1372
2022-10-11 10:06:16 - train: epoch 0036, iter [02000, 05004], lr: 0.072141, loss: 4.3900
2022-10-11 10:07:05 - train: epoch 0036, iter [02100, 05004], lr: 0.072113, loss: 4.5998
2022-10-11 10:07:56 - train: epoch 0036, iter [02200, 05004], lr: 0.072085, loss: 4.4664
2022-10-11 10:08:45 - train: epoch 0036, iter [02300, 05004], lr: 0.072057, loss: 4.6023
2022-10-11 10:09:35 - train: epoch 0036, iter [02400, 05004], lr: 0.072029, loss: 4.2415
2022-10-11 10:10:25 - train: epoch 0036, iter [02500, 05004], lr: 0.072000, loss: 4.2646
2022-10-11 10:11:16 - train: epoch 0036, iter [02600, 05004], lr: 0.071972, loss: 4.6090
2022-10-11 10:12:05 - train: epoch 0036, iter [02700, 05004], lr: 0.071944, loss: 4.2435
2022-10-11 10:12:56 - train: epoch 0036, iter [02800, 05004], lr: 0.071916, loss: 4.8588
2022-10-11 10:13:45 - train: epoch 0036, iter [02900, 05004], lr: 0.071888, loss: 3.2702
2022-10-11 10:14:35 - train: epoch 0036, iter [03000, 05004], lr: 0.071859, loss: 3.6048
2022-10-11 10:15:25 - train: epoch 0036, iter [03100, 05004], lr: 0.071831, loss: 3.7502
2022-10-11 10:16:16 - train: epoch 0036, iter [03200, 05004], lr: 0.071803, loss: 4.2106
2022-10-11 10:17:06 - train: epoch 0036, iter [03300, 05004], lr: 0.071775, loss: 4.2696
2022-10-11 10:17:56 - train: epoch 0036, iter [03400, 05004], lr: 0.071746, loss: 4.4986
2022-10-11 10:18:46 - train: epoch 0036, iter [03500, 05004], lr: 0.071718, loss: 4.0693
2022-10-11 10:19:36 - train: epoch 0036, iter [03600, 05004], lr: 0.071690, loss: 4.5643
2022-10-11 10:20:26 - train: epoch 0036, iter [03700, 05004], lr: 0.071661, loss: 5.0345
2022-10-11 10:21:15 - train: epoch 0036, iter [03800, 05004], lr: 0.071633, loss: 4.7509
2022-10-11 10:22:05 - train: epoch 0036, iter [03900, 05004], lr: 0.071605, loss: 4.3085
2022-10-11 10:22:55 - train: epoch 0036, iter [04000, 05004], lr: 0.071577, loss: 4.3113
2022-10-11 10:23:44 - train: epoch 0036, iter [04100, 05004], lr: 0.071548, loss: 4.1426
2022-10-11 10:24:34 - train: epoch 0036, iter [04200, 05004], lr: 0.071520, loss: 4.7565
2022-10-11 10:25:23 - train: epoch 0036, iter [04300, 05004], lr: 0.071492, loss: 3.6994
2022-10-11 10:26:12 - train: epoch 0036, iter [04400, 05004], lr: 0.071463, loss: 4.5622
2022-10-11 10:27:02 - train: epoch 0036, iter [04500, 05004], lr: 0.071435, loss: 4.3296
2022-10-11 10:27:52 - train: epoch 0036, iter [04600, 05004], lr: 0.071407, loss: 4.6087
2022-10-11 10:28:42 - train: epoch 0036, iter [04700, 05004], lr: 0.071378, loss: 4.7067
2022-10-11 10:29:33 - train: epoch 0036, iter [04800, 05004], lr: 0.071350, loss: 4.6185
2022-10-11 10:30:23 - train: epoch 0036, iter [04900, 05004], lr: 0.071321, loss: 4.0133
2022-10-11 10:31:09 - train: epoch 0036, iter [05000, 05004], lr: 0.071293, loss: 3.9702
2022-10-11 10:31:12 - train: epoch 036, train_loss: 4.4015
2022-10-11 10:32:58 - eval: epoch: 036, acc1: 55.838%, acc5: 80.524%, test_loss: 2.1088, per_image_load_time: 3.458ms, per_image_inference_time: 0.506ms
2022-10-11 10:32:59 - until epoch: 036, best_acc1: 57.900%
2022-10-11 10:32:59 - epoch 037 lr: 0.071292
2022-10-11 10:33:55 - train: epoch 0037, iter [00100, 05004], lr: 0.071263, loss: 3.9046
2022-10-11 10:34:46 - train: epoch 0037, iter [00200, 05004], lr: 0.071235, loss: 3.6394
2022-10-11 10:35:36 - train: epoch 0037, iter [00300, 05004], lr: 0.071207, loss: 4.4846
2022-10-11 10:36:26 - train: epoch 0037, iter [00400, 05004], lr: 0.071178, loss: 4.5940
2022-10-11 10:37:15 - train: epoch 0037, iter [00500, 05004], lr: 0.071150, loss: 4.1407
2022-10-11 10:38:05 - train: epoch 0037, iter [00600, 05004], lr: 0.071121, loss: 3.8563
2022-10-11 10:38:56 - train: epoch 0037, iter [00700, 05004], lr: 0.071093, loss: 4.5221
2022-10-11 10:39:46 - train: epoch 0037, iter [00800, 05004], lr: 0.071064, loss: 3.4341
2022-10-11 10:40:36 - train: epoch 0037, iter [00900, 05004], lr: 0.071036, loss: 4.9682
2022-10-11 10:41:25 - train: epoch 0037, iter [01000, 05004], lr: 0.071007, loss: 3.7896
2022-10-11 10:42:15 - train: epoch 0037, iter [01100, 05004], lr: 0.070979, loss: 4.5651
2022-10-11 10:43:05 - train: epoch 0037, iter [01200, 05004], lr: 0.070950, loss: 4.1056
2022-10-11 10:43:54 - train: epoch 0037, iter [01300, 05004], lr: 0.070922, loss: 4.6584
2022-10-11 10:44:44 - train: epoch 0037, iter [01400, 05004], lr: 0.070893, loss: 4.2744
2022-10-11 10:45:33 - train: epoch 0037, iter [01500, 05004], lr: 0.070865, loss: 3.5822
2022-10-11 10:46:24 - train: epoch 0037, iter [01600, 05004], lr: 0.070836, loss: 4.5391
2022-10-11 10:47:14 - train: epoch 0037, iter [01700, 05004], lr: 0.070808, loss: 4.7331
2022-10-11 10:48:03 - train: epoch 0037, iter [01800, 05004], lr: 0.070779, loss: 4.1188
2022-10-11 10:48:52 - train: epoch 0037, iter [01900, 05004], lr: 0.070751, loss: 4.7699
2022-10-11 10:49:43 - train: epoch 0037, iter [02000, 05004], lr: 0.070722, loss: 4.6611
2022-10-11 10:50:33 - train: epoch 0037, iter [02100, 05004], lr: 0.070694, loss: 4.4919
2022-10-11 10:51:22 - train: epoch 0037, iter [02200, 05004], lr: 0.070665, loss: 3.9750
2022-10-11 10:52:11 - train: epoch 0037, iter [02300, 05004], lr: 0.070636, loss: 4.8334
2022-10-11 10:53:00 - train: epoch 0037, iter [02400, 05004], lr: 0.070608, loss: 3.5204
2022-10-11 10:53:49 - train: epoch 0037, iter [02500, 05004], lr: 0.070579, loss: 4.3632
2022-10-11 10:54:39 - train: epoch 0037, iter [02600, 05004], lr: 0.070551, loss: 4.2367
2022-10-11 10:55:29 - train: epoch 0037, iter [02700, 05004], lr: 0.070522, loss: 4.7003
2022-10-11 10:56:19 - train: epoch 0037, iter [02800, 05004], lr: 0.070493, loss: 4.7154
2022-10-11 10:57:08 - train: epoch 0037, iter [02900, 05004], lr: 0.070465, loss: 4.3170
2022-10-11 10:57:59 - train: epoch 0037, iter [03000, 05004], lr: 0.070436, loss: 4.6914
2022-10-11 10:58:49 - train: epoch 0037, iter [03100, 05004], lr: 0.070407, loss: 4.4896
2022-10-11 10:59:38 - train: epoch 0037, iter [03200, 05004], lr: 0.070379, loss: 4.0616
2022-10-11 11:00:30 - train: epoch 0037, iter [03300, 05004], lr: 0.070350, loss: 5.1764
2022-10-11 11:01:20 - train: epoch 0037, iter [03400, 05004], lr: 0.070321, loss: 5.0204
2022-10-11 11:02:10 - train: epoch 0037, iter [03500, 05004], lr: 0.070293, loss: 4.8457
2022-10-11 11:03:01 - train: epoch 0037, iter [03600, 05004], lr: 0.070264, loss: 3.9590
2022-10-11 11:03:51 - train: epoch 0037, iter [03700, 05004], lr: 0.070235, loss: 3.6158
2022-10-11 11:04:40 - train: epoch 0037, iter [03800, 05004], lr: 0.070207, loss: 4.8011
2022-10-11 11:05:30 - train: epoch 0037, iter [03900, 05004], lr: 0.070178, loss: 4.3716
2022-10-11 11:06:20 - train: epoch 0037, iter [04000, 05004], lr: 0.070149, loss: 5.3084
2022-10-11 11:07:09 - train: epoch 0037, iter [04100, 05004], lr: 0.070120, loss: 4.3188
2022-10-11 11:07:59 - train: epoch 0037, iter [04200, 05004], lr: 0.070092, loss: 4.3435
2022-10-11 11:08:48 - train: epoch 0037, iter [04300, 05004], lr: 0.070063, loss: 4.8751
2022-10-11 11:09:38 - train: epoch 0037, iter [04400, 05004], lr: 0.070034, loss: 4.5730
2022-10-11 11:10:29 - train: epoch 0037, iter [04500, 05004], lr: 0.070005, loss: 4.5317
2022-10-11 11:11:19 - train: epoch 0037, iter [04600, 05004], lr: 0.069977, loss: 4.1357
2022-10-11 11:12:08 - train: epoch 0037, iter [04700, 05004], lr: 0.069948, loss: 3.8146
2022-10-11 11:12:58 - train: epoch 0037, iter [04800, 05004], lr: 0.069919, loss: 3.9990
2022-10-11 11:13:48 - train: epoch 0037, iter [04900, 05004], lr: 0.069890, loss: 4.2277
2022-10-11 11:14:35 - train: epoch 0037, iter [05000, 05004], lr: 0.069862, loss: 4.6013
2022-10-11 11:14:38 - train: epoch 037, train_loss: 4.3870
2022-10-11 11:16:23 - eval: epoch: 037, acc1: 58.064%, acc5: 82.540%, test_loss: 2.0539, per_image_load_time: 3.294ms, per_image_inference_time: 0.532ms
2022-10-11 11:16:23 - until epoch: 037, best_acc1: 58.064%
2022-10-11 11:16:23 - epoch 038 lr: 0.069860
2022-10-11 11:17:20 - train: epoch 0038, iter [00100, 05004], lr: 0.069832, loss: 4.0862
2022-10-11 11:18:12 - train: epoch 0038, iter [00200, 05004], lr: 0.069803, loss: 4.5616
2022-10-11 11:19:01 - train: epoch 0038, iter [00300, 05004], lr: 0.069774, loss: 4.7555
2022-10-11 11:19:51 - train: epoch 0038, iter [00400, 05004], lr: 0.069745, loss: 4.4830
2022-10-11 11:20:42 - train: epoch 0038, iter [00500, 05004], lr: 0.069716, loss: 3.4958
2022-10-11 11:21:31 - train: epoch 0038, iter [00600, 05004], lr: 0.069687, loss: 4.4972
2022-10-11 11:22:22 - train: epoch 0038, iter [00700, 05004], lr: 0.069659, loss: 4.2673
2022-10-11 11:23:12 - train: epoch 0038, iter [00800, 05004], lr: 0.069630, loss: 4.5225
2022-10-11 11:24:02 - train: epoch 0038, iter [00900, 05004], lr: 0.069601, loss: 4.3771
2022-10-11 11:24:52 - train: epoch 0038, iter [01000, 05004], lr: 0.069572, loss: 4.9942
2022-10-11 11:25:42 - train: epoch 0038, iter [01100, 05004], lr: 0.069543, loss: 4.8171
2022-10-11 11:26:30 - train: epoch 0038, iter [01200, 05004], lr: 0.069514, loss: 3.9982
2022-10-11 11:27:22 - train: epoch 0038, iter [01300, 05004], lr: 0.069485, loss: 4.7845
2022-10-11 11:28:11 - train: epoch 0038, iter [01400, 05004], lr: 0.069456, loss: 4.1325
2022-10-11 11:29:00 - train: epoch 0038, iter [01500, 05004], lr: 0.069427, loss: 3.9344
2022-10-11 11:29:51 - train: epoch 0038, iter [01600, 05004], lr: 0.069399, loss: 4.8029
2022-10-11 11:30:40 - train: epoch 0038, iter [01700, 05004], lr: 0.069370, loss: 4.6208
2022-10-11 11:31:30 - train: epoch 0038, iter [01800, 05004], lr: 0.069341, loss: 4.7883
2022-10-11 11:32:19 - train: epoch 0038, iter [01900, 05004], lr: 0.069312, loss: 4.9029
2022-10-11 11:33:09 - train: epoch 0038, iter [02000, 05004], lr: 0.069283, loss: 4.9708
2022-10-11 11:33:59 - train: epoch 0038, iter [02100, 05004], lr: 0.069254, loss: 4.3918
2022-10-11 11:34:49 - train: epoch 0038, iter [02200, 05004], lr: 0.069225, loss: 4.6127
2022-10-11 11:35:38 - train: epoch 0038, iter [02300, 05004], lr: 0.069196, loss: 3.9429
2022-10-11 11:36:28 - train: epoch 0038, iter [02400, 05004], lr: 0.069167, loss: 4.7336
2022-10-11 11:37:18 - train: epoch 0038, iter [02500, 05004], lr: 0.069138, loss: 4.2652
2022-10-11 11:38:08 - train: epoch 0038, iter [02600, 05004], lr: 0.069109, loss: 3.6793
2022-10-11 11:38:57 - train: epoch 0038, iter [02700, 05004], lr: 0.069080, loss: 3.3449
2022-10-11 11:39:47 - train: epoch 0038, iter [02800, 05004], lr: 0.069051, loss: 4.3489
2022-10-11 11:40:37 - train: epoch 0038, iter [02900, 05004], lr: 0.069022, loss: 4.5229
2022-10-11 11:41:26 - train: epoch 0038, iter [03000, 05004], lr: 0.068993, loss: 3.8055
2022-10-11 11:42:16 - train: epoch 0038, iter [03100, 05004], lr: 0.068964, loss: 4.1410
2022-10-11 11:43:07 - train: epoch 0038, iter [03200, 05004], lr: 0.068935, loss: 4.4955
2022-10-11 11:43:58 - train: epoch 0038, iter [03300, 05004], lr: 0.068906, loss: 4.4947
2022-10-11 11:44:47 - train: epoch 0038, iter [03400, 05004], lr: 0.068877, loss: 5.1308
2022-10-11 11:45:36 - train: epoch 0038, iter [03500, 05004], lr: 0.068847, loss: 4.3247
2022-10-11 11:46:26 - train: epoch 0038, iter [03600, 05004], lr: 0.068818, loss: 4.3783
2022-10-11 11:47:15 - train: epoch 0038, iter [03700, 05004], lr: 0.068789, loss: 4.2021
2022-10-11 11:48:03 - train: epoch 0038, iter [03800, 05004], lr: 0.068760, loss: 4.0249
2022-10-11 11:48:52 - train: epoch 0038, iter [03900, 05004], lr: 0.068731, loss: 3.9481
2022-10-11 11:49:42 - train: epoch 0038, iter [04000, 05004], lr: 0.068702, loss: 4.6028
2022-10-11 11:50:31 - train: epoch 0038, iter [04100, 05004], lr: 0.068673, loss: 3.4313
2022-10-11 11:51:20 - train: epoch 0038, iter [04200, 05004], lr: 0.068644, loss: 4.6214
2022-10-11 11:52:09 - train: epoch 0038, iter [04300, 05004], lr: 0.068615, loss: 4.6897
2022-10-11 11:52:59 - train: epoch 0038, iter [04400, 05004], lr: 0.068586, loss: 4.4560
2022-10-11 11:53:50 - train: epoch 0038, iter [04500, 05004], lr: 0.068556, loss: 4.3981
2022-10-11 11:54:39 - train: epoch 0038, iter [04600, 05004], lr: 0.068527, loss: 4.6357
2022-10-11 11:55:29 - train: epoch 0038, iter [04700, 05004], lr: 0.068498, loss: 4.2423
2022-10-11 11:56:18 - train: epoch 0038, iter [04800, 05004], lr: 0.068469, loss: 4.5278
2022-10-11 11:57:06 - train: epoch 0038, iter [04900, 05004], lr: 0.068440, loss: 4.5642
2022-10-11 11:57:53 - train: epoch 0038, iter [05000, 05004], lr: 0.068411, loss: 3.6522
2022-10-11 11:57:55 - train: epoch 038, train_loss: 4.3805
2022-10-11 11:59:40 - eval: epoch: 038, acc1: 59.062%, acc5: 82.976%, test_loss: 1.9441, per_image_load_time: 3.249ms, per_image_inference_time: 0.520ms
2022-10-11 11:59:41 - until epoch: 038, best_acc1: 59.062%
2022-10-11 11:59:41 - epoch 039 lr: 0.068409
2022-10-11 12:00:38 - train: epoch 0039, iter [00100, 05004], lr: 0.068380, loss: 4.3233
2022-10-11 12:01:27 - train: epoch 0039, iter [00200, 05004], lr: 0.068351, loss: 4.2208
2022-10-11 12:02:17 - train: epoch 0039, iter [00300, 05004], lr: 0.068322, loss: 3.0249
2022-10-11 12:03:06 - train: epoch 0039, iter [00400, 05004], lr: 0.068293, loss: 4.7498
2022-10-11 12:03:55 - train: epoch 0039, iter [00500, 05004], lr: 0.068263, loss: 4.3050
2022-10-11 12:04:44 - train: epoch 0039, iter [00600, 05004], lr: 0.068234, loss: 3.3301
2022-10-11 12:05:33 - train: epoch 0039, iter [00700, 05004], lr: 0.068205, loss: 4.2205
2022-10-11 12:06:23 - train: epoch 0039, iter [00800, 05004], lr: 0.068176, loss: 3.7694
2022-10-11 12:07:13 - train: epoch 0039, iter [00900, 05004], lr: 0.068146, loss: 4.3686
2022-10-11 12:08:03 - train: epoch 0039, iter [01000, 05004], lr: 0.068117, loss: 4.6600
2022-10-11 12:08:54 - train: epoch 0039, iter [01100, 05004], lr: 0.068088, loss: 4.6955
2022-10-11 12:09:42 - train: epoch 0039, iter [01200, 05004], lr: 0.068059, loss: 4.2103
2022-10-11 12:10:31 - train: epoch 0039, iter [01300, 05004], lr: 0.068029, loss: 3.4384
2022-10-11 12:11:21 - train: epoch 0039, iter [01400, 05004], lr: 0.068000, loss: 4.4059
2022-10-11 12:12:10 - train: epoch 0039, iter [01500, 05004], lr: 0.067971, loss: 4.4728
2022-10-11 12:13:00 - train: epoch 0039, iter [01600, 05004], lr: 0.067942, loss: 4.9095
2022-10-11 12:13:48 - train: epoch 0039, iter [01700, 05004], lr: 0.067912, loss: 4.1942
2022-10-11 12:14:37 - train: epoch 0039, iter [01800, 05004], lr: 0.067883, loss: 5.1201
2022-10-11 12:15:27 - train: epoch 0039, iter [01900, 05004], lr: 0.067854, loss: 3.9604
2022-10-11 12:16:18 - train: epoch 0039, iter [02000, 05004], lr: 0.067824, loss: 4.5360
2022-10-11 12:17:08 - train: epoch 0039, iter [02100, 05004], lr: 0.067795, loss: 4.3189
2022-10-11 12:17:57 - train: epoch 0039, iter [02200, 05004], lr: 0.067766, loss: 5.1949
2022-10-11 12:18:45 - train: epoch 0039, iter [02300, 05004], lr: 0.067736, loss: 4.4877
2022-10-11 12:19:36 - train: epoch 0039, iter [02400, 05004], lr: 0.067707, loss: 4.5557
2022-10-11 12:20:26 - train: epoch 0039, iter [02500, 05004], lr: 0.067678, loss: 3.9088
2022-10-11 12:21:17 - train: epoch 0039, iter [02600, 05004], lr: 0.067648, loss: 3.7396
2022-10-11 12:22:05 - train: epoch 0039, iter [02700, 05004], lr: 0.067619, loss: 3.5702
2022-10-11 12:22:58 - train: epoch 0039, iter [02800, 05004], lr: 0.067589, loss: 4.5465
2022-10-11 12:23:49 - train: epoch 0039, iter [02900, 05004], lr: 0.067560, loss: 4.2889
2022-10-11 12:24:38 - train: epoch 0039, iter [03000, 05004], lr: 0.067531, loss: 3.4313
2022-10-11 12:25:27 - train: epoch 0039, iter [03100, 05004], lr: 0.067501, loss: 4.7454
2022-10-11 12:26:17 - train: epoch 0039, iter [03200, 05004], lr: 0.067472, loss: 3.7742
2022-10-11 12:27:05 - train: epoch 0039, iter [03300, 05004], lr: 0.067442, loss: 4.5560
2022-10-11 12:27:55 - train: epoch 0039, iter [03400, 05004], lr: 0.067413, loss: 4.6619
2022-10-11 12:28:45 - train: epoch 0039, iter [03500, 05004], lr: 0.067384, loss: 3.4869
2022-10-11 12:29:34 - train: epoch 0039, iter [03600, 05004], lr: 0.067354, loss: 4.7447
2022-10-11 12:30:24 - train: epoch 0039, iter [03700, 05004], lr: 0.067325, loss: 4.3291
2022-10-11 12:31:15 - train: epoch 0039, iter [03800, 05004], lr: 0.067295, loss: 4.5109
2022-10-11 12:32:04 - train: epoch 0039, iter [03900, 05004], lr: 0.067266, loss: 4.2214
2022-10-11 12:32:56 - train: epoch 0039, iter [04000, 05004], lr: 0.067236, loss: 4.6060
2022-10-11 12:33:43 - train: epoch 0039, iter [04100, 05004], lr: 0.067207, loss: 4.4865
2022-10-11 12:34:33 - train: epoch 0039, iter [04200, 05004], lr: 0.067177, loss: 4.5588
2022-10-11 12:35:23 - train: epoch 0039, iter [04300, 05004], lr: 0.067148, loss: 4.4648
2022-10-11 12:36:12 - train: epoch 0039, iter [04400, 05004], lr: 0.067118, loss: 4.7553
2022-10-11 12:37:02 - train: epoch 0039, iter [04500, 05004], lr: 0.067089, loss: 4.0006
2022-10-11 12:37:51 - train: epoch 0039, iter [04600, 05004], lr: 0.067059, loss: 4.5101
2022-10-11 12:38:41 - train: epoch 0039, iter [04700, 05004], lr: 0.067030, loss: 3.6139
2022-10-11 12:39:31 - train: epoch 0039, iter [04800, 05004], lr: 0.067000, loss: 4.7116
2022-10-11 12:40:21 - train: epoch 0039, iter [04900, 05004], lr: 0.066971, loss: 4.2410
2022-10-11 12:41:07 - train: epoch 0039, iter [05000, 05004], lr: 0.066941, loss: 4.1355
2022-10-11 12:41:09 - train: epoch 039, train_loss: 4.3696
2022-10-11 12:42:53 - eval: epoch: 039, acc1: 58.520%, acc5: 82.248%, test_loss: 1.9851, per_image_load_time: 3.064ms, per_image_inference_time: 0.510ms
2022-10-11 12:42:53 - until epoch: 039, best_acc1: 59.062%
2022-10-11 12:42:53 - epoch 040 lr: 0.066940
2022-10-11 12:43:50 - train: epoch 0040, iter [00100, 05004], lr: 0.066911, loss: 4.3720
2022-10-11 12:44:39 - train: epoch 0040, iter [00200, 05004], lr: 0.066881, loss: 4.6709
2022-10-11 12:45:29 - train: epoch 0040, iter [00300, 05004], lr: 0.066852, loss: 4.9095
2022-10-11 12:46:18 - train: epoch 0040, iter [00400, 05004], lr: 0.066822, loss: 3.9934
2022-10-11 12:47:07 - train: epoch 0040, iter [00500, 05004], lr: 0.066792, loss: 4.6164
2022-10-11 12:47:56 - train: epoch 0040, iter [00600, 05004], lr: 0.066763, loss: 3.7375
2022-10-11 12:48:46 - train: epoch 0040, iter [00700, 05004], lr: 0.066733, loss: 4.9926
2022-10-11 12:49:35 - train: epoch 0040, iter [00800, 05004], lr: 0.066704, loss: 4.5358
2022-10-11 12:50:24 - train: epoch 0040, iter [00900, 05004], lr: 0.066674, loss: 4.5256
2022-10-11 12:51:13 - train: epoch 0040, iter [01000, 05004], lr: 0.066645, loss: 3.9559
2022-10-11 12:52:02 - train: epoch 0040, iter [01100, 05004], lr: 0.066615, loss: 4.4691
2022-10-11 12:52:53 - train: epoch 0040, iter [01200, 05004], lr: 0.066585, loss: 4.4060
2022-10-11 12:53:43 - train: epoch 0040, iter [01300, 05004], lr: 0.066556, loss: 3.9719
2022-10-11 12:54:32 - train: epoch 0040, iter [01400, 05004], lr: 0.066526, loss: 4.9485
2022-10-11 12:55:21 - train: epoch 0040, iter [01500, 05004], lr: 0.066496, loss: 4.1889
2022-10-11 12:56:11 - train: epoch 0040, iter [01600, 05004], lr: 0.066467, loss: 3.7630
2022-10-11 12:57:01 - train: epoch 0040, iter [01700, 05004], lr: 0.066437, loss: 4.6323
2022-10-11 12:57:49 - train: epoch 0040, iter [01800, 05004], lr: 0.066408, loss: 3.4205
2022-10-11 12:58:39 - train: epoch 0040, iter [01900, 05004], lr: 0.066378, loss: 3.8591
2022-10-11 12:59:28 - train: epoch 0040, iter [02000, 05004], lr: 0.066348, loss: 4.6353
2022-10-11 13:00:17 - train: epoch 0040, iter [02100, 05004], lr: 0.066319, loss: 4.1518
2022-10-11 13:01:06 - train: epoch 0040, iter [02200, 05004], lr: 0.066289, loss: 4.4227
2022-10-11 13:01:56 - train: epoch 0040, iter [02300, 05004], lr: 0.066259, loss: 4.7382
2022-10-11 13:02:44 - train: epoch 0040, iter [02400, 05004], lr: 0.066230, loss: 3.9480
2022-10-11 13:03:35 - train: epoch 0040, iter [02500, 05004], lr: 0.066200, loss: 4.6797
2022-10-11 13:04:25 - train: epoch 0040, iter [02600, 05004], lr: 0.066170, loss: 4.0457
2022-10-11 13:05:12 - train: epoch 0040, iter [02700, 05004], lr: 0.066140, loss: 3.8895
2022-10-11 13:06:02 - train: epoch 0040, iter [02800, 05004], lr: 0.066111, loss: 5.2642
2022-10-11 13:06:52 - train: epoch 0040, iter [02900, 05004], lr: 0.066081, loss: 4.1864
2022-10-11 13:07:41 - train: epoch 0040, iter [03000, 05004], lr: 0.066051, loss: 4.8523
2022-10-11 13:08:32 - train: epoch 0040, iter [03100, 05004], lr: 0.066022, loss: 4.6020
2022-10-11 13:09:20 - train: epoch 0040, iter [03200, 05004], lr: 0.065992, loss: 4.0596
2022-10-11 13:10:09 - train: epoch 0040, iter [03300, 05004], lr: 0.065962, loss: 4.0133
2022-10-11 13:10:59 - train: epoch 0040, iter [03400, 05004], lr: 0.065932, loss: 4.8190
2022-10-11 13:11:47 - train: epoch 0040, iter [03500, 05004], lr: 0.065903, loss: 3.8280
2022-10-11 13:12:36 - train: epoch 0040, iter [03600, 05004], lr: 0.065873, loss: 4.8291
2022-10-11 13:13:26 - train: epoch 0040, iter [03700, 05004], lr: 0.065843, loss: 3.9367
2022-10-11 13:14:17 - train: epoch 0040, iter [03800, 05004], lr: 0.065813, loss: 3.4048
2022-10-11 13:15:08 - train: epoch 0040, iter [03900, 05004], lr: 0.065783, loss: 3.6754
2022-10-11 13:15:57 - train: epoch 0040, iter [04000, 05004], lr: 0.065754, loss: 4.0392
2022-10-11 13:16:46 - train: epoch 0040, iter [04100, 05004], lr: 0.065724, loss: 3.5243
2022-10-11 13:17:34 - train: epoch 0040, iter [04200, 05004], lr: 0.065694, loss: 4.5849
2022-10-11 13:18:25 - train: epoch 0040, iter [04300, 05004], lr: 0.065664, loss: 4.8133
2022-10-11 13:19:15 - train: epoch 0040, iter [04400, 05004], lr: 0.065634, loss: 3.6093
2022-10-11 13:20:04 - train: epoch 0040, iter [04500, 05004], lr: 0.065605, loss: 4.6128
2022-10-11 13:20:53 - train: epoch 0040, iter [04600, 05004], lr: 0.065575, loss: 3.4440
2022-10-11 13:21:43 - train: epoch 0040, iter [04700, 05004], lr: 0.065545, loss: 4.4664
2022-10-11 13:22:32 - train: epoch 0040, iter [04800, 05004], lr: 0.065515, loss: 4.3891
2022-10-11 13:23:22 - train: epoch 0040, iter [04900, 05004], lr: 0.065485, loss: 4.1031
2022-10-11 13:24:07 - train: epoch 0040, iter [05000, 05004], lr: 0.065455, loss: 4.4951
2022-10-11 13:24:10 - train: epoch 040, train_loss: 4.3659
2022-10-11 13:25:57 - eval: epoch: 040, acc1: 59.112%, acc5: 83.054%, test_loss: 1.9650, per_image_load_time: 3.593ms, per_image_inference_time: 0.498ms
2022-10-11 13:25:58 - until epoch: 040, best_acc1: 59.112%
2022-10-11 13:25:58 - epoch 041 lr: 0.065454
2022-10-11 13:26:54 - train: epoch 0041, iter [00100, 05004], lr: 0.065424, loss: 5.2330
2022-10-11 13:27:44 - train: epoch 0041, iter [00200, 05004], lr: 0.065395, loss: 4.5369
2022-10-11 13:28:34 - train: epoch 0041, iter [00300, 05004], lr: 0.065365, loss: 5.0105
2022-10-11 13:29:23 - train: epoch 0041, iter [00400, 05004], lr: 0.065335, loss: 4.5432
2022-10-11 13:30:14 - train: epoch 0041, iter [00500, 05004], lr: 0.065305, loss: 4.8951
2022-10-11 13:31:05 - train: epoch 0041, iter [00600, 05004], lr: 0.065275, loss: 4.4968
2022-10-11 13:31:53 - train: epoch 0041, iter [00700, 05004], lr: 0.065245, loss: 4.5487
2022-10-11 13:32:43 - train: epoch 0041, iter [00800, 05004], lr: 0.065215, loss: 4.2428
2022-10-11 13:33:32 - train: epoch 0041, iter [00900, 05004], lr: 0.065185, loss: 3.4612
2022-10-11 13:34:21 - train: epoch 0041, iter [01000, 05004], lr: 0.065155, loss: 4.5272
2022-10-11 13:35:10 - train: epoch 0041, iter [01100, 05004], lr: 0.065126, loss: 3.6282
2022-10-11 13:36:00 - train: epoch 0041, iter [01200, 05004], lr: 0.065096, loss: 4.5423
2022-10-11 13:36:49 - train: epoch 0041, iter [01300, 05004], lr: 0.065066, loss: 4.4574
2022-10-11 13:37:39 - train: epoch 0041, iter [01400, 05004], lr: 0.065036, loss: 4.3791
2022-10-11 13:38:28 - train: epoch 0041, iter [01500, 05004], lr: 0.065006, loss: 4.7717
2022-10-11 13:39:18 - train: epoch 0041, iter [01600, 05004], lr: 0.064976, loss: 4.6713
2022-10-11 13:40:07 - train: epoch 0041, iter [01700, 05004], lr: 0.064946, loss: 4.6612
2022-10-11 13:40:56 - train: epoch 0041, iter [01800, 05004], lr: 0.064916, loss: 4.2851
2022-10-11 13:41:46 - train: epoch 0041, iter [01900, 05004], lr: 0.064886, loss: 4.9971
2022-10-11 13:42:35 - train: epoch 0041, iter [02000, 05004], lr: 0.064856, loss: 3.6254
2022-10-11 13:43:25 - train: epoch 0041, iter [02100, 05004], lr: 0.064826, loss: 3.5775
2022-10-11 13:44:13 - train: epoch 0041, iter [02200, 05004], lr: 0.064796, loss: 3.4001
2022-10-11 13:45:02 - train: epoch 0041, iter [02300, 05004], lr: 0.064766, loss: 4.9137
2022-10-11 13:45:51 - train: epoch 0041, iter [02400, 05004], lr: 0.064736, loss: 4.3065
2022-10-11 13:46:41 - train: epoch 0041, iter [02500, 05004], lr: 0.064706, loss: 4.5783
2022-10-11 13:47:30 - train: epoch 0041, iter [02600, 05004], lr: 0.064676, loss: 4.4149
2022-10-11 13:48:19 - train: epoch 0041, iter [02700, 05004], lr: 0.064646, loss: 4.4525
2022-10-11 13:49:08 - train: epoch 0041, iter [02800, 05004], lr: 0.064616, loss: 4.6975
2022-10-11 13:49:58 - train: epoch 0041, iter [02900, 05004], lr: 0.064586, loss: 3.9754
2022-10-11 13:50:48 - train: epoch 0041, iter [03000, 05004], lr: 0.064556, loss: 3.7742
2022-10-11 13:51:39 - train: epoch 0041, iter [03100, 05004], lr: 0.064526, loss: 3.9658
2022-10-11 13:52:29 - train: epoch 0041, iter [03200, 05004], lr: 0.064496, loss: 4.3549
2022-10-11 13:53:17 - train: epoch 0041, iter [03300, 05004], lr: 0.064466, loss: 4.3433
2022-10-11 13:54:07 - train: epoch 0041, iter [03400, 05004], lr: 0.064436, loss: 4.4393
2022-10-11 13:54:56 - train: epoch 0041, iter [03500, 05004], lr: 0.064406, loss: 4.8191
2022-10-11 13:55:46 - train: epoch 0041, iter [03600, 05004], lr: 0.064376, loss: 4.5638
2022-10-11 13:56:36 - train: epoch 0041, iter [03700, 05004], lr: 0.064346, loss: 4.4778
2022-10-11 13:57:25 - train: epoch 0041, iter [03800, 05004], lr: 0.064316, loss: 4.1296
2022-10-11 13:58:15 - train: epoch 0041, iter [03900, 05004], lr: 0.064286, loss: 4.3102
2022-10-11 13:59:04 - train: epoch 0041, iter [04000, 05004], lr: 0.064256, loss: 4.7503
2022-10-11 13:59:53 - train: epoch 0041, iter [04100, 05004], lr: 0.064225, loss: 4.6483
2022-10-11 14:00:43 - train: epoch 0041, iter [04200, 05004], lr: 0.064195, loss: 4.0919
2022-10-11 14:01:33 - train: epoch 0041, iter [04300, 05004], lr: 0.064165, loss: 4.3272
2022-10-11 14:02:23 - train: epoch 0041, iter [04400, 05004], lr: 0.064135, loss: 4.3722
2022-10-11 14:03:11 - train: epoch 0041, iter [04500, 05004], lr: 0.064105, loss: 3.7947
2022-10-11 14:04:02 - train: epoch 0041, iter [04600, 05004], lr: 0.064075, loss: 4.2534
2022-10-11 14:04:53 - train: epoch 0041, iter [04700, 05004], lr: 0.064045, loss: 3.8462
2022-10-11 14:05:42 - train: epoch 0041, iter [04800, 05004], lr: 0.064015, loss: 3.9555
2022-10-11 14:06:33 - train: epoch 0041, iter [04900, 05004], lr: 0.063985, loss: 3.2516
2022-10-11 14:07:17 - train: epoch 0041, iter [05000, 05004], lr: 0.063954, loss: 4.6724
2022-10-11 14:07:19 - train: epoch 041, train_loss: 4.3601
2022-10-11 14:09:06 - eval: epoch: 041, acc1: 57.018%, acc5: 81.410%, test_loss: 2.0642, per_image_load_time: 3.509ms, per_image_inference_time: 0.548ms
2022-10-11 14:09:06 - until epoch: 041, best_acc1: 59.112%
2022-10-11 14:09:06 - epoch 042 lr: 0.063953
2022-10-11 14:10:01 - train: epoch 0042, iter [00100, 05004], lr: 0.063923, loss: 3.8532
2022-10-11 14:10:52 - train: epoch 0042, iter [00200, 05004], lr: 0.063893, loss: 4.5133
2022-10-11 14:11:42 - train: epoch 0042, iter [00300, 05004], lr: 0.063863, loss: 4.7914
2022-10-11 14:12:31 - train: epoch 0042, iter [00400, 05004], lr: 0.063833, loss: 4.4979
2022-10-11 14:13:21 - train: epoch 0042, iter [00500, 05004], lr: 0.063802, loss: 5.0324
2022-10-11 14:14:12 - train: epoch 0042, iter [00600, 05004], lr: 0.063772, loss: 3.9427
2022-10-11 14:15:02 - train: epoch 0042, iter [00700, 05004], lr: 0.063742, loss: 4.4142
2022-10-11 14:15:52 - train: epoch 0042, iter [00800, 05004], lr: 0.063712, loss: 4.2321
2022-10-11 14:16:42 - train: epoch 0042, iter [00900, 05004], lr: 0.063682, loss: 4.4512
2022-10-11 14:17:32 - train: epoch 0042, iter [01000, 05004], lr: 0.063651, loss: 4.2953
2022-10-11 14:18:21 - train: epoch 0042, iter [01100, 05004], lr: 0.063621, loss: 3.3174
2022-10-11 14:19:11 - train: epoch 0042, iter [01200, 05004], lr: 0.063591, loss: 4.7650
2022-10-11 14:20:02 - train: epoch 0042, iter [01300, 05004], lr: 0.063561, loss: 4.7810
2022-10-11 14:20:50 - train: epoch 0042, iter [01400, 05004], lr: 0.063531, loss: 5.0706
2022-10-11 14:21:38 - train: epoch 0042, iter [01500, 05004], lr: 0.063500, loss: 3.8839
2022-10-11 14:22:29 - train: epoch 0042, iter [01600, 05004], lr: 0.063470, loss: 4.6412
2022-10-11 14:23:16 - train: epoch 0042, iter [01700, 05004], lr: 0.063440, loss: 4.0422
2022-10-11 14:24:04 - train: epoch 0042, iter [01800, 05004], lr: 0.063410, loss: 4.5807
2022-10-11 14:24:53 - train: epoch 0042, iter [01900, 05004], lr: 0.063379, loss: 4.7538
2022-10-11 14:25:43 - train: epoch 0042, iter [02000, 05004], lr: 0.063349, loss: 4.6990
2022-10-11 14:26:32 - train: epoch 0042, iter [02100, 05004], lr: 0.063319, loss: 4.1981
2022-10-11 14:27:22 - train: epoch 0042, iter [02200, 05004], lr: 0.063289, loss: 4.4078
2022-10-11 14:28:10 - train: epoch 0042, iter [02300, 05004], lr: 0.063258, loss: 3.7771
2022-10-11 14:28:59 - train: epoch 0042, iter [02400, 05004], lr: 0.063228, loss: 4.5095
2022-10-11 14:29:48 - train: epoch 0042, iter [02500, 05004], lr: 0.063198, loss: 5.1156
2022-10-11 14:30:38 - train: epoch 0042, iter [02600, 05004], lr: 0.063168, loss: 4.6441
2022-10-11 14:31:29 - train: epoch 0042, iter [02700, 05004], lr: 0.063137, loss: 4.4155
2022-10-11 14:32:17 - train: epoch 0042, iter [02800, 05004], lr: 0.063107, loss: 4.2899
2022-10-11 14:33:07 - train: epoch 0042, iter [02900, 05004], lr: 0.063077, loss: 4.4968
2022-10-11 14:33:57 - train: epoch 0042, iter [03000, 05004], lr: 0.063046, loss: 4.2159
2022-10-11 14:34:46 - train: epoch 0042, iter [03100, 05004], lr: 0.063016, loss: 4.2232
2022-10-11 14:35:36 - train: epoch 0042, iter [03200, 05004], lr: 0.062986, loss: 4.1083
2022-10-11 14:36:25 - train: epoch 0042, iter [03300, 05004], lr: 0.062956, loss: 4.7607
2022-10-11 14:37:16 - train: epoch 0042, iter [03400, 05004], lr: 0.062925, loss: 4.5293
2022-10-11 14:38:06 - train: epoch 0042, iter [03500, 05004], lr: 0.062895, loss: 4.4449
2022-10-11 14:38:54 - train: epoch 0042, iter [03600, 05004], lr: 0.062865, loss: 4.5873
2022-10-11 14:39:44 - train: epoch 0042, iter [03700, 05004], lr: 0.062834, loss: 4.4532
2022-10-11 14:40:34 - train: epoch 0042, iter [03800, 05004], lr: 0.062804, loss: 4.4351
2022-10-11 14:41:23 - train: epoch 0042, iter [03900, 05004], lr: 0.062774, loss: 4.5613
2022-10-11 14:42:13 - train: epoch 0042, iter [04000, 05004], lr: 0.062743, loss: 4.3706
2022-10-11 14:43:03 - train: epoch 0042, iter [04100, 05004], lr: 0.062713, loss: 4.0606
2022-10-11 14:43:51 - train: epoch 0042, iter [04200, 05004], lr: 0.062683, loss: 4.7478
2022-10-11 14:44:41 - train: epoch 0042, iter [04300, 05004], lr: 0.062652, loss: 4.7548
2022-10-11 14:45:31 - train: epoch 0042, iter [04400, 05004], lr: 0.062622, loss: 4.1166
2022-10-11 14:46:22 - train: epoch 0042, iter [04500, 05004], lr: 0.062591, loss: 4.1514
2022-10-11 14:47:10 - train: epoch 0042, iter [04600, 05004], lr: 0.062561, loss: 4.2285
2022-10-11 14:48:02 - train: epoch 0042, iter [04700, 05004], lr: 0.062531, loss: 4.6493
2022-10-11 14:48:51 - train: epoch 0042, iter [04800, 05004], lr: 0.062500, loss: 3.8656
2022-10-11 14:49:39 - train: epoch 0042, iter [04900, 05004], lr: 0.062470, loss: 3.9247
2022-10-11 14:50:26 - train: epoch 0042, iter [05000, 05004], lr: 0.062439, loss: 3.7827
2022-10-11 14:50:29 - train: epoch 042, train_loss: 4.3543
2022-10-11 14:52:15 - eval: epoch: 042, acc1: 56.758%, acc5: 81.146%, test_loss: 2.1375, per_image_load_time: 3.568ms, per_image_inference_time: 0.528ms
2022-10-11 14:52:16 - until epoch: 042, best_acc1: 59.112%
2022-10-11 14:52:16 - epoch 043 lr: 0.062438
2022-10-11 14:53:11 - train: epoch 0043, iter [00100, 05004], lr: 0.062408, loss: 4.2846
2022-10-11 14:54:00 - train: epoch 0043, iter [00200, 05004], lr: 0.062377, loss: 5.0512
2022-10-11 14:54:50 - train: epoch 0043, iter [00300, 05004], lr: 0.062347, loss: 3.0164
2022-10-11 14:55:39 - train: epoch 0043, iter [00400, 05004], lr: 0.062317, loss: 4.8341
2022-10-11 14:56:30 - train: epoch 0043, iter [00500, 05004], lr: 0.062286, loss: 4.0935
2022-10-11 14:57:19 - train: epoch 0043, iter [00600, 05004], lr: 0.062256, loss: 3.7934
2022-10-11 14:58:07 - train: epoch 0043, iter [00700, 05004], lr: 0.062225, loss: 4.6619
2022-10-11 14:58:58 - train: epoch 0043, iter [00800, 05004], lr: 0.062195, loss: 4.8964
2022-10-11 14:59:49 - train: epoch 0043, iter [00900, 05004], lr: 0.062164, loss: 4.9190
2022-10-11 15:00:38 - train: epoch 0043, iter [01000, 05004], lr: 0.062134, loss: 4.5065
2022-10-11 15:01:26 - train: epoch 0043, iter [01100, 05004], lr: 0.062104, loss: 4.2617
2022-10-11 15:02:17 - train: epoch 0043, iter [01200, 05004], lr: 0.062073, loss: 4.0132
2022-10-11 15:03:07 - train: epoch 0043, iter [01300, 05004], lr: 0.062043, loss: 3.9676
2022-10-11 15:03:57 - train: epoch 0043, iter [01400, 05004], lr: 0.062012, loss: 3.6972
2022-10-11 15:04:48 - train: epoch 0043, iter [01500, 05004], lr: 0.061982, loss: 4.4296
2022-10-11 15:05:38 - train: epoch 0043, iter [01600, 05004], lr: 0.061951, loss: 4.8725
2022-10-11 15:06:27 - train: epoch 0043, iter [01700, 05004], lr: 0.061921, loss: 4.8203
2022-10-11 15:07:16 - train: epoch 0043, iter [01800, 05004], lr: 0.061890, loss: 5.1266
2022-10-11 15:08:08 - train: epoch 0043, iter [01900, 05004], lr: 0.061860, loss: 4.5921
2022-10-11 15:08:57 - train: epoch 0043, iter [02000, 05004], lr: 0.061829, loss: 4.7747
2022-10-11 15:09:46 - train: epoch 0043, iter [02100, 05004], lr: 0.061799, loss: 4.2504
2022-10-11 15:10:36 - train: epoch 0043, iter [02200, 05004], lr: 0.061768, loss: 3.5204
2022-10-11 15:11:26 - train: epoch 0043, iter [02300, 05004], lr: 0.061738, loss: 4.2343
2022-10-11 15:12:13 - train: epoch 0043, iter [02400, 05004], lr: 0.061707, loss: 3.7395
2022-10-11 15:13:03 - train: epoch 0043, iter [02500, 05004], lr: 0.061677, loss: 4.3805
2022-10-11 15:13:54 - train: epoch 0043, iter [02600, 05004], lr: 0.061646, loss: 4.6385
2022-10-11 15:14:45 - train: epoch 0043, iter [02700, 05004], lr: 0.061616, loss: 3.8884
2022-10-11 15:15:32 - train: epoch 0043, iter [02800, 05004], lr: 0.061585, loss: 4.2215
2022-10-11 15:16:22 - train: epoch 0043, iter [02900, 05004], lr: 0.061555, loss: 4.7015
2022-10-11 15:17:12 - train: epoch 0043, iter [03000, 05004], lr: 0.061524, loss: 4.5462
2022-10-11 15:18:01 - train: epoch 0043, iter [03100, 05004], lr: 0.061494, loss: 3.2891
2022-10-11 15:18:50 - train: epoch 0043, iter [03200, 05004], lr: 0.061463, loss: 4.5907
2022-10-11 15:19:40 - train: epoch 0043, iter [03300, 05004], lr: 0.061432, loss: 3.8027
2022-10-11 15:20:29 - train: epoch 0043, iter [03400, 05004], lr: 0.061402, loss: 4.5438
2022-10-11 15:21:16 - train: epoch 0043, iter [03500, 05004], lr: 0.061371, loss: 4.2184
2022-10-11 15:22:06 - train: epoch 0043, iter [03600, 05004], lr: 0.061341, loss: 4.1998
2022-10-11 15:22:55 - train: epoch 0043, iter [03700, 05004], lr: 0.061310, loss: 4.1583
2022-10-11 15:23:44 - train: epoch 0043, iter [03800, 05004], lr: 0.061280, loss: 4.4820
2022-10-11 15:24:34 - train: epoch 0043, iter [03900, 05004], lr: 0.061249, loss: 4.2011
2022-10-11 15:25:23 - train: epoch 0043, iter [04000, 05004], lr: 0.061218, loss: 4.5024
2022-10-11 15:26:13 - train: epoch 0043, iter [04100, 05004], lr: 0.061188, loss: 4.2127
2022-10-11 15:27:02 - train: epoch 0043, iter [04200, 05004], lr: 0.061157, loss: 4.8228
2022-10-11 15:27:52 - train: epoch 0043, iter [04300, 05004], lr: 0.061127, loss: 4.5659
2022-10-11 15:28:41 - train: epoch 0043, iter [04400, 05004], lr: 0.061096, loss: 4.9115
2022-10-11 15:29:31 - train: epoch 0043, iter [04500, 05004], lr: 0.061065, loss: 4.2277
2022-10-11 15:30:21 - train: epoch 0043, iter [04600, 05004], lr: 0.061035, loss: 3.9863
2022-10-11 15:31:09 - train: epoch 0043, iter [04700, 05004], lr: 0.061004, loss: 4.3422
2022-10-11 15:31:58 - train: epoch 0043, iter [04800, 05004], lr: 0.060974, loss: 3.9509
2022-10-11 15:32:47 - train: epoch 0043, iter [04900, 05004], lr: 0.060943, loss: 3.5919
2022-10-11 15:33:34 - train: epoch 0043, iter [05000, 05004], lr: 0.060912, loss: 4.1968
2022-10-11 15:33:37 - train: epoch 043, train_loss: 4.3300
2022-10-11 15:35:23 - eval: epoch: 043, acc1: 60.638%, acc5: 84.246%, test_loss: 1.9350, per_image_load_time: 3.406ms, per_image_inference_time: 0.543ms
2022-10-11 15:35:24 - until epoch: 043, best_acc1: 60.638%
2022-10-11 15:35:24 - epoch 044 lr: 0.060911
2022-10-11 15:36:20 - train: epoch 0044, iter [00100, 05004], lr: 0.060880, loss: 4.6895
2022-10-11 15:37:08 - train: epoch 0044, iter [00200, 05004], lr: 0.060850, loss: 4.1649
2022-10-11 15:37:59 - train: epoch 0044, iter [00300, 05004], lr: 0.060819, loss: 4.2670
2022-10-11 15:38:48 - train: epoch 0044, iter [00400, 05004], lr: 0.060789, loss: 4.4875
2022-10-11 15:39:37 - train: epoch 0044, iter [00500, 05004], lr: 0.060758, loss: 4.6248
2022-10-11 15:40:26 - train: epoch 0044, iter [00600, 05004], lr: 0.060727, loss: 4.4985
2022-10-11 15:41:15 - train: epoch 0044, iter [00700, 05004], lr: 0.060697, loss: 4.7648
2022-10-11 15:42:03 - train: epoch 0044, iter [00800, 05004], lr: 0.060666, loss: 4.6108
2022-10-11 15:42:53 - train: epoch 0044, iter [00900, 05004], lr: 0.060635, loss: 4.4576
2022-10-11 15:43:41 - train: epoch 0044, iter [01000, 05004], lr: 0.060605, loss: 4.1977
2022-10-11 15:44:31 - train: epoch 0044, iter [01100, 05004], lr: 0.060574, loss: 4.5675
2022-10-11 15:45:19 - train: epoch 0044, iter [01200, 05004], lr: 0.060543, loss: 4.6161
2022-10-11 15:46:08 - train: epoch 0044, iter [01300, 05004], lr: 0.060512, loss: 5.0918
2022-10-11 15:46:56 - train: epoch 0044, iter [01400, 05004], lr: 0.060482, loss: 4.4542
2022-10-11 15:47:45 - train: epoch 0044, iter [01500, 05004], lr: 0.060451, loss: 3.8085
2022-10-11 15:48:34 - train: epoch 0044, iter [01600, 05004], lr: 0.060420, loss: 4.9256
2022-10-11 15:49:24 - train: epoch 0044, iter [01700, 05004], lr: 0.060390, loss: 4.9272
2022-10-11 15:50:14 - train: epoch 0044, iter [01800, 05004], lr: 0.060359, loss: 4.2024
2022-10-11 15:51:02 - train: epoch 0044, iter [01900, 05004], lr: 0.060328, loss: 4.2511
2022-10-11 15:51:53 - train: epoch 0044, iter [02000, 05004], lr: 0.060298, loss: 3.8796
2022-10-11 15:52:42 - train: epoch 0044, iter [02100, 05004], lr: 0.060267, loss: 4.3822
2022-10-11 15:53:30 - train: epoch 0044, iter [02200, 05004], lr: 0.060236, loss: 4.5685
2022-10-11 15:54:21 - train: epoch 0044, iter [02300, 05004], lr: 0.060205, loss: 4.6669
2022-10-11 15:55:11 - train: epoch 0044, iter [02400, 05004], lr: 0.060175, loss: 4.2534
2022-10-11 15:56:01 - train: epoch 0044, iter [02500, 05004], lr: 0.060144, loss: 4.5254
2022-10-11 15:56:50 - train: epoch 0044, iter [02600, 05004], lr: 0.060113, loss: 4.3295
2022-10-11 15:57:39 - train: epoch 0044, iter [02700, 05004], lr: 0.060082, loss: 3.7739
2022-10-11 15:58:28 - train: epoch 0044, iter [02800, 05004], lr: 0.060052, loss: 3.8562
2022-10-11 15:59:20 - train: epoch 0044, iter [02900, 05004], lr: 0.060021, loss: 4.8013
2022-10-11 16:00:09 - train: epoch 0044, iter [03000, 05004], lr: 0.059990, loss: 4.3206
2022-10-11 16:00:58 - train: epoch 0044, iter [03100, 05004], lr: 0.059959, loss: 4.2046
2022-10-11 16:01:47 - train: epoch 0044, iter [03200, 05004], lr: 0.059929, loss: 3.7801
2022-10-11 16:02:37 - train: epoch 0044, iter [03300, 05004], lr: 0.059898, loss: 3.2355
2022-10-11 16:03:26 - train: epoch 0044, iter [03400, 05004], lr: 0.059867, loss: 3.9162
2022-10-11 16:04:15 - train: epoch 0044, iter [03500, 05004], lr: 0.059836, loss: 4.1876
2022-10-11 16:05:05 - train: epoch 0044, iter [03600, 05004], lr: 0.059806, loss: 5.1135
2022-10-11 16:05:54 - train: epoch 0044, iter [03700, 05004], lr: 0.059775, loss: 4.8498
2022-10-11 16:06:43 - train: epoch 0044, iter [03800, 05004], lr: 0.059744, loss: 4.1452
2022-10-11 16:07:32 - train: epoch 0044, iter [03900, 05004], lr: 0.059713, loss: 4.7625
2022-10-11 16:08:21 - train: epoch 0044, iter [04000, 05004], lr: 0.059682, loss: 4.3780
2022-10-11 16:09:11 - train: epoch 0044, iter [04100, 05004], lr: 0.059652, loss: 4.8654
2022-10-11 16:10:02 - train: epoch 0044, iter [04200, 05004], lr: 0.059621, loss: 4.4193
2022-10-11 16:10:50 - train: epoch 0044, iter [04300, 05004], lr: 0.059590, loss: 4.5169
2022-10-11 16:11:38 - train: epoch 0044, iter [04400, 05004], lr: 0.059559, loss: 4.5478
2022-10-11 16:12:28 - train: epoch 0044, iter [04500, 05004], lr: 0.059528, loss: 4.4244
2022-10-11 16:13:16 - train: epoch 0044, iter [04600, 05004], lr: 0.059498, loss: 4.6502
2022-10-11 16:14:05 - train: epoch 0044, iter [04700, 05004], lr: 0.059467, loss: 3.8772
2022-10-11 16:14:55 - train: epoch 0044, iter [04800, 05004], lr: 0.059436, loss: 4.6791
2022-10-11 16:15:44 - train: epoch 0044, iter [04900, 05004], lr: 0.059405, loss: 4.8893
2022-10-11 16:16:30 - train: epoch 0044, iter [05000, 05004], lr: 0.059374, loss: 4.2775
2022-10-11 16:16:33 - train: epoch 044, train_loss: 4.3257
2022-10-11 16:18:18 - eval: epoch: 044, acc1: 58.888%, acc5: 83.114%, test_loss: 1.9303, per_image_load_time: 1.157ms, per_image_inference_time: 0.507ms
2022-10-11 16:18:18 - until epoch: 044, best_acc1: 60.638%
2022-10-11 16:18:18 - epoch 045 lr: 0.059373
2022-10-11 16:19:17 - train: epoch 0045, iter [00100, 05004], lr: 0.059342, loss: 4.2502
2022-10-11 16:20:08 - train: epoch 0045, iter [00200, 05004], lr: 0.059311, loss: 4.4715
2022-10-11 16:20:56 - train: epoch 0045, iter [00300, 05004], lr: 0.059281, loss: 4.7566
2022-10-11 16:21:46 - train: epoch 0045, iter [00400, 05004], lr: 0.059250, loss: 4.0191
2022-10-11 16:22:36 - train: epoch 0045, iter [00500, 05004], lr: 0.059219, loss: 4.3201
2022-10-11 16:23:25 - train: epoch 0045, iter [00600, 05004], lr: 0.059188, loss: 4.3078
2022-10-11 16:24:17 - train: epoch 0045, iter [00700, 05004], lr: 0.059157, loss: 4.1616
2022-10-11 16:25:07 - train: epoch 0045, iter [00800, 05004], lr: 0.059126, loss: 3.9591
2022-10-11 16:25:58 - train: epoch 0045, iter [00900, 05004], lr: 0.059095, loss: 4.1209
2022-10-11 16:26:47 - train: epoch 0045, iter [01000, 05004], lr: 0.059065, loss: 4.5083
2022-10-11 16:27:37 - train: epoch 0045, iter [01100, 05004], lr: 0.059034, loss: 4.7512
2022-10-11 16:28:26 - train: epoch 0045, iter [01200, 05004], lr: 0.059003, loss: 4.8337
2022-10-11 16:29:17 - train: epoch 0045, iter [01300, 05004], lr: 0.058972, loss: 4.0017
2022-10-11 16:30:08 - train: epoch 0045, iter [01400, 05004], lr: 0.058941, loss: 3.9609
2022-10-11 16:30:58 - train: epoch 0045, iter [01500, 05004], lr: 0.058910, loss: 4.6693
2022-10-11 16:31:48 - train: epoch 0045, iter [01600, 05004], lr: 0.058879, loss: 4.5836
2022-10-11 16:32:38 - train: epoch 0045, iter [01700, 05004], lr: 0.058848, loss: 3.5638
2022-10-11 16:33:28 - train: epoch 0045, iter [01800, 05004], lr: 0.058818, loss: 4.0353
2022-10-11 16:34:19 - train: epoch 0045, iter [01900, 05004], lr: 0.058787, loss: 3.8157
2022-10-11 16:35:09 - train: epoch 0045, iter [02000, 05004], lr: 0.058756, loss: 3.9201
2022-10-11 16:35:58 - train: epoch 0045, iter [02100, 05004], lr: 0.058725, loss: 3.9703
2022-10-11 16:36:48 - train: epoch 0045, iter [02200, 05004], lr: 0.058694, loss: 3.9442
2022-10-11 16:37:38 - train: epoch 0045, iter [02300, 05004], lr: 0.058663, loss: 3.3200
2022-10-11 16:38:28 - train: epoch 0045, iter [02400, 05004], lr: 0.058632, loss: 4.0650
2022-10-11 16:39:18 - train: epoch 0045, iter [02500, 05004], lr: 0.058601, loss: 4.2550
2022-10-11 16:40:09 - train: epoch 0045, iter [02600, 05004], lr: 0.058570, loss: 4.1063
2022-10-11 16:40:59 - train: epoch 0045, iter [02700, 05004], lr: 0.058539, loss: 4.6383
2022-10-11 16:41:50 - train: epoch 0045, iter [02800, 05004], lr: 0.058508, loss: 4.4485
2022-10-11 16:42:39 - train: epoch 0045, iter [02900, 05004], lr: 0.058478, loss: 4.2823
2022-10-11 16:43:27 - train: epoch 0045, iter [03000, 05004], lr: 0.058447, loss: 4.1918
2022-10-11 16:44:16 - train: epoch 0045, iter [03100, 05004], lr: 0.058416, loss: 4.0558
2022-10-11 16:45:05 - train: epoch 0045, iter [03200, 05004], lr: 0.058385, loss: 4.4469
2022-10-11 16:45:54 - train: epoch 0045, iter [03300, 05004], lr: 0.058354, loss: 4.8173
2022-10-11 16:46:43 - train: epoch 0045, iter [03400, 05004], lr: 0.058323, loss: 4.5146
2022-10-11 16:47:31 - train: epoch 0045, iter [03500, 05004], lr: 0.058292, loss: 4.8962
2022-10-11 16:48:20 - train: epoch 0045, iter [03600, 05004], lr: 0.058261, loss: 4.2100
2022-10-11 16:49:09 - train: epoch 0045, iter [03700, 05004], lr: 0.058230, loss: 4.3130
2022-10-11 16:50:00 - train: epoch 0045, iter [03800, 05004], lr: 0.058199, loss: 3.8668
2022-10-11 16:50:50 - train: epoch 0045, iter [03900, 05004], lr: 0.058168, loss: 5.0046
2022-10-11 16:51:42 - train: epoch 0045, iter [04000, 05004], lr: 0.058137, loss: 4.4552
2022-10-11 16:52:31 - train: epoch 0045, iter [04100, 05004], lr: 0.058106, loss: 4.4783
2022-10-11 16:53:20 - train: epoch 0045, iter [04200, 05004], lr: 0.058075, loss: 4.7646
2022-10-11 16:54:09 - train: epoch 0045, iter [04300, 05004], lr: 0.058044, loss: 3.9519
2022-10-11 16:55:01 - train: epoch 0045, iter [04400, 05004], lr: 0.058013, loss: 4.4961
2022-10-11 16:55:50 - train: epoch 0045, iter [04500, 05004], lr: 0.057982, loss: 4.4481
2022-10-11 16:56:40 - train: epoch 0045, iter [04600, 05004], lr: 0.057951, loss: 4.4030
2022-10-11 16:57:29 - train: epoch 0045, iter [04700, 05004], lr: 0.057920, loss: 3.8629
2022-10-11 16:58:19 - train: epoch 0045, iter [04800, 05004], lr: 0.057889, loss: 4.5786
2022-10-11 16:59:09 - train: epoch 0045, iter [04900, 05004], lr: 0.057858, loss: 3.8343
2022-10-11 16:59:57 - train: epoch 0045, iter [05000, 05004], lr: 0.057827, loss: 3.5398
2022-10-11 17:00:00 - train: epoch 045, train_loss: 4.2945
2022-10-11 17:01:45 - eval: epoch: 045, acc1: 59.114%, acc5: 83.158%, test_loss: 1.9753, per_image_load_time: 1.312ms, per_image_inference_time: 0.534ms
2022-10-11 17:01:46 - until epoch: 045, best_acc1: 60.638%
2022-10-11 17:01:46 - epoch 046 lr: 0.057826
2022-10-11 17:02:44 - train: epoch 0046, iter [00100, 05004], lr: 0.057795, loss: 3.8997
2022-10-11 17:03:35 - train: epoch 0046, iter [00200, 05004], lr: 0.057764, loss: 4.0743
2022-10-11 17:04:24 - train: epoch 0046, iter [00300, 05004], lr: 0.057733, loss: 4.8668
2022-10-11 17:05:15 - train: epoch 0046, iter [00400, 05004], lr: 0.057702, loss: 3.8395
2022-10-11 17:06:04 - train: epoch 0046, iter [00500, 05004], lr: 0.057671, loss: 4.0446
2022-10-11 17:06:53 - train: epoch 0046, iter [00600, 05004], lr: 0.057640, loss: 4.7575
2022-10-11 17:07:43 - train: epoch 0046, iter [00700, 05004], lr: 0.057609, loss: 3.6538
2022-10-11 17:08:33 - train: epoch 0046, iter [00800, 05004], lr: 0.057578, loss: 4.4910
2022-10-11 17:09:22 - train: epoch 0046, iter [00900, 05004], lr: 0.057547, loss: 4.3321
2022-10-11 17:10:14 - train: epoch 0046, iter [01000, 05004], lr: 0.057516, loss: 4.1863
2022-10-11 17:11:02 - train: epoch 0046, iter [01100, 05004], lr: 0.057485, loss: 4.3876
2022-10-11 17:11:52 - train: epoch 0046, iter [01200, 05004], lr: 0.057454, loss: 4.0472
2022-10-11 17:12:41 - train: epoch 0046, iter [01300, 05004], lr: 0.057423, loss: 4.3662
2022-10-11 17:13:30 - train: epoch 0046, iter [01400, 05004], lr: 0.057392, loss: 4.7288
2022-10-11 17:14:18 - train: epoch 0046, iter [01500, 05004], lr: 0.057361, loss: 4.4663
2022-10-11 17:15:09 - train: epoch 0046, iter [01600, 05004], lr: 0.057330, loss: 4.5491
2022-10-11 17:15:57 - train: epoch 0046, iter [01700, 05004], lr: 0.057298, loss: 4.2505
2022-10-11 17:16:48 - train: epoch 0046, iter [01800, 05004], lr: 0.057267, loss: 4.7131
2022-10-11 17:17:37 - train: epoch 0046, iter [01900, 05004], lr: 0.057236, loss: 4.1155
2022-10-11 17:18:26 - train: epoch 0046, iter [02000, 05004], lr: 0.057205, loss: 4.7436
2022-10-11 17:19:15 - train: epoch 0046, iter [02100, 05004], lr: 0.057174, loss: 4.5412
2022-10-11 17:20:07 - train: epoch 0046, iter [02200, 05004], lr: 0.057143, loss: 4.4293
2022-10-11 17:20:55 - train: epoch 0046, iter [02300, 05004], lr: 0.057112, loss: 4.6677
2022-10-11 17:21:44 - train: epoch 0046, iter [02400, 05004], lr: 0.057081, loss: 3.9353
2022-10-11 17:22:32 - train: epoch 0046, iter [02500, 05004], lr: 0.057050, loss: 3.2409
2022-10-11 17:23:23 - train: epoch 0046, iter [02600, 05004], lr: 0.057019, loss: 4.3990
2022-10-11 17:24:13 - train: epoch 0046, iter [02700, 05004], lr: 0.056988, loss: 4.7252
2022-10-11 17:25:02 - train: epoch 0046, iter [02800, 05004], lr: 0.056957, loss: 4.7966
2022-10-11 17:25:51 - train: epoch 0046, iter [02900, 05004], lr: 0.056926, loss: 4.4192
2022-10-11 17:26:41 - train: epoch 0046, iter [03000, 05004], lr: 0.056895, loss: 4.1859
2022-10-11 17:27:31 - train: epoch 0046, iter [03100, 05004], lr: 0.056863, loss: 4.6076
2022-10-11 17:28:20 - train: epoch 0046, iter [03200, 05004], lr: 0.056832, loss: 4.8358
2022-10-11 17:29:09 - train: epoch 0046, iter [03300, 05004], lr: 0.056801, loss: 4.5916
2022-10-11 17:29:58 - train: epoch 0046, iter [03400, 05004], lr: 0.056770, loss: 4.6390
2022-10-11 17:30:48 - train: epoch 0046, iter [03500, 05004], lr: 0.056739, loss: 4.1927
2022-10-11 17:31:37 - train: epoch 0046, iter [03600, 05004], lr: 0.056708, loss: 3.7612
2022-10-11 17:32:26 - train: epoch 0046, iter [03700, 05004], lr: 0.056677, loss: 3.7150
2022-10-11 17:33:16 - train: epoch 0046, iter [03800, 05004], lr: 0.056646, loss: 4.7163
2022-10-11 17:34:05 - train: epoch 0046, iter [03900, 05004], lr: 0.056615, loss: 4.1958
2022-10-11 17:34:55 - train: epoch 0046, iter [04000, 05004], lr: 0.056584, loss: 3.9306
2022-10-11 17:35:44 - train: epoch 0046, iter [04100, 05004], lr: 0.056552, loss: 4.8500
2022-10-11 17:36:33 - train: epoch 0046, iter [04200, 05004], lr: 0.056521, loss: 4.0944
2022-10-11 17:37:23 - train: epoch 0046, iter [04300, 05004], lr: 0.056490, loss: 3.2887
2022-10-11 17:38:12 - train: epoch 0046, iter [04400, 05004], lr: 0.056459, loss: 3.5156
2022-10-11 17:39:02 - train: epoch 0046, iter [04500, 05004], lr: 0.056428, loss: 4.3820
2022-10-11 17:39:52 - train: epoch 0046, iter [04600, 05004], lr: 0.056397, loss: 4.5358
2022-10-11 17:40:42 - train: epoch 0046, iter [04700, 05004], lr: 0.056366, loss: 4.2139
2022-10-11 17:41:31 - train: epoch 0046, iter [04800, 05004], lr: 0.056335, loss: 3.5411
2022-10-11 17:42:20 - train: epoch 0046, iter [04900, 05004], lr: 0.056303, loss: 4.5557
2022-10-11 17:43:07 - train: epoch 0046, iter [05000, 05004], lr: 0.056272, loss: 4.3233
2022-10-11 17:43:10 - train: epoch 046, train_loss: 4.2975
2022-10-11 17:44:55 - eval: epoch: 046, acc1: 58.136%, acc5: 82.444%, test_loss: 2.0237, per_image_load_time: 1.911ms, per_image_inference_time: 0.519ms
2022-10-11 17:44:56 - until epoch: 046, best_acc1: 60.638%
2022-10-11 17:44:56 - epoch 047 lr: 0.056271
2022-10-11 17:45:51 - train: epoch 0047, iter [00100, 05004], lr: 0.056240, loss: 4.0467
2022-10-11 17:46:42 - train: epoch 0047, iter [00200, 05004], lr: 0.056209, loss: 4.1379
2022-10-11 17:47:31 - train: epoch 0047, iter [00300, 05004], lr: 0.056178, loss: 3.1576
2022-10-11 17:48:20 - train: epoch 0047, iter [00400, 05004], lr: 0.056146, loss: 3.8709
2022-10-11 17:49:09 - train: epoch 0047, iter [00500, 05004], lr: 0.056115, loss: 3.6602
2022-10-11 17:50:00 - train: epoch 0047, iter [00600, 05004], lr: 0.056084, loss: 4.0539
2022-10-11 17:50:49 - train: epoch 0047, iter [00700, 05004], lr: 0.056053, loss: 4.4624
2022-10-11 17:51:38 - train: epoch 0047, iter [00800, 05004], lr: 0.056022, loss: 4.5577
2022-10-11 17:52:27 - train: epoch 0047, iter [00900, 05004], lr: 0.055991, loss: 4.6691
2022-10-11 17:53:17 - train: epoch 0047, iter [01000, 05004], lr: 0.055960, loss: 4.4840
2022-10-11 17:54:05 - train: epoch 0047, iter [01100, 05004], lr: 0.055928, loss: 3.9982
2022-10-11 17:54:53 - train: epoch 0047, iter [01200, 05004], lr: 0.055897, loss: 4.6771
2022-10-11 17:55:43 - train: epoch 0047, iter [01300, 05004], lr: 0.055866, loss: 4.9802
2022-10-11 17:56:33 - train: epoch 0047, iter [01400, 05004], lr: 0.055835, loss: 4.2079
2022-10-11 17:57:19 - train: epoch 0047, iter [01500, 05004], lr: 0.055804, loss: 4.5942
2022-10-11 17:58:09 - train: epoch 0047, iter [01600, 05004], lr: 0.055772, loss: 3.5333
2022-10-11 17:58:55 - train: epoch 0047, iter [01700, 05004], lr: 0.055741, loss: 4.3836
2022-10-11 17:59:43 - train: epoch 0047, iter [01800, 05004], lr: 0.055710, loss: 4.0886
2022-10-11 18:00:31 - train: epoch 0047, iter [01900, 05004], lr: 0.055679, loss: 4.4990
2022-10-11 18:01:21 - train: epoch 0047, iter [02000, 05004], lr: 0.055648, loss: 3.8713
2022-10-11 18:02:09 - train: epoch 0047, iter [02100, 05004], lr: 0.055617, loss: 4.6731
2022-10-11 18:02:58 - train: epoch 0047, iter [02200, 05004], lr: 0.055585, loss: 3.8658
2022-10-11 18:03:46 - train: epoch 0047, iter [02300, 05004], lr: 0.055554, loss: 4.6208
2022-10-11 18:04:35 - train: epoch 0047, iter [02400, 05004], lr: 0.055523, loss: 4.2387
2022-10-11 18:05:26 - train: epoch 0047, iter [02500, 05004], lr: 0.055492, loss: 4.6590
2022-10-11 18:06:17 - train: epoch 0047, iter [02600, 05004], lr: 0.055461, loss: 4.7394
2022-10-11 18:07:06 - train: epoch 0047, iter [02700, 05004], lr: 0.055429, loss: 4.3030
2022-10-11 18:07:57 - train: epoch 0047, iter [02800, 05004], lr: 0.055398, loss: 4.5592
2022-10-11 18:08:45 - train: epoch 0047, iter [02900, 05004], lr: 0.055367, loss: 3.6207
2022-10-11 18:09:34 - train: epoch 0047, iter [03000, 05004], lr: 0.055336, loss: 4.5050
2022-10-11 18:10:26 - train: epoch 0047, iter [03100, 05004], lr: 0.055305, loss: 4.0365
2022-10-11 18:11:16 - train: epoch 0047, iter [03200, 05004], lr: 0.055273, loss: 4.1207
2022-10-11 18:12:05 - train: epoch 0047, iter [03300, 05004], lr: 0.055242, loss: 4.7836
2022-10-11 18:12:54 - train: epoch 0047, iter [03400, 05004], lr: 0.055211, loss: 3.9810
2022-10-11 18:13:44 - train: epoch 0047, iter [03500, 05004], lr: 0.055180, loss: 4.7339
2022-10-11 18:14:32 - train: epoch 0047, iter [03600, 05004], lr: 0.055148, loss: 4.3460
2022-10-11 18:15:23 - train: epoch 0047, iter [03700, 05004], lr: 0.055117, loss: 4.6496
2022-10-11 18:16:12 - train: epoch 0047, iter [03800, 05004], lr: 0.055086, loss: 4.1768
2022-10-11 18:17:03 - train: epoch 0047, iter [03900, 05004], lr: 0.055055, loss: 4.6941
2022-10-11 18:17:53 - train: epoch 0047, iter [04000, 05004], lr: 0.055024, loss: 4.5433
2022-10-11 18:18:43 - train: epoch 0047, iter [04100, 05004], lr: 0.054992, loss: 4.6675
2022-10-11 18:19:33 - train: epoch 0047, iter [04200, 05004], lr: 0.054961, loss: 5.2684
2022-10-11 18:20:23 - train: epoch 0047, iter [04300, 05004], lr: 0.054930, loss: 4.0837
2022-10-11 18:21:14 - train: epoch 0047, iter [04400, 05004], lr: 0.054899, loss: 4.6933
2022-10-11 18:22:04 - train: epoch 0047, iter [04500, 05004], lr: 0.054867, loss: 3.7544
2022-10-11 18:22:54 - train: epoch 0047, iter [04600, 05004], lr: 0.054836, loss: 4.3016
2022-10-11 18:23:43 - train: epoch 0047, iter [04700, 05004], lr: 0.054805, loss: 3.7728
2022-10-11 18:24:32 - train: epoch 0047, iter [04800, 05004], lr: 0.054774, loss: 4.3073
2022-10-11 18:25:22 - train: epoch 0047, iter [04900, 05004], lr: 0.054742, loss: 4.5985
2022-10-11 18:26:08 - train: epoch 0047, iter [05000, 05004], lr: 0.054711, loss: 3.9699
2022-10-11 18:26:11 - train: epoch 047, train_loss: 4.2843
2022-10-11 18:27:58 - eval: epoch: 047, acc1: 59.948%, acc5: 83.588%, test_loss: 2.0695, per_image_load_time: 1.436ms, per_image_inference_time: 0.523ms
2022-10-11 18:27:59 - until epoch: 047, best_acc1: 60.638%
2022-10-11 18:27:59 - epoch 048 lr: 0.054710
2022-10-11 18:28:57 - train: epoch 0048, iter [00100, 05004], lr: 0.054679, loss: 4.5446
2022-10-11 18:29:47 - train: epoch 0048, iter [00200, 05004], lr: 0.054647, loss: 4.7008
2022-10-11 18:30:37 - train: epoch 0048, iter [00300, 05004], lr: 0.054616, loss: 3.7877
2022-10-11 18:31:28 - train: epoch 0048, iter [00400, 05004], lr: 0.054585, loss: 3.8034
2022-10-11 18:32:18 - train: epoch 0048, iter [00500, 05004], lr: 0.054554, loss: 4.6617
2022-10-11 18:33:07 - train: epoch 0048, iter [00600, 05004], lr: 0.054522, loss: 3.6761
2022-10-11 18:33:58 - train: epoch 0048, iter [00700, 05004], lr: 0.054491, loss: 4.7476
2022-10-11 18:34:47 - train: epoch 0048, iter [00800, 05004], lr: 0.054460, loss: 4.4956
2022-10-11 18:35:35 - train: epoch 0048, iter [00900, 05004], lr: 0.054429, loss: 4.8250
2022-10-11 18:36:25 - train: epoch 0048, iter [01000, 05004], lr: 0.054397, loss: 4.3105
2022-10-11 18:37:16 - train: epoch 0048, iter [01100, 05004], lr: 0.054366, loss: 4.7202
2022-10-11 18:38:06 - train: epoch 0048, iter [01200, 05004], lr: 0.054335, loss: 4.5784
2022-10-11 18:38:56 - train: epoch 0048, iter [01300, 05004], lr: 0.054304, loss: 4.6806
2022-10-11 18:39:46 - train: epoch 0048, iter [01400, 05004], lr: 0.054272, loss: 4.8003
2022-10-11 18:40:36 - train: epoch 0048, iter [01500, 05004], lr: 0.054241, loss: 4.1232
2022-10-11 18:41:26 - train: epoch 0048, iter [01600, 05004], lr: 0.054210, loss: 4.5307
2022-10-11 18:42:16 - train: epoch 0048, iter [01700, 05004], lr: 0.054178, loss: 4.7062
2022-10-11 18:43:06 - train: epoch 0048, iter [01800, 05004], lr: 0.054147, loss: 4.7028
2022-10-11 18:43:56 - train: epoch 0048, iter [01900, 05004], lr: 0.054116, loss: 4.6333
2022-10-11 18:44:45 - train: epoch 0048, iter [02000, 05004], lr: 0.054085, loss: 3.8129
2022-10-11 18:45:34 - train: epoch 0048, iter [02100, 05004], lr: 0.054053, loss: 4.4435
2022-10-11 18:46:25 - train: epoch 0048, iter [02200, 05004], lr: 0.054022, loss: 4.4566
2022-10-11 18:47:14 - train: epoch 0048, iter [02300, 05004], lr: 0.053991, loss: 3.9942
2022-10-11 18:48:03 - train: epoch 0048, iter [02400, 05004], lr: 0.053959, loss: 3.7768
2022-10-11 18:48:52 - train: epoch 0048, iter [02500, 05004], lr: 0.053928, loss: 4.1245
2022-10-11 18:49:42 - train: epoch 0048, iter [02600, 05004], lr: 0.053897, loss: 4.3410
2022-10-11 18:50:33 - train: epoch 0048, iter [02700, 05004], lr: 0.053866, loss: 4.4349
2022-10-11 18:51:24 - train: epoch 0048, iter [02800, 05004], lr: 0.053834, loss: 4.8207
2022-10-11 18:52:14 - train: epoch 0048, iter [02900, 05004], lr: 0.053803, loss: 4.6466
2022-10-11 18:53:04 - train: epoch 0048, iter [03000, 05004], lr: 0.053772, loss: 4.9355
2022-10-11 18:53:53 - train: epoch 0048, iter [03100, 05004], lr: 0.053740, loss: 4.4844
2022-10-11 18:54:42 - train: epoch 0048, iter [03200, 05004], lr: 0.053709, loss: 4.6627
2022-10-11 18:55:34 - train: epoch 0048, iter [03300, 05004], lr: 0.053678, loss: 4.5411
2022-10-11 18:56:25 - train: epoch 0048, iter [03400, 05004], lr: 0.053647, loss: 3.9212
2022-10-11 18:57:12 - train: epoch 0048, iter [03500, 05004], lr: 0.053615, loss: 4.8905
2022-10-11 18:58:00 - train: epoch 0048, iter [03600, 05004], lr: 0.053584, loss: 4.4857
2022-10-11 18:58:50 - train: epoch 0048, iter [03700, 05004], lr: 0.053553, loss: 4.5308
2022-10-11 18:59:40 - train: epoch 0048, iter [03800, 05004], lr: 0.053521, loss: 4.2228
2022-10-11 19:00:31 - train: epoch 0048, iter [03900, 05004], lr: 0.053490, loss: 5.1669
2022-10-11 19:01:21 - train: epoch 0048, iter [04000, 05004], lr: 0.053459, loss: 4.4516
2022-10-11 19:02:10 - train: epoch 0048, iter [04100, 05004], lr: 0.053427, loss: 4.0349
2022-10-11 19:02:59 - train: epoch 0048, iter [04200, 05004], lr: 0.053396, loss: 4.3686
2022-10-11 19:03:49 - train: epoch 0048, iter [04300, 05004], lr: 0.053365, loss: 3.9043
2022-10-11 19:04:39 - train: epoch 0048, iter [04400, 05004], lr: 0.053333, loss: 4.5224
2022-10-11 19:05:31 - train: epoch 0048, iter [04500, 05004], lr: 0.053302, loss: 4.5107
2022-10-11 19:06:20 - train: epoch 0048, iter [04600, 05004], lr: 0.053271, loss: 4.1537
2022-10-11 19:07:10 - train: epoch 0048, iter [04700, 05004], lr: 0.053239, loss: 4.3295
2022-10-11 19:07:58 - train: epoch 0048, iter [04800, 05004], lr: 0.053208, loss: 4.6162
2022-10-11 19:08:47 - train: epoch 0048, iter [04900, 05004], lr: 0.053177, loss: 4.8368
2022-10-11 19:09:34 - train: epoch 0048, iter [05000, 05004], lr: 0.053145, loss: 4.8180
2022-10-11 19:09:36 - train: epoch 048, train_loss: 4.2851
2022-10-11 19:11:22 - eval: epoch: 048, acc1: 59.900%, acc5: 83.448%, test_loss: 1.9923, per_image_load_time: 1.478ms, per_image_inference_time: 0.523ms
2022-10-11 19:11:23 - until epoch: 048, best_acc1: 60.638%
2022-10-11 19:11:23 - epoch 049 lr: 0.053144
2022-10-11 19:12:22 - train: epoch 0049, iter [00100, 05004], lr: 0.053113, loss: 4.2432
2022-10-11 19:13:12 - train: epoch 0049, iter [00200, 05004], lr: 0.053082, loss: 3.4499
2022-10-11 19:14:03 - train: epoch 0049, iter [00300, 05004], lr: 0.053050, loss: 4.8245
2022-10-11 19:14:52 - train: epoch 0049, iter [00400, 05004], lr: 0.053019, loss: 4.2814
2022-10-11 19:15:41 - train: epoch 0049, iter [00500, 05004], lr: 0.052988, loss: 4.7585
2022-10-11 19:16:30 - train: epoch 0049, iter [00600, 05004], lr: 0.052956, loss: 3.9820
2022-10-11 19:17:21 - train: epoch 0049, iter [00700, 05004], lr: 0.052925, loss: 3.5743
2022-10-11 19:18:11 - train: epoch 0049, iter [00800, 05004], lr: 0.052894, loss: 4.7736
2022-10-11 19:19:00 - train: epoch 0049, iter [00900, 05004], lr: 0.052862, loss: 4.1309
2022-10-11 19:19:50 - train: epoch 0049, iter [01000, 05004], lr: 0.052831, loss: 4.7353
2022-10-11 19:20:40 - train: epoch 0049, iter [01100, 05004], lr: 0.052800, loss: 4.5438
2022-10-11 19:21:30 - train: epoch 0049, iter [01200, 05004], lr: 0.052768, loss: 4.4578
2022-10-11 19:22:20 - train: epoch 0049, iter [01300, 05004], lr: 0.052737, loss: 3.9735
2022-10-11 19:23:09 - train: epoch 0049, iter [01400, 05004], lr: 0.052706, loss: 4.7267
2022-10-11 19:23:59 - train: epoch 0049, iter [01500, 05004], lr: 0.052674, loss: 4.4693
2022-10-11 19:24:50 - train: epoch 0049, iter [01600, 05004], lr: 0.052643, loss: 4.6628
2022-10-11 19:25:38 - train: epoch 0049, iter [01700, 05004], lr: 0.052612, loss: 5.0129
2022-10-11 19:26:29 - train: epoch 0049, iter [01800, 05004], lr: 0.052580, loss: 4.3119
2022-10-11 19:27:19 - train: epoch 0049, iter [01900, 05004], lr: 0.052549, loss: 4.9047
2022-10-11 19:28:09 - train: epoch 0049, iter [02000, 05004], lr: 0.052517, loss: 4.4134
2022-10-11 19:28:59 - train: epoch 0049, iter [02100, 05004], lr: 0.052486, loss: 4.4255
2022-10-11 19:29:47 - train: epoch 0049, iter [02200, 05004], lr: 0.052455, loss: 4.5628
2022-10-11 19:30:37 - train: epoch 0049, iter [02300, 05004], lr: 0.052423, loss: 3.3405
2022-10-11 19:31:26 - train: epoch 0049, iter [02400, 05004], lr: 0.052392, loss: 3.9721
2022-10-11 19:32:16 - train: epoch 0049, iter [02500, 05004], lr: 0.052361, loss: 4.4252
2022-10-11 19:33:05 - train: epoch 0049, iter [02600, 05004], lr: 0.052329, loss: 3.7583
2022-10-11 19:33:55 - train: epoch 0049, iter [02700, 05004], lr: 0.052298, loss: 4.7270
2022-10-11 19:34:44 - train: epoch 0049, iter [02800, 05004], lr: 0.052267, loss: 4.2390
2022-10-11 19:35:34 - train: epoch 0049, iter [02900, 05004], lr: 0.052235, loss: 4.5044
2022-10-11 19:36:24 - train: epoch 0049, iter [03000, 05004], lr: 0.052204, loss: 3.2032
2022-10-11 19:37:13 - train: epoch 0049, iter [03100, 05004], lr: 0.052173, loss: 4.2755
2022-10-11 19:38:04 - train: epoch 0049, iter [03200, 05004], lr: 0.052141, loss: 4.6377
2022-10-11 19:38:53 - train: epoch 0049, iter [03300, 05004], lr: 0.052110, loss: 4.5058
2022-10-11 19:39:43 - train: epoch 0049, iter [03400, 05004], lr: 0.052079, loss: 4.6418
2022-10-11 19:40:31 - train: epoch 0049, iter [03500, 05004], lr: 0.052047, loss: 4.8457
2022-10-11 19:41:20 - train: epoch 0049, iter [03600, 05004], lr: 0.052016, loss: 4.3459
2022-10-11 19:42:11 - train: epoch 0049, iter [03700, 05004], lr: 0.051984, loss: 4.1432
2022-10-11 19:43:00 - train: epoch 0049, iter [03800, 05004], lr: 0.051953, loss: 4.6661
2022-10-11 19:43:51 - train: epoch 0049, iter [03900, 05004], lr: 0.051922, loss: 4.1466
2022-10-11 19:44:41 - train: epoch 0049, iter [04000, 05004], lr: 0.051890, loss: 4.1010
2022-10-11 19:45:31 - train: epoch 0049, iter [04100, 05004], lr: 0.051859, loss: 4.0272
2022-10-11 19:46:20 - train: epoch 0049, iter [04200, 05004], lr: 0.051828, loss: 4.1137
2022-10-11 19:47:10 - train: epoch 0049, iter [04300, 05004], lr: 0.051796, loss: 4.5332
2022-10-11 19:48:01 - train: epoch 0049, iter [04400, 05004], lr: 0.051765, loss: 3.8492
2022-10-11 19:48:50 - train: epoch 0049, iter [04500, 05004], lr: 0.051733, loss: 4.6487
2022-10-11 19:49:39 - train: epoch 0049, iter [04600, 05004], lr: 0.051702, loss: 4.1752
2022-10-11 19:50:31 - train: epoch 0049, iter [04700, 05004], lr: 0.051671, loss: 4.3970
2022-10-11 19:51:19 - train: epoch 0049, iter [04800, 05004], lr: 0.051639, loss: 3.4792
2022-10-11 19:52:07 - train: epoch 0049, iter [04900, 05004], lr: 0.051608, loss: 3.8284
2022-10-11 19:52:55 - train: epoch 0049, iter [05000, 05004], lr: 0.051577, loss: 4.1016
2022-10-11 19:52:57 - train: epoch 049, train_loss: 4.2642
2022-10-11 19:54:41 - eval: epoch: 049, acc1: 60.702%, acc5: 83.890%, test_loss: 1.9569, per_image_load_time: 1.297ms, per_image_inference_time: 0.510ms
2022-10-11 19:54:42 - until epoch: 049, best_acc1: 60.702%
2022-10-11 19:54:42 - epoch 050 lr: 0.051575
2022-10-11 19:55:37 - train: epoch 0050, iter [00100, 05004], lr: 0.051544, loss: 3.6756
2022-10-11 19:56:29 - train: epoch 0050, iter [00200, 05004], lr: 0.051513, loss: 4.9231
2022-10-11 19:57:19 - train: epoch 0050, iter [00300, 05004], lr: 0.051481, loss: 4.6041
2022-10-11 19:58:09 - train: epoch 0050, iter [00400, 05004], lr: 0.051450, loss: 4.5222
2022-10-11 19:58:58 - train: epoch 0050, iter [00500, 05004], lr: 0.051419, loss: 4.4518
2022-10-11 19:59:48 - train: epoch 0050, iter [00600, 05004], lr: 0.051387, loss: 5.0486
2022-10-11 20:00:38 - train: epoch 0050, iter [00700, 05004], lr: 0.051356, loss: 4.7486
2022-10-11 20:01:26 - train: epoch 0050, iter [00800, 05004], lr: 0.051324, loss: 3.8596
2022-10-11 20:02:14 - train: epoch 0050, iter [00900, 05004], lr: 0.051293, loss: 4.4592
2022-10-11 20:03:03 - train: epoch 0050, iter [01000, 05004], lr: 0.051262, loss: 3.7478
2022-10-11 20:03:51 - train: epoch 0050, iter [01100, 05004], lr: 0.051230, loss: 4.2350
2022-10-11 20:04:40 - train: epoch 0050, iter [01200, 05004], lr: 0.051199, loss: 4.3615
2022-10-11 20:05:28 - train: epoch 0050, iter [01300, 05004], lr: 0.051167, loss: 4.6412
2022-10-11 20:06:18 - train: epoch 0050, iter [01400, 05004], lr: 0.051136, loss: 4.7768
2022-10-11 20:07:06 - train: epoch 0050, iter [01500, 05004], lr: 0.051105, loss: 5.1266
2022-10-11 20:07:56 - train: epoch 0050, iter [01600, 05004], lr: 0.051073, loss: 3.5472
2022-10-11 20:08:47 - train: epoch 0050, iter [01700, 05004], lr: 0.051042, loss: 4.5933
2022-10-11 20:09:36 - train: epoch 0050, iter [01800, 05004], lr: 0.051011, loss: 3.6574
2022-10-11 20:10:25 - train: epoch 0050, iter [01900, 05004], lr: 0.050979, loss: 4.4651
2022-10-11 20:11:15 - train: epoch 0050, iter [02000, 05004], lr: 0.050948, loss: 4.3632
2022-10-11 20:12:02 - train: epoch 0050, iter [02100, 05004], lr: 0.050916, loss: 4.0471
2022-10-11 20:12:50 - train: epoch 0050, iter [02200, 05004], lr: 0.050885, loss: 3.7858
2022-10-11 20:13:39 - train: epoch 0050, iter [02300, 05004], lr: 0.050854, loss: 4.3500
2022-10-11 20:14:29 - train: epoch 0050, iter [02400, 05004], lr: 0.050822, loss: 4.1111
2022-10-11 20:15:19 - train: epoch 0050, iter [02500, 05004], lr: 0.050791, loss: 4.2046
2022-10-11 20:16:08 - train: epoch 0050, iter [02600, 05004], lr: 0.050760, loss: 3.1998
2022-10-11 20:16:59 - train: epoch 0050, iter [02700, 05004], lr: 0.050728, loss: 4.2127
2022-10-11 20:17:49 - train: epoch 0050, iter [02800, 05004], lr: 0.050697, loss: 4.5486
2022-10-11 20:18:40 - train: epoch 0050, iter [02900, 05004], lr: 0.050665, loss: 4.6174
2022-10-11 20:19:30 - train: epoch 0050, iter [03000, 05004], lr: 0.050634, loss: 4.1827
2022-10-11 20:20:20 - train: epoch 0050, iter [03100, 05004], lr: 0.050603, loss: 4.1291
2022-10-11 20:21:08 - train: epoch 0050, iter [03200, 05004], lr: 0.050571, loss: 4.0134
2022-10-11 20:21:59 - train: epoch 0050, iter [03300, 05004], lr: 0.050540, loss: 4.4755
2022-10-11 20:22:49 - train: epoch 0050, iter [03400, 05004], lr: 0.050508, loss: 3.8323
2022-10-11 20:23:38 - train: epoch 0050, iter [03500, 05004], lr: 0.050477, loss: 3.6436
2022-10-11 20:24:28 - train: epoch 0050, iter [03600, 05004], lr: 0.050446, loss: 3.6757
2022-10-11 20:25:18 - train: epoch 0050, iter [03700, 05004], lr: 0.050414, loss: 4.0762
2022-10-11 20:26:08 - train: epoch 0050, iter [03800, 05004], lr: 0.050383, loss: 4.0912
2022-10-11 20:26:57 - train: epoch 0050, iter [03900, 05004], lr: 0.050352, loss: 4.3011
2022-10-11 20:27:48 - train: epoch 0050, iter [04000, 05004], lr: 0.050320, loss: 4.1975
2022-10-11 20:28:37 - train: epoch 0050, iter [04100, 05004], lr: 0.050289, loss: 4.4738
2022-10-11 20:29:27 - train: epoch 0050, iter [04200, 05004], lr: 0.050257, loss: 5.1623
2022-10-11 20:30:17 - train: epoch 0050, iter [04300, 05004], lr: 0.050226, loss: 4.3774
2022-10-11 20:31:06 - train: epoch 0050, iter [04400, 05004], lr: 0.050195, loss: 3.9143
2022-10-11 20:31:54 - train: epoch 0050, iter [04500, 05004], lr: 0.050163, loss: 4.6085
2022-10-11 20:32:47 - train: epoch 0050, iter [04600, 05004], lr: 0.050132, loss: 3.8234
2022-10-11 20:33:38 - train: epoch 0050, iter [04700, 05004], lr: 0.050100, loss: 4.8030
2022-10-11 20:34:27 - train: epoch 0050, iter [04800, 05004], lr: 0.050069, loss: 3.7026
2022-10-11 20:35:17 - train: epoch 0050, iter [04900, 05004], lr: 0.050038, loss: 4.3653
2022-10-11 20:36:03 - train: epoch 0050, iter [05000, 05004], lr: 0.050006, loss: 4.2764
2022-10-11 20:36:05 - train: epoch 050, train_loss: 4.2533
2022-10-11 20:37:49 - eval: epoch: 050, acc1: 61.398%, acc5: 84.558%, test_loss: 1.8654, per_image_load_time: 2.084ms, per_image_inference_time: 0.527ms
2022-10-11 20:37:50 - until epoch: 050, best_acc1: 61.398%
2022-10-11 20:37:50 - epoch 051 lr: 0.050005
2022-10-11 20:38:47 - train: epoch 0051, iter [00100, 05004], lr: 0.049974, loss: 4.5783
2022-10-11 20:39:37 - train: epoch 0051, iter [00200, 05004], lr: 0.049942, loss: 4.1625
2022-10-11 20:40:25 - train: epoch 0051, iter [00300, 05004], lr: 0.049911, loss: 4.4936
2022-10-11 20:41:14 - train: epoch 0051, iter [00400, 05004], lr: 0.049879, loss: 3.9292
2022-10-11 20:42:03 - train: epoch 0051, iter [00500, 05004], lr: 0.049848, loss: 3.8482
2022-10-11 20:42:53 - train: epoch 0051, iter [00600, 05004], lr: 0.049817, loss: 4.5539
2022-10-11 20:43:44 - train: epoch 0051, iter [00700, 05004], lr: 0.049785, loss: 4.0104
2022-10-11 20:44:34 - train: epoch 0051, iter [00800, 05004], lr: 0.049754, loss: 4.9171
2022-10-11 20:45:25 - train: epoch 0051, iter [00900, 05004], lr: 0.049723, loss: 3.6765
2022-10-11 20:46:15 - train: epoch 0051, iter [01000, 05004], lr: 0.049691, loss: 4.2972
2022-10-11 20:47:05 - train: epoch 0051, iter [01100, 05004], lr: 0.049660, loss: 4.1263
2022-10-11 20:47:56 - train: epoch 0051, iter [01200, 05004], lr: 0.049628, loss: 4.0323
2022-10-11 20:48:46 - train: epoch 0051, iter [01300, 05004], lr: 0.049597, loss: 3.4740
2022-10-11 20:49:36 - train: epoch 0051, iter [01400, 05004], lr: 0.049566, loss: 3.5668
2022-10-11 20:50:25 - train: epoch 0051, iter [01500, 05004], lr: 0.049534, loss: 4.6134
2022-10-11 20:51:15 - train: epoch 0051, iter [01600, 05004], lr: 0.049503, loss: 3.7344
2022-10-11 20:52:05 - train: epoch 0051, iter [01700, 05004], lr: 0.049471, loss: 4.3797
2022-10-11 20:52:55 - train: epoch 0051, iter [01800, 05004], lr: 0.049440, loss: 3.9446
2022-10-11 20:53:44 - train: epoch 0051, iter [01900, 05004], lr: 0.049409, loss: 3.9999
2022-10-11 20:54:36 - train: epoch 0051, iter [02000, 05004], lr: 0.049377, loss: 5.0148
2022-10-11 20:55:27 - train: epoch 0051, iter [02100, 05004], lr: 0.049346, loss: 4.7000
2022-10-11 20:56:18 - train: epoch 0051, iter [02200, 05004], lr: 0.049314, loss: 4.4648
2022-10-11 20:57:08 - train: epoch 0051, iter [02300, 05004], lr: 0.049283, loss: 4.7122
2022-10-11 20:57:59 - train: epoch 0051, iter [02400, 05004], lr: 0.049252, loss: 4.8385
2022-10-11 20:58:50 - train: epoch 0051, iter [02500, 05004], lr: 0.049220, loss: 4.0492
2022-10-11 20:59:40 - train: epoch 0051, iter [02600, 05004], lr: 0.049189, loss: 3.7709
2022-10-11 21:00:29 - train: epoch 0051, iter [02700, 05004], lr: 0.049158, loss: 4.7051
2022-10-11 21:01:18 - train: epoch 0051, iter [02800, 05004], lr: 0.049126, loss: 4.5064
2022-10-11 21:02:08 - train: epoch 0051, iter [02900, 05004], lr: 0.049095, loss: 3.8122
2022-10-11 21:02:59 - train: epoch 0051, iter [03000, 05004], lr: 0.049063, loss: 3.9898
2022-10-11 21:03:50 - train: epoch 0051, iter [03100, 05004], lr: 0.049032, loss: 4.2813
2022-10-11 21:04:40 - train: epoch 0051, iter [03200, 05004], lr: 0.049001, loss: 3.7314
2022-10-11 21:05:32 - train: epoch 0051, iter [03300, 05004], lr: 0.048969, loss: 4.6422
2022-10-11 21:06:21 - train: epoch 0051, iter [03400, 05004], lr: 0.048938, loss: 4.2927
2022-10-11 21:07:11 - train: epoch 0051, iter [03500, 05004], lr: 0.048907, loss: 3.3743
2022-10-11 21:08:02 - train: epoch 0051, iter [03600, 05004], lr: 0.048875, loss: 4.0604
2022-10-11 21:08:53 - train: epoch 0051, iter [03700, 05004], lr: 0.048844, loss: 4.3645
2022-10-11 21:09:43 - train: epoch 0051, iter [03800, 05004], lr: 0.048812, loss: 3.9884
2022-10-11 21:10:32 - train: epoch 0051, iter [03900, 05004], lr: 0.048781, loss: 3.6404
2022-10-11 21:11:22 - train: epoch 0051, iter [04000, 05004], lr: 0.048750, loss: 4.0235
2022-10-11 21:12:13 - train: epoch 0051, iter [04100, 05004], lr: 0.048718, loss: 4.1166
2022-10-11 21:13:02 - train: epoch 0051, iter [04200, 05004], lr: 0.048687, loss: 4.0477
2022-10-11 21:13:56 - train: epoch 0051, iter [04300, 05004], lr: 0.048655, loss: 4.4266
2022-10-11 21:14:47 - train: epoch 0051, iter [04400, 05004], lr: 0.048624, loss: 4.6241
2022-10-11 21:15:37 - train: epoch 0051, iter [04500, 05004], lr: 0.048593, loss: 3.9582
2022-10-11 21:16:26 - train: epoch 0051, iter [04600, 05004], lr: 0.048561, loss: 3.7978
2022-10-11 21:17:18 - train: epoch 0051, iter [04700, 05004], lr: 0.048530, loss: 4.2731
2022-10-11 21:18:08 - train: epoch 0051, iter [04800, 05004], lr: 0.048499, loss: 4.4667
2022-10-11 21:18:59 - train: epoch 0051, iter [04900, 05004], lr: 0.048467, loss: 4.3752
2022-10-11 21:19:45 - train: epoch 0051, iter [05000, 05004], lr: 0.048436, loss: 4.5234
2022-10-11 21:19:47 - train: epoch 051, train_loss: 4.2349
2022-10-11 21:21:33 - eval: epoch: 051, acc1: 61.254%, acc5: 84.236%, test_loss: 1.9922, per_image_load_time: 3.539ms, per_image_inference_time: 0.513ms
2022-10-11 21:21:34 - until epoch: 051, best_acc1: 61.398%
2022-10-11 21:21:34 - epoch 052 lr: 0.048435
2022-10-11 21:22:31 - train: epoch 0052, iter [00100, 05004], lr: 0.048403, loss: 4.3889
2022-10-11 21:23:22 - train: epoch 0052, iter [00200, 05004], lr: 0.048372, loss: 4.1862
2022-10-11 21:24:13 - train: epoch 0052, iter [00300, 05004], lr: 0.048341, loss: 4.1556
2022-10-11 21:25:03 - train: epoch 0052, iter [00400, 05004], lr: 0.048309, loss: 4.1169
2022-10-11 21:25:54 - train: epoch 0052, iter [00500, 05004], lr: 0.048278, loss: 4.5667
2022-10-11 21:26:46 - train: epoch 0052, iter [00600, 05004], lr: 0.048246, loss: 4.0162
2022-10-11 21:27:36 - train: epoch 0052, iter [00700, 05004], lr: 0.048215, loss: 4.4128
2022-10-11 21:28:27 - train: epoch 0052, iter [00800, 05004], lr: 0.048184, loss: 3.9637
2022-10-11 21:29:15 - train: epoch 0052, iter [00900, 05004], lr: 0.048152, loss: 3.8052
2022-10-11 21:30:06 - train: epoch 0052, iter [01000, 05004], lr: 0.048121, loss: 4.0484
2022-10-11 21:30:55 - train: epoch 0052, iter [01100, 05004], lr: 0.048090, loss: 5.1396
2022-10-11 21:31:46 - train: epoch 0052, iter [01200, 05004], lr: 0.048058, loss: 3.9212
2022-10-11 21:32:37 - train: epoch 0052, iter [01300, 05004], lr: 0.048027, loss: 4.8969
2022-10-11 21:33:27 - train: epoch 0052, iter [01400, 05004], lr: 0.047995, loss: 4.3031
2022-10-11 21:34:17 - train: epoch 0052, iter [01500, 05004], lr: 0.047964, loss: 3.5400
2022-10-11 21:35:08 - train: epoch 0052, iter [01600, 05004], lr: 0.047933, loss: 3.8433
2022-10-11 21:35:57 - train: epoch 0052, iter [01700, 05004], lr: 0.047901, loss: 4.6653
2022-10-11 21:36:47 - train: epoch 0052, iter [01800, 05004], lr: 0.047870, loss: 4.1118
2022-10-11 21:37:37 - train: epoch 0052, iter [01900, 05004], lr: 0.047839, loss: 4.0312
2022-10-11 21:38:27 - train: epoch 0052, iter [02000, 05004], lr: 0.047807, loss: 4.6995
2022-10-11 21:39:16 - train: epoch 0052, iter [02100, 05004], lr: 0.047776, loss: 4.3101
2022-10-11 21:40:05 - train: epoch 0052, iter [02200, 05004], lr: 0.047745, loss: 4.8348
2022-10-11 21:40:56 - train: epoch 0052, iter [02300, 05004], lr: 0.047713, loss: 3.7619
2022-10-11 21:41:46 - train: epoch 0052, iter [02400, 05004], lr: 0.047682, loss: 3.8942
2022-10-11 21:42:37 - train: epoch 0052, iter [02500, 05004], lr: 0.047651, loss: 4.1944
2022-10-11 21:43:27 - train: epoch 0052, iter [02600, 05004], lr: 0.047619, loss: 4.4690
2022-10-11 21:44:17 - train: epoch 0052, iter [02700, 05004], lr: 0.047588, loss: 4.6074
2022-10-11 21:45:07 - train: epoch 0052, iter [02800, 05004], lr: 0.047556, loss: 4.3595
2022-10-11 21:45:57 - train: epoch 0052, iter [02900, 05004], lr: 0.047525, loss: 4.6406
2022-10-11 21:46:47 - train: epoch 0052, iter [03000, 05004], lr: 0.047494, loss: 3.4382
2022-10-11 21:47:37 - train: epoch 0052, iter [03100, 05004], lr: 0.047462, loss: 3.1034
2022-10-11 21:48:26 - train: epoch 0052, iter [03200, 05004], lr: 0.047431, loss: 4.6470
2022-10-11 21:49:16 - train: epoch 0052, iter [03300, 05004], lr: 0.047400, loss: 4.7520
2022-10-11 21:50:05 - train: epoch 0052, iter [03400, 05004], lr: 0.047368, loss: 4.6352
2022-10-11 21:50:55 - train: epoch 0052, iter [03500, 05004], lr: 0.047337, loss: 3.8630
2022-10-11 21:51:45 - train: epoch 0052, iter [03600, 05004], lr: 0.047306, loss: 3.7844
2022-10-11 21:52:36 - train: epoch 0052, iter [03700, 05004], lr: 0.047274, loss: 4.6632
2022-10-11 21:53:27 - train: epoch 0052, iter [03800, 05004], lr: 0.047243, loss: 4.4035
2022-10-11 21:54:16 - train: epoch 0052, iter [03900, 05004], lr: 0.047212, loss: 4.1736
2022-10-11 21:55:08 - train: epoch 0052, iter [04000, 05004], lr: 0.047180, loss: 4.6746
2022-10-11 21:55:56 - train: epoch 0052, iter [04100, 05004], lr: 0.047149, loss: 3.9218
2022-10-11 21:56:48 - train: epoch 0052, iter [04200, 05004], lr: 0.047118, loss: 4.3398
2022-10-11 21:57:38 - train: epoch 0052, iter [04300, 05004], lr: 0.047086, loss: 4.1062
2022-10-11 21:58:28 - train: epoch 0052, iter [04400, 05004], lr: 0.047055, loss: 4.5482
2022-10-11 21:59:16 - train: epoch 0052, iter [04500, 05004], lr: 0.047024, loss: 4.1453
2022-10-11 22:00:07 - train: epoch 0052, iter [04600, 05004], lr: 0.046992, loss: 4.0188
2022-10-11 22:00:58 - train: epoch 0052, iter [04700, 05004], lr: 0.046961, loss: 3.2770
2022-10-11 22:01:49 - train: epoch 0052, iter [04800, 05004], lr: 0.046930, loss: 3.7141
2022-10-11 22:02:39 - train: epoch 0052, iter [04900, 05004], lr: 0.046898, loss: 3.8301
2022-10-11 22:03:28 - train: epoch 0052, iter [05000, 05004], lr: 0.046867, loss: 3.4508
2022-10-11 22:03:30 - train: epoch 052, train_loss: 4.2198
2022-10-11 22:05:15 - eval: epoch: 052, acc1: 62.004%, acc5: 85.034%, test_loss: 1.8470, per_image_load_time: 2.473ms, per_image_inference_time: 0.508ms
2022-10-11 22:05:15 - until epoch: 052, best_acc1: 62.004%
2022-10-11 22:05:15 - epoch 053 lr: 0.046866
2022-10-11 22:06:13 - train: epoch 0053, iter [00100, 05004], lr: 0.046834, loss: 5.0540
2022-10-11 22:07:02 - train: epoch 0053, iter [00200, 05004], lr: 0.046803, loss: 4.3785
2022-10-11 22:07:53 - train: epoch 0053, iter [00300, 05004], lr: 0.046772, loss: 4.0594
2022-10-11 22:08:42 - train: epoch 0053, iter [00400, 05004], lr: 0.046740, loss: 4.7155
2022-10-11 22:09:32 - train: epoch 0053, iter [00500, 05004], lr: 0.046709, loss: 4.3646
2022-10-11 22:10:21 - train: epoch 0053, iter [00600, 05004], lr: 0.046678, loss: 4.0969
2022-10-11 22:11:11 - train: epoch 0053, iter [00700, 05004], lr: 0.046647, loss: 4.1236
2022-10-11 22:12:03 - train: epoch 0053, iter [00800, 05004], lr: 0.046615, loss: 4.7004
2022-10-11 22:12:55 - train: epoch 0053, iter [00900, 05004], lr: 0.046584, loss: 4.3104
2022-10-11 22:13:44 - train: epoch 0053, iter [01000, 05004], lr: 0.046553, loss: 4.4251
2022-10-11 22:14:34 - train: epoch 0053, iter [01100, 05004], lr: 0.046521, loss: 3.5099
2022-10-11 22:15:25 - train: epoch 0053, iter [01200, 05004], lr: 0.046490, loss: 4.7184
2022-10-11 22:16:16 - train: epoch 0053, iter [01300, 05004], lr: 0.046459, loss: 3.9539
2022-10-11 22:17:06 - train: epoch 0053, iter [01400, 05004], lr: 0.046427, loss: 3.9451
2022-10-11 22:17:56 - train: epoch 0053, iter [01500, 05004], lr: 0.046396, loss: 4.0995
2022-10-11 22:18:45 - train: epoch 0053, iter [01600, 05004], lr: 0.046365, loss: 4.6919
2022-10-11 22:19:35 - train: epoch 0053, iter [01700, 05004], lr: 0.046333, loss: 4.7068
2022-10-11 22:20:26 - train: epoch 0053, iter [01800, 05004], lr: 0.046302, loss: 3.4393
2022-10-11 22:21:17 - train: epoch 0053, iter [01900, 05004], lr: 0.046271, loss: 4.3963
2022-10-11 22:22:07 - train: epoch 0053, iter [02000, 05004], lr: 0.046240, loss: 4.7540
2022-10-11 22:22:57 - train: epoch 0053, iter [02100, 05004], lr: 0.046208, loss: 5.0742
2022-10-11 22:23:47 - train: epoch 0053, iter [02200, 05004], lr: 0.046177, loss: 4.4956
2022-10-11 22:24:37 - train: epoch 0053, iter [02300, 05004], lr: 0.046146, loss: 4.3600
2022-10-11 22:25:28 - train: epoch 0053, iter [02400, 05004], lr: 0.046114, loss: 3.2304
2022-10-11 22:26:19 - train: epoch 0053, iter [02500, 05004], lr: 0.046083, loss: 4.5146
2022-10-11 22:27:09 - train: epoch 0053, iter [02600, 05004], lr: 0.046052, loss: 3.4407
2022-10-11 22:27:57 - train: epoch 0053, iter [02700, 05004], lr: 0.046020, loss: 4.2783
2022-10-11 22:28:47 - train: epoch 0053, iter [02800, 05004], lr: 0.045989, loss: 3.7779
2022-10-11 22:29:36 - train: epoch 0053, iter [02900, 05004], lr: 0.045958, loss: 3.3043
2022-10-11 22:30:27 - train: epoch 0053, iter [03000, 05004], lr: 0.045927, loss: 3.9248
2022-10-11 22:31:16 - train: epoch 0053, iter [03100, 05004], lr: 0.045895, loss: 4.8133
2022-10-11 22:32:07 - train: epoch 0053, iter [03200, 05004], lr: 0.045864, loss: 4.1026
2022-10-11 22:32:57 - train: epoch 0053, iter [03300, 05004], lr: 0.045833, loss: 4.0129
2022-10-11 22:33:47 - train: epoch 0053, iter [03400, 05004], lr: 0.045802, loss: 3.9145
2022-10-11 22:34:37 - train: epoch 0053, iter [03500, 05004], lr: 0.045770, loss: 4.7235
2022-10-11 22:35:28 - train: epoch 0053, iter [03600, 05004], lr: 0.045739, loss: 4.1844
2022-10-11 22:36:18 - train: epoch 0053, iter [03700, 05004], lr: 0.045708, loss: 4.2933
2022-10-11 22:37:07 - train: epoch 0053, iter [03800, 05004], lr: 0.045676, loss: 4.3095
2022-10-11 22:37:56 - train: epoch 0053, iter [03900, 05004], lr: 0.045645, loss: 4.6290
2022-10-11 22:38:46 - train: epoch 0053, iter [04000, 05004], lr: 0.045614, loss: 4.2243
2022-10-11 22:39:35 - train: epoch 0053, iter [04100, 05004], lr: 0.045583, loss: 4.4427
2022-10-11 22:40:25 - train: epoch 0053, iter [04200, 05004], lr: 0.045551, loss: 4.0432
2022-10-11 22:41:16 - train: epoch 0053, iter [04300, 05004], lr: 0.045520, loss: 4.4612
2022-10-11 22:42:06 - train: epoch 0053, iter [04400, 05004], lr: 0.045489, loss: 4.8974
2022-10-11 22:42:56 - train: epoch 0053, iter [04500, 05004], lr: 0.045458, loss: 3.7725
2022-10-11 22:43:47 - train: epoch 0053, iter [04600, 05004], lr: 0.045426, loss: 4.2784
2022-10-11 22:44:36 - train: epoch 0053, iter [04700, 05004], lr: 0.045395, loss: 4.6417
2022-10-11 22:45:27 - train: epoch 0053, iter [04800, 05004], lr: 0.045364, loss: 4.4795
2022-10-11 22:46:17 - train: epoch 0053, iter [04900, 05004], lr: 0.045333, loss: 4.4992
2022-10-11 22:47:05 - train: epoch 0053, iter [05000, 05004], lr: 0.045301, loss: 4.0586
2022-10-11 22:47:07 - train: epoch 053, train_loss: 4.2099
2022-10-11 22:48:50 - eval: epoch: 053, acc1: 61.626%, acc5: 84.802%, test_loss: 1.8983, per_image_load_time: 2.188ms, per_image_inference_time: 0.511ms
2022-10-11 22:48:51 - until epoch: 053, best_acc1: 62.004%
2022-10-11 22:48:51 - epoch 054 lr: 0.045300
2022-10-11 22:49:48 - train: epoch 0054, iter [00100, 05004], lr: 0.045269, loss: 4.3683
2022-10-11 22:50:38 - train: epoch 0054, iter [00200, 05004], lr: 0.045238, loss: 4.2877
2022-10-11 22:51:28 - train: epoch 0054, iter [00300, 05004], lr: 0.045206, loss: 4.5996
2022-10-11 22:52:20 - train: epoch 0054, iter [00400, 05004], lr: 0.045175, loss: 4.0260
2022-10-11 22:53:09 - train: epoch 0054, iter [00500, 05004], lr: 0.045144, loss: 4.9488
2022-10-11 22:54:00 - train: epoch 0054, iter [00600, 05004], lr: 0.045113, loss: 4.2424
2022-10-11 22:54:50 - train: epoch 0054, iter [00700, 05004], lr: 0.045081, loss: 4.5198
2022-10-11 22:55:42 - train: epoch 0054, iter [00800, 05004], lr: 0.045050, loss: 4.2178
2022-10-11 22:56:31 - train: epoch 0054, iter [00900, 05004], lr: 0.045019, loss: 4.4255
2022-10-11 22:57:20 - train: epoch 0054, iter [01000, 05004], lr: 0.044988, loss: 4.4681
2022-10-11 22:58:10 - train: epoch 0054, iter [01100, 05004], lr: 0.044956, loss: 4.1965
2022-10-11 22:58:58 - train: epoch 0054, iter [01200, 05004], lr: 0.044925, loss: 3.8574
2022-10-11 22:59:48 - train: epoch 0054, iter [01300, 05004], lr: 0.044894, loss: 4.1122
2022-10-11 23:00:38 - train: epoch 0054, iter [01400, 05004], lr: 0.044863, loss: 4.2713
2022-10-11 23:01:28 - train: epoch 0054, iter [01500, 05004], lr: 0.044832, loss: 4.0987
2022-10-11 23:02:17 - train: epoch 0054, iter [01600, 05004], lr: 0.044800, loss: 4.8332
2022-10-11 23:03:09 - train: epoch 0054, iter [01700, 05004], lr: 0.044769, loss: 4.6405
2022-10-11 23:03:59 - train: epoch 0054, iter [01800, 05004], lr: 0.044738, loss: 4.3448
2022-10-11 23:04:50 - train: epoch 0054, iter [01900, 05004], lr: 0.044707, loss: 4.8394
2022-10-11 23:05:39 - train: epoch 0054, iter [02000, 05004], lr: 0.044675, loss: 4.8334
2022-10-11 23:06:29 - train: epoch 0054, iter [02100, 05004], lr: 0.044644, loss: 4.2212
2022-10-11 23:07:16 - train: epoch 0054, iter [02200, 05004], lr: 0.044613, loss: 3.1209
2022-10-11 23:08:07 - train: epoch 0054, iter [02300, 05004], lr: 0.044582, loss: 3.5837
2022-10-11 23:08:58 - train: epoch 0054, iter [02400, 05004], lr: 0.044551, loss: 3.7664
2022-10-11 23:09:47 - train: epoch 0054, iter [02500, 05004], lr: 0.044519, loss: 4.0823
2022-10-11 23:10:37 - train: epoch 0054, iter [02600, 05004], lr: 0.044488, loss: 4.3160
2022-10-11 23:11:29 - train: epoch 0054, iter [02700, 05004], lr: 0.044457, loss: 4.2589
2022-10-11 23:12:19 - train: epoch 0054, iter [02800, 05004], lr: 0.044426, loss: 5.0415
2022-10-11 23:13:10 - train: epoch 0054, iter [02900, 05004], lr: 0.044395, loss: 3.2200
2022-10-11 23:14:00 - train: epoch 0054, iter [03000, 05004], lr: 0.044363, loss: 3.4897
2022-10-11 23:14:50 - train: epoch 0054, iter [03100, 05004], lr: 0.044332, loss: 3.1143
2022-10-11 23:15:39 - train: epoch 0054, iter [03200, 05004], lr: 0.044301, loss: 4.3586
2022-10-11 23:16:28 - train: epoch 0054, iter [03300, 05004], lr: 0.044270, loss: 4.5014
2022-10-11 23:17:17 - train: epoch 0054, iter [03400, 05004], lr: 0.044239, loss: 4.5063
2022-10-11 23:18:05 - train: epoch 0054, iter [03500, 05004], lr: 0.044208, loss: 4.1155
2022-10-11 23:18:54 - train: epoch 0054, iter [03600, 05004], lr: 0.044176, loss: 4.8324
2022-10-11 23:19:41 - train: epoch 0054, iter [03700, 05004], lr: 0.044145, loss: 4.3403
2022-10-11 23:20:30 - train: epoch 0054, iter [03800, 05004], lr: 0.044114, loss: 3.8169
2022-10-11 23:21:19 - train: epoch 0054, iter [03900, 05004], lr: 0.044083, loss: 4.8187
2022-10-11 23:22:08 - train: epoch 0054, iter [04000, 05004], lr: 0.044052, loss: 4.4089
2022-10-11 23:22:57 - train: epoch 0054, iter [04100, 05004], lr: 0.044021, loss: 4.3842
2022-10-11 23:23:46 - train: epoch 0054, iter [04200, 05004], lr: 0.043989, loss: 4.0083
2022-10-11 23:24:35 - train: epoch 0054, iter [04300, 05004], lr: 0.043958, loss: 4.1853
2022-10-11 23:25:23 - train: epoch 0054, iter [04400, 05004], lr: 0.043927, loss: 4.6775
2022-10-11 23:26:11 - train: epoch 0054, iter [04500, 05004], lr: 0.043896, loss: 3.3287
2022-10-11 23:26:58 - train: epoch 0054, iter [04600, 05004], lr: 0.043865, loss: 3.7936
2022-10-11 23:27:48 - train: epoch 0054, iter [04700, 05004], lr: 0.043834, loss: 4.1697
2022-10-11 23:28:36 - train: epoch 0054, iter [04800, 05004], lr: 0.043802, loss: 4.0227
2022-10-11 23:29:27 - train: epoch 0054, iter [04900, 05004], lr: 0.043771, loss: 4.2766
2022-10-11 23:30:11 - train: epoch 0054, iter [05000, 05004], lr: 0.043740, loss: 3.8461
2022-10-11 23:30:13 - train: epoch 054, train_loss: 4.2043
2022-10-11 23:31:55 - eval: epoch: 054, acc1: 62.232%, acc5: 85.030%, test_loss: 1.7850, per_image_load_time: 3.183ms, per_image_inference_time: 0.491ms
2022-10-11 23:31:55 - until epoch: 054, best_acc1: 62.232%
2022-10-11 23:31:55 - epoch 055 lr: 0.043739
2022-10-11 23:32:49 - train: epoch 0055, iter [00100, 05004], lr: 0.043708, loss: 4.4354
2022-10-11 23:33:39 - train: epoch 0055, iter [00200, 05004], lr: 0.043677, loss: 3.8366
2022-10-11 23:34:27 - train: epoch 0055, iter [00300, 05004], lr: 0.043646, loss: 4.1600
2022-10-11 23:35:16 - train: epoch 0055, iter [00400, 05004], lr: 0.043614, loss: 4.1267
2022-10-11 23:36:06 - train: epoch 0055, iter [00500, 05004], lr: 0.043583, loss: 3.8871
2022-10-11 23:36:54 - train: epoch 0055, iter [00600, 05004], lr: 0.043552, loss: 4.7235
2022-10-11 23:37:42 - train: epoch 0055, iter [00700, 05004], lr: 0.043521, loss: 4.3107
2022-10-11 23:38:30 - train: epoch 0055, iter [00800, 05004], lr: 0.043490, loss: 3.4890
2022-10-11 23:39:20 - train: epoch 0055, iter [00900, 05004], lr: 0.043459, loss: 3.9649
2022-10-11 23:40:07 - train: epoch 0055, iter [01000, 05004], lr: 0.043428, loss: 4.4820
2022-10-11 23:40:56 - train: epoch 0055, iter [01100, 05004], lr: 0.043397, loss: 4.4058
2022-10-11 23:41:45 - train: epoch 0055, iter [01200, 05004], lr: 0.043365, loss: 5.0059
2022-10-11 23:42:34 - train: epoch 0055, iter [01300, 05004], lr: 0.043334, loss: 4.0328
2022-10-11 23:43:24 - train: epoch 0055, iter [01400, 05004], lr: 0.043303, loss: 4.4182
2022-10-11 23:44:13 - train: epoch 0055, iter [01500, 05004], lr: 0.043272, loss: 4.0913
2022-10-11 23:45:01 - train: epoch 0055, iter [01600, 05004], lr: 0.043241, loss: 4.7242
2022-10-11 23:45:49 - train: epoch 0055, iter [01700, 05004], lr: 0.043210, loss: 3.9063
2022-10-11 23:46:38 - train: epoch 0055, iter [01800, 05004], lr: 0.043179, loss: 4.7116
2022-10-11 23:47:27 - train: epoch 0055, iter [01900, 05004], lr: 0.043148, loss: 4.4634
2022-10-11 23:48:15 - train: epoch 0055, iter [02000, 05004], lr: 0.043117, loss: 4.7417
2022-10-11 23:49:04 - train: epoch 0055, iter [02100, 05004], lr: 0.043086, loss: 3.9145
2022-10-11 23:49:53 - train: epoch 0055, iter [02200, 05004], lr: 0.043055, loss: 4.5056
2022-10-11 23:50:41 - train: epoch 0055, iter [02300, 05004], lr: 0.043023, loss: 3.3141
2022-10-11 23:51:30 - train: epoch 0055, iter [02400, 05004], lr: 0.042992, loss: 4.4367
2022-10-11 23:52:21 - train: epoch 0055, iter [02500, 05004], lr: 0.042961, loss: 4.2130
2022-10-11 23:53:09 - train: epoch 0055, iter [02600, 05004], lr: 0.042930, loss: 2.7980
2022-10-11 23:53:58 - train: epoch 0055, iter [02700, 05004], lr: 0.042899, loss: 4.7598
2022-10-11 23:54:49 - train: epoch 0055, iter [02800, 05004], lr: 0.042868, loss: 4.2930
2022-10-11 23:55:37 - train: epoch 0055, iter [02900, 05004], lr: 0.042837, loss: 4.3165
2022-10-11 23:56:27 - train: epoch 0055, iter [03000, 05004], lr: 0.042806, loss: 4.0082
2022-10-11 23:57:16 - train: epoch 0055, iter [03100, 05004], lr: 0.042775, loss: 4.6750
2022-10-11 23:58:06 - train: epoch 0055, iter [03200, 05004], lr: 0.042744, loss: 3.8557
2022-10-11 23:58:58 - train: epoch 0055, iter [03300, 05004], lr: 0.042713, loss: 3.9171
2022-10-11 23:59:47 - train: epoch 0055, iter [03400, 05004], lr: 0.042682, loss: 3.8805
2022-10-12 00:00:37 - train: epoch 0055, iter [03500, 05004], lr: 0.042651, loss: 4.1644
2022-10-12 00:01:26 - train: epoch 0055, iter [03600, 05004], lr: 0.042620, loss: 4.2900
2022-10-12 00:02:16 - train: epoch 0055, iter [03700, 05004], lr: 0.042589, loss: 4.5495
2022-10-12 00:03:06 - train: epoch 0055, iter [03800, 05004], lr: 0.042558, loss: 4.5402
2022-10-12 00:03:58 - train: epoch 0055, iter [03900, 05004], lr: 0.042526, loss: 3.9318
2022-10-12 00:04:48 - train: epoch 0055, iter [04000, 05004], lr: 0.042495, loss: 4.5175
2022-10-12 00:05:36 - train: epoch 0055, iter [04100, 05004], lr: 0.042464, loss: 4.0055
2022-10-12 00:06:25 - train: epoch 0055, iter [04200, 05004], lr: 0.042433, loss: 4.4805
2022-10-12 00:07:16 - train: epoch 0055, iter [04300, 05004], lr: 0.042402, loss: 3.9421
2022-10-12 00:08:05 - train: epoch 0055, iter [04400, 05004], lr: 0.042371, loss: 4.0035
2022-10-12 00:08:55 - train: epoch 0055, iter [04500, 05004], lr: 0.042340, loss: 3.6306
2022-10-12 00:09:44 - train: epoch 0055, iter [04600, 05004], lr: 0.042309, loss: 3.8630
2022-10-12 00:10:33 - train: epoch 0055, iter [04700, 05004], lr: 0.042278, loss: 4.1080
2022-10-12 00:11:24 - train: epoch 0055, iter [04800, 05004], lr: 0.042247, loss: 4.6964
2022-10-12 00:12:15 - train: epoch 0055, iter [04900, 05004], lr: 0.042216, loss: 4.1477
2022-10-12 00:13:04 - train: epoch 0055, iter [05000, 05004], lr: 0.042185, loss: 4.8011
2022-10-12 00:13:06 - train: epoch 055, train_loss: 4.1893
2022-10-12 00:14:51 - eval: epoch: 055, acc1: 63.556%, acc5: 85.910%, test_loss: 1.8399, per_image_load_time: 3.079ms, per_image_inference_time: 0.536ms
2022-10-12 00:14:51 - until epoch: 055, best_acc1: 63.556%
2022-10-12 00:14:51 - epoch 056 lr: 0.042184
2022-10-12 00:15:47 - train: epoch 0056, iter [00100, 05004], lr: 0.042153, loss: 4.5490
2022-10-12 00:16:39 - train: epoch 0056, iter [00200, 05004], lr: 0.042122, loss: 3.2715
2022-10-12 00:17:29 - train: epoch 0056, iter [00300, 05004], lr: 0.042091, loss: 4.6166
2022-10-12 00:18:18 - train: epoch 0056, iter [00400, 05004], lr: 0.042060, loss: 4.2314
2022-10-12 00:19:09 - train: epoch 0056, iter [00500, 05004], lr: 0.042029, loss: 4.2326
2022-10-12 00:19:59 - train: epoch 0056, iter [00600, 05004], lr: 0.041998, loss: 4.1491
2022-10-12 00:20:48 - train: epoch 0056, iter [00700, 05004], lr: 0.041967, loss: 4.4260
2022-10-12 00:21:39 - train: epoch 0056, iter [00800, 05004], lr: 0.041936, loss: 4.5225
2022-10-12 00:22:28 - train: epoch 0056, iter [00900, 05004], lr: 0.041905, loss: 3.7811
2022-10-12 00:23:19 - train: epoch 0056, iter [01000, 05004], lr: 0.041874, loss: 4.5469
2022-10-12 00:24:08 - train: epoch 0056, iter [01100, 05004], lr: 0.041843, loss: 4.3938
2022-10-12 00:24:57 - train: epoch 0056, iter [01200, 05004], lr: 0.041812, loss: 4.3248
2022-10-12 00:25:46 - train: epoch 0056, iter [01300, 05004], lr: 0.041781, loss: 4.1143
2022-10-12 00:26:37 - train: epoch 0056, iter [01400, 05004], lr: 0.041750, loss: 3.3819
2022-10-12 00:27:27 - train: epoch 0056, iter [01500, 05004], lr: 0.041719, loss: 4.3309
2022-10-12 00:28:17 - train: epoch 0056, iter [01600, 05004], lr: 0.041688, loss: 3.6061
2022-10-12 00:29:06 - train: epoch 0056, iter [01700, 05004], lr: 0.041657, loss: 4.8810
2022-10-12 00:29:57 - train: epoch 0056, iter [01800, 05004], lr: 0.041627, loss: 3.2319
2022-10-12 00:30:47 - train: epoch 0056, iter [01900, 05004], lr: 0.041596, loss: 4.2211
2022-10-12 00:31:36 - train: epoch 0056, iter [02000, 05004], lr: 0.041565, loss: 4.0800
2022-10-12 00:32:26 - train: epoch 0056, iter [02100, 05004], lr: 0.041534, loss: 4.6247
2022-10-12 00:33:17 - train: epoch 0056, iter [02200, 05004], lr: 0.041503, loss: 3.9680
2022-10-12 00:34:06 - train: epoch 0056, iter [02300, 05004], lr: 0.041472, loss: 4.5517
2022-10-12 00:34:55 - train: epoch 0056, iter [02400, 05004], lr: 0.041441, loss: 3.6806
2022-10-12 00:35:45 - train: epoch 0056, iter [02500, 05004], lr: 0.041410, loss: 4.6682
2022-10-12 00:36:35 - train: epoch 0056, iter [02600, 05004], lr: 0.041379, loss: 4.3270
2022-10-12 00:37:25 - train: epoch 0056, iter [02700, 05004], lr: 0.041348, loss: 4.4986
2022-10-12 00:38:15 - train: epoch 0056, iter [02800, 05004], lr: 0.041317, loss: 4.6981
2022-10-12 00:39:05 - train: epoch 0056, iter [02900, 05004], lr: 0.041286, loss: 4.7332
2022-10-12 00:39:56 - train: epoch 0056, iter [03000, 05004], lr: 0.041255, loss: 4.8335
2022-10-12 00:40:45 - train: epoch 0056, iter [03100, 05004], lr: 0.041225, loss: 4.3893
2022-10-12 00:41:35 - train: epoch 0056, iter [03200, 05004], lr: 0.041194, loss: 4.7758
2022-10-12 00:42:26 - train: epoch 0056, iter [03300, 05004], lr: 0.041163, loss: 3.0356
2022-10-12 00:43:15 - train: epoch 0056, iter [03400, 05004], lr: 0.041132, loss: 4.0140
2022-10-12 00:44:04 - train: epoch 0056, iter [03500, 05004], lr: 0.041101, loss: 3.9974
2022-10-12 00:44:52 - train: epoch 0056, iter [03600, 05004], lr: 0.041070, loss: 3.6917
2022-10-12 00:45:42 - train: epoch 0056, iter [03700, 05004], lr: 0.041039, loss: 5.0270
2022-10-12 00:46:34 - train: epoch 0056, iter [03800, 05004], lr: 0.041008, loss: 4.8860
2022-10-12 00:47:24 - train: epoch 0056, iter [03900, 05004], lr: 0.040977, loss: 3.8794
2022-10-12 00:48:14 - train: epoch 0056, iter [04000, 05004], lr: 0.040947, loss: 4.2869
2022-10-12 00:49:04 - train: epoch 0056, iter [04100, 05004], lr: 0.040916, loss: 4.4900
2022-10-12 00:49:55 - train: epoch 0056, iter [04200, 05004], lr: 0.040885, loss: 3.9055
2022-10-12 00:50:45 - train: epoch 0056, iter [04300, 05004], lr: 0.040854, loss: 3.7160
2022-10-12 00:51:36 - train: epoch 0056, iter [04400, 05004], lr: 0.040823, loss: 4.5986
2022-10-12 00:52:26 - train: epoch 0056, iter [04500, 05004], lr: 0.040792, loss: 4.8807
2022-10-12 00:53:14 - train: epoch 0056, iter [04600, 05004], lr: 0.040761, loss: 3.7316
2022-10-12 00:54:03 - train: epoch 0056, iter [04700, 05004], lr: 0.040731, loss: 4.3104
2022-10-12 00:54:52 - train: epoch 0056, iter [04800, 05004], lr: 0.040700, loss: 4.2117
2022-10-12 00:55:42 - train: epoch 0056, iter [04900, 05004], lr: 0.040669, loss: 4.5591
2022-10-12 00:56:30 - train: epoch 0056, iter [05000, 05004], lr: 0.040638, loss: 4.4686
2022-10-12 00:56:33 - train: epoch 056, train_loss: 4.1752
2022-10-12 00:58:20 - eval: epoch: 056, acc1: 62.250%, acc5: 85.130%, test_loss: 1.7767, per_image_load_time: 3.264ms, per_image_inference_time: 0.509ms
2022-10-12 00:58:20 - until epoch: 056, best_acc1: 63.556%
2022-10-12 00:58:20 - epoch 057 lr: 0.040637
2022-10-12 00:59:16 - train: epoch 0057, iter [00100, 05004], lr: 0.040606, loss: 3.2201
2022-10-12 01:00:06 - train: epoch 0057, iter [00200, 05004], lr: 0.040575, loss: 4.5461
2022-10-12 01:00:58 - train: epoch 0057, iter [00300, 05004], lr: 0.040544, loss: 4.4707
2022-10-12 01:01:49 - train: epoch 0057, iter [00400, 05004], lr: 0.040514, loss: 4.6500
2022-10-12 01:02:39 - train: epoch 0057, iter [00500, 05004], lr: 0.040483, loss: 4.0525
2022-10-12 01:03:29 - train: epoch 0057, iter [00600, 05004], lr: 0.040452, loss: 4.8745
2022-10-12 01:04:18 - train: epoch 0057, iter [00700, 05004], lr: 0.040421, loss: 4.1882
2022-10-12 01:05:09 - train: epoch 0057, iter [00800, 05004], lr: 0.040390, loss: 3.9779
2022-10-12 01:05:58 - train: epoch 0057, iter [00900, 05004], lr: 0.040360, loss: 4.2255
2022-10-12 01:06:47 - train: epoch 0057, iter [01000, 05004], lr: 0.040329, loss: 3.4491
2022-10-12 01:07:38 - train: epoch 0057, iter [01100, 05004], lr: 0.040298, loss: 4.2300
2022-10-12 01:08:28 - train: epoch 0057, iter [01200, 05004], lr: 0.040267, loss: 3.6885
2022-10-12 01:09:17 - train: epoch 0057, iter [01300, 05004], lr: 0.040236, loss: 4.3202
2022-10-12 01:10:07 - train: epoch 0057, iter [01400, 05004], lr: 0.040206, loss: 4.0979
2022-10-12 01:10:56 - train: epoch 0057, iter [01500, 05004], lr: 0.040175, loss: 5.0928
2022-10-12 01:11:48 - train: epoch 0057, iter [01600, 05004], lr: 0.040144, loss: 4.6076
2022-10-12 01:12:36 - train: epoch 0057, iter [01700, 05004], lr: 0.040113, loss: 4.4186
2022-10-12 01:13:26 - train: epoch 0057, iter [01800, 05004], lr: 0.040083, loss: 3.9146
2022-10-12 01:14:15 - train: epoch 0057, iter [01900, 05004], lr: 0.040052, loss: 3.6159
2022-10-12 01:15:05 - train: epoch 0057, iter [02000, 05004], lr: 0.040021, loss: 4.7348
2022-10-12 01:15:55 - train: epoch 0057, iter [02100, 05004], lr: 0.039990, loss: 3.7266
2022-10-12 01:16:44 - train: epoch 0057, iter [02200, 05004], lr: 0.039959, loss: 4.3233
2022-10-12 01:17:36 - train: epoch 0057, iter [02300, 05004], lr: 0.039929, loss: 3.9785
2022-10-12 01:18:27 - train: epoch 0057, iter [02400, 05004], lr: 0.039898, loss: 4.0428
2022-10-12 01:19:18 - train: epoch 0057, iter [02500, 05004], lr: 0.039867, loss: 4.4520
2022-10-12 01:20:07 - train: epoch 0057, iter [02600, 05004], lr: 0.039837, loss: 3.7931
2022-10-12 01:20:59 - train: epoch 0057, iter [02700, 05004], lr: 0.039806, loss: 3.6534
2022-10-12 01:21:47 - train: epoch 0057, iter [02800, 05004], lr: 0.039775, loss: 4.4218
2022-10-12 01:22:37 - train: epoch 0057, iter [02900, 05004], lr: 0.039744, loss: 4.2346
2022-10-12 01:23:26 - train: epoch 0057, iter [03000, 05004], lr: 0.039714, loss: 4.8261
2022-10-12 01:24:17 - train: epoch 0057, iter [03100, 05004], lr: 0.039683, loss: 4.7182
2022-10-12 01:25:07 - train: epoch 0057, iter [03200, 05004], lr: 0.039652, loss: 3.7235
2022-10-12 01:25:55 - train: epoch 0057, iter [03300, 05004], lr: 0.039622, loss: 3.5207
2022-10-12 01:26:45 - train: epoch 0057, iter [03400, 05004], lr: 0.039591, loss: 3.7805
2022-10-12 01:27:37 - train: epoch 0057, iter [03500, 05004], lr: 0.039560, loss: 4.3796
2022-10-12 01:28:28 - train: epoch 0057, iter [03600, 05004], lr: 0.039529, loss: 3.8200
2022-10-12 01:29:18 - train: epoch 0057, iter [03700, 05004], lr: 0.039499, loss: 4.3944
2022-10-12 01:30:07 - train: epoch 0057, iter [03800, 05004], lr: 0.039468, loss: 3.8869
2022-10-12 01:30:57 - train: epoch 0057, iter [03900, 05004], lr: 0.039437, loss: 4.8536
2022-10-12 01:31:49 - train: epoch 0057, iter [04000, 05004], lr: 0.039407, loss: 4.0444
2022-10-12 01:32:37 - train: epoch 0057, iter [04100, 05004], lr: 0.039376, loss: 4.1224
2022-10-12 01:33:26 - train: epoch 0057, iter [04200, 05004], lr: 0.039345, loss: 4.6402
2022-10-12 01:34:16 - train: epoch 0057, iter [04300, 05004], lr: 0.039315, loss: 4.0231
2022-10-12 01:35:07 - train: epoch 0057, iter [04400, 05004], lr: 0.039284, loss: 3.3997
2022-10-12 01:35:56 - train: epoch 0057, iter [04500, 05004], lr: 0.039253, loss: 4.0391
2022-10-12 01:36:47 - train: epoch 0057, iter [04600, 05004], lr: 0.039223, loss: 3.6271
2022-10-12 01:37:36 - train: epoch 0057, iter [04700, 05004], lr: 0.039192, loss: 4.0557
2022-10-12 01:38:27 - train: epoch 0057, iter [04800, 05004], lr: 0.039161, loss: 3.6563
2022-10-12 01:39:16 - train: epoch 0057, iter [04900, 05004], lr: 0.039131, loss: 4.6839
2022-10-12 01:40:04 - train: epoch 0057, iter [05000, 05004], lr: 0.039100, loss: 4.0429
2022-10-12 01:40:07 - train: epoch 057, train_loss: 4.1612
2022-10-12 01:41:50 - eval: epoch: 057, acc1: 64.056%, acc5: 86.700%, test_loss: 1.8034, per_image_load_time: 3.421ms, per_image_inference_time: 0.514ms
2022-10-12 01:41:51 - until epoch: 057, best_acc1: 64.056%
2022-10-12 01:41:51 - epoch 058 lr: 0.039099
2022-10-12 01:42:44 - train: epoch 0058, iter [00100, 05004], lr: 0.039068, loss: 4.1014
2022-10-12 01:43:36 - train: epoch 0058, iter [00200, 05004], lr: 0.039038, loss: 3.9272
2022-10-12 01:44:26 - train: epoch 0058, iter [00300, 05004], lr: 0.039007, loss: 4.0705
2022-10-12 01:45:15 - train: epoch 0058, iter [00400, 05004], lr: 0.038976, loss: 3.6182
2022-10-12 01:46:05 - train: epoch 0058, iter [00500, 05004], lr: 0.038946, loss: 4.2193
2022-10-12 01:46:56 - train: epoch 0058, iter [00600, 05004], lr: 0.038915, loss: 4.6404
2022-10-12 01:47:47 - train: epoch 0058, iter [00700, 05004], lr: 0.038885, loss: 3.7539
2022-10-12 01:48:36 - train: epoch 0058, iter [00800, 05004], lr: 0.038854, loss: 3.5154
2022-10-12 01:49:27 - train: epoch 0058, iter [00900, 05004], lr: 0.038823, loss: 3.8132
2022-10-12 01:50:17 - train: epoch 0058, iter [01000, 05004], lr: 0.038793, loss: 4.6307
2022-10-12 01:51:08 - train: epoch 0058, iter [01100, 05004], lr: 0.038762, loss: 4.1029
2022-10-12 01:51:57 - train: epoch 0058, iter [01200, 05004], lr: 0.038732, loss: 3.1919
2022-10-12 01:52:46 - train: epoch 0058, iter [01300, 05004], lr: 0.038701, loss: 3.4677
2022-10-12 01:53:36 - train: epoch 0058, iter [01400, 05004], lr: 0.038671, loss: 4.8838
2022-10-12 01:54:25 - train: epoch 0058, iter [01500, 05004], lr: 0.038640, loss: 4.2382
2022-10-12 01:55:15 - train: epoch 0058, iter [01600, 05004], lr: 0.038609, loss: 4.4307
2022-10-12 01:56:05 - train: epoch 0058, iter [01700, 05004], lr: 0.038579, loss: 4.6294
2022-10-12 01:56:55 - train: epoch 0058, iter [01800, 05004], lr: 0.038548, loss: 3.9790
2022-10-12 01:57:45 - train: epoch 0058, iter [01900, 05004], lr: 0.038518, loss: 4.8501
2022-10-12 01:58:37 - train: epoch 0058, iter [02000, 05004], lr: 0.038487, loss: 4.0651
2022-10-12 01:59:27 - train: epoch 0058, iter [02100, 05004], lr: 0.038457, loss: 4.8449
2022-10-12 02:00:17 - train: epoch 0058, iter [02200, 05004], lr: 0.038426, loss: 4.0810
2022-10-12 02:01:07 - train: epoch 0058, iter [02300, 05004], lr: 0.038396, loss: 3.6131
2022-10-12 02:01:57 - train: epoch 0058, iter [02400, 05004], lr: 0.038365, loss: 3.5024
2022-10-12 02:02:46 - train: epoch 0058, iter [02500, 05004], lr: 0.038335, loss: 4.0644
2022-10-12 02:03:36 - train: epoch 0058, iter [02600, 05004], lr: 0.038304, loss: 4.1887
2022-10-12 02:04:25 - train: epoch 0058, iter [02700, 05004], lr: 0.038273, loss: 4.4067
2022-10-12 02:05:16 - train: epoch 0058, iter [02800, 05004], lr: 0.038243, loss: 3.8313
2022-10-12 02:06:06 - train: epoch 0058, iter [02900, 05004], lr: 0.038212, loss: 4.1857
2022-10-12 02:06:56 - train: epoch 0058, iter [03000, 05004], lr: 0.038182, loss: 4.4150
2022-10-12 02:07:46 - train: epoch 0058, iter [03100, 05004], lr: 0.038151, loss: 4.1996
2022-10-12 02:08:37 - train: epoch 0058, iter [03200, 05004], lr: 0.038121, loss: 4.2104
2022-10-12 02:09:28 - train: epoch 0058, iter [03300, 05004], lr: 0.038090, loss: 3.3311
2022-10-12 02:10:17 - train: epoch 0058, iter [03400, 05004], lr: 0.038060, loss: 3.5773
2022-10-12 02:11:06 - train: epoch 0058, iter [03500, 05004], lr: 0.038030, loss: 4.4539
2022-10-12 02:11:56 - train: epoch 0058, iter [03600, 05004], lr: 0.037999, loss: 4.3756
2022-10-12 02:12:46 - train: epoch 0058, iter [03700, 05004], lr: 0.037969, loss: 3.8147
2022-10-12 02:13:35 - train: epoch 0058, iter [03800, 05004], lr: 0.037938, loss: 4.1710
2022-10-12 02:14:25 - train: epoch 0058, iter [03900, 05004], lr: 0.037908, loss: 4.3261
2022-10-12 02:15:16 - train: epoch 0058, iter [04000, 05004], lr: 0.037877, loss: 4.7974
2022-10-12 02:16:06 - train: epoch 0058, iter [04100, 05004], lr: 0.037847, loss: 4.1302
2022-10-12 02:16:57 - train: epoch 0058, iter [04200, 05004], lr: 0.037816, loss: 4.6780
2022-10-12 02:17:47 - train: epoch 0058, iter [04300, 05004], lr: 0.037786, loss: 4.6723
2022-10-12 02:18:36 - train: epoch 0058, iter [04400, 05004], lr: 0.037755, loss: 4.1426
2022-10-12 02:19:26 - train: epoch 0058, iter [04500, 05004], lr: 0.037725, loss: 4.1433
2022-10-12 02:20:16 - train: epoch 0058, iter [04600, 05004], lr: 0.037695, loss: 3.8748
2022-10-12 02:21:05 - train: epoch 0058, iter [04700, 05004], lr: 0.037664, loss: 4.4251
2022-10-12 02:21:55 - train: epoch 0058, iter [04800, 05004], lr: 0.037634, loss: 3.3269
2022-10-12 02:22:43 - train: epoch 0058, iter [04900, 05004], lr: 0.037603, loss: 3.7389
2022-10-12 02:23:32 - train: epoch 0058, iter [05000, 05004], lr: 0.037573, loss: 4.0643
2022-10-12 02:23:34 - train: epoch 058, train_loss: 4.1458
2022-10-12 02:25:20 - eval: epoch: 058, acc1: 63.596%, acc5: 85.724%, test_loss: 1.7966, per_image_load_time: 3.188ms, per_image_inference_time: 0.523ms
2022-10-12 02:25:20 - until epoch: 058, best_acc1: 64.056%
2022-10-12 02:25:20 - epoch 059 lr: 0.037572
2022-10-12 02:26:16 - train: epoch 0059, iter [00100, 05004], lr: 0.037541, loss: 3.8691
2022-10-12 02:27:06 - train: epoch 0059, iter [00200, 05004], lr: 0.037511, loss: 3.7631
2022-10-12 02:27:57 - train: epoch 0059, iter [00300, 05004], lr: 0.037481, loss: 3.8762
2022-10-12 02:28:48 - train: epoch 0059, iter [00400, 05004], lr: 0.037450, loss: 4.3360
2022-10-12 02:29:38 - train: epoch 0059, iter [00500, 05004], lr: 0.037420, loss: 4.6857
2022-10-12 02:30:29 - train: epoch 0059, iter [00600, 05004], lr: 0.037389, loss: 4.3991
2022-10-12 02:31:18 - train: epoch 0059, iter [00700, 05004], lr: 0.037359, loss: 5.1583
2022-10-12 02:32:06 - train: epoch 0059, iter [00800, 05004], lr: 0.037329, loss: 3.2681
2022-10-12 02:32:56 - train: epoch 0059, iter [00900, 05004], lr: 0.037298, loss: 4.2061
2022-10-12 02:33:47 - train: epoch 0059, iter [01000, 05004], lr: 0.037268, loss: 3.6173
2022-10-12 02:34:36 - train: epoch 0059, iter [01100, 05004], lr: 0.037238, loss: 4.8817
2022-10-12 02:35:27 - train: epoch 0059, iter [01200, 05004], lr: 0.037207, loss: 3.8261
2022-10-12 02:36:17 - train: epoch 0059, iter [01300, 05004], lr: 0.037177, loss: 4.0735
2022-10-12 02:37:07 - train: epoch 0059, iter [01400, 05004], lr: 0.037147, loss: 3.7731
2022-10-12 02:37:57 - train: epoch 0059, iter [01500, 05004], lr: 0.037116, loss: 4.0112
2022-10-12 02:38:46 - train: epoch 0059, iter [01600, 05004], lr: 0.037086, loss: 4.3596
2022-10-12 02:39:36 - train: epoch 0059, iter [01700, 05004], lr: 0.037056, loss: 4.0514
2022-10-12 02:40:25 - train: epoch 0059, iter [01800, 05004], lr: 0.037025, loss: 4.3898
2022-10-12 02:41:14 - train: epoch 0059, iter [01900, 05004], lr: 0.036995, loss: 4.3688
2022-10-12 02:42:03 - train: epoch 0059, iter [02000, 05004], lr: 0.036965, loss: 4.2074
2022-10-12 02:42:53 - train: epoch 0059, iter [02100, 05004], lr: 0.036934, loss: 3.9777
2022-10-12 02:43:44 - train: epoch 0059, iter [02200, 05004], lr: 0.036904, loss: 3.5882
2022-10-12 02:44:34 - train: epoch 0059, iter [02300, 05004], lr: 0.036874, loss: 4.0429
2022-10-12 02:45:24 - train: epoch 0059, iter [02400, 05004], lr: 0.036844, loss: 4.4165
2022-10-12 02:46:14 - train: epoch 0059, iter [02500, 05004], lr: 0.036813, loss: 4.3596
2022-10-12 02:47:05 - train: epoch 0059, iter [02600, 05004], lr: 0.036783, loss: 3.9070
2022-10-12 02:47:55 - train: epoch 0059, iter [02700, 05004], lr: 0.036753, loss: 4.4461
2022-10-12 02:48:45 - train: epoch 0059, iter [02800, 05004], lr: 0.036722, loss: 4.8486
2022-10-12 02:49:35 - train: epoch 0059, iter [02900, 05004], lr: 0.036692, loss: 4.0968
2022-10-12 02:50:24 - train: epoch 0059, iter [03000, 05004], lr: 0.036662, loss: 3.0604
2022-10-12 02:51:13 - train: epoch 0059, iter [03100, 05004], lr: 0.036632, loss: 3.5400
2022-10-12 02:52:03 - train: epoch 0059, iter [03200, 05004], lr: 0.036601, loss: 4.4310
2022-10-12 02:52:53 - train: epoch 0059, iter [03300, 05004], lr: 0.036571, loss: 4.4240
2022-10-12 02:53:44 - train: epoch 0059, iter [03400, 05004], lr: 0.036541, loss: 3.7974
2022-10-12 02:54:33 - train: epoch 0059, iter [03500, 05004], lr: 0.036511, loss: 3.7283
2022-10-12 02:55:22 - train: epoch 0059, iter [03600, 05004], lr: 0.036481, loss: 4.6827
2022-10-12 02:56:11 - train: epoch 0059, iter [03700, 05004], lr: 0.036450, loss: 3.5073
2022-10-12 02:57:01 - train: epoch 0059, iter [03800, 05004], lr: 0.036420, loss: 3.7279
2022-10-12 02:57:49 - train: epoch 0059, iter [03900, 05004], lr: 0.036390, loss: 4.3884
2022-10-12 02:58:39 - train: epoch 0059, iter [04000, 05004], lr: 0.036360, loss: 4.9085
2022-10-12 02:59:27 - train: epoch 0059, iter [04100, 05004], lr: 0.036330, loss: 3.5187
2022-10-12 03:00:16 - train: epoch 0059, iter [04200, 05004], lr: 0.036299, loss: 4.5939
2022-10-12 03:01:05 - train: epoch 0059, iter [04300, 05004], lr: 0.036269, loss: 4.1585
2022-10-12 03:01:53 - train: epoch 0059, iter [04400, 05004], lr: 0.036239, loss: 4.1378
2022-10-12 03:02:41 - train: epoch 0059, iter [04500, 05004], lr: 0.036209, loss: 4.4335
2022-10-12 03:03:31 - train: epoch 0059, iter [04600, 05004], lr: 0.036179, loss: 4.5940
2022-10-12 03:04:20 - train: epoch 0059, iter [04700, 05004], lr: 0.036148, loss: 3.7072
2022-10-12 03:05:09 - train: epoch 0059, iter [04800, 05004], lr: 0.036118, loss: 3.6427
2022-10-12 03:05:58 - train: epoch 0059, iter [04900, 05004], lr: 0.036088, loss: 4.0383
2022-10-12 03:06:44 - train: epoch 0059, iter [05000, 05004], lr: 0.036058, loss: 4.1986
2022-10-12 03:06:46 - train: epoch 059, train_loss: 4.1396
2022-10-12 03:08:29 - eval: epoch: 059, acc1: 64.184%, acc5: 86.190%, test_loss: 1.8068, per_image_load_time: 2.977ms, per_image_inference_time: 0.494ms
2022-10-12 03:08:30 - until epoch: 059, best_acc1: 64.184%
2022-10-12 03:08:30 - epoch 060 lr: 0.036057
2022-10-12 03:09:24 - train: epoch 0060, iter [00100, 05004], lr: 0.036027, loss: 3.8888
2022-10-12 03:10:13 - train: epoch 0060, iter [00200, 05004], lr: 0.035997, loss: 4.1936
2022-10-12 03:11:02 - train: epoch 0060, iter [00300, 05004], lr: 0.035966, loss: 4.1543
2022-10-12 03:11:51 - train: epoch 0060, iter [00400, 05004], lr: 0.035936, loss: 3.9340
2022-10-12 03:12:39 - train: epoch 0060, iter [00500, 05004], lr: 0.035906, loss: 3.8239
2022-10-12 03:13:29 - train: epoch 0060, iter [00600, 05004], lr: 0.035876, loss: 4.6211
2022-10-12 03:14:17 - train: epoch 0060, iter [00700, 05004], lr: 0.035846, loss: 3.3605
2022-10-12 03:15:05 - train: epoch 0060, iter [00800, 05004], lr: 0.035816, loss: 4.0549
2022-10-12 03:15:54 - train: epoch 0060, iter [00900, 05004], lr: 0.035786, loss: 4.0885
2022-10-12 03:16:43 - train: epoch 0060, iter [01000, 05004], lr: 0.035756, loss: 4.7564
2022-10-12 03:17:32 - train: epoch 0060, iter [01100, 05004], lr: 0.035726, loss: 3.5723
2022-10-12 03:18:22 - train: epoch 0060, iter [01200, 05004], lr: 0.035696, loss: 3.4450
2022-10-12 03:19:09 - train: epoch 0060, iter [01300, 05004], lr: 0.035665, loss: 4.0341
2022-10-12 03:19:58 - train: epoch 0060, iter [01400, 05004], lr: 0.035635, loss: 4.6658
2022-10-12 03:20:46 - train: epoch 0060, iter [01500, 05004], lr: 0.035605, loss: 3.9526
2022-10-12 03:21:36 - train: epoch 0060, iter [01600, 05004], lr: 0.035575, loss: 4.0920
2022-10-12 03:22:26 - train: epoch 0060, iter [01700, 05004], lr: 0.035545, loss: 3.7057
2022-10-12 03:23:15 - train: epoch 0060, iter [01800, 05004], lr: 0.035515, loss: 3.8677
2022-10-12 03:24:04 - train: epoch 0060, iter [01900, 05004], lr: 0.035485, loss: 4.1260
2022-10-12 03:24:53 - train: epoch 0060, iter [02000, 05004], lr: 0.035455, loss: 4.2903
2022-10-12 03:25:41 - train: epoch 0060, iter [02100, 05004], lr: 0.035425, loss: 4.7644
2022-10-12 03:26:30 - train: epoch 0060, iter [02200, 05004], lr: 0.035395, loss: 3.6689
2022-10-12 03:27:19 - train: epoch 0060, iter [02300, 05004], lr: 0.035365, loss: 4.0093
2022-10-12 03:28:08 - train: epoch 0060, iter [02400, 05004], lr: 0.035335, loss: 3.8744
2022-10-12 03:28:56 - train: epoch 0060, iter [02500, 05004], lr: 0.035305, loss: 4.1552
2022-10-12 03:29:45 - train: epoch 0060, iter [02600, 05004], lr: 0.035275, loss: 4.4433
2022-10-12 03:30:34 - train: epoch 0060, iter [02700, 05004], lr: 0.035245, loss: 3.1295
2022-10-12 03:31:24 - train: epoch 0060, iter [02800, 05004], lr: 0.035215, loss: 5.1188
2022-10-12 03:32:12 - train: epoch 0060, iter [02900, 05004], lr: 0.035185, loss: 3.8152
2022-10-12 03:33:01 - train: epoch 0060, iter [03000, 05004], lr: 0.035155, loss: 4.2366
2022-10-12 03:33:52 - train: epoch 0060, iter [03100, 05004], lr: 0.035125, loss: 4.6667
2022-10-12 03:34:42 - train: epoch 0060, iter [03200, 05004], lr: 0.035095, loss: 3.5402
2022-10-12 03:35:31 - train: epoch 0060, iter [03300, 05004], lr: 0.035065, loss: 4.7169
2022-10-12 03:36:22 - train: epoch 0060, iter [03400, 05004], lr: 0.035035, loss: 3.8149
2022-10-12 03:37:11 - train: epoch 0060, iter [03500, 05004], lr: 0.035005, loss: 3.6540
2022-10-12 03:38:02 - train: epoch 0060, iter [03600, 05004], lr: 0.034975, loss: 4.5322
2022-10-12 03:38:50 - train: epoch 0060, iter [03700, 05004], lr: 0.034945, loss: 4.6866
2022-10-12 03:39:39 - train: epoch 0060, iter [03800, 05004], lr: 0.034916, loss: 4.3811
2022-10-12 03:40:27 - train: epoch 0060, iter [03900, 05004], lr: 0.034886, loss: 3.8538
2022-10-12 03:41:17 - train: epoch 0060, iter [04000, 05004], lr: 0.034856, loss: 4.5723
2022-10-12 03:42:07 - train: epoch 0060, iter [04100, 05004], lr: 0.034826, loss: 4.5136
2022-10-12 03:42:58 - train: epoch 0060, iter [04200, 05004], lr: 0.034796, loss: 4.0124
2022-10-12 03:43:47 - train: epoch 0060, iter [04300, 05004], lr: 0.034766, loss: 3.6505
2022-10-12 03:44:38 - train: epoch 0060, iter [04400, 05004], lr: 0.034736, loss: 4.4786
2022-10-12 03:45:28 - train: epoch 0060, iter [04500, 05004], lr: 0.034706, loss: 4.4676
2022-10-12 03:46:19 - train: epoch 0060, iter [04600, 05004], lr: 0.034676, loss: 3.6587
2022-10-12 03:47:09 - train: epoch 0060, iter [04700, 05004], lr: 0.034646, loss: 4.6708
2022-10-12 03:47:59 - train: epoch 0060, iter [04800, 05004], lr: 0.034617, loss: 3.5463
2022-10-12 03:48:48 - train: epoch 0060, iter [04900, 05004], lr: 0.034587, loss: 4.3476
2022-10-12 03:49:34 - train: epoch 0060, iter [05000, 05004], lr: 0.034557, loss: 4.2012
2022-10-12 03:49:36 - train: epoch 060, train_loss: 4.1211
2022-10-12 03:51:22 - eval: epoch: 060, acc1: 64.198%, acc5: 86.470%, test_loss: 1.7302, per_image_load_time: 3.471ms, per_image_inference_time: 0.521ms
2022-10-12 03:51:23 - until epoch: 060, best_acc1: 64.198%
2022-10-12 03:51:23 - epoch 061 lr: 0.034556
2022-10-12 03:52:20 - train: epoch 0061, iter [00100, 05004], lr: 0.034526, loss: 4.4406
2022-10-12 03:53:11 - train: epoch 0061, iter [00200, 05004], lr: 0.034496, loss: 3.9486
2022-10-12 03:54:00 - train: epoch 0061, iter [00300, 05004], lr: 0.034466, loss: 3.8889
2022-10-12 03:54:49 - train: epoch 0061, iter [00400, 05004], lr: 0.034436, loss: 4.2075
2022-10-12 03:55:40 - train: epoch 0061, iter [00500, 05004], lr: 0.034407, loss: 4.2812
2022-10-12 03:56:31 - train: epoch 0061, iter [00600, 05004], lr: 0.034377, loss: 3.8057
2022-10-12 03:57:23 - train: epoch 0061, iter [00700, 05004], lr: 0.034347, loss: 3.9942
2022-10-12 03:58:11 - train: epoch 0061, iter [00800, 05004], lr: 0.034317, loss: 3.9574
2022-10-12 03:59:01 - train: epoch 0061, iter [00900, 05004], lr: 0.034287, loss: 3.5626
2022-10-12 03:59:50 - train: epoch 0061, iter [01000, 05004], lr: 0.034257, loss: 3.2047
2022-10-12 04:00:39 - train: epoch 0061, iter [01100, 05004], lr: 0.034228, loss: 3.4491
2022-10-12 04:01:31 - train: epoch 0061, iter [01200, 05004], lr: 0.034198, loss: 3.9545
2022-10-12 04:02:21 - train: epoch 0061, iter [01300, 05004], lr: 0.034168, loss: 3.8981
2022-10-12 04:03:11 - train: epoch 0061, iter [01400, 05004], lr: 0.034138, loss: 4.4289
2022-10-12 04:04:01 - train: epoch 0061, iter [01500, 05004], lr: 0.034109, loss: 4.0753
2022-10-12 04:04:51 - train: epoch 0061, iter [01600, 05004], lr: 0.034079, loss: 4.5809
2022-10-12 04:05:41 - train: epoch 0061, iter [01700, 05004], lr: 0.034049, loss: 4.6910
2022-10-12 04:06:32 - train: epoch 0061, iter [01800, 05004], lr: 0.034019, loss: 4.7787
2022-10-12 04:07:22 - train: epoch 0061, iter [01900, 05004], lr: 0.033990, loss: 3.6005
2022-10-12 04:08:10 - train: epoch 0061, iter [02000, 05004], lr: 0.033960, loss: 4.4914
2022-10-12 04:09:01 - train: epoch 0061, iter [02100, 05004], lr: 0.033930, loss: 4.5076
2022-10-12 04:09:50 - train: epoch 0061, iter [02200, 05004], lr: 0.033900, loss: 4.1568
2022-10-12 04:10:40 - train: epoch 0061, iter [02300, 05004], lr: 0.033871, loss: 4.2694
2022-10-12 04:11:31 - train: epoch 0061, iter [02400, 05004], lr: 0.033841, loss: 3.6438
2022-10-12 04:12:21 - train: epoch 0061, iter [02500, 05004], lr: 0.033811, loss: 3.4015
2022-10-12 04:13:11 - train: epoch 0061, iter [02600, 05004], lr: 0.033782, loss: 4.1402
2022-10-12 04:14:02 - train: epoch 0061, iter [02700, 05004], lr: 0.033752, loss: 3.4872
2022-10-12 04:14:52 - train: epoch 0061, iter [02800, 05004], lr: 0.033722, loss: 3.6290
2022-10-12 04:15:42 - train: epoch 0061, iter [02900, 05004], lr: 0.033693, loss: 4.1013
2022-10-12 04:16:32 - train: epoch 0061, iter [03000, 05004], lr: 0.033663, loss: 4.5981
2022-10-12 04:17:22 - train: epoch 0061, iter [03100, 05004], lr: 0.033633, loss: 4.2033
2022-10-12 04:18:11 - train: epoch 0061, iter [03200, 05004], lr: 0.033604, loss: 4.3879
2022-10-12 04:18:58 - train: epoch 0061, iter [03300, 05004], lr: 0.033574, loss: 4.6784
2022-10-12 04:19:48 - train: epoch 0061, iter [03400, 05004], lr: 0.033544, loss: 3.6969
2022-10-12 04:20:38 - train: epoch 0061, iter [03500, 05004], lr: 0.033515, loss: 4.0572
2022-10-12 04:21:27 - train: epoch 0061, iter [03600, 05004], lr: 0.033485, loss: 3.7492
2022-10-12 04:22:18 - train: epoch 0061, iter [03700, 05004], lr: 0.033455, loss: 4.3188
2022-10-12 04:23:09 - train: epoch 0061, iter [03800, 05004], lr: 0.033426, loss: 3.4869
2022-10-12 04:23:58 - train: epoch 0061, iter [03900, 05004], lr: 0.033396, loss: 4.4114
2022-10-12 04:24:48 - train: epoch 0061, iter [04000, 05004], lr: 0.033367, loss: 3.9537
2022-10-12 04:25:40 - train: epoch 0061, iter [04100, 05004], lr: 0.033337, loss: 4.6036
2022-10-12 04:26:29 - train: epoch 0061, iter [04200, 05004], lr: 0.033307, loss: 3.5652
2022-10-12 04:27:18 - train: epoch 0061, iter [04300, 05004], lr: 0.033278, loss: 3.3043
2022-10-12 04:28:08 - train: epoch 0061, iter [04400, 05004], lr: 0.033248, loss: 3.1729
2022-10-12 04:28:56 - train: epoch 0061, iter [04500, 05004], lr: 0.033219, loss: 4.1405
2022-10-12 04:29:46 - train: epoch 0061, iter [04600, 05004], lr: 0.033189, loss: 3.6256
2022-10-12 04:30:37 - train: epoch 0061, iter [04700, 05004], lr: 0.033160, loss: 3.7910
2022-10-12 04:31:27 - train: epoch 0061, iter [04800, 05004], lr: 0.033130, loss: 3.9950
2022-10-12 04:32:18 - train: epoch 0061, iter [04900, 05004], lr: 0.033101, loss: 4.4278
2022-10-12 04:33:06 - train: epoch 0061, iter [05000, 05004], lr: 0.033071, loss: 3.7904
2022-10-12 04:33:08 - train: epoch 061, train_loss: 4.1030
2022-10-12 04:34:51 - eval: epoch: 061, acc1: 64.922%, acc5: 87.036%, test_loss: 1.8011, per_image_load_time: 2.386ms, per_image_inference_time: 0.502ms
2022-10-12 04:34:52 - until epoch: 061, best_acc1: 64.922%
2022-10-12 04:34:52 - epoch 062 lr: 0.033070
2022-10-12 04:35:47 - train: epoch 0062, iter [00100, 05004], lr: 0.033040, loss: 4.0479
2022-10-12 04:36:38 - train: epoch 0062, iter [00200, 05004], lr: 0.033011, loss: 3.2801
2022-10-12 04:37:27 - train: epoch 0062, iter [00300, 05004], lr: 0.032981, loss: 4.5263
2022-10-12 04:38:16 - train: epoch 0062, iter [00400, 05004], lr: 0.032952, loss: 2.8234
2022-10-12 04:39:06 - train: epoch 0062, iter [00500, 05004], lr: 0.032922, loss: 3.9277
2022-10-12 04:39:56 - train: epoch 0062, iter [00600, 05004], lr: 0.032893, loss: 3.4523
2022-10-12 04:40:46 - train: epoch 0062, iter [00700, 05004], lr: 0.032863, loss: 4.3946
2022-10-12 04:41:38 - train: epoch 0062, iter [00800, 05004], lr: 0.032834, loss: 4.4774
2022-10-12 04:42:27 - train: epoch 0062, iter [00900, 05004], lr: 0.032804, loss: 4.9597
2022-10-12 04:43:17 - train: epoch 0062, iter [01000, 05004], lr: 0.032775, loss: 4.1443
2022-10-12 04:44:07 - train: epoch 0062, iter [01100, 05004], lr: 0.032745, loss: 4.0718
2022-10-12 04:44:58 - train: epoch 0062, iter [01200, 05004], lr: 0.032716, loss: 4.3660
2022-10-12 04:45:49 - train: epoch 0062, iter [01300, 05004], lr: 0.032686, loss: 4.0597
2022-10-12 04:46:39 - train: epoch 0062, iter [01400, 05004], lr: 0.032657, loss: 4.3451
2022-10-12 04:47:27 - train: epoch 0062, iter [01500, 05004], lr: 0.032628, loss: 3.9883
2022-10-12 04:48:17 - train: epoch 0062, iter [01600, 05004], lr: 0.032598, loss: 4.1393
2022-10-12 04:49:07 - train: epoch 0062, iter [01700, 05004], lr: 0.032569, loss: 3.6769
2022-10-12 04:49:58 - train: epoch 0062, iter [01800, 05004], lr: 0.032539, loss: 3.4865
2022-10-12 04:50:49 - train: epoch 0062, iter [01900, 05004], lr: 0.032510, loss: 3.2826
2022-10-12 04:51:39 - train: epoch 0062, iter [02000, 05004], lr: 0.032481, loss: 3.8056
2022-10-12 04:52:29 - train: epoch 0062, iter [02100, 05004], lr: 0.032451, loss: 3.7530
2022-10-12 04:53:18 - train: epoch 0062, iter [02200, 05004], lr: 0.032422, loss: 3.1938
2022-10-12 04:54:10 - train: epoch 0062, iter [02300, 05004], lr: 0.032392, loss: 4.3913
2022-10-12 04:55:00 - train: epoch 0062, iter [02400, 05004], lr: 0.032363, loss: 4.0849
2022-10-12 04:55:50 - train: epoch 0062, iter [02500, 05004], lr: 0.032334, loss: 4.1833
2022-10-12 04:56:39 - train: epoch 0062, iter [02600, 05004], lr: 0.032304, loss: 4.7720
2022-10-12 04:57:27 - train: epoch 0062, iter [02700, 05004], lr: 0.032275, loss: 4.3868
2022-10-12 04:58:15 - train: epoch 0062, iter [02800, 05004], lr: 0.032246, loss: 3.6468
2022-10-12 04:59:06 - train: epoch 0062, iter [02900, 05004], lr: 0.032216, loss: 3.9807
2022-10-12 04:59:56 - train: epoch 0062, iter [03000, 05004], lr: 0.032187, loss: 4.4604
2022-10-12 05:00:45 - train: epoch 0062, iter [03100, 05004], lr: 0.032158, loss: 3.9962
2022-10-12 05:01:38 - train: epoch 0062, iter [03200, 05004], lr: 0.032128, loss: 4.3234
2022-10-12 05:02:27 - train: epoch 0062, iter [03300, 05004], lr: 0.032099, loss: 4.2515
2022-10-12 05:03:18 - train: epoch 0062, iter [03400, 05004], lr: 0.032070, loss: 4.2205
2022-10-12 05:04:07 - train: epoch 0062, iter [03500, 05004], lr: 0.032040, loss: 3.9113
2022-10-12 05:04:58 - train: epoch 0062, iter [03600, 05004], lr: 0.032011, loss: 3.8847
2022-10-12 05:05:48 - train: epoch 0062, iter [03700, 05004], lr: 0.031982, loss: 3.7464
2022-10-12 05:06:36 - train: epoch 0062, iter [03800, 05004], lr: 0.031953, loss: 4.3296
2022-10-12 05:07:25 - train: epoch 0062, iter [03900, 05004], lr: 0.031923, loss: 3.8351
2022-10-12 05:08:15 - train: epoch 0062, iter [04000, 05004], lr: 0.031894, loss: 3.7726
2022-10-12 05:09:06 - train: epoch 0062, iter [04100, 05004], lr: 0.031865, loss: 4.2000
2022-10-12 05:09:56 - train: epoch 0062, iter [04200, 05004], lr: 0.031835, loss: 4.3525
2022-10-12 05:10:47 - train: epoch 0062, iter [04300, 05004], lr: 0.031806, loss: 3.8477
2022-10-12 05:11:36 - train: epoch 0062, iter [04400, 05004], lr: 0.031777, loss: 4.1986
2022-10-12 05:12:25 - train: epoch 0062, iter [04500, 05004], lr: 0.031748, loss: 4.1695
2022-10-12 05:13:17 - train: epoch 0062, iter [04600, 05004], lr: 0.031719, loss: 3.2733
2022-10-12 05:14:08 - train: epoch 0062, iter [04700, 05004], lr: 0.031689, loss: 4.6164
2022-10-12 05:14:58 - train: epoch 0062, iter [04800, 05004], lr: 0.031660, loss: 4.1444
2022-10-12 05:15:47 - train: epoch 0062, iter [04900, 05004], lr: 0.031631, loss: 3.4719
2022-10-12 05:16:34 - train: epoch 0062, iter [05000, 05004], lr: 0.031602, loss: 3.6532
2022-10-12 05:16:36 - train: epoch 062, train_loss: 4.0939
2022-10-12 05:18:20 - eval: epoch: 062, acc1: 65.628%, acc5: 87.352%, test_loss: 1.6815, per_image_load_time: 3.397ms, per_image_inference_time: 0.525ms
2022-10-12 05:18:21 - until epoch: 062, best_acc1: 65.628%
2022-10-12 05:18:21 - epoch 063 lr: 0.031601
2022-10-12 05:19:19 - train: epoch 0063, iter [00100, 05004], lr: 0.031571, loss: 4.4184
2022-10-12 05:20:09 - train: epoch 0063, iter [00200, 05004], lr: 0.031542, loss: 4.4944
2022-10-12 05:20:59 - train: epoch 0063, iter [00300, 05004], lr: 0.031513, loss: 3.1582
2022-10-12 05:21:50 - train: epoch 0063, iter [00400, 05004], lr: 0.031484, loss: 3.8961
2022-10-12 05:22:39 - train: epoch 0063, iter [00500, 05004], lr: 0.031455, loss: 3.4398
2022-10-12 05:23:30 - train: epoch 0063, iter [00600, 05004], lr: 0.031426, loss: 4.5374
2022-10-12 05:24:20 - train: epoch 0063, iter [00700, 05004], lr: 0.031397, loss: 3.9006
2022-10-12 05:25:09 - train: epoch 0063, iter [00800, 05004], lr: 0.031367, loss: 4.8719
2022-10-12 05:25:58 - train: epoch 0063, iter [00900, 05004], lr: 0.031338, loss: 4.2438
2022-10-12 05:26:48 - train: epoch 0063, iter [01000, 05004], lr: 0.031309, loss: 3.2711
2022-10-12 05:27:36 - train: epoch 0063, iter [01100, 05004], lr: 0.031280, loss: 4.3591
2022-10-12 05:28:26 - train: epoch 0063, iter [01200, 05004], lr: 0.031251, loss: 4.4015
2022-10-12 05:29:17 - train: epoch 0063, iter [01300, 05004], lr: 0.031222, loss: 3.8013
2022-10-12 05:30:06 - train: epoch 0063, iter [01400, 05004], lr: 0.031193, loss: 4.4099
2022-10-12 05:30:56 - train: epoch 0063, iter [01500, 05004], lr: 0.031164, loss: 3.5069
2022-10-12 05:31:46 - train: epoch 0063, iter [01600, 05004], lr: 0.031135, loss: 4.4273
2022-10-12 05:32:35 - train: epoch 0063, iter [01700, 05004], lr: 0.031106, loss: 3.6125
2022-10-12 05:33:25 - train: epoch 0063, iter [01800, 05004], lr: 0.031076, loss: 4.0732
2022-10-12 05:34:13 - train: epoch 0063, iter [01900, 05004], lr: 0.031047, loss: 4.1641
2022-10-12 05:35:04 - train: epoch 0063, iter [02000, 05004], lr: 0.031018, loss: 4.1823
2022-10-12 05:35:52 - train: epoch 0063, iter [02100, 05004], lr: 0.030989, loss: 4.1150
2022-10-12 05:36:41 - train: epoch 0063, iter [02200, 05004], lr: 0.030960, loss: 5.0104
2022-10-12 05:37:32 - train: epoch 0063, iter [02300, 05004], lr: 0.030931, loss: 4.2126
2022-10-12 05:38:21 - train: epoch 0063, iter [02400, 05004], lr: 0.030902, loss: 3.8488
2022-10-12 05:39:10 - train: epoch 0063, iter [02500, 05004], lr: 0.030873, loss: 4.6611
2022-10-12 05:40:01 - train: epoch 0063, iter [02600, 05004], lr: 0.030844, loss: 3.6488
2022-10-12 05:40:51 - train: epoch 0063, iter [02700, 05004], lr: 0.030815, loss: 4.6724
2022-10-12 05:41:40 - train: epoch 0063, iter [02800, 05004], lr: 0.030786, loss: 4.3814
2022-10-12 05:42:30 - train: epoch 0063, iter [02900, 05004], lr: 0.030757, loss: 4.3085
2022-10-12 05:43:20 - train: epoch 0063, iter [03000, 05004], lr: 0.030728, loss: 4.1382
2022-10-12 05:44:11 - train: epoch 0063, iter [03100, 05004], lr: 0.030699, loss: 4.4460
2022-10-12 05:45:00 - train: epoch 0063, iter [03200, 05004], lr: 0.030671, loss: 3.6424
2022-10-12 05:45:50 - train: epoch 0063, iter [03300, 05004], lr: 0.030642, loss: 4.4908
2022-10-12 05:46:38 - train: epoch 0063, iter [03400, 05004], lr: 0.030613, loss: 4.1415
2022-10-12 05:47:28 - train: epoch 0063, iter [03500, 05004], lr: 0.030584, loss: 4.5483
2022-10-12 05:48:17 - train: epoch 0063, iter [03600, 05004], lr: 0.030555, loss: 4.4729
2022-10-12 05:49:09 - train: epoch 0063, iter [03700, 05004], lr: 0.030526, loss: 4.0776
2022-10-12 05:49:59 - train: epoch 0063, iter [03800, 05004], lr: 0.030497, loss: 4.0525
2022-10-12 05:50:47 - train: epoch 0063, iter [03900, 05004], lr: 0.030468, loss: 4.6172
2022-10-12 05:51:38 - train: epoch 0063, iter [04000, 05004], lr: 0.030439, loss: 4.4670
2022-10-12 05:52:27 - train: epoch 0063, iter [04100, 05004], lr: 0.030410, loss: 3.7431
2022-10-12 05:53:17 - train: epoch 0063, iter [04200, 05004], lr: 0.030381, loss: 4.6289
2022-10-12 05:54:06 - train: epoch 0063, iter [04300, 05004], lr: 0.030353, loss: 3.3809
2022-10-12 05:54:56 - train: epoch 0063, iter [04400, 05004], lr: 0.030324, loss: 3.4325
2022-10-12 05:55:44 - train: epoch 0063, iter [04500, 05004], lr: 0.030295, loss: 4.3930
2022-10-12 05:56:34 - train: epoch 0063, iter [04600, 05004], lr: 0.030266, loss: 3.4656
2022-10-12 05:57:25 - train: epoch 0063, iter [04700, 05004], lr: 0.030237, loss: 3.6875
2022-10-12 05:58:14 - train: epoch 0063, iter [04800, 05004], lr: 0.030208, loss: 4.0052
2022-10-12 05:59:05 - train: epoch 0063, iter [04900, 05004], lr: 0.030180, loss: 3.6801
2022-10-12 05:59:51 - train: epoch 0063, iter [05000, 05004], lr: 0.030151, loss: 4.4709
2022-10-12 05:59:54 - train: epoch 063, train_loss: 4.0699
2022-10-12 06:01:39 - eval: epoch: 063, acc1: 64.498%, acc5: 86.700%, test_loss: 1.7270, per_image_load_time: 2.651ms, per_image_inference_time: 0.537ms
2022-10-12 06:01:39 - until epoch: 063, best_acc1: 65.628%
2022-10-12 06:01:39 - epoch 064 lr: 0.030150
2022-10-12 06:02:38 - train: epoch 0064, iter [00100, 05004], lr: 0.030121, loss: 3.6833
2022-10-12 06:03:27 - train: epoch 0064, iter [00200, 05004], lr: 0.030092, loss: 4.3398
2022-10-12 06:04:16 - train: epoch 0064, iter [00300, 05004], lr: 0.030063, loss: 4.4181
2022-10-12 06:05:06 - train: epoch 0064, iter [00400, 05004], lr: 0.030034, loss: 4.0341
2022-10-12 06:05:55 - train: epoch 0064, iter [00500, 05004], lr: 0.030006, loss: 4.2720
2022-10-12 06:06:45 - train: epoch 0064, iter [00600, 05004], lr: 0.029977, loss: 4.1919
2022-10-12 06:07:35 - train: epoch 0064, iter [00700, 05004], lr: 0.029948, loss: 4.2544
2022-10-12 06:08:26 - train: epoch 0064, iter [00800, 05004], lr: 0.029919, loss: 4.1519
2022-10-12 06:09:17 - train: epoch 0064, iter [00900, 05004], lr: 0.029891, loss: 4.4554
2022-10-12 06:10:07 - train: epoch 0064, iter [01000, 05004], lr: 0.029862, loss: 4.1325
2022-10-12 06:10:58 - train: epoch 0064, iter [01100, 05004], lr: 0.029833, loss: 3.6899
2022-10-12 06:11:49 - train: epoch 0064, iter [01200, 05004], lr: 0.029804, loss: 3.3110
2022-10-12 06:12:39 - train: epoch 0064, iter [01300, 05004], lr: 0.029776, loss: 3.6122
2022-10-12 06:13:29 - train: epoch 0064, iter [01400, 05004], lr: 0.029747, loss: 3.5860
2022-10-12 06:14:19 - train: epoch 0064, iter [01500, 05004], lr: 0.029718, loss: 4.4251
2022-10-12 06:15:08 - train: epoch 0064, iter [01600, 05004], lr: 0.029690, loss: 3.7680
2022-10-12 06:15:57 - train: epoch 0064, iter [01700, 05004], lr: 0.029661, loss: 3.4049
2022-10-12 06:16:49 - train: epoch 0064, iter [01800, 05004], lr: 0.029632, loss: 4.1958
2022-10-12 06:17:38 - train: epoch 0064, iter [01900, 05004], lr: 0.029604, loss: 3.7087
2022-10-12 06:18:28 - train: epoch 0064, iter [02000, 05004], lr: 0.029575, loss: 3.2957
2022-10-12 06:19:18 - train: epoch 0064, iter [02100, 05004], lr: 0.029546, loss: 4.5781
2022-10-12 06:20:09 - train: epoch 0064, iter [02200, 05004], lr: 0.029518, loss: 4.7683
2022-10-12 06:20:59 - train: epoch 0064, iter [02300, 05004], lr: 0.029489, loss: 3.9876
2022-10-12 06:21:49 - train: epoch 0064, iter [02400, 05004], lr: 0.029461, loss: 4.2300
2022-10-12 06:22:40 - train: epoch 0064, iter [02500, 05004], lr: 0.029432, loss: 4.5658
2022-10-12 06:23:30 - train: epoch 0064, iter [02600, 05004], lr: 0.029403, loss: 3.4336
2022-10-12 06:24:19 - train: epoch 0064, iter [02700, 05004], lr: 0.029375, loss: 4.6444
2022-10-12 06:25:08 - train: epoch 0064, iter [02800, 05004], lr: 0.029346, loss: 4.2646
2022-10-12 06:25:57 - train: epoch 0064, iter [02900, 05004], lr: 0.029318, loss: 3.8262
2022-10-12 06:26:49 - train: epoch 0064, iter [03000, 05004], lr: 0.029289, loss: 4.0692
2022-10-12 06:27:37 - train: epoch 0064, iter [03100, 05004], lr: 0.029260, loss: 4.2876
2022-10-12 06:28:27 - train: epoch 0064, iter [03200, 05004], lr: 0.029232, loss: 4.1617
2022-10-12 06:29:17 - train: epoch 0064, iter [03300, 05004], lr: 0.029203, loss: 4.0790
2022-10-12 06:30:06 - train: epoch 0064, iter [03400, 05004], lr: 0.029175, loss: 3.8127
2022-10-12 06:30:57 - train: epoch 0064, iter [03500, 05004], lr: 0.029146, loss: 2.8198
2022-10-12 06:31:47 - train: epoch 0064, iter [03600, 05004], lr: 0.029118, loss: 4.1876
2022-10-12 06:32:38 - train: epoch 0064, iter [03700, 05004], lr: 0.029089, loss: 3.8430
2022-10-12 06:33:28 - train: epoch 0064, iter [03800, 05004], lr: 0.029061, loss: 4.3713
2022-10-12 06:34:16 - train: epoch 0064, iter [03900, 05004], lr: 0.029032, loss: 4.3152
2022-10-12 06:35:03 - train: epoch 0064, iter [04000, 05004], lr: 0.029004, loss: 4.3039
2022-10-12 06:35:52 - train: epoch 0064, iter [04100, 05004], lr: 0.028975, loss: 3.5749
2022-10-12 06:36:41 - train: epoch 0064, iter [04200, 05004], lr: 0.028947, loss: 3.6806
2022-10-12 06:37:30 - train: epoch 0064, iter [04300, 05004], lr: 0.028918, loss: 4.1299
2022-10-12 06:38:20 - train: epoch 0064, iter [04400, 05004], lr: 0.028890, loss: 3.8560
2022-10-12 06:39:08 - train: epoch 0064, iter [04500, 05004], lr: 0.028861, loss: 3.5147
2022-10-12 06:39:56 - train: epoch 0064, iter [04600, 05004], lr: 0.028833, loss: 4.7875
2022-10-12 06:40:44 - train: epoch 0064, iter [04700, 05004], lr: 0.028805, loss: 3.4416
2022-10-12 06:41:33 - train: epoch 0064, iter [04800, 05004], lr: 0.028776, loss: 4.4373
2022-10-12 06:42:21 - train: epoch 0064, iter [04900, 05004], lr: 0.028748, loss: 4.1195
2022-10-12 06:43:07 - train: epoch 0064, iter [05000, 05004], lr: 0.028719, loss: 4.6637
2022-10-12 06:43:09 - train: epoch 064, train_loss: 4.0673
2022-10-12 06:44:49 - eval: epoch: 064, acc1: 65.788%, acc5: 87.600%, test_loss: 1.6781, per_image_load_time: 3.339ms, per_image_inference_time: 0.510ms
2022-10-12 06:44:50 - until epoch: 064, best_acc1: 65.788%
2022-10-12 06:44:50 - epoch 065 lr: 0.028718
2022-10-12 06:45:44 - train: epoch 0065, iter [00100, 05004], lr: 0.028690, loss: 3.6767
2022-10-12 06:46:35 - train: epoch 0065, iter [00200, 05004], lr: 0.028661, loss: 3.9215
2022-10-12 06:47:23 - train: epoch 0065, iter [00300, 05004], lr: 0.028633, loss: 4.0497
2022-10-12 06:48:13 - train: epoch 0065, iter [00400, 05004], lr: 0.028605, loss: 4.4475
2022-10-12 06:49:01 - train: epoch 0065, iter [00500, 05004], lr: 0.028576, loss: 4.1513
2022-10-12 06:49:50 - train: epoch 0065, iter [00600, 05004], lr: 0.028548, loss: 3.9867
2022-10-12 06:50:38 - train: epoch 0065, iter [00700, 05004], lr: 0.028520, loss: 4.5719
2022-10-12 06:51:26 - train: epoch 0065, iter [00800, 05004], lr: 0.028491, loss: 3.7821
2022-10-12 06:52:16 - train: epoch 0065, iter [00900, 05004], lr: 0.028463, loss: 4.1091
2022-10-12 06:53:04 - train: epoch 0065, iter [01000, 05004], lr: 0.028435, loss: 4.2353
2022-10-12 06:53:53 - train: epoch 0065, iter [01100, 05004], lr: 0.028406, loss: 4.0593
2022-10-12 06:54:41 - train: epoch 0065, iter [01200, 05004], lr: 0.028378, loss: 3.7392
2022-10-12 06:55:30 - train: epoch 0065, iter [01300, 05004], lr: 0.028350, loss: 4.1432
2022-10-12 06:56:19 - train: epoch 0065, iter [01400, 05004], lr: 0.028321, loss: 2.9238
2022-10-12 06:57:07 - train: epoch 0065, iter [01500, 05004], lr: 0.028293, loss: 3.3379
2022-10-12 06:57:57 - train: epoch 0065, iter [01600, 05004], lr: 0.028265, loss: 4.1987
2022-10-12 06:58:46 - train: epoch 0065, iter [01700, 05004], lr: 0.028237, loss: 3.9695
2022-10-12 06:59:35 - train: epoch 0065, iter [01800, 05004], lr: 0.028208, loss: 3.7340
2022-10-12 07:00:25 - train: epoch 0065, iter [01900, 05004], lr: 0.028180, loss: 4.1783
2022-10-12 07:01:13 - train: epoch 0065, iter [02000, 05004], lr: 0.028152, loss: 3.7396
2022-10-12 07:02:02 - train: epoch 0065, iter [02100, 05004], lr: 0.028124, loss: 5.0650
2022-10-12 07:02:51 - train: epoch 0065, iter [02200, 05004], lr: 0.028095, loss: 4.2764
2022-10-12 07:03:39 - train: epoch 0065, iter [02300, 05004], lr: 0.028067, loss: 4.5532
2022-10-12 07:04:28 - train: epoch 0065, iter [02400, 05004], lr: 0.028039, loss: 4.2996
2022-10-12 07:05:18 - train: epoch 0065, iter [02500, 05004], lr: 0.028011, loss: 4.6685
2022-10-12 07:06:08 - train: epoch 0065, iter [02600, 05004], lr: 0.027983, loss: 3.7105
2022-10-12 07:06:56 - train: epoch 0065, iter [02700, 05004], lr: 0.027954, loss: 3.9288
2022-10-12 07:07:45 - train: epoch 0065, iter [02800, 05004], lr: 0.027926, loss: 4.6305
2022-10-12 07:08:32 - train: epoch 0065, iter [02900, 05004], lr: 0.027898, loss: 3.9607
2022-10-12 07:09:23 - train: epoch 0065, iter [03000, 05004], lr: 0.027870, loss: 3.7016
2022-10-12 07:10:11 - train: epoch 0065, iter [03100, 05004], lr: 0.027842, loss: 4.2252
2022-10-12 07:11:00 - train: epoch 0065, iter [03200, 05004], lr: 0.027814, loss: 4.0951
2022-10-12 07:11:50 - train: epoch 0065, iter [03300, 05004], lr: 0.027786, loss: 3.8598
2022-10-12 07:12:38 - train: epoch 0065, iter [03400, 05004], lr: 0.027757, loss: 3.5097
2022-10-12 07:13:28 - train: epoch 0065, iter [03500, 05004], lr: 0.027729, loss: 3.9351
2022-10-12 07:14:18 - train: epoch 0065, iter [03600, 05004], lr: 0.027701, loss: 3.7775
2022-10-12 07:15:09 - train: epoch 0065, iter [03700, 05004], lr: 0.027673, loss: 3.7970
2022-10-12 07:15:58 - train: epoch 0065, iter [03800, 05004], lr: 0.027645, loss: 4.5085
2022-10-12 07:16:48 - train: epoch 0065, iter [03900, 05004], lr: 0.027617, loss: 3.6233
2022-10-12 07:17:39 - train: epoch 0065, iter [04000, 05004], lr: 0.027589, loss: 4.0031
2022-10-12 07:18:28 - train: epoch 0065, iter [04100, 05004], lr: 0.027561, loss: 3.8391
2022-10-12 07:19:19 - train: epoch 0065, iter [04200, 05004], lr: 0.027533, loss: 4.0205
2022-10-12 07:20:08 - train: epoch 0065, iter [04300, 05004], lr: 0.027505, loss: 3.9707
2022-10-12 07:20:59 - train: epoch 0065, iter [04400, 05004], lr: 0.027477, loss: 3.9311
2022-10-12 07:21:48 - train: epoch 0065, iter [04500, 05004], lr: 0.027449, loss: 4.0661
2022-10-12 07:22:38 - train: epoch 0065, iter [04600, 05004], lr: 0.027421, loss: 4.3548
2022-10-12 07:23:28 - train: epoch 0065, iter [04700, 05004], lr: 0.027393, loss: 4.2166
2022-10-12 07:24:17 - train: epoch 0065, iter [04800, 05004], lr: 0.027365, loss: 3.8646
2022-10-12 07:25:05 - train: epoch 0065, iter [04900, 05004], lr: 0.027337, loss: 3.6708
2022-10-12 07:25:52 - train: epoch 0065, iter [05000, 05004], lr: 0.027309, loss: 4.2821
2022-10-12 07:25:54 - train: epoch 065, train_loss: 4.0437
2022-10-12 07:27:40 - eval: epoch: 065, acc1: 65.788%, acc5: 87.274%, test_loss: 1.6653, per_image_load_time: 2.067ms, per_image_inference_time: 0.530ms
2022-10-12 07:27:41 - until epoch: 065, best_acc1: 65.788%
2022-10-12 07:27:41 - epoch 066 lr: 0.027308
2022-10-12 07:28:40 - train: epoch 0066, iter [00100, 05004], lr: 0.027280, loss: 3.6965
2022-10-12 07:29:29 - train: epoch 0066, iter [00200, 05004], lr: 0.027252, loss: 4.8752
2022-10-12 07:30:21 - train: epoch 0066, iter [00300, 05004], lr: 0.027224, loss: 4.6835
2022-10-12 07:31:11 - train: epoch 0066, iter [00400, 05004], lr: 0.027196, loss: 3.0746
2022-10-12 07:32:02 - train: epoch 0066, iter [00500, 05004], lr: 0.027168, loss: 3.8791
2022-10-12 07:32:51 - train: epoch 0066, iter [00600, 05004], lr: 0.027140, loss: 4.5590
2022-10-12 07:33:39 - train: epoch 0066, iter [00700, 05004], lr: 0.027112, loss: 4.4169
2022-10-12 07:34:28 - train: epoch 0066, iter [00800, 05004], lr: 0.027084, loss: 3.9627
2022-10-12 07:35:19 - train: epoch 0066, iter [00900, 05004], lr: 0.027056, loss: 3.9092
2022-10-12 07:36:09 - train: epoch 0066, iter [01000, 05004], lr: 0.027029, loss: 3.8473
2022-10-12 07:36:59 - train: epoch 0066, iter [01100, 05004], lr: 0.027001, loss: 4.4041
2022-10-12 07:37:49 - train: epoch 0066, iter [01200, 05004], lr: 0.026973, loss: 3.7729
2022-10-12 07:38:38 - train: epoch 0066, iter [01300, 05004], lr: 0.026945, loss: 4.5274
2022-10-12 07:39:28 - train: epoch 0066, iter [01400, 05004], lr: 0.026917, loss: 4.2853
2022-10-12 07:40:19 - train: epoch 0066, iter [01500, 05004], lr: 0.026889, loss: 3.5322
2022-10-12 07:41:09 - train: epoch 0066, iter [01600, 05004], lr: 0.026861, loss: 3.8018
2022-10-12 07:41:59 - train: epoch 0066, iter [01700, 05004], lr: 0.026834, loss: 3.9170
2022-10-12 07:42:47 - train: epoch 0066, iter [01800, 05004], lr: 0.026806, loss: 3.8940
2022-10-12 07:43:37 - train: epoch 0066, iter [01900, 05004], lr: 0.026778, loss: 4.0774
2022-10-12 07:44:27 - train: epoch 0066, iter [02000, 05004], lr: 0.026750, loss: 3.6647
2022-10-12 07:45:18 - train: epoch 0066, iter [02100, 05004], lr: 0.026722, loss: 3.8378
2022-10-12 07:46:08 - train: epoch 0066, iter [02200, 05004], lr: 0.026695, loss: 3.9886
2022-10-12 07:46:59 - train: epoch 0066, iter [02300, 05004], lr: 0.026667, loss: 3.4726
2022-10-12 07:47:49 - train: epoch 0066, iter [02400, 05004], lr: 0.026639, loss: 3.9877
2022-10-12 07:48:39 - train: epoch 0066, iter [02500, 05004], lr: 0.026611, loss: 3.1342
2022-10-12 07:49:30 - train: epoch 0066, iter [02600, 05004], lr: 0.026584, loss: 3.6309
2022-10-12 07:50:20 - train: epoch 0066, iter [02700, 05004], lr: 0.026556, loss: 4.4134
2022-10-12 07:51:09 - train: epoch 0066, iter [02800, 05004], lr: 0.026528, loss: 3.7255
2022-10-12 07:51:59 - train: epoch 0066, iter [02900, 05004], lr: 0.026501, loss: 3.8481
2022-10-12 07:52:49 - train: epoch 0066, iter [03000, 05004], lr: 0.026473, loss: 5.0076
2022-10-12 07:53:38 - train: epoch 0066, iter [03100, 05004], lr: 0.026445, loss: 4.0614
2022-10-12 07:54:27 - train: epoch 0066, iter [03200, 05004], lr: 0.026417, loss: 3.7890
2022-10-12 07:55:17 - train: epoch 0066, iter [03300, 05004], lr: 0.026390, loss: 4.3207
2022-10-12 07:56:07 - train: epoch 0066, iter [03400, 05004], lr: 0.026362, loss: 4.5503
2022-10-12 07:56:56 - train: epoch 0066, iter [03500, 05004], lr: 0.026334, loss: 4.1450
2022-10-12 07:57:46 - train: epoch 0066, iter [03600, 05004], lr: 0.026307, loss: 4.3058
2022-10-12 07:58:37 - train: epoch 0066, iter [03700, 05004], lr: 0.026279, loss: 3.7789
2022-10-12 07:59:27 - train: epoch 0066, iter [03800, 05004], lr: 0.026252, loss: 4.2810
2022-10-12 08:00:16 - train: epoch 0066, iter [03900, 05004], lr: 0.026224, loss: 3.9005
2022-10-12 08:01:07 - train: epoch 0066, iter [04000, 05004], lr: 0.026196, loss: 4.2899
2022-10-12 08:01:55 - train: epoch 0066, iter [04100, 05004], lr: 0.026169, loss: 4.0771
2022-10-12 08:02:44 - train: epoch 0066, iter [04200, 05004], lr: 0.026141, loss: 3.6003
2022-10-12 08:03:34 - train: epoch 0066, iter [04300, 05004], lr: 0.026114, loss: 4.1787
2022-10-12 08:04:25 - train: epoch 0066, iter [04400, 05004], lr: 0.026086, loss: 4.3926
2022-10-12 08:05:15 - train: epoch 0066, iter [04500, 05004], lr: 0.026058, loss: 4.6822
2022-10-12 08:06:05 - train: epoch 0066, iter [04600, 05004], lr: 0.026031, loss: 4.6182
2022-10-12 08:06:54 - train: epoch 0066, iter [04700, 05004], lr: 0.026003, loss: 4.8987
2022-10-12 08:07:44 - train: epoch 0066, iter [04800, 05004], lr: 0.025976, loss: 3.6526
2022-10-12 08:08:34 - train: epoch 0066, iter [04900, 05004], lr: 0.025948, loss: 4.3265
2022-10-12 08:09:21 - train: epoch 0066, iter [05000, 05004], lr: 0.025921, loss: 3.5913
2022-10-12 08:09:24 - train: epoch 066, train_loss: 4.0277
2022-10-12 08:11:08 - eval: epoch: 066, acc1: 64.736%, acc5: 86.822%, test_loss: 1.6719, per_image_load_time: 3.240ms, per_image_inference_time: 0.521ms
2022-10-12 08:11:09 - until epoch: 066, best_acc1: 65.788%
2022-10-12 08:11:09 - epoch 067 lr: 0.025920
2022-10-12 08:12:04 - train: epoch 0067, iter [00100, 05004], lr: 0.025892, loss: 4.2841
2022-10-12 08:12:53 - train: epoch 0067, iter [00200, 05004], lr: 0.025865, loss: 3.6684
2022-10-12 08:13:43 - train: epoch 0067, iter [00300, 05004], lr: 0.025837, loss: 3.7922
2022-10-12 08:14:32 - train: epoch 0067, iter [00400, 05004], lr: 0.025810, loss: 4.1583
2022-10-12 08:15:23 - train: epoch 0067, iter [00500, 05004], lr: 0.025782, loss: 3.9349
2022-10-12 08:16:14 - train: epoch 0067, iter [00600, 05004], lr: 0.025755, loss: 4.2878
2022-10-12 08:17:03 - train: epoch 0067, iter [00700, 05004], lr: 0.025727, loss: 4.1274
2022-10-12 08:17:53 - train: epoch 0067, iter [00800, 05004], lr: 0.025700, loss: 3.9971
2022-10-12 08:18:45 - train: epoch 0067, iter [00900, 05004], lr: 0.025673, loss: 4.1286
2022-10-12 08:19:34 - train: epoch 0067, iter [01000, 05004], lr: 0.025645, loss: 3.8435
2022-10-12 08:20:24 - train: epoch 0067, iter [01100, 05004], lr: 0.025618, loss: 3.8571
2022-10-12 08:21:12 - train: epoch 0067, iter [01200, 05004], lr: 0.025590, loss: 4.5386
2022-10-12 08:22:01 - train: epoch 0067, iter [01300, 05004], lr: 0.025563, loss: 4.0006
2022-10-12 08:22:51 - train: epoch 0067, iter [01400, 05004], lr: 0.025536, loss: 3.2175
2022-10-12 08:23:42 - train: epoch 0067, iter [01500, 05004], lr: 0.025508, loss: 4.1285
2022-10-12 08:24:32 - train: epoch 0067, iter [01600, 05004], lr: 0.025481, loss: 4.1368
2022-10-12 08:25:21 - train: epoch 0067, iter [01700, 05004], lr: 0.025454, loss: 3.3897
2022-10-12 08:26:11 - train: epoch 0067, iter [01800, 05004], lr: 0.025426, loss: 4.0462
2022-10-12 08:27:01 - train: epoch 0067, iter [01900, 05004], lr: 0.025399, loss: 4.2712
2022-10-12 08:27:52 - train: epoch 0067, iter [02000, 05004], lr: 0.025372, loss: 3.9174
2022-10-12 08:28:42 - train: epoch 0067, iter [02100, 05004], lr: 0.025344, loss: 3.7796
2022-10-12 08:29:33 - train: epoch 0067, iter [02200, 05004], lr: 0.025317, loss: 4.5822
2022-10-12 08:30:22 - train: epoch 0067, iter [02300, 05004], lr: 0.025290, loss: 4.4426
2022-10-12 08:31:11 - train: epoch 0067, iter [02400, 05004], lr: 0.025262, loss: 3.9528
2022-10-12 08:31:59 - train: epoch 0067, iter [02500, 05004], lr: 0.025235, loss: 4.3341
2022-10-12 08:32:50 - train: epoch 0067, iter [02600, 05004], lr: 0.025208, loss: 3.8075
2022-10-12 08:33:39 - train: epoch 0067, iter [02700, 05004], lr: 0.025181, loss: 4.4251
2022-10-12 08:34:31 - train: epoch 0067, iter [02800, 05004], lr: 0.025153, loss: 3.6267
2022-10-12 08:35:19 - train: epoch 0067, iter [02900, 05004], lr: 0.025126, loss: 3.8435
2022-10-12 08:36:09 - train: epoch 0067, iter [03000, 05004], lr: 0.025099, loss: 4.3245
2022-10-12 08:36:59 - train: epoch 0067, iter [03100, 05004], lr: 0.025072, loss: 4.5669
2022-10-12 08:37:50 - train: epoch 0067, iter [03200, 05004], lr: 0.025044, loss: 4.5266
2022-10-12 08:38:39 - train: epoch 0067, iter [03300, 05004], lr: 0.025017, loss: 4.1196
2022-10-12 08:39:30 - train: epoch 0067, iter [03400, 05004], lr: 0.024990, loss: 3.6835
2022-10-12 08:40:19 - train: epoch 0067, iter [03500, 05004], lr: 0.024963, loss: 4.4377
2022-10-12 08:41:08 - train: epoch 0067, iter [03600, 05004], lr: 0.024936, loss: 4.5750
2022-10-12 08:41:58 - train: epoch 0067, iter [03700, 05004], lr: 0.024909, loss: 4.0303
2022-10-12 08:42:48 - train: epoch 0067, iter [03800, 05004], lr: 0.024881, loss: 4.7642
2022-10-12 08:43:39 - train: epoch 0067, iter [03900, 05004], lr: 0.024854, loss: 3.7374
2022-10-12 08:44:29 - train: epoch 0067, iter [04000, 05004], lr: 0.024827, loss: 3.3647
2022-10-12 08:45:20 - train: epoch 0067, iter [04100, 05004], lr: 0.024800, loss: 4.0823
2022-10-12 08:46:09 - train: epoch 0067, iter [04200, 05004], lr: 0.024773, loss: 4.2725
2022-10-12 08:47:01 - train: epoch 0067, iter [04300, 05004], lr: 0.024746, loss: 4.1126
2022-10-12 08:47:51 - train: epoch 0067, iter [04400, 05004], lr: 0.024719, loss: 3.7131
2022-10-12 08:48:42 - train: epoch 0067, iter [04500, 05004], lr: 0.024692, loss: 4.2834
2022-10-12 08:49:32 - train: epoch 0067, iter [04600, 05004], lr: 0.024665, loss: 3.8799
2022-10-12 08:50:19 - train: epoch 0067, iter [04700, 05004], lr: 0.024638, loss: 4.2628
2022-10-12 08:51:09 - train: epoch 0067, iter [04800, 05004], lr: 0.024611, loss: 3.5178
2022-10-12 08:52:00 - train: epoch 0067, iter [04900, 05004], lr: 0.024584, loss: 3.7839
2022-10-12 08:52:46 - train: epoch 0067, iter [05000, 05004], lr: 0.024557, loss: 3.7454
2022-10-12 08:52:49 - train: epoch 067, train_loss: 3.9998
2022-10-12 08:54:37 - eval: epoch: 067, acc1: 67.052%, acc5: 88.090%, test_loss: 1.5468, per_image_load_time: 3.414ms, per_image_inference_time: 0.507ms
2022-10-12 08:54:38 - until epoch: 067, best_acc1: 67.052%
2022-10-12 08:54:38 - epoch 068 lr: 0.024555
2022-10-12 08:55:34 - train: epoch 0068, iter [00100, 05004], lr: 0.024528, loss: 3.7768
2022-10-12 08:56:24 - train: epoch 0068, iter [00200, 05004], lr: 0.024501, loss: 4.2944
2022-10-12 08:57:14 - train: epoch 0068, iter [00300, 05004], lr: 0.024474, loss: 4.3053
2022-10-12 08:58:03 - train: epoch 0068, iter [00400, 05004], lr: 0.024447, loss: 3.7085
2022-10-12 08:58:53 - train: epoch 0068, iter [00500, 05004], lr: 0.024421, loss: 3.3556
2022-10-12 08:59:42 - train: epoch 0068, iter [00600, 05004], lr: 0.024394, loss: 4.3869
2022-10-12 09:00:32 - train: epoch 0068, iter [00700, 05004], lr: 0.024367, loss: 4.7135
2022-10-12 09:01:22 - train: epoch 0068, iter [00800, 05004], lr: 0.024340, loss: 4.2879
2022-10-12 09:02:13 - train: epoch 0068, iter [00900, 05004], lr: 0.024313, loss: 3.5838
2022-10-12 09:03:02 - train: epoch 0068, iter [01000, 05004], lr: 0.024286, loss: 3.7403
2022-10-12 09:03:53 - train: epoch 0068, iter [01100, 05004], lr: 0.024259, loss: 4.1551
2022-10-12 09:04:43 - train: epoch 0068, iter [01200, 05004], lr: 0.024232, loss: 4.2535
2022-10-12 09:05:34 - train: epoch 0068, iter [01300, 05004], lr: 0.024205, loss: 3.9457
2022-10-12 09:06:24 - train: epoch 0068, iter [01400, 05004], lr: 0.024178, loss: 4.2327
2022-10-12 09:07:15 - train: epoch 0068, iter [01500, 05004], lr: 0.024151, loss: 3.9423
2022-10-12 09:08:06 - train: epoch 0068, iter [01600, 05004], lr: 0.024124, loss: 3.4150
2022-10-12 09:08:55 - train: epoch 0068, iter [01700, 05004], lr: 0.024098, loss: 4.1799
2022-10-12 09:09:45 - train: epoch 0068, iter [01800, 05004], lr: 0.024071, loss: 4.6003
2022-10-12 09:10:34 - train: epoch 0068, iter [01900, 05004], lr: 0.024044, loss: 3.5956
2022-10-12 09:11:25 - train: epoch 0068, iter [02000, 05004], lr: 0.024017, loss: 3.9825
2022-10-12 09:12:16 - train: epoch 0068, iter [02100, 05004], lr: 0.023990, loss: 4.7004
2022-10-12 09:13:06 - train: epoch 0068, iter [02200, 05004], lr: 0.023964, loss: 3.9482
2022-10-12 09:13:56 - train: epoch 0068, iter [02300, 05004], lr: 0.023937, loss: 4.2076
2022-10-12 09:14:47 - train: epoch 0068, iter [02400, 05004], lr: 0.023910, loss: 4.3409
2022-10-12 09:15:34 - train: epoch 0068, iter [02500, 05004], lr: 0.023883, loss: 4.5423
2022-10-12 09:16:23 - train: epoch 0068, iter [02600, 05004], lr: 0.023856, loss: 3.9066
2022-10-12 09:17:11 - train: epoch 0068, iter [02700, 05004], lr: 0.023830, loss: 4.7770
2022-10-12 09:18:00 - train: epoch 0068, iter [02800, 05004], lr: 0.023803, loss: 3.8951
2022-10-12 09:18:48 - train: epoch 0068, iter [02900, 05004], lr: 0.023776, loss: 4.7459
2022-10-12 09:19:36 - train: epoch 0068, iter [03000, 05004], lr: 0.023750, loss: 4.5821
2022-10-12 09:20:24 - train: epoch 0068, iter [03100, 05004], lr: 0.023723, loss: 3.7444
2022-10-12 09:21:12 - train: epoch 0068, iter [03200, 05004], lr: 0.023696, loss: 4.1335
2022-10-12 09:22:00 - train: epoch 0068, iter [03300, 05004], lr: 0.023669, loss: 4.5295
2022-10-12 09:22:50 - train: epoch 0068, iter [03400, 05004], lr: 0.023643, loss: 3.2608
2022-10-12 09:23:38 - train: epoch 0068, iter [03500, 05004], lr: 0.023616, loss: 3.9942
2022-10-12 09:24:25 - train: epoch 0068, iter [03600, 05004], lr: 0.023589, loss: 4.1350
2022-10-12 09:25:14 - train: epoch 0068, iter [03700, 05004], lr: 0.023563, loss: 3.9226
2022-10-12 09:26:01 - train: epoch 0068, iter [03800, 05004], lr: 0.023536, loss: 4.4043
2022-10-12 09:26:51 - train: epoch 0068, iter [03900, 05004], lr: 0.023510, loss: 4.6721
2022-10-12 09:27:38 - train: epoch 0068, iter [04000, 05004], lr: 0.023483, loss: 3.3131
2022-10-12 09:28:25 - train: epoch 0068, iter [04100, 05004], lr: 0.023456, loss: 4.7983
2022-10-12 09:29:13 - train: epoch 0068, iter [04200, 05004], lr: 0.023430, loss: 4.0361
2022-10-12 09:30:02 - train: epoch 0068, iter [04300, 05004], lr: 0.023403, loss: 4.1012
2022-10-12 09:30:51 - train: epoch 0068, iter [04400, 05004], lr: 0.023377, loss: 3.5129
2022-10-12 09:31:39 - train: epoch 0068, iter [04500, 05004], lr: 0.023350, loss: 4.5021
2022-10-12 09:32:29 - train: epoch 0068, iter [04600, 05004], lr: 0.023323, loss: 4.6654
2022-10-12 09:33:19 - train: epoch 0068, iter [04700, 05004], lr: 0.023297, loss: 3.9236
2022-10-12 09:34:09 - train: epoch 0068, iter [04800, 05004], lr: 0.023270, loss: 4.6164
2022-10-12 09:34:57 - train: epoch 0068, iter [04900, 05004], lr: 0.023244, loss: 4.0474
2022-10-12 09:35:45 - train: epoch 0068, iter [05000, 05004], lr: 0.023217, loss: 4.8212
2022-10-12 09:35:47 - train: epoch 068, train_loss: 3.9863
2022-10-12 09:37:37 - eval: epoch: 068, acc1: 67.510%, acc5: 88.574%, test_loss: 1.5446, per_image_load_time: 3.621ms, per_image_inference_time: 0.515ms
2022-10-12 09:37:37 - until epoch: 068, best_acc1: 67.510%
2022-10-12 09:37:37 - epoch 069 lr: 0.023216
2022-10-12 09:38:32 - train: epoch 0069, iter [00100, 05004], lr: 0.023190, loss: 3.0278
2022-10-12 09:39:23 - train: epoch 0069, iter [00200, 05004], lr: 0.023163, loss: 3.7769
2022-10-12 09:40:12 - train: epoch 0069, iter [00300, 05004], lr: 0.023137, loss: 3.3080
2022-10-12 09:41:02 - train: epoch 0069, iter [00400, 05004], lr: 0.023110, loss: 3.7796
2022-10-12 09:41:51 - train: epoch 0069, iter [00500, 05004], lr: 0.023084, loss: 3.1744
2022-10-12 09:42:43 - train: epoch 0069, iter [00600, 05004], lr: 0.023058, loss: 3.5036
2022-10-12 09:43:34 - train: epoch 0069, iter [00700, 05004], lr: 0.023031, loss: 3.9642
2022-10-12 09:44:24 - train: epoch 0069, iter [00800, 05004], lr: 0.023005, loss: 4.2616
2022-10-12 09:45:14 - train: epoch 0069, iter [00900, 05004], lr: 0.022978, loss: 4.3282
2022-10-12 09:46:04 - train: epoch 0069, iter [01000, 05004], lr: 0.022952, loss: 3.3479
2022-10-12 09:46:53 - train: epoch 0069, iter [01100, 05004], lr: 0.022925, loss: 3.6670
2022-10-12 09:47:46 - train: epoch 0069, iter [01200, 05004], lr: 0.022899, loss: 3.5342
2022-10-12 09:48:33 - train: epoch 0069, iter [01300, 05004], lr: 0.022873, loss: 3.3926
2022-10-12 09:49:23 - train: epoch 0069, iter [01400, 05004], lr: 0.022846, loss: 3.7723
2022-10-12 09:50:13 - train: epoch 0069, iter [01500, 05004], lr: 0.022820, loss: 3.7500
2022-10-12 09:51:04 - train: epoch 0069, iter [01600, 05004], lr: 0.022794, loss: 3.9860
2022-10-12 09:51:53 - train: epoch 0069, iter [01700, 05004], lr: 0.022767, loss: 3.3386
2022-10-12 09:52:43 - train: epoch 0069, iter [01800, 05004], lr: 0.022741, loss: 4.2135
2022-10-12 09:53:32 - train: epoch 0069, iter [01900, 05004], lr: 0.022715, loss: 3.6458
2022-10-12 09:54:24 - train: epoch 0069, iter [02000, 05004], lr: 0.022688, loss: 4.1222
2022-10-12 09:55:15 - train: epoch 0069, iter [02100, 05004], lr: 0.022662, loss: 3.6599
2022-10-12 09:56:05 - train: epoch 0069, iter [02200, 05004], lr: 0.022636, loss: 4.1455
2022-10-12 09:56:55 - train: epoch 0069, iter [02300, 05004], lr: 0.022610, loss: 3.7235
2022-10-12 09:57:45 - train: epoch 0069, iter [02400, 05004], lr: 0.022583, loss: 4.6138
2022-10-12 09:58:35 - train: epoch 0069, iter [02500, 05004], lr: 0.022557, loss: 3.1461
2022-10-12 09:59:26 - train: epoch 0069, iter [02600, 05004], lr: 0.022531, loss: 4.3396
2022-10-12 10:00:16 - train: epoch 0069, iter [02700, 05004], lr: 0.022505, loss: 4.4238
2022-10-12 10:01:06 - train: epoch 0069, iter [02800, 05004], lr: 0.022478, loss: 4.3187
2022-10-12 10:01:57 - train: epoch 0069, iter [02900, 05004], lr: 0.022452, loss: 4.1458
2022-10-12 10:02:47 - train: epoch 0069, iter [03000, 05004], lr: 0.022426, loss: 3.5090
2022-10-12 10:03:38 - train: epoch 0069, iter [03100, 05004], lr: 0.022400, loss: 4.4633
2022-10-12 10:04:28 - train: epoch 0069, iter [03200, 05004], lr: 0.022374, loss: 3.9807
2022-10-12 10:05:17 - train: epoch 0069, iter [03300, 05004], lr: 0.022348, loss: 3.5217
2022-10-12 10:06:09 - train: epoch 0069, iter [03400, 05004], lr: 0.022321, loss: 4.3200
2022-10-12 10:07:00 - train: epoch 0069, iter [03500, 05004], lr: 0.022295, loss: 4.3606
2022-10-12 10:07:50 - train: epoch 0069, iter [03600, 05004], lr: 0.022269, loss: 3.8375
2022-10-12 10:08:41 - train: epoch 0069, iter [03700, 05004], lr: 0.022243, loss: 4.1686
2022-10-12 10:09:31 - train: epoch 0069, iter [03800, 05004], lr: 0.022217, loss: 3.6790
2022-10-12 10:10:20 - train: epoch 0069, iter [03900, 05004], lr: 0.022191, loss: 4.1135
2022-10-12 10:11:10 - train: epoch 0069, iter [04000, 05004], lr: 0.022165, loss: 3.9899
2022-10-12 10:11:59 - train: epoch 0069, iter [04100, 05004], lr: 0.022139, loss: 3.8793
2022-10-12 10:12:48 - train: epoch 0069, iter [04200, 05004], lr: 0.022113, loss: 3.4615
2022-10-12 10:13:37 - train: epoch 0069, iter [04300, 05004], lr: 0.022087, loss: 3.7798
2022-10-12 10:14:26 - train: epoch 0069, iter [04400, 05004], lr: 0.022061, loss: 3.7454
2022-10-12 10:15:14 - train: epoch 0069, iter [04500, 05004], lr: 0.022035, loss: 4.5967
2022-10-12 10:16:04 - train: epoch 0069, iter [04600, 05004], lr: 0.022009, loss: 4.3815
2022-10-12 10:16:50 - train: epoch 0069, iter [04700, 05004], lr: 0.021983, loss: 4.3368
2022-10-12 10:17:39 - train: epoch 0069, iter [04800, 05004], lr: 0.021957, loss: 3.6657
2022-10-12 10:18:27 - train: epoch 0069, iter [04900, 05004], lr: 0.021931, loss: 3.8897
2022-10-12 10:19:12 - train: epoch 0069, iter [05000, 05004], lr: 0.021905, loss: 4.4357
2022-10-12 10:19:15 - train: epoch 069, train_loss: 3.9804
2022-10-12 10:20:57 - eval: epoch: 069, acc1: 67.532%, acc5: 88.342%, test_loss: 1.5887, per_image_load_time: 3.429ms, per_image_inference_time: 0.510ms
2022-10-12 10:20:58 - until epoch: 069, best_acc1: 67.532%
2022-10-12 10:20:58 - epoch 070 lr: 0.021904
2022-10-12 10:21:51 - train: epoch 0070, iter [00100, 05004], lr: 0.021878, loss: 4.1930
2022-10-12 10:22:41 - train: epoch 0070, iter [00200, 05004], lr: 0.021852, loss: 4.1930
2022-10-12 10:23:29 - train: epoch 0070, iter [00300, 05004], lr: 0.021826, loss: 4.0637
2022-10-12 10:24:18 - train: epoch 0070, iter [00400, 05004], lr: 0.021800, loss: 3.6717
2022-10-12 10:25:05 - train: epoch 0070, iter [00500, 05004], lr: 0.021774, loss: 4.0350
2022-10-12 10:25:54 - train: epoch 0070, iter [00600, 05004], lr: 0.021748, loss: 4.1102
2022-10-12 10:26:42 - train: epoch 0070, iter [00700, 05004], lr: 0.021722, loss: 3.9493
2022-10-12 10:27:29 - train: epoch 0070, iter [00800, 05004], lr: 0.021696, loss: 3.1550
2022-10-12 10:28:16 - train: epoch 0070, iter [00900, 05004], lr: 0.021670, loss: 4.3137
2022-10-12 10:29:06 - train: epoch 0070, iter [01000, 05004], lr: 0.021645, loss: 4.3887
2022-10-12 10:29:54 - train: epoch 0070, iter [01100, 05004], lr: 0.021619, loss: 4.1921
2022-10-12 10:30:41 - train: epoch 0070, iter [01200, 05004], lr: 0.021593, loss: 4.0260
2022-10-12 10:31:28 - train: epoch 0070, iter [01300, 05004], lr: 0.021567, loss: 2.9245
2022-10-12 10:32:17 - train: epoch 0070, iter [01400, 05004], lr: 0.021541, loss: 4.5336
2022-10-12 10:33:05 - train: epoch 0070, iter [01500, 05004], lr: 0.021515, loss: 3.7275
2022-10-12 10:33:53 - train: epoch 0070, iter [01600, 05004], lr: 0.021490, loss: 4.9312
2022-10-12 10:34:41 - train: epoch 0070, iter [01700, 05004], lr: 0.021464, loss: 4.0547
2022-10-12 10:35:28 - train: epoch 0070, iter [01800, 05004], lr: 0.021438, loss: 3.4830
2022-10-12 10:36:17 - train: epoch 0070, iter [01900, 05004], lr: 0.021412, loss: 4.5382
2022-10-12 10:37:04 - train: epoch 0070, iter [02000, 05004], lr: 0.021387, loss: 4.3293
2022-10-12 10:37:53 - train: epoch 0070, iter [02100, 05004], lr: 0.021361, loss: 4.4256
2022-10-12 10:38:42 - train: epoch 0070, iter [02200, 05004], lr: 0.021335, loss: 3.8067
2022-10-12 10:39:30 - train: epoch 0070, iter [02300, 05004], lr: 0.021310, loss: 4.7413
2022-10-12 10:40:19 - train: epoch 0070, iter [02400, 05004], lr: 0.021284, loss: 4.0716
2022-10-12 10:41:06 - train: epoch 0070, iter [02500, 05004], lr: 0.021258, loss: 3.6871
2022-10-12 10:41:56 - train: epoch 0070, iter [02600, 05004], lr: 0.021232, loss: 3.8618
2022-10-12 10:42:44 - train: epoch 0070, iter [02700, 05004], lr: 0.021207, loss: 3.3431
2022-10-12 10:43:33 - train: epoch 0070, iter [02800, 05004], lr: 0.021181, loss: 4.4107
2022-10-12 10:44:21 - train: epoch 0070, iter [02900, 05004], lr: 0.021155, loss: 3.9778
2022-10-12 10:45:09 - train: epoch 0070, iter [03000, 05004], lr: 0.021130, loss: 4.2064
2022-10-12 10:45:55 - train: epoch 0070, iter [03100, 05004], lr: 0.021104, loss: 3.6011
2022-10-12 10:46:44 - train: epoch 0070, iter [03200, 05004], lr: 0.021079, loss: 4.3386
2022-10-12 10:47:33 - train: epoch 0070, iter [03300, 05004], lr: 0.021053, loss: 4.6447
2022-10-12 10:48:21 - train: epoch 0070, iter [03400, 05004], lr: 0.021027, loss: 4.2182
2022-10-12 10:49:07 - train: epoch 0070, iter [03500, 05004], lr: 0.021002, loss: 4.4013
2022-10-12 10:49:57 - train: epoch 0070, iter [03600, 05004], lr: 0.020976, loss: 4.2775
2022-10-12 10:50:48 - train: epoch 0070, iter [03700, 05004], lr: 0.020951, loss: 4.0646
2022-10-12 10:51:38 - train: epoch 0070, iter [03800, 05004], lr: 0.020925, loss: 4.5871
2022-10-12 10:52:28 - train: epoch 0070, iter [03900, 05004], lr: 0.020900, loss: 3.9270
2022-10-12 10:53:18 - train: epoch 0070, iter [04000, 05004], lr: 0.020874, loss: 4.8562
2022-10-12 10:54:07 - train: epoch 0070, iter [04100, 05004], lr: 0.020849, loss: 4.1286
2022-10-12 10:54:58 - train: epoch 0070, iter [04200, 05004], lr: 0.020823, loss: 4.4113
2022-10-12 10:55:48 - train: epoch 0070, iter [04300, 05004], lr: 0.020798, loss: 3.4846
2022-10-12 10:56:39 - train: epoch 0070, iter [04400, 05004], lr: 0.020772, loss: 2.8892
2022-10-12 10:57:30 - train: epoch 0070, iter [04500, 05004], lr: 0.020747, loss: 4.2832
2022-10-12 10:58:20 - train: epoch 0070, iter [04600, 05004], lr: 0.020721, loss: 4.3180
2022-10-12 10:59:11 - train: epoch 0070, iter [04700, 05004], lr: 0.020696, loss: 3.7505
2022-10-12 11:00:01 - train: epoch 0070, iter [04800, 05004], lr: 0.020671, loss: 4.2584
2022-10-12 11:00:51 - train: epoch 0070, iter [04900, 05004], lr: 0.020645, loss: 4.7539
2022-10-12 11:01:39 - train: epoch 0070, iter [05000, 05004], lr: 0.020620, loss: 3.6234
2022-10-12 11:01:42 - train: epoch 070, train_loss: 3.9704
2022-10-12 11:03:32 - eval: epoch: 070, acc1: 68.638%, acc5: 89.154%, test_loss: 1.5465, per_image_load_time: 3.636ms, per_image_inference_time: 0.521ms
2022-10-12 11:03:32 - until epoch: 070, best_acc1: 68.638%
2022-10-12 11:03:32 - epoch 071 lr: 0.020619
2022-10-12 11:04:30 - train: epoch 0071, iter [00100, 05004], lr: 0.020593, loss: 4.0154
2022-10-12 11:05:21 - train: epoch 0071, iter [00200, 05004], lr: 0.020568, loss: 3.7474
2022-10-12 11:06:11 - train: epoch 0071, iter [00300, 05004], lr: 0.020543, loss: 4.3297
2022-10-12 11:07:01 - train: epoch 0071, iter [00400, 05004], lr: 0.020517, loss: 3.8759
2022-10-12 11:07:52 - train: epoch 0071, iter [00500, 05004], lr: 0.020492, loss: 4.0053
2022-10-12 11:08:43 - train: epoch 0071, iter [00600, 05004], lr: 0.020467, loss: 3.8670
2022-10-12 11:09:33 - train: epoch 0071, iter [00700, 05004], lr: 0.020441, loss: 4.7215
2022-10-12 11:10:24 - train: epoch 0071, iter [00800, 05004], lr: 0.020416, loss: 4.4379
2022-10-12 11:11:14 - train: epoch 0071, iter [00900, 05004], lr: 0.020391, loss: 3.2933
2022-10-12 11:12:05 - train: epoch 0071, iter [01000, 05004], lr: 0.020365, loss: 3.9910
2022-10-12 11:12:55 - train: epoch 0071, iter [01100, 05004], lr: 0.020340, loss: 4.0144
2022-10-12 11:13:44 - train: epoch 0071, iter [01200, 05004], lr: 0.020315, loss: 3.1603
2022-10-12 11:14:36 - train: epoch 0071, iter [01300, 05004], lr: 0.020290, loss: 4.5314
2022-10-12 11:15:25 - train: epoch 0071, iter [01400, 05004], lr: 0.020264, loss: 3.9468
2022-10-12 11:16:16 - train: epoch 0071, iter [01500, 05004], lr: 0.020239, loss: 3.5672
2022-10-12 11:17:06 - train: epoch 0071, iter [01600, 05004], lr: 0.020214, loss: 4.2916
2022-10-12 11:17:55 - train: epoch 0071, iter [01700, 05004], lr: 0.020189, loss: 4.2241
2022-10-12 11:18:44 - train: epoch 0071, iter [01800, 05004], lr: 0.020163, loss: 3.5312
2022-10-12 11:19:34 - train: epoch 0071, iter [01900, 05004], lr: 0.020138, loss: 3.0083
2022-10-12 11:20:23 - train: epoch 0071, iter [02000, 05004], lr: 0.020113, loss: 4.1515
2022-10-12 11:21:14 - train: epoch 0071, iter [02100, 05004], lr: 0.020088, loss: 3.8458
2022-10-12 11:22:06 - train: epoch 0071, iter [02200, 05004], lr: 0.020063, loss: 3.4521
2022-10-12 11:22:55 - train: epoch 0071, iter [02300, 05004], lr: 0.020038, loss: 4.1140
2022-10-12 11:23:44 - train: epoch 0071, iter [02400, 05004], lr: 0.020013, loss: 3.9370
2022-10-12 11:24:35 - train: epoch 0071, iter [02500, 05004], lr: 0.019987, loss: 4.1831
2022-10-12 11:25:24 - train: epoch 0071, iter [02600, 05004], lr: 0.019962, loss: 4.2798
2022-10-12 11:26:14 - train: epoch 0071, iter [02700, 05004], lr: 0.019937, loss: 3.8038
2022-10-12 11:27:03 - train: epoch 0071, iter [02800, 05004], lr: 0.019912, loss: 4.4167
2022-10-12 11:27:53 - train: epoch 0071, iter [02900, 05004], lr: 0.019887, loss: 3.4756
2022-10-12 11:28:43 - train: epoch 0071, iter [03000, 05004], lr: 0.019862, loss: 3.9243
2022-10-12 11:29:32 - train: epoch 0071, iter [03100, 05004], lr: 0.019837, loss: 4.5904
2022-10-12 11:30:22 - train: epoch 0071, iter [03200, 05004], lr: 0.019812, loss: 3.6086
2022-10-12 11:31:12 - train: epoch 0071, iter [03300, 05004], lr: 0.019787, loss: 4.0470
2022-10-12 11:32:04 - train: epoch 0071, iter [03400, 05004], lr: 0.019762, loss: 4.1769
2022-10-12 11:32:53 - train: epoch 0071, iter [03500, 05004], lr: 0.019737, loss: 4.0048
2022-10-12 11:33:45 - train: epoch 0071, iter [03600, 05004], lr: 0.019712, loss: 3.6300
2022-10-12 11:34:34 - train: epoch 0071, iter [03700, 05004], lr: 0.019687, loss: 4.3783
2022-10-12 11:35:23 - train: epoch 0071, iter [03800, 05004], lr: 0.019662, loss: 3.6900
2022-10-12 11:36:13 - train: epoch 0071, iter [03900, 05004], lr: 0.019637, loss: 4.1089
2022-10-12 11:37:02 - train: epoch 0071, iter [04000, 05004], lr: 0.019612, loss: 4.3392
2022-10-12 11:37:52 - train: epoch 0071, iter [04100, 05004], lr: 0.019587, loss: 4.1395
2022-10-12 11:38:41 - train: epoch 0071, iter [04200, 05004], lr: 0.019563, loss: 3.7499
2022-10-12 11:39:30 - train: epoch 0071, iter [04300, 05004], lr: 0.019538, loss: 2.7583
2022-10-12 11:40:20 - train: epoch 0071, iter [04400, 05004], lr: 0.019513, loss: 4.7811
2022-10-12 11:41:10 - train: epoch 0071, iter [04500, 05004], lr: 0.019488, loss: 3.9367
2022-10-12 11:41:59 - train: epoch 0071, iter [04600, 05004], lr: 0.019463, loss: 3.7823
2022-10-12 11:42:50 - train: epoch 0071, iter [04700, 05004], lr: 0.019438, loss: 3.2273
2022-10-12 11:43:40 - train: epoch 0071, iter [04800, 05004], lr: 0.019413, loss: 4.2398
2022-10-12 11:44:29 - train: epoch 0071, iter [04900, 05004], lr: 0.019389, loss: 3.7743
2022-10-12 11:45:17 - train: epoch 0071, iter [05000, 05004], lr: 0.019364, loss: 4.2712
2022-10-12 11:45:19 - train: epoch 071, train_loss: 3.9432
2022-10-12 11:47:08 - eval: epoch: 071, acc1: 69.062%, acc5: 89.308%, test_loss: 1.5590, per_image_load_time: 3.672ms, per_image_inference_time: 0.513ms
2022-10-12 11:47:09 - until epoch: 071, best_acc1: 69.062%
2022-10-12 11:47:09 - epoch 072 lr: 0.019363
2022-10-12 11:48:04 - train: epoch 0072, iter [00100, 05004], lr: 0.019338, loss: 4.0735
2022-10-12 11:48:56 - train: epoch 0072, iter [00200, 05004], lr: 0.019313, loss: 3.7968
2022-10-12 11:49:45 - train: epoch 0072, iter [00300, 05004], lr: 0.019288, loss: 3.9971
2022-10-12 11:50:35 - train: epoch 0072, iter [00400, 05004], lr: 0.019264, loss: 3.5097
2022-10-12 11:51:25 - train: epoch 0072, iter [00500, 05004], lr: 0.019239, loss: 4.5419
2022-10-12 11:52:15 - train: epoch 0072, iter [00600, 05004], lr: 0.019214, loss: 4.0972
2022-10-12 11:53:05 - train: epoch 0072, iter [00700, 05004], lr: 0.019189, loss: 4.3323
2022-10-12 11:53:53 - train: epoch 0072, iter [00800, 05004], lr: 0.019165, loss: 4.2455
2022-10-12 11:54:45 - train: epoch 0072, iter [00900, 05004], lr: 0.019140, loss: 3.8792
2022-10-12 11:55:34 - train: epoch 0072, iter [01000, 05004], lr: 0.019115, loss: 4.1356
2022-10-12 11:56:24 - train: epoch 0072, iter [01100, 05004], lr: 0.019091, loss: 3.7392
2022-10-12 11:57:13 - train: epoch 0072, iter [01200, 05004], lr: 0.019066, loss: 4.1730
2022-10-12 11:58:04 - train: epoch 0072, iter [01300, 05004], lr: 0.019041, loss: 4.2852
2022-10-12 11:58:53 - train: epoch 0072, iter [01400, 05004], lr: 0.019017, loss: 4.0858
2022-10-12 11:59:43 - train: epoch 0072, iter [01500, 05004], lr: 0.018992, loss: 3.4259
2022-10-12 12:00:32 - train: epoch 0072, iter [01600, 05004], lr: 0.018967, loss: 3.4196
2022-10-12 12:01:22 - train: epoch 0072, iter [01700, 05004], lr: 0.018943, loss: 4.0359
2022-10-12 12:02:13 - train: epoch 0072, iter [01800, 05004], lr: 0.018918, loss: 3.9278
2022-10-12 12:03:01 - train: epoch 0072, iter [01900, 05004], lr: 0.018894, loss: 3.9086
2022-10-12 12:03:52 - train: epoch 0072, iter [02000, 05004], lr: 0.018869, loss: 4.3228
2022-10-12 12:04:41 - train: epoch 0072, iter [02100, 05004], lr: 0.018845, loss: 4.1548
2022-10-12 12:05:32 - train: epoch 0072, iter [02200, 05004], lr: 0.018820, loss: 4.5567
2022-10-12 12:06:20 - train: epoch 0072, iter [02300, 05004], lr: 0.018796, loss: 4.1659
2022-10-12 12:07:11 - train: epoch 0072, iter [02400, 05004], lr: 0.018771, loss: 3.5013
2022-10-12 12:08:02 - train: epoch 0072, iter [02500, 05004], lr: 0.018746, loss: 3.3300
2022-10-12 12:08:51 - train: epoch 0072, iter [02600, 05004], lr: 0.018722, loss: 2.9633
2022-10-12 12:09:40 - train: epoch 0072, iter [02700, 05004], lr: 0.018698, loss: 4.1577
2022-10-12 12:10:31 - train: epoch 0072, iter [02800, 05004], lr: 0.018673, loss: 4.0988
2022-10-12 12:11:22 - train: epoch 0072, iter [02900, 05004], lr: 0.018649, loss: 3.4108
2022-10-12 12:12:11 - train: epoch 0072, iter [03000, 05004], lr: 0.018624, loss: 3.5516
2022-10-12 12:13:02 - train: epoch 0072, iter [03100, 05004], lr: 0.018600, loss: 3.3701
2022-10-12 12:13:51 - train: epoch 0072, iter [03200, 05004], lr: 0.018575, loss: 3.9707
2022-10-12 12:14:41 - train: epoch 0072, iter [03300, 05004], lr: 0.018551, loss: 3.0631
2022-10-12 12:15:30 - train: epoch 0072, iter [03400, 05004], lr: 0.018527, loss: 4.0614
2022-10-12 12:16:21 - train: epoch 0072, iter [03500, 05004], lr: 0.018502, loss: 2.7790
2022-10-12 12:17:10 - train: epoch 0072, iter [03600, 05004], lr: 0.018478, loss: 3.2876
2022-10-12 12:18:01 - train: epoch 0072, iter [03700, 05004], lr: 0.018453, loss: 3.4275
2022-10-12 12:18:51 - train: epoch 0072, iter [03800, 05004], lr: 0.018429, loss: 2.9311
2022-10-12 12:19:40 - train: epoch 0072, iter [03900, 05004], lr: 0.018405, loss: 3.7923
2022-10-12 12:20:29 - train: epoch 0072, iter [04000, 05004], lr: 0.018380, loss: 3.1317
2022-10-12 12:21:18 - train: epoch 0072, iter [04100, 05004], lr: 0.018356, loss: 3.7265
2022-10-12 12:22:09 - train: epoch 0072, iter [04200, 05004], lr: 0.018332, loss: 3.2785
2022-10-12 12:23:00 - train: epoch 0072, iter [04300, 05004], lr: 0.018308, loss: 4.1560
2022-10-12 12:23:49 - train: epoch 0072, iter [04400, 05004], lr: 0.018283, loss: 3.1113
2022-10-12 12:24:37 - train: epoch 0072, iter [04500, 05004], lr: 0.018259, loss: 4.0836
2022-10-12 12:25:27 - train: epoch 0072, iter [04600, 05004], lr: 0.018235, loss: 3.0743
2022-10-12 12:26:17 - train: epoch 0072, iter [04700, 05004], lr: 0.018211, loss: 4.1540
2022-10-12 12:27:09 - train: epoch 0072, iter [04800, 05004], lr: 0.018186, loss: 3.5201
2022-10-12 12:27:58 - train: epoch 0072, iter [04900, 05004], lr: 0.018162, loss: 4.6322
2022-10-12 12:28:44 - train: epoch 0072, iter [05000, 05004], lr: 0.018138, loss: 3.6401
2022-10-12 12:28:46 - train: epoch 072, train_loss: 3.9214
2022-10-12 12:30:35 - eval: epoch: 072, acc1: 68.814%, acc5: 89.170%, test_loss: 1.5107, per_image_load_time: 3.643ms, per_image_inference_time: 0.538ms
2022-10-12 12:30:35 - until epoch: 072, best_acc1: 69.062%
2022-10-12 12:30:35 - epoch 073 lr: 0.018137
2022-10-12 12:31:32 - train: epoch 0073, iter [00100, 05004], lr: 0.018113, loss: 4.1181
2022-10-12 12:32:23 - train: epoch 0073, iter [00200, 05004], lr: 0.018089, loss: 3.6595
2022-10-12 12:33:13 - train: epoch 0073, iter [00300, 05004], lr: 0.018064, loss: 3.7932
2022-10-12 12:34:02 - train: epoch 0073, iter [00400, 05004], lr: 0.018040, loss: 4.3299
2022-10-12 12:34:52 - train: epoch 0073, iter [00500, 05004], lr: 0.018016, loss: 4.2282
2022-10-12 12:35:40 - train: epoch 0073, iter [00600, 05004], lr: 0.017992, loss: 3.9810
2022-10-12 12:36:30 - train: epoch 0073, iter [00700, 05004], lr: 0.017968, loss: 3.9422
2022-10-12 12:37:16 - train: epoch 0073, iter [00800, 05004], lr: 0.017944, loss: 3.6057
2022-10-12 12:38:07 - train: epoch 0073, iter [00900, 05004], lr: 0.017920, loss: 3.7627
2022-10-12 12:38:56 - train: epoch 0073, iter [01000, 05004], lr: 0.017896, loss: 3.9660
2022-10-12 12:39:46 - train: epoch 0073, iter [01100, 05004], lr: 0.017872, loss: 4.2419
2022-10-12 12:40:37 - train: epoch 0073, iter [01200, 05004], lr: 0.017848, loss: 3.8499
2022-10-12 12:41:27 - train: epoch 0073, iter [01300, 05004], lr: 0.017824, loss: 3.8885
2022-10-12 12:42:17 - train: epoch 0073, iter [01400, 05004], lr: 0.017800, loss: 4.3263
2022-10-12 12:43:06 - train: epoch 0073, iter [01500, 05004], lr: 0.017776, loss: 3.4132
2022-10-12 12:43:58 - train: epoch 0073, iter [01600, 05004], lr: 0.017752, loss: 4.3015
2022-10-12 12:44:49 - train: epoch 0073, iter [01700, 05004], lr: 0.017728, loss: 4.3645
2022-10-12 12:45:39 - train: epoch 0073, iter [01800, 05004], lr: 0.017704, loss: 3.5375
2022-10-12 12:46:28 - train: epoch 0073, iter [01900, 05004], lr: 0.017680, loss: 3.6345
2022-10-12 12:47:16 - train: epoch 0073, iter [02000, 05004], lr: 0.017656, loss: 3.7443
2022-10-12 12:48:06 - train: epoch 0073, iter [02100, 05004], lr: 0.017632, loss: 4.1303
2022-10-12 12:48:55 - train: epoch 0073, iter [02200, 05004], lr: 0.017608, loss: 3.9819
2022-10-12 12:49:44 - train: epoch 0073, iter [02300, 05004], lr: 0.017584, loss: 4.1905
2022-10-12 12:50:31 - train: epoch 0073, iter [02400, 05004], lr: 0.017560, loss: 3.5950
2022-10-12 12:51:21 - train: epoch 0073, iter [02500, 05004], lr: 0.017536, loss: 4.4170
2022-10-12 12:52:10 - train: epoch 0073, iter [02600, 05004], lr: 0.017512, loss: 3.4778
2022-10-12 12:53:01 - train: epoch 0073, iter [02700, 05004], lr: 0.017489, loss: 3.7226
2022-10-12 12:53:52 - train: epoch 0073, iter [02800, 05004], lr: 0.017465, loss: 4.0334
2022-10-12 12:54:41 - train: epoch 0073, iter [02900, 05004], lr: 0.017441, loss: 3.7809
2022-10-12 12:55:31 - train: epoch 0073, iter [03000, 05004], lr: 0.017417, loss: 4.2247
2022-10-12 12:56:21 - train: epoch 0073, iter [03100, 05004], lr: 0.017393, loss: 4.1526
2022-10-12 12:57:11 - train: epoch 0073, iter [03200, 05004], lr: 0.017370, loss: 4.2437
2022-10-12 12:58:01 - train: epoch 0073, iter [03300, 05004], lr: 0.017346, loss: 3.7412
2022-10-12 12:58:51 - train: epoch 0073, iter [03400, 05004], lr: 0.017322, loss: 4.0826
2022-10-12 12:59:40 - train: epoch 0073, iter [03500, 05004], lr: 0.017298, loss: 3.6059
2022-10-12 13:00:30 - train: epoch 0073, iter [03600, 05004], lr: 0.017275, loss: 3.0598
2022-10-12 13:01:18 - train: epoch 0073, iter [03700, 05004], lr: 0.017251, loss: 3.9758
2022-10-12 13:02:07 - train: epoch 0073, iter [03800, 05004], lr: 0.017227, loss: 3.3983
2022-10-12 13:02:57 - train: epoch 0073, iter [03900, 05004], lr: 0.017203, loss: 4.3905
2022-10-12 13:03:45 - train: epoch 0073, iter [04000, 05004], lr: 0.017180, loss: 4.3937
2022-10-12 13:04:33 - train: epoch 0073, iter [04100, 05004], lr: 0.017156, loss: 3.6143
2022-10-12 13:05:22 - train: epoch 0073, iter [04200, 05004], lr: 0.017132, loss: 4.4546
2022-10-12 13:06:13 - train: epoch 0073, iter [04300, 05004], lr: 0.017109, loss: 4.2634
2022-10-12 13:07:03 - train: epoch 0073, iter [04400, 05004], lr: 0.017085, loss: 3.8680
2022-10-12 13:07:53 - train: epoch 0073, iter [04500, 05004], lr: 0.017062, loss: 3.4067
2022-10-12 13:08:41 - train: epoch 0073, iter [04600, 05004], lr: 0.017038, loss: 3.9339
2022-10-12 13:09:31 - train: epoch 0073, iter [04700, 05004], lr: 0.017014, loss: 3.8286
2022-10-12 13:10:21 - train: epoch 0073, iter [04800, 05004], lr: 0.016991, loss: 3.8196
2022-10-12 13:11:12 - train: epoch 0073, iter [04900, 05004], lr: 0.016967, loss: 3.2797
2022-10-12 13:11:58 - train: epoch 0073, iter [05000, 05004], lr: 0.016944, loss: 4.0578
2022-10-12 13:12:00 - train: epoch 073, train_loss: 3.8973
2022-10-12 13:13:48 - eval: epoch: 073, acc1: 68.308%, acc5: 88.922%, test_loss: 1.5829, per_image_load_time: 3.593ms, per_image_inference_time: 0.537ms
2022-10-12 13:13:49 - until epoch: 073, best_acc1: 69.062%
2022-10-12 13:13:49 - epoch 074 lr: 0.016943
2022-10-12 13:14:45 - train: epoch 0074, iter [00100, 05004], lr: 0.016919, loss: 4.2489
2022-10-12 13:15:34 - train: epoch 0074, iter [00200, 05004], lr: 0.016896, loss: 3.3902
2022-10-12 13:16:27 - train: epoch 0074, iter [00300, 05004], lr: 0.016872, loss: 3.9243
2022-10-12 13:17:15 - train: epoch 0074, iter [00400, 05004], lr: 0.016849, loss: 3.5745
2022-10-12 13:18:05 - train: epoch 0074, iter [00500, 05004], lr: 0.016825, loss: 4.0032
2022-10-12 13:18:56 - train: epoch 0074, iter [00600, 05004], lr: 0.016802, loss: 3.7322
2022-10-12 13:19:46 - train: epoch 0074, iter [00700, 05004], lr: 0.016778, loss: 3.6035
2022-10-12 13:20:36 - train: epoch 0074, iter [00800, 05004], lr: 0.016755, loss: 4.2608
2022-10-12 13:21:27 - train: epoch 0074, iter [00900, 05004], lr: 0.016731, loss: 4.1046
2022-10-12 13:22:17 - train: epoch 0074, iter [01000, 05004], lr: 0.016708, loss: 3.8643
2022-10-12 13:23:08 - train: epoch 0074, iter [01100, 05004], lr: 0.016685, loss: 4.3502
2022-10-12 13:23:59 - train: epoch 0074, iter [01200, 05004], lr: 0.016661, loss: 3.8604
2022-10-12 13:24:49 - train: epoch 0074, iter [01300, 05004], lr: 0.016638, loss: 4.3584
2022-10-12 13:25:40 - train: epoch 0074, iter [01400, 05004], lr: 0.016614, loss: 3.5209
2022-10-12 13:26:30 - train: epoch 0074, iter [01500, 05004], lr: 0.016591, loss: 3.9586
2022-10-12 13:27:18 - train: epoch 0074, iter [01600, 05004], lr: 0.016568, loss: 3.6005
2022-10-12 13:28:09 - train: epoch 0074, iter [01700, 05004], lr: 0.016544, loss: 4.0782
2022-10-12 13:29:00 - train: epoch 0074, iter [01800, 05004], lr: 0.016521, loss: 3.9524
2022-10-12 13:29:50 - train: epoch 0074, iter [01900, 05004], lr: 0.016498, loss: 4.0914
2022-10-12 13:30:40 - train: epoch 0074, iter [02000, 05004], lr: 0.016474, loss: 3.8579
2022-10-12 13:31:29 - train: epoch 0074, iter [02100, 05004], lr: 0.016451, loss: 3.5539
2022-10-12 13:32:18 - train: epoch 0074, iter [02200, 05004], lr: 0.016428, loss: 4.1738
2022-10-12 13:33:07 - train: epoch 0074, iter [02300, 05004], lr: 0.016405, loss: 3.5593
2022-10-12 13:33:57 - train: epoch 0074, iter [02400, 05004], lr: 0.016381, loss: 4.2505
2022-10-12 13:34:47 - train: epoch 0074, iter [02500, 05004], lr: 0.016358, loss: 4.0892
2022-10-12 13:35:36 - train: epoch 0074, iter [02600, 05004], lr: 0.016335, loss: 4.6097
2022-10-12 13:36:28 - train: epoch 0074, iter [02700, 05004], lr: 0.016312, loss: 2.6663
2022-10-12 13:37:18 - train: epoch 0074, iter [02800, 05004], lr: 0.016289, loss: 3.2495
2022-10-12 13:38:08 - train: epoch 0074, iter [02900, 05004], lr: 0.016265, loss: 3.5304
2022-10-12 13:39:00 - train: epoch 0074, iter [03000, 05004], lr: 0.016242, loss: 3.8538
2022-10-12 13:39:49 - train: epoch 0074, iter [03100, 05004], lr: 0.016219, loss: 4.2092
2022-10-12 13:40:38 - train: epoch 0074, iter [03200, 05004], lr: 0.016196, loss: 4.3282
2022-10-12 13:41:29 - train: epoch 0074, iter [03300, 05004], lr: 0.016173, loss: 4.0950
2022-10-12 13:42:19 - train: epoch 0074, iter [03400, 05004], lr: 0.016150, loss: 4.1713
2022-10-12 13:43:08 - train: epoch 0074, iter [03500, 05004], lr: 0.016127, loss: 3.6321
2022-10-12 13:43:59 - train: epoch 0074, iter [03600, 05004], lr: 0.016104, loss: 4.2196
2022-10-12 13:44:48 - train: epoch 0074, iter [03700, 05004], lr: 0.016081, loss: 3.9665
2022-10-12 13:45:38 - train: epoch 0074, iter [03800, 05004], lr: 0.016058, loss: 4.2012
2022-10-12 13:46:28 - train: epoch 0074, iter [03900, 05004], lr: 0.016034, loss: 3.7945
2022-10-12 13:47:18 - train: epoch 0074, iter [04000, 05004], lr: 0.016011, loss: 3.7453
2022-10-12 13:48:08 - train: epoch 0074, iter [04100, 05004], lr: 0.015988, loss: 4.3429
2022-10-12 13:48:58 - train: epoch 0074, iter [04200, 05004], lr: 0.015965, loss: 3.9679
2022-10-12 13:49:48 - train: epoch 0074, iter [04300, 05004], lr: 0.015942, loss: 3.8174
2022-10-12 13:50:37 - train: epoch 0074, iter [04400, 05004], lr: 0.015920, loss: 4.2132
2022-10-12 13:51:26 - train: epoch 0074, iter [04500, 05004], lr: 0.015897, loss: 3.7032
2022-10-12 13:52:13 - train: epoch 0074, iter [04600, 05004], lr: 0.015874, loss: 3.7964
2022-10-12 13:53:00 - train: epoch 0074, iter [04700, 05004], lr: 0.015851, loss: 4.2359
2022-10-12 13:53:50 - train: epoch 0074, iter [04800, 05004], lr: 0.015828, loss: 3.8301
2022-10-12 13:54:40 - train: epoch 0074, iter [04900, 05004], lr: 0.015805, loss: 3.7841
2022-10-12 13:55:25 - train: epoch 0074, iter [05000, 05004], lr: 0.015782, loss: 4.4060
2022-10-12 13:55:27 - train: epoch 074, train_loss: 3.8793
2022-10-12 13:57:09 - eval: epoch: 074, acc1: 68.942%, acc5: 89.242%, test_loss: 1.5315, per_image_load_time: 3.384ms, per_image_inference_time: 0.522ms
2022-10-12 13:57:09 - until epoch: 074, best_acc1: 69.062%
2022-10-12 13:57:09 - epoch 075 lr: 0.015781
2022-10-12 13:58:03 - train: epoch 0075, iter [00100, 05004], lr: 0.015758, loss: 3.4785
2022-10-12 13:58:51 - train: epoch 0075, iter [00200, 05004], lr: 0.015735, loss: 3.9659
2022-10-12 13:59:39 - train: epoch 0075, iter [00300, 05004], lr: 0.015712, loss: 3.3691
2022-10-12 14:00:28 - train: epoch 0075, iter [00400, 05004], lr: 0.015690, loss: 4.5127
2022-10-12 14:01:17 - train: epoch 0075, iter [00500, 05004], lr: 0.015667, loss: 3.7995
2022-10-12 14:02:05 - train: epoch 0075, iter [00600, 05004], lr: 0.015644, loss: 3.8063
2022-10-12 14:02:52 - train: epoch 0075, iter [00700, 05004], lr: 0.015621, loss: 3.7298
2022-10-12 14:03:41 - train: epoch 0075, iter [00800, 05004], lr: 0.015598, loss: 4.2786
2022-10-12 14:04:28 - train: epoch 0075, iter [00900, 05004], lr: 0.015576, loss: 3.4085
2022-10-12 14:05:15 - train: epoch 0075, iter [01000, 05004], lr: 0.015553, loss: 3.3967
2022-10-12 14:06:05 - train: epoch 0075, iter [01100, 05004], lr: 0.015530, loss: 3.8808
2022-10-12 14:06:54 - train: epoch 0075, iter [01200, 05004], lr: 0.015507, loss: 3.5750
2022-10-12 14:07:43 - train: epoch 0075, iter [01300, 05004], lr: 0.015485, loss: 3.4996
2022-10-12 14:08:32 - train: epoch 0075, iter [01400, 05004], lr: 0.015462, loss: 3.8176
2022-10-12 14:09:20 - train: epoch 0075, iter [01500, 05004], lr: 0.015439, loss: 4.1140
2022-10-12 14:10:09 - train: epoch 0075, iter [01600, 05004], lr: 0.015417, loss: 3.9022
2022-10-12 14:10:57 - train: epoch 0075, iter [01700, 05004], lr: 0.015394, loss: 3.7242
2022-10-12 14:11:46 - train: epoch 0075, iter [01800, 05004], lr: 0.015371, loss: 3.3974
2022-10-12 14:12:33 - train: epoch 0075, iter [01900, 05004], lr: 0.015349, loss: 3.3280
2022-10-12 14:13:21 - train: epoch 0075, iter [02000, 05004], lr: 0.015326, loss: 3.4521
2022-10-12 14:14:10 - train: epoch 0075, iter [02100, 05004], lr: 0.015304, loss: 4.0490
2022-10-12 14:14:58 - train: epoch 0075, iter [02200, 05004], lr: 0.015281, loss: 4.3454
2022-10-12 14:15:45 - train: epoch 0075, iter [02300, 05004], lr: 0.015258, loss: 4.4842
2022-10-12 14:16:35 - train: epoch 0075, iter [02400, 05004], lr: 0.015236, loss: 3.7265
2022-10-12 14:17:22 - train: epoch 0075, iter [02500, 05004], lr: 0.015213, loss: 4.1106
2022-10-12 14:18:10 - train: epoch 0075, iter [02600, 05004], lr: 0.015191, loss: 3.9969
2022-10-12 14:18:59 - train: epoch 0075, iter [02700, 05004], lr: 0.015168, loss: 3.5603
2022-10-12 14:19:49 - train: epoch 0075, iter [02800, 05004], lr: 0.015146, loss: 4.2968
2022-10-12 14:20:37 - train: epoch 0075, iter [02900, 05004], lr: 0.015123, loss: 4.5262
2022-10-12 14:21:26 - train: epoch 0075, iter [03000, 05004], lr: 0.015101, loss: 4.1659
2022-10-12 14:22:16 - train: epoch 0075, iter [03100, 05004], lr: 0.015078, loss: 3.6969
2022-10-12 14:23:05 - train: epoch 0075, iter [03200, 05004], lr: 0.015056, loss: 3.9617
2022-10-12 14:23:54 - train: epoch 0075, iter [03300, 05004], lr: 0.015033, loss: 4.6355
2022-10-12 14:24:42 - train: epoch 0075, iter [03400, 05004], lr: 0.015011, loss: 4.0332
2022-10-12 14:25:31 - train: epoch 0075, iter [03500, 05004], lr: 0.014989, loss: 4.1718
2022-10-12 14:26:18 - train: epoch 0075, iter [03600, 05004], lr: 0.014966, loss: 3.9719
2022-10-12 14:27:06 - train: epoch 0075, iter [03700, 05004], lr: 0.014944, loss: 4.3801
2022-10-12 14:27:54 - train: epoch 0075, iter [03800, 05004], lr: 0.014921, loss: 3.1508
2022-10-12 14:28:45 - train: epoch 0075, iter [03900, 05004], lr: 0.014899, loss: 4.3599
2022-10-12 14:29:35 - train: epoch 0075, iter [04000, 05004], lr: 0.014877, loss: 4.5017
2022-10-12 14:30:24 - train: epoch 0075, iter [04100, 05004], lr: 0.014854, loss: 3.8831
2022-10-12 14:31:14 - train: epoch 0075, iter [04200, 05004], lr: 0.014832, loss: 4.2262
2022-10-12 14:32:04 - train: epoch 0075, iter [04300, 05004], lr: 0.014810, loss: 4.0864
2022-10-12 14:32:55 - train: epoch 0075, iter [04400, 05004], lr: 0.014788, loss: 3.9372
2022-10-12 14:33:45 - train: epoch 0075, iter [04500, 05004], lr: 0.014765, loss: 4.5029
2022-10-12 14:34:37 - train: epoch 0075, iter [04600, 05004], lr: 0.014743, loss: 3.6893
2022-10-12 14:35:27 - train: epoch 0075, iter [04700, 05004], lr: 0.014721, loss: 4.2599
2022-10-12 14:36:18 - train: epoch 0075, iter [04800, 05004], lr: 0.014699, loss: 3.0039
2022-10-12 14:37:06 - train: epoch 0075, iter [04900, 05004], lr: 0.014676, loss: 4.1296
2022-10-12 14:37:52 - train: epoch 0075, iter [05000, 05004], lr: 0.014654, loss: 3.9359
2022-10-12 14:37:54 - train: epoch 075, train_loss: 3.8681
2022-10-12 14:39:44 - eval: epoch: 075, acc1: 70.038%, acc5: 89.948%, test_loss: 1.4708, per_image_load_time: 3.661ms, per_image_inference_time: 0.535ms
2022-10-12 14:39:45 - until epoch: 075, best_acc1: 70.038%
2022-10-12 14:39:45 - epoch 076 lr: 0.014653
2022-10-12 14:40:41 - train: epoch 0076, iter [00100, 05004], lr: 0.014631, loss: 4.0219
2022-10-12 14:41:30 - train: epoch 0076, iter [00200, 05004], lr: 0.014609, loss: 3.5406
2022-10-12 14:42:20 - train: epoch 0076, iter [00300, 05004], lr: 0.014587, loss: 3.9155
2022-10-12 14:43:07 - train: epoch 0076, iter [00400, 05004], lr: 0.014565, loss: 4.1268
2022-10-12 14:44:00 - train: epoch 0076, iter [00500, 05004], lr: 0.014542, loss: 3.4433
2022-10-12 14:44:50 - train: epoch 0076, iter [00600, 05004], lr: 0.014520, loss: 4.3048
2022-10-12 14:45:40 - train: epoch 0076, iter [00700, 05004], lr: 0.014498, loss: 3.6674
2022-10-12 14:46:32 - train: epoch 0076, iter [00800, 05004], lr: 0.014476, loss: 3.4260
2022-10-12 14:47:23 - train: epoch 0076, iter [00900, 05004], lr: 0.014454, loss: 2.9802
2022-10-12 14:48:13 - train: epoch 0076, iter [01000, 05004], lr: 0.014432, loss: 4.0305
2022-10-12 14:49:03 - train: epoch 0076, iter [01100, 05004], lr: 0.014410, loss: 4.3335
2022-10-12 14:49:53 - train: epoch 0076, iter [01200, 05004], lr: 0.014388, loss: 2.8649
2022-10-12 14:50:43 - train: epoch 0076, iter [01300, 05004], lr: 0.014366, loss: 4.3996
2022-10-12 14:51:32 - train: epoch 0076, iter [01400, 05004], lr: 0.014344, loss: 3.8409
2022-10-12 14:52:24 - train: epoch 0076, iter [01500, 05004], lr: 0.014322, loss: 4.3619
2022-10-12 14:53:12 - train: epoch 0076, iter [01600, 05004], lr: 0.014300, loss: 4.1003
2022-10-12 14:54:03 - train: epoch 0076, iter [01700, 05004], lr: 0.014278, loss: 3.1135
2022-10-12 14:54:52 - train: epoch 0076, iter [01800, 05004], lr: 0.014256, loss: 3.6108
2022-10-12 14:55:39 - train: epoch 0076, iter [01900, 05004], lr: 0.014234, loss: 4.2255
2022-10-12 14:56:29 - train: epoch 0076, iter [02000, 05004], lr: 0.014212, loss: 4.0575
2022-10-12 14:57:20 - train: epoch 0076, iter [02100, 05004], lr: 0.014190, loss: 3.7585
2022-10-12 14:58:10 - train: epoch 0076, iter [02200, 05004], lr: 0.014168, loss: 3.8846
2022-10-12 14:59:01 - train: epoch 0076, iter [02300, 05004], lr: 0.014146, loss: 4.4186
2022-10-12 14:59:54 - train: epoch 0076, iter [02400, 05004], lr: 0.014125, loss: 4.0550
2022-10-12 15:00:44 - train: epoch 0076, iter [02500, 05004], lr: 0.014103, loss: 3.9316
2022-10-12 15:01:34 - train: epoch 0076, iter [02600, 05004], lr: 0.014081, loss: 4.3932
2022-10-12 15:02:25 - train: epoch 0076, iter [02700, 05004], lr: 0.014059, loss: 3.5681
2022-10-12 15:03:15 - train: epoch 0076, iter [02800, 05004], lr: 0.014037, loss: 4.3715
2022-10-12 15:04:06 - train: epoch 0076, iter [02900, 05004], lr: 0.014015, loss: 4.2738
2022-10-12 15:04:53 - train: epoch 0076, iter [03000, 05004], lr: 0.013994, loss: 3.4822
2022-10-12 15:05:45 - train: epoch 0076, iter [03100, 05004], lr: 0.013972, loss: 2.8360
2022-10-12 15:06:34 - train: epoch 0076, iter [03200, 05004], lr: 0.013950, loss: 4.1493
2022-10-12 15:07:26 - train: epoch 0076, iter [03300, 05004], lr: 0.013928, loss: 3.9572
2022-10-12 15:08:15 - train: epoch 0076, iter [03400, 05004], lr: 0.013907, loss: 4.3690
2022-10-12 15:09:05 - train: epoch 0076, iter [03500, 05004], lr: 0.013885, loss: 3.9273
2022-10-12 15:09:55 - train: epoch 0076, iter [03600, 05004], lr: 0.013863, loss: 2.6301
2022-10-12 15:10:47 - train: epoch 0076, iter [03700, 05004], lr: 0.013842, loss: 3.4737
2022-10-12 15:11:38 - train: epoch 0076, iter [03800, 05004], lr: 0.013820, loss: 4.1360
2022-10-12 15:12:28 - train: epoch 0076, iter [03900, 05004], lr: 0.013798, loss: 4.5211
2022-10-12 15:13:18 - train: epoch 0076, iter [04000, 05004], lr: 0.013777, loss: 4.1031
2022-10-12 15:14:08 - train: epoch 0076, iter [04100, 05004], lr: 0.013755, loss: 3.5497
2022-10-12 15:14:59 - train: epoch 0076, iter [04200, 05004], lr: 0.013733, loss: 4.0348
2022-10-12 15:15:49 - train: epoch 0076, iter [04300, 05004], lr: 0.013712, loss: 3.6234
2022-10-12 15:16:40 - train: epoch 0076, iter [04400, 05004], lr: 0.013690, loss: 3.4506
2022-10-12 15:17:30 - train: epoch 0076, iter [04500, 05004], lr: 0.013669, loss: 4.6875
2022-10-12 15:18:19 - train: epoch 0076, iter [04600, 05004], lr: 0.013647, loss: 3.9400
2022-10-12 15:19:09 - train: epoch 0076, iter [04700, 05004], lr: 0.013626, loss: 3.3834
2022-10-12 15:19:58 - train: epoch 0076, iter [04800, 05004], lr: 0.013604, loss: 4.3652
2022-10-12 15:20:51 - train: epoch 0076, iter [04900, 05004], lr: 0.013583, loss: 3.4234
2022-10-12 15:21:37 - train: epoch 0076, iter [05000, 05004], lr: 0.013561, loss: 4.1778
2022-10-12 15:21:40 - train: epoch 076, train_loss: 3.8488
2022-10-12 15:23:28 - eval: epoch: 076, acc1: 69.568%, acc5: 89.866%, test_loss: 1.4432, per_image_load_time: 3.638ms, per_image_inference_time: 0.515ms
2022-10-12 15:23:29 - until epoch: 076, best_acc1: 70.038%
2022-10-12 15:23:29 - epoch 077 lr: 0.013560
2022-10-12 15:24:25 - train: epoch 0077, iter [00100, 05004], lr: 0.013539, loss: 3.7683
2022-10-12 15:25:15 - train: epoch 0077, iter [00200, 05004], lr: 0.013517, loss: 4.0447
2022-10-12 15:26:05 - train: epoch 0077, iter [00300, 05004], lr: 0.013496, loss: 3.7327
2022-10-12 15:26:55 - train: epoch 0077, iter [00400, 05004], lr: 0.013474, loss: 4.2413
2022-10-12 15:27:47 - train: epoch 0077, iter [00500, 05004], lr: 0.013453, loss: 4.1380
2022-10-12 15:28:37 - train: epoch 0077, iter [00600, 05004], lr: 0.013432, loss: 3.5442
2022-10-12 15:29:29 - train: epoch 0077, iter [00700, 05004], lr: 0.013410, loss: 4.3659
2022-10-12 15:30:18 - train: epoch 0077, iter [00800, 05004], lr: 0.013389, loss: 4.1517
2022-10-12 15:31:09 - train: epoch 0077, iter [00900, 05004], lr: 0.013367, loss: 3.8548
2022-10-12 15:31:58 - train: epoch 0077, iter [01000, 05004], lr: 0.013346, loss: 3.9095
2022-10-12 15:32:47 - train: epoch 0077, iter [01100, 05004], lr: 0.013325, loss: 4.1912
2022-10-12 15:33:37 - train: epoch 0077, iter [01200, 05004], lr: 0.013303, loss: 3.7644
2022-10-12 15:34:27 - train: epoch 0077, iter [01300, 05004], lr: 0.013282, loss: 4.1252
2022-10-12 15:35:18 - train: epoch 0077, iter [01400, 05004], lr: 0.013261, loss: 3.5523
2022-10-12 15:36:08 - train: epoch 0077, iter [01500, 05004], lr: 0.013240, loss: 3.8095
2022-10-12 15:36:59 - train: epoch 0077, iter [01600, 05004], lr: 0.013218, loss: 3.8788
2022-10-12 15:37:49 - train: epoch 0077, iter [01700, 05004], lr: 0.013197, loss: 3.9979
2022-10-12 15:38:38 - train: epoch 0077, iter [01800, 05004], lr: 0.013176, loss: 3.8751
2022-10-12 15:39:29 - train: epoch 0077, iter [01900, 05004], lr: 0.013155, loss: 3.3973
2022-10-12 15:40:20 - train: epoch 0077, iter [02000, 05004], lr: 0.013133, loss: 3.5387
2022-10-12 15:41:10 - train: epoch 0077, iter [02100, 05004], lr: 0.013112, loss: 4.4282
2022-10-12 15:42:00 - train: epoch 0077, iter [02200, 05004], lr: 0.013091, loss: 4.4394
2022-10-12 15:42:51 - train: epoch 0077, iter [02300, 05004], lr: 0.013070, loss: 4.2839
2022-10-12 15:43:43 - train: epoch 0077, iter [02400, 05004], lr: 0.013049, loss: 3.5295
2022-10-12 15:44:33 - train: epoch 0077, iter [02500, 05004], lr: 0.013028, loss: 3.7385
2022-10-12 15:45:22 - train: epoch 0077, iter [02600, 05004], lr: 0.013006, loss: 3.2945
2022-10-12 15:46:14 - train: epoch 0077, iter [02700, 05004], lr: 0.012985, loss: 4.1916
2022-10-12 15:47:03 - train: epoch 0077, iter [02800, 05004], lr: 0.012964, loss: 4.1305
2022-10-12 15:47:50 - train: epoch 0077, iter [02900, 05004], lr: 0.012943, loss: 4.2235
2022-10-12 15:48:41 - train: epoch 0077, iter [03000, 05004], lr: 0.012922, loss: 4.6544
2022-10-12 15:49:33 - train: epoch 0077, iter [03100, 05004], lr: 0.012901, loss: 4.0555
2022-10-12 15:50:23 - train: epoch 0077, iter [03200, 05004], lr: 0.012880, loss: 4.2697
2022-10-12 15:51:14 - train: epoch 0077, iter [03300, 05004], lr: 0.012859, loss: 3.9450
2022-10-12 15:52:03 - train: epoch 0077, iter [03400, 05004], lr: 0.012838, loss: 4.4608
2022-10-12 15:52:52 - train: epoch 0077, iter [03500, 05004], lr: 0.012817, loss: 3.9725
2022-10-12 15:53:42 - train: epoch 0077, iter [03600, 05004], lr: 0.012796, loss: 4.3530
2022-10-12 15:54:32 - train: epoch 0077, iter [03700, 05004], lr: 0.012775, loss: 3.7702
2022-10-12 15:55:23 - train: epoch 0077, iter [03800, 05004], lr: 0.012754, loss: 4.1851
2022-10-12 15:56:12 - train: epoch 0077, iter [03900, 05004], lr: 0.012733, loss: 4.2705
2022-10-12 15:57:03 - train: epoch 0077, iter [04000, 05004], lr: 0.012712, loss: 4.4787
2022-10-12 15:57:54 - train: epoch 0077, iter [04100, 05004], lr: 0.012691, loss: 3.9907
2022-10-12 15:58:43 - train: epoch 0077, iter [04200, 05004], lr: 0.012671, loss: 4.3713
2022-10-12 15:59:32 - train: epoch 0077, iter [04300, 05004], lr: 0.012650, loss: 3.2444
2022-10-12 16:00:20 - train: epoch 0077, iter [04400, 05004], lr: 0.012629, loss: 4.3041
2022-10-12 16:01:11 - train: epoch 0077, iter [04500, 05004], lr: 0.012608, loss: 3.3914
2022-10-12 16:02:03 - train: epoch 0077, iter [04600, 05004], lr: 0.012587, loss: 4.2476
2022-10-12 16:02:53 - train: epoch 0077, iter [04700, 05004], lr: 0.012566, loss: 4.5709
2022-10-12 16:03:43 - train: epoch 0077, iter [04800, 05004], lr: 0.012546, loss: 4.3506
2022-10-12 16:04:34 - train: epoch 0077, iter [04900, 05004], lr: 0.012525, loss: 3.5708
2022-10-12 16:05:21 - train: epoch 0077, iter [05000, 05004], lr: 0.012504, loss: 4.0754
2022-10-12 16:05:24 - train: epoch 077, train_loss: 3.8252
2022-10-12 16:07:10 - eval: epoch: 077, acc1: 70.340%, acc5: 90.016%, test_loss: 1.5049, per_image_load_time: 3.504ms, per_image_inference_time: 0.531ms
2022-10-12 16:07:10 - until epoch: 077, best_acc1: 70.340%
2022-10-12 16:07:10 - epoch 078 lr: 0.012503
2022-10-12 16:08:07 - train: epoch 0078, iter [00100, 05004], lr: 0.012482, loss: 4.5090
2022-10-12 16:08:57 - train: epoch 0078, iter [00200, 05004], lr: 0.012462, loss: 4.0624
2022-10-12 16:09:48 - train: epoch 0078, iter [00300, 05004], lr: 0.012441, loss: 3.8836
2022-10-12 16:10:37 - train: epoch 0078, iter [00400, 05004], lr: 0.012420, loss: 3.6982
2022-10-12 16:11:28 - train: epoch 0078, iter [00500, 05004], lr: 0.012400, loss: 3.3965
2022-10-12 16:12:18 - train: epoch 0078, iter [00600, 05004], lr: 0.012379, loss: 2.9501
2022-10-12 16:13:07 - train: epoch 0078, iter [00700, 05004], lr: 0.012358, loss: 3.6081
2022-10-12 16:13:55 - train: epoch 0078, iter [00800, 05004], lr: 0.012338, loss: 3.8107
2022-10-12 16:14:46 - train: epoch 0078, iter [00900, 05004], lr: 0.012317, loss: 3.4743
2022-10-12 16:15:35 - train: epoch 0078, iter [01000, 05004], lr: 0.012296, loss: 3.9094
2022-10-12 16:16:25 - train: epoch 0078, iter [01100, 05004], lr: 0.012276, loss: 3.7775
2022-10-12 16:17:15 - train: epoch 0078, iter [01200, 05004], lr: 0.012255, loss: 3.9554
2022-10-12 16:18:06 - train: epoch 0078, iter [01300, 05004], lr: 0.012235, loss: 3.9973
2022-10-12 16:18:56 - train: epoch 0078, iter [01400, 05004], lr: 0.012214, loss: 3.7062
2022-10-12 16:19:44 - train: epoch 0078, iter [01500, 05004], lr: 0.012194, loss: 3.7979
2022-10-12 16:20:33 - train: epoch 0078, iter [01600, 05004], lr: 0.012173, loss: 4.5170
2022-10-12 16:21:22 - train: epoch 0078, iter [01700, 05004], lr: 0.012152, loss: 4.1869
2022-10-12 16:22:13 - train: epoch 0078, iter [01800, 05004], lr: 0.012132, loss: 3.5727
2022-10-12 16:23:03 - train: epoch 0078, iter [01900, 05004], lr: 0.012111, loss: 3.7647
2022-10-12 16:23:53 - train: epoch 0078, iter [02000, 05004], lr: 0.012091, loss: 3.6545
2022-10-12 16:24:42 - train: epoch 0078, iter [02100, 05004], lr: 0.012071, loss: 3.4860
2022-10-12 16:25:32 - train: epoch 0078, iter [02200, 05004], lr: 0.012050, loss: 3.6410
2022-10-12 16:26:19 - train: epoch 0078, iter [02300, 05004], lr: 0.012030, loss: 3.7578
2022-10-12 16:27:07 - train: epoch 0078, iter [02400, 05004], lr: 0.012009, loss: 2.9404
2022-10-12 16:27:58 - train: epoch 0078, iter [02500, 05004], lr: 0.011989, loss: 4.0464
2022-10-12 16:28:49 - train: epoch 0078, iter [02600, 05004], lr: 0.011969, loss: 3.7840
2022-10-12 16:29:39 - train: epoch 0078, iter [02700, 05004], lr: 0.011948, loss: 4.0297
2022-10-12 16:30:29 - train: epoch 0078, iter [02800, 05004], lr: 0.011928, loss: 3.8616
2022-10-12 16:31:19 - train: epoch 0078, iter [02900, 05004], lr: 0.011907, loss: 4.0772
2022-10-12 16:32:07 - train: epoch 0078, iter [03000, 05004], lr: 0.011887, loss: 4.5011
2022-10-12 16:32:57 - train: epoch 0078, iter [03100, 05004], lr: 0.011867, loss: 4.0858
2022-10-12 16:33:47 - train: epoch 0078, iter [03200, 05004], lr: 0.011847, loss: 4.1867
2022-10-12 16:34:36 - train: epoch 0078, iter [03300, 05004], lr: 0.011826, loss: 4.1475
2022-10-12 16:35:26 - train: epoch 0078, iter [03400, 05004], lr: 0.011806, loss: 3.9837
2022-10-12 16:36:15 - train: epoch 0078, iter [03500, 05004], lr: 0.011786, loss: 4.0272
2022-10-12 16:37:06 - train: epoch 0078, iter [03600, 05004], lr: 0.011766, loss: 4.5278
2022-10-12 16:37:56 - train: epoch 0078, iter [03700, 05004], lr: 0.011745, loss: 3.2835
2022-10-12 16:38:43 - train: epoch 0078, iter [03800, 05004], lr: 0.011725, loss: 3.5696
2022-10-12 16:39:32 - train: epoch 0078, iter [03900, 05004], lr: 0.011705, loss: 4.0570
2022-10-12 16:40:21 - train: epoch 0078, iter [04000, 05004], lr: 0.011685, loss: 3.6060
2022-10-12 16:41:10 - train: epoch 0078, iter [04100, 05004], lr: 0.011665, loss: 3.9365
2022-10-12 16:42:00 - train: epoch 0078, iter [04200, 05004], lr: 0.011645, loss: 4.7397
2022-10-12 16:42:47 - train: epoch 0078, iter [04300, 05004], lr: 0.011624, loss: 4.1951
2022-10-12 16:43:37 - train: epoch 0078, iter [04400, 05004], lr: 0.011604, loss: 4.2265
2022-10-12 16:44:26 - train: epoch 0078, iter [04500, 05004], lr: 0.011584, loss: 3.7210
2022-10-12 16:45:15 - train: epoch 0078, iter [04600, 05004], lr: 0.011564, loss: 4.3557
2022-10-12 16:46:05 - train: epoch 0078, iter [04700, 05004], lr: 0.011544, loss: 3.4258
2022-10-12 16:46:54 - train: epoch 0078, iter [04800, 05004], lr: 0.011524, loss: 4.4090
2022-10-12 16:47:43 - train: epoch 0078, iter [04900, 05004], lr: 0.011504, loss: 4.2571
2022-10-12 16:48:30 - train: epoch 0078, iter [05000, 05004], lr: 0.011484, loss: 3.4212
2022-10-12 16:48:32 - train: epoch 078, train_loss: 3.7988
2022-10-12 16:50:19 - eval: epoch: 078, acc1: 70.688%, acc5: 90.394%, test_loss: 1.4618, per_image_load_time: 3.593ms, per_image_inference_time: 0.482ms
2022-10-12 16:50:19 - until epoch: 078, best_acc1: 70.688%
2022-10-12 16:50:19 - epoch 079 lr: 0.011483
2022-10-12 16:51:16 - train: epoch 0079, iter [00100, 05004], lr: 0.011463, loss: 3.2859
2022-10-12 16:52:05 - train: epoch 0079, iter [00200, 05004], lr: 0.011443, loss: 4.5992
2022-10-12 16:52:54 - train: epoch 0079, iter [00300, 05004], lr: 0.011423, loss: 4.1550
2022-10-12 16:53:44 - train: epoch 0079, iter [00400, 05004], lr: 0.011403, loss: 3.8701
2022-10-12 16:54:34 - train: epoch 0079, iter [00500, 05004], lr: 0.011383, loss: 3.4117
2022-10-12 16:55:23 - train: epoch 0079, iter [00600, 05004], lr: 0.011363, loss: 3.6394
2022-10-12 16:56:15 - train: epoch 0079, iter [00700, 05004], lr: 0.011344, loss: 3.7975
2022-10-12 16:57:03 - train: epoch 0079, iter [00800, 05004], lr: 0.011324, loss: 4.7189
2022-10-12 16:57:53 - train: epoch 0079, iter [00900, 05004], lr: 0.011304, loss: 4.4363
2022-10-12 16:58:43 - train: epoch 0079, iter [01000, 05004], lr: 0.011284, loss: 3.8657
2022-10-12 16:59:33 - train: epoch 0079, iter [01100, 05004], lr: 0.011264, loss: 4.1429
2022-10-12 17:00:23 - train: epoch 0079, iter [01200, 05004], lr: 0.011244, loss: 4.0458
2022-10-12 17:01:12 - train: epoch 0079, iter [01300, 05004], lr: 0.011224, loss: 2.9403
2022-10-12 17:02:02 - train: epoch 0079, iter [01400, 05004], lr: 0.011205, loss: 4.4730
2022-10-12 17:02:52 - train: epoch 0079, iter [01500, 05004], lr: 0.011185, loss: 3.4097
2022-10-12 17:03:40 - train: epoch 0079, iter [01600, 05004], lr: 0.011165, loss: 3.0289
2022-10-12 17:04:32 - train: epoch 0079, iter [01700, 05004], lr: 0.011145, loss: 3.8495
2022-10-12 17:05:22 - train: epoch 0079, iter [01800, 05004], lr: 0.011126, loss: 4.1262
2022-10-12 17:06:11 - train: epoch 0079, iter [01900, 05004], lr: 0.011106, loss: 4.1163
2022-10-12 17:06:59 - train: epoch 0079, iter [02000, 05004], lr: 0.011086, loss: 4.1015
2022-10-12 17:07:49 - train: epoch 0079, iter [02100, 05004], lr: 0.011066, loss: 3.5376
2022-10-12 17:08:40 - train: epoch 0079, iter [02200, 05004], lr: 0.011047, loss: 3.9352
2022-10-12 17:09:31 - train: epoch 0079, iter [02300, 05004], lr: 0.011027, loss: 3.8289
2022-10-12 17:10:19 - train: epoch 0079, iter [02400, 05004], lr: 0.011007, loss: 3.6789
2022-10-12 17:11:10 - train: epoch 0079, iter [02500, 05004], lr: 0.010988, loss: 3.7896
2022-10-12 17:11:58 - train: epoch 0079, iter [02600, 05004], lr: 0.010968, loss: 3.2117
2022-10-12 17:12:47 - train: epoch 0079, iter [02700, 05004], lr: 0.010949, loss: 3.8014
2022-10-12 17:13:34 - train: epoch 0079, iter [02800, 05004], lr: 0.010929, loss: 3.6963
2022-10-12 17:14:24 - train: epoch 0079, iter [02900, 05004], lr: 0.010909, loss: 3.8236
2022-10-12 17:15:15 - train: epoch 0079, iter [03000, 05004], lr: 0.010890, loss: 3.3871
2022-10-12 17:16:04 - train: epoch 0079, iter [03100, 05004], lr: 0.010870, loss: 3.7174
2022-10-12 17:16:53 - train: epoch 0079, iter [03200, 05004], lr: 0.010851, loss: 4.6733
2022-10-12 17:17:43 - train: epoch 0079, iter [03300, 05004], lr: 0.010831, loss: 3.9450
2022-10-12 17:18:33 - train: epoch 0079, iter [03400, 05004], lr: 0.010812, loss: 3.6619
2022-10-12 17:19:24 - train: epoch 0079, iter [03500, 05004], lr: 0.010792, loss: 3.1673
2022-10-12 17:20:13 - train: epoch 0079, iter [03600, 05004], lr: 0.010773, loss: 3.0005
2022-10-12 17:21:01 - train: epoch 0079, iter [03700, 05004], lr: 0.010753, loss: 3.2260
2022-10-12 17:21:49 - train: epoch 0079, iter [03800, 05004], lr: 0.010734, loss: 3.7093
2022-10-12 17:22:39 - train: epoch 0079, iter [03900, 05004], lr: 0.010715, loss: 3.3962
2022-10-12 17:23:27 - train: epoch 0079, iter [04000, 05004], lr: 0.010695, loss: 3.3384
2022-10-12 17:24:17 - train: epoch 0079, iter [04100, 05004], lr: 0.010676, loss: 4.4124
2022-10-12 17:25:07 - train: epoch 0079, iter [04200, 05004], lr: 0.010656, loss: 3.4903
2022-10-12 17:25:55 - train: epoch 0079, iter [04300, 05004], lr: 0.010637, loss: 3.5364
2022-10-12 17:26:43 - train: epoch 0079, iter [04400, 05004], lr: 0.010618, loss: 3.8204
2022-10-12 17:27:33 - train: epoch 0079, iter [04500, 05004], lr: 0.010598, loss: 3.5034
2022-10-12 17:28:23 - train: epoch 0079, iter [04600, 05004], lr: 0.010579, loss: 3.4631
2022-10-12 17:29:13 - train: epoch 0079, iter [04700, 05004], lr: 0.010560, loss: 3.8932
2022-10-12 17:30:00 - train: epoch 0079, iter [04800, 05004], lr: 0.010540, loss: 3.8099
2022-10-12 17:30:49 - train: epoch 0079, iter [04900, 05004], lr: 0.010521, loss: 3.6910
2022-10-12 17:31:36 - train: epoch 0079, iter [05000, 05004], lr: 0.010502, loss: 4.0342
2022-10-12 17:31:39 - train: epoch 079, train_loss: 3.7830
2022-10-12 17:33:25 - eval: epoch: 079, acc1: 71.354%, acc5: 90.700%, test_loss: 1.4137, per_image_load_time: 3.539ms, per_image_inference_time: 0.514ms
2022-10-12 17:33:26 - until epoch: 079, best_acc1: 71.354%
2022-10-12 17:33:26 - epoch 080 lr: 0.010501
2022-10-12 17:34:21 - train: epoch 0080, iter [00100, 05004], lr: 0.010482, loss: 3.9955
2022-10-12 17:35:12 - train: epoch 0080, iter [00200, 05004], lr: 0.010463, loss: 4.0572
2022-10-12 17:36:02 - train: epoch 0080, iter [00300, 05004], lr: 0.010444, loss: 3.3836
2022-10-12 17:36:51 - train: epoch 0080, iter [00400, 05004], lr: 0.010424, loss: 4.5028
2022-10-12 17:37:39 - train: epoch 0080, iter [00500, 05004], lr: 0.010405, loss: 4.1204
2022-10-12 17:38:29 - train: epoch 0080, iter [00600, 05004], lr: 0.010386, loss: 3.6629
2022-10-12 17:39:19 - train: epoch 0080, iter [00700, 05004], lr: 0.010367, loss: 3.4141
2022-10-12 17:40:09 - train: epoch 0080, iter [00800, 05004], lr: 0.010348, loss: 4.0651
2022-10-12 17:40:57 - train: epoch 0080, iter [00900, 05004], lr: 0.010329, loss: 3.8581
2022-10-12 17:41:47 - train: epoch 0080, iter [01000, 05004], lr: 0.010310, loss: 4.4269
2022-10-12 17:42:35 - train: epoch 0080, iter [01100, 05004], lr: 0.010291, loss: 4.2237
2022-10-12 17:43:25 - train: epoch 0080, iter [01200, 05004], lr: 0.010271, loss: 3.2312
2022-10-12 17:44:15 - train: epoch 0080, iter [01300, 05004], lr: 0.010252, loss: 4.1348
2022-10-12 17:45:04 - train: epoch 0080, iter [01400, 05004], lr: 0.010233, loss: 3.1324
2022-10-12 17:45:52 - train: epoch 0080, iter [01500, 05004], lr: 0.010214, loss: 3.4988
2022-10-12 17:46:42 - train: epoch 0080, iter [01600, 05004], lr: 0.010195, loss: 3.4349
2022-10-12 17:47:30 - train: epoch 0080, iter [01700, 05004], lr: 0.010176, loss: 4.4813
2022-10-12 17:48:20 - train: epoch 0080, iter [01800, 05004], lr: 0.010157, loss: 4.1387
2022-10-12 17:49:09 - train: epoch 0080, iter [01900, 05004], lr: 0.010139, loss: 3.8679
2022-10-12 17:49:59 - train: epoch 0080, iter [02000, 05004], lr: 0.010120, loss: 3.8017
2022-10-12 17:50:49 - train: epoch 0080, iter [02100, 05004], lr: 0.010101, loss: 4.1849
2022-10-12 17:51:38 - train: epoch 0080, iter [02200, 05004], lr: 0.010082, loss: 3.0937
2022-10-12 17:52:28 - train: epoch 0080, iter [02300, 05004], lr: 0.010063, loss: 3.3852
2022-10-12 17:53:17 - train: epoch 0080, iter [02400, 05004], lr: 0.010044, loss: 3.9660
2022-10-12 17:54:06 - train: epoch 0080, iter [02500, 05004], lr: 0.010025, loss: 3.9618
2022-10-12 17:54:56 - train: epoch 0080, iter [02600, 05004], lr: 0.010006, loss: 4.2519
2022-10-12 17:55:44 - train: epoch 0080, iter [02700, 05004], lr: 0.009987, loss: 3.0265
2022-10-12 17:56:34 - train: epoch 0080, iter [02800, 05004], lr: 0.009969, loss: 3.5996
2022-10-12 17:57:23 - train: epoch 0080, iter [02900, 05004], lr: 0.009950, loss: 3.7535
2022-10-12 17:58:13 - train: epoch 0080, iter [03000, 05004], lr: 0.009931, loss: 3.6497
2022-10-12 17:59:01 - train: epoch 0080, iter [03100, 05004], lr: 0.009912, loss: 3.6138
2022-10-12 17:59:51 - train: epoch 0080, iter [03200, 05004], lr: 0.009894, loss: 3.7321
2022-10-12 18:00:39 - train: epoch 0080, iter [03300, 05004], lr: 0.009875, loss: 3.8093
2022-10-12 18:01:28 - train: epoch 0080, iter [03400, 05004], lr: 0.009856, loss: 3.9080
2022-10-12 18:02:16 - train: epoch 0080, iter [03500, 05004], lr: 0.009837, loss: 4.1815
2022-10-12 18:03:05 - train: epoch 0080, iter [03600, 05004], lr: 0.009819, loss: 4.3462
2022-10-12 18:03:55 - train: epoch 0080, iter [03700, 05004], lr: 0.009800, loss: 3.6772
2022-10-12 18:04:43 - train: epoch 0080, iter [03800, 05004], lr: 0.009781, loss: 4.5022
2022-10-12 18:05:33 - train: epoch 0080, iter [03900, 05004], lr: 0.009763, loss: 4.1351
2022-10-12 18:06:21 - train: epoch 0080, iter [04000, 05004], lr: 0.009744, loss: 3.7960
2022-10-12 18:07:11 - train: epoch 0080, iter [04100, 05004], lr: 0.009726, loss: 3.3042
2022-10-12 18:07:58 - train: epoch 0080, iter [04200, 05004], lr: 0.009707, loss: 4.0219
2022-10-12 18:08:47 - train: epoch 0080, iter [04300, 05004], lr: 0.009688, loss: 4.4225
2022-10-12 18:09:37 - train: epoch 0080, iter [04400, 05004], lr: 0.009670, loss: 3.2885
2022-10-12 18:10:25 - train: epoch 0080, iter [04500, 05004], lr: 0.009651, loss: 3.9714
2022-10-12 18:11:14 - train: epoch 0080, iter [04600, 05004], lr: 0.009633, loss: 3.9707
2022-10-12 18:12:03 - train: epoch 0080, iter [04700, 05004], lr: 0.009614, loss: 3.9704
2022-10-12 18:12:51 - train: epoch 0080, iter [04800, 05004], lr: 0.009596, loss: 3.3294
2022-10-12 18:13:39 - train: epoch 0080, iter [04900, 05004], lr: 0.009577, loss: 4.0290
2022-10-12 18:14:27 - train: epoch 0080, iter [05000, 05004], lr: 0.009559, loss: 3.3020
2022-10-12 18:14:29 - train: epoch 080, train_loss: 3.7675
2022-10-12 18:16:12 - eval: epoch: 080, acc1: 71.708%, acc5: 90.822%, test_loss: 1.4036, per_image_load_time: 3.457ms, per_image_inference_time: 0.501ms
2022-10-12 18:16:13 - until epoch: 080, best_acc1: 71.708%
2022-10-12 18:16:13 - epoch 081 lr: 0.009558
2022-10-12 18:17:07 - train: epoch 0081, iter [00100, 05004], lr: 0.009540, loss: 3.2392
2022-10-12 18:17:58 - train: epoch 0081, iter [00200, 05004], lr: 0.009521, loss: 3.8582
2022-10-12 18:18:46 - train: epoch 0081, iter [00300, 05004], lr: 0.009503, loss: 2.9268
2022-10-12 18:19:36 - train: epoch 0081, iter [00400, 05004], lr: 0.009485, loss: 3.5894
2022-10-12 18:20:25 - train: epoch 0081, iter [00500, 05004], lr: 0.009466, loss: 3.5900
2022-10-12 18:21:15 - train: epoch 0081, iter [00600, 05004], lr: 0.009448, loss: 4.3684
2022-10-12 18:22:04 - train: epoch 0081, iter [00700, 05004], lr: 0.009429, loss: 4.2691
2022-10-12 18:22:55 - train: epoch 0081, iter [00800, 05004], lr: 0.009411, loss: 3.1784
2022-10-12 18:23:45 - train: epoch 0081, iter [00900, 05004], lr: 0.009393, loss: 4.0278
2022-10-12 18:24:34 - train: epoch 0081, iter [01000, 05004], lr: 0.009375, loss: 3.7308
2022-10-12 18:25:22 - train: epoch 0081, iter [01100, 05004], lr: 0.009356, loss: 4.1862
2022-10-12 18:26:10 - train: epoch 0081, iter [01200, 05004], lr: 0.009338, loss: 3.9095
2022-10-12 18:27:00 - train: epoch 0081, iter [01300, 05004], lr: 0.009320, loss: 3.9175
2022-10-12 18:27:50 - train: epoch 0081, iter [01400, 05004], lr: 0.009301, loss: 3.2104
2022-10-12 18:28:39 - train: epoch 0081, iter [01500, 05004], lr: 0.009283, loss: 2.9231
2022-10-12 18:29:27 - train: epoch 0081, iter [01600, 05004], lr: 0.009265, loss: 3.2899
2022-10-12 18:30:13 - train: epoch 0081, iter [01700, 05004], lr: 0.009247, loss: 3.9532
2022-10-12 18:31:03 - train: epoch 0081, iter [01800, 05004], lr: 0.009229, loss: 3.6383
2022-10-12 18:31:51 - train: epoch 0081, iter [01900, 05004], lr: 0.009211, loss: 3.6687
2022-10-12 18:32:42 - train: epoch 0081, iter [02000, 05004], lr: 0.009192, loss: 4.1186
2022-10-12 18:33:32 - train: epoch 0081, iter [02100, 05004], lr: 0.009174, loss: 3.4038
2022-10-12 18:34:20 - train: epoch 0081, iter [02200, 05004], lr: 0.009156, loss: 3.7542
2022-10-12 18:35:07 - train: epoch 0081, iter [02300, 05004], lr: 0.009138, loss: 3.1467
2022-10-12 18:35:57 - train: epoch 0081, iter [02400, 05004], lr: 0.009120, loss: 4.0288
2022-10-12 18:36:45 - train: epoch 0081, iter [02500, 05004], lr: 0.009102, loss: 4.1862
2022-10-12 18:37:35 - train: epoch 0081, iter [02600, 05004], lr: 0.009084, loss: 3.9955
2022-10-12 18:38:24 - train: epoch 0081, iter [02700, 05004], lr: 0.009066, loss: 3.7612
2022-10-12 18:39:13 - train: epoch 0081, iter [02800, 05004], lr: 0.009048, loss: 4.2742
2022-10-12 18:40:01 - train: epoch 0081, iter [02900, 05004], lr: 0.009030, loss: 3.4814
2022-10-12 18:40:49 - train: epoch 0081, iter [03000, 05004], lr: 0.009012, loss: 4.0341
2022-10-12 18:41:37 - train: epoch 0081, iter [03100, 05004], lr: 0.008994, loss: 4.4311
2022-10-12 18:42:27 - train: epoch 0081, iter [03200, 05004], lr: 0.008976, loss: 3.2657
2022-10-12 18:43:16 - train: epoch 0081, iter [03300, 05004], lr: 0.008958, loss: 3.9647
2022-10-12 18:44:06 - train: epoch 0081, iter [03400, 05004], lr: 0.008940, loss: 4.4110
2022-10-12 18:44:54 - train: epoch 0081, iter [03500, 05004], lr: 0.008922, loss: 4.2158
2022-10-12 18:45:43 - train: epoch 0081, iter [03600, 05004], lr: 0.008904, loss: 2.5340
2022-10-12 18:46:32 - train: epoch 0081, iter [03700, 05004], lr: 0.008887, loss: 3.9257
2022-10-12 18:47:21 - train: epoch 0081, iter [03800, 05004], lr: 0.008869, loss: 4.0632
2022-10-12 18:48:10 - train: epoch 0081, iter [03900, 05004], lr: 0.008851, loss: 3.6678
2022-10-12 18:48:59 - train: epoch 0081, iter [04000, 05004], lr: 0.008833, loss: 4.0943
2022-10-12 18:49:48 - train: epoch 0081, iter [04100, 05004], lr: 0.008815, loss: 3.8666
2022-10-12 18:50:37 - train: epoch 0081, iter [04200, 05004], lr: 0.008797, loss: 3.7400
2022-10-12 18:51:25 - train: epoch 0081, iter [04300, 05004], lr: 0.008780, loss: 3.8884
2022-10-12 18:52:14 - train: epoch 0081, iter [04400, 05004], lr: 0.008762, loss: 3.9466
2022-10-12 18:53:03 - train: epoch 0081, iter [04500, 05004], lr: 0.008744, loss: 3.3865
2022-10-12 18:53:52 - train: epoch 0081, iter [04600, 05004], lr: 0.008727, loss: 3.5305
2022-10-12 18:54:41 - train: epoch 0081, iter [04700, 05004], lr: 0.008709, loss: 3.3620
2022-10-12 18:55:30 - train: epoch 0081, iter [04800, 05004], lr: 0.008691, loss: 3.9676
2022-10-12 18:56:19 - train: epoch 0081, iter [04900, 05004], lr: 0.008673, loss: 3.7582
2022-10-12 18:57:04 - train: epoch 0081, iter [05000, 05004], lr: 0.008656, loss: 3.6614
2022-10-12 18:57:07 - train: epoch 081, train_loss: 3.7349
2022-10-12 18:58:49 - eval: epoch: 081, acc1: 71.916%, acc5: 90.938%, test_loss: 1.4020, per_image_load_time: 2.986ms, per_image_inference_time: 0.502ms
2022-10-12 18:58:50 - until epoch: 081, best_acc1: 71.916%
2022-10-12 18:58:50 - epoch 082 lr: 0.008655
2022-10-12 18:59:46 - train: epoch 0082, iter [00100, 05004], lr: 0.008637, loss: 4.2829
2022-10-12 19:00:35 - train: epoch 0082, iter [00200, 05004], lr: 0.008620, loss: 3.5403
2022-10-12 19:01:25 - train: epoch 0082, iter [00300, 05004], lr: 0.008602, loss: 3.3905
2022-10-12 19:02:15 - train: epoch 0082, iter [00400, 05004], lr: 0.008585, loss: 3.8449
2022-10-12 19:03:05 - train: epoch 0082, iter [00500, 05004], lr: 0.008567, loss: 2.7992
2022-10-12 19:03:53 - train: epoch 0082, iter [00600, 05004], lr: 0.008550, loss: 2.5687
2022-10-12 19:04:41 - train: epoch 0082, iter [00700, 05004], lr: 0.008532, loss: 4.0565
2022-10-12 19:05:31 - train: epoch 0082, iter [00800, 05004], lr: 0.008514, loss: 3.7975
2022-10-12 19:06:20 - train: epoch 0082, iter [00900, 05004], lr: 0.008497, loss: 3.3983
2022-10-12 19:07:10 - train: epoch 0082, iter [01000, 05004], lr: 0.008479, loss: 3.4519
2022-10-12 19:07:58 - train: epoch 0082, iter [01100, 05004], lr: 0.008462, loss: 3.5530
2022-10-12 19:08:46 - train: epoch 0082, iter [01200, 05004], lr: 0.008445, loss: 2.9111
2022-10-12 19:09:36 - train: epoch 0082, iter [01300, 05004], lr: 0.008427, loss: 4.0679
2022-10-12 19:10:24 - train: epoch 0082, iter [01400, 05004], lr: 0.008410, loss: 3.0409
2022-10-12 19:11:12 - train: epoch 0082, iter [01500, 05004], lr: 0.008392, loss: 4.0606
2022-10-12 19:12:01 - train: epoch 0082, iter [01600, 05004], lr: 0.008375, loss: 3.2877
2022-10-12 19:12:50 - train: epoch 0082, iter [01700, 05004], lr: 0.008358, loss: 3.1985
2022-10-12 19:13:41 - train: epoch 0082, iter [01800, 05004], lr: 0.008340, loss: 3.5632
2022-10-12 19:14:30 - train: epoch 0082, iter [01900, 05004], lr: 0.008323, loss: 3.4829
2022-10-12 19:15:19 - train: epoch 0082, iter [02000, 05004], lr: 0.008306, loss: 4.4552
2022-10-12 19:16:10 - train: epoch 0082, iter [02100, 05004], lr: 0.008288, loss: 3.0148
2022-10-12 19:16:59 - train: epoch 0082, iter [02200, 05004], lr: 0.008271, loss: 3.2861
2022-10-12 19:17:47 - train: epoch 0082, iter [02300, 05004], lr: 0.008254, loss: 3.3141
2022-10-12 19:18:37 - train: epoch 0082, iter [02400, 05004], lr: 0.008236, loss: 3.2089
2022-10-12 19:19:25 - train: epoch 0082, iter [02500, 05004], lr: 0.008219, loss: 3.7403
2022-10-12 19:20:14 - train: epoch 0082, iter [02600, 05004], lr: 0.008202, loss: 3.4306
2022-10-12 19:21:03 - train: epoch 0082, iter [02700, 05004], lr: 0.008185, loss: 2.7345
2022-10-12 19:21:52 - train: epoch 0082, iter [02800, 05004], lr: 0.008168, loss: 3.8818
2022-10-12 19:22:40 - train: epoch 0082, iter [02900, 05004], lr: 0.008150, loss: 3.9744
2022-10-12 19:23:29 - train: epoch 0082, iter [03000, 05004], lr: 0.008133, loss: 2.5227
2022-10-12 19:24:16 - train: epoch 0082, iter [03100, 05004], lr: 0.008116, loss: 3.9772
2022-10-12 19:25:05 - train: epoch 0082, iter [03200, 05004], lr: 0.008099, loss: 3.7159
2022-10-12 19:25:54 - train: epoch 0082, iter [03300, 05004], lr: 0.008082, loss: 3.6706
2022-10-12 19:26:42 - train: epoch 0082, iter [03400, 05004], lr: 0.008065, loss: 2.7444
2022-10-12 19:27:32 - train: epoch 0082, iter [03500, 05004], lr: 0.008048, loss: 3.7782
2022-10-12 19:28:21 - train: epoch 0082, iter [03600, 05004], lr: 0.008031, loss: 3.5966
2022-10-12 19:29:11 - train: epoch 0082, iter [03700, 05004], lr: 0.008014, loss: 4.1170
2022-10-12 19:29:59 - train: epoch 0082, iter [03800, 05004], lr: 0.007997, loss: 2.9408
2022-10-12 19:30:48 - train: epoch 0082, iter [03900, 05004], lr: 0.007980, loss: 3.8118
2022-10-12 19:31:37 - train: epoch 0082, iter [04000, 05004], lr: 0.007963, loss: 4.0769
2022-10-12 19:32:27 - train: epoch 0082, iter [04100, 05004], lr: 0.007946, loss: 3.4972
2022-10-12 19:33:16 - train: epoch 0082, iter [04200, 05004], lr: 0.007929, loss: 3.5750
2022-10-12 19:34:05 - train: epoch 0082, iter [04300, 05004], lr: 0.007912, loss: 3.6334
2022-10-12 19:34:54 - train: epoch 0082, iter [04400, 05004], lr: 0.007895, loss: 4.1231
2022-10-12 19:35:44 - train: epoch 0082, iter [04500, 05004], lr: 0.007878, loss: 2.7666
2022-10-12 19:36:32 - train: epoch 0082, iter [04600, 05004], lr: 0.007861, loss: 2.6366
2022-10-12 19:37:21 - train: epoch 0082, iter [04700, 05004], lr: 0.007844, loss: 4.4703
2022-10-12 19:38:09 - train: epoch 0082, iter [04800, 05004], lr: 0.007827, loss: 2.9376
2022-10-12 19:38:57 - train: epoch 0082, iter [04900, 05004], lr: 0.007810, loss: 3.8735
2022-10-12 19:39:43 - train: epoch 0082, iter [05000, 05004], lr: 0.007793, loss: 3.6614
2022-10-12 19:39:46 - train: epoch 082, train_loss: 3.7138
2022-10-12 19:41:33 - eval: epoch: 082, acc1: 72.756%, acc5: 91.352%, test_loss: 1.3488, per_image_load_time: 3.617ms, per_image_inference_time: 0.475ms
2022-10-12 19:41:34 - until epoch: 082, best_acc1: 72.756%
2022-10-12 19:41:34 - epoch 083 lr: 0.007793
2022-10-12 19:42:30 - train: epoch 0083, iter [00100, 05004], lr: 0.007776, loss: 3.3535
2022-10-12 19:43:20 - train: epoch 0083, iter [00200, 05004], lr: 0.007759, loss: 4.2394
2022-10-12 19:44:10 - train: epoch 0083, iter [00300, 05004], lr: 0.007742, loss: 3.6095
2022-10-12 19:44:59 - train: epoch 0083, iter [00400, 05004], lr: 0.007726, loss: 3.5186
2022-10-12 19:45:48 - train: epoch 0083, iter [00500, 05004], lr: 0.007709, loss: 4.0859
2022-10-12 19:46:38 - train: epoch 0083, iter [00600, 05004], lr: 0.007692, loss: 3.3151
2022-10-12 19:47:28 - train: epoch 0083, iter [00700, 05004], lr: 0.007676, loss: 3.7043
2022-10-12 19:48:19 - train: epoch 0083, iter [00800, 05004], lr: 0.007659, loss: 3.5049
2022-10-12 19:49:08 - train: epoch 0083, iter [00900, 05004], lr: 0.007642, loss: 4.3077
2022-10-12 19:49:56 - train: epoch 0083, iter [01000, 05004], lr: 0.007625, loss: 4.0305
2022-10-12 19:50:47 - train: epoch 0083, iter [01100, 05004], lr: 0.007609, loss: 3.6911
2022-10-12 19:51:36 - train: epoch 0083, iter [01200, 05004], lr: 0.007592, loss: 3.7480
2022-10-12 19:52:25 - train: epoch 0083, iter [01300, 05004], lr: 0.007576, loss: 3.6997
2022-10-12 19:53:14 - train: epoch 0083, iter [01400, 05004], lr: 0.007559, loss: 3.4106
2022-10-12 19:54:02 - train: epoch 0083, iter [01500, 05004], lr: 0.007542, loss: 4.1345
2022-10-12 19:54:51 - train: epoch 0083, iter [01600, 05004], lr: 0.007526, loss: 3.2004
2022-10-12 19:55:40 - train: epoch 0083, iter [01700, 05004], lr: 0.007509, loss: 4.2652
2022-10-12 19:56:28 - train: epoch 0083, iter [01800, 05004], lr: 0.007493, loss: 3.7078
2022-10-12 19:57:19 - train: epoch 0083, iter [01900, 05004], lr: 0.007476, loss: 3.9806
2022-10-12 19:58:08 - train: epoch 0083, iter [02000, 05004], lr: 0.007460, loss: 3.7336
2022-10-12 19:58:58 - train: epoch 0083, iter [02100, 05004], lr: 0.007443, loss: 3.4859
2022-10-12 19:59:48 - train: epoch 0083, iter [02200, 05004], lr: 0.007427, loss: 3.6342
2022-10-12 20:00:37 - train: epoch 0083, iter [02300, 05004], lr: 0.007410, loss: 3.7924
2022-10-12 20:01:25 - train: epoch 0083, iter [02400, 05004], lr: 0.007394, loss: 3.0357
2022-10-12 20:02:14 - train: epoch 0083, iter [02500, 05004], lr: 0.007378, loss: 3.0264
2022-10-12 20:03:04 - train: epoch 0083, iter [02600, 05004], lr: 0.007361, loss: 3.5970
2022-10-12 20:03:53 - train: epoch 0083, iter [02700, 05004], lr: 0.007345, loss: 3.9531
2022-10-12 20:04:43 - train: epoch 0083, iter [02800, 05004], lr: 0.007328, loss: 3.1084
2022-10-12 20:05:33 - train: epoch 0083, iter [02900, 05004], lr: 0.007312, loss: 4.0332
2022-10-12 20:06:21 - train: epoch 0083, iter [03000, 05004], lr: 0.007296, loss: 3.0986
2022-10-12 20:07:10 - train: epoch 0083, iter [03100, 05004], lr: 0.007279, loss: 4.1055
2022-10-12 20:08:00 - train: epoch 0083, iter [03200, 05004], lr: 0.007263, loss: 3.2319
2022-10-12 20:08:49 - train: epoch 0083, iter [03300, 05004], lr: 0.007247, loss: 3.2521
2022-10-12 20:09:38 - train: epoch 0083, iter [03400, 05004], lr: 0.007231, loss: 3.3834
2022-10-12 20:10:28 - train: epoch 0083, iter [03500, 05004], lr: 0.007214, loss: 4.0076
2022-10-12 20:11:17 - train: epoch 0083, iter [03600, 05004], lr: 0.007198, loss: 3.6047
2022-10-12 20:12:04 - train: epoch 0083, iter [03700, 05004], lr: 0.007182, loss: 3.8530
2022-10-12 20:12:54 - train: epoch 0083, iter [03800, 05004], lr: 0.007166, loss: 3.5641
2022-10-12 20:13:43 - train: epoch 0083, iter [03900, 05004], lr: 0.007150, loss: 3.9334
2022-10-12 20:14:30 - train: epoch 0083, iter [04000, 05004], lr: 0.007133, loss: 3.8277
2022-10-12 20:15:21 - train: epoch 0083, iter [04100, 05004], lr: 0.007117, loss: 4.2199
2022-10-12 20:16:10 - train: epoch 0083, iter [04200, 05004], lr: 0.007101, loss: 4.1563
2022-10-12 20:17:01 - train: epoch 0083, iter [04300, 05004], lr: 0.007085, loss: 3.9319
2022-10-12 20:17:49 - train: epoch 0083, iter [04400, 05004], lr: 0.007069, loss: 3.6395
2022-10-12 20:18:38 - train: epoch 0083, iter [04500, 05004], lr: 0.007053, loss: 3.9106
2022-10-12 20:19:28 - train: epoch 0083, iter [04600, 05004], lr: 0.007037, loss: 4.0535
2022-10-12 20:20:17 - train: epoch 0083, iter [04700, 05004], lr: 0.007021, loss: 4.0849
2022-10-12 20:21:06 - train: epoch 0083, iter [04800, 05004], lr: 0.007005, loss: 4.5536
2022-10-12 20:21:54 - train: epoch 0083, iter [04900, 05004], lr: 0.006989, loss: 3.8552
2022-10-12 20:22:42 - train: epoch 0083, iter [05000, 05004], lr: 0.006973, loss: 3.8334
2022-10-12 20:22:44 - train: epoch 083, train_loss: 3.7051
2022-10-12 20:24:32 - eval: epoch: 083, acc1: 72.868%, acc5: 91.554%, test_loss: 1.3863, per_image_load_time: 3.610ms, per_image_inference_time: 0.487ms
2022-10-12 20:24:33 - until epoch: 083, best_acc1: 72.868%
2022-10-12 20:24:33 - epoch 084 lr: 0.006972
2022-10-12 20:25:29 - train: epoch 0084, iter [00100, 05004], lr: 0.006956, loss: 2.7572
2022-10-12 20:26:18 - train: epoch 0084, iter [00200, 05004], lr: 0.006940, loss: 3.2466
2022-10-12 20:27:06 - train: epoch 0084, iter [00300, 05004], lr: 0.006924, loss: 3.1496
2022-10-12 20:27:55 - train: epoch 0084, iter [00400, 05004], lr: 0.006908, loss: 2.7928
2022-10-12 20:28:44 - train: epoch 0084, iter [00500, 05004], lr: 0.006893, loss: 4.0519
2022-10-12 20:29:34 - train: epoch 0084, iter [00600, 05004], lr: 0.006877, loss: 3.6979
2022-10-12 20:30:25 - train: epoch 0084, iter [00700, 05004], lr: 0.006861, loss: 3.8255
2022-10-12 20:31:14 - train: epoch 0084, iter [00800, 05004], lr: 0.006845, loss: 3.6086
2022-10-12 20:32:03 - train: epoch 0084, iter [00900, 05004], lr: 0.006829, loss: 3.8242
2022-10-12 20:32:51 - train: epoch 0084, iter [01000, 05004], lr: 0.006813, loss: 3.5412
2022-10-12 20:33:41 - train: epoch 0084, iter [01100, 05004], lr: 0.006797, loss: 3.4367
2022-10-12 20:34:31 - train: epoch 0084, iter [01200, 05004], lr: 0.006782, loss: 3.3273
2022-10-12 20:35:20 - train: epoch 0084, iter [01300, 05004], lr: 0.006766, loss: 3.6686
2022-10-12 20:36:10 - train: epoch 0084, iter [01400, 05004], lr: 0.006750, loss: 2.9500
2022-10-12 20:37:00 - train: epoch 0084, iter [01500, 05004], lr: 0.006734, loss: 3.6013
2022-10-12 20:37:50 - train: epoch 0084, iter [01600, 05004], lr: 0.006719, loss: 3.2216
2022-10-12 20:38:38 - train: epoch 0084, iter [01700, 05004], lr: 0.006703, loss: 4.1817
2022-10-12 20:39:28 - train: epoch 0084, iter [01800, 05004], lr: 0.006687, loss: 4.0842
2022-10-12 20:40:17 - train: epoch 0084, iter [01900, 05004], lr: 0.006672, loss: 4.0607
2022-10-12 20:41:07 - train: epoch 0084, iter [02000, 05004], lr: 0.006656, loss: 3.6544
2022-10-12 20:41:56 - train: epoch 0084, iter [02100, 05004], lr: 0.006640, loss: 3.8676
2022-10-12 20:42:46 - train: epoch 0084, iter [02200, 05004], lr: 0.006625, loss: 4.2002
2022-10-12 20:43:35 - train: epoch 0084, iter [02300, 05004], lr: 0.006609, loss: 3.8099
2022-10-12 20:44:25 - train: epoch 0084, iter [02400, 05004], lr: 0.006594, loss: 4.1531
2022-10-12 20:45:14 - train: epoch 0084, iter [02500, 05004], lr: 0.006578, loss: 4.3016
2022-10-12 20:46:03 - train: epoch 0084, iter [02600, 05004], lr: 0.006563, loss: 3.4775
2022-10-12 20:46:52 - train: epoch 0084, iter [02700, 05004], lr: 0.006547, loss: 3.3289
2022-10-12 20:47:42 - train: epoch 0084, iter [02800, 05004], lr: 0.006532, loss: 3.9183
2022-10-12 20:48:31 - train: epoch 0084, iter [02900, 05004], lr: 0.006516, loss: 4.0503
2022-10-12 20:49:22 - train: epoch 0084, iter [03000, 05004], lr: 0.006501, loss: 3.3747
2022-10-12 20:50:12 - train: epoch 0084, iter [03100, 05004], lr: 0.006485, loss: 4.4699
2022-10-12 20:51:01 - train: epoch 0084, iter [03200, 05004], lr: 0.006470, loss: 3.6246
2022-10-12 20:51:51 - train: epoch 0084, iter [03300, 05004], lr: 0.006454, loss: 4.0463
2022-10-12 20:52:39 - train: epoch 0084, iter [03400, 05004], lr: 0.006439, loss: 3.5420
2022-10-12 20:53:30 - train: epoch 0084, iter [03500, 05004], lr: 0.006423, loss: 3.6740
2022-10-12 20:54:19 - train: epoch 0084, iter [03600, 05004], lr: 0.006408, loss: 4.0726
2022-10-12 20:55:09 - train: epoch 0084, iter [03700, 05004], lr: 0.006393, loss: 3.9122
2022-10-12 20:55:58 - train: epoch 0084, iter [03800, 05004], lr: 0.006377, loss: 4.0330
2022-10-12 20:56:49 - train: epoch 0084, iter [03900, 05004], lr: 0.006362, loss: 3.4966
2022-10-12 20:57:38 - train: epoch 0084, iter [04000, 05004], lr: 0.006347, loss: 3.2566
2022-10-12 20:58:28 - train: epoch 0084, iter [04100, 05004], lr: 0.006331, loss: 3.5966
2022-10-12 20:59:17 - train: epoch 0084, iter [04200, 05004], lr: 0.006316, loss: 4.1007
2022-10-12 21:00:05 - train: epoch 0084, iter [04300, 05004], lr: 0.006301, loss: 3.4796
2022-10-12 21:00:53 - train: epoch 0084, iter [04400, 05004], lr: 0.006286, loss: 3.7513
2022-10-12 21:01:43 - train: epoch 0084, iter [04500, 05004], lr: 0.006270, loss: 2.5630
2022-10-12 21:02:32 - train: epoch 0084, iter [04600, 05004], lr: 0.006255, loss: 3.9598
2022-10-12 21:03:21 - train: epoch 0084, iter [04700, 05004], lr: 0.006240, loss: 3.8106
2022-10-12 21:04:09 - train: epoch 0084, iter [04800, 05004], lr: 0.006225, loss: 4.2248
2022-10-12 21:04:59 - train: epoch 0084, iter [04900, 05004], lr: 0.006210, loss: 3.8603
2022-10-12 21:05:45 - train: epoch 0084, iter [05000, 05004], lr: 0.006195, loss: 2.7251
2022-10-12 21:05:48 - train: epoch 084, train_loss: 3.6820
2022-10-12 21:07:36 - eval: epoch: 084, acc1: 73.316%, acc5: 91.896%, test_loss: 1.3393, per_image_load_time: 3.632ms, per_image_inference_time: 0.483ms
2022-10-12 21:07:37 - until epoch: 084, best_acc1: 73.316%
2022-10-12 21:07:37 - epoch 085 lr: 0.006194
2022-10-12 21:08:34 - train: epoch 0085, iter [00100, 05004], lr: 0.006179, loss: 3.6872
2022-10-12 21:09:24 - train: epoch 0085, iter [00200, 05004], lr: 0.006164, loss: 3.5566
2022-10-12 21:10:13 - train: epoch 0085, iter [00300, 05004], lr: 0.006149, loss: 3.6141
2022-10-12 21:11:03 - train: epoch 0085, iter [00400, 05004], lr: 0.006134, loss: 3.6701
2022-10-12 21:11:53 - train: epoch 0085, iter [00500, 05004], lr: 0.006119, loss: 3.4240
2022-10-12 21:12:41 - train: epoch 0085, iter [00600, 05004], lr: 0.006104, loss: 3.7581
2022-10-12 21:13:32 - train: epoch 0085, iter [00700, 05004], lr: 0.006089, loss: 4.1414
2022-10-12 21:14:21 - train: epoch 0085, iter [00800, 05004], lr: 0.006074, loss: 3.7200
2022-10-12 21:15:12 - train: epoch 0085, iter [00900, 05004], lr: 0.006059, loss: 3.7038
2022-10-12 21:16:01 - train: epoch 0085, iter [01000, 05004], lr: 0.006044, loss: 3.9547
2022-10-12 21:16:50 - train: epoch 0085, iter [01100, 05004], lr: 0.006029, loss: 3.3081
2022-10-12 21:17:38 - train: epoch 0085, iter [01200, 05004], lr: 0.006014, loss: 3.4685
2022-10-12 21:18:29 - train: epoch 0085, iter [01300, 05004], lr: 0.005999, loss: 4.4942
2022-10-12 21:19:17 - train: epoch 0085, iter [01400, 05004], lr: 0.005984, loss: 3.4910
2022-10-12 21:20:08 - train: epoch 0085, iter [01500, 05004], lr: 0.005969, loss: 3.4703
2022-10-12 21:20:58 - train: epoch 0085, iter [01600, 05004], lr: 0.005954, loss: 3.4010
2022-10-12 21:21:49 - train: epoch 0085, iter [01700, 05004], lr: 0.005939, loss: 3.8899
2022-10-12 21:22:37 - train: epoch 0085, iter [01800, 05004], lr: 0.005925, loss: 3.3504
2022-10-12 21:23:26 - train: epoch 0085, iter [01900, 05004], lr: 0.005910, loss: 3.6420
2022-10-12 21:24:16 - train: epoch 0085, iter [02000, 05004], lr: 0.005895, loss: 3.8111
2022-10-12 21:25:07 - train: epoch 0085, iter [02100, 05004], lr: 0.005880, loss: 3.7101
2022-10-12 21:25:57 - train: epoch 0085, iter [02200, 05004], lr: 0.005866, loss: 4.2274
2022-10-12 21:26:45 - train: epoch 0085, iter [02300, 05004], lr: 0.005851, loss: 3.6784
2022-10-12 21:27:36 - train: epoch 0085, iter [02400, 05004], lr: 0.005836, loss: 4.2643
2022-10-12 21:28:25 - train: epoch 0085, iter [02500, 05004], lr: 0.005821, loss: 4.0171
2022-10-12 21:29:14 - train: epoch 0085, iter [02600, 05004], lr: 0.005807, loss: 3.9939
2022-10-12 21:30:04 - train: epoch 0085, iter [02700, 05004], lr: 0.005792, loss: 3.1562
2022-10-12 21:30:55 - train: epoch 0085, iter [02800, 05004], lr: 0.005777, loss: 3.4962
2022-10-12 21:31:44 - train: epoch 0085, iter [02900, 05004], lr: 0.005763, loss: 3.5295
2022-10-12 21:32:32 - train: epoch 0085, iter [03000, 05004], lr: 0.005748, loss: 3.5227
2022-10-12 21:33:22 - train: epoch 0085, iter [03100, 05004], lr: 0.005734, loss: 3.4799
2022-10-12 21:34:11 - train: epoch 0085, iter [03200, 05004], lr: 0.005719, loss: 3.4521
2022-10-12 21:35:00 - train: epoch 0085, iter [03300, 05004], lr: 0.005704, loss: 3.7160
2022-10-12 21:35:49 - train: epoch 0085, iter [03400, 05004], lr: 0.005690, loss: 3.8228
2022-10-12 21:36:40 - train: epoch 0085, iter [03500, 05004], lr: 0.005675, loss: 3.7568
2022-10-12 21:37:31 - train: epoch 0085, iter [03600, 05004], lr: 0.005661, loss: 3.7155
2022-10-12 21:38:21 - train: epoch 0085, iter [03700, 05004], lr: 0.005646, loss: 3.5882
2022-10-12 21:39:10 - train: epoch 0085, iter [03800, 05004], lr: 0.005632, loss: 3.7513
2022-10-12 21:40:00 - train: epoch 0085, iter [03900, 05004], lr: 0.005618, loss: 3.2336
2022-10-12 21:40:50 - train: epoch 0085, iter [04000, 05004], lr: 0.005603, loss: 3.9920
2022-10-12 21:41:39 - train: epoch 0085, iter [04100, 05004], lr: 0.005589, loss: 3.9186
2022-10-12 21:42:30 - train: epoch 0085, iter [04200, 05004], lr: 0.005574, loss: 3.2619
2022-10-12 21:43:19 - train: epoch 0085, iter [04300, 05004], lr: 0.005560, loss: 2.6864
2022-10-12 21:44:09 - train: epoch 0085, iter [04400, 05004], lr: 0.005546, loss: 2.8829
2022-10-12 21:44:58 - train: epoch 0085, iter [04500, 05004], lr: 0.005531, loss: 2.9943
2022-10-12 21:45:48 - train: epoch 0085, iter [04600, 05004], lr: 0.005517, loss: 3.2562
2022-10-12 21:46:38 - train: epoch 0085, iter [04700, 05004], lr: 0.005503, loss: 4.2002
2022-10-12 21:47:28 - train: epoch 0085, iter [04800, 05004], lr: 0.005488, loss: 3.8160
2022-10-12 21:48:17 - train: epoch 0085, iter [04900, 05004], lr: 0.005474, loss: 3.5733
2022-10-12 21:49:03 - train: epoch 0085, iter [05000, 05004], lr: 0.005460, loss: 4.1937
2022-10-12 21:49:06 - train: epoch 085, train_loss: 3.6389
2022-10-12 21:50:55 - eval: epoch: 085, acc1: 73.580%, acc5: 91.884%, test_loss: 1.3843, per_image_load_time: 3.667ms, per_image_inference_time: 0.505ms
2022-10-12 21:50:56 - until epoch: 085, best_acc1: 73.580%
2022-10-12 21:50:56 - epoch 086 lr: 0.005459
2022-10-12 21:51:53 - train: epoch 0086, iter [00100, 05004], lr: 0.005445, loss: 3.9469
2022-10-12 21:52:42 - train: epoch 0086, iter [00200, 05004], lr: 0.005431, loss: 3.5088
2022-10-12 21:53:31 - train: epoch 0086, iter [00300, 05004], lr: 0.005416, loss: 3.8872
2022-10-12 21:54:22 - train: epoch 0086, iter [00400, 05004], lr: 0.005402, loss: 3.7124
2022-10-12 21:55:12 - train: epoch 0086, iter [00500, 05004], lr: 0.005388, loss: 3.8457
2022-10-12 21:56:02 - train: epoch 0086, iter [00600, 05004], lr: 0.005374, loss: 3.6226
2022-10-12 21:56:56 - train: epoch 0086, iter [00700, 05004], lr: 0.005360, loss: 4.5510
2022-10-12 21:57:44 - train: epoch 0086, iter [00800, 05004], lr: 0.005346, loss: 3.7305
2022-10-12 21:58:34 - train: epoch 0086, iter [00900, 05004], lr: 0.005332, loss: 3.6794
2022-10-12 21:59:24 - train: epoch 0086, iter [01000, 05004], lr: 0.005318, loss: 4.0656
2022-10-12 22:00:14 - train: epoch 0086, iter [01100, 05004], lr: 0.005303, loss: 3.2711
2022-10-12 22:01:05 - train: epoch 0086, iter [01200, 05004], lr: 0.005289, loss: 3.4785
2022-10-12 22:01:55 - train: epoch 0086, iter [01300, 05004], lr: 0.005275, loss: 3.0018
2022-10-12 22:02:44 - train: epoch 0086, iter [01400, 05004], lr: 0.005261, loss: 3.1725
2022-10-12 22:03:35 - train: epoch 0086, iter [01500, 05004], lr: 0.005247, loss: 3.3618
2022-10-12 22:04:25 - train: epoch 0086, iter [01600, 05004], lr: 0.005233, loss: 4.2076
2022-10-12 22:05:16 - train: epoch 0086, iter [01700, 05004], lr: 0.005219, loss: 3.6066
2022-10-12 22:06:06 - train: epoch 0086, iter [01800, 05004], lr: 0.005205, loss: 4.2380
2022-10-12 22:06:55 - train: epoch 0086, iter [01900, 05004], lr: 0.005192, loss: 3.1354
2022-10-12 22:07:46 - train: epoch 0086, iter [02000, 05004], lr: 0.005178, loss: 4.1383
2022-10-12 22:08:36 - train: epoch 0086, iter [02100, 05004], lr: 0.005164, loss: 2.7878
2022-10-12 22:09:27 - train: epoch 0086, iter [02200, 05004], lr: 0.005150, loss: 3.6995
2022-10-12 22:10:17 - train: epoch 0086, iter [02300, 05004], lr: 0.005136, loss: 3.0905
2022-10-12 22:11:05 - train: epoch 0086, iter [02400, 05004], lr: 0.005122, loss: 3.5212
2022-10-12 22:11:57 - train: epoch 0086, iter [02500, 05004], lr: 0.005108, loss: 3.6759
2022-10-12 22:12:47 - train: epoch 0086, iter [02600, 05004], lr: 0.005095, loss: 4.4665
2022-10-12 22:13:36 - train: epoch 0086, iter [02700, 05004], lr: 0.005081, loss: 3.9173
2022-10-12 22:14:28 - train: epoch 0086, iter [02800, 05004], lr: 0.005067, loss: 3.8756
2022-10-12 22:15:18 - train: epoch 0086, iter [02900, 05004], lr: 0.005053, loss: 3.8337
2022-10-12 22:16:08 - train: epoch 0086, iter [03000, 05004], lr: 0.005040, loss: 4.0776
2022-10-12 22:16:55 - train: epoch 0086, iter [03100, 05004], lr: 0.005026, loss: 3.6380
2022-10-12 22:17:46 - train: epoch 0086, iter [03200, 05004], lr: 0.005012, loss: 3.2764
2022-10-12 22:18:36 - train: epoch 0086, iter [03300, 05004], lr: 0.004998, loss: 3.7213
2022-10-12 22:19:25 - train: epoch 0086, iter [03400, 05004], lr: 0.004985, loss: 4.2530
2022-10-12 22:20:15 - train: epoch 0086, iter [03500, 05004], lr: 0.004971, loss: 3.5757
2022-10-12 22:21:06 - train: epoch 0086, iter [03600, 05004], lr: 0.004958, loss: 3.9995
2022-10-12 22:21:56 - train: epoch 0086, iter [03700, 05004], lr: 0.004944, loss: 2.6623
2022-10-12 22:22:46 - train: epoch 0086, iter [03800, 05004], lr: 0.004930, loss: 3.7528
2022-10-12 22:23:33 - train: epoch 0086, iter [03900, 05004], lr: 0.004917, loss: 4.1039
2022-10-12 22:24:24 - train: epoch 0086, iter [04000, 05004], lr: 0.004903, loss: 3.8144
2022-10-12 22:25:15 - train: epoch 0086, iter [04100, 05004], lr: 0.004890, loss: 3.2914
2022-10-12 22:26:05 - train: epoch 0086, iter [04200, 05004], lr: 0.004876, loss: 3.1217
2022-10-12 22:26:55 - train: epoch 0086, iter [04300, 05004], lr: 0.004863, loss: 3.8860
2022-10-12 22:27:47 - train: epoch 0086, iter [04400, 05004], lr: 0.004849, loss: 3.8837
2022-10-12 22:28:36 - train: epoch 0086, iter [04500, 05004], lr: 0.004836, loss: 3.8800
2022-10-12 22:29:29 - train: epoch 0086, iter [04600, 05004], lr: 0.004822, loss: 3.1588
2022-10-12 22:30:17 - train: epoch 0086, iter [04700, 05004], lr: 0.004809, loss: 3.3907
2022-10-12 22:31:06 - train: epoch 0086, iter [04800, 05004], lr: 0.004795, loss: 3.7651
2022-10-12 22:31:56 - train: epoch 0086, iter [04900, 05004], lr: 0.004782, loss: 3.7481
2022-10-12 22:32:40 - train: epoch 0086, iter [05000, 05004], lr: 0.004769, loss: 3.6345
2022-10-12 22:32:43 - train: epoch 086, train_loss: 3.6236
2022-10-12 22:34:32 - eval: epoch: 086, acc1: 74.154%, acc5: 92.126%, test_loss: 1.2795, per_image_load_time: 3.620ms, per_image_inference_time: 0.523ms
2022-10-12 22:34:32 - until epoch: 086, best_acc1: 74.154%
2022-10-12 22:34:32 - epoch 087 lr: 0.004768
2022-10-12 22:35:28 - train: epoch 0087, iter [00100, 05004], lr: 0.004755, loss: 4.2531
2022-10-12 22:36:17 - train: epoch 0087, iter [00200, 05004], lr: 0.004741, loss: 3.7990
2022-10-12 22:37:07 - train: epoch 0087, iter [00300, 05004], lr: 0.004728, loss: 3.1708
2022-10-12 22:37:57 - train: epoch 0087, iter [00400, 05004], lr: 0.004715, loss: 3.4506
2022-10-12 22:38:48 - train: epoch 0087, iter [00500, 05004], lr: 0.004702, loss: 4.0792
2022-10-12 22:39:39 - train: epoch 0087, iter [00600, 05004], lr: 0.004688, loss: 4.2594
2022-10-12 22:40:29 - train: epoch 0087, iter [00700, 05004], lr: 0.004675, loss: 3.1354
2022-10-12 22:41:19 - train: epoch 0087, iter [00800, 05004], lr: 0.004662, loss: 3.3726
2022-10-12 22:42:10 - train: epoch 0087, iter [00900, 05004], lr: 0.004649, loss: 3.1907
2022-10-12 22:43:00 - train: epoch 0087, iter [01000, 05004], lr: 0.004635, loss: 3.5576
2022-10-12 22:43:48 - train: epoch 0087, iter [01100, 05004], lr: 0.004622, loss: 3.7237
2022-10-12 22:44:37 - train: epoch 0087, iter [01200, 05004], lr: 0.004609, loss: 4.0804
2022-10-12 22:45:26 - train: epoch 0087, iter [01300, 05004], lr: 0.004596, loss: 3.5680
2022-10-12 22:46:17 - train: epoch 0087, iter [01400, 05004], lr: 0.004583, loss: 2.9724
2022-10-12 22:47:09 - train: epoch 0087, iter [01500, 05004], lr: 0.004570, loss: 3.6172
2022-10-12 22:47:58 - train: epoch 0087, iter [01600, 05004], lr: 0.004557, loss: 3.7017
2022-10-12 22:48:48 - train: epoch 0087, iter [01700, 05004], lr: 0.004544, loss: 4.0635
2022-10-12 22:49:35 - train: epoch 0087, iter [01800, 05004], lr: 0.004531, loss: 3.6495
2022-10-12 22:50:27 - train: epoch 0087, iter [01900, 05004], lr: 0.004517, loss: 3.6878
2022-10-12 22:51:18 - train: epoch 0087, iter [02000, 05004], lr: 0.004504, loss: 3.8917
2022-10-12 22:52:09 - train: epoch 0087, iter [02100, 05004], lr: 0.004491, loss: 3.3394
2022-10-12 22:52:59 - train: epoch 0087, iter [02200, 05004], lr: 0.004478, loss: 4.2233
2022-10-12 22:53:50 - train: epoch 0087, iter [02300, 05004], lr: 0.004466, loss: 4.0632
2022-10-12 22:54:42 - train: epoch 0087, iter [02400, 05004], lr: 0.004453, loss: 3.1796
2022-10-12 22:55:32 - train: epoch 0087, iter [02500, 05004], lr: 0.004440, loss: 4.2259
2022-10-12 22:56:23 - train: epoch 0087, iter [02600, 05004], lr: 0.004427, loss: 3.9754
2022-10-12 22:57:13 - train: epoch 0087, iter [02700, 05004], lr: 0.004414, loss: 3.3802
2022-10-12 22:58:01 - train: epoch 0087, iter [02800, 05004], lr: 0.004401, loss: 2.9840
2022-10-12 22:58:50 - train: epoch 0087, iter [02900, 05004], lr: 0.004388, loss: 4.0877
2022-10-12 22:59:41 - train: epoch 0087, iter [03000, 05004], lr: 0.004375, loss: 3.5824
2022-10-12 23:00:31 - train: epoch 0087, iter [03100, 05004], lr: 0.004362, loss: 3.6716
2022-10-12 23:01:20 - train: epoch 0087, iter [03200, 05004], lr: 0.004350, loss: 3.6824
2022-10-12 23:02:10 - train: epoch 0087, iter [03300, 05004], lr: 0.004337, loss: 3.2112
2022-10-12 23:03:00 - train: epoch 0087, iter [03400, 05004], lr: 0.004324, loss: 3.5923
2022-10-12 23:03:48 - train: epoch 0087, iter [03500, 05004], lr: 0.004311, loss: 4.1562
2022-10-12 23:04:39 - train: epoch 0087, iter [03600, 05004], lr: 0.004299, loss: 3.2670
2022-10-12 23:05:31 - train: epoch 0087, iter [03700, 05004], lr: 0.004286, loss: 3.5413
2022-10-12 23:06:22 - train: epoch 0087, iter [03800, 05004], lr: 0.004273, loss: 3.8178
2022-10-12 23:07:11 - train: epoch 0087, iter [03900, 05004], lr: 0.004261, loss: 3.6673
2022-10-12 23:08:01 - train: epoch 0087, iter [04000, 05004], lr: 0.004248, loss: 4.1681
2022-10-12 23:08:52 - train: epoch 0087, iter [04100, 05004], lr: 0.004235, loss: 3.7757
2022-10-12 23:09:41 - train: epoch 0087, iter [04200, 05004], lr: 0.004223, loss: 3.9517
2022-10-12 23:10:33 - train: epoch 0087, iter [04300, 05004], lr: 0.004210, loss: 3.6900
2022-10-12 23:11:22 - train: epoch 0087, iter [04400, 05004], lr: 0.004197, loss: 3.5313
2022-10-12 23:12:10 - train: epoch 0087, iter [04500, 05004], lr: 0.004185, loss: 3.0633
2022-10-12 23:13:00 - train: epoch 0087, iter [04600, 05004], lr: 0.004172, loss: 3.6570
2022-10-12 23:13:49 - train: epoch 0087, iter [04700, 05004], lr: 0.004160, loss: 4.2657
2022-10-12 23:14:39 - train: epoch 0087, iter [04800, 05004], lr: 0.004147, loss: 3.0857
2022-10-12 23:15:29 - train: epoch 0087, iter [04900, 05004], lr: 0.004135, loss: 3.5464
2022-10-12 23:16:17 - train: epoch 0087, iter [05000, 05004], lr: 0.004122, loss: 2.9679
2022-10-12 23:16:19 - train: epoch 087, train_loss: 3.5999
2022-10-12 23:18:07 - eval: epoch: 087, acc1: 74.292%, acc5: 92.298%, test_loss: 1.2950, per_image_load_time: 3.562ms, per_image_inference_time: 0.540ms
2022-10-12 23:18:08 - until epoch: 087, best_acc1: 74.292%
2022-10-12 23:18:08 - epoch 088 lr: 0.004122
2022-10-12 23:19:04 - train: epoch 0088, iter [00100, 05004], lr: 0.004109, loss: 3.4522
2022-10-12 23:19:57 - train: epoch 0088, iter [00200, 05004], lr: 0.004097, loss: 3.7793
2022-10-12 23:20:47 - train: epoch 0088, iter [00300, 05004], lr: 0.004085, loss: 4.2161
2022-10-12 23:21:38 - train: epoch 0088, iter [00400, 05004], lr: 0.004072, loss: 3.5172
2022-10-12 23:22:28 - train: epoch 0088, iter [00500, 05004], lr: 0.004060, loss: 3.6548
2022-10-12 23:23:17 - train: epoch 0088, iter [00600, 05004], lr: 0.004047, loss: 3.0144
2022-10-12 23:24:06 - train: epoch 0088, iter [00700, 05004], lr: 0.004035, loss: 4.1468
2022-10-12 23:24:57 - train: epoch 0088, iter [00800, 05004], lr: 0.004023, loss: 4.0766
2022-10-12 23:25:47 - train: epoch 0088, iter [00900, 05004], lr: 0.004010, loss: 3.0503
2022-10-12 23:26:35 - train: epoch 0088, iter [01000, 05004], lr: 0.003998, loss: 3.2912
2022-10-12 23:27:25 - train: epoch 0088, iter [01100, 05004], lr: 0.003986, loss: 4.0737
2022-10-12 23:28:15 - train: epoch 0088, iter [01200, 05004], lr: 0.003974, loss: 3.0963
2022-10-12 23:29:05 - train: epoch 0088, iter [01300, 05004], lr: 0.003961, loss: 4.1075
2022-10-12 23:29:55 - train: epoch 0088, iter [01400, 05004], lr: 0.003949, loss: 3.8966
2022-10-12 23:30:45 - train: epoch 0088, iter [01500, 05004], lr: 0.003937, loss: 4.0615
2022-10-12 23:31:35 - train: epoch 0088, iter [01600, 05004], lr: 0.003925, loss: 3.5232
2022-10-12 23:32:25 - train: epoch 0088, iter [01700, 05004], lr: 0.003913, loss: 3.5572
2022-10-12 23:33:14 - train: epoch 0088, iter [01800, 05004], lr: 0.003900, loss: 3.9861
2022-10-12 23:34:05 - train: epoch 0088, iter [01900, 05004], lr: 0.003888, loss: 3.7706
2022-10-12 23:34:54 - train: epoch 0088, iter [02000, 05004], lr: 0.003876, loss: 3.2710
2022-10-12 23:35:45 - train: epoch 0088, iter [02100, 05004], lr: 0.003864, loss: 3.4486
2022-10-12 23:36:33 - train: epoch 0088, iter [02200, 05004], lr: 0.003852, loss: 3.8500
2022-10-12 23:37:23 - train: epoch 0088, iter [02300, 05004], lr: 0.003840, loss: 2.9121
2022-10-12 23:38:13 - train: epoch 0088, iter [02400, 05004], lr: 0.003828, loss: 3.6240
2022-10-12 23:39:02 - train: epoch 0088, iter [02500, 05004], lr: 0.003816, loss: 4.2189
2022-10-12 23:39:51 - train: epoch 0088, iter [02600, 05004], lr: 0.003804, loss: 3.1268
2022-10-12 23:40:41 - train: epoch 0088, iter [02700, 05004], lr: 0.003792, loss: 3.7077
2022-10-12 23:41:30 - train: epoch 0088, iter [02800, 05004], lr: 0.003780, loss: 3.3567
2022-10-12 23:42:20 - train: epoch 0088, iter [02900, 05004], lr: 0.003768, loss: 3.8897
2022-10-12 23:43:12 - train: epoch 0088, iter [03000, 05004], lr: 0.003756, loss: 3.8550
2022-10-12 23:44:01 - train: epoch 0088, iter [03100, 05004], lr: 0.003744, loss: 4.1225
2022-10-12 23:44:52 - train: epoch 0088, iter [03200, 05004], lr: 0.003732, loss: 4.3374
2022-10-12 23:45:42 - train: epoch 0088, iter [03300, 05004], lr: 0.003720, loss: 2.6118
2022-10-12 23:46:32 - train: epoch 0088, iter [03400, 05004], lr: 0.003709, loss: 4.0666
2022-10-12 23:47:22 - train: epoch 0088, iter [03500, 05004], lr: 0.003697, loss: 3.9899
2022-10-12 23:48:13 - train: epoch 0088, iter [03600, 05004], lr: 0.003685, loss: 3.2534
2022-10-12 23:49:02 - train: epoch 0088, iter [03700, 05004], lr: 0.003673, loss: 3.0623
2022-10-12 23:49:51 - train: epoch 0088, iter [03800, 05004], lr: 0.003661, loss: 4.0374
2022-10-12 23:50:41 - train: epoch 0088, iter [03900, 05004], lr: 0.003650, loss: 3.3916
2022-10-12 23:51:33 - train: epoch 0088, iter [04000, 05004], lr: 0.003638, loss: 3.3778
2022-10-12 23:52:21 - train: epoch 0088, iter [04100, 05004], lr: 0.003626, loss: 3.4651
2022-10-12 23:53:09 - train: epoch 0088, iter [04200, 05004], lr: 0.003614, loss: 3.1978
2022-10-12 23:53:59 - train: epoch 0088, iter [04300, 05004], lr: 0.003603, loss: 3.3715
2022-10-12 23:54:49 - train: epoch 0088, iter [04400, 05004], lr: 0.003591, loss: 3.5951
2022-10-12 23:55:40 - train: epoch 0088, iter [04500, 05004], lr: 0.003579, loss: 3.0462
2022-10-12 23:56:30 - train: epoch 0088, iter [04600, 05004], lr: 0.003568, loss: 3.5245
2022-10-12 23:57:21 - train: epoch 0088, iter [04700, 05004], lr: 0.003556, loss: 4.1784
2022-10-12 23:58:12 - train: epoch 0088, iter [04800, 05004], lr: 0.003544, loss: 3.2515
2022-10-12 23:59:02 - train: epoch 0088, iter [04900, 05004], lr: 0.003533, loss: 3.3269
2022-10-12 23:59:50 - train: epoch 0088, iter [05000, 05004], lr: 0.003521, loss: 3.2923
2022-10-12 23:59:53 - train: epoch 088, train_loss: 3.5897
2022-10-13 00:01:41 - eval: epoch: 088, acc1: 74.768%, acc5: 92.408%, test_loss: 1.3034, per_image_load_time: 3.588ms, per_image_inference_time: 0.555ms
2022-10-13 00:01:42 - until epoch: 088, best_acc1: 74.768%
2022-10-13 00:01:42 - epoch 089 lr: 0.003521
2022-10-13 00:02:38 - train: epoch 0089, iter [00100, 05004], lr: 0.003509, loss: 3.4658
2022-10-13 00:03:27 - train: epoch 0089, iter [00200, 05004], lr: 0.003498, loss: 3.6636
2022-10-13 00:04:18 - train: epoch 0089, iter [00300, 05004], lr: 0.003486, loss: 4.1007
2022-10-13 00:05:05 - train: epoch 0089, iter [00400, 05004], lr: 0.003475, loss: 3.3401
2022-10-13 00:05:56 - train: epoch 0089, iter [00500, 05004], lr: 0.003463, loss: 3.2841
2022-10-13 00:06:46 - train: epoch 0089, iter [00600, 05004], lr: 0.003452, loss: 3.6415
2022-10-13 00:07:35 - train: epoch 0089, iter [00700, 05004], lr: 0.003440, loss: 4.5570
2022-10-13 00:08:25 - train: epoch 0089, iter [00800, 05004], lr: 0.003429, loss: 3.4792
2022-10-13 00:09:16 - train: epoch 0089, iter [00900, 05004], lr: 0.003418, loss: 3.1649
2022-10-13 00:10:05 - train: epoch 0089, iter [01000, 05004], lr: 0.003406, loss: 3.5875
2022-10-13 00:10:56 - train: epoch 0089, iter [01100, 05004], lr: 0.003395, loss: 3.4861
2022-10-13 00:11:47 - train: epoch 0089, iter [01200, 05004], lr: 0.003383, loss: 3.1371
2022-10-13 00:12:37 - train: epoch 0089, iter [01300, 05004], lr: 0.003372, loss: 3.6718
2022-10-13 00:13:28 - train: epoch 0089, iter [01400, 05004], lr: 0.003361, loss: 4.0603
2022-10-13 00:14:19 - train: epoch 0089, iter [01500, 05004], lr: 0.003350, loss: 3.3077
2022-10-13 00:15:09 - train: epoch 0089, iter [01600, 05004], lr: 0.003338, loss: 3.2954
2022-10-13 00:15:57 - train: epoch 0089, iter [01700, 05004], lr: 0.003327, loss: 3.4341
2022-10-13 00:16:47 - train: epoch 0089, iter [01800, 05004], lr: 0.003316, loss: 4.1160
2022-10-13 00:17:37 - train: epoch 0089, iter [01900, 05004], lr: 0.003305, loss: 3.3712
2022-10-13 00:18:26 - train: epoch 0089, iter [02000, 05004], lr: 0.003293, loss: 3.9194
2022-10-13 00:19:15 - train: epoch 0089, iter [02100, 05004], lr: 0.003282, loss: 3.2704
2022-10-13 00:20:07 - train: epoch 0089, iter [02200, 05004], lr: 0.003271, loss: 3.7015
2022-10-13 00:20:56 - train: epoch 0089, iter [02300, 05004], lr: 0.003260, loss: 3.8780
2022-10-13 00:21:43 - train: epoch 0089, iter [02400, 05004], lr: 0.003249, loss: 4.0207
2022-10-13 00:22:33 - train: epoch 0089, iter [02500, 05004], lr: 0.003238, loss: 3.8943
2022-10-13 00:23:25 - train: epoch 0089, iter [02600, 05004], lr: 0.003227, loss: 2.4509
2022-10-13 00:24:15 - train: epoch 0089, iter [02700, 05004], lr: 0.003216, loss: 3.1431
2022-10-13 00:25:06 - train: epoch 0089, iter [02800, 05004], lr: 0.003204, loss: 3.0482
2022-10-13 00:25:56 - train: epoch 0089, iter [02900, 05004], lr: 0.003193, loss: 4.1324
2022-10-13 00:26:47 - train: epoch 0089, iter [03000, 05004], lr: 0.003182, loss: 3.0675
2022-10-13 00:27:36 - train: epoch 0089, iter [03100, 05004], lr: 0.003171, loss: 3.0687
2022-10-13 00:28:25 - train: epoch 0089, iter [03200, 05004], lr: 0.003160, loss: 3.9147
2022-10-13 00:29:14 - train: epoch 0089, iter [03300, 05004], lr: 0.003150, loss: 4.2103
2022-10-13 00:30:05 - train: epoch 0089, iter [03400, 05004], lr: 0.003139, loss: 3.8934
2022-10-13 00:30:52 - train: epoch 0089, iter [03500, 05004], lr: 0.003128, loss: 2.9688
2022-10-13 00:31:44 - train: epoch 0089, iter [03600, 05004], lr: 0.003117, loss: 4.1019
2022-10-13 00:32:35 - train: epoch 0089, iter [03700, 05004], lr: 0.003106, loss: 3.6412
2022-10-13 00:33:26 - train: epoch 0089, iter [03800, 05004], lr: 0.003095, loss: 2.8602
2022-10-13 00:34:15 - train: epoch 0089, iter [03900, 05004], lr: 0.003084, loss: 3.4258
2022-10-13 00:35:04 - train: epoch 0089, iter [04000, 05004], lr: 0.003073, loss: 3.3834
2022-10-13 00:35:53 - train: epoch 0089, iter [04100, 05004], lr: 0.003063, loss: 3.9732
2022-10-13 00:36:44 - train: epoch 0089, iter [04200, 05004], lr: 0.003052, loss: 3.3597
2022-10-13 00:37:35 - train: epoch 0089, iter [04300, 05004], lr: 0.003041, loss: 4.2966
2022-10-13 00:38:26 - train: epoch 0089, iter [04400, 05004], lr: 0.003030, loss: 3.9609
2022-10-13 00:39:17 - train: epoch 0089, iter [04500, 05004], lr: 0.003019, loss: 3.1448
2022-10-13 00:40:08 - train: epoch 0089, iter [04600, 05004], lr: 0.003009, loss: 4.0871
2022-10-13 00:40:57 - train: epoch 0089, iter [04700, 05004], lr: 0.002998, loss: 3.5194
2022-10-13 00:41:47 - train: epoch 0089, iter [04800, 05004], lr: 0.002987, loss: 3.6438
2022-10-13 00:42:37 - train: epoch 0089, iter [04900, 05004], lr: 0.002977, loss: 3.1171
2022-10-13 00:43:22 - train: epoch 0089, iter [05000, 05004], lr: 0.002966, loss: 3.0159
2022-10-13 00:43:24 - train: epoch 089, train_loss: 3.5649
2022-10-13 00:45:10 - eval: epoch: 089, acc1: 74.982%, acc5: 92.688%, test_loss: 1.2643, per_image_load_time: 3.421ms, per_image_inference_time: 0.535ms
2022-10-13 00:45:11 - until epoch: 089, best_acc1: 74.982%
2022-10-13 00:45:11 - epoch 090 lr: 0.002966
2022-10-13 00:46:07 - train: epoch 0090, iter [00100, 05004], lr: 0.002955, loss: 3.3305
2022-10-13 00:46:58 - train: epoch 0090, iter [00200, 05004], lr: 0.002944, loss: 3.3662
2022-10-13 00:47:48 - train: epoch 0090, iter [00300, 05004], lr: 0.002934, loss: 3.3083
2022-10-13 00:48:37 - train: epoch 0090, iter [00400, 05004], lr: 0.002923, loss: 2.6015
2022-10-13 00:49:26 - train: epoch 0090, iter [00500, 05004], lr: 0.002913, loss: 3.3515
2022-10-13 00:50:15 - train: epoch 0090, iter [00600, 05004], lr: 0.002902, loss: 3.9191
2022-10-13 00:51:06 - train: epoch 0090, iter [00700, 05004], lr: 0.002892, loss: 3.3546
2022-10-13 00:51:58 - train: epoch 0090, iter [00800, 05004], lr: 0.002881, loss: 3.9231
2022-10-13 00:52:48 - train: epoch 0090, iter [00900, 05004], lr: 0.002871, loss: 3.8175
2022-10-13 00:53:38 - train: epoch 0090, iter [01000, 05004], lr: 0.002860, loss: 3.4146
2022-10-13 00:54:27 - train: epoch 0090, iter [01100, 05004], lr: 0.002850, loss: 3.3794
2022-10-13 00:55:16 - train: epoch 0090, iter [01200, 05004], lr: 0.002839, loss: 3.6015
2022-10-13 00:56:05 - train: epoch 0090, iter [01300, 05004], lr: 0.002829, loss: 3.7967
2022-10-13 00:56:54 - train: epoch 0090, iter [01400, 05004], lr: 0.002819, loss: 3.4562
2022-10-13 00:57:44 - train: epoch 0090, iter [01500, 05004], lr: 0.002808, loss: 3.3516
2022-10-13 00:58:34 - train: epoch 0090, iter [01600, 05004], lr: 0.002798, loss: 3.2709
2022-10-13 00:59:23 - train: epoch 0090, iter [01700, 05004], lr: 0.002788, loss: 3.3972
2022-10-13 01:00:14 - train: epoch 0090, iter [01800, 05004], lr: 0.002777, loss: 3.0134
2022-10-13 01:01:05 - train: epoch 0090, iter [01900, 05004], lr: 0.002767, loss: 3.7702
2022-10-13 01:01:54 - train: epoch 0090, iter [02000, 05004], lr: 0.002757, loss: 4.3172
2022-10-13 01:02:43 - train: epoch 0090, iter [02100, 05004], lr: 0.002746, loss: 3.4735
2022-10-13 01:03:32 - train: epoch 0090, iter [02200, 05004], lr: 0.002736, loss: 3.2143
2022-10-13 01:04:23 - train: epoch 0090, iter [02300, 05004], lr: 0.002726, loss: 3.5026
2022-10-13 01:05:13 - train: epoch 0090, iter [02400, 05004], lr: 0.002716, loss: 3.4527
2022-10-13 01:06:05 - train: epoch 0090, iter [02500, 05004], lr: 0.002706, loss: 3.0917
2022-10-13 01:06:52 - train: epoch 0090, iter [02600, 05004], lr: 0.002696, loss: 3.8437
2022-10-13 01:07:41 - train: epoch 0090, iter [02700, 05004], lr: 0.002685, loss: 3.6208
2022-10-13 01:08:31 - train: epoch 0090, iter [02800, 05004], lr: 0.002675, loss: 3.8443
2022-10-13 01:09:19 - train: epoch 0090, iter [02900, 05004], lr: 0.002665, loss: 2.7918
2022-10-13 01:10:09 - train: epoch 0090, iter [03000, 05004], lr: 0.002655, loss: 3.9568
2022-10-13 01:10:59 - train: epoch 0090, iter [03100, 05004], lr: 0.002645, loss: 2.8857
2022-10-13 01:11:49 - train: epoch 0090, iter [03200, 05004], lr: 0.002635, loss: 3.0266
2022-10-13 01:12:39 - train: epoch 0090, iter [03300, 05004], lr: 0.002625, loss: 3.7661
2022-10-13 01:13:30 - train: epoch 0090, iter [03400, 05004], lr: 0.002615, loss: 4.1386
2022-10-13 01:14:20 - train: epoch 0090, iter [03500, 05004], lr: 0.002605, loss: 3.1284
2022-10-13 01:15:10 - train: epoch 0090, iter [03600, 05004], lr: 0.002595, loss: 3.5096
2022-10-13 01:16:01 - train: epoch 0090, iter [03700, 05004], lr: 0.002585, loss: 3.3655
2022-10-13 01:16:48 - train: epoch 0090, iter [03800, 05004], lr: 0.002575, loss: 3.7414
2022-10-13 01:17:37 - train: epoch 0090, iter [03900, 05004], lr: 0.002565, loss: 3.1550
2022-10-13 01:18:27 - train: epoch 0090, iter [04000, 05004], lr: 0.002555, loss: 4.2271
2022-10-13 01:19:19 - train: epoch 0090, iter [04100, 05004], lr: 0.002545, loss: 3.8941
2022-10-13 01:20:06 - train: epoch 0090, iter [04200, 05004], lr: 0.002536, loss: 4.2433
2022-10-13 01:20:54 - train: epoch 0090, iter [04300, 05004], lr: 0.002526, loss: 3.3683
2022-10-13 01:21:44 - train: epoch 0090, iter [04400, 05004], lr: 0.002516, loss: 4.1236
2022-10-13 01:22:35 - train: epoch 0090, iter [04500, 05004], lr: 0.002506, loss: 3.1015
2022-10-13 01:23:24 - train: epoch 0090, iter [04600, 05004], lr: 0.002496, loss: 3.4446
2022-10-13 01:24:13 - train: epoch 0090, iter [04700, 05004], lr: 0.002487, loss: 3.9324
2022-10-13 01:25:04 - train: epoch 0090, iter [04800, 05004], lr: 0.002477, loss: 3.8178
2022-10-13 01:25:55 - train: epoch 0090, iter [04900, 05004], lr: 0.002467, loss: 3.5756
2022-10-13 01:26:42 - train: epoch 0090, iter [05000, 05004], lr: 0.002457, loss: 4.0207
2022-10-13 01:26:45 - train: epoch 090, train_loss: 3.5432
2022-10-13 01:28:32 - eval: epoch: 090, acc1: 75.354%, acc5: 92.776%, test_loss: 1.2723, per_image_load_time: 3.580ms, per_image_inference_time: 0.537ms
2022-10-13 01:28:33 - until epoch: 090, best_acc1: 75.354%
2022-10-13 01:28:33 - epoch 091 lr: 0.002457
2022-10-13 01:29:29 - train: epoch 0091, iter [00100, 05004], lr: 0.002447, loss: 2.8294
2022-10-13 01:30:18 - train: epoch 0091, iter [00200, 05004], lr: 0.002438, loss: 3.2745
2022-10-13 01:31:06 - train: epoch 0091, iter [00300, 05004], lr: 0.002428, loss: 3.5840
2022-10-13 01:31:57 - train: epoch 0091, iter [00400, 05004], lr: 0.002418, loss: 3.5644
2022-10-13 01:32:47 - train: epoch 0091, iter [00500, 05004], lr: 0.002409, loss: 3.2710
2022-10-13 01:33:34 - train: epoch 0091, iter [00600, 05004], lr: 0.002399, loss: 3.6626
2022-10-13 01:34:23 - train: epoch 0091, iter [00700, 05004], lr: 0.002389, loss: 3.7494
2022-10-13 01:35:13 - train: epoch 0091, iter [00800, 05004], lr: 0.002380, loss: 2.8970
2022-10-13 01:36:01 - train: epoch 0091, iter [00900, 05004], lr: 0.002370, loss: 3.9205
2022-10-13 01:36:52 - train: epoch 0091, iter [01000, 05004], lr: 0.002361, loss: 3.4437
2022-10-13 01:37:41 - train: epoch 0091, iter [01100, 05004], lr: 0.002351, loss: 3.4079
2022-10-13 01:38:32 - train: epoch 0091, iter [01200, 05004], lr: 0.002342, loss: 3.6873
2022-10-13 01:39:24 - train: epoch 0091, iter [01300, 05004], lr: 0.002332, loss: 3.3907
2022-10-13 01:40:13 - train: epoch 0091, iter [01400, 05004], lr: 0.002323, loss: 3.6629
2022-10-13 01:41:05 - train: epoch 0091, iter [01500, 05004], lr: 0.002314, loss: 3.6240
2022-10-13 01:41:55 - train: epoch 0091, iter [01600, 05004], lr: 0.002304, loss: 3.2573
2022-10-13 01:42:45 - train: epoch 0091, iter [01700, 05004], lr: 0.002295, loss: 2.7550
2022-10-13 01:43:35 - train: epoch 0091, iter [01800, 05004], lr: 0.002285, loss: 3.3528
2022-10-13 01:44:22 - train: epoch 0091, iter [01900, 05004], lr: 0.002276, loss: 3.7249
2022-10-13 01:45:11 - train: epoch 0091, iter [02000, 05004], lr: 0.002267, loss: 3.1925
2022-10-13 01:46:02 - train: epoch 0091, iter [02100, 05004], lr: 0.002257, loss: 3.3858
2022-10-13 01:46:49 - train: epoch 0091, iter [02200, 05004], lr: 0.002248, loss: 4.1391
2022-10-13 01:47:39 - train: epoch 0091, iter [02300, 05004], lr: 0.002239, loss: 3.5262
2022-10-13 01:48:30 - train: epoch 0091, iter [02400, 05004], lr: 0.002230, loss: 3.5261
2022-10-13 01:49:21 - train: epoch 0091, iter [02500, 05004], lr: 0.002220, loss: 4.1117
2022-10-13 01:50:11 - train: epoch 0091, iter [02600, 05004], lr: 0.002211, loss: 2.9490
2022-10-13 01:51:00 - train: epoch 0091, iter [02700, 05004], lr: 0.002202, loss: 3.5796
2022-10-13 01:51:51 - train: epoch 0091, iter [02800, 05004], lr: 0.002193, loss: 3.1090
2022-10-13 01:52:41 - train: epoch 0091, iter [02900, 05004], lr: 0.002184, loss: 3.6191
2022-10-13 01:53:32 - train: epoch 0091, iter [03000, 05004], lr: 0.002174, loss: 3.4419
2022-10-13 01:54:22 - train: epoch 0091, iter [03100, 05004], lr: 0.002165, loss: 3.4206
2022-10-13 01:55:11 - train: epoch 0091, iter [03200, 05004], lr: 0.002156, loss: 3.1341
2022-10-13 01:56:01 - train: epoch 0091, iter [03300, 05004], lr: 0.002147, loss: 3.2059
2022-10-13 01:56:53 - train: epoch 0091, iter [03400, 05004], lr: 0.002138, loss: 3.3078
2022-10-13 01:57:43 - train: epoch 0091, iter [03500, 05004], lr: 0.002129, loss: 3.1381
2022-10-13 01:58:29 - train: epoch 0091, iter [03600, 05004], lr: 0.002120, loss: 3.6412
2022-10-13 01:59:18 - train: epoch 0091, iter [03700, 05004], lr: 0.002111, loss: 3.8920
2022-10-13 02:00:08 - train: epoch 0091, iter [03800, 05004], lr: 0.002102, loss: 3.1475
2022-10-13 02:00:58 - train: epoch 0091, iter [03900, 05004], lr: 0.002093, loss: 3.4883
2022-10-13 02:01:49 - train: epoch 0091, iter [04000, 05004], lr: 0.002084, loss: 3.9011
2022-10-13 02:02:39 - train: epoch 0091, iter [04100, 05004], lr: 0.002075, loss: 3.5408
2022-10-13 02:03:30 - train: epoch 0091, iter [04200, 05004], lr: 0.002066, loss: 2.7712
2022-10-13 02:04:20 - train: epoch 0091, iter [04300, 05004], lr: 0.002057, loss: 4.0115
2022-10-13 02:05:10 - train: epoch 0091, iter [04400, 05004], lr: 0.002048, loss: 3.7384
2022-10-13 02:06:01 - train: epoch 0091, iter [04500, 05004], lr: 0.002039, loss: 3.4495
2022-10-13 02:06:51 - train: epoch 0091, iter [04600, 05004], lr: 0.002031, loss: 3.5248
2022-10-13 02:07:41 - train: epoch 0091, iter [04700, 05004], lr: 0.002022, loss: 3.6091
2022-10-13 02:08:31 - train: epoch 0091, iter [04800, 05004], lr: 0.002013, loss: 3.6675
2022-10-13 02:09:21 - train: epoch 0091, iter [04900, 05004], lr: 0.002004, loss: 2.5065
2022-10-13 02:10:10 - train: epoch 0091, iter [05000, 05004], lr: 0.001995, loss: 3.7810
2022-10-13 02:10:13 - train: epoch 091, train_loss: 3.5162
2022-10-13 02:11:57 - eval: epoch: 091, acc1: 75.376%, acc5: 92.984%, test_loss: 1.2448, per_image_load_time: 3.407ms, per_image_inference_time: 0.531ms
2022-10-13 02:11:58 - until epoch: 091, best_acc1: 75.376%
2022-10-13 02:11:58 - epoch 092 lr: 0.001995
2022-10-13 02:12:52 - train: epoch 0092, iter [00100, 05004], lr: 0.001986, loss: 3.3016
2022-10-13 02:13:43 - train: epoch 0092, iter [00200, 05004], lr: 0.001978, loss: 3.1627
2022-10-13 02:14:34 - train: epoch 0092, iter [00300, 05004], lr: 0.001969, loss: 3.3818
2022-10-13 02:15:25 - train: epoch 0092, iter [00400, 05004], lr: 0.001960, loss: 3.6647
2022-10-13 02:16:16 - train: epoch 0092, iter [00500, 05004], lr: 0.001952, loss: 3.6882
2022-10-13 02:17:06 - train: epoch 0092, iter [00600, 05004], lr: 0.001943, loss: 3.6751
2022-10-13 02:17:54 - train: epoch 0092, iter [00700, 05004], lr: 0.001934, loss: 3.7122
2022-10-13 02:18:45 - train: epoch 0092, iter [00800, 05004], lr: 0.001926, loss: 3.2499
2022-10-13 02:19:35 - train: epoch 0092, iter [00900, 05004], lr: 0.001917, loss: 3.3051
2022-10-13 02:20:26 - train: epoch 0092, iter [01000, 05004], lr: 0.001908, loss: 3.4487
2022-10-13 02:21:16 - train: epoch 0092, iter [01100, 05004], lr: 0.001900, loss: 2.2829
2022-10-13 02:22:07 - train: epoch 0092, iter [01200, 05004], lr: 0.001891, loss: 3.3270
2022-10-13 02:22:57 - train: epoch 0092, iter [01300, 05004], lr: 0.001883, loss: 3.7818
2022-10-13 02:23:46 - train: epoch 0092, iter [01400, 05004], lr: 0.001874, loss: 2.8118
2022-10-13 02:24:34 - train: epoch 0092, iter [01500, 05004], lr: 0.001866, loss: 3.4684
2022-10-13 02:25:24 - train: epoch 0092, iter [01600, 05004], lr: 0.001857, loss: 3.7888
2022-10-13 02:26:11 - train: epoch 0092, iter [01700, 05004], lr: 0.001849, loss: 4.0346
2022-10-13 02:27:01 - train: epoch 0092, iter [01800, 05004], lr: 0.001841, loss: 3.1907
2022-10-13 02:27:52 - train: epoch 0092, iter [01900, 05004], lr: 0.001832, loss: 3.6720
2022-10-13 02:28:43 - train: epoch 0092, iter [02000, 05004], lr: 0.001824, loss: 3.1054
2022-10-13 02:29:32 - train: epoch 0092, iter [02100, 05004], lr: 0.001815, loss: 3.3866
2022-10-13 02:30:23 - train: epoch 0092, iter [02200, 05004], lr: 0.001807, loss: 3.7322
2022-10-13 02:31:14 - train: epoch 0092, iter [02300, 05004], lr: 0.001799, loss: 3.6777
2022-10-13 02:32:03 - train: epoch 0092, iter [02400, 05004], lr: 0.001790, loss: 3.8317
2022-10-13 02:32:54 - train: epoch 0092, iter [02500, 05004], lr: 0.001782, loss: 3.6646
2022-10-13 02:33:44 - train: epoch 0092, iter [02600, 05004], lr: 0.001774, loss: 3.4870
2022-10-13 02:34:34 - train: epoch 0092, iter [02700, 05004], lr: 0.001766, loss: 3.3428
2022-10-13 02:35:25 - train: epoch 0092, iter [02800, 05004], lr: 0.001757, loss: 3.4780
2022-10-13 02:36:16 - train: epoch 0092, iter [02900, 05004], lr: 0.001749, loss: 3.5585
2022-10-13 02:37:04 - train: epoch 0092, iter [03000, 05004], lr: 0.001741, loss: 3.9909
2022-10-13 02:37:51 - train: epoch 0092, iter [03100, 05004], lr: 0.001733, loss: 3.4058
2022-10-13 02:38:42 - train: epoch 0092, iter [03200, 05004], lr: 0.001725, loss: 3.5582
2022-10-13 02:39:29 - train: epoch 0092, iter [03300, 05004], lr: 0.001716, loss: 3.7292
2022-10-13 02:40:18 - train: epoch 0092, iter [03400, 05004], lr: 0.001708, loss: 3.1725
2022-10-13 02:41:09 - train: epoch 0092, iter [03500, 05004], lr: 0.001700, loss: 3.6446
2022-10-13 02:42:00 - train: epoch 0092, iter [03600, 05004], lr: 0.001692, loss: 3.9427
2022-10-13 02:42:50 - train: epoch 0092, iter [03700, 05004], lr: 0.001684, loss: 3.5751
2022-10-13 02:43:40 - train: epoch 0092, iter [03800, 05004], lr: 0.001676, loss: 3.4255
2022-10-13 02:44:30 - train: epoch 0092, iter [03900, 05004], lr: 0.001668, loss: 2.6536
2022-10-13 02:45:20 - train: epoch 0092, iter [04000, 05004], lr: 0.001660, loss: 4.0397
2022-10-13 02:46:11 - train: epoch 0092, iter [04100, 05004], lr: 0.001652, loss: 3.5235
2022-10-13 02:47:02 - train: epoch 0092, iter [04200, 05004], lr: 0.001644, loss: 3.0477
2022-10-13 02:47:52 - train: epoch 0092, iter [04300, 05004], lr: 0.001636, loss: 3.5667
2022-10-13 02:48:42 - train: epoch 0092, iter [04400, 05004], lr: 0.001628, loss: 3.1628
2022-10-13 02:49:32 - train: epoch 0092, iter [04500, 05004], lr: 0.001620, loss: 2.3457
2022-10-13 02:50:19 - train: epoch 0092, iter [04600, 05004], lr: 0.001612, loss: 3.5168
2022-10-13 02:51:08 - train: epoch 0092, iter [04700, 05004], lr: 0.001605, loss: 3.4986
2022-10-13 02:52:00 - train: epoch 0092, iter [04800, 05004], lr: 0.001597, loss: 3.8730
2022-10-13 02:52:48 - train: epoch 0092, iter [04900, 05004], lr: 0.001589, loss: 3.6361
2022-10-13 02:53:32 - train: epoch 0092, iter [05000, 05004], lr: 0.001581, loss: 3.7650
2022-10-13 02:53:35 - train: epoch 092, train_loss: 3.4964
2022-10-13 02:55:21 - eval: epoch: 092, acc1: 75.732%, acc5: 93.136%, test_loss: 1.2637, per_image_load_time: 3.573ms, per_image_inference_time: 0.511ms
2022-10-13 02:55:22 - until epoch: 092, best_acc1: 75.732%
2022-10-13 02:55:22 - epoch 093 lr: 0.001581
2022-10-13 02:56:19 - train: epoch 0093, iter [00100, 05004], lr: 0.001573, loss: 3.8994
2022-10-13 02:57:08 - train: epoch 0093, iter [00200, 05004], lr: 0.001565, loss: 3.8382
2022-10-13 02:57:59 - train: epoch 0093, iter [00300, 05004], lr: 0.001557, loss: 4.0054
2022-10-13 02:58:50 - train: epoch 0093, iter [00400, 05004], lr: 0.001550, loss: 4.4568
2022-10-13 02:59:41 - train: epoch 0093, iter [00500, 05004], lr: 0.001542, loss: 3.5879
2022-10-13 03:00:32 - train: epoch 0093, iter [00600, 05004], lr: 0.001534, loss: 3.7265
2022-10-13 03:01:22 - train: epoch 0093, iter [00700, 05004], lr: 0.001527, loss: 3.5587
2022-10-13 03:02:12 - train: epoch 0093, iter [00800, 05004], lr: 0.001519, loss: 3.5048
2022-10-13 03:03:00 - train: epoch 0093, iter [00900, 05004], lr: 0.001511, loss: 4.2766
2022-10-13 03:03:49 - train: epoch 0093, iter [01000, 05004], lr: 0.001504, loss: 3.0806
2022-10-13 03:04:38 - train: epoch 0093, iter [01100, 05004], lr: 0.001496, loss: 2.9262
2022-10-13 03:05:27 - train: epoch 0093, iter [01200, 05004], lr: 0.001488, loss: 3.9530
2022-10-13 03:06:17 - train: epoch 0093, iter [01300, 05004], lr: 0.001481, loss: 3.1661
2022-10-13 03:07:06 - train: epoch 0093, iter [01400, 05004], lr: 0.001473, loss: 3.6151
2022-10-13 03:07:54 - train: epoch 0093, iter [01500, 05004], lr: 0.001466, loss: 3.5744
2022-10-13 03:08:44 - train: epoch 0093, iter [01600, 05004], lr: 0.001458, loss: 2.8534
2022-10-13 03:09:35 - train: epoch 0093, iter [01700, 05004], lr: 0.001451, loss: 2.9445
2022-10-13 03:10:25 - train: epoch 0093, iter [01800, 05004], lr: 0.001443, loss: 3.8312
2022-10-13 03:11:15 - train: epoch 0093, iter [01900, 05004], lr: 0.001436, loss: 3.5080
2022-10-13 03:12:07 - train: epoch 0093, iter [02000, 05004], lr: 0.001428, loss: 3.5623
2022-10-13 03:12:57 - train: epoch 0093, iter [02100, 05004], lr: 0.001421, loss: 3.7571
2022-10-13 03:13:46 - train: epoch 0093, iter [02200, 05004], lr: 0.001414, loss: 3.7467
2022-10-13 03:14:37 - train: epoch 0093, iter [02300, 05004], lr: 0.001406, loss: 3.8815
2022-10-13 03:15:27 - train: epoch 0093, iter [02400, 05004], lr: 0.001399, loss: 3.4660
2022-10-13 03:16:15 - train: epoch 0093, iter [02500, 05004], lr: 0.001392, loss: 3.9869
2022-10-13 03:17:04 - train: epoch 0093, iter [02600, 05004], lr: 0.001384, loss: 4.2985
2022-10-13 03:17:52 - train: epoch 0093, iter [02700, 05004], lr: 0.001377, loss: 3.3396
2022-10-13 03:18:43 - train: epoch 0093, iter [02800, 05004], lr: 0.001370, loss: 3.3876
2022-10-13 03:19:33 - train: epoch 0093, iter [02900, 05004], lr: 0.001362, loss: 3.7792
2022-10-13 03:20:24 - train: epoch 0093, iter [03000, 05004], lr: 0.001355, loss: 2.8771
2022-10-13 03:21:12 - train: epoch 0093, iter [03100, 05004], lr: 0.001348, loss: 3.8467
2022-10-13 03:22:00 - train: epoch 0093, iter [03200, 05004], lr: 0.001341, loss: 3.5103
2022-10-13 03:22:50 - train: epoch 0093, iter [03300, 05004], lr: 0.001334, loss: 3.5776
2022-10-13 03:23:42 - train: epoch 0093, iter [03400, 05004], lr: 0.001326, loss: 3.2531
2022-10-13 03:24:32 - train: epoch 0093, iter [03500, 05004], lr: 0.001319, loss: 3.3842
2022-10-13 03:25:22 - train: epoch 0093, iter [03600, 05004], lr: 0.001312, loss: 3.9015
2022-10-13 03:26:12 - train: epoch 0093, iter [03700, 05004], lr: 0.001305, loss: 3.8578
2022-10-13 03:27:03 - train: epoch 0093, iter [03800, 05004], lr: 0.001298, loss: 3.1328
2022-10-13 03:27:53 - train: epoch 0093, iter [03900, 05004], lr: 0.001291, loss: 3.2391
2022-10-13 03:28:42 - train: epoch 0093, iter [04000, 05004], lr: 0.001284, loss: 2.6155
2022-10-13 03:29:32 - train: epoch 0093, iter [04100, 05004], lr: 0.001277, loss: 4.1849
2022-10-13 03:30:20 - train: epoch 0093, iter [04200, 05004], lr: 0.001270, loss: 3.8067
2022-10-13 03:31:08 - train: epoch 0093, iter [04300, 05004], lr: 0.001263, loss: 3.4356
2022-10-13 03:31:57 - train: epoch 0093, iter [04400, 05004], lr: 0.001256, loss: 3.6299
2022-10-13 03:32:48 - train: epoch 0093, iter [04500, 05004], lr: 0.001249, loss: 3.8214
2022-10-13 03:33:37 - train: epoch 0093, iter [04600, 05004], lr: 0.001242, loss: 2.8130
2022-10-13 03:34:28 - train: epoch 0093, iter [04700, 05004], lr: 0.001235, loss: 3.4197
2022-10-13 03:35:15 - train: epoch 0093, iter [04800, 05004], lr: 0.001228, loss: 2.8617
2022-10-13 03:36:04 - train: epoch 0093, iter [04900, 05004], lr: 0.001221, loss: 3.1701
2022-10-13 03:36:52 - train: epoch 0093, iter [05000, 05004], lr: 0.001214, loss: 3.5201
2022-10-13 03:36:55 - train: epoch 093, train_loss: 3.4873
2022-10-13 03:38:43 - eval: epoch: 093, acc1: 76.120%, acc5: 93.258%, test_loss: 1.2178, per_image_load_time: 3.550ms, per_image_inference_time: 0.547ms
2022-10-13 03:38:44 - until epoch: 093, best_acc1: 76.120%
2022-10-13 03:38:44 - epoch 094 lr: 0.001214
2022-10-13 03:39:40 - train: epoch 0094, iter [00100, 05004], lr: 0.001207, loss: 3.6708
2022-10-13 03:40:32 - train: epoch 0094, iter [00200, 05004], lr: 0.001200, loss: 4.1283
2022-10-13 03:41:23 - train: epoch 0094, iter [00300, 05004], lr: 0.001194, loss: 3.7263
2022-10-13 03:42:11 - train: epoch 0094, iter [00400, 05004], lr: 0.001187, loss: 3.4277
2022-10-13 03:42:58 - train: epoch 0094, iter [00500, 05004], lr: 0.001180, loss: 3.3066
2022-10-13 03:43:46 - train: epoch 0094, iter [00600, 05004], lr: 0.001173, loss: 3.9034
2022-10-13 03:44:37 - train: epoch 0094, iter [00700, 05004], lr: 0.001167, loss: 3.3010
2022-10-13 03:45:26 - train: epoch 0094, iter [00800, 05004], lr: 0.001160, loss: 3.7477
2022-10-13 03:46:19 - train: epoch 0094, iter [00900, 05004], lr: 0.001153, loss: 3.8947
2022-10-13 03:47:09 - train: epoch 0094, iter [01000, 05004], lr: 0.001147, loss: 3.6247
2022-10-13 03:47:58 - train: epoch 0094, iter [01100, 05004], lr: 0.001140, loss: 3.4703
2022-10-13 03:48:48 - train: epoch 0094, iter [01200, 05004], lr: 0.001133, loss: 3.3077
2022-10-13 03:49:36 - train: epoch 0094, iter [01300, 05004], lr: 0.001127, loss: 4.0652
2022-10-13 03:50:27 - train: epoch 0094, iter [01400, 05004], lr: 0.001120, loss: 3.6950
2022-10-13 03:51:16 - train: epoch 0094, iter [01500, 05004], lr: 0.001114, loss: 3.9582
2022-10-13 03:52:07 - train: epoch 0094, iter [01600, 05004], lr: 0.001107, loss: 3.4526
2022-10-13 03:52:57 - train: epoch 0094, iter [01700, 05004], lr: 0.001100, loss: 2.8545
2022-10-13 03:53:48 - train: epoch 0094, iter [01800, 05004], lr: 0.001094, loss: 3.5051
2022-10-13 03:54:37 - train: epoch 0094, iter [01900, 05004], lr: 0.001087, loss: 3.0463
2022-10-13 03:55:25 - train: epoch 0094, iter [02000, 05004], lr: 0.001081, loss: 3.5847
2022-10-13 03:56:12 - train: epoch 0094, iter [02100, 05004], lr: 0.001074, loss: 2.6133
2022-10-13 03:57:03 - train: epoch 0094, iter [02200, 05004], lr: 0.001068, loss: 3.9125
2022-10-13 03:57:54 - train: epoch 0094, iter [02300, 05004], lr: 0.001062, loss: 2.5958
2022-10-13 03:58:43 - train: epoch 0094, iter [02400, 05004], lr: 0.001055, loss: 3.6377
2022-10-13 03:59:33 - train: epoch 0094, iter [02500, 05004], lr: 0.001049, loss: 3.0798
2022-10-13 04:00:22 - train: epoch 0094, iter [02600, 05004], lr: 0.001043, loss: 3.5731
2022-10-13 04:01:13 - train: epoch 0094, iter [02700, 05004], lr: 0.001036, loss: 3.5336
2022-10-13 04:02:02 - train: epoch 0094, iter [02800, 05004], lr: 0.001030, loss: 2.5087
2022-10-13 04:02:53 - train: epoch 0094, iter [02900, 05004], lr: 0.001024, loss: 3.2658
2022-10-13 04:03:44 - train: epoch 0094, iter [03000, 05004], lr: 0.001017, loss: 3.7674
2022-10-13 04:04:34 - train: epoch 0094, iter [03100, 05004], lr: 0.001011, loss: 4.0551
2022-10-13 04:05:24 - train: epoch 0094, iter [03200, 05004], lr: 0.001005, loss: 3.0057
2022-10-13 04:06:16 - train: epoch 0094, iter [03300, 05004], lr: 0.000999, loss: 3.1672
2022-10-13 04:07:03 - train: epoch 0094, iter [03400, 05004], lr: 0.000992, loss: 3.2101
2022-10-13 04:07:54 - train: epoch 0094, iter [03500, 05004], lr: 0.000986, loss: 3.7230
2022-10-13 04:08:43 - train: epoch 0094, iter [03600, 05004], lr: 0.000980, loss: 3.1189
2022-10-13 04:09:33 - train: epoch 0094, iter [03700, 05004], lr: 0.000974, loss: 2.8561
2022-10-13 04:10:22 - train: epoch 0094, iter [03800, 05004], lr: 0.000968, loss: 2.5509
2022-10-13 04:11:14 - train: epoch 0094, iter [03900, 05004], lr: 0.000962, loss: 3.2136
2022-10-13 04:12:04 - train: epoch 0094, iter [04000, 05004], lr: 0.000956, loss: 3.8268
2022-10-13 04:12:53 - train: epoch 0094, iter [04100, 05004], lr: 0.000950, loss: 3.1603
2022-10-13 04:13:43 - train: epoch 0094, iter [04200, 05004], lr: 0.000943, loss: 3.4597
2022-10-13 04:14:33 - train: epoch 0094, iter [04300, 05004], lr: 0.000937, loss: 3.8774
2022-10-13 04:15:24 - train: epoch 0094, iter [04400, 05004], lr: 0.000931, loss: 3.7359
2022-10-13 04:16:11 - train: epoch 0094, iter [04500, 05004], lr: 0.000925, loss: 3.6598
2022-10-13 04:17:02 - train: epoch 0094, iter [04600, 05004], lr: 0.000919, loss: 3.8139
2022-10-13 04:17:52 - train: epoch 0094, iter [04700, 05004], lr: 0.000914, loss: 3.3885
2022-10-13 04:18:41 - train: epoch 0094, iter [04800, 05004], lr: 0.000908, loss: 3.5757
2022-10-13 04:19:31 - train: epoch 0094, iter [04900, 05004], lr: 0.000902, loss: 3.4268
2022-10-13 04:20:17 - train: epoch 0094, iter [05000, 05004], lr: 0.000896, loss: 3.8537
2022-10-13 04:20:20 - train: epoch 094, train_loss: 3.4653
2022-10-13 04:22:03 - eval: epoch: 094, acc1: 76.372%, acc5: 93.352%, test_loss: 1.1651, per_image_load_time: 3.402ms, per_image_inference_time: 0.534ms
2022-10-13 04:22:04 - until epoch: 094, best_acc1: 76.372%
2022-10-13 04:22:04 - epoch 095 lr: 0.000896
2022-10-13 04:23:00 - train: epoch 0095, iter [00100, 05004], lr: 0.000890, loss: 3.1479
2022-10-13 04:23:50 - train: epoch 0095, iter [00200, 05004], lr: 0.000884, loss: 3.9767
2022-10-13 04:24:40 - train: epoch 0095, iter [00300, 05004], lr: 0.000878, loss: 3.7421
2022-10-13 04:25:30 - train: epoch 0095, iter [00400, 05004], lr: 0.000872, loss: 3.7306
2022-10-13 04:26:21 - train: epoch 0095, iter [00500, 05004], lr: 0.000866, loss: 3.7365
2022-10-13 04:27:12 - train: epoch 0095, iter [00600, 05004], lr: 0.000861, loss: 4.1001
2022-10-13 04:28:01 - train: epoch 0095, iter [00700, 05004], lr: 0.000855, loss: 3.9440
2022-10-13 04:28:52 - train: epoch 0095, iter [00800, 05004], lr: 0.000849, loss: 3.1321
2022-10-13 04:29:41 - train: epoch 0095, iter [00900, 05004], lr: 0.000843, loss: 4.0562
2022-10-13 04:30:29 - train: epoch 0095, iter [01000, 05004], lr: 0.000838, loss: 3.4930
2022-10-13 04:31:18 - train: epoch 0095, iter [01100, 05004], lr: 0.000832, loss: 3.3715
2022-10-13 04:32:09 - train: epoch 0095, iter [01200, 05004], lr: 0.000826, loss: 2.3808
2022-10-13 04:32:59 - train: epoch 0095, iter [01300, 05004], lr: 0.000821, loss: 3.4552
2022-10-13 04:33:46 - train: epoch 0095, iter [01400, 05004], lr: 0.000815, loss: 3.1631
2022-10-13 04:34:35 - train: epoch 0095, iter [01500, 05004], lr: 0.000810, loss: 3.4892
2022-10-13 04:35:25 - train: epoch 0095, iter [01600, 05004], lr: 0.000804, loss: 4.0039
2022-10-13 04:36:16 - train: epoch 0095, iter [01700, 05004], lr: 0.000798, loss: 3.2785
2022-10-13 04:37:06 - train: epoch 0095, iter [01800, 05004], lr: 0.000793, loss: 3.6245
2022-10-13 04:37:56 - train: epoch 0095, iter [01900, 05004], lr: 0.000787, loss: 3.6311
2022-10-13 04:38:46 - train: epoch 0095, iter [02000, 05004], lr: 0.000782, loss: 2.6792
2022-10-13 04:39:36 - train: epoch 0095, iter [02100, 05004], lr: 0.000776, loss: 3.0344
2022-10-13 04:40:28 - train: epoch 0095, iter [02200, 05004], lr: 0.000771, loss: 3.1136
2022-10-13 04:41:18 - train: epoch 0095, iter [02300, 05004], lr: 0.000765, loss: 3.3803
2022-10-13 04:42:08 - train: epoch 0095, iter [02400, 05004], lr: 0.000760, loss: 3.0864
2022-10-13 04:42:59 - train: epoch 0095, iter [02500, 05004], lr: 0.000755, loss: 3.6461
2022-10-13 04:43:46 - train: epoch 0095, iter [02600, 05004], lr: 0.000749, loss: 3.5766
2022-10-13 04:44:37 - train: epoch 0095, iter [02700, 05004], lr: 0.000744, loss: 4.2525
2022-10-13 04:45:27 - train: epoch 0095, iter [02800, 05004], lr: 0.000738, loss: 3.4142
2022-10-13 04:46:15 - train: epoch 0095, iter [02900, 05004], lr: 0.000733, loss: 3.2989
2022-10-13 04:47:05 - train: epoch 0095, iter [03000, 05004], lr: 0.000728, loss: 4.1232
2022-10-13 04:47:55 - train: epoch 0095, iter [03100, 05004], lr: 0.000723, loss: 3.7929
2022-10-13 04:48:44 - train: epoch 0095, iter [03200, 05004], lr: 0.000717, loss: 3.7328
2022-10-13 04:49:35 - train: epoch 0095, iter [03300, 05004], lr: 0.000712, loss: 3.0847
2022-10-13 04:50:26 - train: epoch 0095, iter [03400, 05004], lr: 0.000707, loss: 3.5183
2022-10-13 04:51:16 - train: epoch 0095, iter [03500, 05004], lr: 0.000702, loss: 3.3165
2022-10-13 04:52:07 - train: epoch 0095, iter [03600, 05004], lr: 0.000696, loss: 2.8841
2022-10-13 04:52:56 - train: epoch 0095, iter [03700, 05004], lr: 0.000691, loss: 3.3479
2022-10-13 04:53:47 - train: epoch 0095, iter [03800, 05004], lr: 0.000686, loss: 3.6842
2022-10-13 04:54:36 - train: epoch 0095, iter [03900, 05004], lr: 0.000681, loss: 3.6070
2022-10-13 04:55:27 - train: epoch 0095, iter [04000, 05004], lr: 0.000676, loss: 3.2226
2022-10-13 04:56:17 - train: epoch 0095, iter [04100, 05004], lr: 0.000671, loss: 3.3603
2022-10-13 04:57:07 - train: epoch 0095, iter [04200, 05004], lr: 0.000666, loss: 3.2921
2022-10-13 04:57:54 - train: epoch 0095, iter [04300, 05004], lr: 0.000661, loss: 3.1601
2022-10-13 04:58:43 - train: epoch 0095, iter [04400, 05004], lr: 0.000656, loss: 3.5056
2022-10-13 04:59:31 - train: epoch 0095, iter [04500, 05004], lr: 0.000651, loss: 3.7221
2022-10-13 05:00:19 - train: epoch 0095, iter [04600, 05004], lr: 0.000646, loss: 3.8289
2022-10-13 05:01:10 - train: epoch 0095, iter [04700, 05004], lr: 0.000641, loss: 3.2713
2022-10-13 05:01:59 - train: epoch 0095, iter [04800, 05004], lr: 0.000636, loss: 3.3205
2022-10-13 05:02:50 - train: epoch 0095, iter [04900, 05004], lr: 0.000631, loss: 3.4715
2022-10-13 05:03:37 - train: epoch 0095, iter [05000, 05004], lr: 0.000626, loss: 3.6394
2022-10-13 05:03:40 - train: epoch 095, train_loss: 3.4461
2022-10-13 05:05:27 - eval: epoch: 095, acc1: 76.466%, acc5: 93.398%, test_loss: 1.1877, per_image_load_time: 3.564ms, per_image_inference_time: 0.550ms
2022-10-13 05:05:28 - until epoch: 095, best_acc1: 76.466%
2022-10-13 05:05:28 - epoch 096 lr: 0.000626
2022-10-13 05:06:24 - train: epoch 0096, iter [00100, 05004], lr: 0.000621, loss: 3.6042
2022-10-13 05:07:16 - train: epoch 0096, iter [00200, 05004], lr: 0.000616, loss: 3.5482
2022-10-13 05:08:07 - train: epoch 0096, iter [00300, 05004], lr: 0.000611, loss: 3.2872
2022-10-13 05:08:56 - train: epoch 0096, iter [00400, 05004], lr: 0.000606, loss: 2.9507
2022-10-13 05:09:46 - train: epoch 0096, iter [00500, 05004], lr: 0.000601, loss: 2.9170
2022-10-13 05:10:37 - train: epoch 0096, iter [00600, 05004], lr: 0.000596, loss: 3.6115
2022-10-13 05:11:26 - train: epoch 0096, iter [00700, 05004], lr: 0.000592, loss: 3.2940
2022-10-13 05:12:14 - train: epoch 0096, iter [00800, 05004], lr: 0.000587, loss: 3.1827
2022-10-13 05:13:03 - train: epoch 0096, iter [00900, 05004], lr: 0.000582, loss: 3.5283
2022-10-13 05:13:53 - train: epoch 0096, iter [01000, 05004], lr: 0.000577, loss: 3.6290
2022-10-13 05:14:45 - train: epoch 0096, iter [01100, 05004], lr: 0.000573, loss: 3.1429
2022-10-13 05:15:35 - train: epoch 0096, iter [01200, 05004], lr: 0.000568, loss: 3.1052
2022-10-13 05:16:25 - train: epoch 0096, iter [01300, 05004], lr: 0.000563, loss: 3.6887
2022-10-13 05:17:16 - train: epoch 0096, iter [01400, 05004], lr: 0.000559, loss: 3.1531
2022-10-13 05:18:05 - train: epoch 0096, iter [01500, 05004], lr: 0.000554, loss: 3.8609
2022-10-13 05:18:58 - train: epoch 0096, iter [01600, 05004], lr: 0.000549, loss: 2.9523
2022-10-13 05:19:48 - train: epoch 0096, iter [01700, 05004], lr: 0.000545, loss: 2.9790
2022-10-13 05:20:38 - train: epoch 0096, iter [01800, 05004], lr: 0.000540, loss: 3.2354
2022-10-13 05:21:28 - train: epoch 0096, iter [01900, 05004], lr: 0.000536, loss: 3.3125
2022-10-13 05:22:18 - train: epoch 0096, iter [02000, 05004], lr: 0.000531, loss: 4.0818
2022-10-13 05:23:09 - train: epoch 0096, iter [02100, 05004], lr: 0.000527, loss: 3.7560
2022-10-13 05:24:00 - train: epoch 0096, iter [02200, 05004], lr: 0.000522, loss: 3.9013
2022-10-13 05:24:50 - train: epoch 0096, iter [02300, 05004], lr: 0.000518, loss: 3.1613
2022-10-13 05:25:36 - train: epoch 0096, iter [02400, 05004], lr: 0.000513, loss: 2.6898
2022-10-13 05:26:26 - train: epoch 0096, iter [02500, 05004], lr: 0.000509, loss: 3.6564
2022-10-13 05:27:18 - train: epoch 0096, iter [02600, 05004], lr: 0.000504, loss: 3.8440
2022-10-13 05:28:08 - train: epoch 0096, iter [02700, 05004], lr: 0.000500, loss: 3.7462
2022-10-13 05:28:59 - train: epoch 0096, iter [02800, 05004], lr: 0.000496, loss: 2.5708
2022-10-13 05:29:49 - train: epoch 0096, iter [02900, 05004], lr: 0.000491, loss: 2.7889
2022-10-13 05:30:38 - train: epoch 0096, iter [03000, 05004], lr: 0.000487, loss: 3.8361
2022-10-13 05:31:28 - train: epoch 0096, iter [03100, 05004], lr: 0.000483, loss: 3.5587
2022-10-13 05:32:18 - train: epoch 0096, iter [03200, 05004], lr: 0.000478, loss: 2.4218
2022-10-13 05:33:08 - train: epoch 0096, iter [03300, 05004], lr: 0.000474, loss: 2.7985
2022-10-13 05:33:57 - train: epoch 0096, iter [03400, 05004], lr: 0.000470, loss: 3.9774
2022-10-13 05:34:50 - train: epoch 0096, iter [03500, 05004], lr: 0.000466, loss: 3.5492
2022-10-13 05:35:40 - train: epoch 0096, iter [03600, 05004], lr: 0.000461, loss: 2.8438
2022-10-13 05:36:30 - train: epoch 0096, iter [03700, 05004], lr: 0.000457, loss: 3.1172
2022-10-13 05:37:20 - train: epoch 0096, iter [03800, 05004], lr: 0.000453, loss: 3.3365
2022-10-13 05:38:06 - train: epoch 0096, iter [03900, 05004], lr: 0.000449, loss: 3.4869
2022-10-13 05:38:57 - train: epoch 0096, iter [04000, 05004], lr: 0.000445, loss: 3.6501
2022-10-13 05:39:45 - train: epoch 0096, iter [04100, 05004], lr: 0.000441, loss: 3.1955
2022-10-13 05:40:34 - train: epoch 0096, iter [04200, 05004], lr: 0.000436, loss: 3.6227
2022-10-13 05:41:24 - train: epoch 0096, iter [04300, 05004], lr: 0.000432, loss: 3.0360
2022-10-13 05:42:14 - train: epoch 0096, iter [04400, 05004], lr: 0.000428, loss: 3.8825
2022-10-13 05:43:04 - train: epoch 0096, iter [04500, 05004], lr: 0.000424, loss: 3.9117
2022-10-13 05:43:55 - train: epoch 0096, iter [04600, 05004], lr: 0.000420, loss: 3.2189
2022-10-13 05:44:46 - train: epoch 0096, iter [04700, 05004], lr: 0.000416, loss: 3.3851
2022-10-13 05:45:36 - train: epoch 0096, iter [04800, 05004], lr: 0.000412, loss: 3.7194
2022-10-13 05:46:27 - train: epoch 0096, iter [04900, 05004], lr: 0.000408, loss: 3.6660
2022-10-13 05:47:14 - train: epoch 0096, iter [05000, 05004], lr: 0.000404, loss: 2.8205
2022-10-13 05:47:16 - train: epoch 096, train_loss: 3.4462
2022-10-13 05:49:03 - eval: epoch: 096, acc1: 76.614%, acc5: 93.404%, test_loss: 1.2027, per_image_load_time: 3.574ms, per_image_inference_time: 0.523ms
2022-10-13 05:49:03 - until epoch: 096, best_acc1: 76.614%
2022-10-13 05:49:03 - epoch 097 lr: 0.000404
2022-10-13 05:49:57 - train: epoch 0097, iter [00100, 05004], lr: 0.000400, loss: 3.3124
2022-10-13 05:50:46 - train: epoch 0097, iter [00200, 05004], lr: 0.000396, loss: 3.9558
2022-10-13 05:51:36 - train: epoch 0097, iter [00300, 05004], lr: 0.000393, loss: 3.6233
2022-10-13 05:52:26 - train: epoch 0097, iter [00400, 05004], lr: 0.000389, loss: 3.0004
2022-10-13 05:53:16 - train: epoch 0097, iter [00500, 05004], lr: 0.000385, loss: 3.9125
2022-10-13 05:54:05 - train: epoch 0097, iter [00600, 05004], lr: 0.000381, loss: 3.2466
2022-10-13 05:54:55 - train: epoch 0097, iter [00700, 05004], lr: 0.000377, loss: 3.5802
2022-10-13 05:55:46 - train: epoch 0097, iter [00800, 05004], lr: 0.000373, loss: 3.2367
2022-10-13 05:56:36 - train: epoch 0097, iter [00900, 05004], lr: 0.000370, loss: 3.8912
2022-10-13 05:57:25 - train: epoch 0097, iter [01000, 05004], lr: 0.000366, loss: 3.8349
2022-10-13 05:58:17 - train: epoch 0097, iter [01100, 05004], lr: 0.000362, loss: 3.4566
2022-10-13 05:59:07 - train: epoch 0097, iter [01200, 05004], lr: 0.000358, loss: 3.8770
2022-10-13 05:59:57 - train: epoch 0097, iter [01300, 05004], lr: 0.000355, loss: 3.4641
2022-10-13 06:00:47 - train: epoch 0097, iter [01400, 05004], lr: 0.000351, loss: 3.5216
2022-10-13 06:01:37 - train: epoch 0097, iter [01500, 05004], lr: 0.000347, loss: 3.8300
2022-10-13 06:02:27 - train: epoch 0097, iter [01600, 05004], lr: 0.000344, loss: 3.8623
2022-10-13 06:03:16 - train: epoch 0097, iter [01700, 05004], lr: 0.000340, loss: 3.5479
2022-10-13 06:04:05 - train: epoch 0097, iter [01800, 05004], lr: 0.000337, loss: 3.5020
2022-10-13 06:04:56 - train: epoch 0097, iter [01900, 05004], lr: 0.000333, loss: 3.2121
2022-10-13 06:05:46 - train: epoch 0097, iter [02000, 05004], lr: 0.000329, loss: 3.0255
2022-10-13 06:06:36 - train: epoch 0097, iter [02100, 05004], lr: 0.000326, loss: 3.6570
2022-10-13 06:07:24 - train: epoch 0097, iter [02200, 05004], lr: 0.000322, loss: 3.6153
2022-10-13 06:08:14 - train: epoch 0097, iter [02300, 05004], lr: 0.000319, loss: 2.7152
2022-10-13 06:09:03 - train: epoch 0097, iter [02400, 05004], lr: 0.000315, loss: 2.9467
2022-10-13 06:09:54 - train: epoch 0097, iter [02500, 05004], lr: 0.000312, loss: 3.3614
2022-10-13 06:10:44 - train: epoch 0097, iter [02600, 05004], lr: 0.000309, loss: 3.3270
2022-10-13 06:11:34 - train: epoch 0097, iter [02700, 05004], lr: 0.000305, loss: 3.4889
2022-10-13 06:12:25 - train: epoch 0097, iter [02800, 05004], lr: 0.000302, loss: 2.9243
2022-10-13 06:13:16 - train: epoch 0097, iter [02900, 05004], lr: 0.000298, loss: 3.9909
2022-10-13 06:14:06 - train: epoch 0097, iter [03000, 05004], lr: 0.000295, loss: 3.5153
2022-10-13 06:14:57 - train: epoch 0097, iter [03100, 05004], lr: 0.000292, loss: 3.4961
2022-10-13 06:15:47 - train: epoch 0097, iter [03200, 05004], lr: 0.000288, loss: 3.1052
2022-10-13 06:16:34 - train: epoch 0097, iter [03300, 05004], lr: 0.000285, loss: 3.6674
2022-10-13 06:17:24 - train: epoch 0097, iter [03400, 05004], lr: 0.000282, loss: 3.0563
2022-10-13 06:18:14 - train: epoch 0097, iter [03500, 05004], lr: 0.000279, loss: 3.6713
2022-10-13 06:19:04 - train: epoch 0097, iter [03600, 05004], lr: 0.000275, loss: 3.2803
2022-10-13 06:19:54 - train: epoch 0097, iter [03700, 05004], lr: 0.000272, loss: 3.8551
2022-10-13 06:20:41 - train: epoch 0097, iter [03800, 05004], lr: 0.000269, loss: 3.6246
2022-10-13 06:21:30 - train: epoch 0097, iter [03900, 05004], lr: 0.000266, loss: 3.2254
2022-10-13 06:22:21 - train: epoch 0097, iter [04000, 05004], lr: 0.000263, loss: 2.8213
2022-10-13 06:23:12 - train: epoch 0097, iter [04100, 05004], lr: 0.000259, loss: 2.9494
2022-10-13 06:24:02 - train: epoch 0097, iter [04200, 05004], lr: 0.000256, loss: 2.9068
2022-10-13 06:24:51 - train: epoch 0097, iter [04300, 05004], lr: 0.000253, loss: 3.5664
2022-10-13 06:25:42 - train: epoch 0097, iter [04400, 05004], lr: 0.000250, loss: 3.6233
2022-10-13 06:26:32 - train: epoch 0097, iter [04500, 05004], lr: 0.000247, loss: 2.8647
2022-10-13 06:27:22 - train: epoch 0097, iter [04600, 05004], lr: 0.000244, loss: 3.5939
2022-10-13 06:28:12 - train: epoch 0097, iter [04700, 05004], lr: 0.000241, loss: 3.8767
2022-10-13 06:29:00 - train: epoch 0097, iter [04800, 05004], lr: 0.000238, loss: 3.1298
2022-10-13 06:29:48 - train: epoch 0097, iter [04900, 05004], lr: 0.000235, loss: 4.1141
2022-10-13 06:30:35 - train: epoch 0097, iter [05000, 05004], lr: 0.000232, loss: 2.9704
2022-10-13 06:30:37 - train: epoch 097, train_loss: 3.4389
2022-10-13 06:32:24 - eval: epoch: 097, acc1: 76.612%, acc5: 93.488%, test_loss: 1.1827, per_image_load_time: 3.080ms, per_image_inference_time: 0.553ms
2022-10-13 06:32:25 - until epoch: 097, best_acc1: 76.614%
2022-10-13 06:32:25 - epoch 098 lr: 0.000232
2022-10-13 06:33:21 - train: epoch 0098, iter [00100, 05004], lr: 0.000229, loss: 3.0754
2022-10-13 06:34:12 - train: epoch 0098, iter [00200, 05004], lr: 0.000226, loss: 3.2495
2022-10-13 06:35:00 - train: epoch 0098, iter [00300, 05004], lr: 0.000223, loss: 3.3983
2022-10-13 06:35:49 - train: epoch 0098, iter [00400, 05004], lr: 0.000220, loss: 3.2477
2022-10-13 06:36:39 - train: epoch 0098, iter [00500, 05004], lr: 0.000217, loss: 2.9028
2022-10-13 06:37:30 - train: epoch 0098, iter [00600, 05004], lr: 0.000215, loss: 3.3577
2022-10-13 06:38:21 - train: epoch 0098, iter [00700, 05004], lr: 0.000212, loss: 3.2919
2022-10-13 06:39:12 - train: epoch 0098, iter [00800, 05004], lr: 0.000209, loss: 3.3252
2022-10-13 06:40:02 - train: epoch 0098, iter [00900, 05004], lr: 0.000206, loss: 3.1370
2022-10-13 06:40:53 - train: epoch 0098, iter [01000, 05004], lr: 0.000203, loss: 3.1648
2022-10-13 06:41:39 - train: epoch 0098, iter [01100, 05004], lr: 0.000201, loss: 3.2935
2022-10-13 06:42:28 - train: epoch 0098, iter [01200, 05004], lr: 0.000198, loss: 2.9378
2022-10-13 06:43:19 - train: epoch 0098, iter [01300, 05004], lr: 0.000195, loss: 3.7181
2022-10-13 06:44:08 - train: epoch 0098, iter [01400, 05004], lr: 0.000192, loss: 3.8035
2022-10-13 06:44:58 - train: epoch 0098, iter [01500, 05004], lr: 0.000190, loss: 3.3778
2022-10-13 06:45:49 - train: epoch 0098, iter [01600, 05004], lr: 0.000187, loss: 3.6111
2022-10-13 06:46:38 - train: epoch 0098, iter [01700, 05004], lr: 0.000185, loss: 3.5541
2022-10-13 06:47:28 - train: epoch 0098, iter [01800, 05004], lr: 0.000182, loss: 3.6823
2022-10-13 06:48:18 - train: epoch 0098, iter [01900, 05004], lr: 0.000179, loss: 2.8551
2022-10-13 06:49:05 - train: epoch 0098, iter [02000, 05004], lr: 0.000177, loss: 3.5154
2022-10-13 06:49:55 - train: epoch 0098, iter [02100, 05004], lr: 0.000174, loss: 3.0625
2022-10-13 06:50:46 - train: epoch 0098, iter [02200, 05004], lr: 0.000172, loss: 2.8612
2022-10-13 06:51:36 - train: epoch 0098, iter [02300, 05004], lr: 0.000169, loss: 2.8335
2022-10-13 06:52:27 - train: epoch 0098, iter [02400, 05004], lr: 0.000167, loss: 3.1694
2022-10-13 06:53:18 - train: epoch 0098, iter [02500, 05004], lr: 0.000164, loss: 3.2728
2022-10-13 06:54:05 - train: epoch 0098, iter [02600, 05004], lr: 0.000162, loss: 3.1427
2022-10-13 06:54:53 - train: epoch 0098, iter [02700, 05004], lr: 0.000159, loss: 3.8806
2022-10-13 06:55:43 - train: epoch 0098, iter [02800, 05004], lr: 0.000157, loss: 3.3971
2022-10-13 06:56:33 - train: epoch 0098, iter [02900, 05004], lr: 0.000154, loss: 3.5171
2022-10-13 06:57:23 - train: epoch 0098, iter [03000, 05004], lr: 0.000152, loss: 2.8992
2022-10-13 06:58:14 - train: epoch 0098, iter [03100, 05004], lr: 0.000150, loss: 3.8744
2022-10-13 06:59:04 - train: epoch 0098, iter [03200, 05004], lr: 0.000147, loss: 2.8600
2022-10-13 06:59:54 - train: epoch 0098, iter [03300, 05004], lr: 0.000145, loss: 3.3553
2022-10-13 07:00:46 - train: epoch 0098, iter [03400, 05004], lr: 0.000143, loss: 4.3150
2022-10-13 07:01:34 - train: epoch 0098, iter [03500, 05004], lr: 0.000141, loss: 3.2887
2022-10-13 07:02:22 - train: epoch 0098, iter [03600, 05004], lr: 0.000138, loss: 3.7260
2022-10-13 07:03:12 - train: epoch 0098, iter [03700, 05004], lr: 0.000136, loss: 3.6553
2022-10-13 07:04:02 - train: epoch 0098, iter [03800, 05004], lr: 0.000134, loss: 3.4292
2022-10-13 07:04:53 - train: epoch 0098, iter [03900, 05004], lr: 0.000132, loss: 3.7309
2022-10-13 07:05:42 - train: epoch 0098, iter [04000, 05004], lr: 0.000129, loss: 3.0538
2022-10-13 07:06:32 - train: epoch 0098, iter [04100, 05004], lr: 0.000127, loss: 4.0193
2022-10-13 07:07:21 - train: epoch 0098, iter [04200, 05004], lr: 0.000125, loss: 3.4938
2022-10-13 07:08:09 - train: epoch 0098, iter [04300, 05004], lr: 0.000123, loss: 3.2311
2022-10-13 07:08:59 - train: epoch 0098, iter [04400, 05004], lr: 0.000121, loss: 3.4478
2022-10-13 07:09:50 - train: epoch 0098, iter [04500, 05004], lr: 0.000119, loss: 2.6498
2022-10-13 07:10:40 - train: epoch 0098, iter [04600, 05004], lr: 0.000117, loss: 2.8647
2022-10-13 07:11:31 - train: epoch 0098, iter [04700, 05004], lr: 0.000115, loss: 3.3069
2022-10-13 07:12:21 - train: epoch 0098, iter [04800, 05004], lr: 0.000113, loss: 3.0676
2022-10-13 07:13:11 - train: epoch 0098, iter [04900, 05004], lr: 0.000111, loss: 3.1121
2022-10-13 07:14:00 - train: epoch 0098, iter [05000, 05004], lr: 0.000109, loss: 3.0110
2022-10-13 07:14:02 - train: epoch 098, train_loss: 3.4295
2022-10-13 07:15:49 - eval: epoch: 098, acc1: 76.696%, acc5: 93.496%, test_loss: 1.1869, per_image_load_time: 3.498ms, per_image_inference_time: 0.563ms
2022-10-13 07:15:50 - until epoch: 098, best_acc1: 76.696%
2022-10-13 07:15:50 - epoch 099 lr: 0.000109
2022-10-13 07:16:43 - train: epoch 0099, iter [00100, 05004], lr: 0.000107, loss: 3.3680
2022-10-13 07:17:36 - train: epoch 0099, iter [00200, 05004], lr: 0.000105, loss: 3.8819
2022-10-13 07:18:26 - train: epoch 0099, iter [00300, 05004], lr: 0.000103, loss: 3.4893
2022-10-13 07:19:15 - train: epoch 0099, iter [00400, 05004], lr: 0.000101, loss: 3.7959
2022-10-13 07:20:03 - train: epoch 0099, iter [00500, 05004], lr: 0.000099, loss: 3.0006
2022-10-13 07:20:51 - train: epoch 0099, iter [00600, 05004], lr: 0.000097, loss: 3.4906
2022-10-13 07:21:42 - train: epoch 0099, iter [00700, 05004], lr: 0.000095, loss: 3.3644
2022-10-13 07:22:32 - train: epoch 0099, iter [00800, 05004], lr: 0.000094, loss: 2.7136
2022-10-13 07:23:22 - train: epoch 0099, iter [00900, 05004], lr: 0.000092, loss: 3.9077
2022-10-13 07:24:13 - train: epoch 0099, iter [01000, 05004], lr: 0.000090, loss: 3.2213
2022-10-13 07:25:02 - train: epoch 0099, iter [01100, 05004], lr: 0.000088, loss: 3.4948
2022-10-13 07:25:53 - train: epoch 0099, iter [01200, 05004], lr: 0.000086, loss: 3.3099
2022-10-13 07:26:45 - train: epoch 0099, iter [01300, 05004], lr: 0.000085, loss: 3.4976
2022-10-13 07:27:34 - train: epoch 0099, iter [01400, 05004], lr: 0.000083, loss: 2.9280
2022-10-13 07:28:25 - train: epoch 0099, iter [01500, 05004], lr: 0.000081, loss: 2.9899
2022-10-13 07:29:15 - train: epoch 0099, iter [01600, 05004], lr: 0.000080, loss: 4.0504
2022-10-13 07:30:05 - train: epoch 0099, iter [01700, 05004], lr: 0.000078, loss: 3.0913
2022-10-13 07:30:53 - train: epoch 0099, iter [01800, 05004], lr: 0.000076, loss: 3.7678
2022-10-13 07:31:43 - train: epoch 0099, iter [01900, 05004], lr: 0.000075, loss: 3.6376
2022-10-13 07:32:32 - train: epoch 0099, iter [02000, 05004], lr: 0.000073, loss: 3.7626
2022-10-13 07:33:19 - train: epoch 0099, iter [02100, 05004], lr: 0.000072, loss: 3.4019
2022-10-13 07:34:08 - train: epoch 0099, iter [02200, 05004], lr: 0.000070, loss: 3.2746
2022-10-13 07:34:58 - train: epoch 0099, iter [02300, 05004], lr: 0.000069, loss: 3.3272
2022-10-13 07:35:49 - train: epoch 0099, iter [02400, 05004], lr: 0.000067, loss: 3.8145
2022-10-13 07:36:38 - train: epoch 0099, iter [02500, 05004], lr: 0.000066, loss: 3.4336
2022-10-13 07:37:29 - train: epoch 0099, iter [02600, 05004], lr: 0.000064, loss: 3.7467
2022-10-13 07:38:19 - train: epoch 0099, iter [02700, 05004], lr: 0.000063, loss: 3.7225
2022-10-13 07:39:09 - train: epoch 0099, iter [02800, 05004], lr: 0.000061, loss: 3.0804
2022-10-13 07:40:00 - train: epoch 0099, iter [02900, 05004], lr: 0.000060, loss: 2.8979
2022-10-13 07:40:50 - train: epoch 0099, iter [03000, 05004], lr: 0.000058, loss: 3.8201
2022-10-13 07:41:41 - train: epoch 0099, iter [03100, 05004], lr: 0.000057, loss: 2.8653
2022-10-13 07:42:34 - train: epoch 0099, iter [03200, 05004], lr: 0.000056, loss: 3.7331
2022-10-13 07:43:22 - train: epoch 0099, iter [03300, 05004], lr: 0.000054, loss: 3.6873
2022-10-13 07:44:11 - train: epoch 0099, iter [03400, 05004], lr: 0.000053, loss: 3.4597
2022-10-13 07:44:59 - train: epoch 0099, iter [03500, 05004], lr: 0.000052, loss: 3.6701
2022-10-13 07:45:49 - train: epoch 0099, iter [03600, 05004], lr: 0.000050, loss: 3.8577
2022-10-13 07:46:36 - train: epoch 0099, iter [03700, 05004], lr: 0.000049, loss: 3.6078
2022-10-13 07:47:26 - train: epoch 0099, iter [03800, 05004], lr: 0.000048, loss: 3.5526
2022-10-13 07:48:16 - train: epoch 0099, iter [03900, 05004], lr: 0.000047, loss: 3.2526
2022-10-13 07:49:05 - train: epoch 0099, iter [04000, 05004], lr: 0.000046, loss: 3.8020
2022-10-13 07:49:57 - train: epoch 0099, iter [04100, 05004], lr: 0.000044, loss: 3.0982
2022-10-13 07:50:47 - train: epoch 0099, iter [04200, 05004], lr: 0.000043, loss: 3.9541
2022-10-13 07:51:36 - train: epoch 0099, iter [04300, 05004], lr: 0.000042, loss: 3.4910
2022-10-13 07:52:28 - train: epoch 0099, iter [04400, 05004], lr: 0.000041, loss: 2.8369
2022-10-13 07:53:17 - train: epoch 0099, iter [04500, 05004], lr: 0.000040, loss: 3.4900
2022-10-13 07:54:08 - train: epoch 0099, iter [04600, 05004], lr: 0.000039, loss: 4.0790
2022-10-13 07:54:59 - train: epoch 0099, iter [04700, 05004], lr: 0.000038, loss: 3.2930
2022-10-13 07:55:48 - train: epoch 0099, iter [04800, 05004], lr: 0.000037, loss: 3.4161
2022-10-13 07:56:37 - train: epoch 0099, iter [04900, 05004], lr: 0.000036, loss: 3.0376
2022-10-13 07:57:24 - train: epoch 0099, iter [05000, 05004], lr: 0.000035, loss: 3.5641
2022-10-13 07:57:26 - train: epoch 099, train_loss: 3.4229
2022-10-13 07:59:10 - eval: epoch: 099, acc1: 76.740%, acc5: 93.488%, test_loss: 1.1865, per_image_load_time: 3.431ms, per_image_inference_time: 0.517ms
2022-10-13 07:59:11 - until epoch: 099, best_acc1: 76.740%
2022-10-13 07:59:11 - epoch 100 lr: 0.000035
2022-10-13 08:00:06 - train: epoch 0100, iter [00100, 05004], lr: 0.000034, loss: 3.4013
2022-10-13 08:00:56 - train: epoch 0100, iter [00200, 05004], lr: 0.000033, loss: 3.6748
2022-10-13 08:01:47 - train: epoch 0100, iter [00300, 05004], lr: 0.000032, loss: 3.1167
2022-10-13 08:02:38 - train: epoch 0100, iter [00400, 05004], lr: 0.000031, loss: 3.6276
2022-10-13 08:03:28 - train: epoch 0100, iter [00500, 05004], lr: 0.000030, loss: 2.6924
2022-10-13 08:04:18 - train: epoch 0100, iter [00600, 05004], lr: 0.000029, loss: 4.0092
2022-10-13 08:05:08 - train: epoch 0100, iter [00700, 05004], lr: 0.000028, loss: 2.9764
2022-10-13 08:05:58 - train: epoch 0100, iter [00800, 05004], lr: 0.000027, loss: 3.1525
2022-10-13 08:06:49 - train: epoch 0100, iter [00900, 05004], lr: 0.000027, loss: 2.9563
2022-10-13 08:07:39 - train: epoch 0100, iter [01000, 05004], lr: 0.000026, loss: 3.8126
2022-10-13 08:08:30 - train: epoch 0100, iter [01100, 05004], lr: 0.000025, loss: 3.6015
2022-10-13 08:09:21 - train: epoch 0100, iter [01200, 05004], lr: 0.000024, loss: 3.7770
2022-10-13 08:10:10 - train: epoch 0100, iter [01300, 05004], lr: 0.000024, loss: 3.0391
2022-10-13 08:10:58 - train: epoch 0100, iter [01400, 05004], lr: 0.000023, loss: 3.5407
2022-10-13 08:11:45 - train: epoch 0100, iter [01500, 05004], lr: 0.000022, loss: 3.2101
2022-10-13 08:12:33 - train: epoch 0100, iter [01600, 05004], lr: 0.000021, loss: 3.9891
2022-10-13 08:13:24 - train: epoch 0100, iter [01700, 05004], lr: 0.000021, loss: 3.0773
2022-10-13 08:14:14 - train: epoch 0100, iter [01800, 05004], lr: 0.000020, loss: 3.3615
2022-10-13 08:15:05 - train: epoch 0100, iter [01900, 05004], lr: 0.000019, loss: 3.6453
2022-10-13 08:15:55 - train: epoch 0100, iter [02000, 05004], lr: 0.000019, loss: 4.0878
2022-10-13 08:16:44 - train: epoch 0100, iter [02100, 05004], lr: 0.000018, loss: 3.2564
2022-10-13 08:17:34 - train: epoch 0100, iter [02200, 05004], lr: 0.000018, loss: 3.9493
2022-10-13 08:18:24 - train: epoch 0100, iter [02300, 05004], lr: 0.000017, loss: 2.8908
2022-10-13 08:19:13 - train: epoch 0100, iter [02400, 05004], lr: 0.000017, loss: 3.3085
2022-10-13 08:20:03 - train: epoch 0100, iter [02500, 05004], lr: 0.000016, loss: 3.8494
2022-10-13 08:20:51 - train: epoch 0100, iter [02600, 05004], lr: 0.000016, loss: 3.5096
2022-10-13 08:21:40 - train: epoch 0100, iter [02700, 05004], lr: 0.000015, loss: 3.4903
2022-10-13 08:22:30 - train: epoch 0100, iter [02800, 05004], lr: 0.000015, loss: 3.2124
2022-10-13 08:23:19 - train: epoch 0100, iter [02900, 05004], lr: 0.000014, loss: 3.4366
2022-10-13 08:24:08 - train: epoch 0100, iter [03000, 05004], lr: 0.000014, loss: 3.6094
2022-10-13 08:24:58 - train: epoch 0100, iter [03100, 05004], lr: 0.000014, loss: 3.6233
2022-10-13 08:25:45 - train: epoch 0100, iter [03200, 05004], lr: 0.000013, loss: 3.6668
2022-10-13 08:26:34 - train: epoch 0100, iter [03300, 05004], lr: 0.000013, loss: 3.8921
2022-10-13 08:27:23 - train: epoch 0100, iter [03400, 05004], lr: 0.000013, loss: 3.9956
2022-10-13 08:28:14 - train: epoch 0100, iter [03500, 05004], lr: 0.000012, loss: 3.0976
2022-10-13 08:29:05 - train: epoch 0100, iter [03600, 05004], lr: 0.000012, loss: 3.4544
2022-10-13 08:29:54 - train: epoch 0100, iter [03700, 05004], lr: 0.000012, loss: 2.7088
2022-10-13 08:30:46 - train: epoch 0100, iter [03800, 05004], lr: 0.000011, loss: 3.0880
2022-10-13 08:31:37 - train: epoch 0100, iter [03900, 05004], lr: 0.000011, loss: 3.6178
2022-10-13 08:32:27 - train: epoch 0100, iter [04000, 05004], lr: 0.000011, loss: 2.9171
2022-10-13 08:33:16 - train: epoch 0100, iter [04100, 05004], lr: 0.000011, loss: 3.1277
2022-10-13 08:34:07 - train: epoch 0100, iter [04200, 05004], lr: 0.000011, loss: 4.0410
2022-10-13 08:34:56 - train: epoch 0100, iter [04300, 05004], lr: 0.000010, loss: 3.3814
2022-10-13 08:35:45 - train: epoch 0100, iter [04400, 05004], lr: 0.000010, loss: 3.3129
2022-10-13 08:36:35 - train: epoch 0100, iter [04500, 05004], lr: 0.000010, loss: 2.9693
2022-10-13 08:37:24 - train: epoch 0100, iter [04600, 05004], lr: 0.000010, loss: 3.6164
2022-10-13 08:38:13 - train: epoch 0100, iter [04700, 05004], lr: 0.000010, loss: 3.9504
2022-10-13 08:39:03 - train: epoch 0100, iter [04800, 05004], lr: 0.000010, loss: 3.4552
2022-10-13 08:39:51 - train: epoch 0100, iter [04900, 05004], lr: 0.000010, loss: 3.1936
2022-10-13 08:40:37 - train: epoch 0100, iter [05000, 05004], lr: 0.000010, loss: 3.1208
2022-10-13 08:40:39 - train: epoch 100, train_loss: 3.4146
2022-10-13 08:42:27 - eval: epoch: 100, acc1: 76.696%, acc5: 93.512%, test_loss: 1.2176, per_image_load_time: 3.470ms, per_image_inference_time: 0.547ms
2022-10-13 08:42:27 - until epoch: 100, best_acc1: 76.740%
2022-10-13 08:42:27 - train done. model: resnet50, train time: 72.084 hours, best_acc1: 76.740%
