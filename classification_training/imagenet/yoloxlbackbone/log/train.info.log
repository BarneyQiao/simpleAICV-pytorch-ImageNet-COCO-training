2022-03-18 21:39:25 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 1.1111
2022-03-18 21:39:57 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 1.0279
2022-03-18 21:40:30 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 1.3908
2022-03-18 21:41:02 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 1.2456
2022-03-18 21:41:34 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 1.0124
2022-03-18 21:42:06 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 1.1450
2022-03-18 21:42:38 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 0.9220
2022-03-18 21:43:10 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 1.1709
2022-03-18 21:43:42 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 1.1169
2022-03-18 21:44:14 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 1.1961
2022-03-18 21:44:45 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 1.2793
2022-03-18 21:45:18 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 1.2165
2022-03-18 21:45:50 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 1.0943
2022-03-18 21:46:22 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 0.9644
2022-03-18 21:46:53 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 1.1492
2022-03-18 21:47:25 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 1.2988
2022-03-18 21:47:58 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 1.1046
2022-03-18 21:48:31 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 1.0963
2022-03-18 21:49:03 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 1.0334
2022-03-18 21:49:35 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 1.1378
2022-03-18 21:50:07 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 1.0483
2022-03-18 21:50:39 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 1.0983
2022-03-18 21:51:11 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 1.0704
2022-03-18 21:51:43 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 0.8656
2022-03-18 21:52:15 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 1.0872
2022-03-18 21:52:47 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 1.0665
2022-03-18 21:53:18 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 1.1014
2022-03-18 21:53:51 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 1.3192
2022-03-18 21:54:22 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 0.9607
2022-03-18 21:54:23 - train: epoch 089, train_loss: 1.1225
2022-03-18 21:55:37 - eval: epoch: 089, acc1: 73.380%, acc5: 91.440%, test_loss: 1.0547, per_image_load_time: 2.331ms, per_image_inference_time: 0.539ms
2022-03-18 21:55:37 - until epoch: 089, best_acc1: 73.534%
2022-03-18 21:55:37 - epoch 090 lr: 0.0010000000000000002
2022-03-18 21:56:16 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 0.9649
2022-03-18 21:56:47 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 1.0445
2022-03-18 21:57:19 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 1.0546
2022-03-18 21:57:51 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 1.1015
2022-03-18 21:58:23 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 1.1669
2022-03-18 21:58:55 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 1.4807
2022-03-18 21:59:27 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 1.0994
2022-03-18 22:00:00 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 1.2201
2022-03-18 22:00:31 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 0.9911
2022-03-18 22:01:03 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 1.0557
2022-03-18 22:01:35 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 1.0953
2022-03-18 22:02:06 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 0.9727
2022-03-18 22:02:38 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 1.2413
2022-03-18 22:03:10 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 1.0438
2022-03-18 22:03:42 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 1.1980
2022-03-18 22:04:14 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 1.0902
2022-03-18 22:04:46 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 0.9509
2022-03-18 22:05:18 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 1.0894
2022-03-18 22:05:49 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 0.9638
2022-03-18 22:06:21 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 1.1166
2022-03-18 22:06:54 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 1.0215
2022-03-18 22:07:26 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 1.2025
2022-03-18 22:07:58 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 1.2757
2022-03-18 22:08:30 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 1.0715
2022-03-18 22:09:02 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 0.9429
2022-03-18 22:09:34 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 1.0446
2022-03-18 22:10:06 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 1.0044
2022-03-18 22:10:38 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 1.1623
2022-03-18 22:11:10 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 1.2203
2022-03-18 22:11:42 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 1.0683
2022-03-18 22:12:14 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 1.1139
2022-03-18 22:12:46 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 1.1923
2022-03-18 22:13:18 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 1.3269
2022-03-18 22:13:50 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 1.0490
2022-03-18 22:14:22 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 1.1971
2022-03-18 22:14:55 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 0.8564
2022-03-18 22:15:27 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 1.0645
2022-03-18 22:15:59 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 1.2090
2022-03-18 22:16:31 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 1.0823
2022-03-18 22:17:04 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 0.9623
2022-03-18 22:17:36 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 1.4335
2022-03-18 22:18:08 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 1.2185
2022-03-18 22:18:40 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 1.0633
2022-03-18 22:19:12 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 1.0315
2022-03-18 22:19:44 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 1.0591
2022-03-18 22:20:17 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 0.9170
2022-03-18 22:20:48 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 1.0723
2022-03-18 22:21:21 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 1.2819
2022-03-18 22:21:52 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 1.0512
2022-03-18 22:22:24 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 1.0487
2022-03-18 22:22:25 - train: epoch 090, train_loss: 1.1215
2022-03-18 22:23:39 - eval: epoch: 090, acc1: 73.446%, acc5: 91.378%, test_loss: 1.0533, per_image_load_time: 2.285ms, per_image_inference_time: 0.513ms
2022-03-18 22:23:40 - until epoch: 090, best_acc1: 73.534%
2022-03-18 22:23:40 - epoch 091 lr: 0.00010000000000000003
2022-03-18 22:24:18 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 0.9387
2022-03-18 22:24:50 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 1.0856
2022-03-18 22:25:22 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 1.0165
2022-03-18 22:25:53 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 1.2027
2022-03-18 22:26:25 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 1.0523
2022-03-18 22:26:57 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 1.1070
2022-03-18 22:27:28 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 1.3093
2022-03-18 22:28:00 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 0.9719
2022-03-18 22:28:32 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 1.0231
2022-03-18 22:29:04 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 1.0314
2022-03-18 22:29:36 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 0.9084
2022-03-18 22:30:08 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 1.1252
2022-03-18 22:30:40 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 1.1487
2022-03-18 22:31:12 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 1.1936
2022-03-18 22:31:44 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 1.0634
2022-03-18 22:32:16 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 1.0837
2022-03-18 22:32:48 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 1.1239
2022-03-18 22:33:20 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 1.1773
2022-03-18 22:33:52 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 1.0248
2022-03-18 22:34:24 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 0.8725
2022-03-18 22:34:56 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 0.9305
2022-03-18 22:35:27 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 1.0554
2022-03-18 22:35:59 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 1.2698
2022-03-18 22:36:31 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 0.8889
2022-03-18 22:37:03 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 1.1547
2022-03-18 22:37:35 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 1.1784
2022-03-18 22:38:07 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 0.9996
2022-03-18 22:38:40 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 1.2811
2022-03-18 22:39:12 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 1.1813
2022-03-18 22:39:43 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 1.1645
2022-03-18 22:40:15 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 1.0730
2022-03-18 22:40:47 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 1.0103
2022-03-18 22:41:19 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 1.1035
2022-03-18 22:41:50 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 1.0756
2022-03-18 22:42:23 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 1.0954
2022-03-18 22:42:55 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 1.0226
2022-03-18 22:43:27 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 1.2305
2022-03-18 22:43:59 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 1.0387
2022-03-18 22:44:31 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 1.0399
2022-03-18 22:45:02 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 1.0174
2022-03-18 22:45:34 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 1.0487
2022-03-18 22:46:06 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 1.1664
2022-03-18 22:46:38 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 1.0389
2022-03-18 22:47:09 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 0.9106
2022-03-18 22:47:41 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 0.9381
2022-03-18 22:48:13 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 1.0927
2022-03-18 22:48:44 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 1.2095
2022-03-18 22:49:16 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 1.1089
2022-03-18 22:49:48 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 0.9348
2022-03-18 22:50:19 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 1.2133
2022-03-18 22:50:21 - train: epoch 091, train_loss: 1.0895
2022-03-18 22:51:33 - eval: epoch: 091, acc1: 73.962%, acc5: 91.554%, test_loss: 1.0358, per_image_load_time: 1.945ms, per_image_inference_time: 0.522ms
2022-03-18 22:51:34 - until epoch: 091, best_acc1: 73.962%
2022-03-18 22:51:34 - epoch 092 lr: 0.00010000000000000003
2022-03-18 22:52:13 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 1.0246
2022-03-18 22:52:45 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 1.1066
2022-03-18 22:53:17 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 1.1580
2022-03-18 22:53:49 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 1.0929
2022-03-18 22:54:21 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 1.0099
2022-03-18 22:54:53 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 1.0307
2022-03-18 22:55:25 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 1.1075
2022-03-18 22:55:56 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 1.1169
2022-03-18 22:56:28 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 1.0774
2022-03-18 22:57:00 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 1.0762
2022-03-18 22:57:32 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 0.9064
2022-03-18 22:58:04 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 0.9988
2022-03-18 22:58:36 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 0.8864
2022-03-18 22:59:08 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 0.9259
2022-03-18 22:59:40 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 1.0158
2022-03-18 23:00:12 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 1.2006
2022-03-18 23:00:44 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 1.2340
2022-03-18 23:01:17 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 1.0023
2022-03-18 23:01:49 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 0.9684
2022-03-18 23:02:21 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 1.1476
2022-03-18 23:02:52 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 0.9410
2022-03-18 23:03:24 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 1.0052
2022-03-18 23:03:56 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 1.1489
2022-03-18 23:04:28 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 1.0076
2022-03-18 23:05:01 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 1.0220
2022-03-18 23:05:33 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 1.2294
2022-03-18 23:06:06 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 1.0348
2022-03-18 23:06:38 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 1.0773
2022-03-18 23:07:10 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 1.2229
2022-03-18 23:07:43 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 1.1490
2022-03-18 23:08:15 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 1.1213
2022-03-18 23:08:47 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 1.0172
2022-03-18 23:09:19 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 1.0853
2022-03-18 23:09:51 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 1.2045
2022-03-18 23:10:23 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 1.0859
2022-03-18 23:10:55 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 1.1116
2022-03-18 23:11:27 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 1.1196
2022-03-18 23:11:59 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 1.1083
2022-03-18 23:12:31 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 1.2226
2022-03-18 23:13:04 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 1.1827
2022-03-18 23:13:36 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 1.0504
2022-03-18 23:14:08 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 0.9651
2022-03-18 23:14:40 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 1.1185
2022-03-18 23:15:12 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 1.0992
2022-03-18 23:15:45 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 0.9246
2022-03-18 23:16:17 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 1.1253
2022-03-18 23:16:49 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 0.9807
2022-03-18 23:17:21 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 1.1656
2022-03-18 23:17:53 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 1.0073
2022-03-18 23:18:24 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 1.0368
2022-03-18 23:18:26 - train: epoch 092, train_loss: 1.0808
2022-03-18 23:19:39 - eval: epoch: 092, acc1: 73.948%, acc5: 91.594%, test_loss: 1.0346, per_image_load_time: 2.307ms, per_image_inference_time: 0.509ms
2022-03-18 23:19:39 - until epoch: 092, best_acc1: 73.962%
2022-03-18 23:19:39 - epoch 093 lr: 0.00010000000000000003
2022-03-18 23:20:18 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 1.0125
2022-03-18 23:20:50 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 0.9687
2022-03-18 23:21:22 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 1.1680
2022-03-18 23:21:54 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 1.1488
2022-03-18 23:22:26 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 1.1123
2022-03-18 23:22:58 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 0.9768
2022-03-18 23:23:29 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 0.9885
2022-03-18 23:24:02 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 0.9572
2022-03-18 23:24:34 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 0.9746
2022-03-18 23:25:05 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 1.0087
2022-03-18 23:25:38 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 1.1812
2022-03-18 23:26:10 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 1.2010
2022-03-18 23:26:42 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 0.9827
2022-03-18 23:27:14 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 1.1383
2022-03-18 23:27:46 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 1.1045
2022-03-18 23:28:18 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 1.0860
2022-03-18 23:28:50 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 1.1042
2022-03-18 23:29:22 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 1.1860
2022-03-18 23:29:54 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 1.0220
2022-03-18 23:30:25 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 1.0134
2022-03-18 23:30:57 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 1.0091
2022-03-18 23:31:29 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 1.1538
2022-03-18 23:32:01 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 1.1324
2022-03-18 23:32:33 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 0.8904
2022-03-18 23:33:05 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 1.0909
2022-03-18 23:33:37 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 1.2128
2022-03-18 23:34:09 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 0.9129
2022-03-18 23:34:41 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 1.0928
2022-03-18 23:35:13 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 0.9196
2022-03-18 23:35:45 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 1.0282
2022-03-18 23:36:17 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 1.1440
2022-03-18 23:36:49 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 1.0654
2022-03-18 23:37:21 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 1.0776
2022-03-18 23:37:53 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 1.1756
2022-03-18 23:38:25 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 0.9554
2022-03-18 23:38:56 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 1.2594
2022-03-18 23:39:28 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 1.1146
2022-03-18 23:40:00 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 0.9720
2022-03-18 23:40:32 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 1.1906
2022-03-18 23:41:03 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 1.2818
2022-03-18 23:41:36 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 1.0907
2022-03-18 23:42:08 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 1.1462
2022-03-18 23:42:39 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 1.2747
2022-03-18 23:43:11 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 1.0403
2022-03-18 23:43:43 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 1.0128
2022-03-18 23:44:15 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 0.9253
2022-03-18 23:44:47 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 1.2243
2022-03-18 23:45:19 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 0.9053
2022-03-18 23:45:50 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 1.1674
2022-03-18 23:46:21 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 1.0797
2022-03-18 23:46:23 - train: epoch 093, train_loss: 1.0761
2022-03-18 23:47:36 - eval: epoch: 093, acc1: 73.976%, acc5: 91.594%, test_loss: 1.0314, per_image_load_time: 2.343ms, per_image_inference_time: 0.525ms
2022-03-18 23:47:37 - until epoch: 093, best_acc1: 73.976%
2022-03-18 23:47:37 - epoch 094 lr: 0.00010000000000000003
2022-03-18 23:48:14 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 0.9983
2022-03-18 23:48:45 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 1.2109
2022-03-18 23:49:17 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 1.2591
2022-03-18 23:49:49 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 1.2384
2022-03-18 23:50:21 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 1.0833
2022-03-18 23:50:52 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 0.9480
2022-03-18 23:51:24 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 1.0918
2022-03-18 23:51:56 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 1.0164
2022-03-18 23:52:28 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 1.1007
2022-03-18 23:53:00 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 1.2150
2022-03-18 23:53:32 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 1.1856
2022-03-18 23:54:03 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 0.9749
2022-03-18 23:54:35 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 1.0559
2022-03-18 23:55:07 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 1.0356
2022-03-18 23:55:39 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 1.1383
2022-03-18 23:56:11 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.3288
2022-03-18 23:56:43 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 1.0340
2022-03-18 23:57:15 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 0.9131
2022-03-18 23:57:47 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 1.1659
2022-03-18 23:58:18 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 0.9432
2022-03-18 23:58:51 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 1.0097
2022-03-18 23:59:23 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 1.0787
2022-03-18 23:59:55 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 0.9108
2022-03-19 00:00:28 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 1.1065
2022-03-19 00:01:00 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 1.1145
2022-03-19 00:01:32 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 1.1739
2022-03-19 00:02:04 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 0.9731
2022-03-19 00:02:36 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 1.1086
2022-03-19 00:03:08 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 0.9964
2022-03-19 00:03:39 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 1.0924
2022-03-19 00:04:11 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 1.1888
2022-03-19 00:04:43 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 1.0982
2022-03-19 00:05:14 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 1.0318
2022-03-19 00:05:46 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 1.0647
2022-03-19 00:06:18 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 1.1712
2022-03-19 00:06:50 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 1.0601
2022-03-19 00:07:21 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 1.2429
2022-03-19 00:07:52 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 0.9268
2022-03-19 00:08:24 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 0.9860
2022-03-19 00:08:56 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 0.9904
2022-03-19 00:09:27 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 1.2117
2022-03-19 00:09:59 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 0.8348
2022-03-19 00:10:30 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 1.0712
2022-03-19 00:11:02 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 1.2205
2022-03-19 00:11:34 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 1.0070
2022-03-19 00:12:06 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 1.0874
2022-03-19 00:12:37 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 1.1139
2022-03-19 00:13:09 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 0.9401
2022-03-19 00:13:41 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 1.1851
2022-03-19 00:14:11 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 1.0447
2022-03-19 00:14:13 - train: epoch 094, train_loss: 1.0752
2022-03-19 00:15:26 - eval: epoch: 094, acc1: 73.866%, acc5: 91.650%, test_loss: 1.0321, per_image_load_time: 2.303ms, per_image_inference_time: 0.537ms
2022-03-19 00:15:26 - until epoch: 094, best_acc1: 73.976%
2022-03-19 00:15:26 - epoch 095 lr: 0.00010000000000000003
2022-03-19 00:16:04 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 1.1101
2022-03-19 00:16:36 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 1.0423
2022-03-19 00:17:07 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 1.0155
2022-03-19 00:17:39 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 0.9701
2022-03-19 00:18:11 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 1.1058
2022-03-19 00:18:43 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 0.9918
2022-03-19 00:19:14 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 1.1701
2022-03-19 00:19:46 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 1.2184
2022-03-19 00:20:17 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 1.2914
2022-03-19 00:20:49 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 1.1141
2022-03-19 00:21:21 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 1.0382
2022-03-19 00:21:52 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 1.0055
2022-03-19 00:22:24 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 1.0658
2022-03-19 00:22:56 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 1.0803
2022-03-19 00:23:28 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 1.1188
2022-03-19 00:23:59 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 0.8835
2022-03-19 00:24:31 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 1.0700
2022-03-19 00:25:03 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 1.1483
2022-03-19 00:25:35 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 0.9966
2022-03-19 00:26:07 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 1.0249
2022-03-19 00:26:39 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 1.1228
2022-03-19 00:27:11 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 0.8269
2022-03-19 00:27:43 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 1.1158
2022-03-19 00:28:14 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 1.2481
2022-03-19 00:28:46 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 0.9772
2022-03-19 00:29:18 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 0.9852
2022-03-19 00:29:50 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 1.0277
2022-03-19 00:30:22 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 0.9356
2022-03-19 00:30:54 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 1.1964
2022-03-19 00:31:26 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 1.1268
2022-03-19 00:31:58 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 1.3196
2022-03-19 00:32:30 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 1.1103
2022-03-19 00:33:02 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 1.1101
2022-03-19 00:33:35 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 1.1015
2022-03-19 00:34:06 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 0.9688
2022-03-19 00:34:38 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 0.8345
2022-03-19 00:35:11 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 1.0639
2022-03-19 00:35:42 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 1.0588
2022-03-19 00:36:14 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 1.0329
2022-03-19 00:36:46 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 0.8844
2022-03-19 00:37:17 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 1.0267
2022-03-19 00:37:49 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 1.0435
2022-03-19 00:38:21 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 0.9950
2022-03-19 00:38:53 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 1.2517
2022-03-19 00:39:25 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 0.9373
2022-03-19 00:39:57 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 1.0594
2022-03-19 00:40:29 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 0.8126
2022-03-19 00:41:01 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 0.9705
2022-03-19 00:41:32 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 1.0526
2022-03-19 00:42:03 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 0.9090
2022-03-19 00:42:05 - train: epoch 095, train_loss: 1.0728
2022-03-19 00:43:18 - eval: epoch: 095, acc1: 73.926%, acc5: 91.680%, test_loss: 1.0304, per_image_load_time: 2.205ms, per_image_inference_time: 0.515ms
2022-03-19 00:43:18 - until epoch: 095, best_acc1: 73.976%
2022-03-19 00:43:18 - epoch 096 lr: 0.00010000000000000003
2022-03-19 00:43:57 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 1.0848
2022-03-19 00:44:28 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 0.9975
2022-03-19 00:45:00 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 1.1509
2022-03-19 00:45:32 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 0.9008
2022-03-19 00:46:04 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 0.9640
2022-03-19 00:46:36 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 1.1019
2022-03-19 00:47:09 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 1.0458
2022-03-19 00:47:41 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 1.0170
2022-03-19 00:48:12 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 1.1271
2022-03-19 00:48:43 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 1.0202
2022-03-19 00:49:15 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 1.0538
2022-03-19 00:49:46 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 0.9609
2022-03-19 00:50:18 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 1.0353
2022-03-19 00:50:50 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 1.1801
2022-03-19 00:51:21 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 1.2039
2022-03-19 00:51:53 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 0.9726
2022-03-19 00:52:25 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 0.9909
2022-03-19 00:52:56 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 1.0473
2022-03-19 00:53:28 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 0.9956
2022-03-19 00:53:59 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 1.1096
2022-03-19 00:54:31 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 1.0225
2022-03-19 00:55:03 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 0.8528
2022-03-19 00:55:35 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 1.0742
2022-03-19 00:56:07 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 1.1757
2022-03-19 00:56:38 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 0.9325
2022-03-19 00:57:10 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 1.0177
2022-03-19 00:57:43 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 1.1618
2022-03-19 00:58:14 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 1.0877
2022-03-19 00:58:46 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 0.9766
2022-03-19 00:59:18 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 1.2058
2022-03-19 00:59:50 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 1.2921
2022-03-19 01:00:22 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 1.0535
2022-03-19 01:00:54 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 1.1910
2022-03-19 01:01:26 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 1.0481
2022-03-19 01:01:57 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 1.0480
2022-03-19 01:02:29 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 1.0593
2022-03-19 01:03:01 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 1.1065
2022-03-19 01:03:33 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 1.0910
2022-03-19 01:04:04 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 0.9948
2022-03-19 01:04:37 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 0.9409
2022-03-19 01:05:08 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 1.1315
2022-03-19 01:05:40 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 0.8904
2022-03-19 01:06:12 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 0.8171
2022-03-19 01:06:44 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 1.0799
2022-03-19 01:07:16 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 1.0689
2022-03-19 01:07:48 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 1.0604
2022-03-19 01:08:20 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 1.1176
2022-03-19 01:08:52 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 1.1000
2022-03-19 01:09:24 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 1.1996
2022-03-19 01:09:55 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 1.1915
2022-03-19 01:09:56 - train: epoch 096, train_loss: 1.0703
2022-03-19 01:11:09 - eval: epoch: 096, acc1: 73.948%, acc5: 91.684%, test_loss: 1.0300, per_image_load_time: 2.318ms, per_image_inference_time: 0.480ms
2022-03-19 01:11:09 - until epoch: 096, best_acc1: 73.976%
2022-03-19 01:11:09 - epoch 097 lr: 0.00010000000000000003
2022-03-19 01:11:47 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 1.0270
2022-03-19 01:12:19 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 1.1093
2022-03-19 01:12:51 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 1.1042
2022-03-19 01:13:23 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 1.1790
2022-03-19 01:13:55 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 1.2451
2022-03-19 01:14:26 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 1.1341
2022-03-19 01:14:58 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 1.0756
2022-03-19 01:15:30 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 1.0304
2022-03-19 01:16:02 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 0.9550
2022-03-19 01:16:34 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 1.1703
2022-03-19 01:17:06 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 0.9413
2022-03-19 01:17:37 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 0.9832
2022-03-19 01:18:09 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 1.0374
2022-03-19 01:18:41 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 1.2006
2022-03-19 01:19:13 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 0.9904
2022-03-19 01:19:45 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 0.8889
2022-03-19 01:20:17 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 0.9868
2022-03-19 01:20:49 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 1.1461
2022-03-19 01:21:21 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 0.9465
2022-03-19 01:21:52 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 1.0362
2022-03-19 01:22:24 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 0.9579
2022-03-19 01:22:55 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 1.0761
2022-03-19 01:23:27 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 0.9884
2022-03-19 01:23:58 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 1.0756
2022-03-19 01:24:30 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 1.0669
2022-03-19 01:25:01 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 1.0644
2022-03-19 01:25:33 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 1.0454
2022-03-19 01:26:04 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 1.1229
2022-03-19 01:26:36 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 1.0007
2022-03-19 01:27:08 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 0.9832
2022-03-19 01:27:39 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 0.8584
2022-03-19 01:28:11 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 1.1311
2022-03-19 01:28:43 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 1.1838
2022-03-19 01:29:14 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 0.9886
2022-03-19 01:29:46 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 1.0294
2022-03-19 01:30:17 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 1.0695
2022-03-19 01:30:49 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 0.9471
2022-03-19 01:31:21 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 0.9777
2022-03-19 01:31:52 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 1.0385
2022-03-19 01:32:24 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 1.1063
2022-03-19 01:32:56 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 1.1060
2022-03-19 01:33:28 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 1.0335
2022-03-19 01:33:59 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 1.0105
2022-03-19 01:34:31 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 0.9717
2022-03-19 01:35:03 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 1.1921
2022-03-19 01:35:34 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 1.0449
2022-03-19 01:36:06 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 0.8812
2022-03-19 01:36:38 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 1.0729
2022-03-19 01:37:09 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 1.3090
2022-03-19 01:37:41 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 0.9914
2022-03-19 01:37:42 - train: epoch 097, train_loss: 1.0704
2022-03-19 01:38:55 - eval: epoch: 097, acc1: 73.966%, acc5: 91.640%, test_loss: 1.0287, per_image_load_time: 2.346ms, per_image_inference_time: 0.477ms
2022-03-19 01:38:56 - until epoch: 097, best_acc1: 73.976%
2022-03-19 01:38:56 - epoch 098 lr: 0.00010000000000000003
2022-03-19 01:39:34 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 1.0498
2022-03-19 01:40:05 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 1.0525
2022-03-19 01:40:37 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 1.1430
2022-03-19 01:41:09 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 1.1314
2022-03-19 01:41:41 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 0.9757
2022-03-19 01:42:12 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 0.8738
2022-03-19 01:42:44 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 1.1070
2022-03-19 01:43:16 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 1.2175
2022-03-19 01:43:47 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 1.0722
2022-03-19 01:44:19 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 1.0702
2022-03-19 01:44:51 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 1.0288
2022-03-19 01:45:22 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 1.1218
2022-03-19 01:45:54 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 1.0648
2022-03-19 01:46:26 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 1.2164
2022-03-19 01:46:58 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 1.0849
2022-03-19 01:47:29 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 1.3310
2022-03-19 01:48:01 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 1.1611
2022-03-19 01:48:33 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 0.9117
2022-03-19 01:49:05 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 0.9084
2022-03-19 01:49:37 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 1.0946
2022-03-19 01:50:09 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 0.9921
2022-03-19 01:50:40 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 1.0477
2022-03-19 01:51:12 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 1.0197
2022-03-19 01:51:44 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 1.1882
2022-03-19 01:52:16 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 1.1436
2022-03-19 01:52:48 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 0.9242
2022-03-19 01:53:19 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 1.0798
2022-03-19 01:53:51 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 1.1558
2022-03-19 01:54:23 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 1.0907
2022-03-19 01:54:55 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 0.8771
2022-03-19 01:55:27 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 0.9501
2022-03-19 01:55:58 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 1.1034
2022-03-19 01:56:30 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 0.9850
2022-03-19 01:57:02 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 0.9028
2022-03-19 01:57:34 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 1.1323
2022-03-19 01:58:05 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 1.2439
2022-03-19 01:58:37 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 1.0420
2022-03-19 01:59:09 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 0.9698
2022-03-19 01:59:41 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 0.9546
2022-03-19 02:00:13 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 1.1360
2022-03-19 02:00:45 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 1.1736
2022-03-19 02:01:17 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 1.0763
2022-03-19 02:01:48 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 0.8169
2022-03-19 02:02:20 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 1.2192
2022-03-19 02:02:52 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 1.0560
2022-03-19 02:03:24 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 1.1469
2022-03-19 02:03:56 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 1.0715
2022-03-19 02:04:27 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 0.9560
2022-03-19 02:04:59 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 1.1093
2022-03-19 02:05:31 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 0.9114
2022-03-19 02:05:32 - train: epoch 098, train_loss: 1.0712
2022-03-19 02:06:45 - eval: epoch: 098, acc1: 73.934%, acc5: 91.588%, test_loss: 1.0293, per_image_load_time: 2.171ms, per_image_inference_time: 0.511ms
2022-03-19 02:06:45 - until epoch: 098, best_acc1: 73.976%
2022-03-19 02:06:45 - epoch 099 lr: 0.00010000000000000003
2022-03-19 02:07:23 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 1.0726
2022-03-19 02:07:55 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 1.0366
2022-03-19 02:08:26 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 1.0523
2022-03-19 02:08:58 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 0.9424
2022-03-19 02:09:30 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 0.8903
2022-03-19 02:10:01 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 1.1987
2022-03-19 02:10:33 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 1.1563
2022-03-19 02:11:05 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 1.0337
2022-03-19 02:11:36 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 1.0721
2022-03-19 02:12:08 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 0.9674
2022-03-19 02:12:40 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 1.1533
2022-03-19 02:13:12 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 1.0925
2022-03-19 02:13:44 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 0.8705
2022-03-19 02:14:15 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 1.0654
2022-03-19 02:14:47 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 0.9626
2022-03-19 02:15:19 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 1.0432
2022-03-19 02:15:50 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 1.0734
2022-03-19 02:16:22 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 1.0285
2022-03-19 02:16:54 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 1.1761
2022-03-19 02:17:26 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 0.9547
2022-03-19 02:17:58 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 1.0262
2022-03-19 02:18:30 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 1.0166
2022-03-19 02:19:02 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 1.0069
2022-03-19 02:19:34 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 1.1646
2022-03-19 02:20:06 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 0.9079
2022-03-19 02:20:38 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 0.9862
2022-03-19 02:21:10 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 1.1230
2022-03-19 02:21:42 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 1.0214
2022-03-19 02:22:14 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 1.0444
2022-03-19 02:22:46 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 1.1281
2022-03-19 02:23:18 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 1.0093
2022-03-19 02:23:50 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 1.2012
2022-03-19 02:24:21 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 1.0329
2022-03-19 02:24:53 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 1.0075
2022-03-19 02:25:25 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 1.1782
2022-03-19 02:25:56 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 1.0238
2022-03-19 02:26:28 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 0.8729
2022-03-19 02:27:00 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 1.0815
2022-03-19 02:27:32 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 1.1493
2022-03-19 02:28:04 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 1.1411
2022-03-19 02:28:36 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 0.8624
2022-03-19 02:29:07 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 1.2488
2022-03-19 02:29:39 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 1.1234
2022-03-19 02:30:11 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 1.2353
2022-03-19 02:30:43 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 1.1311
2022-03-19 02:31:14 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 1.1344
2022-03-19 02:31:46 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 1.0273
2022-03-19 02:32:19 - train: epoch 0099, iter [04800, 05004], lr: 0.000100, loss: 1.1925
2022-03-19 02:32:50 - train: epoch 0099, iter [04900, 05004], lr: 0.000100, loss: 1.1458
2022-03-19 02:33:21 - train: epoch 0099, iter [05000, 05004], lr: 0.000100, loss: 1.0072
2022-03-19 02:33:23 - train: epoch 099, train_loss: 1.0679
2022-03-19 02:34:36 - eval: epoch: 099, acc1: 73.918%, acc5: 91.654%, test_loss: 1.0293, per_image_load_time: 2.322ms, per_image_inference_time: 0.514ms
2022-03-19 02:34:36 - until epoch: 099, best_acc1: 73.976%
2022-03-19 02:34:36 - epoch 100 lr: 0.00010000000000000003
2022-03-19 02:35:14 - train: epoch 0100, iter [00100, 05004], lr: 0.000100, loss: 1.2016
2022-03-19 02:35:46 - train: epoch 0100, iter [00200, 05004], lr: 0.000100, loss: 1.1426
2022-03-19 02:36:18 - train: epoch 0100, iter [00300, 05004], lr: 0.000100, loss: 1.0781
2022-03-19 02:36:50 - train: epoch 0100, iter [00400, 05004], lr: 0.000100, loss: 1.0094
2022-03-19 02:37:22 - train: epoch 0100, iter [00500, 05004], lr: 0.000100, loss: 1.1097
2022-03-19 02:37:54 - train: epoch 0100, iter [00600, 05004], lr: 0.000100, loss: 1.1883
2022-03-19 02:38:26 - train: epoch 0100, iter [00700, 05004], lr: 0.000100, loss: 0.8738
2022-03-19 02:38:58 - train: epoch 0100, iter [00800, 05004], lr: 0.000100, loss: 1.0321
2022-03-19 02:39:31 - train: epoch 0100, iter [00900, 05004], lr: 0.000100, loss: 1.2041
2022-03-19 02:40:03 - train: epoch 0100, iter [01000, 05004], lr: 0.000100, loss: 1.2152
2022-03-19 02:40:35 - train: epoch 0100, iter [01100, 05004], lr: 0.000100, loss: 0.9775
2022-03-19 02:41:07 - train: epoch 0100, iter [01200, 05004], lr: 0.000100, loss: 1.0098
2022-03-19 02:41:39 - train: epoch 0100, iter [01300, 05004], lr: 0.000100, loss: 0.9650
2022-03-19 02:42:11 - train: epoch 0100, iter [01400, 05004], lr: 0.000100, loss: 1.0718
2022-03-19 02:42:43 - train: epoch 0100, iter [01500, 05004], lr: 0.000100, loss: 1.2193
2022-03-19 02:43:15 - train: epoch 0100, iter [01600, 05004], lr: 0.000100, loss: 1.0762
2022-03-19 02:43:47 - train: epoch 0100, iter [01700, 05004], lr: 0.000100, loss: 1.1211
2022-03-19 02:44:19 - train: epoch 0100, iter [01800, 05004], lr: 0.000100, loss: 0.9879
2022-03-19 02:44:51 - train: epoch 0100, iter [01900, 05004], lr: 0.000100, loss: 0.9774
2022-03-19 02:45:23 - train: epoch 0100, iter [02000, 05004], lr: 0.000100, loss: 1.1241
2022-03-19 02:45:55 - train: epoch 0100, iter [02100, 05004], lr: 0.000100, loss: 1.1040
2022-03-19 02:46:27 - train: epoch 0100, iter [02200, 05004], lr: 0.000100, loss: 1.1751
2022-03-19 02:46:59 - train: epoch 0100, iter [02300, 05004], lr: 0.000100, loss: 1.0786
2022-03-19 02:47:31 - train: epoch 0100, iter [02400, 05004], lr: 0.000100, loss: 1.1137
2022-03-19 02:48:03 - train: epoch 0100, iter [02500, 05004], lr: 0.000100, loss: 1.2403
2022-03-19 02:48:35 - train: epoch 0100, iter [02600, 05004], lr: 0.000100, loss: 1.0931
2022-03-19 02:49:07 - train: epoch 0100, iter [02700, 05004], lr: 0.000100, loss: 1.0461
2022-03-19 02:49:38 - train: epoch 0100, iter [02800, 05004], lr: 0.000100, loss: 1.0244
2022-03-19 02:50:10 - train: epoch 0100, iter [02900, 05004], lr: 0.000100, loss: 1.0667
2022-03-19 02:50:42 - train: epoch 0100, iter [03000, 05004], lr: 0.000100, loss: 0.9965
2022-03-19 02:51:15 - train: epoch 0100, iter [03100, 05004], lr: 0.000100, loss: 0.7838
2022-03-19 02:51:47 - train: epoch 0100, iter [03200, 05004], lr: 0.000100, loss: 1.1176
2022-03-19 02:52:19 - train: epoch 0100, iter [03300, 05004], lr: 0.000100, loss: 1.0336
2022-03-19 02:52:51 - train: epoch 0100, iter [03400, 05004], lr: 0.000100, loss: 1.0934
2022-03-19 02:53:23 - train: epoch 0100, iter [03500, 05004], lr: 0.000100, loss: 0.8745
2022-03-19 02:53:55 - train: epoch 0100, iter [03600, 05004], lr: 0.000100, loss: 0.9774
2022-03-19 02:54:27 - train: epoch 0100, iter [03700, 05004], lr: 0.000100, loss: 0.9062
2022-03-19 02:54:59 - train: epoch 0100, iter [03800, 05004], lr: 0.000100, loss: 1.0345
2022-03-19 02:55:31 - train: epoch 0100, iter [03900, 05004], lr: 0.000100, loss: 1.1505
2022-03-19 02:56:03 - train: epoch 0100, iter [04000, 05004], lr: 0.000100, loss: 0.9236
2022-03-19 02:56:35 - train: epoch 0100, iter [04100, 05004], lr: 0.000100, loss: 0.9333
2022-03-19 02:57:07 - train: epoch 0100, iter [04200, 05004], lr: 0.000100, loss: 1.1208
2022-03-19 02:57:39 - train: epoch 0100, iter [04300, 05004], lr: 0.000100, loss: 0.9135
2022-03-19 02:58:11 - train: epoch 0100, iter [04400, 05004], lr: 0.000100, loss: 1.0033
2022-03-19 02:58:43 - train: epoch 0100, iter [04500, 05004], lr: 0.000100, loss: 1.0166
2022-03-19 02:59:15 - train: epoch 0100, iter [04600, 05004], lr: 0.000100, loss: 0.9235
2022-03-19 02:59:47 - train: epoch 0100, iter [04700, 05004], lr: 0.000100, loss: 1.1241
2022-03-19 03:00:19 - train: epoch 0100, iter [04800, 05004], lr: 0.000100, loss: 0.9851
2022-03-19 03:00:51 - train: epoch 0100, iter [04900, 05004], lr: 0.000100, loss: 1.0070
2022-03-19 03:01:23 - train: epoch 0100, iter [05000, 05004], lr: 0.000100, loss: 1.3163
2022-03-19 03:01:24 - train: epoch 100, train_loss: 1.0684
2022-03-19 03:02:37 - eval: epoch: 100, acc1: 74.004%, acc5: 91.562%, test_loss: 1.0290, per_image_load_time: 2.346ms, per_image_inference_time: 0.524ms
2022-03-19 03:02:38 - until epoch: 100, best_acc1: 74.004%
2022-03-19 03:02:38 - train done. model: yoloxlbackbone, train time: 48.193 hours, best_acc1: 74.004%
