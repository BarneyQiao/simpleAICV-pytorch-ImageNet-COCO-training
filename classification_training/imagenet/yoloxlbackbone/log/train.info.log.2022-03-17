2022-03-17 00:13:53 - network: yoloxlbackbone
2022-03-17 00:13:53 - num_classes: 1000
2022-03-17 00:13:53 - input_image_size: 256
2022-03-17 00:13:53 - scale: 1.1428571428571428
2022-03-17 00:13:53 - trained_model_path: 
2022-03-17 00:13:53 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-17 00:13:53 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fb914957a30>
2022-03-17 00:13:53 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fb914957d00>
2022-03-17 00:13:53 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fb914957d30>
2022-03-17 00:13:53 - seed: 0
2022-03-17 00:13:53 - batch_size: 256
2022-03-17 00:13:53 - num_workers: 16
2022-03-17 00:13:53 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-17 00:13:53 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-03-17 00:13:53 - epochs: 100
2022-03-17 00:13:53 - print_interval: 100
2022-03-17 00:13:53 - distributed: True
2022-03-17 00:13:53 - sync_bn: False
2022-03-17 00:13:53 - apex: True
2022-03-17 00:13:53 - gpus_type: NVIDIA RTX A5000
2022-03-17 00:13:53 - gpus_num: 2
2022-03-17 00:13:53 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fb8f48ad970>
2022-03-17 00:13:53 - --------------------parameters--------------------
2022-03-17 00:13:53 - name: conv.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: conv.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: conv.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer1.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer1.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer1.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.4.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.4.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.4.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.4.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.4.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.4.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.5.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.5.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.5.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.5.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.5.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.5.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.6.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.6.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.6.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.6.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.6.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.6.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.7.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.7.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.7.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.7.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.7.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.7.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.8.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.8.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.8.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.8.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.8.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.8.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.4.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.4.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.4.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.4.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.4.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.4.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.5.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.5.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.5.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.5.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.5.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.5.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.6.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.6.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.6.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.6.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.6.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.6.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.7.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.7.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.7.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.7.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.7.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.7.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.8.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.8.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.8.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.8.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.8.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.8.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer4.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer4.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer4.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-17 00:13:53 - name: fc.weight, grad: True
2022-03-17 00:13:53 - name: fc.bias, grad: True
2022-03-17 00:13:53 - --------------------buffers--------------------
2022-03-17 00:13:53 - name: conv.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: conv.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: conv.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer1.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer1.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer1.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer1.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.4.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.4.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.4.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.4.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.5.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.5.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.5.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.5.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.6.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.6.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.6.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.6.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.7.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.7.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.7.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.7.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.8.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.8.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.8.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.8.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.8.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer2.1.bottlenecks.8.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.4.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.4.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.4.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.4.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.5.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.5.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.5.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.5.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.6.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.6.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.6.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.6.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.7.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.7.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.7.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.7.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.8.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.8.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.8.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.8.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.8.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer3.1.bottlenecks.8.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer4.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer4.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer4.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-17 00:13:53 - name: layer4.2.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-17 00:13:53 - epoch 001 lr: 0.1
2022-03-17 00:14:33 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9052
2022-03-17 00:15:06 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.8652
2022-03-17 00:15:39 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.7779
2022-03-17 00:16:12 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.8162
2022-03-17 00:16:45 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.7036
2022-03-17 00:17:18 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.6055
2022-03-17 00:17:51 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.6735
2022-03-17 00:18:24 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.5683
2022-03-17 00:18:57 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.4735
2022-03-17 00:19:30 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.3784
2022-03-17 00:20:03 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.2282
2022-03-17 00:20:35 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 6.0668
2022-03-17 00:21:08 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 6.0834
2022-03-17 00:21:40 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.9183
2022-03-17 00:22:13 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.9405
2022-03-17 00:22:46 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.9433
2022-03-17 00:23:18 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.6565
2022-03-17 00:23:51 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.7039
2022-03-17 00:24:24 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.6605
2022-03-17 00:24:57 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.5656
2022-03-17 00:25:29 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.4696
2022-03-17 00:26:02 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.4719
2022-03-17 00:26:35 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 5.2701
2022-03-17 00:27:07 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 5.2254
2022-03-17 00:27:40 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.1473
2022-03-17 00:28:13 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.3369
2022-03-17 00:28:46 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.3017
2022-03-17 00:29:19 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 5.0976
2022-03-17 00:29:52 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 5.0189
2022-03-17 00:30:25 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.1220
2022-03-17 00:30:58 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.0956
2022-03-17 00:31:31 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.9328
2022-03-17 00:32:04 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.7875
2022-03-17 00:32:37 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.7553
2022-03-17 00:33:09 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.7470
2022-03-17 00:33:42 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.7908
2022-03-17 00:34:15 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.8570
2022-03-17 00:34:47 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.6810
2022-03-17 00:35:20 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.8249
2022-03-17 00:35:53 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.5351
2022-03-17 00:36:25 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.6747
2022-03-17 00:36:58 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.5115
2022-03-17 00:37:30 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.4652
2022-03-17 00:38:03 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.3118
2022-03-17 00:38:36 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.6085
2022-03-17 00:39:09 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.5734
2022-03-17 00:39:42 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.2968
2022-03-17 00:40:15 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.5530
2022-03-17 00:40:47 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.3555
2022-03-17 00:41:19 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.1936
2022-03-17 00:41:21 - train: epoch 001, train_loss: 5.4088
2022-03-17 00:42:36 - eval: epoch: 001, acc1: 17.938%, acc5: 39.798%, test_loss: 4.1034, per_image_load_time: 2.367ms, per_image_inference_time: 0.555ms
2022-03-17 00:42:36 - until epoch: 001, best_acc1: 17.938%
2022-03-17 00:42:36 - epoch 002 lr: 0.1
2022-03-17 00:43:16 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.3209
2022-03-17 00:43:49 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.2016
2022-03-17 00:44:22 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.4932
2022-03-17 00:44:54 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.3304
2022-03-17 00:45:27 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.0224
2022-03-17 00:46:00 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.0201
2022-03-17 00:46:33 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.3646
2022-03-17 00:47:06 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.9531
2022-03-17 00:47:39 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.8033
2022-03-17 00:48:12 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.3036
2022-03-17 00:48:45 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.1893
2022-03-17 00:49:18 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.0840
2022-03-17 00:49:51 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.9100
2022-03-17 00:50:24 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.1552
2022-03-17 00:50:57 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.2543
2022-03-17 00:51:30 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.8993
2022-03-17 00:52:03 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.9933
2022-03-17 00:52:35 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.9809
2022-03-17 00:53:08 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.8411
2022-03-17 00:53:41 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.6391
2022-03-17 00:54:14 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.8871
2022-03-17 00:54:47 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.6027
2022-03-17 00:55:20 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.8422
2022-03-17 00:55:53 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.6979
2022-03-17 00:56:25 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.7552
2022-03-17 00:56:58 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.7731
2022-03-17 00:57:31 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 4.0695
2022-03-17 00:58:04 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.9963
2022-03-17 00:58:37 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.6743
2022-03-17 00:59:09 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.5901
2022-03-17 00:59:42 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.6645
2022-03-17 01:00:15 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.7639
2022-03-17 01:00:47 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.6583
2022-03-17 01:01:20 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.8467
2022-03-17 01:01:53 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.6383
2022-03-17 01:02:25 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.7024
2022-03-17 01:02:58 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.8310
2022-03-17 01:03:31 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.5424
2022-03-17 01:04:04 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.7706
2022-03-17 01:04:37 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.6716
2022-03-17 01:05:10 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.7482
2022-03-17 01:05:43 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.6319
2022-03-17 01:06:16 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.5041
2022-03-17 01:06:49 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.4357
2022-03-17 01:07:22 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.4294
2022-03-17 01:07:55 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.5840
2022-03-17 01:08:28 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.5449
2022-03-17 01:09:00 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.5345
2022-03-17 01:09:33 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.4022
2022-03-17 01:10:05 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.4604
2022-03-17 01:10:06 - train: epoch 002, train_loss: 3.8577
2022-03-17 01:11:21 - eval: epoch: 002, acc1: 28.938%, acc5: 54.930%, test_loss: 3.3413, per_image_load_time: 1.373ms, per_image_inference_time: 0.543ms
2022-03-17 01:11:22 - until epoch: 002, best_acc1: 28.938%
2022-03-17 01:11:22 - epoch 003 lr: 0.1
2022-03-17 01:12:01 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.5939
2022-03-17 01:12:34 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.4771
2022-03-17 01:13:06 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.4842
2022-03-17 01:13:39 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.4363
2022-03-17 01:14:12 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.6277
2022-03-17 01:14:44 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.3330
2022-03-17 01:15:17 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.7304
2022-03-17 01:15:49 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.5118
2022-03-17 01:16:22 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.4473
2022-03-17 01:16:55 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.3390
2022-03-17 01:17:27 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.3499
2022-03-17 01:18:00 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.4262
2022-03-17 01:18:33 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.4311
2022-03-17 01:19:05 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.2752
2022-03-17 01:19:38 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.5849
2022-03-17 01:20:11 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.3927
2022-03-17 01:20:44 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.2767
2022-03-17 01:21:17 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.1702
2022-03-17 01:21:50 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.4737
2022-03-17 01:22:23 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.6095
2022-03-17 01:22:56 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.6123
2022-03-17 01:23:28 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.7246
2022-03-17 01:24:01 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.3207
2022-03-17 01:24:34 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.2492
2022-03-17 01:25:07 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.3998
2022-03-17 01:25:40 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.3157
2022-03-17 01:26:13 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.5726
2022-03-17 01:26:46 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.2184
2022-03-17 01:27:19 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.2707
2022-03-17 01:27:52 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.5205
2022-03-17 01:28:25 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.3711
2022-03-17 01:28:58 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.3562
2022-03-17 01:29:31 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.3224
2022-03-17 01:30:04 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.4578
2022-03-17 01:30:37 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.0849
2022-03-17 01:31:10 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.2211
2022-03-17 01:31:44 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.2197
2022-03-17 01:32:16 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.3245
2022-03-17 01:32:49 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.5088
2022-03-17 01:33:22 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.1336
2022-03-17 01:33:55 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.2033
2022-03-17 01:34:28 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.1610
2022-03-17 01:35:01 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.9530
2022-03-17 01:35:34 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.9933
2022-03-17 01:36:07 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.1370
2022-03-17 01:36:40 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.1544
2022-03-17 01:37:13 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.1253
2022-03-17 01:37:46 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.2218
2022-03-17 01:38:19 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.3211
2022-03-17 01:38:51 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.2070
2022-03-17 01:38:53 - train: epoch 003, train_loss: 3.3305
2022-03-17 01:40:08 - eval: epoch: 003, acc1: 35.234%, acc5: 61.944%, test_loss: 2.9538, per_image_load_time: 2.149ms, per_image_inference_time: 0.572ms
2022-03-17 01:40:09 - until epoch: 003, best_acc1: 35.234%
2022-03-17 01:40:09 - epoch 004 lr: 0.1
2022-03-17 01:40:48 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.0619
2022-03-17 01:41:21 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.1031
2022-03-17 01:41:54 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.0771
2022-03-17 01:42:27 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 3.1718
2022-03-17 01:42:59 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.0578
2022-03-17 01:43:32 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.2876
2022-03-17 01:44:05 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.1694
2022-03-17 01:44:38 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.9882
2022-03-17 01:45:11 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.8312
2022-03-17 01:45:44 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.1151
2022-03-17 01:46:17 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.1992
2022-03-17 01:46:50 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.9457
2022-03-17 01:47:23 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 3.0463
2022-03-17 01:47:56 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.1160
2022-03-17 01:48:29 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.2537
2022-03-17 01:49:03 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.9208
2022-03-17 01:49:36 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.1195
2022-03-17 01:50:09 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.3783
2022-03-17 01:50:42 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.1461
2022-03-17 01:51:15 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.0846
2022-03-17 01:51:47 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.1298
2022-03-17 01:52:20 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.0904
2022-03-17 01:52:53 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.8987
2022-03-17 01:53:26 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.9067
2022-03-17 01:53:59 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.8560
2022-03-17 01:54:33 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.1229
2022-03-17 01:55:06 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.8530
2022-03-17 01:55:39 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.0047
2022-03-17 01:56:12 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.9832
2022-03-17 01:56:45 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.9713
2022-03-17 01:57:18 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.0638
2022-03-17 01:57:50 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 3.0063
2022-03-17 01:58:23 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.0616
2022-03-17 01:58:56 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.9806
2022-03-17 01:59:29 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.9666
2022-03-17 02:00:01 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.7905
2022-03-17 02:00:35 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.0069
2022-03-17 02:01:08 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.8777
2022-03-17 02:01:41 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.9578
2022-03-17 02:02:13 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.6250
2022-03-17 02:02:46 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.9090
2022-03-17 02:03:19 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.7791
2022-03-17 02:03:52 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.9140
2022-03-17 02:04:25 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.7176
2022-03-17 02:04:58 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.5320
2022-03-17 02:05:31 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.8598
2022-03-17 02:06:03 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.7518
2022-03-17 02:06:36 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.8170
2022-03-17 02:07:09 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.7496
2022-03-17 02:07:41 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.9352
2022-03-17 02:07:42 - train: epoch 004, train_loss: 3.0465
2022-03-17 02:08:58 - eval: epoch: 004, acc1: 40.926%, acc5: 67.326%, test_loss: 2.6523, per_image_load_time: 2.389ms, per_image_inference_time: 0.521ms
2022-03-17 02:08:59 - until epoch: 004, best_acc1: 40.926%
2022-03-17 02:08:59 - epoch 005 lr: 0.1
2022-03-17 02:09:37 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.9706
2022-03-17 02:10:10 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.9447
2022-03-17 02:10:43 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.0441
2022-03-17 02:11:16 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.9478
2022-03-17 02:11:48 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.7429
2022-03-17 02:12:21 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.7963
2022-03-17 02:12:53 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 3.0474
2022-03-17 02:13:26 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.2335
2022-03-17 02:13:58 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.8556
2022-03-17 02:14:30 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.9279
2022-03-17 02:15:02 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.0247
2022-03-17 02:15:35 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.9129
2022-03-17 02:16:07 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.9272
2022-03-17 02:16:40 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.0490
2022-03-17 02:17:13 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.7561
2022-03-17 02:17:46 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.7569
2022-03-17 02:18:19 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.8631
2022-03-17 02:18:52 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.9354
2022-03-17 02:19:25 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.7180
2022-03-17 02:19:58 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.9041
2022-03-17 02:20:31 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.7162
2022-03-17 02:21:04 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.7828
2022-03-17 02:21:37 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.6982
2022-03-17 02:22:10 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.7886
2022-03-17 02:22:43 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.9444
2022-03-17 02:23:16 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.1245
2022-03-17 02:23:49 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.0682
2022-03-17 02:24:22 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.8823
2022-03-17 02:24:55 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.6970
2022-03-17 02:25:27 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.8152
2022-03-17 02:26:01 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.9051
2022-03-17 02:26:33 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.9003
2022-03-17 02:27:06 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.6706
2022-03-17 02:27:39 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.6872
2022-03-17 02:28:12 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.9942
2022-03-17 02:28:45 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.8732
2022-03-17 02:29:18 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.7537
2022-03-17 02:29:51 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.7449
2022-03-17 02:30:24 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.1565
2022-03-17 02:30:57 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.8587
2022-03-17 02:31:30 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.8610
2022-03-17 02:32:03 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.7921
2022-03-17 02:32:36 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.7318
2022-03-17 02:33:10 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.9569
2022-03-17 02:33:43 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.9277
2022-03-17 02:34:16 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.8220
2022-03-17 02:34:49 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.6301
2022-03-17 02:35:22 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.7019
2022-03-17 02:35:55 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.8762
2022-03-17 02:36:28 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.7347
2022-03-17 02:36:29 - train: epoch 005, train_loss: 2.8733
2022-03-17 02:37:43 - eval: epoch: 005, acc1: 41.232%, acc5: 68.144%, test_loss: 2.5944, per_image_load_time: 1.375ms, per_image_inference_time: 0.506ms
2022-03-17 02:37:44 - until epoch: 005, best_acc1: 41.232%
2022-03-17 02:37:44 - epoch 006 lr: 0.1
2022-03-17 02:38:23 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.7528
2022-03-17 02:38:55 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.9187
2022-03-17 02:39:28 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.5403
2022-03-17 02:40:01 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.8015
2022-03-17 02:40:35 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.7568
2022-03-17 02:41:07 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.8371
2022-03-17 02:41:40 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.9377
2022-03-17 02:42:13 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.8199
2022-03-17 02:42:46 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.7552
2022-03-17 02:43:19 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.6082
2022-03-17 02:43:52 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.7560
2022-03-17 02:44:26 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.9069
2022-03-17 02:44:58 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.9921
2022-03-17 02:45:32 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.8408
2022-03-17 02:46:05 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.9401
2022-03-17 02:46:38 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.6161
2022-03-17 02:47:11 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.8544
2022-03-17 02:47:44 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.9834
2022-03-17 02:48:17 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.6539
2022-03-17 02:48:50 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.9023
2022-03-17 02:49:23 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.9504
2022-03-17 02:49:56 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.6794
2022-03-17 02:50:29 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.5940
2022-03-17 02:51:02 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.7625
2022-03-17 02:51:36 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.8350
2022-03-17 02:52:09 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.6154
2022-03-17 02:52:42 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.8373
2022-03-17 02:53:15 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.6764
2022-03-17 02:53:49 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.8210
2022-03-17 02:54:22 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.6739
2022-03-17 02:54:55 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.6007
2022-03-17 02:55:28 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.6695
2022-03-17 02:56:01 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.5216
2022-03-17 02:56:34 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.9129
2022-03-17 02:57:08 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.8436
2022-03-17 02:57:41 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.8229
2022-03-17 02:58:14 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.7953
2022-03-17 02:58:47 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.5887
2022-03-17 02:59:20 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.5812
2022-03-17 02:59:53 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.0157
2022-03-17 03:00:27 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.6154
2022-03-17 03:01:00 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.6144
2022-03-17 03:01:33 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.6517
2022-03-17 03:02:06 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.7918
2022-03-17 03:02:40 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.7807
2022-03-17 03:03:13 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.7880
2022-03-17 03:03:46 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.8059
2022-03-17 03:04:19 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.8306
2022-03-17 03:04:52 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.8123
2022-03-17 03:05:24 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.4759
2022-03-17 03:05:26 - train: epoch 006, train_loss: 2.7568
2022-03-17 03:06:41 - eval: epoch: 006, acc1: 45.234%, acc5: 71.568%, test_loss: 2.4085, per_image_load_time: 2.310ms, per_image_inference_time: 0.518ms
2022-03-17 03:06:42 - until epoch: 006, best_acc1: 45.234%
2022-03-17 03:06:42 - epoch 007 lr: 0.1
2022-03-17 03:07:21 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.5516
2022-03-17 03:07:54 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.8333
2022-03-17 03:08:28 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.9704
2022-03-17 03:09:01 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.7460
2022-03-17 03:09:34 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.5951
2022-03-17 03:10:07 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.7666
2022-03-17 03:10:41 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.7640
2022-03-17 03:11:14 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.6999
2022-03-17 03:11:47 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.7615
2022-03-17 03:12:20 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.6399
2022-03-17 03:12:53 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.5499
2022-03-17 03:13:27 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.6389
2022-03-17 03:14:00 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.5824
2022-03-17 03:14:33 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.8056
2022-03-17 03:15:06 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.8052
2022-03-17 03:15:40 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.6657
2022-03-17 03:16:13 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.7034
2022-03-17 03:16:46 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.6449
2022-03-17 03:17:19 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.7197
2022-03-17 03:17:52 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.4453
2022-03-17 03:18:26 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.7353
2022-03-17 03:18:59 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.4541
2022-03-17 03:19:32 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.7122
2022-03-17 03:20:06 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.7050
2022-03-17 03:20:39 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.6479
2022-03-17 03:21:12 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.6286
2022-03-17 03:21:45 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.6166
2022-03-17 03:22:19 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.6425
2022-03-17 03:22:52 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.6146
2022-03-17 03:23:25 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.8475
2022-03-17 03:23:58 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.4998
2022-03-17 03:24:32 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.6128
2022-03-17 03:25:05 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 3.0007
2022-03-17 03:25:38 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.5137
2022-03-17 03:26:12 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.7959
2022-03-17 03:26:45 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.5535
2022-03-17 03:27:19 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.7700
2022-03-17 03:27:52 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.9028
2022-03-17 03:28:25 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.6335
2022-03-17 03:28:58 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.7185
2022-03-17 03:29:32 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.5063
2022-03-17 03:30:05 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.5097
2022-03-17 03:30:38 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.8941
2022-03-17 03:31:12 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.4813
2022-03-17 03:31:45 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.7142
2022-03-17 03:32:18 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.5909
2022-03-17 03:32:52 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.8049
2022-03-17 03:33:25 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.8652
2022-03-17 03:33:59 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.6591
2022-03-17 03:34:32 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.6291
2022-03-17 03:34:33 - train: epoch 007, train_loss: 2.6742
2022-03-17 03:35:49 - eval: epoch: 007, acc1: 45.766%, acc5: 72.308%, test_loss: 2.3524, per_image_load_time: 2.348ms, per_image_inference_time: 0.516ms
2022-03-17 03:35:50 - until epoch: 007, best_acc1: 45.766%
2022-03-17 03:35:50 - epoch 008 lr: 0.1
2022-03-17 03:36:29 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.6537
2022-03-17 03:37:02 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.6873
2022-03-17 03:37:35 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.5067
2022-03-17 03:38:08 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.4409
2022-03-17 03:38:42 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.4210
2022-03-17 03:39:15 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.6010
2022-03-17 03:39:49 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.9493
2022-03-17 03:40:22 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.5265
2022-03-17 03:40:55 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.6775
2022-03-17 03:41:28 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.5665
2022-03-17 03:42:02 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.4001
2022-03-17 03:42:35 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.3887
2022-03-17 03:43:08 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.6400
2022-03-17 03:43:42 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.5007
2022-03-17 03:44:15 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.7555
2022-03-17 03:44:49 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.7209
2022-03-17 03:45:22 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.5629
2022-03-17 03:45:55 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.7699
2022-03-17 03:46:28 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.3731
2022-03-17 03:47:01 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.6567
2022-03-17 03:47:35 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.6258
2022-03-17 03:48:08 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.5639
2022-03-17 03:48:42 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.7133
2022-03-17 03:49:15 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.4364
2022-03-17 03:49:48 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.5991
2022-03-17 03:50:22 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.8262
2022-03-17 03:50:55 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.7288
2022-03-17 03:51:28 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.7188
2022-03-17 03:52:02 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.6762
2022-03-17 03:52:35 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.7138
2022-03-17 03:53:08 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.5047
2022-03-17 03:53:42 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.8134
2022-03-17 03:54:15 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.7355
2022-03-17 03:54:48 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.7537
2022-03-17 03:55:21 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.6004
2022-03-17 03:55:55 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.7611
2022-03-17 03:56:28 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.5169
2022-03-17 03:57:01 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.4099
2022-03-17 03:57:34 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.7473
2022-03-17 03:58:08 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.9076
2022-03-17 03:58:41 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.5049
2022-03-17 03:59:14 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.5006
2022-03-17 03:59:47 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.3127
2022-03-17 04:00:21 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.5646
2022-03-17 04:00:54 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.6962
2022-03-17 04:01:27 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.6547
2022-03-17 04:02:00 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.4568
2022-03-17 04:02:34 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.4984
2022-03-17 04:03:07 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.5907
2022-03-17 04:03:40 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.6012
2022-03-17 04:03:41 - train: epoch 008, train_loss: 2.6106
2022-03-17 04:04:56 - eval: epoch: 008, acc1: 46.390%, acc5: 73.214%, test_loss: 2.3187, per_image_load_time: 2.393ms, per_image_inference_time: 0.482ms
2022-03-17 04:04:57 - until epoch: 008, best_acc1: 46.390%
2022-03-17 04:04:57 - epoch 009 lr: 0.1
2022-03-17 04:05:37 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.3946
2022-03-17 04:06:10 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.4363
2022-03-17 04:06:43 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.3070
2022-03-17 04:07:16 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.7865
2022-03-17 04:07:49 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.4552
2022-03-17 04:08:22 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.5914
2022-03-17 04:08:55 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.4112
2022-03-17 04:09:29 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.3728
2022-03-17 04:10:02 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.4724
2022-03-17 04:10:35 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.3965
2022-03-17 04:11:08 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.8494
2022-03-17 04:11:42 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.7325
2022-03-17 04:12:15 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.6982
2022-03-17 04:12:48 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.2532
2022-03-17 04:13:21 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.4809
2022-03-17 04:13:54 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.5959
2022-03-17 04:14:27 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.6071
2022-03-17 04:15:01 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.4948
2022-03-17 04:15:34 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.2943
2022-03-17 04:16:07 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.2899
2022-03-17 04:16:41 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.6022
2022-03-17 04:17:14 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.6616
2022-03-17 04:17:47 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.2886
2022-03-17 04:18:21 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.4463
2022-03-17 04:18:54 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.3679
2022-03-17 04:19:27 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.5257
2022-03-17 04:20:01 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.5427
2022-03-17 04:20:34 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.6835
2022-03-17 04:21:07 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.2507
2022-03-17 04:21:41 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.5079
2022-03-17 04:22:14 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.5822
2022-03-17 04:22:47 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.5736
2022-03-17 04:23:20 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.5535
2022-03-17 04:23:53 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.8607
2022-03-17 04:24:27 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.6136
2022-03-17 04:25:00 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.4461
2022-03-17 04:25:33 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.6352
2022-03-17 04:26:06 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.6686
2022-03-17 04:26:39 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.3348
2022-03-17 04:27:13 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.7311
2022-03-17 04:27:46 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.5668
2022-03-17 04:28:19 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.5464
2022-03-17 04:28:53 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.7438
2022-03-17 04:29:26 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.5453
2022-03-17 04:30:00 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.6060
2022-03-17 04:30:33 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.6004
2022-03-17 04:31:06 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.6667
2022-03-17 04:31:39 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.7765
2022-03-17 04:32:12 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.5420
2022-03-17 04:32:45 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.4052
2022-03-17 04:32:46 - train: epoch 009, train_loss: 2.5621
2022-03-17 04:34:02 - eval: epoch: 009, acc1: 47.814%, acc5: 74.504%, test_loss: 2.2358, per_image_load_time: 2.414ms, per_image_inference_time: 0.509ms
2022-03-17 04:34:03 - until epoch: 009, best_acc1: 47.814%
2022-03-17 04:34:03 - epoch 010 lr: 0.1
2022-03-17 04:34:42 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.4748
2022-03-17 04:35:16 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.6351
2022-03-17 04:35:50 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.5437
2022-03-17 04:36:23 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.5441
2022-03-17 04:36:56 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.4361
2022-03-17 04:37:30 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.4908
2022-03-17 04:38:03 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.5391
2022-03-17 04:38:37 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.4947
2022-03-17 04:39:10 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.3260
2022-03-17 04:39:43 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.3456
2022-03-17 04:40:17 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.5775
2022-03-17 04:40:50 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.3399
2022-03-17 04:41:24 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.3685
2022-03-17 04:41:57 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.5997
2022-03-17 04:42:31 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.0972
2022-03-17 04:43:04 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.5509
2022-03-17 04:43:37 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.7736
2022-03-17 04:44:11 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.5328
2022-03-17 04:44:44 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.5257
2022-03-17 04:45:17 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.5881
2022-03-17 04:45:50 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.5145
2022-03-17 04:46:24 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.6590
2022-03-17 04:46:57 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.6419
2022-03-17 04:47:31 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.6211
2022-03-17 04:48:04 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.6431
2022-03-17 04:48:38 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.5967
2022-03-17 04:49:11 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.3489
2022-03-17 04:49:45 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.5298
2022-03-17 04:50:18 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.6389
2022-03-17 04:50:51 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.4483
2022-03-17 04:51:24 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.6455
2022-03-17 04:51:57 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.4229
2022-03-17 04:52:30 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.5382
2022-03-17 04:53:03 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.7319
2022-03-17 04:53:36 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.7160
2022-03-17 04:54:09 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.8108
2022-03-17 04:54:43 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.5024
2022-03-17 04:55:16 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.6033
2022-03-17 04:55:49 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.1942
2022-03-17 04:56:22 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.5055
2022-03-17 04:56:56 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.4283
2022-03-17 04:57:29 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.6656
2022-03-17 04:58:02 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.5310
2022-03-17 04:58:35 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.5642
2022-03-17 04:59:09 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.3708
2022-03-17 04:59:42 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.5472
2022-03-17 05:00:15 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.5080
2022-03-17 05:00:48 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.4178
2022-03-17 05:01:22 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.2507
2022-03-17 05:01:54 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.3650
2022-03-17 05:01:56 - train: epoch 010, train_loss: 2.5210
2022-03-17 05:03:11 - eval: epoch: 010, acc1: 48.776%, acc5: 75.102%, test_loss: 2.1859, per_image_load_time: 2.406ms, per_image_inference_time: 0.533ms
2022-03-17 05:03:12 - until epoch: 010, best_acc1: 48.776%
2022-03-17 05:03:12 - epoch 011 lr: 0.1
2022-03-17 05:03:51 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.1987
2022-03-17 05:04:25 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.6618
2022-03-17 05:04:58 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.2829
2022-03-17 05:05:31 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.5330
2022-03-17 05:06:05 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.3969
2022-03-17 05:06:38 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.4222
2022-03-17 05:07:11 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.4241
2022-03-17 05:07:44 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.5721
2022-03-17 05:08:18 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.5988
2022-03-17 05:08:51 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.4116
2022-03-17 05:09:25 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.6356
2022-03-17 05:09:58 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.7643
2022-03-17 05:10:31 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.5807
2022-03-17 05:11:05 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.5317
2022-03-17 05:11:38 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.3379
2022-03-17 05:12:11 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.5106
2022-03-17 05:12:45 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.4994
2022-03-17 05:13:18 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.3605
2022-03-17 05:13:52 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.2735
2022-03-17 05:14:25 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.5492
2022-03-17 05:14:58 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.4119
2022-03-17 05:15:31 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.4457
2022-03-17 05:16:05 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.7123
2022-03-17 05:16:38 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.3066
2022-03-17 05:17:11 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.8220
2022-03-17 05:17:45 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.4989
2022-03-17 05:18:18 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.5527
2022-03-17 05:18:52 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.1816
2022-03-17 05:19:25 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.6045
2022-03-17 05:19:58 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.8147
2022-03-17 05:20:32 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.6095
2022-03-17 05:21:05 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.2376
2022-03-17 05:21:38 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.5704
2022-03-17 05:22:12 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.4957
2022-03-17 05:22:45 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.5400
2022-03-17 05:23:18 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.5386
2022-03-17 05:23:52 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.6206
2022-03-17 05:24:25 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.1914
2022-03-17 05:24:58 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.5165
2022-03-17 05:25:31 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.4384
2022-03-17 05:26:04 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.3207
2022-03-17 05:26:38 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.4737
2022-03-17 05:27:11 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.4830
2022-03-17 05:27:44 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.4194
2022-03-17 05:28:17 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.3189
2022-03-17 05:28:50 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.3888
2022-03-17 05:29:23 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.1769
2022-03-17 05:29:56 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.3556
2022-03-17 05:30:30 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.3637
2022-03-17 05:31:02 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.3825
2022-03-17 05:31:03 - train: epoch 011, train_loss: 2.4893
2022-03-17 05:32:19 - eval: epoch: 011, acc1: 50.116%, acc5: 76.312%, test_loss: 2.1347, per_image_load_time: 2.360ms, per_image_inference_time: 0.567ms
2022-03-17 05:32:20 - until epoch: 011, best_acc1: 50.116%
2022-03-17 05:32:20 - epoch 012 lr: 0.1
2022-03-17 05:32:59 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.2576
2022-03-17 05:33:32 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.3865
2022-03-17 05:34:05 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.5148
2022-03-17 05:34:38 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.6221
2022-03-17 05:35:12 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.6887
2022-03-17 05:35:45 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.1566
2022-03-17 05:36:18 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.2571
2022-03-17 05:36:51 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.5399
2022-03-17 05:37:24 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.5293
2022-03-17 05:37:57 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.2955
2022-03-17 05:38:30 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.8459
2022-03-17 05:39:02 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.3651
2022-03-17 05:39:35 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.4190
2022-03-17 05:40:08 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.5620
2022-03-17 05:40:41 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.3218
2022-03-17 05:41:14 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.5293
2022-03-17 05:41:47 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.3047
2022-03-17 05:42:20 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.5359
2022-03-17 05:42:54 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.4995
2022-03-17 05:43:27 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.6770
2022-03-17 05:44:00 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.5078
2022-03-17 05:44:33 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.6833
2022-03-17 05:45:06 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.4650
2022-03-17 05:45:39 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.5312
2022-03-17 05:46:12 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.1778
2022-03-17 05:46:45 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.3626
2022-03-17 05:47:18 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.3999
2022-03-17 05:47:52 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.4225
2022-03-17 05:48:25 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.3791
2022-03-17 05:48:57 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.3579
2022-03-17 05:49:31 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.5238
2022-03-17 05:50:04 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.1532
2022-03-17 05:50:37 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.4556
2022-03-17 05:51:10 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.4959
2022-03-17 05:51:43 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.6352
2022-03-17 05:52:16 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.5622
2022-03-17 05:52:49 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.3545
2022-03-17 05:53:22 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.3641
2022-03-17 05:53:55 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.3992
2022-03-17 05:54:29 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.3860
2022-03-17 05:55:02 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.3475
2022-03-17 05:55:35 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.4095
2022-03-17 05:56:08 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.4804
2022-03-17 05:56:41 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.2658
2022-03-17 05:57:14 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.4920
2022-03-17 05:57:47 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.8412
2022-03-17 05:58:21 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.3236
2022-03-17 05:58:54 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.6205
2022-03-17 05:59:27 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.5158
2022-03-17 05:59:59 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.1487
2022-03-17 06:00:01 - train: epoch 012, train_loss: 2.4623
2022-03-17 06:01:16 - eval: epoch: 012, acc1: 50.264%, acc5: 75.960%, test_loss: 2.1339, per_image_load_time: 2.353ms, per_image_inference_time: 0.552ms
2022-03-17 06:01:17 - until epoch: 012, best_acc1: 50.264%
2022-03-17 06:01:17 - epoch 013 lr: 0.1
2022-03-17 06:01:56 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.2260
2022-03-17 06:02:30 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.5258
2022-03-17 06:03:03 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.2957
2022-03-17 06:03:37 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.2097
2022-03-17 06:04:10 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.3846
2022-03-17 06:04:43 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.5633
2022-03-17 06:05:16 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.3136
2022-03-17 06:05:50 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.5196
2022-03-17 06:06:23 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.3319
2022-03-17 06:06:56 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.5348
2022-03-17 06:07:30 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.5835
2022-03-17 06:08:03 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.6042
2022-03-17 06:08:37 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.4596
2022-03-17 06:09:10 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.4309
2022-03-17 06:09:44 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.5606
2022-03-17 06:10:17 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.1053
2022-03-17 06:10:50 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.5345
2022-03-17 06:11:24 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.4088
2022-03-17 06:11:57 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.5215
2022-03-17 06:12:30 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.6326
2022-03-17 06:13:04 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.7401
2022-03-17 06:13:37 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.3492
2022-03-17 06:14:10 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.4240
2022-03-17 06:14:44 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.4607
2022-03-17 06:15:17 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.4103
2022-03-17 06:15:50 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.3232
2022-03-17 06:16:23 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.3418
2022-03-17 06:16:56 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.4951
2022-03-17 06:17:30 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.3933
2022-03-17 06:18:02 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.2848
2022-03-17 06:18:35 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.2767
2022-03-17 06:19:08 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.3743
2022-03-17 06:19:41 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.3168
2022-03-17 06:20:15 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.2918
2022-03-17 06:20:48 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.2755
2022-03-17 06:21:21 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.6543
2022-03-17 06:21:54 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.2139
2022-03-17 06:22:27 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.6082
2022-03-17 06:23:00 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.4802
2022-03-17 06:23:34 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.4644
2022-03-17 06:24:07 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.3017
2022-03-17 06:24:40 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.3902
2022-03-17 06:25:13 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.3730
2022-03-17 06:25:46 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.4600
2022-03-17 06:26:20 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.4513
2022-03-17 06:26:53 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.3948
2022-03-17 06:27:26 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.5526
2022-03-17 06:27:58 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.5560
2022-03-17 06:28:31 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.4401
2022-03-17 06:29:04 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.6183
2022-03-17 06:29:05 - train: epoch 013, train_loss: 2.4399
2022-03-17 06:30:21 - eval: epoch: 013, acc1: 51.474%, acc5: 77.180%, test_loss: 2.0655, per_image_load_time: 2.362ms, per_image_inference_time: 0.568ms
2022-03-17 06:30:22 - until epoch: 013, best_acc1: 51.474%
2022-03-17 06:30:22 - epoch 014 lr: 0.1
2022-03-17 06:31:01 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.2883
2022-03-17 06:31:34 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.5167
2022-03-17 06:32:08 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.0641
2022-03-17 06:32:41 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.3569
2022-03-17 06:33:14 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.3355
2022-03-17 06:33:47 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.4248
2022-03-17 06:34:20 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.4350
2022-03-17 06:34:53 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.4751
2022-03-17 06:35:26 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.5368
2022-03-17 06:35:58 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.6681
2022-03-17 06:36:31 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.4237
2022-03-17 06:37:04 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.4431
2022-03-17 06:37:37 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.4088
2022-03-17 06:38:10 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.5963
2022-03-17 06:38:43 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.5843
2022-03-17 06:39:17 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.3040
2022-03-17 06:39:50 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.6395
2022-03-17 06:40:23 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.5964
2022-03-17 06:40:56 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.2925
2022-03-17 06:41:29 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.3220
2022-03-17 06:42:02 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.5264
2022-03-17 06:42:36 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.4506
2022-03-17 06:43:09 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.3998
2022-03-17 06:43:42 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.6475
2022-03-17 06:44:15 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.4333
2022-03-17 06:44:48 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.4085
2022-03-17 06:45:22 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.4813
2022-03-17 06:45:55 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.6180
2022-03-17 06:46:28 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.2539
2022-03-17 06:47:01 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.5135
2022-03-17 06:47:35 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.2982
2022-03-17 06:48:08 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.3401
2022-03-17 06:48:41 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.2480
2022-03-17 06:49:15 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.4072
2022-03-17 06:49:48 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.3120
2022-03-17 06:50:21 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.3341
2022-03-17 06:50:54 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.3977
2022-03-17 06:51:27 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.5288
2022-03-17 06:52:01 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.3142
2022-03-17 06:52:34 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.6484
2022-03-17 06:53:07 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.4253
2022-03-17 06:53:41 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.2444
2022-03-17 06:54:14 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.2890
2022-03-17 06:54:47 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.2262
2022-03-17 06:55:20 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.4167
2022-03-17 06:55:54 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.3290
2022-03-17 06:56:27 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.2926
2022-03-17 06:57:01 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.3573
2022-03-17 06:57:34 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.1902
2022-03-17 06:58:06 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.5284
2022-03-17 06:58:08 - train: epoch 014, train_loss: 2.4205
2022-03-17 06:59:24 - eval: epoch: 014, acc1: 50.402%, acc5: 76.450%, test_loss: 2.1246, per_image_load_time: 2.364ms, per_image_inference_time: 0.537ms
2022-03-17 06:59:24 - until epoch: 014, best_acc1: 51.474%
2022-03-17 06:59:24 - epoch 015 lr: 0.1
2022-03-17 07:00:04 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.1184
2022-03-17 07:00:37 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.6801
2022-03-17 07:01:10 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.6043
2022-03-17 07:01:43 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.4035
2022-03-17 07:02:17 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.3136
2022-03-17 07:02:50 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.6499
2022-03-17 07:03:23 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.3183
2022-03-17 07:03:56 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.0989
2022-03-17 07:04:29 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.3165
2022-03-17 07:05:02 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.5059
2022-03-17 07:05:35 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.2254
2022-03-17 07:06:09 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.4203
2022-03-17 07:06:42 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.5697
2022-03-17 07:07:15 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.2861
2022-03-17 07:07:48 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.2085
2022-03-17 07:08:21 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.3653
2022-03-17 07:08:54 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.5622
2022-03-17 07:09:27 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.2468
2022-03-17 07:10:00 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.3191
2022-03-17 07:10:33 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.2673
2022-03-17 07:11:06 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.2943
2022-03-17 07:11:39 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.5634
2022-03-17 07:12:12 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.3204
2022-03-17 07:12:45 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.4588
2022-03-17 07:13:18 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.5409
2022-03-17 07:13:51 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.1330
2022-03-17 07:14:24 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.4539
2022-03-17 07:14:58 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.5479
2022-03-17 07:15:31 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.3238
2022-03-17 07:16:04 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.1963
2022-03-17 07:16:37 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.3618
2022-03-17 07:17:10 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.3996
2022-03-17 07:17:43 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.3328
2022-03-17 07:18:16 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.4332
2022-03-17 07:18:49 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.5942
2022-03-17 07:19:22 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.3939
2022-03-17 07:19:55 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.3308
2022-03-17 07:20:29 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.4156
2022-03-17 07:21:02 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.5479
2022-03-17 07:21:35 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.3535
2022-03-17 07:22:09 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.6415
2022-03-17 07:22:42 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.1266
2022-03-17 07:23:15 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.5420
2022-03-17 07:23:49 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.3953
2022-03-17 07:24:22 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.3989
2022-03-17 07:24:55 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.3335
2022-03-17 07:25:29 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.6163
2022-03-17 07:26:02 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.4083
2022-03-17 07:26:35 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.5291
2022-03-17 07:27:08 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.4839
2022-03-17 07:27:09 - train: epoch 015, train_loss: 2.4042
2022-03-17 07:28:25 - eval: epoch: 015, acc1: 51.922%, acc5: 77.484%, test_loss: 2.0519, per_image_load_time: 2.424ms, per_image_inference_time: 0.512ms
2022-03-17 07:28:26 - until epoch: 015, best_acc1: 51.922%
2022-03-17 07:28:26 - epoch 016 lr: 0.1
2022-03-17 07:29:06 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.3876
2022-03-17 07:29:39 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.1133
2022-03-17 07:30:12 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.1929
2022-03-17 07:30:45 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.7113
2022-03-17 07:31:19 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.1402
2022-03-17 07:31:52 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.5569
2022-03-17 07:32:25 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.0361
2022-03-17 07:32:58 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.4534
2022-03-17 07:33:32 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.3831
2022-03-17 07:34:05 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.2564
2022-03-17 07:34:38 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.1757
2022-03-17 07:35:11 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.2879
2022-03-17 07:35:44 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.5284
2022-03-17 07:36:18 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.2187
2022-03-17 07:36:51 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.4157
2022-03-17 07:37:24 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.5358
2022-03-17 07:37:57 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.3169
2022-03-17 07:38:31 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.3292
2022-03-17 07:39:04 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.3645
2022-03-17 07:39:37 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.0567
2022-03-17 07:40:10 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.4567
2022-03-17 07:40:43 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.3599
2022-03-17 07:41:16 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.6097
2022-03-17 07:41:49 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.4766
2022-03-17 07:42:23 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.2289
2022-03-17 07:42:56 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.5039
2022-03-17 07:43:29 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.3297
2022-03-17 07:44:02 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.2152
2022-03-17 07:44:35 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.4708
2022-03-17 07:45:09 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.5937
2022-03-17 07:45:42 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.4974
2022-03-17 07:46:15 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.5152
2022-03-17 07:46:48 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.4782
2022-03-17 07:47:21 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.3682
2022-03-17 07:47:55 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.3403
2022-03-17 07:48:28 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.2566
2022-03-17 07:49:01 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.5135
2022-03-17 07:49:34 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.8199
2022-03-17 07:50:07 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.4917
2022-03-17 07:50:39 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.3219
2022-03-17 07:51:12 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.3514
2022-03-17 07:51:45 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.4645
2022-03-17 07:52:18 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.3062
2022-03-17 07:52:51 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.3123
2022-03-17 07:53:24 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.5847
2022-03-17 07:53:57 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.2594
2022-03-17 07:54:30 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.6912
2022-03-17 07:55:03 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.3115
2022-03-17 07:55:35 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.3570
2022-03-17 07:56:07 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.5368
2022-03-17 07:56:09 - train: epoch 016, train_loss: 2.3900
2022-03-17 07:57:24 - eval: epoch: 016, acc1: 51.016%, acc5: 76.962%, test_loss: 2.0856, per_image_load_time: 2.347ms, per_image_inference_time: 0.537ms
2022-03-17 07:57:25 - until epoch: 016, best_acc1: 51.922%
2022-03-17 07:57:25 - epoch 017 lr: 0.1
2022-03-17 07:58:05 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.2278
2022-03-17 07:58:38 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.4883
2022-03-17 07:59:11 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.6032
2022-03-17 07:59:44 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.1879
2022-03-17 08:00:18 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.4172
2022-03-17 08:00:51 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.6737
2022-03-17 08:01:24 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.4227
2022-03-17 08:01:57 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.4031
2022-03-17 08:02:31 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.3571
2022-03-17 08:03:04 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.2318
2022-03-17 08:03:37 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.6700
2022-03-17 08:04:10 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.5636
2022-03-17 08:04:44 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.4294
2022-03-17 08:05:17 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.4684
2022-03-17 08:05:50 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 1.9721
2022-03-17 08:06:24 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.1311
2022-03-17 08:06:57 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.3825
2022-03-17 08:07:31 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.4018
2022-03-17 08:08:04 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.1403
2022-03-17 08:08:37 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.4684
2022-03-17 08:09:11 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.3161
2022-03-17 08:09:44 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.2247
2022-03-17 08:10:17 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.3592
2022-03-17 08:10:50 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.4509
2022-03-17 08:11:24 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.5732
2022-03-17 08:11:57 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.3674
2022-03-17 08:12:30 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.3510
2022-03-17 08:13:03 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.5554
2022-03-17 08:13:37 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.6562
2022-03-17 08:14:10 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.2458
2022-03-17 08:14:43 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.5156
2022-03-17 08:15:16 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.3304
2022-03-17 08:15:49 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.4695
2022-03-17 08:16:22 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.1289
2022-03-17 08:16:55 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.3450
2022-03-17 08:17:28 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.5784
2022-03-17 08:18:01 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.3934
2022-03-17 08:18:35 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.5672
2022-03-17 08:19:08 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.2528
2022-03-17 08:19:41 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.1701
2022-03-17 08:20:15 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.4500
2022-03-17 08:20:48 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.1941
2022-03-17 08:21:21 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.3272
2022-03-17 08:21:54 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.2988
2022-03-17 08:22:27 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.4692
2022-03-17 08:23:00 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.3787
2022-03-17 08:23:33 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.4475
2022-03-17 08:24:07 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.4743
2022-03-17 08:24:40 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.1294
2022-03-17 08:25:12 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.2764
2022-03-17 08:25:14 - train: epoch 017, train_loss: 2.3773
2022-03-17 08:26:29 - eval: epoch: 017, acc1: 51.338%, acc5: 77.506%, test_loss: 2.0593, per_image_load_time: 2.395ms, per_image_inference_time: 0.521ms
2022-03-17 08:26:30 - until epoch: 017, best_acc1: 51.922%
2022-03-17 08:26:30 - epoch 018 lr: 0.1
2022-03-17 08:27:09 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.3919
2022-03-17 08:27:42 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.3189
2022-03-17 08:28:16 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.6020
2022-03-17 08:28:48 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.5743
2022-03-17 08:29:21 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.3172
2022-03-17 08:29:55 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.5463
2022-03-17 08:30:28 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.1868
2022-03-17 08:31:01 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.2973
2022-03-17 08:31:34 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.4723
2022-03-17 08:32:07 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.1469
2022-03-17 08:32:40 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.5993
2022-03-17 08:33:13 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.4700
2022-03-17 08:33:46 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.5969
2022-03-17 08:34:19 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.3235
2022-03-17 08:34:51 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.5353
2022-03-17 08:35:23 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.3865
2022-03-17 08:35:55 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.2246
2022-03-17 08:36:27 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.2434
2022-03-17 08:37:00 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.2986
2022-03-17 08:37:34 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.6333
2022-03-17 08:38:07 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.6221
2022-03-17 08:38:40 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.3911
2022-03-17 08:39:13 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.4675
2022-03-17 08:39:46 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.1879
2022-03-17 08:40:20 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.0623
2022-03-17 08:40:53 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.1733
2022-03-17 08:41:26 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.5705
2022-03-17 08:41:59 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.1048
2022-03-17 08:42:33 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.3183
2022-03-17 08:43:06 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.1962
2022-03-17 08:43:39 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.7118
2022-03-17 08:44:12 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.2068
2022-03-17 08:44:45 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.2060
2022-03-17 08:45:19 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.3228
2022-03-17 08:45:52 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.6106
2022-03-17 08:46:25 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.4724
2022-03-17 08:46:58 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.6207
2022-03-17 08:47:32 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.4648
2022-03-17 08:48:05 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.4173
2022-03-17 08:48:39 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.2205
2022-03-17 08:49:12 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.2464
2022-03-17 08:49:45 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.3415
2022-03-17 08:50:18 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.1516
2022-03-17 08:50:52 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.4833
2022-03-17 08:51:25 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.4114
2022-03-17 08:51:59 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.1639
2022-03-17 08:52:32 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.4836
2022-03-17 08:53:05 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.4504
2022-03-17 08:53:38 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.3097
2022-03-17 08:54:11 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.4852
2022-03-17 08:54:12 - train: epoch 018, train_loss: 2.3639
2022-03-17 08:55:28 - eval: epoch: 018, acc1: 52.090%, acc5: 77.926%, test_loss: 2.0267, per_image_load_time: 2.333ms, per_image_inference_time: 0.553ms
2022-03-17 08:55:29 - until epoch: 018, best_acc1: 52.090%
2022-03-17 08:55:29 - epoch 019 lr: 0.1
2022-03-17 08:56:08 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.1348
2022-03-17 08:56:41 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.5320
2022-03-17 08:57:15 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.5054
2022-03-17 08:57:48 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.2248
2022-03-17 08:58:21 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.3036
2022-03-17 08:58:54 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.2470
2022-03-17 08:59:27 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.0814
2022-03-17 09:00:00 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.5366
2022-03-17 09:00:33 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.4494
2022-03-17 09:01:06 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.4856
2022-03-17 09:01:39 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.2349
2022-03-17 09:02:12 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.4370
2022-03-17 09:02:45 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.4146
2022-03-17 09:03:18 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.2335
2022-03-17 09:03:51 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.7543
2022-03-17 09:04:24 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.2812
2022-03-17 09:04:58 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.4875
2022-03-17 09:05:31 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.1464
2022-03-17 09:06:04 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.5883
2022-03-17 09:06:37 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.3591
2022-03-17 09:07:10 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.2472
2022-03-17 09:07:44 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.3361
2022-03-17 09:08:17 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.3109
2022-03-17 09:08:50 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.3834
2022-03-17 09:09:24 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.3024
2022-03-17 09:09:57 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.4235
2022-03-17 09:10:30 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.5136
2022-03-17 09:11:04 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.4366
2022-03-17 09:11:37 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.4133
2022-03-17 09:12:10 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.6313
2022-03-17 09:12:44 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.3840
2022-03-17 09:13:17 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.9775
2022-03-17 09:13:51 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.3185
2022-03-17 09:14:24 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.4653
2022-03-17 09:14:57 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.3640
2022-03-17 09:15:31 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.1039
2022-03-17 09:16:04 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.4034
2022-03-17 09:16:38 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.4466
2022-03-17 09:17:11 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.1867
2022-03-17 09:17:44 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.3565
2022-03-17 09:18:17 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.4542
2022-03-17 09:18:50 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.3838
2022-03-17 09:19:24 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.2224
2022-03-17 09:19:57 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.4968
2022-03-17 09:20:30 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.6948
2022-03-17 09:21:03 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.2469
2022-03-17 09:21:36 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.3604
2022-03-17 09:22:09 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.3390
2022-03-17 09:22:43 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.3777
2022-03-17 09:23:15 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.3574
2022-03-17 09:23:16 - train: epoch 019, train_loss: 2.3574
2022-03-17 09:24:31 - eval: epoch: 019, acc1: 52.018%, acc5: 77.640%, test_loss: 2.0362, per_image_load_time: 1.906ms, per_image_inference_time: 0.566ms
2022-03-17 09:24:32 - until epoch: 019, best_acc1: 52.090%
2022-03-17 09:24:32 - epoch 020 lr: 0.1
2022-03-17 09:25:11 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.5035
2022-03-17 09:25:45 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.1088
2022-03-17 09:26:18 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.4612
2022-03-17 09:26:51 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.0991
2022-03-17 09:27:24 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.2721
2022-03-17 09:27:57 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.4563
2022-03-17 09:28:30 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.1898
2022-03-17 09:29:04 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.4449
2022-03-17 09:29:37 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.8351
2022-03-17 09:30:10 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.4926
2022-03-17 09:30:43 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.2704
2022-03-17 09:31:16 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.1372
2022-03-17 09:31:49 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.3259
2022-03-17 09:32:22 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.4684
2022-03-17 09:32:55 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.3137
2022-03-17 09:33:28 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.2202
2022-03-17 09:34:01 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.0785
2022-03-17 09:34:35 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.2339
2022-03-17 09:35:08 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.1760
2022-03-17 09:35:41 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.3470
2022-03-17 09:36:14 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.5519
2022-03-17 09:36:47 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.1696
2022-03-17 09:37:20 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.3224
2022-03-17 09:37:53 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.4907
2022-03-17 09:38:26 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.2702
2022-03-17 09:38:59 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.1137
2022-03-17 09:39:32 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.3572
2022-03-17 09:40:06 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.4887
2022-03-17 09:40:38 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.4321
2022-03-17 09:41:12 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.1735
2022-03-17 09:41:45 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.4434
2022-03-17 09:42:18 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.5911
2022-03-17 09:42:51 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.2161
2022-03-17 09:43:24 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.2487
2022-03-17 09:43:57 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.2285
2022-03-17 09:44:31 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.3702
2022-03-17 09:45:04 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.2718
2022-03-17 09:45:37 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.3956
2022-03-17 09:46:10 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.5332
2022-03-17 09:46:43 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.2852
2022-03-17 09:47:16 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.1941
2022-03-17 09:47:50 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.2577
2022-03-17 09:48:23 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.4210
2022-03-17 09:48:56 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.2698
2022-03-17 09:49:29 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.4426
2022-03-17 09:50:03 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.4888
2022-03-17 09:50:36 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.2052
2022-03-17 09:51:09 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.3569
2022-03-17 09:51:42 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.3722
2022-03-17 09:52:15 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.3074
2022-03-17 09:52:16 - train: epoch 020, train_loss: 2.3436
2022-03-17 09:53:32 - eval: epoch: 020, acc1: 52.356%, acc5: 77.960%, test_loss: 2.0248, per_image_load_time: 2.172ms, per_image_inference_time: 0.559ms
2022-03-17 09:53:33 - until epoch: 020, best_acc1: 52.356%
2022-03-17 09:53:33 - epoch 021 lr: 0.1
2022-03-17 09:54:12 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.3327
2022-03-17 09:54:45 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.3490
2022-03-17 09:55:18 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.9822
2022-03-17 09:55:52 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.3094
2022-03-17 09:56:25 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.1940
2022-03-17 09:56:58 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.2120
2022-03-17 09:57:31 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.2207
2022-03-17 09:58:04 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.3827
2022-03-17 09:58:37 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.2455
2022-03-17 09:59:10 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.1747
2022-03-17 09:59:44 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.2092
2022-03-17 10:00:17 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.1836
2022-03-17 10:00:50 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.2467
2022-03-17 10:01:23 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.2490
2022-03-17 10:01:56 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.1815
2022-03-17 10:02:30 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.2719
2022-03-17 10:03:03 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.3505
2022-03-17 10:03:36 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.3057
2022-03-17 10:04:09 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.4120
2022-03-17 10:04:43 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.7873
2022-03-17 10:05:16 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.1309
2022-03-17 10:05:49 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.3650
2022-03-17 10:06:22 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.1918
2022-03-17 10:06:56 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.2504
2022-03-17 10:07:29 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.1901
2022-03-17 10:08:02 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.5551
2022-03-17 10:08:36 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.1392
2022-03-17 10:09:09 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.4275
2022-03-17 10:09:43 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.2989
2022-03-17 10:10:16 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.4956
2022-03-17 10:10:49 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.4092
2022-03-17 10:11:23 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.3994
2022-03-17 10:11:56 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.6793
2022-03-17 10:12:29 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.5356
2022-03-17 10:13:02 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.1661
2022-03-17 10:13:36 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.2772
2022-03-17 10:14:09 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.3283
2022-03-17 10:14:42 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.2605
2022-03-17 10:15:15 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.1956
2022-03-17 10:15:49 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.5985
2022-03-17 10:16:22 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.1436
2022-03-17 10:16:55 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.3785
2022-03-17 10:17:29 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.3172
2022-03-17 10:18:02 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.4478
2022-03-17 10:18:35 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.5165
2022-03-17 10:19:08 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.2527
2022-03-17 10:19:42 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.5041
2022-03-17 10:20:15 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.4222
2022-03-17 10:20:49 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.1608
2022-03-17 10:21:21 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.1788
2022-03-17 10:21:23 - train: epoch 021, train_loss: 2.3363
2022-03-17 10:22:38 - eval: epoch: 021, acc1: 51.390%, acc5: 77.282%, test_loss: 2.0685, per_image_load_time: 2.318ms, per_image_inference_time: 0.529ms
2022-03-17 10:22:38 - until epoch: 021, best_acc1: 52.356%
2022-03-17 10:22:38 - epoch 022 lr: 0.1
2022-03-17 10:23:17 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.1230
2022-03-17 10:23:51 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.2206
2022-03-17 10:24:24 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.1460
2022-03-17 10:24:57 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.2388
2022-03-17 10:25:30 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.3590
2022-03-17 10:26:03 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.3695
2022-03-17 10:26:36 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.4230
2022-03-17 10:27:10 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.5106
2022-03-17 10:27:43 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.4697
2022-03-17 10:28:16 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.3100
2022-03-17 10:28:50 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.3666
2022-03-17 10:29:23 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.0970
2022-03-17 10:29:56 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.2988
2022-03-17 10:30:29 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.3726
2022-03-17 10:31:02 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.3251
2022-03-17 10:31:35 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.1308
2022-03-17 10:32:08 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.0963
2022-03-17 10:32:41 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.6345
2022-03-17 10:33:14 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.1730
2022-03-17 10:33:47 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.4084
2022-03-17 10:34:20 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.3470
2022-03-17 10:34:53 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.1347
2022-03-17 10:35:26 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.3717
2022-03-17 10:35:59 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.3317
2022-03-17 10:36:33 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.2716
2022-03-17 10:37:06 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.1614
2022-03-17 10:37:39 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.1629
2022-03-17 10:38:12 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.5760
2022-03-17 10:38:45 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.1670
2022-03-17 10:39:19 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.4178
2022-03-17 10:39:52 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.3998
2022-03-17 10:40:25 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.4180
2022-03-17 10:40:58 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.2733
2022-03-17 10:41:31 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.1964
2022-03-17 10:42:04 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.5481
2022-03-17 10:42:38 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.4221
2022-03-17 10:43:11 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.4092
2022-03-17 10:43:44 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.5825
2022-03-17 10:44:18 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.2616
2022-03-17 10:44:51 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.4622
2022-03-17 10:45:24 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.1820
2022-03-17 10:45:57 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.2309
2022-03-17 10:46:31 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.4080
2022-03-17 10:47:04 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.4179
2022-03-17 10:47:37 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.3040
2022-03-17 10:48:10 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.6718
2022-03-17 10:48:44 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.3306
2022-03-17 10:49:17 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.1866
2022-03-17 10:49:50 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.2469
2022-03-17 10:50:22 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.2454
2022-03-17 10:50:24 - train: epoch 022, train_loss: 2.3326
2022-03-17 10:51:39 - eval: epoch: 022, acc1: 51.670%, acc5: 77.734%, test_loss: 2.0420, per_image_load_time: 2.258ms, per_image_inference_time: 0.545ms
2022-03-17 10:51:40 - until epoch: 022, best_acc1: 52.356%
2022-03-17 10:51:40 - epoch 023 lr: 0.1
2022-03-17 10:52:19 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.2995
2022-03-17 10:52:52 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.1688
2022-03-17 10:53:25 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.0858
2022-03-17 10:53:58 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.4221
2022-03-17 10:54:32 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.3757
2022-03-17 10:55:05 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.2946
2022-03-17 10:55:38 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.1504
2022-03-17 10:56:12 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.1724
2022-03-17 10:56:45 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.4385
2022-03-17 10:57:19 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.1985
2022-03-17 10:57:52 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.3795
2022-03-17 10:58:25 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.1956
2022-03-17 10:58:59 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.2737
2022-03-17 10:59:32 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.2290
2022-03-17 11:00:05 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.2353
2022-03-17 11:00:39 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.3026
2022-03-17 11:01:12 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.4029
2022-03-17 11:01:46 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.2041
2022-03-17 11:02:19 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.3597
2022-03-17 11:02:52 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.9260
2022-03-17 11:03:26 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.4536
2022-03-17 11:03:59 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.9606
2022-03-17 11:04:32 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.2152
2022-03-17 11:05:06 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.2717
2022-03-17 11:05:39 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.4262
2022-03-17 11:06:12 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.2066
2022-03-17 11:06:45 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.2858
2022-03-17 11:07:18 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.3495
2022-03-17 11:07:50 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.2718
2022-03-17 11:08:23 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.4977
2022-03-17 11:08:57 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.4420
2022-03-17 11:09:30 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.4361
2022-03-17 11:10:03 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.5005
2022-03-17 11:10:36 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.4532
2022-03-17 11:11:09 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.1620
2022-03-17 11:11:42 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.1454
2022-03-17 11:12:15 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.3295
2022-03-17 11:12:48 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.3431
2022-03-17 11:13:21 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.2982
2022-03-17 11:13:54 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.2453
2022-03-17 11:14:27 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.3069
2022-03-17 11:15:00 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.3128
2022-03-17 11:15:33 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.1684
2022-03-17 11:16:06 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 1.9981
2022-03-17 11:16:40 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.2488
2022-03-17 11:17:13 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.3296
2022-03-17 11:17:46 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.0887
2022-03-17 11:18:19 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.1913
2022-03-17 11:18:52 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.2329
2022-03-17 11:19:24 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.5108
2022-03-17 11:19:26 - train: epoch 023, train_loss: 2.3229
2022-03-17 11:20:41 - eval: epoch: 023, acc1: 52.866%, acc5: 78.284%, test_loss: 1.9971, per_image_load_time: 2.325ms, per_image_inference_time: 0.529ms
2022-03-17 11:20:42 - until epoch: 023, best_acc1: 52.866%
2022-03-17 11:20:42 - epoch 024 lr: 0.1
2022-03-17 11:21:22 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.2842
2022-03-17 11:21:55 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.4385
2022-03-17 11:22:28 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.2452
2022-03-17 11:23:01 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.3205
2022-03-17 11:23:34 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.2280
2022-03-17 11:24:08 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.1325
2022-03-17 11:24:41 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.1555
2022-03-17 11:25:14 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.1971
2022-03-17 11:25:47 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.3835
2022-03-17 11:26:19 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.2475
2022-03-17 11:26:52 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.9192
2022-03-17 11:27:28 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.1635
2022-03-17 11:28:04 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.6485
2022-03-17 11:28:40 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.1654
2022-03-17 11:29:15 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.5201
2022-03-17 11:29:53 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.2819
2022-03-17 11:30:30 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.2492
2022-03-17 11:31:05 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.5108
2022-03-17 11:31:42 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.1613
2022-03-17 11:32:17 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.2669
2022-03-17 11:32:54 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.3882
2022-03-17 11:33:31 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.0590
2022-03-17 11:34:07 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.3531
2022-03-17 11:34:44 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.2342
2022-03-17 11:35:21 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.2640
2022-03-17 11:35:57 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.2289
2022-03-17 11:36:34 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.3820
2022-03-17 11:37:10 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.4191
2022-03-17 11:37:47 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.4462
2022-03-17 11:38:19 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.2235
2022-03-17 11:38:57 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.2705
2022-03-17 11:39:34 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.4097
2022-03-17 11:40:10 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.9995
2022-03-17 11:40:47 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.3627
2022-03-17 11:41:24 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.1787
2022-03-17 11:42:00 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.3022
2022-03-17 11:42:37 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.3256
2022-03-17 11:43:13 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.5551
2022-03-17 11:43:50 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.3562
2022-03-17 11:44:27 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.3479
2022-03-17 11:45:03 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.3095
2022-03-17 11:45:39 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.2815
2022-03-17 11:46:16 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.2747
2022-03-17 11:46:53 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.2520
2022-03-17 11:47:29 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.2402
2022-03-17 11:48:06 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.4937
2022-03-17 11:48:43 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.3097
2022-03-17 11:49:19 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.2130
2022-03-17 11:49:56 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.3777
2022-03-17 11:50:31 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.4038
2022-03-17 11:50:33 - train: epoch 024, train_loss: 2.3170
2022-03-17 11:51:53 - eval: epoch: 024, acc1: 52.810%, acc5: 78.346%, test_loss: 1.9975, per_image_load_time: 2.568ms, per_image_inference_time: 0.527ms
2022-03-17 11:51:54 - until epoch: 024, best_acc1: 52.866%
2022-03-17 11:51:54 - epoch 025 lr: 0.1
2022-03-17 11:52:33 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.1518
2022-03-17 11:53:08 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.0891
2022-03-17 11:53:46 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.1955
2022-03-17 11:54:22 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.2887
2022-03-17 11:55:00 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.2834
2022-03-17 11:55:36 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.3099
2022-03-17 11:56:12 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.2982
2022-03-17 11:56:49 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.3191
2022-03-17 11:57:26 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.1340
2022-03-17 11:58:03 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.0994
2022-03-17 11:58:40 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.3160
2022-03-17 11:59:16 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.2243
2022-03-17 11:59:54 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.2334
2022-03-17 12:00:30 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.4349
2022-03-17 12:01:07 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.3488
2022-03-17 12:01:44 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.1331
2022-03-17 12:02:21 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.3428
2022-03-17 12:02:58 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.1079
2022-03-17 12:03:35 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.1924
2022-03-17 12:04:12 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.3587
2022-03-17 12:04:49 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.1813
2022-03-17 12:05:26 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.3278
2022-03-17 12:06:03 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.2698
2022-03-17 12:06:39 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.1018
2022-03-17 12:07:12 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.3965
2022-03-17 12:07:47 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.4692
2022-03-17 12:08:24 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.3602
2022-03-17 12:09:01 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.2532
2022-03-17 12:09:38 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.4266
2022-03-17 12:10:15 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.5747
2022-03-17 12:10:51 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.3549
2022-03-17 12:11:28 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.4643
2022-03-17 12:12:05 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.3995
2022-03-17 12:12:42 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.3665
2022-03-17 12:13:19 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.0770
2022-03-17 12:13:56 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.4367
2022-03-17 12:14:32 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.3390
2022-03-17 12:15:09 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.3959
2022-03-17 12:15:46 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.5699
2022-03-17 12:16:22 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.3533
2022-03-17 12:16:59 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.4375
2022-03-17 12:17:35 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.4067
2022-03-17 12:18:11 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.1501
2022-03-17 12:18:48 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.2094
2022-03-17 12:19:24 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.3047
2022-03-17 12:20:01 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.2954
2022-03-17 12:20:37 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.1515
2022-03-17 12:21:10 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.1137
2022-03-17 12:21:43 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.2305
2022-03-17 12:22:20 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.3949
2022-03-17 12:22:21 - train: epoch 025, train_loss: 2.3108
2022-03-17 12:23:44 - eval: epoch: 025, acc1: 52.670%, acc5: 78.098%, test_loss: 2.0096, per_image_load_time: 2.666ms, per_image_inference_time: 0.516ms
2022-03-17 12:23:44 - until epoch: 025, best_acc1: 52.866%
2022-03-17 12:23:44 - epoch 026 lr: 0.1
2022-03-17 12:24:27 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.0971
2022-03-17 12:25:03 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.9710
2022-03-17 12:25:40 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.2047
2022-03-17 12:26:16 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.2322
2022-03-17 12:26:52 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.3237
2022-03-17 12:27:29 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.3929
2022-03-17 12:28:05 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.2124
2022-03-17 12:28:42 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.0000
2022-03-17 12:29:19 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.5990
2022-03-17 12:29:55 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.1698
2022-03-17 12:30:31 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.2630
2022-03-17 12:31:09 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.3981
2022-03-17 12:31:46 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.2228
2022-03-17 12:32:23 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.4247
2022-03-17 12:33:00 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.3213
2022-03-17 12:33:37 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.4871
2022-03-17 12:34:14 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.2298
2022-03-17 12:34:50 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.3879
2022-03-17 12:35:28 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.5972
2022-03-17 12:36:01 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.4251
2022-03-17 12:36:33 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.4568
2022-03-17 12:37:11 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.3929
2022-03-17 12:37:48 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.3036
2022-03-17 12:38:25 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.5908
2022-03-17 12:39:02 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.4489
2022-03-17 12:39:38 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.2954
2022-03-17 12:40:15 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.2122
2022-03-17 12:40:51 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.1890
2022-03-17 12:41:29 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.4371
2022-03-17 12:42:05 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.1872
2022-03-17 12:42:42 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.2630
2022-03-17 12:43:19 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.2893
2022-03-17 12:43:56 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.2487
2022-03-17 12:44:32 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.5499
2022-03-17 12:45:09 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.3881
2022-03-17 12:45:46 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.2598
2022-03-17 12:46:23 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.3708
2022-03-17 12:47:00 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.4563
2022-03-17 12:47:37 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.4361
2022-03-17 12:48:14 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.5752
2022-03-17 12:48:50 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.3289
2022-03-17 12:49:27 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.3095
2022-03-17 12:50:05 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.4789
2022-03-17 12:50:41 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.4803
2022-03-17 12:51:17 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.5853
2022-03-17 12:51:50 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.3261
2022-03-17 12:52:25 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.1705
2022-03-17 12:53:03 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.2649
2022-03-17 12:53:40 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.3489
2022-03-17 12:54:15 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.5093
2022-03-17 12:54:17 - train: epoch 026, train_loss: 2.3053
2022-03-17 12:55:40 - eval: epoch: 026, acc1: 52.232%, acc5: 78.334%, test_loss: 2.0090, per_image_load_time: 2.020ms, per_image_inference_time: 0.541ms
2022-03-17 12:55:41 - until epoch: 026, best_acc1: 52.866%
2022-03-17 12:55:41 - epoch 027 lr: 0.1
2022-03-17 12:56:25 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.4847
2022-03-17 12:57:02 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.2543
2022-03-17 12:57:38 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.4469
2022-03-17 12:58:16 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.5196
2022-03-17 12:58:53 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.3806
2022-03-17 12:59:30 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.4749
2022-03-17 13:00:07 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.5452
2022-03-17 13:00:43 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.3502
2022-03-17 13:01:21 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.2153
2022-03-17 13:01:58 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.2933
2022-03-17 13:02:35 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.1913
2022-03-17 13:03:12 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.5509
2022-03-17 13:03:50 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.3162
2022-03-17 13:04:27 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.5059
2022-03-17 13:05:04 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.3750
2022-03-17 13:05:41 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.3352
2022-03-17 13:06:14 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.2527
2022-03-17 13:06:47 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.2726
2022-03-17 13:07:25 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.3788
2022-03-17 13:08:03 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.2835
2022-03-17 13:08:39 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.5179
2022-03-17 13:09:16 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.4679
2022-03-17 13:09:53 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.6871
2022-03-17 13:10:30 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.2726
2022-03-17 13:11:07 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.1381
2022-03-17 13:11:44 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.4634
2022-03-17 13:12:22 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.3242
2022-03-17 13:12:59 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.3431
2022-03-17 13:13:36 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.3173
2022-03-17 13:14:13 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.1081
2022-03-17 13:14:51 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.0980
2022-03-17 13:15:27 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.0883
2022-03-17 13:16:04 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.3366
2022-03-17 13:16:41 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.2225
2022-03-17 13:17:18 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.5499
2022-03-17 13:17:55 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.1591
2022-03-17 13:18:32 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.2444
2022-03-17 13:19:09 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.0912
2022-03-17 13:19:45 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.1236
2022-03-17 13:20:22 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.4138
2022-03-17 13:21:00 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.3485
2022-03-17 13:21:33 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.3953
2022-03-17 13:22:06 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.1972
2022-03-17 13:22:43 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.3088
2022-03-17 13:23:20 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.1900
2022-03-17 13:23:57 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.2711
2022-03-17 13:24:34 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.5479
2022-03-17 13:25:11 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.5522
2022-03-17 13:25:48 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.2602
2022-03-17 13:26:23 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.0018
2022-03-17 13:26:25 - train: epoch 027, train_loss: 2.3006
2022-03-17 13:27:48 - eval: epoch: 027, acc1: 52.478%, acc5: 78.140%, test_loss: 2.0112, per_image_load_time: 2.624ms, per_image_inference_time: 0.547ms
2022-03-17 13:27:49 - until epoch: 027, best_acc1: 52.866%
2022-03-17 13:27:49 - epoch 028 lr: 0.1
2022-03-17 13:28:33 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.1476
2022-03-17 13:29:10 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.2242
2022-03-17 13:29:46 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.1659
2022-03-17 13:30:24 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.1452
2022-03-17 13:31:01 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.2178
2022-03-17 13:31:38 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.3715
2022-03-17 13:32:14 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.5102
2022-03-17 13:32:52 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.0143
2022-03-17 13:33:29 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.0863
2022-03-17 13:34:06 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.4256
2022-03-17 13:34:43 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.2456
2022-03-17 13:35:20 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.1467
2022-03-17 13:35:57 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.2935
2022-03-17 13:36:32 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.5903
2022-03-17 13:37:05 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.2384
2022-03-17 13:37:40 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.3046
2022-03-17 13:38:18 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.1482
2022-03-17 13:38:56 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.1291
2022-03-17 13:39:33 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.1858
2022-03-17 13:40:10 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.4594
2022-03-17 13:40:47 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.4198
2022-03-17 13:41:25 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.3543
2022-03-17 13:42:01 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.4564
2022-03-17 13:42:39 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.4849
2022-03-17 13:43:17 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.2443
2022-03-17 13:43:54 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.2101
2022-03-17 13:44:31 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.2642
2022-03-17 13:45:07 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.2792
2022-03-17 13:45:45 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.4091
2022-03-17 13:46:22 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.3515
2022-03-17 13:46:59 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.3790
2022-03-17 13:47:36 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.2045
2022-03-17 13:48:13 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.3648
2022-03-17 13:48:51 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.1910
2022-03-17 13:49:27 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.2700
2022-03-17 13:50:04 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.0830
2022-03-17 13:50:41 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.2958
2022-03-17 13:51:19 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.0906
2022-03-17 13:51:52 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.3552
2022-03-17 13:52:25 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.3951
2022-03-17 13:53:01 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.4565
2022-03-17 13:53:39 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.4098
2022-03-17 13:54:15 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.1722
2022-03-17 13:54:52 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.2778
2022-03-17 13:55:26 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.2328
2022-03-17 13:55:59 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.3949
2022-03-17 13:56:33 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.3209
2022-03-17 13:57:06 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.2207
2022-03-17 13:57:39 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.2004
2022-03-17 13:58:12 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.2081
2022-03-17 13:58:13 - train: epoch 028, train_loss: 2.2942
2022-03-17 13:59:28 - eval: epoch: 028, acc1: 53.040%, acc5: 78.610%, test_loss: 1.9808, per_image_load_time: 1.289ms, per_image_inference_time: 0.572ms
2022-03-17 13:59:29 - until epoch: 028, best_acc1: 53.040%
2022-03-17 13:59:29 - epoch 029 lr: 0.1
2022-03-17 14:00:09 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.4338
2022-03-17 14:00:42 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.4544
2022-03-17 14:01:15 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.3986
2022-03-17 14:01:47 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.3070
2022-03-17 14:02:20 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.3007
2022-03-17 14:02:53 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.6299
2022-03-17 14:03:26 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.9327
2022-03-17 14:03:59 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.4057
2022-03-17 14:04:32 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.2167
2022-03-17 14:05:05 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.1179
2022-03-17 14:05:38 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.1876
2022-03-17 14:06:11 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.5105
2022-03-17 14:06:44 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.3453
2022-03-17 14:07:17 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.3757
2022-03-17 14:07:50 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.3748
2022-03-17 14:08:23 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.0891
2022-03-17 14:08:56 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.2816
2022-03-17 14:09:29 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.3965
2022-03-17 14:10:03 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.0947
2022-03-17 14:10:36 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.6099
2022-03-17 14:11:09 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.1904
2022-03-17 14:11:42 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.3346
2022-03-17 14:12:16 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.2531
2022-03-17 14:12:49 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.2041
2022-03-17 14:13:22 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.2880
2022-03-17 14:13:58 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.3172
2022-03-17 14:14:35 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.2897
2022-03-17 14:15:11 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.2180
2022-03-17 14:15:48 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.1394
2022-03-17 14:16:25 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.3997
2022-03-17 14:17:02 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.2662
2022-03-17 14:17:39 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.2246
2022-03-17 14:18:16 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.1900
2022-03-17 14:18:53 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.0944
2022-03-17 14:19:30 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.4079
2022-03-17 14:20:06 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.2724
2022-03-17 14:20:43 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.1919
2022-03-17 14:21:20 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.2211
2022-03-17 14:21:57 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.1396
2022-03-17 14:22:34 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.2627
2022-03-17 14:23:12 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.1097
2022-03-17 14:23:48 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.1730
2022-03-17 14:24:25 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.4012
2022-03-17 14:25:02 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.1849
2022-03-17 14:25:39 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.5187
2022-03-17 14:26:12 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.4294
2022-03-17 14:26:49 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.0684
2022-03-17 14:27:25 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.4323
2022-03-17 14:28:02 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.7075
2022-03-17 14:28:38 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.0841
2022-03-17 14:28:40 - train: epoch 029, train_loss: 2.2896
2022-03-17 14:30:03 - eval: epoch: 029, acc1: 53.342%, acc5: 78.584%, test_loss: 1.9784, per_image_load_time: 2.725ms, per_image_inference_time: 0.529ms
2022-03-17 14:30:04 - until epoch: 029, best_acc1: 53.342%
2022-03-17 14:30:04 - epoch 030 lr: 0.1
2022-03-17 14:30:47 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.4365
2022-03-17 14:31:24 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.3189
2022-03-17 14:32:01 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.2507
2022-03-17 14:32:38 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.0370
2022-03-17 14:33:15 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.6305
2022-03-17 14:33:53 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.0888
2022-03-17 14:34:30 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.3049
2022-03-17 14:35:07 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.3200
2022-03-17 14:35:44 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.3829
2022-03-17 14:36:21 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 1.9990
2022-03-17 14:36:58 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.1174
2022-03-17 14:37:35 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.2974
2022-03-17 14:38:12 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.1144
2022-03-17 14:38:45 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.1573
2022-03-17 14:39:22 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.2243
2022-03-17 14:39:59 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.2547
2022-03-17 14:40:36 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.5423
2022-03-17 14:41:13 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.2825
2022-03-17 14:41:50 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.4484
2022-03-17 14:42:26 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.2489
2022-03-17 14:43:03 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.4261
2022-03-17 14:43:40 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.2324
2022-03-17 14:44:16 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.1423
2022-03-17 14:44:53 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.4391
2022-03-17 14:45:30 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.2676
2022-03-17 14:46:07 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.2624
2022-03-17 14:46:44 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.2364
2022-03-17 14:47:21 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.3785
2022-03-17 14:47:57 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.2504
2022-03-17 14:48:34 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.3709
2022-03-17 14:49:11 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.3428
2022-03-17 14:49:47 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.0571
2022-03-17 14:50:23 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.4323
2022-03-17 14:50:57 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.3175
2022-03-17 14:51:32 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.4330
2022-03-17 14:52:09 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.1333
2022-03-17 14:52:45 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.1850
2022-03-17 14:53:22 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.5141
2022-03-17 14:53:59 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.3919
2022-03-17 14:54:35 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.1856
2022-03-17 14:55:12 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.0662
2022-03-17 14:55:49 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.5457
2022-03-17 14:56:25 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.2329
2022-03-17 14:57:02 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.3133
2022-03-17 14:57:39 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.4143
2022-03-17 14:58:16 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.0484
2022-03-17 14:58:53 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.3736
2022-03-17 14:59:30 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.3219
2022-03-17 15:00:07 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.4465
2022-03-17 15:00:43 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.4219
2022-03-17 15:00:44 - train: epoch 030, train_loss: 2.2895
2022-03-17 15:02:08 - eval: epoch: 030, acc1: 53.280%, acc5: 78.530%, test_loss: 1.9855, per_image_load_time: 2.710ms, per_image_inference_time: 0.523ms
2022-03-17 15:02:08 - until epoch: 030, best_acc1: 53.342%
2022-03-17 15:02:08 - epoch 031 lr: 0.010000000000000002
2022-03-17 15:02:52 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.1878
2022-03-17 15:03:28 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.8990
2022-03-17 15:04:01 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.8693
2022-03-17 15:04:38 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.0442
2022-03-17 15:05:14 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.7492
2022-03-17 15:05:51 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.8466
2022-03-17 15:06:28 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.7695
2022-03-17 15:07:03 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.7880
2022-03-17 15:07:36 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.7418
2022-03-17 15:08:10 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.0889
2022-03-17 15:08:43 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.0477
2022-03-17 15:09:16 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.8058
2022-03-17 15:09:50 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.5133
2022-03-17 15:10:23 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.8172
2022-03-17 15:10:56 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.8686
2022-03-17 15:11:29 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.6675
2022-03-17 15:12:03 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.6828
2022-03-17 15:12:36 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.7801
2022-03-17 15:13:09 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.8911
2022-03-17 15:13:42 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.7652
2022-03-17 15:14:15 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.6429
2022-03-17 15:14:49 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.5409
2022-03-17 15:15:22 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.6693
2022-03-17 15:15:55 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.6855
2022-03-17 15:16:28 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.6397
2022-03-17 15:17:02 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.7243
2022-03-17 15:17:35 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.7536
2022-03-17 15:18:08 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.9763
2022-03-17 15:18:45 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.8610
2022-03-17 15:19:22 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.9426
2022-03-17 15:20:00 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.6230
2022-03-17 15:20:37 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.8816
2022-03-17 15:21:12 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.8306
2022-03-17 15:21:46 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.8907
2022-03-17 15:22:19 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.7983
2022-03-17 15:22:56 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.7559
2022-03-17 15:23:29 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.6665
2022-03-17 15:24:06 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.4940
2022-03-17 15:24:39 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.6951
2022-03-17 15:25:15 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.6542
2022-03-17 15:25:52 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.5831
2022-03-17 15:26:29 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.8310
2022-03-17 15:27:06 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.6611
2022-03-17 15:27:43 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.7966
2022-03-17 15:28:20 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.7613
2022-03-17 15:28:57 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.7129
2022-03-17 15:29:34 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.7281
2022-03-17 15:30:11 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.5549
2022-03-17 15:30:47 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.5292
2022-03-17 15:31:23 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.6940
2022-03-17 15:31:24 - train: epoch 031, train_loss: 1.7660
2022-03-17 15:32:46 - eval: epoch: 031, acc1: 66.138%, acc5: 87.034%, test_loss: 1.3794, per_image_load_time: 2.686ms, per_image_inference_time: 0.492ms
2022-03-17 15:32:47 - until epoch: 031, best_acc1: 66.138%
2022-03-17 15:32:47 - epoch 032 lr: 0.010000000000000002
2022-03-17 15:33:28 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.5867
2022-03-17 15:34:05 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.6565
2022-03-17 15:34:40 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.6428
2022-03-17 15:35:17 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.8469
2022-03-17 15:35:53 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.7242
2022-03-17 15:36:29 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.8203
2022-03-17 15:37:05 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.6333
2022-03-17 15:37:42 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.7164
2022-03-17 15:38:19 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.6441
2022-03-17 15:38:55 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.7357
2022-03-17 15:39:32 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.7593
2022-03-17 15:40:08 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.6380
2022-03-17 15:40:45 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.6434
2022-03-17 15:41:22 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.7602
2022-03-17 15:41:58 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.7347
2022-03-17 15:42:32 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.8296
2022-03-17 15:43:09 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.7621
2022-03-17 15:43:45 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.0040
2022-03-17 15:44:22 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.5415
2022-03-17 15:44:58 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.6294
2022-03-17 15:45:34 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.5598
2022-03-17 15:46:07 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.5668
2022-03-17 15:46:41 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.6768
2022-03-17 15:47:14 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.6634
2022-03-17 15:47:47 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.8213
2022-03-17 15:48:21 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.4091
2022-03-17 15:48:54 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.5962
2022-03-17 15:49:27 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.8211
2022-03-17 15:50:01 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.4805
2022-03-17 15:50:34 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.4902
2022-03-17 15:51:08 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.6699
2022-03-17 15:51:41 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.6523
2022-03-17 15:52:15 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.4519
2022-03-17 15:52:48 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.5415
2022-03-17 15:53:22 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.6542
2022-03-17 15:53:56 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.6649
2022-03-17 15:54:29 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.7210
2022-03-17 15:55:02 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.4671
2022-03-17 15:55:35 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.6877
2022-03-17 15:56:09 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.5301
2022-03-17 15:56:42 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.6123
2022-03-17 15:57:15 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.6005
2022-03-17 15:57:49 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.8631
2022-03-17 15:58:22 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.5528
2022-03-17 15:58:55 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.8469
2022-03-17 15:59:28 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.5972
2022-03-17 16:00:02 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.7118
2022-03-17 16:00:35 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.5400
2022-03-17 16:01:08 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.7339
2022-03-17 16:01:41 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.6574
2022-03-17 16:01:42 - train: epoch 032, train_loss: 1.6462
2022-03-17 16:02:58 - eval: epoch: 032, acc1: 66.882%, acc5: 87.510%, test_loss: 1.3411, per_image_load_time: 1.996ms, per_image_inference_time: 0.525ms
2022-03-17 16:02:59 - until epoch: 032, best_acc1: 66.882%
2022-03-17 16:02:59 - epoch 033 lr: 0.010000000000000002
2022-03-17 16:03:38 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.4940
2022-03-17 16:04:12 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.6571
2022-03-17 16:04:45 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.4972
2022-03-17 16:05:18 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.4800
2022-03-17 16:05:51 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.7212
2022-03-17 16:06:25 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.4643
2022-03-17 16:06:58 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.6396
2022-03-17 16:07:32 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.7241
2022-03-17 16:08:05 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.7023
2022-03-17 16:08:39 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.6837
2022-03-17 16:09:12 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.5723
2022-03-17 16:09:46 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.6397
2022-03-17 16:10:19 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.4800
2022-03-17 16:10:53 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.8102
2022-03-17 16:11:26 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.8580
2022-03-17 16:11:59 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.8135
2022-03-17 16:12:33 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.4423
2022-03-17 16:13:06 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.7766
2022-03-17 16:13:39 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.6358
2022-03-17 16:14:12 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.4843
2022-03-17 16:14:45 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.6017
2022-03-17 16:15:18 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.6768
2022-03-17 16:15:51 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.4913
2022-03-17 16:16:25 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.9120
2022-03-17 16:16:58 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.5694
2022-03-17 16:17:31 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.4993
2022-03-17 16:18:05 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.7292
2022-03-17 16:18:38 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.6269
2022-03-17 16:19:11 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.7120
2022-03-17 16:19:44 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.6956
2022-03-17 16:20:18 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.5163
2022-03-17 16:20:51 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.5735
2022-03-17 16:21:24 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.6411
2022-03-17 16:21:57 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.5457
2022-03-17 16:22:30 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.7320
2022-03-17 16:23:03 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.7765
2022-03-17 16:23:36 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.4726
2022-03-17 16:24:10 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.4020
2022-03-17 16:24:43 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.7042
2022-03-17 16:25:17 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.6530
2022-03-17 16:25:50 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.6187
2022-03-17 16:26:23 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.5114
2022-03-17 16:26:57 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.7364
2022-03-17 16:27:30 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.6941
2022-03-17 16:28:03 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.7682
2022-03-17 16:28:36 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.4570
2022-03-17 16:29:09 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.5456
2022-03-17 16:29:42 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.9390
2022-03-17 16:30:16 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.4222
2022-03-17 16:30:48 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.4754
2022-03-17 16:30:49 - train: epoch 033, train_loss: 1.6003
2022-03-17 16:32:05 - eval: epoch: 033, acc1: 67.206%, acc5: 88.082%, test_loss: 1.3128, per_image_load_time: 2.199ms, per_image_inference_time: 0.532ms
2022-03-17 16:32:06 - until epoch: 033, best_acc1: 67.206%
2022-03-17 16:32:06 - epoch 034 lr: 0.010000000000000002
2022-03-17 16:32:45 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.5163
2022-03-17 16:33:19 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.5927
2022-03-17 16:33:52 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.5087
2022-03-17 16:34:25 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.2953
2022-03-17 16:34:58 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.5766
2022-03-17 16:35:30 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.8366
2022-03-17 16:36:03 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.5459
2022-03-17 16:36:36 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.4606
2022-03-17 16:37:08 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.5182
2022-03-17 16:37:41 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.5046
2022-03-17 16:38:14 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.5288
2022-03-17 16:38:47 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.5362
2022-03-17 16:39:20 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.5420
2022-03-17 16:39:53 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.6387
2022-03-17 16:40:27 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.5052
2022-03-17 16:41:00 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.4629
2022-03-17 16:41:33 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.5415
2022-03-17 16:42:07 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.8336
2022-03-17 16:42:40 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.6255
2022-03-17 16:43:14 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.6901
2022-03-17 16:43:47 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.8077
2022-03-17 16:44:20 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.4425
2022-03-17 16:44:54 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.7756
2022-03-17 16:45:27 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.4658
2022-03-17 16:46:00 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.6033
2022-03-17 16:46:34 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.5500
2022-03-17 16:47:07 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.6519
2022-03-17 16:47:40 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.3610
2022-03-17 16:48:13 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.3590
2022-03-17 16:48:46 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.3307
2022-03-17 16:49:20 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.5828
2022-03-17 16:49:53 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.5848
2022-03-17 16:50:27 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.5814
2022-03-17 16:51:00 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.6238
2022-03-17 16:51:33 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.4124
2022-03-17 16:52:06 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.4338
2022-03-17 16:52:40 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.3907
2022-03-17 16:53:13 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.4879
2022-03-17 16:53:46 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.6673
2022-03-17 16:54:19 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.4260
2022-03-17 16:54:53 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.5692
2022-03-17 16:55:27 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.4849
2022-03-17 16:56:00 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.5875
2022-03-17 16:56:33 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.5863
2022-03-17 16:57:07 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.6960
2022-03-17 16:57:40 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.6972
2022-03-17 16:58:13 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.6385
2022-03-17 16:58:46 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.4774
2022-03-17 16:59:19 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.4970
2022-03-17 16:59:52 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.3461
2022-03-17 16:59:53 - train: epoch 034, train_loss: 1.5739
2022-03-17 17:01:09 - eval: epoch: 034, acc1: 67.878%, acc5: 88.144%, test_loss: 1.2949, per_image_load_time: 2.382ms, per_image_inference_time: 0.558ms
2022-03-17 17:01:10 - until epoch: 034, best_acc1: 67.878%
2022-03-17 17:01:10 - epoch 035 lr: 0.010000000000000002
2022-03-17 17:01:49 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.4989
2022-03-17 17:02:22 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.3359
2022-03-17 17:02:55 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.6909
2022-03-17 17:03:28 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.4424
2022-03-17 17:04:00 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.5747
2022-03-17 17:04:33 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.4837
2022-03-17 17:05:06 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.4378
2022-03-17 17:05:39 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.5168
2022-03-17 17:06:12 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.7039
2022-03-17 17:06:44 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.7782
2022-03-17 17:07:17 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.6127
2022-03-17 17:07:50 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.4009
2022-03-17 17:08:23 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.7142
2022-03-17 17:08:56 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.5967
2022-03-17 17:09:29 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.6231
2022-03-17 17:10:02 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.5688
2022-03-17 17:10:35 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.3490
2022-03-17 17:11:08 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.6249
2022-03-17 17:11:41 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.5670
2022-03-17 17:12:15 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.6119
2022-03-17 17:12:48 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.5316
2022-03-17 17:13:21 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.6567
2022-03-17 17:13:54 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.5659
2022-03-17 17:14:27 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.6938
2022-03-17 17:15:01 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.5328
2022-03-17 17:15:34 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.6466
2022-03-17 17:16:08 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.6489
2022-03-17 17:16:41 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.4515
2022-03-17 17:17:14 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.5553
2022-03-17 17:17:48 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.6374
2022-03-17 17:18:21 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.5448
2022-03-17 17:18:54 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.3038
2022-03-17 17:19:28 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.5371
2022-03-17 17:20:01 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.6328
2022-03-17 17:20:34 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.3279
2022-03-17 17:21:07 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.6134
2022-03-17 17:21:40 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.2829
2022-03-17 17:22:14 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.5029
2022-03-17 17:22:47 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.7272
2022-03-17 17:23:20 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.3465
2022-03-17 17:23:53 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.7902
2022-03-17 17:24:26 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.4218
2022-03-17 17:24:59 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.5913
2022-03-17 17:25:32 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.6763
2022-03-17 17:26:05 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.6371
2022-03-17 17:26:38 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.4258
2022-03-17 17:27:11 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.8429
2022-03-17 17:27:45 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.6789
2022-03-17 17:28:18 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.5658
2022-03-17 17:28:50 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.4148
2022-03-17 17:28:51 - train: epoch 035, train_loss: 1.5571
2022-03-17 17:30:07 - eval: epoch: 035, acc1: 67.818%, acc5: 88.296%, test_loss: 1.2948, per_image_load_time: 2.268ms, per_image_inference_time: 0.571ms
2022-03-17 17:30:08 - until epoch: 035, best_acc1: 67.878%
2022-03-17 17:30:08 - epoch 036 lr: 0.010000000000000002
2022-03-17 17:30:47 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.7289
2022-03-17 17:31:20 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.2794
2022-03-17 17:31:53 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.5127
2022-03-17 17:32:26 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.5518
2022-03-17 17:32:59 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.6198
2022-03-17 17:33:33 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.5278
2022-03-17 17:34:06 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.3906
2022-03-17 17:34:39 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.3486
2022-03-17 17:35:12 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.5479
2022-03-17 17:35:44 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.5931
2022-03-17 17:36:17 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.6626
2022-03-17 17:36:50 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.4975
2022-03-17 17:37:22 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.4262
2022-03-17 17:37:54 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.7454
2022-03-17 17:38:27 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.8475
2022-03-17 17:39:00 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.5433
2022-03-17 17:39:32 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.6374
2022-03-17 17:40:05 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.4633
2022-03-17 17:40:38 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.5301
2022-03-17 17:41:10 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.4620
2022-03-17 17:41:43 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.6272
2022-03-17 17:42:16 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.4717
2022-03-17 17:42:49 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.5396
2022-03-17 17:43:22 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.6851
2022-03-17 17:43:55 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.4038
2022-03-17 17:44:28 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.6567
2022-03-17 17:45:00 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.5550
2022-03-17 17:45:33 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.4317
2022-03-17 17:46:07 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.3790
2022-03-17 17:46:39 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.7019
2022-03-17 17:47:12 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.5625
2022-03-17 17:47:45 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.5362
2022-03-17 17:48:18 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.4257
2022-03-17 17:48:51 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.4764
2022-03-17 17:49:24 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.6172
2022-03-17 17:49:58 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.6819
2022-03-17 17:50:32 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.5747
2022-03-17 17:51:05 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.4520
2022-03-17 17:51:39 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.5032
2022-03-17 17:52:12 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.5987
2022-03-17 17:52:45 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.5879
2022-03-17 17:53:19 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.5005
2022-03-17 17:53:52 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.4709
2022-03-17 17:54:25 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.6213
2022-03-17 17:54:58 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.2447
2022-03-17 17:55:31 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.3744
2022-03-17 17:56:04 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.5217
2022-03-17 17:56:38 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.4877
2022-03-17 17:57:11 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.5555
2022-03-17 17:57:43 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.4892
2022-03-17 17:57:45 - train: epoch 036, train_loss: 1.5453
2022-03-17 17:59:00 - eval: epoch: 036, acc1: 68.022%, acc5: 88.296%, test_loss: 1.2909, per_image_load_time: 2.400ms, per_image_inference_time: 0.517ms
2022-03-17 17:59:01 - until epoch: 036, best_acc1: 68.022%
2022-03-17 17:59:01 - epoch 037 lr: 0.010000000000000002
2022-03-17 17:59:40 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.3484
2022-03-17 18:00:13 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.3630
2022-03-17 18:00:46 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.3482
2022-03-17 18:01:19 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.5900
2022-03-17 18:01:53 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.5856
2022-03-17 18:02:26 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.6548
2022-03-17 18:02:59 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.6047
2022-03-17 18:03:31 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.4988
2022-03-17 18:04:04 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.8394
2022-03-17 18:04:36 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.3688
2022-03-17 18:05:09 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.5476
2022-03-17 18:05:41 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.5152
2022-03-17 18:06:14 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.5149
2022-03-17 18:06:47 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.6521
2022-03-17 18:07:20 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.4132
2022-03-17 18:07:53 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.3844
2022-03-17 18:08:26 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.7277
2022-03-17 18:08:59 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.6608
2022-03-17 18:09:32 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.6156
2022-03-17 18:10:05 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.6074
2022-03-17 18:10:38 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.5889
2022-03-17 18:11:11 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.5517
2022-03-17 18:11:44 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.4288
2022-03-17 18:12:17 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.5341
2022-03-17 18:12:50 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.4580
2022-03-17 18:13:23 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.7333
2022-03-17 18:13:56 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.6590
2022-03-17 18:14:29 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.5443
2022-03-17 18:15:03 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.5153
2022-03-17 18:15:36 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.6194
2022-03-17 18:16:09 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.5066
2022-03-17 18:16:42 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.5761
2022-03-17 18:17:15 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.4192
2022-03-17 18:17:48 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.3901
2022-03-17 18:18:21 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.5291
2022-03-17 18:18:54 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.6436
2022-03-17 18:19:28 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.6809
2022-03-17 18:20:01 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.5322
2022-03-17 18:20:34 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.7256
2022-03-17 18:21:07 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.5221
2022-03-17 18:21:40 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.5253
2022-03-17 18:22:13 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.7448
2022-03-17 18:22:46 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.5652
2022-03-17 18:23:19 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.4844
2022-03-17 18:23:53 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.4614
2022-03-17 18:24:26 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.5399
2022-03-17 18:24:59 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.5582
2022-03-17 18:25:32 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.5498
2022-03-17 18:26:06 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.6135
2022-03-17 18:26:38 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.5254
2022-03-17 18:26:40 - train: epoch 037, train_loss: 1.5410
2022-03-17 18:27:55 - eval: epoch: 037, acc1: 68.108%, acc5: 88.578%, test_loss: 1.2824, per_image_load_time: 2.265ms, per_image_inference_time: 0.570ms
2022-03-17 18:27:56 - until epoch: 037, best_acc1: 68.108%
2022-03-17 18:27:56 - epoch 038 lr: 0.010000000000000002
2022-03-17 18:28:35 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.4739
2022-03-17 18:29:07 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.3002
2022-03-17 18:29:40 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.2156
2022-03-17 18:30:13 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.4528
2022-03-17 18:30:46 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.3455
2022-03-17 18:31:20 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.6794
2022-03-17 18:31:53 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.3327
2022-03-17 18:32:26 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.4076
2022-03-17 18:32:59 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.5429
2022-03-17 18:33:31 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.8071
2022-03-17 18:34:04 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.5276
2022-03-17 18:34:37 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.5520
2022-03-17 18:35:10 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.7050
2022-03-17 18:35:43 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.5896
2022-03-17 18:36:16 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.5385
2022-03-17 18:36:49 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.6234
2022-03-17 18:37:22 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.6675
2022-03-17 18:37:56 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.7036
2022-03-17 18:38:29 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.5919
2022-03-17 18:39:02 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.5351
2022-03-17 18:39:35 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.5804
2022-03-17 18:40:08 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.4366
2022-03-17 18:40:41 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.5905
2022-03-17 18:41:15 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.6906
2022-03-17 18:41:48 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.4121
2022-03-17 18:42:21 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.5454
2022-03-17 18:42:55 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.6137
2022-03-17 18:43:28 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.7063
2022-03-17 18:44:01 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.6025
2022-03-17 18:44:35 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.5165
2022-03-17 18:45:08 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.4589
2022-03-17 18:45:41 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.2927
2022-03-17 18:46:14 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.3460
2022-03-17 18:46:47 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.5028
2022-03-17 18:47:21 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.5568
2022-03-17 18:47:54 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.5849
2022-03-17 18:48:27 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.3127
2022-03-17 18:49:00 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.6429
2022-03-17 18:49:34 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.4720
2022-03-17 18:50:06 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.5745
2022-03-17 18:50:40 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.4974
2022-03-17 18:51:13 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.3563
2022-03-17 18:51:47 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.6771
2022-03-17 18:52:20 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.6356
2022-03-17 18:52:53 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.6366
2022-03-17 18:53:26 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.7131
2022-03-17 18:53:59 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.4929
2022-03-17 18:54:33 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.4715
2022-03-17 18:55:06 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.5439
2022-03-17 18:55:38 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.3929
2022-03-17 18:55:39 - train: epoch 038, train_loss: 1.5370
2022-03-17 18:56:55 - eval: epoch: 038, acc1: 67.924%, acc5: 88.106%, test_loss: 1.2946, per_image_load_time: 1.174ms, per_image_inference_time: 0.544ms
2022-03-17 18:56:55 - until epoch: 038, best_acc1: 68.108%
2022-03-17 18:56:55 - epoch 039 lr: 0.010000000000000002
2022-03-17 18:57:35 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.5315
2022-03-17 18:58:08 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.7103
2022-03-17 18:58:41 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.3493
2022-03-17 18:59:14 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.5290
2022-03-17 18:59:48 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.4182
2022-03-17 19:00:21 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.3524
2022-03-17 19:00:55 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.6046
2022-03-17 19:01:28 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.4533
2022-03-17 19:02:01 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.6478
2022-03-17 19:02:35 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.3787
2022-03-17 19:03:08 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.6579
2022-03-17 19:03:40 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.6012
2022-03-17 19:04:13 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.8110
2022-03-17 19:04:46 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.5454
2022-03-17 19:05:19 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.4666
2022-03-17 19:05:52 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.6314
2022-03-17 19:06:25 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.2763
2022-03-17 19:06:58 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.5979
2022-03-17 19:07:32 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.3138
2022-03-17 19:08:05 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.4425
2022-03-17 19:08:38 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.5393
2022-03-17 19:09:11 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.4605
2022-03-17 19:09:44 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.6864
2022-03-17 19:10:18 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.6996
2022-03-17 19:10:50 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.4817
2022-03-17 19:11:24 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.5907
2022-03-17 19:11:56 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.6446
2022-03-17 19:12:29 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.4786
2022-03-17 19:13:02 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.3806
2022-03-17 19:13:35 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.5845
2022-03-17 19:14:09 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.3900
2022-03-17 19:14:42 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.5756
2022-03-17 19:15:15 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.5484
2022-03-17 19:15:48 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.5817
2022-03-17 19:16:21 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.7440
2022-03-17 19:16:54 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.6369
2022-03-17 19:17:27 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.5622
2022-03-17 19:18:00 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.3747
2022-03-17 19:18:33 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.5877
2022-03-17 19:19:06 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.5230
2022-03-17 19:19:40 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.5580
2022-03-17 19:20:12 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.5995
2022-03-17 19:20:45 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.4381
2022-03-17 19:21:18 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.3346
2022-03-17 19:21:51 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.3362
2022-03-17 19:22:24 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.6426
2022-03-17 19:22:57 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.6606
2022-03-17 19:23:30 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.6100
2022-03-17 19:24:03 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.4456
2022-03-17 19:24:35 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.3920
2022-03-17 19:24:37 - train: epoch 039, train_loss: 1.5339
2022-03-17 19:25:53 - eval: epoch: 039, acc1: 67.680%, acc5: 88.354%, test_loss: 1.2911, per_image_load_time: 2.231ms, per_image_inference_time: 0.543ms
2022-03-17 19:25:54 - until epoch: 039, best_acc1: 68.108%
2022-03-17 19:25:54 - epoch 040 lr: 0.010000000000000002
2022-03-17 19:26:33 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.7745
2022-03-17 19:27:06 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.7339
2022-03-17 19:27:39 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.7124
2022-03-17 19:28:12 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.4999
2022-03-17 19:28:46 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.3539
2022-03-17 19:29:19 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.6347
2022-03-17 19:29:52 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.5608
2022-03-17 19:30:25 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.6312
2022-03-17 19:30:58 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.4550
2022-03-17 19:31:31 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.2102
2022-03-17 19:32:04 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.4853
2022-03-17 19:32:37 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.4396
2022-03-17 19:33:10 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.4682
2022-03-17 19:33:43 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.4204
2022-03-17 19:34:16 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.7617
2022-03-17 19:34:48 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.6257
2022-03-17 19:35:21 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.6551
2022-03-17 19:35:54 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.4222
2022-03-17 19:36:27 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.5832
2022-03-17 19:37:00 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.5506
2022-03-17 19:37:33 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.4899
2022-03-17 19:38:06 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.3060
2022-03-17 19:38:39 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.4951
2022-03-17 19:39:12 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.5888
2022-03-17 19:39:45 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.6622
2022-03-17 19:40:18 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.4808
2022-03-17 19:40:51 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.6438
2022-03-17 19:41:24 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.4343
2022-03-17 19:41:56 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.7845
2022-03-17 19:42:29 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.6484
2022-03-17 19:43:02 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.4521
2022-03-17 19:43:36 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.6317
2022-03-17 19:44:09 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.6044
2022-03-17 19:44:42 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.5526
2022-03-17 19:45:15 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.6576
2022-03-17 19:45:48 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.5935
2022-03-17 19:46:21 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.5842
2022-03-17 19:46:54 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.3758
2022-03-17 19:47:27 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.5860
2022-03-17 19:48:00 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.6213
2022-03-17 19:48:33 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.6189
2022-03-17 19:49:06 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.4172
2022-03-17 19:49:40 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.5950
2022-03-17 19:50:13 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.5471
2022-03-17 19:50:46 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.3370
2022-03-17 19:51:19 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.5800
2022-03-17 19:51:52 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.4410
2022-03-17 19:52:26 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.5199
2022-03-17 19:52:59 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.5823
2022-03-17 19:53:31 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.6232
2022-03-17 19:53:33 - train: epoch 040, train_loss: 1.5316
2022-03-17 19:54:49 - eval: epoch: 040, acc1: 68.048%, acc5: 88.416%, test_loss: 1.2871, per_image_load_time: 2.388ms, per_image_inference_time: 0.523ms
2022-03-17 19:54:49 - until epoch: 040, best_acc1: 68.108%
2022-03-17 19:54:49 - epoch 041 lr: 0.010000000000000002
2022-03-17 19:55:29 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.6560
2022-03-17 19:56:02 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.5846
2022-03-17 19:56:35 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.4624
2022-03-17 19:57:08 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.5612
2022-03-17 19:57:41 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.3298
2022-03-17 19:58:14 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.6654
2022-03-17 19:58:47 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.5489
2022-03-17 19:59:21 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.3122
2022-03-17 19:59:54 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.2196
2022-03-17 20:00:27 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.7415
2022-03-17 20:01:00 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.4373
2022-03-17 20:01:32 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.2995
2022-03-17 20:02:04 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.5925
2022-03-17 20:02:36 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.6163
2022-03-17 20:03:07 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.8562
2022-03-17 20:03:39 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.2545
2022-03-17 20:04:11 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.4817
2022-03-17 20:04:43 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.4193
2022-03-17 20:05:15 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.5948
2022-03-17 20:05:47 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.4625
2022-03-17 20:06:19 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.3091
2022-03-17 20:06:51 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.4013
2022-03-17 20:07:22 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.2873
2022-03-17 20:07:55 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.7689
2022-03-17 20:08:26 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.4274
2022-03-17 20:08:58 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.6775
2022-03-17 20:09:30 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.7602
2022-03-17 20:10:02 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.5984
2022-03-17 20:10:34 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.6591
2022-03-17 20:11:06 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.6344
2022-03-17 20:11:38 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.7330
2022-03-17 20:12:10 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.4361
2022-03-17 20:12:42 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.4388
2022-03-17 20:13:14 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.3929
2022-03-17 20:13:46 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.5718
2022-03-17 20:14:18 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.6195
2022-03-17 20:14:50 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.4415
2022-03-17 20:15:23 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.4734
2022-03-17 20:15:56 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.5403
2022-03-17 20:16:29 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.3886
2022-03-17 20:17:03 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.6089
2022-03-17 20:17:36 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.4189
2022-03-17 20:18:09 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.5966
2022-03-17 20:18:42 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.5046
2022-03-17 20:19:15 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.5573
2022-03-17 20:19:48 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.5943
2022-03-17 20:20:21 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.7623
2022-03-17 20:20:54 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.5341
2022-03-17 20:21:28 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.5454
2022-03-17 20:22:00 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.7103
2022-03-17 20:22:01 - train: epoch 041, train_loss: 1.5338
2022-03-17 20:23:17 - eval: epoch: 041, acc1: 68.000%, acc5: 88.356%, test_loss: 1.2838, per_image_load_time: 2.362ms, per_image_inference_time: 0.563ms
2022-03-17 20:23:17 - until epoch: 041, best_acc1: 68.108%
2022-03-17 20:23:17 - epoch 042 lr: 0.010000000000000002
2022-03-17 20:23:57 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.2885
2022-03-17 20:24:29 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.5032
2022-03-17 20:25:02 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.5420
2022-03-17 20:25:34 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.2360
2022-03-17 20:26:07 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.5671
2022-03-17 20:26:39 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.3622
2022-03-17 20:27:12 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.6171
2022-03-17 20:27:45 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.4733
2022-03-17 20:28:18 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.7923
2022-03-17 20:28:51 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.6203
2022-03-17 20:29:25 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.3558
2022-03-17 20:29:57 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.4053
2022-03-17 20:30:31 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.4938
2022-03-17 20:31:04 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.8747
2022-03-17 20:31:37 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.4329
2022-03-17 20:32:10 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.5975
2022-03-17 20:32:43 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.5187
2022-03-17 20:33:16 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.5228
2022-03-17 20:33:49 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.7163
2022-03-17 20:34:22 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.3902
2022-03-17 20:34:56 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.3000
2022-03-17 20:35:29 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.4372
2022-03-17 20:36:02 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.3801
2022-03-17 20:36:35 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.8164
2022-03-17 20:37:08 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.6220
2022-03-17 20:37:41 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.5777
2022-03-17 20:38:15 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.4028
2022-03-17 20:38:48 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.4718
2022-03-17 20:39:22 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.5433
2022-03-17 20:39:55 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.5511
2022-03-17 20:40:28 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.5535
2022-03-17 20:41:01 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.6897
2022-03-17 20:41:34 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.6566
2022-03-17 20:42:07 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.2975
2022-03-17 20:42:40 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.5380
2022-03-17 20:43:13 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.6322
2022-03-17 20:43:46 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.5121
2022-03-17 20:44:19 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.4319
2022-03-17 20:44:52 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.4315
2022-03-17 20:45:26 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.5056
2022-03-17 20:45:59 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.6849
2022-03-17 20:46:33 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.6253
2022-03-17 20:47:06 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.4049
2022-03-17 20:47:40 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.3697
2022-03-17 20:48:14 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.5338
2022-03-17 20:48:47 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.7349
2022-03-17 20:49:21 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.3570
2022-03-17 20:49:55 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.6921
2022-03-17 20:50:29 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.5908
2022-03-17 20:51:03 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.4385
2022-03-17 20:51:04 - train: epoch 042, train_loss: 1.5314
2022-03-17 20:52:21 - eval: epoch: 042, acc1: 67.536%, acc5: 88.342%, test_loss: 1.2979, per_image_load_time: 1.546ms, per_image_inference_time: 0.557ms
2022-03-17 20:52:22 - until epoch: 042, best_acc1: 68.108%
2022-03-17 23:57:33 - epoch 044 lr: 0.010000000000000002
2022-03-17 23:58:13 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.5761
2022-03-17 23:58:46 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.4788
2022-03-17 23:59:19 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.5398
2022-03-17 23:59:52 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.2633
2022-03-18 00:00:26 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.6028
2022-03-18 00:00:59 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.2763
2022-03-18 00:01:32 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.4714
2022-03-18 00:02:05 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.3819
2022-03-18 00:02:38 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.5246
2022-03-18 00:03:11 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.3984
2022-03-18 00:03:44 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.3151
2022-03-18 00:04:17 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.3240
2022-03-18 00:04:50 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.7911
2022-03-18 00:05:23 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.4692
2022-03-18 00:05:56 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.6112
2022-03-18 00:06:29 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.4361
2022-03-18 00:07:02 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.5483
2022-03-18 00:07:36 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.3564
2022-03-18 00:08:09 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.6661
2022-03-18 00:08:42 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.3679
2022-03-18 00:09:16 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.6593
2022-03-18 00:09:49 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.4680
2022-03-18 00:10:22 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.7436
2022-03-18 00:10:55 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.6826
2022-03-18 00:11:29 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.6057
2022-03-18 00:12:02 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.5436
2022-03-18 00:12:35 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.6381
2022-03-18 00:13:08 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.3425
2022-03-18 00:13:41 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.2955
2022-03-18 00:14:14 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.3317
2022-03-18 00:14:47 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.4019
2022-03-18 00:15:20 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.4763
2022-03-18 00:15:53 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.4670
2022-03-18 00:16:26 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.6596
2022-03-18 00:16:59 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.4900
2022-03-18 00:17:32 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.6107
2022-03-18 00:18:05 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.7079
2022-03-18 00:18:38 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.2161
2022-03-18 00:19:11 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.7592
2022-03-18 00:19:44 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.5008
2022-03-18 00:20:17 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.4258
2022-03-18 00:20:50 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.6060
2022-03-18 00:21:23 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.4277
2022-03-18 00:21:56 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.3407
2022-03-18 00:22:29 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.7628
2022-03-18 00:23:02 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.4647
2022-03-18 00:23:35 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.6588
2022-03-18 00:24:08 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.5585
2022-03-18 00:24:41 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.6056
2022-03-18 00:25:14 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.6639
2022-03-18 00:25:15 - train: epoch 044, train_loss: 1.5295
2022-03-18 00:26:30 - eval: epoch: 044, acc1: 67.846%, acc5: 88.432%, test_loss: 1.2913, per_image_load_time: 2.401ms, per_image_inference_time: 0.528ms
2022-03-18 00:26:31 - until epoch: 044, best_acc1: 68.108%
2022-03-18 00:26:31 - epoch 045 lr: 0.010000000000000002
2022-03-18 00:27:11 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.5232
2022-03-18 00:27:44 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.5999
2022-03-18 00:28:17 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.4770
2022-03-18 00:28:50 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.6128
2022-03-18 00:29:24 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.7604
2022-03-18 00:29:57 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.7367
2022-03-18 00:30:31 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.2048
2022-03-18 00:31:04 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.4469
2022-03-18 00:31:37 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.3603
2022-03-18 00:32:10 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.4750
2022-03-18 00:32:43 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.5746
2022-03-18 00:33:16 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.6129
2022-03-18 00:33:49 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.5290
2022-03-18 00:34:22 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.5106
2022-03-18 00:34:55 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.3837
2022-03-18 00:35:28 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.4219
2022-03-18 00:36:01 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.5887
2022-03-18 00:36:34 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.4709
2022-03-18 00:37:07 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.4275
2022-03-18 00:37:41 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.6828
2022-03-18 00:38:14 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.5975
2022-03-18 00:38:47 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.5837
2022-03-18 00:39:20 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.2763
2022-03-18 00:39:53 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.5750
2022-03-18 00:40:27 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.4382
2022-03-18 00:41:00 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.5533
2022-03-18 00:41:33 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.2596
2022-03-18 00:42:06 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.5261
2022-03-18 00:42:39 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.6330
2022-03-18 00:43:12 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.5684
2022-03-18 00:43:46 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.6239
2022-03-18 00:44:19 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.5434
2022-03-18 00:44:52 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.5354
2022-03-18 00:45:25 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.3656
2022-03-18 00:45:59 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.5937
2022-03-18 00:46:32 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.5776
2022-03-18 00:47:05 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.3723
2022-03-18 00:47:39 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.6013
2022-03-18 00:48:12 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.6302
2022-03-18 00:48:45 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.4389
2022-03-18 00:49:18 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.4031
2022-03-18 00:49:51 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.5810
2022-03-18 00:50:25 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.6459
2022-03-18 00:50:58 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.5243
2022-03-18 00:51:31 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.4789
2022-03-18 00:52:05 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.7039
2022-03-18 00:52:38 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.4206
2022-03-18 00:53:11 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.5086
2022-03-18 00:53:45 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.3694
2022-03-18 00:54:17 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.4415
2022-03-18 00:54:19 - train: epoch 045, train_loss: 1.5255
2022-03-18 00:55:34 - eval: epoch: 045, acc1: 67.804%, acc5: 88.326%, test_loss: 1.2926, per_image_load_time: 2.343ms, per_image_inference_time: 0.569ms
2022-03-18 00:55:35 - until epoch: 045, best_acc1: 68.108%
2022-03-18 00:55:35 - epoch 046 lr: 0.010000000000000002
2022-03-18 00:56:15 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.3780
2022-03-18 00:56:48 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.2187
2022-03-18 00:57:21 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.4035
2022-03-18 00:57:54 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.4185
2022-03-18 00:58:28 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.3930
2022-03-18 00:59:01 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.4941
2022-03-18 00:59:34 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.4012
2022-03-18 01:00:07 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.7073
2022-03-18 01:00:40 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.4907
2022-03-18 01:01:13 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.3548
2022-03-18 01:01:46 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.4736
2022-03-18 01:02:19 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.5446
2022-03-18 01:02:53 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.4309
2022-03-18 01:03:26 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.7363
2022-03-18 01:03:59 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.4075
2022-03-18 01:04:32 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.5801
2022-03-18 01:05:05 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.5887
2022-03-18 01:05:39 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.4628
2022-03-18 01:06:12 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.3964
2022-03-18 01:06:45 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.6684
2022-03-18 01:07:18 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.9025
2022-03-18 01:07:52 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.3449
2022-03-18 01:08:25 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.6849
2022-03-18 01:08:58 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.5147
2022-03-18 01:09:32 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.4649
2022-03-18 01:10:05 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.6676
2022-03-18 01:10:38 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.3469
2022-03-18 01:11:11 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.3578
2022-03-18 01:11:44 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.5233
2022-03-18 01:12:17 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.3345
2022-03-18 01:12:50 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.3093
2022-03-18 01:13:23 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.5937
2022-03-18 01:13:57 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.5854
2022-03-18 01:14:30 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.5386
2022-03-18 01:15:03 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.5207
2022-03-18 01:15:36 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.5625
2022-03-18 01:16:09 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.4800
2022-03-18 01:16:42 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.3888
2022-03-18 01:17:15 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.5668
2022-03-18 01:17:48 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.5879
2022-03-18 01:18:21 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.5465
2022-03-18 01:18:55 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.3448
2022-03-18 01:19:28 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.6689
2022-03-18 01:20:01 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.5252
2022-03-18 01:20:34 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.4503
2022-03-18 01:21:07 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.6038
2022-03-18 01:21:41 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.4344
2022-03-18 01:22:14 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.5277
2022-03-18 01:22:47 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.6722
2022-03-18 01:23:19 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.3617
2022-03-18 01:23:21 - train: epoch 046, train_loss: 1.5244
2022-03-18 01:24:36 - eval: epoch: 046, acc1: 68.002%, acc5: 88.536%, test_loss: 1.2853, per_image_load_time: 2.421ms, per_image_inference_time: 0.526ms
2022-03-18 01:24:36 - until epoch: 046, best_acc1: 68.108%
2022-03-18 01:24:36 - epoch 047 lr: 0.010000000000000002
2022-03-18 01:25:16 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.5278
2022-03-18 01:25:49 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.6304
2022-03-18 01:26:22 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.3875
2022-03-18 01:26:56 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.3550
2022-03-18 01:27:29 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.5860
2022-03-18 01:28:02 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.6605
2022-03-18 01:28:35 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.6008
2022-03-18 01:29:08 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.5733
2022-03-18 01:29:41 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.6683
2022-03-18 01:30:14 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.6035
2022-03-18 01:30:47 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.5348
2022-03-18 01:31:20 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.4660
2022-03-18 01:31:53 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.5046
2022-03-18 01:32:26 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.6775
2022-03-18 01:32:59 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.5349
2022-03-18 01:33:32 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.4172
2022-03-18 01:34:06 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.3571
2022-03-18 01:34:39 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.4142
2022-03-18 01:35:12 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.3722
2022-03-18 01:35:45 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.4650
2022-03-18 01:36:19 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.5872
2022-03-18 01:36:52 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.6498
2022-03-18 01:37:25 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.6510
2022-03-18 01:37:58 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.4321
2022-03-18 01:38:31 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.5176
2022-03-18 01:39:04 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.5925
2022-03-18 01:39:37 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.3578
2022-03-18 01:40:10 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.4968
2022-03-18 01:40:43 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.3781
2022-03-18 01:41:16 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.6034
2022-03-18 01:41:49 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.6896
2022-03-18 01:42:22 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.5641
2022-03-18 01:42:55 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.5707
2022-03-18 01:43:29 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.4303
2022-03-18 01:44:02 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.6682
2022-03-18 01:44:35 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.3992
2022-03-18 01:45:08 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.5561
2022-03-18 01:45:41 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.5321
2022-03-18 01:46:14 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.5894
2022-03-18 01:46:47 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.5679
2022-03-18 01:47:20 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.6147
2022-03-18 01:47:53 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.5652
2022-03-18 01:48:27 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.4308
2022-03-18 01:49:00 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.4436
2022-03-18 01:49:33 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.4517
2022-03-18 01:50:06 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.4485
2022-03-18 01:50:39 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.7341
2022-03-18 01:51:12 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.3371
2022-03-18 01:51:45 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.5162
2022-03-18 01:52:18 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.5365
2022-03-18 01:52:19 - train: epoch 047, train_loss: 1.5262
2022-03-18 01:53:34 - eval: epoch: 047, acc1: 67.860%, acc5: 88.228%, test_loss: 1.2964, per_image_load_time: 2.367ms, per_image_inference_time: 0.569ms
2022-03-18 01:53:35 - until epoch: 047, best_acc1: 68.108%
2022-03-18 01:53:35 - epoch 048 lr: 0.010000000000000002
2022-03-18 01:54:14 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.5939
2022-03-18 01:54:47 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.7256
2022-03-18 01:55:20 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.4631
2022-03-18 01:55:53 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.4700
2022-03-18 01:56:26 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.4832
2022-03-18 01:56:59 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.4687
2022-03-18 01:57:32 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.4734
2022-03-18 01:58:06 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.4784
2022-03-18 01:58:39 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.5077
2022-03-18 01:59:12 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.5454
2022-03-18 01:59:45 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.5531
2022-03-18 02:00:18 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.6455
2022-03-18 02:00:52 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.4909
2022-03-18 02:01:25 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.5911
2022-03-18 02:01:58 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.5051
2022-03-18 02:02:31 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.5606
2022-03-18 02:03:04 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.6194
2022-03-18 02:03:38 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.3920
2022-03-18 02:04:11 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.7166
2022-03-18 02:04:44 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.7090
2022-03-18 02:05:17 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.3919
2022-03-18 02:05:51 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.8770
2022-03-18 02:06:24 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.3557
2022-03-18 02:06:57 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.6765
2022-03-18 02:07:31 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.5509
2022-03-18 02:08:04 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.5865
2022-03-18 02:08:38 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.6681
2022-03-18 02:09:11 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.4138
2022-03-18 02:09:44 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.7069
2022-03-18 02:10:17 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.5584
2022-03-18 02:10:51 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.4280
2022-03-18 02:11:24 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.3288
2022-03-18 02:11:57 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.6812
2022-03-18 02:12:31 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.5759
2022-03-18 02:13:04 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.6931
2022-03-18 02:13:37 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.5305
2022-03-18 02:14:11 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.8251
2022-03-18 02:14:44 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.5915
2022-03-18 02:15:18 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.7383
2022-03-18 02:15:51 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.1911
2022-03-18 02:16:24 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.8712
2022-03-18 02:16:58 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.3819
2022-03-18 02:17:31 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.6095
2022-03-18 02:18:04 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.4906
2022-03-18 02:18:37 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.5091
2022-03-18 02:19:11 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.5867
2022-03-18 02:19:44 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.6254
2022-03-18 02:20:17 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.8236
2022-03-18 02:20:50 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.5990
2022-03-18 02:21:23 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.4436
2022-03-18 02:21:24 - train: epoch 048, train_loss: 1.5184
2022-03-18 02:22:39 - eval: epoch: 048, acc1: 67.954%, acc5: 88.314%, test_loss: 1.2935, per_image_load_time: 2.325ms, per_image_inference_time: 0.574ms
2022-03-18 02:22:40 - until epoch: 048, best_acc1: 68.108%
2022-03-18 02:22:40 - epoch 049 lr: 0.010000000000000002
2022-03-18 02:23:20 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.6241
2022-03-18 02:23:52 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.4297
2022-03-18 02:24:26 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.5442
2022-03-18 02:24:59 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.4896
2022-03-18 02:25:32 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.7343
2022-03-18 02:26:05 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.1999
2022-03-18 02:26:39 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.4521
2022-03-18 02:27:12 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.7173
2022-03-18 02:27:45 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.2315
2022-03-18 02:28:18 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.5271
2022-03-18 02:28:52 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.5799
2022-03-18 02:29:25 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.4209
2022-03-18 02:29:58 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.8875
2022-03-18 02:30:31 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.6591
2022-03-18 02:31:05 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.4140
2022-03-18 02:31:38 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.4342
2022-03-18 02:32:11 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.5621
2022-03-18 02:32:45 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.4841
2022-03-18 02:33:18 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.3721
2022-03-18 02:33:52 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.4966
2022-03-18 02:34:25 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.4693
2022-03-18 02:34:59 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.6830
2022-03-18 02:35:32 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.4893
2022-03-18 02:36:05 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.5447
2022-03-18 02:36:39 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.7143
2022-03-18 02:37:12 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.6258
2022-03-18 02:37:46 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.3943
2022-03-18 02:38:20 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.7021
2022-03-18 02:38:53 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.7563
2022-03-18 02:39:27 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.5304
2022-03-18 02:40:00 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.4918
2022-03-18 02:40:33 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.8332
2022-03-18 02:41:07 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.4467
2022-03-18 02:41:40 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.5325
2022-03-18 02:42:13 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.4991
2022-03-18 02:42:47 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.6209
2022-03-18 02:43:20 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.5155
2022-03-18 02:43:54 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.5826
2022-03-18 02:44:27 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.6977
2022-03-18 02:45:01 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.5593
2022-03-18 02:45:34 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.5630
2022-03-18 02:46:07 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.6236
2022-03-18 02:46:41 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.7354
2022-03-18 02:47:15 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.7307
2022-03-18 02:47:48 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.4471
2022-03-18 02:48:21 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.8129
2022-03-18 02:48:55 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.8221
2022-03-18 02:49:29 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.4682
2022-03-18 02:50:03 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.4357
2022-03-18 02:50:35 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.3550
2022-03-18 02:50:37 - train: epoch 049, train_loss: 1.5180
2022-03-18 02:51:52 - eval: epoch: 049, acc1: 67.860%, acc5: 88.208%, test_loss: 1.2907, per_image_load_time: 2.285ms, per_image_inference_time: 0.552ms
2022-03-18 02:51:53 - until epoch: 049, best_acc1: 68.108%
2022-03-18 02:51:53 - epoch 050 lr: 0.010000000000000002
2022-03-18 02:52:32 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.3931
2022-03-18 02:53:05 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.5141
2022-03-18 02:53:38 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.5678
2022-03-18 02:54:11 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.2881
2022-03-18 02:54:44 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.3639
2022-03-18 02:55:18 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.7953
2022-03-18 02:55:51 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.2780
2022-03-18 02:56:24 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.3635
2022-03-18 02:56:58 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.3820
2022-03-18 02:57:31 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.4087
2022-03-18 02:58:05 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.6446
2022-03-18 02:58:39 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.4970
2022-03-18 02:59:12 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.3836
2022-03-18 02:59:46 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.3829
2022-03-18 03:00:19 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.4228
2022-03-18 03:00:53 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.5900
2022-03-18 03:01:27 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.6937
2022-03-18 03:02:00 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.5669
2022-03-18 03:02:34 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.4240
2022-03-18 03:03:07 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.4639
2022-03-18 03:03:41 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.3759
2022-03-18 03:04:15 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.4747
2022-03-18 03:04:48 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.2417
2022-03-18 03:05:22 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.3998
2022-03-18 03:05:55 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.5202
2022-03-18 03:06:29 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.3938
2022-03-18 03:07:02 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.4849
2022-03-18 03:07:36 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.4871
2022-03-18 03:08:10 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.7402
2022-03-18 03:08:43 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.5966
2022-03-18 03:09:17 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.4443
2022-03-18 03:09:50 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.6152
2022-03-18 03:10:24 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.4691
2022-03-18 03:10:58 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.4679
2022-03-18 03:11:31 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.5606
2022-03-18 03:12:04 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.4442
2022-03-18 03:12:38 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.7292
2022-03-18 03:13:11 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.3820
2022-03-18 03:13:45 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.3007
2022-03-18 03:14:18 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.5712
2022-03-18 03:14:52 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.3400
2022-03-18 03:15:25 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.8123
2022-03-18 03:15:58 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.8057
2022-03-18 03:16:32 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.4533
2022-03-18 03:17:05 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.5739
2022-03-18 03:17:39 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.4849
2022-03-18 03:18:12 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.6379
2022-03-18 03:18:46 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.6325
2022-03-18 03:19:19 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.4703
2022-03-18 03:19:52 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.4655
2022-03-18 03:19:53 - train: epoch 050, train_loss: 1.5158
2022-03-18 03:21:09 - eval: epoch: 050, acc1: 67.730%, acc5: 88.468%, test_loss: 1.2911, per_image_load_time: 2.075ms, per_image_inference_time: 0.564ms
2022-03-18 03:21:10 - until epoch: 050, best_acc1: 68.108%
2022-03-18 03:21:10 - epoch 051 lr: 0.010000000000000002
2022-03-18 03:21:50 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.7094
2022-03-18 03:22:22 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.6146
2022-03-18 03:22:55 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.4082
2022-03-18 03:23:27 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.3315
2022-03-18 03:23:59 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.3941
2022-03-18 03:24:32 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.2318
2022-03-18 03:25:04 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.5782
2022-03-18 03:25:37 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.6540
2022-03-18 03:26:10 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.5608
2022-03-18 03:26:43 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.9364
2022-03-18 03:27:16 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.4568
2022-03-18 03:27:49 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.3674
2022-03-18 03:28:22 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.2579
2022-03-18 03:28:56 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.5256
2022-03-18 03:29:29 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.5590
2022-03-18 03:30:02 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.1558
2022-03-18 03:30:36 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.6097
2022-03-18 03:31:09 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.4379
2022-03-18 03:31:42 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.2922
2022-03-18 03:32:15 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.3703
2022-03-18 03:32:49 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.4968
2022-03-18 03:33:22 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.4942
2022-03-18 03:33:55 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.6833
2022-03-18 03:34:29 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.6759
2022-03-18 03:35:03 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.5306
2022-03-18 03:35:36 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.4472
2022-03-18 03:36:09 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.4631
2022-03-18 03:36:43 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.2339
2022-03-18 03:37:16 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.2458
2022-03-18 03:37:49 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.4640
2022-03-18 03:38:23 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.4368
2022-03-18 03:38:56 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.4818
2022-03-18 03:39:30 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.4767
2022-03-18 03:40:03 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.5642
2022-03-18 03:40:37 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.5358
2022-03-18 03:41:10 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.3308
2022-03-18 03:41:43 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.6705
2022-03-18 03:42:17 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.4071
2022-03-18 03:42:50 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.6859
2022-03-18 03:43:23 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.4983
2022-03-18 03:43:57 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.6023
2022-03-18 03:44:30 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.6849
2022-03-18 03:45:03 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.4217
2022-03-18 03:45:36 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.5805
2022-03-18 03:46:10 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.3251
2022-03-18 03:46:43 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.5536
2022-03-18 03:47:17 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.4437
2022-03-18 03:47:50 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.3713
2022-03-18 03:48:24 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.6829
2022-03-18 03:48:56 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.5068
2022-03-18 03:48:58 - train: epoch 051, train_loss: 1.5139
2022-03-18 03:50:14 - eval: epoch: 051, acc1: 67.482%, acc5: 88.214%, test_loss: 1.3012, per_image_load_time: 1.418ms, per_image_inference_time: 0.561ms
2022-03-18 03:50:15 - until epoch: 051, best_acc1: 68.108%
2022-03-18 03:50:15 - epoch 052 lr: 0.010000000000000002
2022-03-18 03:50:55 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.6884
2022-03-18 03:51:28 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.5100
2022-03-18 03:52:01 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.3848
2022-03-18 03:52:34 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.6328
2022-03-18 03:53:08 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.4908
2022-03-18 03:53:41 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.4375
2022-03-18 03:54:15 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.6684
2022-03-18 03:54:48 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.3151
2022-03-18 03:55:22 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.6017
2022-03-18 03:55:55 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.5869
2022-03-18 03:56:29 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.5860
2022-03-18 03:57:02 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.4877
2022-03-18 03:57:35 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.4815
2022-03-18 03:58:09 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.7930
2022-03-18 03:58:43 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.2473
2022-03-18 03:59:16 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.2820
2022-03-18 03:59:50 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.3781
2022-03-18 04:00:24 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.4626
2022-03-18 04:00:57 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.3872
2022-03-18 04:01:31 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.6798
2022-03-18 04:02:04 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.3508
2022-03-18 04:02:38 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.7334
2022-03-18 04:03:12 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.2710
2022-03-18 04:03:45 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.2659
2022-03-18 04:04:18 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.4035
2022-03-18 04:04:52 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.3725
2022-03-18 04:05:25 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.5242
2022-03-18 04:05:59 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.6185
2022-03-18 04:06:33 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.4071
2022-03-18 04:07:06 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.3893
2022-03-18 04:07:40 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.3961
2022-03-18 04:08:13 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.6198
2022-03-18 04:08:47 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.5300
2022-03-18 04:09:20 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.5521
2022-03-18 04:09:53 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.6499
2022-03-18 04:10:27 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.6318
2022-03-18 04:11:01 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.5571
2022-03-18 04:11:34 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.7084
2022-03-18 04:12:07 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.2613
2022-03-18 04:12:41 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.8094
2022-03-18 04:13:15 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.3154
2022-03-18 04:13:48 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.6042
2022-03-18 04:14:22 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.6452
2022-03-18 04:14:55 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.7451
2022-03-18 04:15:29 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.3644
2022-03-18 04:16:02 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.5594
2022-03-18 04:16:36 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.5149
2022-03-18 04:17:10 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.3803
2022-03-18 04:17:43 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.4618
2022-03-18 04:18:16 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.3728
2022-03-18 04:18:17 - train: epoch 052, train_loss: 1.5081
2022-03-18 04:19:33 - eval: epoch: 052, acc1: 67.682%, acc5: 88.358%, test_loss: 1.2930, per_image_load_time: 2.033ms, per_image_inference_time: 0.543ms
2022-03-18 04:19:34 - until epoch: 052, best_acc1: 68.108%
2022-03-18 04:19:34 - epoch 053 lr: 0.010000000000000002
2022-03-18 04:20:13 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.4814
2022-03-18 04:20:47 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.6454
2022-03-18 04:21:20 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.4840
2022-03-18 04:21:54 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.3174
2022-03-18 04:22:27 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.4501
2022-03-18 04:23:00 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.5168
2022-03-18 04:23:33 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.5691
2022-03-18 04:24:06 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.7114
2022-03-18 04:24:39 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.4033
2022-03-18 04:25:12 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.3706
2022-03-18 04:25:45 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.3793
2022-03-18 04:26:19 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.3957
2022-03-18 04:26:52 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.5332
2022-03-18 04:27:25 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.6624
2022-03-18 04:27:59 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.4874
2022-03-18 04:28:32 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.7125
2022-03-18 04:29:05 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.7389
2022-03-18 04:29:39 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.5628
2022-03-18 04:30:12 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.4510
2022-03-18 04:30:45 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.4926
2022-03-18 04:31:19 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.6511
2022-03-18 04:31:52 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.4799
2022-03-18 04:32:26 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.5226
2022-03-18 04:32:59 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.5219
2022-03-18 04:33:33 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.5550
2022-03-18 04:34:06 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.6783
2022-03-18 04:34:39 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.6974
2022-03-18 04:35:13 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.6488
2022-03-18 04:35:46 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.4009
2022-03-18 04:36:20 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.3373
2022-03-18 04:36:53 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.7935
2022-03-18 04:37:27 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.6562
2022-03-18 04:38:00 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.6225
2022-03-18 04:38:34 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.5091
2022-03-18 04:39:07 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.6506
2022-03-18 04:39:41 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.5877
2022-03-18 04:40:14 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.5673
2022-03-18 04:40:47 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.4911
2022-03-18 04:41:19 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.6985
2022-03-18 04:41:52 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.6176
2022-03-18 04:42:25 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.5706
2022-03-18 04:42:58 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.7004
2022-03-18 04:43:31 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.6905
2022-03-18 04:44:04 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.4143
2022-03-18 04:44:36 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.6208
2022-03-18 04:45:09 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.3696
2022-03-18 04:45:42 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.5544
2022-03-18 04:46:15 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.7589
2022-03-18 04:46:48 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.3842
2022-03-18 04:47:20 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.3902
2022-03-18 04:47:21 - train: epoch 053, train_loss: 1.5060
2022-03-18 04:48:37 - eval: epoch: 053, acc1: 68.188%, acc5: 88.498%, test_loss: 1.2833, per_image_load_time: 1.340ms, per_image_inference_time: 0.533ms
2022-03-18 04:48:38 - until epoch: 053, best_acc1: 68.188%
2022-03-18 04:48:38 - epoch 054 lr: 0.010000000000000002
2022-03-18 04:49:17 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.4582
2022-03-18 04:49:49 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.5578
2022-03-18 04:50:22 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.3601
2022-03-18 04:50:55 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.3813
2022-03-18 04:51:28 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.4594
2022-03-18 04:52:01 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.5052
2022-03-18 04:52:34 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.6050
2022-03-18 04:53:07 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.5220
2022-03-18 04:53:40 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.2312
2022-03-18 04:54:13 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.3586
2022-03-18 04:54:46 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.3701
2022-03-18 04:55:19 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.7861
2022-03-18 04:55:52 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.4809
2022-03-18 04:56:25 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.5646
2022-03-18 04:56:58 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.4227
2022-03-18 04:57:31 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.3446
2022-03-18 04:58:04 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.5160
2022-03-18 04:58:37 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.4363
2022-03-18 04:59:10 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.5373
2022-03-18 04:59:43 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.6100
2022-03-18 05:00:16 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.4332
2022-03-18 05:00:49 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.5431
2022-03-18 05:01:22 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.2333
2022-03-18 05:01:56 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.4772
2022-03-18 05:02:29 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.6766
2022-03-18 05:03:02 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.3889
2022-03-18 05:03:35 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.5479
2022-03-18 05:04:08 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.7901
2022-03-18 05:04:41 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.3997
2022-03-18 05:05:13 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.5848
2022-03-18 05:05:46 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.4965
2022-03-18 05:06:19 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.6695
2022-03-18 05:06:52 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.4591
2022-03-18 05:07:25 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.3717
2022-03-18 05:07:58 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.4681
2022-03-18 05:08:31 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.6935
2022-03-18 05:09:05 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.4160
2022-03-18 05:09:39 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.4625
2022-03-18 05:10:12 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.6278
2022-03-18 05:10:45 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.4937
2022-03-18 05:11:19 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.5666
2022-03-18 05:11:52 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.5247
2022-03-18 05:12:25 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.6083
2022-03-18 05:12:58 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.3349
2022-03-18 05:13:31 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.2772
2022-03-18 05:14:04 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.5331
2022-03-18 05:14:37 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.7741
2022-03-18 05:15:10 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.6627
2022-03-18 05:15:43 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.4595
2022-03-18 05:16:15 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.4671
2022-03-18 05:16:17 - train: epoch 054, train_loss: 1.5068
2022-03-18 05:17:32 - eval: epoch: 054, acc1: 68.170%, acc5: 88.580%, test_loss: 1.2766, per_image_load_time: 1.526ms, per_image_inference_time: 0.542ms
2022-03-18 05:17:32 - until epoch: 054, best_acc1: 68.188%
2022-03-18 05:17:32 - epoch 055 lr: 0.010000000000000002
2022-03-18 05:18:12 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.4539
2022-03-18 05:18:45 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.4552
2022-03-18 05:19:18 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.4674
2022-03-18 05:19:51 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.5107
2022-03-18 05:20:23 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.3837
2022-03-18 05:20:56 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.5538
2022-03-18 05:21:29 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.4012
2022-03-18 05:22:02 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.2652
2022-03-18 05:22:35 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.6382
2022-03-18 05:23:08 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.4379
2022-03-18 05:23:41 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.3717
2022-03-18 05:24:14 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.5394
2022-03-18 05:24:47 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.7627
2022-03-18 05:25:20 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.5028
2022-03-18 05:25:53 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.4481
2022-03-18 05:26:27 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.5638
2022-03-18 05:27:00 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.6443
2022-03-18 05:27:33 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.4313
2022-03-18 05:28:06 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.4901
2022-03-18 05:28:39 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.4114
2022-03-18 05:29:13 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.4514
2022-03-18 05:29:46 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.6772
2022-03-18 05:30:19 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.5313
2022-03-18 05:30:52 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.3812
2022-03-18 05:31:25 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.5212
2022-03-18 05:31:59 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.5143
2022-03-18 05:32:31 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.4135
2022-03-18 05:33:05 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.5224
2022-03-18 05:33:38 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.4744
2022-03-18 05:34:10 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.7253
2022-03-18 05:34:43 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.6698
2022-03-18 05:35:16 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.3133
2022-03-18 05:35:49 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.2382
2022-03-18 05:36:22 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.4254
2022-03-18 05:36:55 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.3981
2022-03-18 05:37:29 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.5802
2022-03-18 05:38:02 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.2248
2022-03-18 05:38:35 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.6493
2022-03-18 05:39:08 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.7731
2022-03-18 05:39:41 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.3950
2022-03-18 05:40:14 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.5027
2022-03-18 05:40:47 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.5215
2022-03-18 05:41:20 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.5828
2022-03-18 05:41:53 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.6425
2022-03-18 05:42:26 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.4579
2022-03-18 05:42:59 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.4544
2022-03-18 05:43:32 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.5049
2022-03-18 05:44:05 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.4706
2022-03-18 05:44:38 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.4722
2022-03-18 05:45:10 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.6399
2022-03-18 05:45:12 - train: epoch 055, train_loss: 1.4991
2022-03-18 05:46:26 - eval: epoch: 055, acc1: 68.358%, acc5: 88.732%, test_loss: 1.2636, per_image_load_time: 2.334ms, per_image_inference_time: 0.564ms
2022-03-18 05:46:27 - until epoch: 055, best_acc1: 68.358%
2022-03-18 05:46:27 - epoch 056 lr: 0.010000000000000002
2022-03-18 05:47:06 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.5545
2022-03-18 05:47:39 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.4354
2022-03-18 05:48:12 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.3990
2022-03-18 05:48:45 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.5575
2022-03-18 05:49:18 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.5317
2022-03-18 05:49:51 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.5535
2022-03-18 05:50:24 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.5712
2022-03-18 05:50:56 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.4043
2022-03-18 05:51:29 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.6593
2022-03-18 05:52:02 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.4891
2022-03-18 05:52:35 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.4857
2022-03-18 05:53:08 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.5918
2022-03-18 05:53:41 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.6475
2022-03-18 05:54:14 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.4605
2022-03-18 05:54:47 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.5847
2022-03-18 05:55:21 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.3608
2022-03-18 05:55:54 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.7263
2022-03-18 05:56:28 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.7772
2022-03-18 05:57:01 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.7078
2022-03-18 05:57:35 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.4315
2022-03-18 05:58:08 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.6141
2022-03-18 05:58:41 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.4958
2022-03-18 05:59:13 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.5485
2022-03-18 05:59:46 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.6229
2022-03-18 06:00:19 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.8605
2022-03-18 06:00:52 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.5097
2022-03-18 06:01:25 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.4891
2022-03-18 06:01:58 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.4995
2022-03-18 06:02:31 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.7702
2022-03-18 06:03:04 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.4386
2022-03-18 06:03:37 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.2819
2022-03-18 06:04:10 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.4023
2022-03-18 06:04:43 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.4111
2022-03-18 06:05:16 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.4519
2022-03-18 06:05:49 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.3692
2022-03-18 06:06:22 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.4075
2022-03-18 06:06:55 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.4356
2022-03-18 06:07:28 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.5973
2022-03-18 06:08:01 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.5805
2022-03-18 06:08:34 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.6890
2022-03-18 06:09:08 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.6390
2022-03-18 06:09:41 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.5803
2022-03-18 06:10:14 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.5826
2022-03-18 06:10:47 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.5408
2022-03-18 06:11:19 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.4503
2022-03-18 06:11:53 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.5873
2022-03-18 06:12:25 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.5300
2022-03-18 06:12:58 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.6297
2022-03-18 06:13:31 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.5043
2022-03-18 06:14:04 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.4387
2022-03-18 06:14:05 - train: epoch 056, train_loss: 1.4978
2022-03-18 06:15:20 - eval: epoch: 056, acc1: 68.196%, acc5: 88.672%, test_loss: 1.2774, per_image_load_time: 1.985ms, per_image_inference_time: 0.539ms
2022-03-18 06:15:20 - until epoch: 056, best_acc1: 68.358%
2022-03-18 06:15:20 - epoch 057 lr: 0.010000000000000002
2022-03-18 06:16:00 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.5932
2022-03-18 06:16:34 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.4134
2022-03-18 06:17:06 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.6149
2022-03-18 06:17:39 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.2735
2022-03-18 06:18:12 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.4291
2022-03-18 06:18:45 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.4376
2022-03-18 06:19:18 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.3725
2022-03-18 06:19:51 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.3644
2022-03-18 06:20:24 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.5676
2022-03-18 06:20:57 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.3788
2022-03-18 06:21:30 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.3569
2022-03-18 06:22:03 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.4493
2022-03-18 06:22:36 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.5653
2022-03-18 06:23:09 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.4478
2022-03-18 06:23:42 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.7314
2022-03-18 06:24:15 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.3665
2022-03-18 06:24:48 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.5511
2022-03-18 06:25:21 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.5184
2022-03-18 06:25:54 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.4441
2022-03-18 06:26:27 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.4370
2022-03-18 06:26:59 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.5707
2022-03-18 06:27:33 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.4792
2022-03-18 06:28:06 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.6656
2022-03-18 06:28:39 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.5598
2022-03-18 06:29:12 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 2.0378
2022-03-18 06:29:45 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.3204
2022-03-18 06:30:18 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.3690
2022-03-18 06:30:51 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.4801
2022-03-18 06:31:24 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.4863
2022-03-18 06:31:57 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.6810
2022-03-18 06:32:30 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.5806
2022-03-18 06:33:03 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.3966
2022-03-18 06:33:36 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.4034
2022-03-18 06:34:09 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.5424
2022-03-18 06:34:42 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.7261
2022-03-18 06:35:15 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.6295
2022-03-18 06:35:48 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.4749
2022-03-18 06:36:21 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.5075
2022-03-18 06:36:54 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.5532
2022-03-18 06:37:27 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.4545
2022-03-18 06:38:00 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.6303
2022-03-18 06:38:33 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.5436
2022-03-18 06:39:06 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.2939
2022-03-18 06:39:40 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.4620
2022-03-18 06:40:12 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.5542
2022-03-18 06:40:45 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.4725
2022-03-18 06:41:18 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.5669
2022-03-18 06:41:52 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.4951
2022-03-18 06:42:25 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.6462
2022-03-18 06:42:57 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.4062
2022-03-18 06:42:59 - train: epoch 057, train_loss: 1.4927
2022-03-18 06:44:14 - eval: epoch: 057, acc1: 68.482%, acc5: 88.578%, test_loss: 1.2712, per_image_load_time: 2.389ms, per_image_inference_time: 0.520ms
2022-03-18 06:44:15 - until epoch: 057, best_acc1: 68.482%
2022-03-18 06:44:15 - epoch 058 lr: 0.010000000000000002
2022-03-18 06:44:54 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.4662
2022-03-18 06:45:27 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.4237
2022-03-18 06:46:00 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.5374
2022-03-18 06:46:32 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.4985
2022-03-18 06:47:05 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.4353
2022-03-18 06:47:38 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.6983
2022-03-18 06:48:11 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.4199
2022-03-18 06:48:44 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.5237
2022-03-18 06:49:17 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.3323
2022-03-18 06:49:50 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.5820
2022-03-18 06:50:23 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.2412
2022-03-18 06:50:56 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.3504
2022-03-18 06:51:29 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.6334
2022-03-18 06:52:02 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.6093
2022-03-18 06:52:35 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.2765
2022-03-18 06:53:08 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.4247
2022-03-18 06:53:41 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.6065
2022-03-18 06:54:14 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.5255
2022-03-18 06:54:46 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.5494
2022-03-18 06:55:20 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.6594
2022-03-18 06:55:53 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.3762
2022-03-18 06:56:26 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.6244
2022-03-18 06:56:59 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.4439
2022-03-18 06:57:32 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.6059
2022-03-18 06:58:05 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.4177
2022-03-18 06:58:38 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.4237
2022-03-18 06:59:11 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.6729
2022-03-18 06:59:44 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.5734
2022-03-18 07:00:17 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.3750
2022-03-18 07:00:50 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.6393
2022-03-18 07:01:23 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.4952
2022-03-18 07:01:56 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.1812
2022-03-18 07:02:29 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.5546
2022-03-18 07:03:02 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.4603
2022-03-18 07:03:35 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.3863
2022-03-18 07:04:08 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.4821
2022-03-18 07:04:42 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.5348
2022-03-18 07:05:15 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.5235
2022-03-18 07:05:48 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.4695
2022-03-18 07:06:21 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.5272
2022-03-18 07:06:54 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.5483
2022-03-18 07:07:27 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.3759
2022-03-18 07:08:00 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.5321
2022-03-18 07:08:33 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.5446
2022-03-18 07:09:06 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.3772
2022-03-18 07:09:39 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.4244
2022-03-18 07:10:12 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.6215
2022-03-18 07:10:45 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.5734
2022-03-18 07:11:18 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.5864
2022-03-18 07:11:50 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.4294
2022-03-18 07:11:52 - train: epoch 058, train_loss: 1.4899
2022-03-18 07:13:07 - eval: epoch: 058, acc1: 68.332%, acc5: 88.648%, test_loss: 1.2758, per_image_load_time: 1.394ms, per_image_inference_time: 0.554ms
2022-03-18 07:13:08 - until epoch: 058, best_acc1: 68.482%
2022-03-18 07:13:08 - epoch 059 lr: 0.010000000000000002
2022-03-18 07:13:47 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.6029
2022-03-18 07:14:20 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.4878
2022-03-18 07:14:52 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.5507
2022-03-18 07:15:25 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.4497
2022-03-18 07:15:58 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.6167
2022-03-18 07:16:31 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.5455
2022-03-18 07:17:04 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.3822
2022-03-18 07:17:37 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.5550
2022-03-18 07:18:10 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.4452
2022-03-18 07:18:43 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.6672
2022-03-18 07:19:16 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 1.8219
2022-03-18 07:19:49 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.4824
2022-03-18 07:20:22 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.7089
2022-03-18 07:20:55 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.5970
2022-03-18 07:21:28 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.4193
2022-03-18 07:22:01 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.3983
2022-03-18 07:22:34 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.6028
2022-03-18 07:23:07 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.5495
2022-03-18 07:23:39 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.4166
2022-03-18 07:24:12 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.3897
2022-03-18 07:24:45 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.5435
2022-03-18 07:25:18 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.3882
2022-03-18 07:25:51 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.4383
2022-03-18 07:26:24 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.6172
2022-03-18 07:26:57 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.7466
2022-03-18 07:27:30 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.5864
2022-03-18 07:28:03 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.5791
2022-03-18 07:28:36 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.6366
2022-03-18 07:29:10 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.5178
2022-03-18 07:29:43 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 1.7666
2022-03-18 07:30:16 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.2272
2022-03-18 07:30:50 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.6573
2022-03-18 07:31:23 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.4248
2022-03-18 07:31:57 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 1.6480
2022-03-18 07:32:30 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.5497
2022-03-18 07:33:03 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.5157
2022-03-18 07:33:36 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.5161
2022-03-18 07:34:10 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.4151
2022-03-18 07:34:43 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.4638
2022-03-18 07:35:16 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 1.6064
2022-03-18 07:35:49 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.3769
2022-03-18 07:36:21 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.4820
2022-03-18 07:36:55 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.6354
2022-03-18 07:37:28 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.5014
2022-03-18 07:38:01 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.6017
2022-03-18 07:38:33 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.4694
2022-03-18 07:39:07 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.4653
2022-03-18 07:39:40 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.3868
2022-03-18 07:40:13 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.3976
2022-03-18 07:40:45 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.5770
2022-03-18 07:40:46 - train: epoch 059, train_loss: 1.4863
2022-03-18 07:42:02 - eval: epoch: 059, acc1: 68.170%, acc5: 88.622%, test_loss: 1.2782, per_image_load_time: 2.126ms, per_image_inference_time: 0.544ms
2022-03-18 07:42:02 - until epoch: 059, best_acc1: 68.482%
2022-03-18 07:42:02 - epoch 060 lr: 0.010000000000000002
2022-03-18 07:42:41 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.4420
2022-03-18 07:43:14 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.3278
2022-03-18 07:43:47 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.5113
2022-03-18 07:44:19 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.5528
2022-03-18 07:44:52 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.8246
2022-03-18 07:45:25 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.5007
2022-03-18 07:45:58 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.3990
2022-03-18 07:46:31 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.4297
2022-03-18 07:47:04 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.2901
2022-03-18 07:47:37 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.1635
2022-03-18 07:48:11 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.4884
2022-03-18 07:48:44 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.4571
2022-03-18 07:49:17 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.5570
2022-03-18 07:49:51 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.6844
2022-03-18 07:50:24 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.7513
2022-03-18 07:50:57 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.4181
2022-03-18 07:51:30 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.6683
2022-03-18 07:52:04 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.4572
2022-03-18 07:52:37 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.6123
2022-03-18 07:53:10 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.5651
2022-03-18 07:53:43 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.5734
2022-03-18 07:54:17 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.6343
2022-03-18 07:54:50 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.3281
2022-03-18 07:55:23 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.6072
2022-03-18 07:55:57 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.5469
2022-03-18 07:56:30 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.4240
2022-03-18 07:57:03 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.5507
2022-03-18 07:57:37 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.6689
2022-03-18 07:58:10 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.5434
2022-03-18 07:58:44 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.5167
2022-03-18 07:59:17 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.6298
2022-03-18 07:59:50 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.4193
2022-03-18 08:00:24 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.2098
2022-03-18 08:00:57 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.5380
2022-03-18 08:01:30 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.5298
2022-03-18 08:02:04 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.5105
2022-03-18 08:02:37 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.4880
2022-03-18 08:03:11 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.4825
2022-03-18 08:03:44 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.3587
2022-03-18 08:04:17 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.4240
2022-03-18 08:04:50 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.5431
2022-03-18 08:05:24 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.6914
2022-03-18 08:05:57 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.3016
2022-03-18 08:06:31 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.5559
2022-03-18 08:07:04 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.5854
2022-03-18 08:07:37 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.2523
2022-03-18 08:08:11 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.4886
2022-03-18 08:08:45 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.5113
2022-03-18 08:09:18 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.4460
2022-03-18 08:09:51 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.6434
2022-03-18 08:09:52 - train: epoch 060, train_loss: 1.4826
2022-03-18 08:11:08 - eval: epoch: 060, acc1: 68.520%, acc5: 88.590%, test_loss: 1.2710, per_image_load_time: 1.760ms, per_image_inference_time: 0.515ms
2022-03-18 08:11:09 - until epoch: 060, best_acc1: 68.520%
2022-03-18 08:11:09 - epoch 061 lr: 0.0010000000000000002
2022-03-18 08:11:49 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.2630
2022-03-18 08:12:22 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.3915
2022-03-18 08:12:55 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.1899
2022-03-18 08:13:29 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.5473
2022-03-18 08:14:03 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.4382
2022-03-18 08:14:36 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.2951
2022-03-18 08:15:10 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.2036
2022-03-18 08:15:43 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.4180
2022-03-18 08:16:16 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.3149
2022-03-18 08:16:50 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.3670
2022-03-18 08:17:23 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.0793
2022-03-18 08:17:56 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.1449
2022-03-18 08:18:29 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.3445
2022-03-18 08:19:02 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.1596
2022-03-18 08:19:35 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.3400
2022-03-18 08:20:08 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.0924
2022-03-18 08:20:41 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.2441
2022-03-18 08:21:14 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.2567
2022-03-18 08:21:47 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.4768
2022-03-18 08:22:20 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.2701
2022-03-18 08:22:52 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.6280
2022-03-18 08:23:25 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.1969
2022-03-18 08:23:58 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.3201
2022-03-18 08:24:31 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.0645
2022-03-18 08:25:04 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.2708
2022-03-18 08:25:36 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.3766
2022-03-18 08:26:09 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.6203
2022-03-18 08:26:42 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.3425
2022-03-18 08:27:15 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.4946
2022-03-18 08:27:48 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.1608
2022-03-18 08:28:21 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.4383
2022-03-18 08:28:54 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.3055
2022-03-18 08:29:27 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.2396
2022-03-18 08:30:00 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.1716
2022-03-18 08:30:33 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.4171
2022-03-18 08:31:06 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.2943
2022-03-18 08:31:40 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.2254
2022-03-18 08:32:13 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.0746
2022-03-18 08:32:46 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.4498
2022-03-18 08:33:19 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.2439
2022-03-18 08:33:52 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.5829
2022-03-18 08:34:25 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.2626
2022-03-18 08:34:58 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.2359
2022-03-18 08:35:31 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.4481
2022-03-18 08:36:04 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.2733
2022-03-18 08:36:37 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.1593
2022-03-18 08:37:10 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.0674
2022-03-18 08:37:43 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.2830
2022-03-18 08:38:16 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.4037
2022-03-18 08:38:48 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.3159
2022-03-18 08:38:50 - train: epoch 061, train_loss: 1.2920
2022-03-18 08:40:05 - eval: epoch: 061, acc1: 71.984%, acc5: 90.604%, test_loss: 1.1131, per_image_load_time: 1.256ms, per_image_inference_time: 0.546ms
2022-03-18 08:40:06 - until epoch: 061, best_acc1: 71.984%
2022-03-18 08:40:06 - epoch 062 lr: 0.0010000000000000002
2022-03-18 08:40:45 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.3404
2022-03-18 08:41:18 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.2872
2022-03-18 08:41:51 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.3676
2022-03-18 08:42:23 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.1257
2022-03-18 08:42:56 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.2931
2022-03-18 08:43:29 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.1514
2022-03-18 08:44:02 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.3338
2022-03-18 08:44:34 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.3747
2022-03-18 08:45:07 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.2615
2022-03-18 08:45:40 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.3434
2022-03-18 08:46:13 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.3038
2022-03-18 08:46:47 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.4135
2022-03-18 08:47:19 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.1961
2022-03-18 08:47:52 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.1892
2022-03-18 08:48:26 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.2943
2022-03-18 08:48:59 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.5014
2022-03-18 08:49:32 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.3111
2022-03-18 08:50:05 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 0.9823
2022-03-18 08:50:39 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.2851
2022-03-18 08:51:12 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.4032
2022-03-18 08:51:46 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.3125
2022-03-18 08:52:19 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.1566
2022-03-18 08:52:53 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.2803
2022-03-18 08:53:26 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.1720
2022-03-18 08:53:59 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.3285
2022-03-18 08:54:32 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.2799
2022-03-18 08:55:05 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.2054
2022-03-18 08:55:38 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.4471
2022-03-18 08:56:11 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.1539
2022-03-18 08:56:44 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.3678
2022-03-18 08:57:17 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.3553
2022-03-18 08:57:50 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.1062
2022-03-18 08:58:23 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.1961
2022-03-18 08:58:56 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.1973
2022-03-18 08:59:30 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.1902
2022-03-18 09:00:03 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.2733
2022-03-18 09:00:36 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.3683
2022-03-18 09:01:09 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.2494
2022-03-18 09:01:42 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.3752
2022-03-18 09:02:15 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.2317
2022-03-18 09:02:48 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.3435
2022-03-18 09:03:21 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 1.1078
2022-03-18 09:03:53 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.1960
2022-03-18 09:04:27 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.1013
2022-03-18 09:05:00 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.3660
2022-03-18 09:05:33 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 1.0853
2022-03-18 09:06:06 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.0551
2022-03-18 09:06:39 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.2940
2022-03-18 09:07:12 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.1428
2022-03-18 09:07:44 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.3664
2022-03-18 09:07:46 - train: epoch 062, train_loss: 1.2438
2022-03-18 09:09:01 - eval: epoch: 062, acc1: 72.484%, acc5: 90.736%, test_loss: 1.0948, per_image_load_time: 2.349ms, per_image_inference_time: 0.531ms
2022-03-18 09:09:02 - until epoch: 062, best_acc1: 72.484%
2022-03-18 09:09:02 - epoch 063 lr: 0.0010000000000000002
2022-03-18 09:09:41 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 1.3426
2022-03-18 09:10:14 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.4608
2022-03-18 09:10:47 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.5076
2022-03-18 09:11:19 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.1930
2022-03-18 09:11:52 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 0.9162
2022-03-18 09:12:25 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.1432
2022-03-18 09:12:58 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.4955
2022-03-18 09:13:30 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.2388
2022-03-18 09:14:03 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.3098
2022-03-18 09:14:36 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.3515
2022-03-18 09:15:09 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.3088
2022-03-18 09:15:42 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.1566
2022-03-18 09:16:15 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 0.9211
2022-03-18 09:16:48 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.3284
2022-03-18 09:17:21 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.1628
2022-03-18 09:17:54 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.1214
2022-03-18 09:18:27 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.1208
2022-03-18 09:19:00 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.2181
2022-03-18 09:19:33 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.2891
2022-03-18 09:20:06 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 0.9930
2022-03-18 09:20:39 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.2032
2022-03-18 09:21:12 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.5867
2022-03-18 09:21:45 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.4503
2022-03-18 09:22:18 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.2168
2022-03-18 09:22:52 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.2707
2022-03-18 09:23:24 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.2465
2022-03-18 09:23:57 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.3162
2022-03-18 09:24:30 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.3899
2022-03-18 09:25:03 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.2650
2022-03-18 09:25:36 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.3866
2022-03-18 09:26:09 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.3889
2022-03-18 09:26:42 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.2286
2022-03-18 09:27:15 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.2142
2022-03-18 09:27:48 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.0744
2022-03-18 09:28:21 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.3018
2022-03-18 09:28:54 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.3070
2022-03-18 09:29:27 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.2264
2022-03-18 09:30:00 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.3093
2022-03-18 09:30:33 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 1.2007
2022-03-18 09:31:06 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 0.9175
2022-03-18 09:31:39 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.3230
2022-03-18 09:32:12 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.2780
2022-03-18 09:32:45 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.2468
2022-03-18 09:33:18 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.4770
2022-03-18 09:33:52 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.2045
2022-03-18 09:34:25 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.2450
2022-03-18 09:34:58 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.2134
2022-03-18 09:35:31 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.1371
2022-03-18 09:36:04 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.2794
2022-03-18 09:36:37 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.0836
2022-03-18 09:36:38 - train: epoch 063, train_loss: 1.2248
2022-03-18 09:37:54 - eval: epoch: 063, acc1: 72.460%, acc5: 91.030%, test_loss: 1.0866, per_image_load_time: 1.176ms, per_image_inference_time: 0.497ms
2022-03-18 09:37:55 - until epoch: 063, best_acc1: 72.484%
2022-03-18 09:37:55 - epoch 064 lr: 0.0010000000000000002
2022-03-18 09:38:35 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.1150
2022-03-18 09:39:08 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.2491
2022-03-18 09:39:42 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.1398
2022-03-18 09:40:14 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.1835
2022-03-18 09:40:46 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.1704
2022-03-18 09:41:19 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.2341
2022-03-18 09:41:52 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.4009
2022-03-18 09:42:25 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 1.0925
2022-03-18 09:42:57 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.1691
2022-03-18 09:43:30 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.2152
2022-03-18 09:44:03 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.0259
2022-03-18 09:44:36 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 0.9123
2022-03-18 09:45:09 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.2729
2022-03-18 09:45:42 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.3100
2022-03-18 09:46:15 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.2577
2022-03-18 09:46:48 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.0976
2022-03-18 09:47:21 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.1166
2022-03-18 09:47:54 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.1364
2022-03-18 09:48:27 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.2718
2022-03-18 09:49:00 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.2051
2022-03-18 09:49:33 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 1.1251
2022-03-18 09:50:06 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.2447
2022-03-18 09:50:39 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.1893
2022-03-18 09:51:12 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.1321
2022-03-18 09:51:45 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 1.1851
2022-03-18 09:52:18 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.1819
2022-03-18 09:52:51 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.0749
2022-03-18 09:53:24 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.1004
2022-03-18 09:53:57 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.3838
2022-03-18 09:54:30 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.1698
2022-03-18 09:55:03 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.2863
2022-03-18 09:55:36 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.2052
2022-03-18 09:56:09 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.3935
2022-03-18 09:56:42 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.1434
2022-03-18 09:57:15 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.0772
2022-03-18 09:57:49 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.1066
2022-03-18 09:58:22 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 0.8739
2022-03-18 09:58:55 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.1530
2022-03-18 09:59:28 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.1102
2022-03-18 10:00:01 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.2203
2022-03-18 10:00:34 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.2650
2022-03-18 10:01:07 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 1.1709
2022-03-18 10:01:39 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.1478
2022-03-18 10:02:12 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.0241
2022-03-18 10:02:45 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 1.0585
2022-03-18 10:03:19 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.2863
2022-03-18 10:03:51 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.4512
2022-03-18 10:04:24 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.4097
2022-03-18 10:04:57 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.3811
2022-03-18 10:05:30 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.0951
2022-03-18 10:05:31 - train: epoch 064, train_loss: 1.2159
2022-03-18 10:06:46 - eval: epoch: 064, acc1: 72.638%, acc5: 91.036%, test_loss: 1.0818, per_image_load_time: 1.769ms, per_image_inference_time: 0.559ms
2022-03-18 10:06:46 - until epoch: 064, best_acc1: 72.638%
2022-03-18 10:06:46 - epoch 065 lr: 0.0010000000000000002
2022-03-18 10:07:25 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.3800
2022-03-18 10:07:58 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.2070
2022-03-18 10:08:31 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.1718
2022-03-18 10:09:04 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.1390
2022-03-18 10:09:37 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.2545
2022-03-18 10:10:10 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.3088
2022-03-18 10:10:43 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.3702
2022-03-18 10:11:15 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.0993
2022-03-18 10:11:49 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.2610
2022-03-18 10:12:22 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.2861
2022-03-18 10:12:55 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.3114
2022-03-18 10:13:28 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.4524
2022-03-18 10:14:01 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.2012
2022-03-18 10:14:35 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 1.1204
2022-03-18 10:15:08 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.1767
2022-03-18 10:15:41 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.1570
2022-03-18 10:16:13 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.1828
2022-03-18 10:16:47 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.2151
2022-03-18 10:17:20 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 0.8866
2022-03-18 10:17:53 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.2531
2022-03-18 10:18:26 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.2305
2022-03-18 10:18:59 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.2851
2022-03-18 10:19:32 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.0626
2022-03-18 10:20:05 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.1952
2022-03-18 10:20:38 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.3126
2022-03-18 10:21:12 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.4180
2022-03-18 10:21:45 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.2603
2022-03-18 10:22:18 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.1668
2022-03-18 10:22:51 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.1567
2022-03-18 10:23:25 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.1218
2022-03-18 10:23:58 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.0723
2022-03-18 10:24:32 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.2086
2022-03-18 10:25:05 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.1253
2022-03-18 10:25:39 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.0943
2022-03-18 10:26:12 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.3043
2022-03-18 10:26:46 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.1218
2022-03-18 10:27:19 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.1044
2022-03-18 10:27:51 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.1438
2022-03-18 10:28:25 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.2646
2022-03-18 10:28:58 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.2326
2022-03-18 10:29:30 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.2601
2022-03-18 10:30:03 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.1479
2022-03-18 10:30:37 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.1821
2022-03-18 10:31:09 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.1572
2022-03-18 10:31:42 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.2226
2022-03-18 10:32:16 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.1301
2022-03-18 10:32:49 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.0965
2022-03-18 10:33:22 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 0.9862
2022-03-18 10:33:55 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.1975
2022-03-18 10:34:27 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.2038
2022-03-18 10:34:29 - train: epoch 065, train_loss: 1.2034
2022-03-18 10:35:44 - eval: epoch: 065, acc1: 72.888%, acc5: 91.034%, test_loss: 1.0766, per_image_load_time: 1.705ms, per_image_inference_time: 0.553ms
2022-03-18 10:35:45 - until epoch: 065, best_acc1: 72.888%
2022-03-18 10:35:45 - epoch 066 lr: 0.0010000000000000002
2022-03-18 10:36:24 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.1688
2022-03-18 10:36:57 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.3158
2022-03-18 10:37:30 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 1.1060
2022-03-18 10:38:03 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 0.9890
2022-03-18 10:38:36 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.0677
2022-03-18 10:39:08 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.0572
2022-03-18 10:39:41 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.3457
2022-03-18 10:40:13 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.2342
2022-03-18 10:40:46 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.1112
2022-03-18 10:41:19 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.1343
2022-03-18 10:41:52 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.3192
2022-03-18 10:42:25 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.3876
2022-03-18 10:42:58 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.0934
2022-03-18 10:43:31 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 1.1401
2022-03-18 10:44:04 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.2059
2022-03-18 10:44:38 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.1749
2022-03-18 10:45:10 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.2784
2022-03-18 10:45:43 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 0.9677
2022-03-18 10:46:17 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.3057
2022-03-18 10:46:50 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 0.9978
2022-03-18 10:47:23 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.1559
2022-03-18 10:47:56 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 0.9955
2022-03-18 10:48:29 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.2411
2022-03-18 10:49:02 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.1897
2022-03-18 10:49:35 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.2190
2022-03-18 10:50:08 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.3003
2022-03-18 10:50:42 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.3303
2022-03-18 10:51:15 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.1432
2022-03-18 10:51:48 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.0543
2022-03-18 10:52:21 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.3140
2022-03-18 10:52:55 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.3185
2022-03-18 10:53:28 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.1705
2022-03-18 10:54:01 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.1755
2022-03-18 10:54:34 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.4165
2022-03-18 10:55:08 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.3653
2022-03-18 10:55:41 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.1558
2022-03-18 10:56:14 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.1849
2022-03-18 10:56:47 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.1817
2022-03-18 10:57:20 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.2154
2022-03-18 10:57:53 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.2417
2022-03-18 10:58:26 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 1.3001
2022-03-18 10:58:59 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 1.0115
2022-03-18 10:59:33 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.0082
2022-03-18 11:00:06 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.2545
2022-03-18 11:00:39 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.3188
2022-03-18 11:01:12 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.3309
2022-03-18 11:01:45 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.0624
2022-03-18 11:02:18 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.1374
2022-03-18 11:02:52 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.1198
2022-03-18 11:03:24 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 0.9627
2022-03-18 11:03:25 - train: epoch 066, train_loss: 1.1937
2022-03-18 11:04:40 - eval: epoch: 066, acc1: 72.900%, acc5: 91.130%, test_loss: 1.0753, per_image_load_time: 2.321ms, per_image_inference_time: 0.546ms
2022-03-18 11:04:41 - until epoch: 066, best_acc1: 72.900%
2022-03-18 11:04:41 - epoch 067 lr: 0.0010000000000000002
2022-03-18 11:05:20 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.2615
2022-03-18 11:05:53 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.2385
2022-03-18 11:06:26 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.4516
2022-03-18 11:06:59 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.1085
2022-03-18 11:07:31 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.1917
2022-03-18 11:08:04 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 1.0385
2022-03-18 11:08:37 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.2902
2022-03-18 11:09:10 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.1841
2022-03-18 11:09:43 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.2979
2022-03-18 11:10:16 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 1.0223
2022-03-18 11:10:48 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 0.9902
2022-03-18 11:11:22 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.2033
2022-03-18 11:11:55 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.3765
2022-03-18 11:12:28 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.2658
2022-03-18 11:13:01 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.1268
2022-03-18 11:13:34 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.2875
2022-03-18 11:14:07 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.0459
2022-03-18 11:14:40 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.3454
2022-03-18 11:15:13 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.2862
2022-03-18 11:15:45 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.2747
2022-03-18 11:16:18 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 0.9254
2022-03-18 11:16:51 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.1256
2022-03-18 11:17:23 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.2815
2022-03-18 11:17:56 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 0.9471
2022-03-18 11:18:28 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 1.1416
2022-03-18 11:19:01 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.1374
2022-03-18 11:19:34 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 0.9797
2022-03-18 11:20:07 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.3467
2022-03-18 11:20:40 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.1643
2022-03-18 11:21:13 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.1416
2022-03-18 11:21:46 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 1.0750
2022-03-18 11:22:19 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.0882
2022-03-18 11:22:52 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 0.9551
2022-03-18 11:23:25 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.1305
2022-03-18 11:23:58 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.2808
2022-03-18 11:24:31 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.0842
2022-03-18 11:25:04 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.3543
2022-03-18 11:25:37 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.1814
2022-03-18 11:26:10 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 1.3171
2022-03-18 11:26:44 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.1775
2022-03-18 11:27:17 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.1362
2022-03-18 11:27:51 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.0673
2022-03-18 11:28:24 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.1231
2022-03-18 11:28:58 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.0581
2022-03-18 11:29:31 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 1.0898
2022-03-18 11:30:05 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.1058
2022-03-18 11:30:38 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.2742
2022-03-18 11:31:12 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.0967
2022-03-18 11:31:46 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.1007
2022-03-18 11:32:19 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.4319
2022-03-18 11:32:20 - train: epoch 067, train_loss: 1.1891
2022-03-18 11:33:36 - eval: epoch: 067, acc1: 72.868%, acc5: 91.126%, test_loss: 1.0728, per_image_load_time: 2.206ms, per_image_inference_time: 0.543ms
2022-03-18 11:33:36 - until epoch: 067, best_acc1: 72.900%
2022-03-18 11:33:36 - epoch 068 lr: 0.0010000000000000002
2022-03-18 11:34:16 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.2772
2022-03-18 11:34:49 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.2304
2022-03-18 11:35:22 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.1720
2022-03-18 11:35:55 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.0340
2022-03-18 11:36:28 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.2513
2022-03-18 11:37:01 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.1910
2022-03-18 11:37:34 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.2639
2022-03-18 11:38:08 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.0713
2022-03-18 11:38:41 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.0712
2022-03-18 11:39:14 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.1562
2022-03-18 11:39:48 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.2604
2022-03-18 11:40:22 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 0.9508
2022-03-18 11:40:55 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.1145
2022-03-18 11:41:28 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.1102
2022-03-18 11:42:02 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.3060
2022-03-18 11:42:35 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.2358
2022-03-18 11:43:09 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.1229
2022-03-18 11:43:42 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.2861
2022-03-18 11:44:15 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 0.9642
2022-03-18 11:44:49 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.2767
2022-03-18 11:45:22 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.2933
2022-03-18 11:45:56 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.1624
2022-03-18 11:46:29 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 1.2169
2022-03-18 11:47:03 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.2076
2022-03-18 11:47:36 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.2443
2022-03-18 11:48:09 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.1271
2022-03-18 11:48:43 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.0859
2022-03-18 11:49:17 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.3559
2022-03-18 11:49:50 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.1580
2022-03-18 11:50:23 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.2155
2022-03-18 11:50:57 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.0372
2022-03-18 11:51:30 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.4400
2022-03-18 11:52:04 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.1517
2022-03-18 11:52:37 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 1.1619
2022-03-18 11:53:10 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.1895
2022-03-18 11:53:44 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.0434
2022-03-18 11:54:17 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.1359
2022-03-18 11:54:51 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.2102
2022-03-18 11:55:24 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.2633
2022-03-18 11:55:57 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.1395
2022-03-18 11:56:31 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.2195
2022-03-18 11:57:04 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.1865
2022-03-18 11:57:38 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 0.9769
2022-03-18 11:58:11 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.2750
2022-03-18 11:58:44 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.2304
2022-03-18 11:59:18 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.1018
2022-03-18 11:59:51 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.1708
2022-03-18 12:00:24 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.2297
2022-03-18 12:00:57 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.0475
2022-03-18 12:01:30 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.1215
2022-03-18 12:01:31 - train: epoch 068, train_loss: 1.1835
2022-03-18 12:02:47 - eval: epoch: 068, acc1: 72.972%, acc5: 91.198%, test_loss: 1.0665, per_image_load_time: 1.429ms, per_image_inference_time: 0.533ms
2022-03-18 12:02:48 - until epoch: 068, best_acc1: 72.972%
2022-03-18 12:02:48 - epoch 069 lr: 0.0010000000000000002
2022-03-18 12:03:27 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.3249
2022-03-18 12:04:00 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.2355
2022-03-18 12:04:33 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.1955
2022-03-18 12:05:06 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.1451
2022-03-18 12:05:39 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.1997
2022-03-18 12:06:12 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 1.1050
2022-03-18 12:06:45 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.1730
2022-03-18 12:07:18 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.2322
2022-03-18 12:07:51 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.0954
2022-03-18 12:08:24 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.2980
2022-03-18 12:08:57 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.1179
2022-03-18 12:09:30 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.1498
2022-03-18 12:10:03 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.2371
2022-03-18 12:10:36 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.1963
2022-03-18 12:11:09 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.1139
2022-03-18 12:11:42 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.2299
2022-03-18 12:12:15 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.1641
2022-03-18 12:12:47 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.1709
2022-03-18 12:13:20 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.2889
2022-03-18 12:13:53 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 1.0163
2022-03-18 12:14:26 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.2069
2022-03-18 12:14:59 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.3096
2022-03-18 12:15:32 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 0.9857
2022-03-18 12:16:05 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.2358
2022-03-18 12:16:38 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.0467
2022-03-18 12:17:11 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 0.9630
2022-03-18 12:17:44 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.2845
2022-03-18 12:18:17 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.2119
2022-03-18 12:18:50 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 1.1761
2022-03-18 12:19:23 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.1925
2022-03-18 12:19:57 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 1.3193
2022-03-18 12:20:30 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 0.9526
2022-03-18 12:21:03 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.0727
2022-03-18 12:21:37 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 1.3283
2022-03-18 12:22:10 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 1.2104
2022-03-18 12:22:43 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 0.9589
2022-03-18 12:23:16 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.3414
2022-03-18 12:23:49 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.2008
2022-03-18 12:24:23 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.1401
2022-03-18 12:24:55 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.2283
2022-03-18 12:25:28 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.2701
2022-03-18 12:26:01 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.0934
2022-03-18 12:26:34 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.2160
2022-03-18 12:27:07 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.1525
2022-03-18 12:27:40 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.3126
2022-03-18 12:28:13 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.1480
2022-03-18 12:28:46 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.1591
2022-03-18 12:29:19 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.3550
2022-03-18 12:29:53 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 1.0932
2022-03-18 12:30:25 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.1155
2022-03-18 12:30:26 - train: epoch 069, train_loss: 1.1784
2022-03-18 12:31:42 - eval: epoch: 069, acc1: 73.008%, acc5: 91.212%, test_loss: 1.0661, per_image_load_time: 2.400ms, per_image_inference_time: 0.528ms
2022-03-18 12:31:43 - until epoch: 069, best_acc1: 73.008%
2022-03-18 12:31:43 - epoch 070 lr: 0.0010000000000000002
2022-03-18 12:32:22 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.2454
2022-03-18 12:32:55 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.1092
2022-03-18 12:33:29 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.2212
2022-03-18 12:34:02 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.0936
2022-03-18 12:34:36 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.2195
2022-03-18 12:35:09 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.2912
2022-03-18 12:35:42 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.3724
2022-03-18 12:36:15 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.1462
2022-03-18 12:36:47 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.1142
2022-03-18 12:37:20 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.1697
2022-03-18 12:37:53 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.3665
2022-03-18 12:38:26 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 1.0616
2022-03-18 12:38:59 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.2504
2022-03-18 12:39:32 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.1070
2022-03-18 12:40:05 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 1.1689
2022-03-18 12:40:38 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.2306
2022-03-18 12:41:11 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.1802
2022-03-18 12:41:44 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.0697
2022-03-18 12:42:17 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.0836
2022-03-18 12:42:50 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.2625
2022-03-18 12:43:23 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 1.1834
2022-03-18 12:43:56 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.2297
2022-03-18 12:44:30 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.3453
2022-03-18 12:45:03 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.2480
2022-03-18 12:45:36 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.0969
2022-03-18 12:46:10 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.2854
2022-03-18 12:46:43 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.2522
2022-03-18 12:47:17 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.3684
2022-03-18 12:47:50 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.1984
2022-03-18 12:48:23 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.1096
2022-03-18 12:48:57 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.1561
2022-03-18 12:49:30 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.2452
2022-03-18 12:50:03 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 1.1919
2022-03-18 12:50:37 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.0834
2022-03-18 12:51:10 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.1492
2022-03-18 12:51:43 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.2339
2022-03-18 12:52:16 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.1456
2022-03-18 12:52:49 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.0783
2022-03-18 12:53:22 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 0.9644
2022-03-18 12:53:55 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.2670
2022-03-18 12:54:29 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 1.1917
2022-03-18 12:55:02 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.1875
2022-03-18 12:55:35 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.1710
2022-03-18 12:56:09 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.1607
2022-03-18 12:56:42 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.2118
2022-03-18 12:57:15 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.2750
2022-03-18 12:57:49 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.2057
2022-03-18 12:58:22 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.0212
2022-03-18 12:58:55 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.0570
2022-03-18 12:59:28 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.0648
2022-03-18 12:59:29 - train: epoch 070, train_loss: 1.1750
2022-03-18 13:00:44 - eval: epoch: 070, acc1: 72.994%, acc5: 91.144%, test_loss: 1.0660, per_image_load_time: 2.037ms, per_image_inference_time: 0.539ms
2022-03-18 13:00:45 - until epoch: 070, best_acc1: 73.008%
2022-03-18 13:00:45 - epoch 071 lr: 0.0010000000000000002
2022-03-18 13:01:24 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.0066
2022-03-18 13:01:57 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.2827
2022-03-18 13:02:31 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.4099
2022-03-18 13:03:04 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.1381
2022-03-18 13:03:37 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.2390
2022-03-18 13:04:09 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.2356
2022-03-18 13:04:42 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.1083
2022-03-18 13:05:14 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.1427
2022-03-18 13:05:47 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.3151
2022-03-18 13:06:20 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.2333
2022-03-18 13:06:53 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.4174
2022-03-18 13:07:25 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.1094
2022-03-18 13:07:58 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.0295
2022-03-18 13:08:31 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.0067
2022-03-18 13:09:04 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 1.0092
2022-03-18 13:09:37 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 1.1364
2022-03-18 13:10:10 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.1822
2022-03-18 13:10:42 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.2316
2022-03-18 13:11:15 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 0.9987
2022-03-18 13:11:48 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.2263
2022-03-18 13:12:21 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.0162
2022-03-18 13:12:54 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 0.9364
2022-03-18 13:13:26 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 1.0144
2022-03-18 13:13:59 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.0918
2022-03-18 13:14:32 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.2501
2022-03-18 13:15:05 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.1109
2022-03-18 13:15:38 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.0664
2022-03-18 13:16:11 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.1679
2022-03-18 13:16:45 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.0320
2022-03-18 13:17:18 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.2872
2022-03-18 13:17:51 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 0.9736
2022-03-18 13:18:24 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.1050
2022-03-18 13:18:57 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.2742
2022-03-18 13:19:30 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.1262
2022-03-18 13:20:03 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.1764
2022-03-18 13:20:36 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.3485
2022-03-18 13:21:10 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.0844
2022-03-18 13:21:43 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.1370
2022-03-18 13:22:17 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.1829
2022-03-18 13:22:50 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.1452
2022-03-18 13:23:24 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.1270
2022-03-18 13:23:57 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.2495
2022-03-18 13:24:30 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.1224
2022-03-18 13:25:04 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.1063
2022-03-18 13:25:37 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.0732
2022-03-18 13:26:10 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.1827
2022-03-18 13:26:43 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.1005
2022-03-18 13:27:15 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 1.2453
2022-03-18 13:27:49 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 1.0788
2022-03-18 13:28:21 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.0227
2022-03-18 13:28:23 - train: epoch 071, train_loss: 1.1675
2022-03-18 13:29:37 - eval: epoch: 071, acc1: 73.076%, acc5: 91.194%, test_loss: 1.0633, per_image_load_time: 2.264ms, per_image_inference_time: 0.541ms
2022-03-18 13:29:38 - until epoch: 071, best_acc1: 73.076%
2022-03-18 13:29:38 - epoch 072 lr: 0.0010000000000000002
2022-03-18 13:30:17 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.2007
2022-03-18 13:30:50 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.1680
2022-03-18 13:31:22 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.0213
2022-03-18 13:31:55 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.2140
2022-03-18 13:32:28 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.2085
2022-03-18 13:33:01 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.2099
2022-03-18 13:33:34 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.1160
2022-03-18 13:34:07 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.3726
2022-03-18 13:34:40 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.0848
2022-03-18 13:35:13 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.0830
2022-03-18 13:35:46 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 1.2844
2022-03-18 13:36:19 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 1.2441
2022-03-18 13:36:51 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.1311
2022-03-18 13:37:24 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.1361
2022-03-18 13:37:56 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.1438
2022-03-18 13:38:29 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.0274
2022-03-18 13:39:02 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 1.3690
2022-03-18 13:39:35 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.0843
2022-03-18 13:40:07 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.4027
2022-03-18 13:40:40 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.1479
2022-03-18 13:41:13 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.2868
2022-03-18 13:41:46 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.3530
2022-03-18 13:42:18 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.1769
2022-03-18 13:42:51 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 1.2600
2022-03-18 13:43:24 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.1771
2022-03-18 13:43:56 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 1.0797
2022-03-18 13:44:29 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 1.1950
2022-03-18 13:45:02 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.2041
2022-03-18 13:45:35 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.0262
2022-03-18 13:46:08 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.1495
2022-03-18 13:46:41 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.1386
2022-03-18 13:47:14 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.1400
2022-03-18 13:47:47 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.1296
2022-03-18 13:48:20 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 1.1619
2022-03-18 13:48:53 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 1.1643
2022-03-18 13:49:26 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.1260
2022-03-18 13:49:59 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.3463
2022-03-18 13:50:32 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.2881
2022-03-18 13:51:05 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.1422
2022-03-18 13:51:38 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.2175
2022-03-18 13:52:11 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.3314
2022-03-18 13:52:44 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.0060
2022-03-18 13:53:17 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.1924
2022-03-18 13:53:50 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 1.0055
2022-03-18 13:54:23 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.2042
2022-03-18 13:54:56 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.1041
2022-03-18 13:55:29 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.1859
2022-03-18 13:56:02 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.4709
2022-03-18 13:56:35 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.2583
2022-03-18 13:57:07 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.1953
2022-03-18 13:57:09 - train: epoch 072, train_loss: 1.1646
2022-03-18 13:58:23 - eval: epoch: 072, acc1: 73.088%, acc5: 91.208%, test_loss: 1.0642, per_image_load_time: 1.958ms, per_image_inference_time: 0.541ms
2022-03-18 13:58:24 - until epoch: 072, best_acc1: 73.088%
2022-03-18 13:58:24 - epoch 073 lr: 0.0010000000000000002
2022-03-18 13:59:03 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.1796
2022-03-18 13:59:35 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.1109
2022-03-18 14:00:08 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.1653
2022-03-18 14:00:41 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 0.9075
2022-03-18 14:01:14 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.1445
2022-03-18 14:01:47 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.0084
2022-03-18 14:02:20 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.1050
2022-03-18 14:02:53 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.1114
2022-03-18 14:03:26 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.0100
2022-03-18 14:03:58 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 1.0995
2022-03-18 14:04:31 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.1032
2022-03-18 14:05:04 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.1962
2022-03-18 14:05:37 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.4239
2022-03-18 14:06:10 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.1453
2022-03-18 14:06:43 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.2408
2022-03-18 14:07:16 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.2606
2022-03-18 14:07:49 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.3802
2022-03-18 14:08:22 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 0.9946
2022-03-18 14:08:56 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.2455
2022-03-18 14:09:29 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.0850
2022-03-18 14:10:02 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.2296
2022-03-18 14:10:36 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.3181
2022-03-18 14:11:09 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.1592
2022-03-18 14:11:43 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.4727
2022-03-18 14:12:16 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.0917
2022-03-18 14:12:49 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 1.0402
2022-03-18 14:13:22 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.0340
2022-03-18 14:13:55 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.2278
2022-03-18 14:14:29 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.2617
2022-03-18 14:15:02 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 1.0354
2022-03-18 14:15:35 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.2772
2022-03-18 14:16:08 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 1.0914
2022-03-18 14:16:41 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 1.1392
2022-03-18 14:17:15 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 1.1911
2022-03-18 14:17:48 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.2173
2022-03-18 14:18:21 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 0.9824
2022-03-18 14:18:54 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.0607
2022-03-18 14:19:27 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.3992
2022-03-18 14:20:01 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.2224
2022-03-18 14:20:34 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.2904
2022-03-18 14:21:07 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 1.0193
2022-03-18 14:21:40 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.2096
2022-03-18 14:22:13 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.0468
2022-03-18 14:22:46 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.1020
2022-03-18 14:23:19 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.0811
2022-03-18 14:23:53 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.3303
2022-03-18 14:24:26 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.0288
2022-03-18 14:24:59 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.1563
2022-03-18 14:25:32 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.1218
2022-03-18 14:26:05 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.3016
2022-03-18 14:26:06 - train: epoch 073, train_loss: 1.1630
2022-03-18 14:27:21 - eval: epoch: 073, acc1: 73.108%, acc5: 91.304%, test_loss: 1.0622, per_image_load_time: 2.316ms, per_image_inference_time: 0.560ms
2022-03-18 14:27:22 - until epoch: 073, best_acc1: 73.108%
2022-03-18 14:27:22 - epoch 074 lr: 0.0010000000000000002
2022-03-18 14:28:01 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.2562
2022-03-18 14:28:34 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 1.2055
2022-03-18 14:29:06 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.1079
2022-03-18 14:29:39 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.3162
2022-03-18 14:30:11 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.0286
2022-03-18 14:30:44 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.1621
2022-03-18 14:31:17 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 0.9698
2022-03-18 14:31:49 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.1772
2022-03-18 14:32:22 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.0452
2022-03-18 14:32:55 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.2514
2022-03-18 14:33:28 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.2514
2022-03-18 14:34:01 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 1.0479
2022-03-18 14:34:34 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.3728
2022-03-18 14:35:07 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 1.0554
2022-03-18 14:35:40 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.1511
2022-03-18 14:36:12 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 1.2060
2022-03-18 14:36:45 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.0812
2022-03-18 14:37:18 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.3035
2022-03-18 14:37:51 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.1908
2022-03-18 14:38:23 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 0.8900
2022-03-18 14:38:57 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.2941
2022-03-18 14:39:29 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.3136
2022-03-18 14:40:02 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.2361
2022-03-18 14:40:35 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.0740
2022-03-18 14:41:08 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.1021
2022-03-18 14:41:41 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.0343
2022-03-18 14:42:14 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 1.0623
2022-03-18 14:42:47 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.3453
2022-03-18 14:43:20 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.1488
2022-03-18 14:43:52 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.4166
2022-03-18 14:44:25 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.2153
2022-03-18 14:44:58 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 0.9886
2022-03-18 14:45:31 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.3767
2022-03-18 14:46:04 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.0781
2022-03-18 14:46:37 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.0815
2022-03-18 14:47:10 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 1.3573
2022-03-18 14:47:43 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.2262
2022-03-18 14:48:16 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.0024
2022-03-18 14:48:49 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 0.9971
2022-03-18 14:49:22 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 1.3660
2022-03-18 14:49:55 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.1558
2022-03-18 14:50:28 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.1129
2022-03-18 14:51:01 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.1039
2022-03-18 14:51:34 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 1.1201
2022-03-18 14:52:07 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 0.9990
2022-03-18 14:52:40 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.1875
2022-03-18 14:53:13 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.1941
2022-03-18 14:53:46 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.2869
2022-03-18 14:54:19 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.1912
2022-03-18 14:54:51 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.2812
2022-03-18 14:54:52 - train: epoch 074, train_loss: 1.1610
2022-03-18 14:56:07 - eval: epoch: 074, acc1: 73.120%, acc5: 91.334%, test_loss: 1.0600, per_image_load_time: 2.348ms, per_image_inference_time: 0.537ms
2022-03-18 14:56:08 - until epoch: 074, best_acc1: 73.120%
2022-03-18 14:56:08 - epoch 075 lr: 0.0010000000000000002
2022-03-18 14:56:47 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 1.2430
2022-03-18 14:57:20 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 1.1586
2022-03-18 14:57:53 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.0357
2022-03-18 14:58:26 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.1211
2022-03-18 14:58:59 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.2913
2022-03-18 14:59:32 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.3135
2022-03-18 15:00:05 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.2231
2022-03-18 15:00:38 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.0926
2022-03-18 15:01:11 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.1737
2022-03-18 15:01:44 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 0.9542
2022-03-18 15:02:17 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.2503
2022-03-18 15:02:50 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.0609
2022-03-18 15:03:23 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.1059
2022-03-18 15:03:56 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.1509
2022-03-18 15:04:29 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.1000
2022-03-18 15:05:01 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 0.9634
2022-03-18 15:05:34 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 1.4524
2022-03-18 15:06:07 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.1857
2022-03-18 15:06:39 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 1.2420
2022-03-18 15:07:12 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.1862
2022-03-18 15:07:44 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.1403
2022-03-18 15:08:17 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.2170
2022-03-18 15:08:49 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 1.1914
2022-03-18 15:09:21 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.2323
2022-03-18 15:09:53 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.1576
2022-03-18 15:10:26 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.1037
2022-03-18 15:10:58 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 0.9742
2022-03-18 15:11:30 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.0770
2022-03-18 15:12:02 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.2969
2022-03-18 15:12:34 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.3072
2022-03-18 15:13:06 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.2006
2022-03-18 15:13:38 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.1021
2022-03-18 15:14:10 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.2542
2022-03-18 15:14:42 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 1.0823
2022-03-18 15:15:14 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.1793
2022-03-18 15:15:46 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.1823
2022-03-18 15:16:18 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.2485
2022-03-18 15:16:50 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 0.9395
2022-03-18 15:17:22 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.1006
2022-03-18 15:17:54 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.0267
2022-03-18 15:18:27 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 1.0313
2022-03-18 15:18:59 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.1911
2022-03-18 15:19:30 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.4063
2022-03-18 15:20:02 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 1.0955
2022-03-18 15:20:34 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.1009
2022-03-18 15:21:06 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 0.9175
2022-03-18 15:21:38 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.2595
2022-03-18 15:22:10 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.1765
2022-03-18 15:22:42 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.2462
2022-03-18 15:23:13 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 0.9891
2022-03-18 15:23:14 - train: epoch 075, train_loss: 1.1569
2022-03-18 15:24:27 - eval: epoch: 075, acc1: 73.170%, acc5: 91.324%, test_loss: 1.0579, per_image_load_time: 2.277ms, per_image_inference_time: 0.519ms
2022-03-18 15:24:28 - until epoch: 075, best_acc1: 73.170%
2022-03-18 15:24:28 - epoch 076 lr: 0.0010000000000000002
2022-03-18 15:25:06 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 1.1069
2022-03-18 15:25:37 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.1736
2022-03-18 15:26:09 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.2289
2022-03-18 15:26:41 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.1987
2022-03-18 15:27:14 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.2418
2022-03-18 15:27:46 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.1528
2022-03-18 15:28:18 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.1597
2022-03-18 15:28:50 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.0396
2022-03-18 15:29:23 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 1.1036
2022-03-18 15:29:55 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.0244
2022-03-18 15:30:27 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.0832
2022-03-18 15:30:59 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.1367
2022-03-18 15:31:31 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.2890
2022-03-18 15:32:04 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 1.1618
2022-03-18 15:32:36 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.3168
2022-03-18 15:33:08 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.2068
2022-03-18 15:33:40 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.0061
2022-03-18 15:34:13 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 1.0487
2022-03-18 15:34:45 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.0505
2022-03-18 15:35:17 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 1.2539
2022-03-18 15:35:48 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.1010
2022-03-18 15:36:20 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.2108
2022-03-18 15:36:52 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 1.0181
2022-03-18 15:37:24 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.2437
2022-03-18 15:37:56 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 1.0844
2022-03-18 15:38:28 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.3643
2022-03-18 15:39:00 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.0661
2022-03-18 15:39:32 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 1.2390
2022-03-18 15:40:04 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 1.0554
2022-03-18 15:40:36 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 1.0846
2022-03-18 15:41:07 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 1.2839
2022-03-18 15:41:39 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.1779
2022-03-18 15:42:11 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.1639
2022-03-18 15:42:43 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.1158
2022-03-18 15:43:16 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 1.0801
2022-03-18 15:43:48 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 0.9980
2022-03-18 15:44:20 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 0.9418
2022-03-18 15:44:52 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 0.9381
2022-03-18 15:45:24 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 1.0325
2022-03-18 15:45:56 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 1.1629
2022-03-18 15:46:28 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 1.1217
2022-03-18 15:46:59 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.0976
2022-03-18 15:47:31 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 1.1484
2022-03-18 15:48:03 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 1.3271
2022-03-18 15:48:35 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 1.2426
2022-03-18 15:49:06 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 1.0149
2022-03-18 15:49:38 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.3118
2022-03-18 15:50:09 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 0.8870
2022-03-18 15:50:41 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 1.2625
2022-03-18 15:51:12 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.4540
2022-03-18 15:51:14 - train: epoch 076, train_loss: 1.1537
2022-03-18 15:52:26 - eval: epoch: 076, acc1: 73.220%, acc5: 91.316%, test_loss: 1.0584, per_image_load_time: 2.297ms, per_image_inference_time: 0.517ms
2022-03-18 15:52:27 - until epoch: 076, best_acc1: 73.220%
2022-03-18 15:52:27 - epoch 077 lr: 0.0010000000000000002
2022-03-18 15:53:05 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 1.3184
2022-03-18 15:53:37 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 1.2313
2022-03-18 15:54:08 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 1.2438
2022-03-18 15:54:40 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.0001
2022-03-18 15:55:12 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 1.1389
2022-03-18 15:55:44 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 1.1793
2022-03-18 15:56:16 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 1.1986
2022-03-18 15:56:48 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.1428
2022-03-18 15:57:20 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.2054
2022-03-18 15:57:51 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 1.0333
2022-03-18 15:58:23 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 1.2220
2022-03-18 15:58:55 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 1.3950
2022-03-18 15:59:28 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 0.8999
2022-03-18 16:00:00 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.1014
2022-03-18 16:00:32 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 1.1937
2022-03-18 16:01:04 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 1.1880
2022-03-18 16:01:37 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.2836
2022-03-18 16:02:09 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 0.9870
2022-03-18 16:02:40 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 0.9850
2022-03-18 16:03:13 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 1.1109
2022-03-18 16:03:45 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 1.2294
2022-03-18 16:04:17 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 1.0341
2022-03-18 16:04:49 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.1484
2022-03-18 16:05:21 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 1.2268
2022-03-18 16:05:53 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.1754
2022-03-18 16:06:25 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 0.9012
2022-03-18 16:06:57 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 1.3210
2022-03-18 16:07:29 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.2997
2022-03-18 16:08:01 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.2368
2022-03-18 16:08:33 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 1.1832
2022-03-18 16:09:05 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 1.0376
2022-03-18 16:09:37 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.2857
2022-03-18 16:10:09 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 1.3018
2022-03-18 16:10:42 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.1257
2022-03-18 16:11:14 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.0524
2022-03-18 16:11:47 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 1.0673
2022-03-18 16:12:19 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 1.1310
2022-03-18 16:12:52 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 0.9802
2022-03-18 16:13:25 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 1.1418
2022-03-18 16:13:57 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 1.2387
2022-03-18 16:14:29 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 1.1230
2022-03-18 16:15:00 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.1231
2022-03-18 16:15:32 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 1.2044
2022-03-18 16:16:04 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 1.0776
2022-03-18 16:16:35 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.4134
2022-03-18 16:17:07 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 0.8694
2022-03-18 16:17:39 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 1.1923
2022-03-18 16:18:11 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 1.1315
2022-03-18 16:18:42 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.0556
2022-03-18 16:19:14 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.1079
2022-03-18 16:19:15 - train: epoch 077, train_loss: 1.1499
2022-03-18 16:20:29 - eval: epoch: 077, acc1: 73.234%, acc5: 91.328%, test_loss: 1.0553, per_image_load_time: 2.370ms, per_image_inference_time: 0.508ms
2022-03-18 16:20:30 - until epoch: 077, best_acc1: 73.234%
2022-03-18 16:20:30 - epoch 078 lr: 0.0010000000000000002
2022-03-18 16:21:09 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 1.1195
2022-03-18 16:21:41 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.3622
2022-03-18 16:22:13 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.2383
2022-03-18 16:22:44 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 1.0933
2022-03-18 16:23:16 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.2604
2022-03-18 16:23:48 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 1.0759
2022-03-18 16:24:19 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.2966
2022-03-18 16:24:51 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.2706
2022-03-18 16:25:22 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 1.0773
2022-03-18 16:25:54 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 1.1464
2022-03-18 16:26:26 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 1.0959
2022-03-18 16:26:58 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 1.1710
2022-03-18 16:27:31 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 1.0061
2022-03-18 16:28:03 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 1.3101
2022-03-18 16:28:36 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 1.1093
2022-03-18 16:29:08 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 1.1560
2022-03-18 16:29:40 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 1.2809
2022-03-18 16:30:12 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 1.0935
2022-03-18 16:30:44 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 1.0618
2022-03-18 16:31:16 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 1.0633
2022-03-18 16:31:48 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.1747
2022-03-18 16:32:20 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 1.1561
2022-03-18 16:32:52 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 1.3365
2022-03-18 16:33:24 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 1.2559
2022-03-18 16:33:56 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.1287
2022-03-18 16:34:28 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 1.1681
2022-03-18 16:35:01 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 1.1694
2022-03-18 16:35:33 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 1.2624
2022-03-18 16:36:05 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 1.0212
2022-03-18 16:36:37 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 1.3105
2022-03-18 16:37:08 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.1267
2022-03-18 16:37:40 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 1.2392
2022-03-18 16:38:12 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.1228
2022-03-18 16:38:44 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 1.3473
2022-03-18 16:39:16 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 1.2432
2022-03-18 16:39:48 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.2045
2022-03-18 16:40:20 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 1.1429
2022-03-18 16:40:52 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 1.0682
2022-03-18 16:41:24 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 1.1512
2022-03-18 16:41:56 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 1.0258
2022-03-18 16:42:28 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.1840
2022-03-18 16:43:00 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.1803
2022-03-18 16:43:32 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 1.2806
2022-03-18 16:44:04 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 1.0785
2022-03-18 16:44:36 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.1802
2022-03-18 16:45:08 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 1.0293
2022-03-18 16:45:40 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 1.2884
2022-03-18 16:46:12 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 1.2900
2022-03-18 16:46:44 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 1.1198
2022-03-18 16:47:15 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 1.0923
2022-03-18 16:47:16 - train: epoch 078, train_loss: 1.1475
2022-03-18 16:48:30 - eval: epoch: 078, acc1: 73.332%, acc5: 91.328%, test_loss: 1.0580, per_image_load_time: 2.355ms, per_image_inference_time: 0.523ms
2022-03-18 16:48:31 - until epoch: 078, best_acc1: 73.332%
2022-03-18 16:48:31 - epoch 079 lr: 0.0010000000000000002
2022-03-18 16:49:08 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 1.1098
2022-03-18 16:49:41 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 0.9579
2022-03-18 16:50:13 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 1.0681
2022-03-18 16:50:45 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 1.0689
2022-03-18 16:51:18 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 1.1005
2022-03-18 16:51:51 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 1.3203
2022-03-18 16:52:23 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 1.0712
2022-03-18 16:52:55 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 1.2582
2022-03-18 16:53:27 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 1.1130
2022-03-18 16:54:00 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 1.3051
2022-03-18 16:54:32 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 1.3237
2022-03-18 16:55:04 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.1977
2022-03-18 16:55:36 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 0.9217
2022-03-18 16:56:09 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 1.1806
2022-03-18 16:56:41 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 1.2125
2022-03-18 16:57:13 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 0.9442
2022-03-18 16:57:46 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 1.1144
2022-03-18 16:58:19 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 1.1205
2022-03-18 16:58:52 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.1089
2022-03-18 16:59:24 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.3774
2022-03-18 16:59:57 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 1.2212
2022-03-18 17:00:29 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 1.1469
2022-03-18 17:01:01 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 1.1590
2022-03-18 17:01:33 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.1704
2022-03-18 17:02:05 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 1.2393
2022-03-18 17:02:37 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 1.1404
2022-03-18 17:03:10 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 0.9199
2022-03-18 17:03:42 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 1.1140
2022-03-18 17:04:14 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 1.2377
2022-03-18 17:04:47 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.1673
2022-03-18 17:05:19 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 1.0851
2022-03-18 17:05:52 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.1055
2022-03-18 17:06:24 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 1.1398
2022-03-18 17:06:56 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 1.0656
2022-03-18 17:07:29 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.0061
2022-03-18 17:08:01 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 0.9000
2022-03-18 17:08:33 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 1.1558
2022-03-18 17:09:05 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 1.1721
2022-03-18 17:09:37 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 1.1510
2022-03-18 17:10:09 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 1.1912
2022-03-18 17:10:41 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.1896
2022-03-18 17:11:13 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.0651
2022-03-18 17:11:46 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 1.1103
2022-03-18 17:12:17 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.3133
2022-03-18 17:12:50 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.2004
2022-03-18 17:13:22 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.2586
2022-03-18 17:13:54 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 0.9816
2022-03-18 17:14:26 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.3721
2022-03-18 17:14:58 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.1396
2022-03-18 17:15:29 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 0.9266
2022-03-18 17:15:31 - train: epoch 079, train_loss: 1.1445
2022-03-18 17:16:44 - eval: epoch: 079, acc1: 73.262%, acc5: 91.352%, test_loss: 1.0553, per_image_load_time: 2.344ms, per_image_inference_time: 0.496ms
2022-03-18 17:16:44 - until epoch: 079, best_acc1: 73.332%
2022-03-18 17:16:44 - epoch 080 lr: 0.0010000000000000002
2022-03-18 17:17:23 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 1.0335
2022-03-18 17:17:55 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 1.1025
2022-03-18 17:18:27 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.0859
2022-03-18 17:18:59 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 1.0008
2022-03-18 17:19:32 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 1.0649
2022-03-18 17:20:04 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 1.2587
2022-03-18 17:20:36 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 1.1727
2022-03-18 17:21:08 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 0.9847
2022-03-18 17:21:40 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 1.1639
2022-03-18 17:22:12 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 0.9946
2022-03-18 17:22:44 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 1.2018
2022-03-18 17:23:16 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 0.9865
2022-03-18 17:23:48 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 0.9236
2022-03-18 17:24:20 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 1.1339
2022-03-18 17:24:52 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 1.1961
2022-03-18 17:25:24 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 1.2939
2022-03-18 17:25:56 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 1.0820
2022-03-18 17:26:28 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 1.3012
2022-03-18 17:27:00 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 1.1607
2022-03-18 17:27:32 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 1.1720
2022-03-18 17:28:04 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 1.2628
2022-03-18 17:28:35 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 1.1274
2022-03-18 17:29:07 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 1.1524
2022-03-18 17:29:39 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 1.3428
2022-03-18 17:30:12 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 1.0152
2022-03-18 17:30:44 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 1.1208
2022-03-18 17:31:15 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 1.3217
2022-03-18 17:31:47 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 1.2603
2022-03-18 17:32:18 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 1.1868
2022-03-18 17:32:50 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 1.2647
2022-03-18 17:33:22 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 1.0068
2022-03-18 17:33:54 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 1.0418
2022-03-18 17:34:25 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 1.0431
2022-03-18 17:34:57 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 1.0619
2022-03-18 17:35:30 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 0.9799
2022-03-18 17:36:01 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 1.0974
2022-03-18 17:36:33 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 1.1631
2022-03-18 17:37:04 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 1.1758
2022-03-18 17:37:36 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 1.1018
2022-03-18 17:38:07 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 1.2363
2022-03-18 17:38:39 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 1.3958
2022-03-18 17:39:10 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 1.1659
2022-03-18 17:39:41 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 1.3198
2022-03-18 17:40:13 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 1.0703
2022-03-18 17:40:44 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 1.1469
2022-03-18 17:41:16 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 1.2063
2022-03-18 17:41:47 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 1.2273
2022-03-18 17:42:18 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 1.1817
2022-03-18 17:42:50 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.1994
2022-03-18 17:43:21 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 1.2018
2022-03-18 17:43:23 - train: epoch 080, train_loss: 1.1413
2022-03-18 17:44:36 - eval: epoch: 080, acc1: 73.442%, acc5: 91.334%, test_loss: 1.0536, per_image_load_time: 2.302ms, per_image_inference_time: 0.487ms
2022-03-18 17:44:37 - until epoch: 080, best_acc1: 73.442%
2022-03-18 17:44:37 - epoch 081 lr: 0.0010000000000000002
2022-03-18 17:45:14 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 1.0608
2022-03-18 17:45:45 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 0.9946
2022-03-18 17:46:17 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 1.1874
2022-03-18 17:46:48 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 1.1954
2022-03-18 17:47:20 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 1.2321
2022-03-18 17:47:52 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 1.1946
2022-03-18 17:48:23 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 1.1103
2022-03-18 17:48:55 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 1.1985
2022-03-18 17:49:26 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 1.1479
2022-03-18 17:49:58 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 1.2071
2022-03-18 17:50:29 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 1.0714
2022-03-18 17:51:01 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 1.2346
2022-03-18 17:51:33 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 1.2527
2022-03-18 17:52:05 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 0.8554
2022-03-18 17:52:37 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 1.2953
2022-03-18 17:53:09 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 1.0290
2022-03-18 17:53:41 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 1.1609
2022-03-18 17:54:13 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 1.2586
2022-03-18 17:54:44 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 1.1343
2022-03-18 17:55:15 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 1.2096
2022-03-18 17:55:47 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 1.1835
2022-03-18 17:56:18 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 1.0709
2022-03-18 17:56:50 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 0.9693
2022-03-18 17:57:22 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 1.0663
2022-03-18 17:57:53 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 1.0746
2022-03-18 17:58:25 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.1298
2022-03-18 17:58:57 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 1.1975
2022-03-18 17:59:29 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 1.1608
2022-03-18 18:00:01 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 1.0609
2022-03-18 18:00:33 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 1.1259
2022-03-18 18:01:04 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 0.9594
2022-03-18 18:01:35 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 1.1587
2022-03-18 18:02:06 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 1.1187
2022-03-18 18:02:38 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 1.1818
2022-03-18 18:03:09 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 1.2958
2022-03-18 18:03:40 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 1.0320
2022-03-18 18:04:12 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 1.2375
2022-03-18 18:04:43 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 1.1167
2022-03-18 18:05:15 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 1.3139
2022-03-18 18:05:46 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 0.9200
2022-03-18 18:06:17 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 1.0695
2022-03-18 18:06:48 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 1.2583
2022-03-18 18:07:20 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 1.2361
2022-03-18 18:07:51 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 1.1077
2022-03-18 18:08:22 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 1.1721
2022-03-18 18:08:53 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 1.2972
2022-03-18 18:09:25 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 1.2684
2022-03-18 18:09:56 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 1.3816
2022-03-18 18:10:27 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 1.0952
2022-03-18 18:10:58 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 1.0351
2022-03-18 18:11:00 - train: epoch 081, train_loss: 1.1419
2022-03-18 18:12:12 - eval: epoch: 081, acc1: 73.234%, acc5: 91.296%, test_loss: 1.0569, per_image_load_time: 1.869ms, per_image_inference_time: 0.546ms
2022-03-18 18:12:12 - until epoch: 081, best_acc1: 73.442%
2022-03-18 18:12:12 - epoch 082 lr: 0.0010000000000000002
2022-03-18 18:12:50 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 1.0028
2022-03-18 18:13:21 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 1.1035
2022-03-18 18:13:52 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 1.2771
2022-03-18 18:14:24 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 1.1779
2022-03-18 18:14:55 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 1.2606
2022-03-18 18:15:26 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 1.0561
2022-03-18 18:15:58 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 1.2905
2022-03-18 18:16:30 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 1.1599
2022-03-18 18:17:03 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 1.1111
2022-03-18 18:17:36 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 1.0263
2022-03-18 18:18:09 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 1.1624
2022-03-18 18:18:42 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 1.2012
2022-03-18 18:19:15 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 1.2360
2022-03-18 18:19:48 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 1.0988
2022-03-18 18:20:21 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 1.1214
2022-03-18 18:20:54 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 1.2260
2022-03-18 18:21:27 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 1.0000
2022-03-18 18:22:00 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 1.0963
2022-03-18 18:22:33 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 1.0983
2022-03-18 18:23:06 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 1.0043
2022-03-18 18:23:39 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 1.2644
2022-03-18 18:24:12 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 1.1550
2022-03-18 18:24:46 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 1.0020
2022-03-18 18:25:19 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 1.0544
2022-03-18 18:25:52 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 0.9848
2022-03-18 18:26:26 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 1.0372
2022-03-18 18:26:59 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 1.0774
2022-03-18 18:27:32 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 0.8552
2022-03-18 18:28:05 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 1.0175
2022-03-18 18:28:39 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 1.1536
2022-03-18 18:29:12 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 1.1532
2022-03-18 18:29:45 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 1.2013
2022-03-18 18:30:18 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 1.1175
2022-03-18 18:30:52 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 1.0361
2022-03-18 18:31:25 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 0.9524
2022-03-18 18:31:58 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 1.2679
2022-03-18 18:32:32 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 1.0666
2022-03-18 18:33:05 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 1.1456
2022-03-18 18:33:38 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 1.1261
2022-03-18 18:34:11 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 1.1370
2022-03-18 18:34:45 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 1.2555
2022-03-18 18:35:18 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 1.2662
2022-03-18 18:35:51 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 1.0558
2022-03-18 18:36:25 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 0.9395
2022-03-18 18:36:58 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 1.1156
2022-03-18 18:37:31 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 1.1586
2022-03-18 18:38:05 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 1.0172
2022-03-18 18:38:38 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 1.0168
2022-03-18 18:39:11 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 1.1392
2022-03-18 18:39:44 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 1.0156
2022-03-18 18:39:45 - train: epoch 082, train_loss: 1.1388
2022-03-18 18:41:01 - eval: epoch: 082, acc1: 73.350%, acc5: 91.314%, test_loss: 1.0553, per_image_load_time: 2.341ms, per_image_inference_time: 0.587ms
2022-03-18 18:41:02 - until epoch: 082, best_acc1: 73.442%
2022-03-18 18:41:02 - epoch 083 lr: 0.0010000000000000002
2022-03-18 18:41:41 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 0.9507
2022-03-18 18:42:14 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 1.1004
2022-03-18 18:42:47 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 1.0427
2022-03-18 18:43:19 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 1.2389
2022-03-18 18:43:51 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 0.9430
2022-03-18 18:44:23 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 1.1986
2022-03-18 18:44:55 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 1.2141
2022-03-18 18:45:27 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 1.0580
2022-03-18 18:45:58 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 1.2536
2022-03-18 18:46:30 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 1.1570
2022-03-18 18:47:03 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 1.1623
2022-03-18 18:47:37 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 1.0860
2022-03-18 18:48:10 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 1.0919
2022-03-18 18:48:41 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 1.2323
2022-03-18 18:49:13 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 1.1485
2022-03-18 18:49:45 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 1.1235
2022-03-18 18:50:16 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 1.2740
2022-03-18 18:50:48 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 1.2001
2022-03-18 18:51:19 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 1.1029
2022-03-18 18:51:51 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 0.9919
2022-03-18 18:52:23 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 1.1451
2022-03-18 18:52:54 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 0.9886
2022-03-18 18:53:26 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 1.0752
2022-03-18 18:53:58 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 1.0627
2022-03-18 18:54:30 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 1.1019
2022-03-18 18:55:01 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 1.1243
2022-03-18 18:55:32 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 0.8941
2022-03-18 18:56:04 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 1.1099
2022-03-18 18:56:35 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 1.2083
2022-03-18 18:57:07 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 1.1989
2022-03-18 18:57:38 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 1.0626
2022-03-18 18:58:10 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 0.9882
2022-03-18 18:58:42 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 1.2947
2022-03-18 18:59:14 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 1.0775
2022-03-18 18:59:45 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 1.1589
2022-03-18 19:00:17 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 1.0025
2022-03-18 19:00:48 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 0.9834
2022-03-18 19:01:20 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 1.0578
2022-03-18 19:01:52 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 1.2162
2022-03-18 19:02:24 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 1.4177
2022-03-18 19:02:56 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 0.9714
2022-03-18 19:03:28 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 1.3220
2022-03-18 19:03:59 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 1.1553
2022-03-18 19:04:31 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 1.0793
2022-03-18 19:05:02 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 1.0592
2022-03-18 19:05:34 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 1.2701
2022-03-18 19:06:06 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.3917
2022-03-18 19:06:38 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 1.3091
2022-03-18 19:07:10 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 1.0767
2022-03-18 19:07:41 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 1.1510
2022-03-18 19:07:42 - train: epoch 083, train_loss: 1.1369
2022-03-18 19:08:55 - eval: epoch: 083, acc1: 73.362%, acc5: 91.360%, test_loss: 1.0541, per_image_load_time: 1.920ms, per_image_inference_time: 0.513ms
2022-03-18 19:08:56 - until epoch: 083, best_acc1: 73.442%
2022-03-18 19:08:56 - epoch 084 lr: 0.0010000000000000002
2022-03-18 19:09:33 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 0.9519
2022-03-18 19:10:05 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 1.3369
2022-03-18 19:10:36 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 0.8944
2022-03-18 19:11:07 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 1.1574
2022-03-18 19:11:39 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 1.3082
2022-03-18 19:12:10 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 1.5038
2022-03-18 19:12:41 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 1.2145
2022-03-18 19:13:12 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 1.1817
2022-03-18 19:13:44 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 1.0910
2022-03-18 19:14:15 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 1.3796
2022-03-18 19:14:46 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 1.1580
2022-03-18 19:15:18 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 1.2862
2022-03-18 19:15:50 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 1.3672
2022-03-18 19:16:22 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 1.0574
2022-03-18 19:16:53 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 1.2248
2022-03-18 19:17:25 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 1.0129
2022-03-18 19:17:57 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 1.2782
2022-03-18 19:18:29 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 1.2687
2022-03-18 19:19:01 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 1.1349
2022-03-18 19:19:32 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 1.1321
2022-03-18 19:20:04 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 1.1243
2022-03-18 19:20:35 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 1.1411
2022-03-18 19:21:06 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 1.0000
2022-03-18 19:21:38 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 0.9821
2022-03-18 19:22:09 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 1.1195
2022-03-18 19:22:41 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 1.0295
2022-03-18 19:23:12 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 1.1691
2022-03-18 19:23:44 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 1.1424
2022-03-18 19:24:15 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 1.2060
2022-03-18 19:24:47 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 1.1507
2022-03-18 19:25:19 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 1.1555
2022-03-18 19:25:51 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 1.0363
2022-03-18 19:26:22 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 1.3660
2022-03-18 19:26:54 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 0.9831
2022-03-18 19:27:26 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 1.0520
2022-03-18 19:27:58 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 1.1144
2022-03-18 19:28:29 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 1.2764
2022-03-18 19:29:01 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 1.1602
2022-03-18 19:29:33 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 1.2500
2022-03-18 19:30:05 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 1.0477
2022-03-18 19:30:37 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 1.0432
2022-03-18 19:31:09 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 1.1461
2022-03-18 19:31:41 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 1.2175
2022-03-18 19:32:13 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.4130
2022-03-18 19:32:44 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 1.2004
2022-03-18 19:33:16 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 1.0918
2022-03-18 19:33:48 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 0.9937
2022-03-18 19:34:19 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 1.0958
2022-03-18 19:34:51 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 1.0001
2022-03-18 19:35:22 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 1.0610
2022-03-18 19:35:23 - train: epoch 084, train_loss: 1.1348
2022-03-18 19:36:36 - eval: epoch: 084, acc1: 73.408%, acc5: 91.356%, test_loss: 1.0535, per_image_load_time: 2.284ms, per_image_inference_time: 0.487ms
2022-03-18 19:36:37 - until epoch: 084, best_acc1: 73.442%
2022-03-18 19:36:37 - epoch 085 lr: 0.0010000000000000002
2022-03-18 19:37:15 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 1.1503
2022-03-18 19:37:46 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 1.0688
2022-03-18 19:38:17 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 1.1720
2022-03-18 19:38:49 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 1.1416
2022-03-18 19:39:21 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 1.3180
2022-03-18 19:39:52 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 0.9416
2022-03-18 19:40:24 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 1.1377
2022-03-18 19:40:56 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 0.9823
2022-03-18 19:41:27 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 1.0031
2022-03-18 19:41:59 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 1.1247
2022-03-18 19:42:31 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 0.9155
2022-03-18 19:43:03 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 1.2097
2022-03-18 19:43:35 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 0.9813
2022-03-18 19:44:07 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 1.1728
2022-03-18 19:44:38 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 1.1665
2022-03-18 19:45:10 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 1.1701
2022-03-18 19:45:41 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 1.3835
2022-03-18 19:46:12 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 0.8352
2022-03-18 19:46:44 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 1.0443
2022-03-18 19:47:16 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 0.9345
2022-03-18 19:47:47 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 1.1062
2022-03-18 19:48:19 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 1.1701
2022-03-18 19:48:51 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 1.0420
2022-03-18 19:49:22 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 1.2211
2022-03-18 19:49:54 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 1.3486
2022-03-18 19:50:26 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 1.0761
2022-03-18 19:50:58 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 1.1358
2022-03-18 19:51:29 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 1.2709
2022-03-18 19:52:01 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 1.2200
2022-03-18 19:52:33 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 1.3903
2022-03-18 19:53:05 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 1.1853
2022-03-18 19:53:36 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 1.1608
2022-03-18 19:54:08 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 1.0843
2022-03-18 19:54:40 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 1.3104
2022-03-18 19:55:12 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 1.2753
2022-03-18 19:55:44 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 1.1409
2022-03-18 19:56:16 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 1.0236
2022-03-18 19:56:48 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 1.2786
2022-03-18 19:57:19 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 1.2017
2022-03-18 19:57:51 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 1.2713
2022-03-18 19:58:23 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 1.0716
2022-03-18 19:58:54 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 1.1463
2022-03-18 19:59:26 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 0.9084
2022-03-18 19:59:57 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 1.0577
2022-03-18 20:00:29 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 1.2222
2022-03-18 20:01:01 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 1.1765
2022-03-18 20:01:33 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.5409
2022-03-18 20:02:04 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 1.0442
2022-03-18 20:02:36 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 1.3264
2022-03-18 20:03:07 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 0.9534
2022-03-18 20:03:08 - train: epoch 085, train_loss: 1.1309
2022-03-18 20:04:21 - eval: epoch: 085, acc1: 73.286%, acc5: 91.350%, test_loss: 1.0553, per_image_load_time: 1.562ms, per_image_inference_time: 0.516ms
2022-03-18 20:04:22 - until epoch: 085, best_acc1: 73.442%
2022-03-18 20:04:22 - epoch 086 lr: 0.0010000000000000002
2022-03-18 20:05:00 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 1.0447
2022-03-18 20:05:31 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 0.9644
2022-03-18 20:06:03 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 1.1632
2022-03-18 20:06:35 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 1.2679
2022-03-18 20:07:07 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 1.1194
2022-03-18 20:07:38 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 0.9821
2022-03-18 20:08:10 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 1.1002
2022-03-18 20:08:42 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 1.2749
2022-03-18 20:09:14 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 1.0447
2022-03-18 20:09:46 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 1.2931
2022-03-18 20:10:17 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 1.0555
2022-03-18 20:10:49 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 1.2178
2022-03-18 20:11:21 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 1.0967
2022-03-18 20:11:53 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 1.0659
2022-03-18 20:12:24 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 0.8956
2022-03-18 20:12:56 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 1.0867
2022-03-18 20:13:28 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 1.1862
2022-03-18 20:14:00 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 1.2653
2022-03-18 20:14:32 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 1.1416
2022-03-18 20:15:04 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 1.1553
2022-03-18 20:15:35 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 0.9549
2022-03-18 20:16:07 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 1.1149
2022-03-18 20:16:39 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 1.0399
2022-03-18 20:17:11 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 1.0995
2022-03-18 20:17:43 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 1.1067
2022-03-18 20:18:14 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 1.2576
2022-03-18 20:18:46 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 1.1206
2022-03-18 20:19:18 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 1.2774
2022-03-18 20:19:50 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 0.9370
2022-03-18 20:20:21 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 1.0581
2022-03-18 20:20:53 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 1.0894
2022-03-18 20:21:25 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 1.2116
2022-03-18 20:21:57 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 1.1477
2022-03-18 20:22:29 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 1.2659
2022-03-18 20:23:01 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 1.1430
2022-03-18 20:23:32 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 1.1925
2022-03-18 20:24:04 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 1.0044
2022-03-18 20:24:36 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 1.1963
2022-03-18 20:25:08 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 1.0652
2022-03-18 20:25:40 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 1.1369
2022-03-18 20:26:12 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 1.0949
2022-03-18 20:26:44 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 1.2233
2022-03-18 20:27:16 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 1.0925
2022-03-18 20:27:48 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 1.2260
2022-03-18 20:28:19 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 1.1358
2022-03-18 20:28:51 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 1.1204
2022-03-18 20:29:23 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 1.1240
2022-03-18 20:29:54 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 1.0832
2022-03-18 20:30:26 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 1.0768
2022-03-18 20:30:57 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 1.1780
2022-03-18 20:30:59 - train: epoch 086, train_loss: 1.1284
2022-03-18 20:32:12 - eval: epoch: 086, acc1: 73.242%, acc5: 91.340%, test_loss: 1.0555, per_image_load_time: 1.388ms, per_image_inference_time: 0.521ms
2022-03-18 20:32:13 - until epoch: 086, best_acc1: 73.442%
2022-03-18 20:32:13 - epoch 087 lr: 0.0010000000000000002
2022-03-18 20:32:50 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 0.9952
2022-03-18 20:33:22 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 1.0465
2022-03-18 20:33:53 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 1.0413
2022-03-18 20:34:24 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 1.2210
2022-03-18 20:34:55 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 1.0234
2022-03-18 20:35:27 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 1.3107
2022-03-18 20:35:58 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 1.0767
2022-03-18 20:36:30 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 1.0591
2022-03-18 20:37:01 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 1.0192
2022-03-18 20:37:33 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 0.8593
2022-03-18 20:38:04 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 1.0948
2022-03-18 20:38:36 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 1.2549
2022-03-18 20:39:08 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 1.2436
2022-03-18 20:39:40 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 1.0132
2022-03-18 20:40:12 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 1.2107
2022-03-18 20:40:44 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 0.9823
2022-03-18 20:41:16 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 1.2159
2022-03-18 20:41:47 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 1.3094
2022-03-18 20:42:19 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 1.3374
2022-03-18 20:42:51 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 1.1210
2022-03-18 20:43:23 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 1.1069
2022-03-18 20:43:55 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 1.1910
2022-03-18 20:44:27 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 1.4179
2022-03-18 20:44:58 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 1.1309
2022-03-18 20:45:30 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 1.2747
2022-03-18 20:46:02 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 1.1714
2022-03-18 20:46:34 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 1.2330
2022-03-18 20:47:05 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 1.2092
2022-03-18 20:47:37 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 1.0986
2022-03-18 20:48:09 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 1.2168
2022-03-18 20:48:40 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 1.1057
2022-03-18 20:49:12 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 1.0773
2022-03-18 20:49:44 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 1.2048
2022-03-18 20:50:16 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 1.1689
2022-03-18 20:50:47 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 1.1393
2022-03-18 20:51:19 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 1.0107
2022-03-18 20:51:51 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 1.0498
2022-03-18 20:52:22 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 1.0990
2022-03-18 20:52:54 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 1.0573
2022-03-18 20:53:26 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 1.1376
2022-03-18 20:53:58 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 1.1780
2022-03-18 20:54:30 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 1.1981
2022-03-18 20:55:01 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 1.1496
2022-03-18 20:55:33 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 1.1755
2022-03-18 20:56:04 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 1.1763
2022-03-18 20:56:36 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 1.3367
2022-03-18 20:57:08 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 1.0815
2022-03-18 20:57:39 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 1.1286
2022-03-18 20:58:11 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 1.0678
2022-03-18 20:58:42 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 1.0998
2022-03-18 20:58:44 - train: epoch 087, train_loss: 1.1276
2022-03-18 20:59:56 - eval: epoch: 087, acc1: 73.448%, acc5: 91.352%, test_loss: 1.0520, per_image_load_time: 1.407ms, per_image_inference_time: 0.522ms
2022-03-18 20:59:57 - until epoch: 087, best_acc1: 73.448%
2022-03-18 20:59:57 - epoch 088 lr: 0.0010000000000000002
2022-03-18 21:00:35 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 1.0580
2022-03-18 21:01:06 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 1.2739
2022-03-18 21:01:37 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 1.2774
2022-03-18 21:02:08 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 1.0377
2022-03-18 21:02:39 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 1.1160
2022-03-18 21:03:10 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 1.2055
2022-03-18 21:03:42 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 1.1732
2022-03-18 21:04:13 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 1.0562
2022-03-18 21:04:44 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 1.1851
2022-03-18 21:05:15 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 1.2912
2022-03-18 21:05:46 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 1.1397
2022-03-18 21:06:18 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 1.2487
2022-03-18 21:06:49 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 1.2851
2022-03-18 21:07:20 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 0.9850
2022-03-18 21:07:51 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 1.1726
2022-03-18 21:08:23 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 1.0133
2022-03-18 21:08:54 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 1.1857
2022-03-18 21:09:25 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 1.0379
2022-03-18 21:09:57 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 1.0795
2022-03-18 21:10:28 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 1.0532
2022-03-18 21:10:59 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 1.3060
2022-03-18 21:11:31 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 0.9947
2022-03-18 21:12:02 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 1.0065
2022-03-18 21:12:33 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 1.3635
2022-03-18 21:13:05 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 1.2431
2022-03-18 21:13:36 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 1.2672
2022-03-18 21:14:08 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 1.1839
2022-03-18 21:14:39 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 1.2896
2022-03-18 21:15:11 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 1.0373
2022-03-18 21:15:42 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 1.2750
2022-03-18 21:16:13 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 0.8989
2022-03-18 21:16:45 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 0.9720
2022-03-18 21:17:16 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 1.0336
2022-03-18 21:17:48 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 1.0755
2022-03-18 21:18:19 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 1.1170
2022-03-18 21:18:51 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 1.1446
2022-03-18 21:19:23 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 1.1971
2022-03-18 21:19:55 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 1.1566
2022-03-18 21:20:26 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 1.1156
2022-03-18 21:20:58 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 0.9512
2022-03-18 21:21:30 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 1.1432
2022-03-18 21:22:02 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 1.0902
2022-03-18 21:22:33 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 1.1531
2022-03-18 21:23:05 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 1.1218
2022-03-18 21:23:37 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 1.0848
2022-03-18 21:24:09 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 1.3365
2022-03-18 21:24:40 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 1.1911
2022-03-18 21:25:12 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 1.1359
2022-03-18 21:25:44 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 0.9861
2022-03-18 21:26:15 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 1.1981
2022-03-18 21:26:17 - train: epoch 088, train_loss: 1.1251
2022-03-18 21:27:30 - eval: epoch: 088, acc1: 73.534%, acc5: 91.380%, test_loss: 1.0522, per_image_load_time: 2.246ms, per_image_inference_time: 0.531ms
2022-03-18 21:27:31 - until epoch: 088, best_acc1: 73.534%
2022-03-18 21:27:31 - epoch 089 lr: 0.0010000000000000002
2022-03-18 21:28:09 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 1.3023
2022-03-18 21:28:41 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 0.9993
2022-03-18 21:29:13 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 1.2312
2022-03-18 21:29:45 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 1.2180
2022-03-18 21:30:17 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 1.2658
2022-03-18 21:30:49 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 1.1775
2022-03-18 21:31:21 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 1.2547
2022-03-18 21:31:53 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 1.2084
2022-03-18 21:32:25 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 1.1748
2022-03-18 21:32:58 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 1.0801
2022-03-18 21:33:30 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 1.0873
2022-03-18 21:34:02 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 1.2385
2022-03-18 21:34:34 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 0.9750
2022-03-18 21:35:06 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 1.4013
2022-03-18 21:35:39 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 1.0234
2022-03-18 21:36:11 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 1.0378
2022-03-18 21:36:43 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 1.2272
2022-03-18 21:37:16 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 1.3372
2022-03-18 21:37:48 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 1.0156
2022-03-18 21:38:20 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 1.0466
2022-03-18 21:38:52 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 1.2205
