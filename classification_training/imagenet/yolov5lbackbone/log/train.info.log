2022-03-04 02:14:46 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 1.1145
2022-03-04 02:14:50 - train: epoch 079, train_loss: 1.2074
2022-03-04 02:16:07 - eval: epoch: 079, acc1: 72.270%, acc5: 90.870%, test_loss: 1.0940, per_image_load_time: 1.618ms, per_image_inference_time: 0.948ms
2022-03-04 02:16:07 - until epoch: 079, best_acc1: 72.474%
2022-03-04 02:16:07 - epoch 080 lr: 0.0010000000000000002
2022-03-04 02:16:54 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 1.0125
2022-03-04 02:17:35 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 1.1040
2022-03-04 02:18:17 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.2910
2022-03-04 02:18:58 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 1.0431
2022-03-04 02:19:40 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 1.2974
2022-03-04 02:20:22 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 1.0972
2022-03-04 02:21:03 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 1.1098
2022-03-04 02:21:45 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 1.0939
2022-03-04 02:22:26 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 1.2130
2022-03-04 02:23:08 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 1.1397
2022-03-04 02:23:49 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 1.1622
2022-03-04 02:24:31 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 1.1526
2022-03-04 02:25:13 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 1.1010
2022-03-04 02:25:54 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 1.2435
2022-03-04 02:26:35 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 1.0607
2022-03-04 02:27:17 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 1.1204
2022-03-04 02:27:58 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 1.2286
2022-03-04 02:28:40 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 1.3455
2022-03-04 02:29:22 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 1.1598
2022-03-04 02:30:03 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 1.2415
2022-03-04 02:30:45 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 1.3156
2022-03-04 02:31:26 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 1.1940
2022-03-04 02:32:08 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 1.1268
2022-03-04 02:32:49 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 1.2878
2022-03-04 02:33:31 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 1.0927
2022-03-04 02:34:13 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 1.1447
2022-03-04 02:34:54 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 1.3384
2022-03-04 02:35:36 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 1.2389
2022-03-04 02:36:18 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 1.0974
2022-03-04 02:37:00 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 1.1770
2022-03-04 02:37:42 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 1.3262
2022-03-04 02:38:24 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 0.9170
2022-03-04 02:39:06 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 1.1736
2022-03-04 02:39:48 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 1.0738
2022-03-04 02:40:30 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 1.0623
2022-03-04 02:41:12 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 1.2235
2022-03-04 02:41:54 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 1.1480
2022-03-04 02:42:36 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 1.2655
2022-03-04 02:43:18 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 1.1252
2022-03-04 02:44:00 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 1.2466
2022-03-04 02:44:42 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 1.2772
2022-03-04 02:45:24 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 1.1780
2022-03-04 02:46:06 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 1.3406
2022-03-04 02:46:48 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 1.1830
2022-03-04 02:47:30 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 1.1527
2022-03-04 02:48:12 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 1.2619
2022-03-04 02:48:54 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 1.2848
2022-03-04 02:49:36 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 1.2356
2022-03-04 02:50:18 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.3653
2022-03-04 02:51:00 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 1.0163
2022-03-04 02:51:03 - train: epoch 080, train_loss: 1.2056
2022-03-04 02:52:20 - eval: epoch: 080, acc1: 72.428%, acc5: 90.892%, test_loss: 1.0951, per_image_load_time: 1.529ms, per_image_inference_time: 0.981ms
2022-03-04 02:52:20 - until epoch: 080, best_acc1: 72.474%
2022-03-04 02:52:20 - epoch 081 lr: 0.0010000000000000002
2022-03-04 02:53:07 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 1.1084
2022-03-04 02:53:49 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 1.3021
2022-03-04 02:54:31 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 1.1251
2022-03-04 02:55:13 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 1.3120
2022-03-04 02:55:55 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 1.3567
2022-03-04 02:56:37 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 1.2359
2022-03-04 02:57:19 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 1.1117
2022-03-04 02:58:01 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 1.4452
2022-03-04 02:58:43 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 1.0792
2022-03-04 02:59:24 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 1.1775
2022-03-04 03:00:06 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 1.1668
2022-03-04 03:00:48 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 1.2336
2022-03-04 03:01:30 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 1.0729
2022-03-04 03:02:12 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 0.9468
2022-03-04 03:02:54 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 1.2167
2022-03-04 03:03:36 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 1.0804
2022-03-04 03:04:19 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 1.3269
2022-03-04 03:05:01 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 1.0877
2022-03-04 03:05:43 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 1.1310
2022-03-04 03:06:25 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 1.3556
2022-03-04 03:07:07 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 1.1030
2022-03-04 03:07:49 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 1.2656
2022-03-04 03:08:31 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 1.1557
2022-03-04 03:09:13 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 1.1115
2022-03-04 03:09:56 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 1.4173
2022-03-04 03:10:38 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.3873
2022-03-04 03:11:20 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 1.1154
2022-03-04 03:12:02 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 1.1584
2022-03-04 03:12:45 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 1.0826
2022-03-04 03:13:27 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 1.2258
2022-03-04 03:14:09 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 1.0416
2022-03-04 03:14:51 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 1.2306
2022-03-04 03:15:34 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 1.2287
2022-03-04 03:16:16 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 1.2914
2022-03-04 03:16:58 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 1.1832
2022-03-04 03:17:40 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 1.0700
2022-03-04 03:18:22 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 1.2268
2022-03-04 03:19:04 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 1.0341
2022-03-04 03:19:46 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 1.2248
2022-03-04 03:20:28 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 1.0473
2022-03-04 03:21:11 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 1.3085
2022-03-04 03:21:53 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 1.2159
2022-03-04 03:22:35 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 1.1891
2022-03-04 03:23:17 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 1.2935
2022-03-04 03:23:59 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 1.2579
2022-03-04 03:24:41 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 1.2750
2022-03-04 03:25:23 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 1.2910
2022-03-04 03:26:05 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 1.2249
2022-03-04 03:26:47 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 1.1358
2022-03-04 03:27:30 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 1.0205
2022-03-04 03:27:33 - train: epoch 081, train_loss: 1.2033
2022-03-04 03:28:50 - eval: epoch: 081, acc1: 72.488%, acc5: 90.844%, test_loss: 1.0920, per_image_load_time: 1.955ms, per_image_inference_time: 0.959ms
2022-03-04 03:28:50 - until epoch: 081, best_acc1: 72.488%
2022-03-04 03:28:50 - epoch 082 lr: 0.0010000000000000002
2022-03-04 03:29:37 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 0.9875
2022-03-04 03:30:19 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 1.0953
2022-03-04 03:31:00 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 1.3042
2022-03-04 03:31:42 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 1.3120
2022-03-04 03:32:23 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 1.2772
2022-03-04 03:33:05 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 1.0972
2022-03-04 03:33:47 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 1.2804
2022-03-04 03:34:28 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 1.2576
2022-03-04 03:35:10 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 1.3449
2022-03-04 03:35:52 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 1.3039
2022-03-04 03:36:33 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 1.3480
2022-03-04 03:37:15 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 1.2519
2022-03-04 03:37:57 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 1.3426
2022-03-04 03:38:38 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 1.0958
2022-03-04 03:39:20 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 1.1437
2022-03-04 03:40:02 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 1.2003
2022-03-04 03:40:44 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 1.1411
2022-03-04 03:41:26 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 1.0563
2022-03-04 03:42:08 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 1.0971
2022-03-04 03:42:50 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 1.0894
2022-03-04 03:43:31 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 1.2334
2022-03-04 03:44:13 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 1.2726
2022-03-04 03:44:55 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 1.1991
2022-03-04 03:45:37 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 1.4038
2022-03-04 03:46:19 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 1.1536
2022-03-04 03:47:01 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 1.1764
2022-03-04 03:47:43 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 1.1304
2022-03-04 03:48:25 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 1.1046
2022-03-04 03:49:07 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 1.0997
2022-03-04 03:49:49 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 1.1764
2022-03-04 03:50:30 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 1.1858
2022-03-04 03:51:12 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 1.3555
2022-03-04 03:51:54 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 1.2001
2022-03-04 03:52:37 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 1.1497
2022-03-04 03:53:19 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 1.1085
2022-03-04 03:54:01 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 1.1497
2022-03-04 03:54:43 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 1.1226
2022-03-04 03:55:25 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 1.4243
2022-03-04 03:56:07 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 1.1934
2022-03-04 03:56:49 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 1.2506
2022-03-04 03:57:31 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 1.1544
2022-03-04 03:58:13 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 1.3207
2022-03-04 03:58:54 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 1.0349
2022-03-04 03:59:36 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 1.1542
2022-03-04 04:00:18 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 1.2313
2022-03-04 04:01:00 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 1.3277
2022-03-04 04:01:42 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 1.0547
2022-03-04 04:02:24 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 1.1647
2022-03-04 04:03:05 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 1.1753
2022-03-04 04:03:47 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 1.1864
2022-03-04 04:03:51 - train: epoch 082, train_loss: 1.2009
2022-03-04 04:05:07 - eval: epoch: 082, acc1: 72.582%, acc5: 90.924%, test_loss: 1.0920, per_image_load_time: 1.875ms, per_image_inference_time: 0.967ms
2022-03-04 04:05:07 - until epoch: 082, best_acc1: 72.582%
2022-03-04 04:05:07 - epoch 083 lr: 0.0010000000000000002
2022-03-04 04:05:55 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 1.2070
2022-03-04 04:06:36 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 1.1284
2022-03-04 04:07:18 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 1.2335
2022-03-04 04:08:00 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 1.3774
2022-03-04 04:08:41 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 1.1962
2022-03-04 04:09:23 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 1.1657
2022-03-04 04:10:05 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 1.1498
2022-03-04 04:10:46 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 1.1044
2022-03-04 04:11:28 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 1.2759
2022-03-04 04:12:10 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 1.4081
2022-03-04 04:12:52 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 1.1867
2022-03-04 04:13:33 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 1.1518
2022-03-04 04:14:15 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 1.1171
2022-03-04 04:14:57 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 1.2234
2022-03-04 04:15:38 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 1.1298
2022-03-04 04:16:20 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 1.2003
2022-03-04 04:17:02 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 1.3744
2022-03-04 04:17:44 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 1.4577
2022-03-04 04:18:25 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 1.0308
2022-03-04 04:19:07 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 0.8879
2022-03-04 04:19:49 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 1.1468
2022-03-04 04:20:31 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 1.1406
2022-03-04 04:21:13 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 1.2817
2022-03-04 04:21:55 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 1.1888
2022-03-04 04:22:37 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 1.2421
2022-03-04 04:23:19 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 1.0467
2022-03-04 04:24:00 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 1.2011
2022-03-04 04:24:42 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 1.1207
2022-03-04 04:25:24 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 1.0987
2022-03-04 04:26:06 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 1.3290
2022-03-04 04:26:48 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 1.0275
2022-03-04 04:27:30 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 1.0449
2022-03-04 04:28:12 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 1.4402
2022-03-04 04:28:54 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 1.2699
2022-03-04 04:29:36 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 1.3692
2022-03-04 04:30:18 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 1.2032
2022-03-04 04:31:00 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 1.2445
2022-03-04 04:31:42 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 1.0972
2022-03-04 04:32:24 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 1.1849
2022-03-04 04:33:06 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 1.3269
2022-03-04 04:33:48 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 1.1765
2022-03-04 04:34:30 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 1.4644
2022-03-04 04:35:12 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 1.2145
2022-03-04 04:35:54 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 1.0361
2022-03-04 04:36:35 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 1.2478
2022-03-04 04:37:17 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 1.3995
2022-03-04 04:38:00 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.3104
2022-03-04 04:38:42 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 1.3275
2022-03-04 04:39:24 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 1.4847
2022-03-04 04:40:06 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 1.3132
2022-03-04 04:40:09 - train: epoch 083, train_loss: 1.1985
2022-03-04 04:41:26 - eval: epoch: 083, acc1: 72.632%, acc5: 90.940%, test_loss: 1.0922, per_image_load_time: 1.835ms, per_image_inference_time: 0.960ms
2022-03-04 04:41:26 - until epoch: 083, best_acc1: 72.632%
2022-03-04 04:41:26 - epoch 084 lr: 0.0010000000000000002
2022-03-04 04:42:13 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 1.2359
2022-03-04 04:42:55 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 1.2118
2022-03-04 04:43:36 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 0.9803
2022-03-04 04:44:18 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 1.2217
2022-03-04 04:45:00 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 1.2144
2022-03-04 04:45:41 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 1.3867
2022-03-04 04:46:23 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 1.2892
2022-03-04 04:47:04 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 1.3472
2022-03-04 04:47:46 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 1.1871
2022-03-04 04:48:28 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 1.2962
2022-03-04 04:49:09 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 1.1524
2022-03-04 04:49:51 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 1.4365
2022-03-04 04:50:33 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 1.1687
2022-03-04 04:51:14 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 1.2350
2022-03-04 04:51:56 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 1.3055
2022-03-04 04:52:37 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 1.1967
2022-03-04 04:53:19 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 1.2979
2022-03-04 04:54:01 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 1.2093
2022-03-04 04:54:42 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 1.2696
2022-03-04 04:55:24 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 1.3283
2022-03-04 04:56:05 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 1.1062
2022-03-04 04:56:47 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 0.9274
2022-03-04 04:57:29 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 1.1126
2022-03-04 04:58:10 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 1.0600
2022-03-04 04:58:52 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 1.3422
2022-03-04 04:59:34 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 1.2984
2022-03-04 05:00:15 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 1.2342
2022-03-04 05:00:57 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 1.2919
2022-03-04 05:01:38 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 1.1287
2022-03-04 05:02:20 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 1.2068
2022-03-04 05:03:02 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 1.0674
2022-03-04 05:03:43 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 1.0943
2022-03-04 05:04:25 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 1.2058
2022-03-04 05:05:07 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 1.0890
2022-03-04 05:05:48 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 1.0159
2022-03-04 05:06:30 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 1.2604
2022-03-04 05:07:12 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 1.2212
2022-03-04 05:07:54 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 1.4898
2022-03-04 05:08:36 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 1.2649
2022-03-04 05:09:18 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 1.2416
2022-03-04 05:09:59 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 1.2063
2022-03-04 05:10:41 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 1.2788
2022-03-04 05:11:23 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 1.2770
2022-03-04 05:12:05 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.4527
2022-03-04 05:12:46 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 1.1287
2022-03-04 05:13:28 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 1.1116
2022-03-04 05:14:10 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 1.4630
2022-03-04 05:14:52 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 1.0480
2022-03-04 05:15:34 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 1.0372
2022-03-04 05:16:16 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 1.2238
2022-03-04 05:16:19 - train: epoch 084, train_loss: 1.1961
2022-03-04 05:17:36 - eval: epoch: 084, acc1: 72.528%, acc5: 90.922%, test_loss: 1.0893, per_image_load_time: 1.946ms, per_image_inference_time: 0.964ms
2022-03-04 05:17:36 - until epoch: 084, best_acc1: 72.632%
2022-03-04 05:17:36 - epoch 085 lr: 0.0010000000000000002
2022-03-04 05:18:23 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 1.0635
2022-03-04 05:19:05 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 1.0276
2022-03-04 05:19:46 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 1.3139
2022-03-04 05:20:28 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 1.1103
2022-03-04 05:21:09 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 1.4027
2022-03-04 05:21:51 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 0.9568
2022-03-04 05:22:33 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 1.0655
2022-03-04 05:23:14 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 1.0570
2022-03-04 05:23:56 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 1.1284
2022-03-04 05:24:37 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 1.3043
2022-03-04 05:25:19 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 1.3189
2022-03-04 05:26:01 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 1.1417
2022-03-04 05:26:43 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 1.2546
2022-03-04 05:27:24 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 1.2119
2022-03-04 05:28:06 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 1.0293
2022-03-04 05:28:48 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 1.2052
2022-03-04 05:29:29 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 1.4159
2022-03-04 05:30:11 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 1.0067
2022-03-04 05:30:53 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 1.0981
2022-03-04 05:31:34 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 1.1828
2022-03-04 05:32:16 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 0.9485
2022-03-04 05:32:58 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 1.2715
2022-03-04 05:33:40 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 0.9899
2022-03-04 05:34:21 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 1.2780
2022-03-04 05:35:03 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 1.4422
2022-03-04 05:35:45 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 1.2168
2022-03-04 05:36:27 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 1.1279
2022-03-04 05:37:09 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 1.3341
2022-03-04 05:37:50 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 1.3163
2022-03-04 05:38:32 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 1.3317
2022-03-04 05:39:14 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 1.2202
2022-03-04 05:39:56 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 1.1187
2022-03-04 05:40:38 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 1.2025
2022-03-04 05:41:19 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 1.2789
2022-03-04 05:42:01 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 1.2905
2022-03-04 05:42:43 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 1.1609
2022-03-04 05:43:25 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 1.0167
2022-03-04 05:44:07 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 1.1300
2022-03-04 05:44:49 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 1.1340
2022-03-04 05:45:31 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 1.4320
2022-03-04 05:46:13 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 1.2230
2022-03-04 05:46:55 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 1.2265
2022-03-04 05:47:37 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 1.0843
2022-03-04 05:48:19 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 1.1788
2022-03-04 05:49:01 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 1.3711
2022-03-04 05:49:43 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 1.2711
2022-03-04 05:50:25 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.3914
2022-03-04 05:51:07 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 0.9794
2022-03-04 05:51:48 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 1.1976
2022-03-04 05:52:30 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 1.1194
2022-03-04 05:52:34 - train: epoch 085, train_loss: 1.1965
2022-03-04 05:53:51 - eval: epoch: 085, acc1: 72.610%, acc5: 90.768%, test_loss: 1.0940, per_image_load_time: 1.909ms, per_image_inference_time: 0.968ms
2022-03-04 05:53:51 - until epoch: 085, best_acc1: 72.632%
2022-03-04 05:53:51 - epoch 086 lr: 0.0010000000000000002
2022-03-04 05:54:38 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 1.0461
2022-03-04 05:55:19 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 1.0845
2022-03-04 05:56:01 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 1.3342
2022-03-04 05:56:42 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 1.2262
2022-03-04 05:57:23 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 1.0957
2022-03-04 05:58:05 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 1.1814
2022-03-04 05:58:46 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 1.0523
2022-03-04 05:59:27 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 1.3674
2022-03-04 06:00:09 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 1.2005
2022-03-04 06:00:50 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 1.2684
2022-03-04 06:01:32 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 1.2327
2022-03-04 06:02:13 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 1.1286
2022-03-04 06:02:54 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 1.1470
2022-03-04 06:03:36 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 1.0888
2022-03-04 06:04:18 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 1.1092
2022-03-04 06:04:59 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 1.2523
2022-03-04 06:05:41 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 1.1233
2022-03-04 06:06:22 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 1.2603
2022-03-04 06:07:04 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 1.3599
2022-03-04 06:07:46 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 1.2596
2022-03-04 06:08:27 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 1.1356
2022-03-04 06:09:09 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 1.2072
2022-03-04 06:09:51 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 1.1303
2022-03-04 06:10:32 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 1.0199
2022-03-04 06:11:14 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 1.1218
2022-03-04 06:11:55 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 1.2067
2022-03-04 06:12:37 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 1.0504
2022-03-04 06:13:19 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 1.2687
2022-03-04 06:14:00 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 1.0138
2022-03-04 06:14:42 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 1.2091
2022-03-04 06:15:24 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 1.0763
2022-03-04 06:16:06 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 1.2427
2022-03-04 06:16:47 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 1.2507
2022-03-04 06:17:29 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 1.2903
2022-03-04 06:18:11 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 1.1790
2022-03-04 06:18:52 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 1.2599
2022-03-04 06:19:34 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 1.2664
2022-03-04 06:20:16 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 1.2399
2022-03-04 06:20:57 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 1.3295
2022-03-04 06:21:39 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 1.2102
2022-03-04 06:22:20 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 1.2461
2022-03-04 06:23:02 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 1.3356
2022-03-04 06:23:44 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 1.3059
2022-03-04 06:24:25 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 1.1261
2022-03-04 06:25:07 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 0.9918
2022-03-04 06:25:49 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 1.2189
2022-03-04 06:26:30 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 1.2350
2022-03-04 06:27:12 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 1.2323
2022-03-04 06:27:54 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 1.2864
2022-03-04 06:28:35 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 1.2061
2022-03-04 06:28:39 - train: epoch 086, train_loss: 1.1950
2022-03-04 06:29:55 - eval: epoch: 086, acc1: 72.620%, acc5: 90.858%, test_loss: 1.0909, per_image_load_time: 1.867ms, per_image_inference_time: 0.960ms
2022-03-04 06:29:56 - until epoch: 086, best_acc1: 72.632%
2022-03-04 06:29:56 - epoch 087 lr: 0.0010000000000000002
2022-03-04 06:30:42 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 1.2581
2022-03-04 06:31:24 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 1.1271
2022-03-04 06:32:05 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 1.2918
2022-03-04 06:32:47 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 1.2941
2022-03-04 06:33:29 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 1.0026
2022-03-04 06:34:10 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 1.2034
2022-03-04 06:34:52 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 1.0502
2022-03-04 06:35:33 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 1.2010
2022-03-04 06:36:15 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 1.3334
2022-03-04 06:36:57 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 1.1415
2022-03-04 06:37:38 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 1.2522
2022-03-04 06:38:20 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 1.2532
2022-03-04 06:39:02 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 1.3368
2022-03-04 06:39:43 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 1.1614
2022-03-04 06:40:25 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 1.1450
2022-03-04 06:41:07 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 1.0064
2022-03-04 06:41:48 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 1.4115
2022-03-04 06:42:30 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 1.2057
2022-03-04 06:43:12 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 1.2611
2022-03-04 06:43:53 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 1.1528
2022-03-04 06:44:35 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 1.2699
2022-03-04 06:45:17 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 1.2314
2022-03-04 06:45:59 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 1.2241
2022-03-04 06:46:41 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 1.1802
2022-03-04 06:47:23 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 1.1086
2022-03-04 06:48:05 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 1.1936
2022-03-04 06:48:47 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 1.0879
2022-03-04 06:49:29 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 1.0051
2022-03-04 06:50:10 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 0.9958
2022-03-04 06:50:52 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 1.2771
2022-03-04 06:51:34 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 1.1851
2022-03-04 06:52:15 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 1.1359
2022-03-04 06:52:57 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 1.1108
2022-03-04 06:53:38 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 1.2151
2022-03-04 06:54:20 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 1.0613
2022-03-04 06:55:02 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 1.0521
2022-03-04 06:55:43 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 1.1457
2022-03-04 06:56:25 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 1.2153
2022-03-04 06:57:07 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 1.1290
2022-03-04 06:57:49 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 1.2960
2022-03-04 06:58:31 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 1.3313
2022-03-04 06:59:13 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 1.2250
2022-03-04 06:59:54 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 1.2332
2022-03-04 07:00:36 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 1.3171
2022-03-04 07:01:18 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 1.2209
2022-03-04 07:02:00 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 1.1709
2022-03-04 07:02:42 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 1.2262
2022-03-04 07:03:24 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 1.1054
2022-03-04 07:04:06 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 1.1187
2022-03-04 07:04:48 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 1.1569
2022-03-04 07:04:51 - train: epoch 087, train_loss: 1.1914
2022-03-04 07:06:07 - eval: epoch: 087, acc1: 72.606%, acc5: 90.926%, test_loss: 1.0895, per_image_load_time: 1.911ms, per_image_inference_time: 0.969ms
2022-03-04 07:06:08 - until epoch: 087, best_acc1: 72.632%
2022-03-04 07:06:08 - epoch 088 lr: 0.0010000000000000002
2022-03-04 07:06:55 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 1.2027
2022-03-04 07:07:36 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 1.1338
2022-03-04 07:08:18 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 1.2275
2022-03-04 07:08:59 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 1.1983
2022-03-04 07:09:41 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 1.1144
2022-03-04 07:10:22 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 1.2261
2022-03-04 07:11:03 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 1.1360
2022-03-04 07:11:45 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 1.2236
2022-03-04 07:12:26 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 1.2955
2022-03-04 07:13:07 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 1.1717
2022-03-04 07:13:49 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 1.1441
2022-03-04 07:14:30 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 1.1918
2022-03-04 07:15:12 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 1.3577
2022-03-04 07:15:54 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 1.0816
2022-03-04 07:16:35 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 1.2082
2022-03-04 07:17:17 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 1.0791
2022-03-04 07:17:59 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 1.2535
2022-03-04 07:18:41 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 1.1762
2022-03-04 07:19:22 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 1.2720
2022-03-04 07:20:04 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 1.0078
2022-03-04 07:20:45 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 1.1363
2022-03-04 07:21:27 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 1.1204
2022-03-04 07:22:09 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 1.0300
2022-03-04 07:22:50 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 1.2313
2022-03-04 07:23:32 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 1.2932
2022-03-04 07:24:14 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 1.2290
2022-03-04 07:24:56 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 1.2094
2022-03-04 07:25:37 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 1.2499
2022-03-04 07:26:19 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 1.1028
2022-03-04 07:27:01 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 1.3599
2022-03-04 07:27:43 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 1.0659
2022-03-04 07:28:25 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 1.0956
2022-03-04 07:29:07 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 1.1089
2022-03-04 07:29:48 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 1.0403
2022-03-04 07:30:30 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 1.1664
2022-03-04 07:31:12 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 1.2218
2022-03-04 07:31:54 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 1.2637
2022-03-04 07:32:36 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 1.1578
2022-03-04 07:33:19 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 1.3859
2022-03-04 07:34:01 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 1.0487
2022-03-04 07:34:43 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 1.2300
2022-03-04 07:35:25 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 1.2808
2022-03-04 07:36:07 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 1.0661
2022-03-04 07:36:49 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 1.1300
2022-03-04 07:37:31 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 1.2365
2022-03-04 07:38:13 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 1.3229
2022-03-04 07:38:55 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 1.1977
2022-03-04 07:39:37 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 0.9719
2022-03-04 07:40:20 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 0.9994
2022-03-04 07:41:02 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 1.1823
2022-03-04 07:41:05 - train: epoch 088, train_loss: 1.1899
2022-03-04 07:42:23 - eval: epoch: 088, acc1: 72.636%, acc5: 90.876%, test_loss: 1.0871, per_image_load_time: 1.958ms, per_image_inference_time: 0.962ms
2022-03-04 07:42:23 - until epoch: 088, best_acc1: 72.636%
2022-03-04 07:42:23 - epoch 089 lr: 0.0010000000000000002
2022-03-04 07:43:10 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 1.4349
2022-03-04 07:43:51 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 1.0118
2022-03-04 07:44:33 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 1.2646
2022-03-04 07:45:14 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 1.0755
2022-03-04 07:45:56 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 1.1811
2022-03-04 07:46:37 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 1.1185
2022-03-04 07:47:19 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 1.2042
2022-03-04 07:48:00 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 1.3643
2022-03-04 07:48:42 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 1.1941
2022-03-04 07:49:23 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 1.3277
2022-03-04 07:50:05 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 1.1689
2022-03-04 07:50:47 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 1.3142
2022-03-04 07:51:28 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 1.2299
2022-03-04 07:52:10 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 1.2973
2022-03-04 07:52:52 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 1.1823
2022-03-04 07:53:34 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 1.0347
2022-03-04 07:54:15 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 1.2248
2022-03-04 07:54:57 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 1.1776
2022-03-04 07:55:39 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 1.0256
2022-03-04 07:56:21 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 1.1748
2022-03-04 07:57:02 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 1.2500
2022-03-04 07:57:44 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 1.2484
2022-03-04 07:58:26 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 1.0487
2022-03-04 07:59:08 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 1.3134
2022-03-04 07:59:50 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 1.1057
2022-03-04 08:00:31 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 1.2625
2022-03-04 08:01:13 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 1.1343
2022-03-04 08:01:55 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 1.1888
2022-03-04 08:02:37 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 1.2096
2022-03-04 08:03:19 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 1.2629
2022-03-04 08:04:01 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 1.3251
2022-03-04 08:04:43 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 1.3596
2022-03-04 08:05:25 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 1.2863
2022-03-04 08:06:07 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 1.1953
2022-03-04 08:06:48 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 0.9744
2022-03-04 08:07:30 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 1.0616
2022-03-04 08:08:12 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 1.3411
2022-03-04 08:08:54 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 1.0368
2022-03-04 08:09:36 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 1.3247
2022-03-04 08:10:17 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 1.1177
2022-03-04 08:10:59 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 1.4661
2022-03-04 08:11:41 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 1.1254
2022-03-04 08:12:23 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 1.1444
2022-03-04 08:13:05 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 1.1109
2022-03-04 08:13:47 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 1.0682
2022-03-04 08:14:29 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 1.1256
2022-03-04 08:15:11 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 1.1712
2022-03-04 08:15:53 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 1.1671
2022-03-04 08:16:35 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 1.2077
2022-03-04 08:17:17 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 0.9455
2022-03-04 08:17:20 - train: epoch 089, train_loss: 1.1914
2022-03-04 08:18:37 - eval: epoch: 089, acc1: 72.724%, acc5: 90.846%, test_loss: 1.0878, per_image_load_time: 1.897ms, per_image_inference_time: 0.953ms
2022-03-04 08:18:38 - until epoch: 089, best_acc1: 72.724%
2022-03-04 22:48:50 - epoch 090 lr: 0.0010000000000000002
2022-03-04 22:49:39 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 1.1581
2022-03-04 22:50:21 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 1.1964
2022-03-04 22:51:03 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 1.0182
2022-03-04 22:51:45 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 0.9403
2022-03-04 22:52:27 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 1.1564
2022-03-04 22:53:09 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 1.2950
2022-03-04 22:53:51 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 1.1554
2022-03-04 22:54:33 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 1.1751
2022-03-04 22:55:15 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 1.0804
2022-03-04 22:55:58 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 1.1305
2022-03-04 22:56:40 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 1.1893
2022-03-04 22:57:22 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 1.0502
2022-03-04 22:58:04 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 1.1283
2022-03-04 22:58:46 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 1.1863
2022-03-04 22:59:29 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 1.2887
2022-03-04 23:00:11 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 1.2279
2022-03-04 23:00:53 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 1.0316
2022-03-04 23:01:35 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 1.2005
2022-03-04 23:02:17 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 1.0318
2022-03-04 23:03:00 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 1.1929
2022-03-04 23:03:42 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 1.2812
2022-03-04 23:04:24 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 1.1821
2022-03-04 23:05:06 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 1.1047
2022-03-04 23:05:48 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 1.2313
2022-03-04 23:06:30 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 1.3416
2022-03-04 23:07:12 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 1.1412
2022-03-04 23:07:54 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 1.2149
2022-03-04 23:08:37 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 1.1334
2022-03-04 23:09:19 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 1.0757
2022-03-04 23:10:01 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 1.0931
2022-03-04 23:10:44 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 1.0012
2022-03-04 23:11:26 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 1.1628
2022-03-04 23:12:08 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 1.2366
2022-03-04 23:12:51 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 1.2251
2022-03-04 23:13:33 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 1.3411
2022-03-04 23:14:15 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 1.1736
2022-03-04 23:14:57 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 1.2260
2022-03-04 23:15:39 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 1.1388
2022-03-04 23:16:22 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 1.1025
2022-03-04 23:17:04 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 1.1293
2022-03-04 23:17:46 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 1.4104
2022-03-04 23:18:29 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 1.2428
2022-03-04 23:19:11 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 1.1141
2022-03-04 23:19:53 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 1.1454
2022-03-04 23:20:35 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 1.1632
2022-03-04 23:21:18 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 1.1257
2022-03-04 23:22:00 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 1.1986
2022-03-04 23:22:42 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 1.3406
2022-03-04 23:23:24 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 1.0423
2022-03-04 23:24:07 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 1.0607
2022-03-04 23:24:09 - train: epoch 090, train_loss: 1.1866
2022-03-04 23:25:24 - eval: epoch: 090, acc1: 72.468%, acc5: 90.888%, test_loss: 1.0880, per_image_load_time: 1.250ms, per_image_inference_time: 0.956ms
2022-03-04 23:25:25 - until epoch: 090, best_acc1: 72.724%
2022-03-04 23:25:25 - epoch 091 lr: 0.00010000000000000003
2022-03-04 23:26:13 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 1.0308
2022-03-04 23:26:55 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 1.2159
2022-03-04 23:27:37 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 1.2066
2022-03-04 23:28:19 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 1.3035
2022-03-04 23:29:01 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 1.0277
2022-03-04 23:29:43 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 1.0978
2022-03-04 23:30:26 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 1.1378
2022-03-04 23:31:08 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 0.8716
2022-03-04 23:31:50 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 1.2418
2022-03-04 23:32:32 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 1.2272
2022-03-04 23:33:14 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 0.8994
2022-03-04 23:33:56 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 1.3074
2022-03-04 23:34:39 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 1.2235
2022-03-04 23:35:21 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 1.1846
2022-03-04 23:36:03 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 1.0600
2022-03-04 23:36:45 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 1.2082
2022-03-04 23:37:27 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 1.3845
2022-03-04 23:38:09 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 1.0442
2022-03-04 23:38:52 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 1.2538
2022-03-04 23:39:34 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 0.9921
2022-03-04 23:40:16 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 1.0672
2022-03-04 23:40:58 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 1.1869
2022-03-04 23:41:40 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 1.4234
2022-03-04 23:42:23 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 1.0142
2022-03-04 23:43:05 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 1.0889
2022-03-04 23:43:47 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 1.2397
2022-03-04 23:44:29 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 1.2755
2022-03-04 23:45:12 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 1.0772
2022-03-04 23:45:54 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 1.3416
2022-03-04 23:46:36 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 1.2752
2022-03-04 23:47:18 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 1.1512
2022-03-04 23:48:01 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 1.1965
2022-03-04 23:48:43 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 1.1239
2022-03-04 23:49:25 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 1.0258
2022-03-04 23:50:07 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 1.1911
2022-03-04 23:50:49 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 1.1578
2022-03-04 23:51:32 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 1.3122
2022-03-04 23:52:14 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 1.0691
2022-03-04 23:52:57 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 1.1307
2022-03-04 23:53:39 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 1.0523
2022-03-04 23:54:21 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 1.1135
2022-03-04 23:55:03 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 1.2279
2022-03-04 23:55:45 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 1.0869
2022-03-04 23:56:28 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 0.9166
2022-03-04 23:57:10 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 1.0072
2022-03-04 23:57:52 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 1.2671
2022-03-04 23:58:34 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 1.1721
2022-03-04 23:59:17 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 1.3070
2022-03-04 23:59:59 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 1.0273
2022-03-05 00:00:41 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 1.3554
2022-03-05 00:00:43 - train: epoch 091, train_loss: 1.1576
2022-03-05 00:01:58 - eval: epoch: 091, acc1: 73.052%, acc5: 91.144%, test_loss: 1.0722, per_image_load_time: 1.894ms, per_image_inference_time: 0.951ms
2022-03-05 00:01:59 - until epoch: 091, best_acc1: 73.052%
2022-03-05 00:01:59 - epoch 092 lr: 0.00010000000000000003
2022-03-05 00:02:47 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 1.3506
2022-03-05 00:03:29 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 1.2397
2022-03-05 00:04:11 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 1.1817
2022-03-05 00:04:53 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 1.0868
2022-03-05 00:05:35 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 1.2372
2022-03-05 00:06:17 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 1.1075
2022-03-05 00:07:00 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 1.1924
2022-03-05 00:07:42 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 1.2888
2022-03-05 00:08:24 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 1.3382
2022-03-05 00:09:06 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 1.1263
2022-03-05 00:09:48 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 1.1070
2022-03-05 00:10:31 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 1.1041
2022-03-05 00:11:13 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 1.0648
2022-03-05 00:11:55 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 0.9334
2022-03-05 00:12:37 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 1.0970
2022-03-05 00:13:19 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 1.1599
2022-03-05 00:14:02 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 1.0886
2022-03-05 00:14:44 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 1.0196
2022-03-05 00:15:26 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 1.0529
2022-03-05 00:16:08 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 1.1817
2022-03-05 00:16:50 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 1.2010
2022-03-05 00:17:33 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 1.2310
2022-03-05 00:18:15 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 1.1819
2022-03-05 00:18:57 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 0.9439
2022-03-05 00:19:39 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 1.2054
2022-03-05 00:20:21 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 1.1505
2022-03-05 00:21:04 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 1.1618
2022-03-05 00:21:46 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 1.1618
2022-03-05 00:22:28 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 1.1902
2022-03-05 00:23:10 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 1.1657
2022-03-05 00:23:52 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 1.2423
2022-03-05 00:24:34 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 1.2286
2022-03-05 00:25:16 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 1.2824
2022-03-05 00:25:59 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 1.4220
2022-03-05 00:26:41 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 1.1104
2022-03-05 00:27:23 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 1.1771
2022-03-05 00:28:05 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 1.1152
2022-03-05 00:28:47 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 1.0379
2022-03-05 00:29:30 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 1.2296
2022-03-05 00:30:12 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 1.1756
2022-03-05 00:30:54 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 1.0265
2022-03-05 00:31:36 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 1.3111
2022-03-05 00:32:19 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 1.1461
2022-03-05 00:33:01 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 1.1179
2022-03-05 00:33:43 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 1.0254
2022-03-05 00:34:25 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 1.0746
2022-03-05 00:35:07 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 0.9692
2022-03-05 00:35:49 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 1.1735
2022-03-05 00:36:31 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 1.0176
2022-03-05 00:37:13 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 1.1166
2022-03-05 00:37:15 - train: epoch 092, train_loss: 1.1511
2022-03-05 00:38:30 - eval: epoch: 092, acc1: 73.122%, acc5: 91.184%, test_loss: 1.0688, per_image_load_time: 1.887ms, per_image_inference_time: 0.952ms
2022-03-05 00:38:31 - until epoch: 092, best_acc1: 73.122%
2022-03-05 00:38:31 - epoch 093 lr: 0.00010000000000000003
2022-03-05 00:39:18 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 1.1389
2022-03-05 00:40:00 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 1.0751
2022-03-05 00:40:43 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 1.2055
2022-03-05 00:41:25 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 1.1345
2022-03-05 00:42:07 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 1.2295
2022-03-05 00:42:50 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 1.1038
2022-03-05 00:43:32 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 1.1156
2022-03-05 00:44:14 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 0.9899
2022-03-05 00:44:56 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 1.1188
2022-03-05 00:45:38 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 0.9442
2022-03-05 00:46:20 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 1.2254
2022-03-05 00:47:03 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 1.2149
2022-03-05 00:47:45 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 1.0657
2022-03-05 00:48:27 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 1.1376
2022-03-05 00:49:09 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 1.2647
2022-03-05 00:49:51 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 1.1473
2022-03-05 00:50:34 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 1.0976
2022-03-05 00:51:16 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 1.3365
2022-03-05 00:51:58 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 1.0503
2022-03-05 00:52:40 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 1.0466
2022-03-05 00:53:23 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 1.0550
2022-03-05 00:54:05 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 1.3911
2022-03-05 00:54:47 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 1.2228
2022-03-05 00:55:29 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 1.0343
2022-03-05 00:56:11 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 1.1928
2022-03-05 00:56:53 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 1.2023
2022-03-05 00:57:35 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 1.1613
2022-03-05 00:58:18 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 1.1822
2022-03-05 00:59:00 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 1.1069
2022-03-05 00:59:42 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 1.1771
2022-03-05 01:00:24 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 1.1509
2022-03-05 01:01:06 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 1.1630
2022-03-05 01:01:49 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 1.1133
2022-03-05 01:02:31 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 1.2473
2022-03-05 01:03:13 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 1.1154
2022-03-05 01:03:55 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 1.2764
2022-03-05 01:04:38 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 1.0645
2022-03-05 01:05:20 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 1.1011
2022-03-05 01:06:02 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 1.2104
2022-03-05 01:06:45 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 1.1997
2022-03-05 01:07:27 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 1.1809
2022-03-05 01:08:09 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 1.3346
2022-03-05 01:08:52 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 1.2647
2022-03-05 01:09:34 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 1.2814
2022-03-05 01:10:16 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 1.0600
2022-03-05 01:10:59 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 1.0990
2022-03-05 01:11:42 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 1.3023
2022-03-05 01:12:24 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 1.1157
2022-03-05 01:13:06 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 1.1447
2022-03-05 01:13:49 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 1.2615
2022-03-05 01:13:51 - train: epoch 093, train_loss: 1.1482
2022-03-05 01:15:06 - eval: epoch: 093, acc1: 73.128%, acc5: 91.226%, test_loss: 1.0686, per_image_load_time: 1.802ms, per_image_inference_time: 0.977ms
2022-03-05 01:15:06 - until epoch: 093, best_acc1: 73.128%
2022-03-05 01:15:06 - epoch 094 lr: 0.00010000000000000003
2022-03-05 01:15:54 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 1.1600
2022-03-05 01:16:36 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 1.0598
2022-03-05 01:17:18 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 1.2254
2022-03-05 01:18:00 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 1.3000
2022-03-05 01:18:42 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 1.1602
2022-03-05 01:19:24 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 0.9329
2022-03-05 01:20:06 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 1.1395
2022-03-05 01:20:49 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 0.9924
2022-03-05 01:21:31 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 1.1393
2022-03-05 01:22:13 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 1.1674
2022-03-05 01:22:55 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 1.3265
2022-03-05 01:23:37 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 1.0699
2022-03-05 01:24:19 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 0.9989
2022-03-05 01:25:01 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 1.1860
2022-03-05 01:25:44 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 1.2507
2022-03-05 01:26:26 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.2543
2022-03-05 01:27:08 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 0.9729
2022-03-05 01:27:50 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 1.0461
2022-03-05 01:28:33 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 1.1900
2022-03-05 01:29:15 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 0.9586
2022-03-05 01:29:57 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 0.9231
2022-03-05 01:30:39 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 1.1199
2022-03-05 01:31:21 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 1.1195
2022-03-05 01:32:04 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 1.1613
2022-03-05 01:32:46 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 1.0049
2022-03-05 01:33:28 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 1.1320
2022-03-05 01:34:10 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 1.1420
2022-03-05 01:34:53 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 1.1692
2022-03-05 01:35:35 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 1.0983
2022-03-05 01:36:17 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 1.2203
2022-03-05 01:36:59 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 1.1854
2022-03-05 01:37:42 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 1.1523
2022-03-05 01:38:24 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 1.1167
2022-03-05 01:39:07 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 1.1184
2022-03-05 01:39:49 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 1.2512
2022-03-05 01:40:31 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 1.0942
2022-03-05 01:41:14 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 1.2554
2022-03-05 01:41:56 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 1.1033
2022-03-05 01:42:38 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 1.2003
2022-03-05 01:43:21 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 1.1147
2022-03-05 01:44:03 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 1.2842
2022-03-05 01:44:45 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 1.1698
2022-03-05 01:45:28 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 1.1489
2022-03-05 01:46:10 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 1.2871
2022-03-05 01:46:52 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 0.9878
2022-03-05 01:47:34 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 1.1432
2022-03-05 01:48:17 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 1.4058
2022-03-05 01:48:59 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 0.9391
2022-03-05 01:49:42 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 1.1122
2022-03-05 01:50:24 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 1.1151
2022-03-05 01:50:26 - train: epoch 094, train_loss: 1.1459
2022-03-05 01:51:41 - eval: epoch: 094, acc1: 73.078%, acc5: 91.184%, test_loss: 1.0676, per_image_load_time: 1.903ms, per_image_inference_time: 0.968ms
2022-03-05 01:51:41 - until epoch: 094, best_acc1: 73.128%
2022-03-05 01:51:41 - epoch 095 lr: 0.00010000000000000003
2022-03-05 01:52:29 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 1.2016
2022-03-05 01:53:12 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 1.3275
2022-03-05 01:53:54 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 1.0458
2022-03-05 01:54:36 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 1.0888
2022-03-05 01:55:18 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 1.3180
2022-03-05 01:56:01 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 1.0415
2022-03-05 01:56:43 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 1.2964
2022-03-05 01:57:25 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 1.1382
2022-03-05 01:58:08 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 1.2047
2022-03-05 01:58:50 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 1.0192
2022-03-05 01:59:32 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 1.1691
2022-03-05 02:00:15 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 0.9516
2022-03-05 02:00:57 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 1.1030
2022-03-05 02:01:39 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 1.2273
2022-03-05 02:02:21 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 1.1239
2022-03-05 02:03:04 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 1.1776
2022-03-05 02:03:46 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 1.2648
2022-03-05 02:04:28 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 1.0967
2022-03-05 02:05:10 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 1.2111
2022-03-05 02:05:53 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 1.1368
2022-03-05 02:06:35 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 1.2434
2022-03-05 02:07:17 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 1.0145
2022-03-05 02:08:00 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 1.2020
2022-03-05 02:08:42 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 1.2100
2022-03-05 02:09:25 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 1.1331
2022-03-05 02:10:07 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 1.2766
2022-03-05 02:10:49 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 1.2139
2022-03-05 02:11:31 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 1.0257
2022-03-05 02:12:14 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 1.1589
2022-03-05 02:12:56 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 1.1937
2022-03-05 02:13:38 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 1.2832
2022-03-05 02:14:20 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 1.1475
2022-03-05 02:15:03 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 1.1844
2022-03-05 02:15:45 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 1.0048
2022-03-05 02:16:27 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 1.1812
2022-03-05 02:17:09 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 1.1599
2022-03-05 02:17:51 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 1.0794
2022-03-05 02:18:34 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 1.0698
2022-03-05 02:19:16 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 1.1360
2022-03-05 02:19:58 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 0.9027
2022-03-05 02:20:40 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 1.1157
2022-03-05 02:21:23 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 1.0186
2022-03-05 02:22:05 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 1.2551
2022-03-05 02:22:47 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 1.2641
2022-03-05 02:23:30 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 0.9802
2022-03-05 02:24:12 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 1.1944
2022-03-05 02:24:54 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 1.1209
2022-03-05 02:25:37 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 1.0314
2022-03-05 02:26:19 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 1.0700
2022-03-05 02:27:01 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 0.9756
2022-03-05 02:27:04 - train: epoch 095, train_loss: 1.1453
2022-03-05 02:28:18 - eval: epoch: 095, acc1: 73.116%, acc5: 91.186%, test_loss: 1.0671, per_image_load_time: 1.754ms, per_image_inference_time: 0.957ms
2022-03-05 02:28:19 - until epoch: 095, best_acc1: 73.128%
2022-03-05 02:28:19 - epoch 096 lr: 0.00010000000000000003
2022-03-05 02:29:06 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 1.1551
2022-03-05 02:29:48 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 1.1489
2022-03-05 02:30:30 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 1.2601
2022-03-05 02:31:12 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 1.1198
2022-03-05 02:31:54 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 1.2740
2022-03-05 02:32:36 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 1.1791
2022-03-05 02:33:18 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 1.3282
2022-03-05 02:34:00 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 1.0704
2022-03-05 02:34:42 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 1.1179
2022-03-05 02:35:25 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 1.2544
2022-03-05 02:36:07 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 1.2277
2022-03-05 02:36:49 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 1.0561
2022-03-05 02:37:31 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 1.1516
2022-03-05 02:38:13 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 1.2156
2022-03-05 02:38:56 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 1.0690
2022-03-05 02:39:38 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 0.9644
2022-03-05 02:40:20 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 1.1526
2022-03-05 02:41:02 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 1.1889
2022-03-05 02:41:45 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 1.3840
2022-03-05 02:42:27 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 1.0685
2022-03-05 02:43:09 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 1.2073
2022-03-05 02:43:51 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 0.9341
2022-03-05 02:44:34 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 1.1772
2022-03-05 02:45:16 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 1.1329
2022-03-05 02:45:58 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 1.1465
2022-03-05 02:46:41 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 1.1381
2022-03-05 02:47:23 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 1.0922
2022-03-05 02:48:06 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 1.0393
2022-03-05 02:48:48 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 1.2766
2022-03-05 02:49:30 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 1.1750
2022-03-05 02:50:13 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 1.2743
2022-03-05 02:50:55 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 1.3114
2022-03-05 02:51:37 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 1.1776
2022-03-05 02:52:20 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 1.1282
2022-03-05 02:53:02 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 1.0088
2022-03-05 02:53:44 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 0.9469
2022-03-05 02:54:27 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 1.1550
2022-03-05 02:55:09 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 0.9839
2022-03-05 02:55:51 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 1.0593
2022-03-05 02:56:34 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 1.2268
2022-03-05 02:57:16 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 1.4463
2022-03-05 02:57:58 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 1.0572
2022-03-05 02:58:41 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 1.0275
2022-03-05 02:59:23 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 1.0786
2022-03-05 03:00:05 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 1.2678
2022-03-05 03:00:48 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 1.1889
2022-03-05 03:01:30 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 1.2307
2022-03-05 03:02:12 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 1.1529
2022-03-05 03:02:55 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 1.1898
2022-03-05 03:03:37 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 1.1792
2022-03-05 03:03:39 - train: epoch 096, train_loss: 1.1424
2022-03-05 03:04:54 - eval: epoch: 096, acc1: 73.102%, acc5: 91.172%, test_loss: 1.0660, per_image_load_time: 1.155ms, per_image_inference_time: 0.975ms
2022-03-05 03:04:54 - until epoch: 096, best_acc1: 73.128%
2022-03-05 03:04:54 - epoch 097 lr: 0.00010000000000000003
2022-03-05 03:05:41 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 1.2313
2022-03-05 03:06:23 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 1.1402
2022-03-05 03:07:05 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 1.0968
2022-03-05 03:07:48 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 1.2836
2022-03-05 03:08:30 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 1.1595
2022-03-05 03:09:12 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 1.3291
2022-03-05 03:09:54 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 1.1319
2022-03-05 03:10:36 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 1.1201
2022-03-05 03:11:18 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 1.0983
2022-03-05 03:12:00 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 1.3417
2022-03-05 03:12:43 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 0.9132
2022-03-05 03:13:25 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 1.0005
2022-03-05 03:14:07 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 0.9881
2022-03-05 03:14:49 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 1.0165
2022-03-05 03:15:31 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 1.0136
2022-03-05 03:16:14 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 1.1139
2022-03-05 03:16:56 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 0.9203
2022-03-05 03:17:38 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 1.1325
2022-03-05 03:18:20 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 0.9887
2022-03-05 03:19:02 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 1.0853
2022-03-05 03:19:44 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 1.1479
2022-03-05 03:20:26 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 1.0836
2022-03-05 03:21:09 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 1.0018
2022-03-05 03:21:51 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 1.1440
2022-03-05 03:22:33 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 1.2539
2022-03-05 03:23:15 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 1.2634
2022-03-05 03:23:57 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 1.2030
2022-03-05 03:24:40 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 1.2010
2022-03-05 03:25:22 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 1.2113
2022-03-05 03:26:04 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 1.2053
2022-03-05 03:26:46 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 1.1333
2022-03-05 03:27:28 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 1.2727
2022-03-05 03:28:10 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 1.3489
2022-03-05 03:28:53 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 1.0669
2022-03-05 03:29:35 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 1.1302
2022-03-05 03:30:17 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 1.3164
2022-03-05 03:30:59 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 1.0573
2022-03-05 03:31:42 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 0.9707
2022-03-05 03:32:24 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 1.0585
2022-03-05 03:33:07 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 1.1482
2022-03-05 03:33:49 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 1.1938
2022-03-05 03:34:32 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 0.9400
2022-03-05 03:35:14 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 0.8614
2022-03-05 03:35:56 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 1.1034
2022-03-05 03:36:39 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 1.3029
2022-03-05 03:37:21 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 1.1839
2022-03-05 03:38:04 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 1.1972
2022-03-05 03:38:46 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 1.1980
2022-03-05 03:39:28 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 1.3624
2022-03-05 03:40:11 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 1.0152
2022-03-05 03:40:13 - train: epoch 097, train_loss: 1.1405
2022-03-05 03:41:28 - eval: epoch: 097, acc1: 73.150%, acc5: 91.166%, test_loss: 1.0644, per_image_load_time: 1.964ms, per_image_inference_time: 0.956ms
2022-03-05 03:41:28 - until epoch: 097, best_acc1: 73.150%
2022-03-05 03:41:28 - epoch 098 lr: 0.00010000000000000003
2022-03-05 03:42:17 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 1.3378
2022-03-05 03:42:59 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 1.0984
2022-03-05 03:43:41 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 1.1797
2022-03-05 03:44:23 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 1.0421
2022-03-05 03:45:05 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 1.2563
2022-03-05 03:45:47 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 1.0291
2022-03-05 03:46:29 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 1.1259
2022-03-05 03:47:11 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 1.0249
2022-03-05 03:47:53 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 1.2566
2022-03-05 03:48:35 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 1.0641
2022-03-05 03:49:17 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 1.1942
2022-03-05 03:50:00 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 1.1403
2022-03-05 03:50:42 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 1.0755
2022-03-05 03:51:24 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 1.1545
2022-03-05 03:52:06 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 1.1093
2022-03-05 03:52:48 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 1.1593
2022-03-05 03:53:30 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 1.2003
2022-03-05 03:54:13 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 1.1391
2022-03-05 03:54:55 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 1.1005
2022-03-05 03:55:37 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 1.2618
2022-03-05 03:56:19 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 1.1583
2022-03-05 03:57:01 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 1.0448
2022-03-05 03:57:43 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 1.1105
2022-03-05 03:58:26 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 1.2415
2022-03-05 03:59:08 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 1.1362
2022-03-05 03:59:50 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 1.2048
2022-03-05 04:00:32 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 1.3181
2022-03-05 04:01:14 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 1.1932
2022-03-05 04:01:57 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 1.3080
2022-03-05 04:02:39 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 1.1651
2022-03-05 04:03:21 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 1.1552
2022-03-05 04:04:03 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 1.0104
2022-03-05 04:04:46 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 1.1285
2022-03-05 04:05:28 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 1.1179
2022-03-05 04:06:10 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 1.2467
2022-03-05 04:06:52 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 1.2537
2022-03-05 04:07:35 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 1.1205
2022-03-05 04:08:17 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 1.1584
2022-03-05 04:08:59 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 1.0660
2022-03-05 04:09:41 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 1.3226
2022-03-05 04:10:23 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 1.0169
2022-03-05 04:11:06 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 1.1672
2022-03-05 04:11:48 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 0.9399
2022-03-05 04:12:30 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 1.2568
2022-03-05 04:13:13 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 1.2315
2022-03-05 04:13:55 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 1.1647
2022-03-05 04:14:37 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 1.1470
2022-03-05 04:15:19 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 0.9400
2022-03-05 04:16:02 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 1.1780
2022-03-05 04:16:44 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 1.1964
2022-03-05 04:16:46 - train: epoch 098, train_loss: 1.1414
2022-03-05 04:18:01 - eval: epoch: 098, acc1: 73.162%, acc5: 91.172%, test_loss: 1.0654, per_image_load_time: 1.615ms, per_image_inference_time: 0.957ms
2022-03-05 04:18:02 - until epoch: 098, best_acc1: 73.162%
2022-03-05 04:18:02 - epoch 099 lr: 0.00010000000000000003
2022-03-05 04:18:50 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 1.1834
2022-03-05 04:19:32 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 0.9922
2022-03-05 04:20:14 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 1.0211
2022-03-05 04:20:56 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 1.1927
2022-03-05 04:21:38 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 1.2248
2022-03-05 04:22:20 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 1.2738
2022-03-05 04:23:02 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 1.2533
2022-03-05 04:23:44 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 1.2599
2022-03-05 04:24:26 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 1.1560
2022-03-05 04:25:08 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 0.9765
2022-03-05 04:25:50 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 1.2026
2022-03-05 04:26:32 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 1.0881
2022-03-05 04:27:14 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 1.1103
2022-03-05 04:27:56 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 1.1239
2022-03-05 04:28:38 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 1.1362
2022-03-05 04:29:20 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 1.2317
2022-03-05 04:30:03 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 1.1293
2022-03-05 04:30:45 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 1.2389
2022-03-05 04:31:27 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 1.1254
2022-03-05 04:32:09 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 1.1737
2022-03-05 04:32:51 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 1.0170
2022-03-05 04:33:33 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 1.0553
2022-03-05 04:34:15 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 1.0926
2022-03-05 04:34:57 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 1.3269
2022-03-05 04:35:39 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 0.9224
2022-03-05 04:36:22 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 1.0825
2022-03-05 04:37:04 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 1.1088
2022-03-05 04:37:46 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 1.2385
2022-03-05 04:38:28 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 1.0167
2022-03-05 04:39:10 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 1.2650
2022-03-05 04:39:52 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 1.0949
2022-03-05 04:40:35 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 1.0856
2022-03-05 04:41:17 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 1.1372
2022-03-05 04:41:59 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 1.2425
2022-03-05 04:42:41 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 1.1948
2022-03-05 04:43:24 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 1.0050
2022-03-05 04:44:06 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 0.9596
2022-03-05 04:44:48 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 1.1440
2022-03-05 04:45:30 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 1.1085
2022-03-05 04:46:13 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 1.1925
2022-03-05 04:46:55 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 1.0003
2022-03-05 04:47:37 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 1.2675
2022-03-05 04:48:19 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 1.2233
2022-03-05 04:49:02 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 1.1934
2022-03-05 04:49:44 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 1.2619
2022-03-05 04:50:26 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 1.1895
2022-03-05 04:51:08 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 1.0919
2022-03-05 04:51:51 - train: epoch 0099, iter [04800, 05004], lr: 0.000100, loss: 1.1257
2022-03-05 04:52:33 - train: epoch 0099, iter [04900, 05004], lr: 0.000100, loss: 1.2781
2022-03-05 04:53:15 - train: epoch 0099, iter [05000, 05004], lr: 0.000100, loss: 1.1252
2022-03-05 04:53:18 - train: epoch 099, train_loss: 1.1408
2022-03-05 04:54:32 - eval: epoch: 099, acc1: 73.182%, acc5: 91.226%, test_loss: 1.0649, per_image_load_time: 1.507ms, per_image_inference_time: 0.974ms
2022-03-05 04:54:33 - until epoch: 099, best_acc1: 73.182%
2022-03-05 04:54:33 - epoch 100 lr: 0.00010000000000000003
2022-03-05 04:55:20 - train: epoch 0100, iter [00100, 05004], lr: 0.000100, loss: 1.2108
2022-03-05 04:56:02 - train: epoch 0100, iter [00200, 05004], lr: 0.000100, loss: 1.0694
2022-03-05 04:56:45 - train: epoch 0100, iter [00300, 05004], lr: 0.000100, loss: 1.0099
2022-03-05 04:57:27 - train: epoch 0100, iter [00400, 05004], lr: 0.000100, loss: 1.1543
2022-03-05 04:58:09 - train: epoch 0100, iter [00500, 05004], lr: 0.000100, loss: 1.1469
2022-03-05 04:58:51 - train: epoch 0100, iter [00600, 05004], lr: 0.000100, loss: 1.1767
2022-03-05 04:59:34 - train: epoch 0100, iter [00700, 05004], lr: 0.000100, loss: 1.0326
2022-03-05 05:00:16 - train: epoch 0100, iter [00800, 05004], lr: 0.000100, loss: 1.1084
2022-03-05 05:00:58 - train: epoch 0100, iter [00900, 05004], lr: 0.000100, loss: 1.0937
2022-03-05 05:01:41 - train: epoch 0100, iter [01000, 05004], lr: 0.000100, loss: 0.9605
2022-03-05 05:02:23 - train: epoch 0100, iter [01100, 05004], lr: 0.000100, loss: 0.9869
2022-03-05 05:03:05 - train: epoch 0100, iter [01200, 05004], lr: 0.000100, loss: 1.3935
2022-03-05 05:03:47 - train: epoch 0100, iter [01300, 05004], lr: 0.000100, loss: 1.1432
2022-03-05 05:04:30 - train: epoch 0100, iter [01400, 05004], lr: 0.000100, loss: 1.2261
2022-03-05 05:05:12 - train: epoch 0100, iter [01500, 05004], lr: 0.000100, loss: 1.1407
2022-03-05 05:05:54 - train: epoch 0100, iter [01600, 05004], lr: 0.000100, loss: 1.0200
2022-03-05 05:06:37 - train: epoch 0100, iter [01700, 05004], lr: 0.000100, loss: 1.0236
2022-03-05 05:07:19 - train: epoch 0100, iter [01800, 05004], lr: 0.000100, loss: 1.2013
2022-03-05 05:08:01 - train: epoch 0100, iter [01900, 05004], lr: 0.000100, loss: 1.1131
2022-03-05 05:08:43 - train: epoch 0100, iter [02000, 05004], lr: 0.000100, loss: 1.1140
2022-03-05 05:09:25 - train: epoch 0100, iter [02100, 05004], lr: 0.000100, loss: 1.0725
2022-03-05 05:10:08 - train: epoch 0100, iter [02200, 05004], lr: 0.000100, loss: 1.1460
2022-03-05 05:10:50 - train: epoch 0100, iter [02300, 05004], lr: 0.000100, loss: 1.1925
2022-03-05 05:11:32 - train: epoch 0100, iter [02400, 05004], lr: 0.000100, loss: 1.2717
2022-03-05 05:12:15 - train: epoch 0100, iter [02500, 05004], lr: 0.000100, loss: 1.2056
2022-03-05 05:12:57 - train: epoch 0100, iter [02600, 05004], lr: 0.000100, loss: 0.9690
2022-03-05 05:13:39 - train: epoch 0100, iter [02700, 05004], lr: 0.000100, loss: 1.0446
2022-03-05 05:14:21 - train: epoch 0100, iter [02800, 05004], lr: 0.000100, loss: 1.0760
2022-03-05 05:15:04 - train: epoch 0100, iter [02900, 05004], lr: 0.000100, loss: 1.2267
2022-03-05 05:15:46 - train: epoch 0100, iter [03000, 05004], lr: 0.000100, loss: 1.0578
2022-03-05 05:16:28 - train: epoch 0100, iter [03100, 05004], lr: 0.000100, loss: 0.9864
2022-03-05 05:17:11 - train: epoch 0100, iter [03200, 05004], lr: 0.000100, loss: 1.1855
2022-03-05 05:17:53 - train: epoch 0100, iter [03300, 05004], lr: 0.000100, loss: 1.3138
2022-03-05 05:18:35 - train: epoch 0100, iter [03400, 05004], lr: 0.000100, loss: 1.2385
2022-03-05 05:19:18 - train: epoch 0100, iter [03500, 05004], lr: 0.000100, loss: 1.0150
2022-03-05 05:20:00 - train: epoch 0100, iter [03600, 05004], lr: 0.000100, loss: 1.1865
2022-03-05 05:20:42 - train: epoch 0100, iter [03700, 05004], lr: 0.000100, loss: 1.0908
2022-03-05 05:21:24 - train: epoch 0100, iter [03800, 05004], lr: 0.000100, loss: 1.1275
2022-03-05 05:22:07 - train: epoch 0100, iter [03900, 05004], lr: 0.000100, loss: 1.0413
2022-03-05 05:22:49 - train: epoch 0100, iter [04000, 05004], lr: 0.000100, loss: 1.2325
2022-03-05 05:23:31 - train: epoch 0100, iter [04100, 05004], lr: 0.000100, loss: 1.1491
2022-03-05 05:24:14 - train: epoch 0100, iter [04200, 05004], lr: 0.000100, loss: 1.1401
2022-03-05 05:24:56 - train: epoch 0100, iter [04300, 05004], lr: 0.000100, loss: 1.0760
2022-03-05 05:25:39 - train: epoch 0100, iter [04400, 05004], lr: 0.000100, loss: 1.1465
2022-03-05 05:26:21 - train: epoch 0100, iter [04500, 05004], lr: 0.000100, loss: 0.9868
2022-03-05 05:27:03 - train: epoch 0100, iter [04600, 05004], lr: 0.000100, loss: 1.1606
2022-03-05 05:27:46 - train: epoch 0100, iter [04700, 05004], lr: 0.000100, loss: 1.2950
2022-03-05 05:28:28 - train: epoch 0100, iter [04800, 05004], lr: 0.000100, loss: 0.9795
2022-03-05 05:29:11 - train: epoch 0100, iter [04900, 05004], lr: 0.000100, loss: 1.1414
2022-03-05 05:29:53 - train: epoch 0100, iter [05000, 05004], lr: 0.000100, loss: 1.2331
2022-03-05 05:29:55 - train: epoch 100, train_loss: 1.1418
2022-03-05 05:31:10 - eval: epoch: 100, acc1: 73.198%, acc5: 91.214%, test_loss: 1.0650, per_image_load_time: 1.936ms, per_image_inference_time: 0.961ms
2022-03-05 05:31:11 - until epoch: 100, best_acc1: 73.198%
2022-03-05 05:31:11 - train done. model: yolov5lbackbone, train time: 60.767 hours, best_acc1: 73.198%
