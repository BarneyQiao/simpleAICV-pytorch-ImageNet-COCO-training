2022-03-02 02:14:16 - network: yolov5lbackbone
2022-03-02 02:14:16 - num_classes: 1000
2022-03-02 02:14:16 - input_image_size: 256
2022-03-02 02:14:16 - scale: 1.1428571428571428
2022-03-02 02:14:16 - trained_model_path: 
2022-03-02 02:14:16 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-02 02:14:16 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f15af925bb0>
2022-03-02 02:14:16 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f15af925e80>
2022-03-02 02:14:16 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f15af925eb0>
2022-03-02 02:14:16 - seed: 0
2022-03-02 02:14:16 - batch_size: 256
2022-03-02 02:14:16 - num_workers: 16
2022-03-02 02:14:16 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-02 02:14:16 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-03-02 02:14:16 - epochs: 100
2022-03-02 02:14:16 - print_interval: 100
2022-03-02 02:14:16 - distributed: True
2022-03-02 02:14:16 - sync_bn: False
2022-03-02 02:14:16 - apex: True
2022-03-02 02:14:16 - gpus_type: NVIDIA RTX A5000
2022-03-02 02:14:16 - gpus_num: 2
2022-03-02 02:14:16 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f15a02b77f0>
2022-03-02 02:14:16 - --------------------parameters--------------------
2022-03-02 02:14:16 - name: conv.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: conv.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: conv.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.conv1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.conv1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.conv1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.conv2.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.conv2.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.conv2.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.conv3.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.conv3.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.conv3.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.4.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.4.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.4.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.4.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.4.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.4.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.5.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.5.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.5.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.5.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.5.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.5.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.2.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.2.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.2.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.conv1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.conv1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.conv1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.conv2.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.conv2.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.conv2.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.conv3.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.conv3.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.conv3.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.4.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.4.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.5.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.5.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.6.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.6.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.6.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.6.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.6.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.6.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.7.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.7.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.7.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.7.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.7.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.7.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.8.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.8.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.8.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.8.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.8.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.8.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.4.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.4.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.4.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.conv1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.conv1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.conv1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.conv2.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.conv2.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.conv2.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.conv3.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.conv3.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.conv3.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: middle_layers.6.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.6.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: middle_layers.6.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: sppf.conv1.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: sppf.conv1.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: sppf.conv1.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: sppf.conv2.layer.0.weight, grad: True
2022-03-02 02:14:16 - name: sppf.conv2.layer.1.weight, grad: True
2022-03-02 02:14:16 - name: sppf.conv2.layer.1.bias, grad: True
2022-03-02 02:14:16 - name: fc.weight, grad: True
2022-03-02 02:14:16 - name: fc.bias, grad: True
2022-03-02 02:14:16 - --------------------buffers--------------------
2022-03-02 02:14:16 - name: conv.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: conv.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: conv.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.conv1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.conv1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.conv2.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.conv2.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.conv3.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.conv3.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.4.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.4.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.4.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.4.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.5.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.5.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.5.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.5.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.1.bottlenecks.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.2.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.2.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.2.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.conv1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.conv1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.conv2.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.conv2.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.conv3.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.conv3.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.6.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.6.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.6.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.6.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.7.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.7.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.7.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.7.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.8.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.8.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.8.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.8.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.8.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.3.bottlenecks.8.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.4.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.4.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.4.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.conv1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.conv1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.conv1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.conv2.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.conv2.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.conv2.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.conv3.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.conv3.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.conv3.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.5.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: middle_layers.6.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: middle_layers.6.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: middle_layers.6.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: sppf.conv1.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: sppf.conv1.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: sppf.conv1.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - name: sppf.conv2.layer.1.running_mean, grad: False
2022-03-02 02:14:16 - name: sppf.conv2.layer.1.running_var, grad: False
2022-03-02 02:14:16 - name: sppf.conv2.layer.1.num_batches_tracked, grad: False
2022-03-02 02:14:16 - epoch 001 lr: 0.1
2022-03-02 02:15:03 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 7.1314
2022-03-02 02:15:44 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.9483
2022-03-02 02:16:26 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.9060
2022-03-02 02:17:07 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.9183
2022-03-02 02:17:48 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.8939
2022-03-02 02:18:30 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.7971
2022-03-02 02:19:11 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.8248
2022-03-02 02:19:53 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.7902
2022-03-02 02:20:34 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.6958
2022-03-02 02:21:16 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.7183
2022-03-02 02:21:57 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.7522
2022-03-02 02:22:39 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 6.6069
2022-03-02 02:23:20 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 6.5623
2022-03-02 02:24:02 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 6.5417
2022-03-02 02:24:43 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 6.3561
2022-03-02 02:25:25 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 6.3699
2022-03-02 02:26:06 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 6.1349
2022-03-02 02:26:48 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 6.0730
2022-03-02 02:27:29 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.9265
2022-03-02 02:28:11 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.8794
2022-03-02 02:28:52 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.8441
2022-03-02 02:29:33 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.6973
2022-03-02 02:30:15 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 5.6624
2022-03-02 02:30:56 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 5.5115
2022-03-02 02:31:38 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.5023
2022-03-02 02:32:19 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.5713
2022-03-02 02:33:01 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.4761
2022-03-02 02:33:43 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 5.1454
2022-03-02 02:34:24 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 5.1556
2022-03-02 02:35:06 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.2758
2022-03-02 02:35:47 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.1811
2022-03-02 02:36:29 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 5.1319
2022-03-02 02:37:11 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.8590
2022-03-02 02:37:53 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.8731
2022-03-02 02:38:35 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.9164
2022-03-02 02:39:16 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.9085
2022-03-02 02:39:58 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.9597
2022-03-02 02:40:40 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.7714
2022-03-02 02:41:22 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.8262
2022-03-02 02:42:03 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.6308
2022-03-02 02:42:45 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.8005
2022-03-02 02:43:27 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.6131
2022-03-02 02:44:08 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.5473
2022-03-02 02:44:50 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.3278
2022-03-02 02:45:32 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.6534
2022-03-02 02:46:14 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.6609
2022-03-02 02:46:56 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.3013
2022-03-02 02:47:37 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.6129
2022-03-02 02:48:19 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.4143
2022-03-02 02:49:02 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.1184
2022-03-02 02:49:04 - train: epoch 001, train_loss: 5.6116
2022-03-02 02:50:20 - eval: epoch: 001, acc1: 18.676%, acc5: 39.618%, test_loss: 4.1539, per_image_load_time: 1.998ms, per_image_inference_time: 0.952ms
2022-03-02 02:50:20 - until epoch: 001, best_acc1: 18.676%
2022-03-02 02:50:20 - epoch 002 lr: 0.1
2022-03-02 02:51:07 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.3507
2022-03-02 02:51:48 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.0738
2022-03-02 02:52:30 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.4423
2022-03-02 02:53:11 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.3515
2022-03-02 02:53:53 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.0377
2022-03-02 02:54:34 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.0212
2022-03-02 02:55:16 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.3718
2022-03-02 02:55:57 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.8881
2022-03-02 02:56:39 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.7148
2022-03-02 02:57:20 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.3662
2022-03-02 02:58:02 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.1708
2022-03-02 02:58:43 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.0202
2022-03-02 02:59:25 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.9925
2022-03-02 03:00:06 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.1600
2022-03-02 03:00:48 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.2041
2022-03-02 03:01:29 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.9424
2022-03-02 03:02:11 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.0014
2022-03-02 03:02:52 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.9851
2022-03-02 03:03:34 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.9597
2022-03-02 03:04:15 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.6896
2022-03-02 03:04:57 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.9689
2022-03-02 03:05:39 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.5883
2022-03-02 03:06:20 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.9504
2022-03-02 03:07:02 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.6373
2022-03-02 03:07:43 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.6730
2022-03-02 03:08:25 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.7674
2022-03-02 03:09:06 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 4.0573
2022-03-02 03:09:48 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.9549
2022-03-02 03:10:30 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.6769
2022-03-02 03:11:11 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.6702
2022-03-02 03:11:53 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.6999
2022-03-02 03:12:35 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.5587
2022-03-02 03:13:16 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.6811
2022-03-02 03:13:58 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.7639
2022-03-02 03:14:39 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.6186
2022-03-02 03:15:21 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.6372
2022-03-02 03:16:02 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.7531
2022-03-02 03:16:44 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.5470
2022-03-02 03:17:25 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.6431
2022-03-02 03:18:07 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.5578
2022-03-02 03:18:49 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.6794
2022-03-02 03:19:30 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.4646
2022-03-02 03:20:12 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.5743
2022-03-02 03:20:54 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.3912
2022-03-02 03:21:35 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.5199
2022-03-02 03:22:17 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.4818
2022-03-02 03:22:59 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.4733
2022-03-02 03:23:40 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.4754
2022-03-02 03:24:22 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.3774
2022-03-02 03:25:04 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.4230
2022-03-02 03:25:06 - train: epoch 002, train_loss: 3.8344
2022-03-02 03:26:22 - eval: epoch: 002, acc1: 30.928%, acc5: 56.422%, test_loss: 3.2737, per_image_load_time: 1.537ms, per_image_inference_time: 0.963ms
2022-03-02 03:26:23 - until epoch: 002, best_acc1: 30.928%
2022-03-02 03:26:23 - epoch 003 lr: 0.1
2022-03-02 03:27:10 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.4404
2022-03-02 03:27:51 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.4548
2022-03-02 03:28:32 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.3561
2022-03-02 03:29:14 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.3164
2022-03-02 03:29:56 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.6857
2022-03-02 03:30:37 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.4096
2022-03-02 03:31:19 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.6079
2022-03-02 03:32:01 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.4837
2022-03-02 03:32:42 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.3663
2022-03-02 03:33:24 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.2330
2022-03-02 03:34:05 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.2380
2022-03-02 03:34:47 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.5526
2022-03-02 03:35:29 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.3132
2022-03-02 03:36:10 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.3411
2022-03-02 03:36:52 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.5720
2022-03-02 03:37:33 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.3159
2022-03-02 03:38:15 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.2463
2022-03-02 03:38:56 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.2088
2022-03-02 03:39:38 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.3646
2022-03-02 03:40:20 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.7618
2022-03-02 03:41:01 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.4840
2022-03-02 03:41:43 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.6624
2022-03-02 03:42:24 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.3200
2022-03-02 03:43:06 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.3181
2022-03-02 03:43:47 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.4749
2022-03-02 03:44:29 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.2977
2022-03-02 03:45:10 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.5752
2022-03-02 03:45:52 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.2073
2022-03-02 03:46:34 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.2253
2022-03-02 03:47:15 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.4566
2022-03-02 03:47:57 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.5421
2022-03-02 03:48:39 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.3185
2022-03-02 03:49:21 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.1683
2022-03-02 03:50:03 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.3553
2022-03-02 03:50:45 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.9678
2022-03-02 03:51:27 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.1669
2022-03-02 03:52:08 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.2411
2022-03-02 03:52:50 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.3232
2022-03-02 03:53:32 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.4902
2022-03-02 03:54:14 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.1566
2022-03-02 03:54:55 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.3375
2022-03-02 03:55:37 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.0902
2022-03-02 03:56:19 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.9606
2022-03-02 03:57:01 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.0529
2022-03-02 03:57:43 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.2580
2022-03-02 03:58:25 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.1748
2022-03-02 03:59:06 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.0288
2022-03-02 03:59:48 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.2292
2022-03-02 04:00:30 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.1856
2022-03-02 04:01:12 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.1653
2022-03-02 04:01:14 - train: epoch 003, train_loss: 3.3041
2022-03-02 04:02:31 - eval: epoch: 003, acc1: 36.516%, acc5: 62.042%, test_loss: 2.9486, per_image_load_time: 1.954ms, per_image_inference_time: 0.971ms
2022-03-02 04:02:31 - until epoch: 003, best_acc1: 36.516%
2022-03-02 04:02:31 - epoch 004 lr: 0.1
2022-03-02 04:03:18 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.2138
2022-03-02 04:04:00 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.1560
2022-03-02 04:04:42 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.0178
2022-03-02 04:05:23 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 3.0286
2022-03-02 04:06:05 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.0198
2022-03-02 04:06:46 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.3643
2022-03-02 04:07:28 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.1195
2022-03-02 04:08:10 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.9298
2022-03-02 04:08:51 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.8617
2022-03-02 04:09:33 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.2569
2022-03-02 04:10:14 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.1115
2022-03-02 04:10:56 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.8332
2022-03-02 04:11:38 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.9130
2022-03-02 04:12:19 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.0947
2022-03-02 04:13:01 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.0853
2022-03-02 04:13:43 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 3.0032
2022-03-02 04:14:24 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.1108
2022-03-02 04:15:06 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.1973
2022-03-02 04:15:48 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.2029
2022-03-02 04:16:30 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.1158
2022-03-02 04:17:11 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.1322
2022-03-02 04:17:53 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.0638
2022-03-02 04:18:35 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.9038
2022-03-02 04:19:17 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.8581
2022-03-02 04:19:58 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.9158
2022-03-02 04:20:40 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.1506
2022-03-02 04:21:22 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.8649
2022-03-02 04:22:03 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.0360
2022-03-02 04:22:45 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.9737
2022-03-02 04:23:27 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 3.0534
2022-03-02 04:24:08 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.0140
2022-03-02 04:24:50 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 3.0789
2022-03-02 04:25:32 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.0180
2022-03-02 04:26:14 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.9731
2022-03-02 04:26:55 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.8896
2022-03-02 04:27:37 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.7334
2022-03-02 04:28:19 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.9711
2022-03-02 04:29:00 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.8680
2022-03-02 04:29:42 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.9210
2022-03-02 04:30:24 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.7944
2022-03-02 04:31:06 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 3.0001
2022-03-02 04:31:48 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.9721
2022-03-02 04:32:30 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.9449
2022-03-02 04:33:12 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.7089
2022-03-02 04:33:54 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.5216
2022-03-02 04:34:35 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.9373
2022-03-02 04:35:17 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.8976
2022-03-02 04:35:59 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.7326
2022-03-02 04:36:41 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 3.0167
2022-03-02 04:37:23 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 3.0495
2022-03-02 04:37:25 - train: epoch 004, train_loss: 3.0377
2022-03-02 04:38:41 - eval: epoch: 004, acc1: 41.244%, acc5: 67.104%, test_loss: 2.6680, per_image_load_time: 1.989ms, per_image_inference_time: 0.955ms
2022-03-02 04:38:42 - until epoch: 004, best_acc1: 41.244%
2022-03-02 04:38:42 - epoch 005 lr: 0.1
2022-03-02 04:39:29 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.9895
2022-03-02 04:40:11 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.8736
2022-03-02 04:40:52 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.0553
2022-03-02 04:41:34 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.9482
2022-03-02 04:42:16 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.7763
2022-03-02 04:42:57 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.9351
2022-03-02 04:43:39 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.9324
2022-03-02 04:44:20 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.1964
2022-03-02 04:45:02 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.9488
2022-03-02 04:45:44 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.9469
2022-03-02 04:46:25 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.9181
2022-03-02 04:47:07 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.8822
2022-03-02 04:47:49 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.8857
2022-03-02 04:48:31 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.0114
2022-03-02 04:49:12 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.7341
2022-03-02 04:49:54 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.6312
2022-03-02 04:50:36 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.7559
2022-03-02 04:51:17 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 3.0146
2022-03-02 04:51:59 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.6419
2022-03-02 04:52:41 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.8604
2022-03-02 04:53:23 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.7782
2022-03-02 04:54:05 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.8413
2022-03-02 04:54:46 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.7436
2022-03-02 04:55:28 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.7756
2022-03-02 04:56:09 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.8714
2022-03-02 04:56:51 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.0461
2022-03-02 04:57:33 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.9729
2022-03-02 04:58:14 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.8812
2022-03-02 04:58:56 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.6495
2022-03-02 04:59:38 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.8819
2022-03-02 05:00:19 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.9057
2022-03-02 05:01:01 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.9590
2022-03-02 05:01:43 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.6599
2022-03-02 05:02:24 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.7750
2022-03-02 05:03:06 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.9052
2022-03-02 05:03:48 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.8776
2022-03-02 05:04:30 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.7722
2022-03-02 05:05:12 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.7035
2022-03-02 05:05:53 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.1232
2022-03-02 05:06:35 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.8671
2022-03-02 05:07:17 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.8365
2022-03-02 05:07:59 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.9147
2022-03-02 05:08:41 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.7569
2022-03-02 05:09:23 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.9335
2022-03-02 05:10:04 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.9554
2022-03-02 05:10:46 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.8203
2022-03-02 05:11:28 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.6985
2022-03-02 05:12:10 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.6132
2022-03-02 05:12:52 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.9870
2022-03-02 05:13:34 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.7185
2022-03-02 05:13:36 - train: epoch 005, train_loss: 2.8779
2022-03-02 05:14:52 - eval: epoch: 005, acc1: 43.478%, acc5: 69.432%, test_loss: 2.5355, per_image_load_time: 1.047ms, per_image_inference_time: 0.956ms
2022-03-02 05:14:53 - until epoch: 005, best_acc1: 43.478%
2022-03-02 05:14:53 - epoch 006 lr: 0.1
2022-03-02 05:15:40 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.6520
2022-03-02 05:16:22 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.8137
2022-03-02 05:17:03 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.6295
2022-03-02 05:17:45 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.8674
2022-03-02 05:18:26 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.7811
2022-03-02 05:19:08 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.7981
2022-03-02 05:19:50 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.8554
2022-03-02 05:20:32 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.8626
2022-03-02 05:21:14 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.6981
2022-03-02 05:21:56 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.6161
2022-03-02 05:22:37 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.6207
2022-03-02 05:23:19 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.8818
2022-03-02 05:24:01 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.9477
2022-03-02 05:24:43 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.8387
2022-03-02 05:25:25 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.9744
2022-03-02 05:26:07 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.6657
2022-03-02 05:26:49 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.8355
2022-03-02 05:27:31 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.8309
2022-03-02 05:28:13 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.7086
2022-03-02 05:28:55 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.9081
2022-03-02 05:29:37 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.8752
2022-03-02 05:30:19 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.6822
2022-03-02 05:31:01 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.7011
2022-03-02 05:31:43 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.7312
2022-03-02 05:32:25 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.9447
2022-03-02 05:33:07 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.7578
2022-03-02 05:33:49 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.8306
2022-03-02 05:34:32 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.7059
2022-03-02 05:35:14 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.8799
2022-03-02 05:35:56 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.6732
2022-03-02 05:36:38 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.5381
2022-03-02 05:37:20 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.6884
2022-03-02 05:38:02 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.5883
2022-03-02 05:38:44 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.9652
2022-03-02 05:39:26 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.8400
2022-03-02 05:40:08 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.7614
2022-03-02 05:40:51 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.8244
2022-03-02 05:41:33 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.5913
2022-03-02 05:42:15 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.6457
2022-03-02 05:42:57 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.0312
2022-03-02 05:43:39 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.5738
2022-03-02 05:44:21 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.5510
2022-03-02 05:45:03 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.6544
2022-03-02 05:45:45 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.7484
2022-03-02 05:46:27 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.8139
2022-03-02 05:47:09 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.7594
2022-03-02 05:47:51 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.7382
2022-03-02 05:48:33 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.8281
2022-03-02 05:49:14 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.8373
2022-03-02 05:49:56 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.5386
2022-03-02 05:49:58 - train: epoch 006, train_loss: 2.7713
2022-03-02 05:51:14 - eval: epoch: 006, acc1: 45.176%, acc5: 70.816%, test_loss: 2.4478, per_image_load_time: 1.846ms, per_image_inference_time: 0.958ms
2022-03-02 05:51:14 - until epoch: 006, best_acc1: 45.176%
2022-03-02 05:51:14 - epoch 007 lr: 0.1
2022-03-02 05:52:02 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.4985
2022-03-02 05:52:43 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.9519
2022-03-02 05:53:25 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.9902
2022-03-02 05:54:06 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.7370
2022-03-02 05:54:48 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.5782
2022-03-02 05:55:30 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.8846
2022-03-02 05:56:12 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.6495
2022-03-02 05:56:53 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.6334
2022-03-02 05:57:35 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.7618
2022-03-02 05:58:17 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.7367
2022-03-02 05:58:59 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.5573
2022-03-02 05:59:41 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.6845
2022-03-02 06:00:23 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.5858
2022-03-02 06:01:05 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.7991
2022-03-02 06:01:47 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.8273
2022-03-02 06:02:29 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.6798
2022-03-02 06:03:11 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.7737
2022-03-02 06:03:53 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.6197
2022-03-02 06:04:35 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.7362
2022-03-02 06:05:17 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.5199
2022-03-02 06:05:59 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.8343
2022-03-02 06:06:41 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.4909
2022-03-02 06:07:23 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.8348
2022-03-02 06:08:05 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.7555
2022-03-02 06:08:48 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.5636
2022-03-02 06:09:30 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.5446
2022-03-02 06:10:12 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.4900
2022-03-02 06:10:54 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.6497
2022-03-02 06:11:36 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.6297
2022-03-02 06:12:18 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.7737
2022-03-02 06:13:00 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.5419
2022-03-02 06:13:42 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.5326
2022-03-02 06:14:24 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.9469
2022-03-02 06:15:07 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.5420
2022-03-02 06:15:49 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.7808
2022-03-02 06:16:31 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.5110
2022-03-02 06:17:13 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.8137
2022-03-02 06:17:55 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.9242
2022-03-02 06:18:37 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.4765
2022-03-02 06:19:19 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.7892
2022-03-02 06:20:01 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.5582
2022-03-02 06:20:43 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.5526
2022-03-02 06:21:25 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.8382
2022-03-02 06:22:08 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.6694
2022-03-02 06:22:50 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.9477
2022-03-02 06:23:32 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.6949
2022-03-02 06:24:15 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.8153
2022-03-02 06:24:57 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.9195
2022-03-02 06:25:39 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.6557
2022-03-02 06:26:21 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.6716
2022-03-02 06:26:24 - train: epoch 007, train_loss: 2.6908
2022-03-02 06:27:39 - eval: epoch: 007, acc1: 46.964%, acc5: 72.640%, test_loss: 2.3447, per_image_load_time: 1.957ms, per_image_inference_time: 0.965ms
2022-03-02 06:27:40 - until epoch: 007, best_acc1: 46.964%
2022-03-02 06:27:40 - epoch 008 lr: 0.1
2022-03-02 06:28:27 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.6415
2022-03-02 06:29:08 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.7287
2022-03-02 06:29:50 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.4303
2022-03-02 06:30:31 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.4762
2022-03-02 06:31:13 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.4506
2022-03-02 06:31:54 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.6357
2022-03-02 06:32:36 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 3.0435
2022-03-02 06:33:17 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.5624
2022-03-02 06:33:59 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.6521
2022-03-02 06:34:40 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.5818
2022-03-02 06:35:22 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.4602
2022-03-02 06:36:03 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.4271
2022-03-02 06:36:45 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.7270
2022-03-02 06:37:26 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.6881
2022-03-02 06:38:07 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.8149
2022-03-02 06:38:49 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.6856
2022-03-02 06:39:31 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.6107
2022-03-02 06:40:12 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.7471
2022-03-02 06:40:53 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.5330
2022-03-02 06:41:35 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.6908
2022-03-02 06:42:16 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.5701
2022-03-02 06:42:58 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.5775
2022-03-02 06:43:39 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.5506
2022-03-02 06:44:21 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.5049
2022-03-02 06:45:02 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.5803
2022-03-02 06:45:44 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.7523
2022-03-02 06:46:25 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.7922
2022-03-02 06:47:06 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.7111
2022-03-02 06:47:48 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.5623
2022-03-02 06:48:29 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.6912
2022-03-02 06:49:11 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.4918
2022-03-02 06:49:52 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.8799
2022-03-02 06:50:34 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.7677
2022-03-02 06:51:15 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.8820
2022-03-02 06:51:57 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.6163
2022-03-02 06:52:38 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.7659
2022-03-02 06:53:20 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.5274
2022-03-02 06:54:01 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.6044
2022-03-02 06:54:43 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.7140
2022-03-02 06:55:24 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.9233
2022-03-02 06:56:06 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.5535
2022-03-02 06:56:47 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.5120
2022-03-02 06:57:28 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.3417
2022-03-02 06:58:10 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.6107
2022-03-02 06:58:51 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.6872
2022-03-02 06:59:33 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.8613
2022-03-02 07:00:14 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.4631
2022-03-02 07:00:56 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.5588
2022-03-02 07:01:37 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.6540
2022-03-02 07:02:19 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.5305
2022-03-02 07:02:21 - train: epoch 008, train_loss: 2.6285
2022-03-02 07:03:38 - eval: epoch: 008, acc1: 47.444%, acc5: 73.258%, test_loss: 2.3055, per_image_load_time: 1.004ms, per_image_inference_time: 0.965ms
2022-03-02 07:03:39 - until epoch: 008, best_acc1: 47.444%
2022-03-02 07:03:39 - epoch 009 lr: 0.1
2022-03-02 07:04:26 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.3144
2022-03-02 07:05:07 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.4997
2022-03-02 07:05:48 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.2664
2022-03-02 07:06:30 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.7348
2022-03-02 07:07:11 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.5466
2022-03-02 07:07:53 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.6302
2022-03-02 07:08:34 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.5039
2022-03-02 07:09:16 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.5207
2022-03-02 07:09:57 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.5397
2022-03-02 07:10:39 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.5506
2022-03-02 07:11:20 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.8032
2022-03-02 07:12:02 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.7147
2022-03-02 07:12:44 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.6702
2022-03-02 07:13:25 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.4322
2022-03-02 07:14:07 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.4547
2022-03-02 07:14:48 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.5808
2022-03-02 07:15:30 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.6024
2022-03-02 07:16:11 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.5897
2022-03-02 07:16:53 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.3835
2022-03-02 07:17:34 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.3789
2022-03-02 07:18:16 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.6304
2022-03-02 07:18:58 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.8582
2022-03-02 07:19:39 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.3420
2022-03-02 07:20:21 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.5250
2022-03-02 07:21:02 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.4379
2022-03-02 07:21:44 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.6632
2022-03-02 07:22:25 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.5194
2022-03-02 07:23:07 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.6841
2022-03-02 07:23:49 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.3132
2022-03-02 07:24:30 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.4381
2022-03-02 07:25:12 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.7392
2022-03-02 07:25:54 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.6156
2022-03-02 07:26:36 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.5240
2022-03-02 07:27:17 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.8528
2022-03-02 07:27:59 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.7424
2022-03-02 07:28:41 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.4858
2022-03-02 07:29:22 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.8099
2022-03-02 07:30:04 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.7194
2022-03-02 07:30:46 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.1831
2022-03-02 07:31:28 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.7719
2022-03-02 07:32:10 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.6080
2022-03-02 07:32:52 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.4515
2022-03-02 07:33:33 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.7589
2022-03-02 07:34:15 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.4796
2022-03-02 07:34:57 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.5893
2022-03-02 07:35:39 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.7098
2022-03-02 07:36:21 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.6600
2022-03-02 07:37:03 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.7142
2022-03-02 07:37:45 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.6464
2022-03-02 07:38:26 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.5228
2022-03-02 07:38:29 - train: epoch 009, train_loss: 2.5807
2022-03-02 07:39:45 - eval: epoch: 009, acc1: 48.380%, acc5: 73.744%, test_loss: 2.2750, per_image_load_time: 1.720ms, per_image_inference_time: 0.962ms
2022-03-02 07:39:45 - until epoch: 009, best_acc1: 48.380%
2022-03-02 07:39:45 - epoch 010 lr: 0.1
2022-03-02 07:40:33 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.5482
2022-03-02 07:41:14 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.5657
2022-03-02 07:41:55 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.5460
2022-03-02 07:42:37 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.5930
2022-03-02 07:43:18 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.4059
2022-03-02 07:44:00 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.6469
2022-03-02 07:44:41 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.6712
2022-03-02 07:45:23 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.3929
2022-03-02 07:46:04 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.3203
2022-03-02 07:46:46 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.4116
2022-03-02 07:47:27 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.5264
2022-03-02 07:48:09 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.3053
2022-03-02 07:48:50 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.4393
2022-03-02 07:49:32 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.5220
2022-03-02 07:50:13 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.2835
2022-03-02 07:50:55 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.5745
2022-03-02 07:51:37 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.7417
2022-03-02 07:52:18 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.5530
2022-03-02 07:53:00 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.4096
2022-03-02 07:53:42 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.5735
2022-03-02 07:54:24 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.5098
2022-03-02 07:55:06 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.7856
2022-03-02 07:55:47 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.7780
2022-03-02 07:56:29 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.5921
2022-03-02 07:57:11 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.5681
2022-03-02 07:57:53 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.7008
2022-03-02 07:58:35 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.3745
2022-03-02 07:59:17 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.5924
2022-03-02 07:59:58 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.6281
2022-03-02 08:00:40 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.4995
2022-03-02 08:01:22 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.6723
2022-03-02 08:02:03 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.5968
2022-03-02 08:02:45 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.8283
2022-03-02 08:03:27 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.6939
2022-03-02 08:04:09 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.7307
2022-03-02 08:04:51 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.7671
2022-03-02 08:05:32 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.3655
2022-03-02 08:06:14 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.6251
2022-03-02 08:06:56 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.2847
2022-03-02 08:07:38 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.5192
2022-03-02 08:08:19 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.4117
2022-03-02 08:09:01 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.7150
2022-03-02 08:09:43 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.6015
2022-03-02 08:10:25 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.5444
2022-03-02 08:11:07 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.3477
2022-03-02 08:11:49 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.6548
2022-03-02 08:12:30 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.4807
2022-03-02 08:13:12 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.3888
2022-03-02 08:13:54 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.4399
2022-03-02 08:14:36 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.2816
2022-03-02 08:14:38 - train: epoch 010, train_loss: 2.5402
2022-03-02 08:15:54 - eval: epoch: 010, acc1: 48.444%, acc5: 74.112%, test_loss: 2.2511, per_image_load_time: 1.963ms, per_image_inference_time: 0.966ms
2022-03-02 08:15:54 - until epoch: 010, best_acc1: 48.444%
2022-03-02 08:15:54 - epoch 011 lr: 0.1
2022-03-02 08:16:41 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.3178
2022-03-02 08:17:23 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.5904
2022-03-02 08:18:04 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.3484
2022-03-02 08:18:46 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.6033
2022-03-02 08:19:27 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.3848
2022-03-02 08:20:09 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.5433
2022-03-02 08:20:50 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.5240
2022-03-02 08:21:32 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.5588
2022-03-02 08:22:13 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.5931
2022-03-02 08:22:55 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.4186
2022-03-02 08:23:37 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.6049
2022-03-02 08:24:18 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.8742
2022-03-02 08:25:00 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.7254
2022-03-02 08:25:41 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.6381
2022-03-02 08:26:23 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.4530
2022-03-02 08:27:04 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.4784
2022-03-02 08:27:46 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.5951
2022-03-02 08:28:28 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.4637
2022-03-02 08:29:09 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.2548
2022-03-02 08:29:51 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.5633
2022-03-02 08:30:33 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.5483
2022-03-02 08:31:15 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.4620
2022-03-02 08:31:57 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.7535
2022-03-02 08:32:38 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.2375
2022-03-02 08:33:20 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.7628
2022-03-02 08:34:02 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.4769
2022-03-02 08:34:44 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.4323
2022-03-02 08:35:26 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.1826
2022-03-02 08:36:08 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.5308
2022-03-02 08:36:50 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.8618
2022-03-02 08:37:32 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.5674
2022-03-02 08:38:14 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.3149
2022-03-02 08:38:56 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.6243
2022-03-02 08:39:37 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.4598
2022-03-02 08:40:19 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.4984
2022-03-02 08:41:01 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.5040
2022-03-02 08:41:43 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.7162
2022-03-02 08:42:25 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.2340
2022-03-02 08:43:07 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.5247
2022-03-02 08:43:48 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.4469
2022-03-02 08:44:30 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.2873
2022-03-02 08:45:12 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.4420
2022-03-02 08:45:54 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.4740
2022-03-02 08:46:36 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.3938
2022-03-02 08:47:17 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.3125
2022-03-02 08:47:59 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.3773
2022-03-02 08:48:41 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.2616
2022-03-02 08:49:23 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.3473
2022-03-02 08:50:05 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.3121
2022-03-02 08:50:47 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.5047
2022-03-02 08:50:49 - train: epoch 011, train_loss: 2.5072
2022-03-02 08:52:05 - eval: epoch: 011, acc1: 49.596%, acc5: 75.258%, test_loss: 2.1987, per_image_load_time: 1.197ms, per_image_inference_time: 0.956ms
2022-03-02 08:52:05 - until epoch: 011, best_acc1: 49.596%
2022-03-02 08:52:05 - epoch 012 lr: 0.1
2022-03-02 08:52:53 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.4291
2022-03-02 08:53:34 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.4333
2022-03-02 08:54:16 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.4991
2022-03-02 08:54:58 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.5625
2022-03-02 08:55:40 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.8083
2022-03-02 08:56:21 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.3444
2022-03-02 08:57:03 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.3847
2022-03-02 08:57:45 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.4954
2022-03-02 08:58:27 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.5396
2022-03-02 08:59:09 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.2047
2022-03-02 08:59:51 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.9196
2022-03-02 09:00:33 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.3905
2022-03-02 09:01:14 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.4588
2022-03-02 09:01:56 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.5879
2022-03-02 09:02:38 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.3494
2022-03-02 09:03:20 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.4747
2022-03-02 09:04:02 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.3246
2022-03-02 09:04:44 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.4568
2022-03-02 09:05:27 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.5112
2022-03-02 09:06:09 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.6355
2022-03-02 09:06:51 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.5375
2022-03-02 09:07:33 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.6408
2022-03-02 09:08:15 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.4980
2022-03-02 09:08:57 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.5909
2022-03-02 09:09:39 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.3097
2022-03-02 09:10:22 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.3150
2022-03-02 09:11:04 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.3582
2022-03-02 09:11:46 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.4464
2022-03-02 09:12:28 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.3098
2022-03-02 09:13:10 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.3781
2022-03-02 09:13:52 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.5156
2022-03-02 09:14:34 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.1425
2022-03-02 09:15:16 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.3576
2022-03-02 09:15:58 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.4743
2022-03-02 09:16:40 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.6164
2022-03-02 09:17:22 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.5650
2022-03-02 09:18:04 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.5276
2022-03-02 09:18:46 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.4006
2022-03-02 09:19:28 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.3859
2022-03-02 09:20:10 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.3552
2022-03-02 09:20:52 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.3716
2022-03-02 09:21:34 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.3525
2022-03-02 09:22:16 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.6000
2022-03-02 09:22:58 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.3556
2022-03-02 09:23:40 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.4747
2022-03-02 09:24:22 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.7526
2022-03-02 09:25:04 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.3172
2022-03-02 09:25:46 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.5774
2022-03-02 09:26:28 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.4591
2022-03-02 09:27:10 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.1648
2022-03-02 09:27:12 - train: epoch 012, train_loss: 2.4812
2022-03-02 09:28:27 - eval: epoch: 012, acc1: 50.864%, acc5: 76.046%, test_loss: 2.1328, per_image_load_time: 1.064ms, per_image_inference_time: 0.961ms
2022-03-02 09:28:28 - until epoch: 012, best_acc1: 50.864%
2022-03-02 09:28:28 - epoch 013 lr: 0.1
2022-03-02 09:29:15 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.3330
2022-03-02 09:29:57 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.4150
2022-03-02 09:30:38 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.4071
2022-03-02 09:31:20 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.2598
2022-03-02 09:32:02 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.3611
2022-03-02 09:32:44 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.6436
2022-03-02 09:33:25 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.3671
2022-03-02 09:34:07 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.5356
2022-03-02 09:34:49 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.3421
2022-03-02 09:35:31 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.5831
2022-03-02 09:36:13 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.6088
2022-03-02 09:36:55 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.6532
2022-03-02 09:37:37 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.5756
2022-03-02 09:38:19 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.4283
2022-03-02 09:39:01 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.5375
2022-03-02 09:39:42 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.1269
2022-03-02 09:40:24 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.5675
2022-03-02 09:41:06 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.4095
2022-03-02 09:41:48 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.5010
2022-03-02 09:42:31 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.6884
2022-03-02 09:43:13 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.7456
2022-03-02 09:43:55 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.4423
2022-03-02 09:44:37 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.4190
2022-03-02 09:45:19 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.5199
2022-03-02 09:46:01 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.2798
2022-03-02 09:46:43 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.3245
2022-03-02 09:47:25 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.2781
2022-03-02 09:48:07 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.5183
2022-03-02 09:48:49 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.5735
2022-03-02 09:49:31 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.4097
2022-03-02 09:50:13 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.3497
2022-03-02 09:50:55 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.3979
2022-03-02 09:51:37 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.3390
2022-03-02 09:52:19 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.4856
2022-03-02 09:53:01 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.2662
2022-03-02 09:53:43 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.6656
2022-03-02 09:54:26 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.2677
2022-03-02 09:55:08 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.6260
2022-03-02 09:55:50 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.5709
2022-03-02 09:56:32 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.4545
2022-03-02 09:57:14 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.4446
2022-03-02 09:57:56 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.4582
2022-03-02 09:58:38 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.4003
2022-03-02 09:59:20 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.3752
2022-03-02 10:00:02 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.5178
2022-03-02 10:00:44 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.4585
2022-03-02 10:01:27 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.5681
2022-03-02 10:02:09 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.5291
2022-03-02 10:02:51 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.5055
2022-03-02 10:03:33 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.5296
2022-03-02 10:03:35 - train: epoch 013, train_loss: 2.4578
2022-03-02 10:04:51 - eval: epoch: 013, acc1: 50.986%, acc5: 76.294%, test_loss: 2.1147, per_image_load_time: 1.936ms, per_image_inference_time: 0.962ms
2022-03-02 10:04:52 - until epoch: 013, best_acc1: 50.986%
2022-03-02 10:04:52 - epoch 014 lr: 0.1
2022-03-02 10:05:39 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.3600
2022-03-02 10:06:21 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.5449
2022-03-02 10:07:02 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.1773
2022-03-02 10:07:44 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.3192
2022-03-02 10:08:25 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.2871
2022-03-02 10:09:07 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.5057
2022-03-02 10:09:49 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.3511
2022-03-02 10:10:30 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.3360
2022-03-02 10:11:12 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.5567
2022-03-02 10:11:53 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.5402
2022-03-02 10:12:35 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.3383
2022-03-02 10:13:16 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.5219
2022-03-02 10:13:57 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.4868
2022-03-02 10:14:39 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.4807
2022-03-02 10:15:20 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.6849
2022-03-02 10:16:02 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.3185
2022-03-02 10:16:43 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.5960
2022-03-02 10:17:25 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.5333
2022-03-02 10:18:06 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.2349
2022-03-02 10:18:48 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.2874
2022-03-02 10:19:30 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.5541
2022-03-02 10:20:11 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.4537
2022-03-02 10:20:53 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.4190
2022-03-02 10:21:34 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.6457
2022-03-02 10:22:16 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.4164
2022-03-02 10:22:58 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.5151
2022-03-02 10:23:39 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.3882
2022-03-02 10:24:21 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.7005
2022-03-02 10:25:02 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.3565
2022-03-02 10:25:44 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.5453
2022-03-02 10:26:26 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.2763
2022-03-02 10:27:07 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.4168
2022-03-02 10:27:49 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.2684
2022-03-02 10:28:30 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.4313
2022-03-02 10:29:12 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.5203
2022-03-02 10:29:54 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.3783
2022-03-02 10:30:36 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.4072
2022-03-02 10:31:18 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.6660
2022-03-02 10:31:59 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.4258
2022-03-02 10:32:41 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.6499
2022-03-02 10:33:23 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.4604
2022-03-02 10:34:05 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.2575
2022-03-02 10:34:46 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.3666
2022-03-02 10:35:28 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.2093
2022-03-02 10:36:10 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.4773
2022-03-02 10:36:52 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.3540
2022-03-02 10:37:33 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.3568
2022-03-02 10:38:15 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.3371
2022-03-02 10:38:57 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.1492
2022-03-02 10:39:39 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.3758
2022-03-02 10:39:41 - train: epoch 014, train_loss: 2.4362
2022-03-02 10:40:57 - eval: epoch: 014, acc1: 49.970%, acc5: 75.360%, test_loss: 2.1715, per_image_load_time: 1.758ms, per_image_inference_time: 0.963ms
2022-03-02 10:40:58 - until epoch: 014, best_acc1: 50.986%
2022-03-02 10:40:58 - epoch 015 lr: 0.1
2022-03-02 10:41:45 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.1725
2022-03-02 10:42:27 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.5313
2022-03-02 10:43:08 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.6238
2022-03-02 10:43:50 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.4705
2022-03-02 10:44:31 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.3026
2022-03-02 10:45:13 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.6066
2022-03-02 10:45:54 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.3150
2022-03-02 10:46:36 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.3167
2022-03-02 10:47:18 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.2747
2022-03-02 10:47:59 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.5638
2022-03-02 10:48:41 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.3544
2022-03-02 10:49:23 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.3913
2022-03-02 10:50:04 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.5198
2022-03-02 10:50:46 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.2841
2022-03-02 10:51:28 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.0936
2022-03-02 10:52:09 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.3268
2022-03-02 10:52:51 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.6631
2022-03-02 10:53:33 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.2838
2022-03-02 10:54:14 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.3144
2022-03-02 10:54:56 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.3360
2022-03-02 10:55:37 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.2860
2022-03-02 10:56:19 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.5083
2022-03-02 10:57:01 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.3138
2022-03-02 10:57:43 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.4482
2022-03-02 10:58:25 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.4727
2022-03-02 10:59:07 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.1838
2022-03-02 10:59:49 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.3964
2022-03-02 11:00:30 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.6157
2022-03-02 11:01:12 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.4424
2022-03-02 11:01:54 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.2354
2022-03-02 11:02:36 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.4051
2022-03-02 11:03:18 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.3577
2022-03-02 11:03:59 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.2302
2022-03-02 11:04:41 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.5058
2022-03-02 11:05:23 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.6742
2022-03-02 11:06:05 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.3858
2022-03-02 11:06:47 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.2825
2022-03-02 11:07:29 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.3507
2022-03-02 11:08:10 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.5475
2022-03-02 11:08:52 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.4839
2022-03-02 11:09:34 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.6541
2022-03-02 11:10:16 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.0646
2022-03-02 11:10:58 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.4201
2022-03-02 11:11:39 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.3425
2022-03-02 11:12:21 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.4414
2022-03-02 11:13:03 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.4201
2022-03-02 11:13:45 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.5585
2022-03-02 11:14:26 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.4592
2022-03-02 11:15:08 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.4630
2022-03-02 11:15:50 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.5513
2022-03-02 11:15:52 - train: epoch 015, train_loss: 2.4198
2022-03-02 11:17:08 - eval: epoch: 015, acc1: 51.236%, acc5: 76.358%, test_loss: 2.1070, per_image_load_time: 1.215ms, per_image_inference_time: 0.966ms
2022-03-02 11:17:08 - until epoch: 015, best_acc1: 51.236%
2022-03-02 11:17:08 - epoch 016 lr: 0.1
2022-03-02 11:17:55 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.3881
2022-03-02 11:18:37 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.3104
2022-03-02 11:19:18 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.3353
2022-03-02 11:19:59 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.6068
2022-03-02 11:20:41 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.2242
2022-03-02 11:21:22 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.5530
2022-03-02 11:22:04 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.0984
2022-03-02 11:22:45 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.3945
2022-03-02 11:23:27 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.4795
2022-03-02 11:24:09 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.2903
2022-03-02 11:24:50 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.2586
2022-03-02 11:25:32 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.3082
2022-03-02 11:26:13 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.5359
2022-03-02 11:26:55 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.2703
2022-03-02 11:27:36 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.4363
2022-03-02 11:28:18 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.4937
2022-03-02 11:28:59 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.3973
2022-03-02 11:29:41 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.3696
2022-03-02 11:30:23 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.4812
2022-03-02 11:31:04 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.0063
2022-03-02 11:31:46 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.5196
2022-03-02 11:32:28 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.5313
2022-03-02 11:33:10 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.6396
2022-03-02 11:33:52 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.6036
2022-03-02 11:34:34 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.2779
2022-03-02 11:35:15 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.5445
2022-03-02 11:35:57 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.3906
2022-03-02 11:36:39 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.2351
2022-03-02 11:37:22 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.4568
2022-03-02 11:38:04 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.5762
2022-03-02 11:38:46 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.5493
2022-03-02 11:39:28 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.6047
2022-03-02 11:40:10 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.5473
2022-03-02 11:40:52 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.4326
2022-03-02 11:41:34 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.3219
2022-03-02 11:42:16 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.2539
2022-03-02 11:42:58 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.5109
2022-03-02 11:43:40 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.7413
2022-03-02 11:44:22 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.5655
2022-03-02 11:45:04 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.4566
2022-03-02 11:45:46 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.4000
2022-03-02 11:46:29 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.3977
2022-03-02 11:47:11 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.2520
2022-03-02 11:47:53 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.3243
2022-03-02 11:48:35 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.4669
2022-03-02 11:49:18 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.2962
2022-03-02 11:50:00 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.7094
2022-03-02 11:50:42 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.3513
2022-03-02 11:51:25 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.5050
2022-03-02 11:52:07 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.5189
2022-03-02 11:52:09 - train: epoch 016, train_loss: 2.4037
2022-03-02 11:53:25 - eval: epoch: 016, acc1: 51.914%, acc5: 76.756%, test_loss: 2.0783, per_image_load_time: 1.947ms, per_image_inference_time: 0.967ms
2022-03-02 11:53:26 - until epoch: 016, best_acc1: 51.914%
2022-03-02 11:53:26 - epoch 017 lr: 0.1
2022-03-02 11:54:13 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.2233
2022-03-02 11:54:55 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.4976
2022-03-02 11:55:36 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.6073
2022-03-02 11:56:18 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.1432
2022-03-02 11:56:59 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.3655
2022-03-02 11:57:41 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.6896
2022-03-02 11:58:22 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.3115
2022-03-02 11:59:04 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.2256
2022-03-02 11:59:45 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.4695
2022-03-02 12:00:27 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.2735
2022-03-02 12:01:09 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.7914
2022-03-02 12:01:50 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.7088
2022-03-02 12:02:32 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.3626
2022-03-02 12:03:13 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.4333
2022-03-02 12:03:55 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.1571
2022-03-02 12:04:37 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.1571
2022-03-02 12:05:19 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.3216
2022-03-02 12:06:00 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.3285
2022-03-02 12:06:42 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.2466
2022-03-02 12:07:24 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.6022
2022-03-02 12:08:06 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.4236
2022-03-02 12:08:47 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.3043
2022-03-02 12:09:29 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.3345
2022-03-02 12:10:11 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.2819
2022-03-02 12:10:52 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.4838
2022-03-02 12:11:34 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.2935
2022-03-02 12:12:16 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.3255
2022-03-02 12:12:58 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.6713
2022-03-02 12:13:40 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.5784
2022-03-02 12:14:22 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.2520
2022-03-02 12:15:04 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.5606
2022-03-02 12:15:46 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.3089
2022-03-02 12:16:28 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.4434
2022-03-02 12:17:11 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.2020
2022-03-02 12:17:53 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.3051
2022-03-02 12:18:35 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.5918
2022-03-02 12:19:17 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.3496
2022-03-02 12:19:59 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.5518
2022-03-02 12:20:41 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.2848
2022-03-02 12:21:24 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.3906
2022-03-02 12:22:06 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.3987
2022-03-02 12:22:48 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.3020
2022-03-02 12:23:30 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.3352
2022-03-02 12:24:12 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.3738
2022-03-02 12:24:54 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.4805
2022-03-02 12:25:36 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.3926
2022-03-02 12:26:18 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.4799
2022-03-02 12:27:00 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.4587
2022-03-02 12:27:43 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.1914
2022-03-02 12:28:25 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.2914
2022-03-02 12:28:27 - train: epoch 017, train_loss: 2.3878
2022-03-02 12:29:42 - eval: epoch: 017, acc1: 51.432%, acc5: 76.798%, test_loss: 2.0893, per_image_load_time: 1.890ms, per_image_inference_time: 0.950ms
2022-03-02 12:29:43 - until epoch: 017, best_acc1: 51.914%
2022-03-02 12:29:43 - epoch 018 lr: 0.1
2022-03-02 12:30:31 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.2876
2022-03-02 12:31:12 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.3232
2022-03-02 12:31:54 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.5395
2022-03-02 12:32:35 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.5033
2022-03-02 12:33:17 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.3327
2022-03-02 12:33:58 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.4878
2022-03-02 12:34:40 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.2700
2022-03-02 12:35:21 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.3024
2022-03-02 12:36:03 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.4296
2022-03-02 12:36:45 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.2127
2022-03-02 12:37:27 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.6240
2022-03-02 12:38:09 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.3385
2022-03-02 12:38:51 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.6994
2022-03-02 12:39:32 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.4385
2022-03-02 12:40:14 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.4927
2022-03-02 12:40:56 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.3215
2022-03-02 12:41:38 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.2345
2022-03-02 12:42:20 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.1342
2022-03-02 12:43:02 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.3722
2022-03-02 12:43:44 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.6736
2022-03-02 12:44:26 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.6436
2022-03-02 12:45:08 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.3759
2022-03-02 12:45:51 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.4309
2022-03-02 12:46:33 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.2399
2022-03-02 12:47:15 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.1362
2022-03-02 12:47:57 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.2204
2022-03-02 12:48:39 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.5577
2022-03-02 12:49:21 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.1619
2022-03-02 12:50:03 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.4214
2022-03-02 12:50:45 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.2641
2022-03-02 12:51:27 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.7807
2022-03-02 12:52:09 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.1967
2022-03-02 12:52:52 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.3539
2022-03-02 12:53:34 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.3500
2022-03-02 12:54:16 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.5235
2022-03-02 12:54:58 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.3951
2022-03-02 12:55:40 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.8249
2022-03-02 12:56:22 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.5615
2022-03-02 12:57:05 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.4354
2022-03-02 12:57:47 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.2580
2022-03-02 12:58:29 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.3242
2022-03-02 12:59:12 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.4284
2022-03-02 12:59:54 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.2008
2022-03-02 13:00:36 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.5431
2022-03-02 13:01:19 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.3989
2022-03-02 13:02:01 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.2347
2022-03-02 13:02:44 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.5703
2022-03-02 13:03:27 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.5082
2022-03-02 13:04:09 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.3984
2022-03-02 13:04:51 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.4297
2022-03-02 13:04:53 - train: epoch 018, train_loss: 2.3751
2022-03-02 13:06:09 - eval: epoch: 018, acc1: 52.250%, acc5: 77.132%, test_loss: 2.0679, per_image_load_time: 1.150ms, per_image_inference_time: 0.972ms
2022-03-02 13:06:10 - until epoch: 018, best_acc1: 52.250%
2022-03-02 13:06:10 - epoch 019 lr: 0.1
2022-03-02 13:06:58 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.1637
2022-03-02 13:07:40 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.5775
2022-03-02 13:08:21 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.5280
2022-03-02 13:09:03 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.3472
2022-03-02 13:09:45 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.3933
2022-03-02 13:10:27 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.3171
2022-03-02 13:11:08 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.0955
2022-03-02 13:11:50 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.5718
2022-03-02 13:12:33 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.3363
2022-03-02 13:13:15 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.5251
2022-03-02 13:13:57 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.2003
2022-03-02 13:14:39 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.4847
2022-03-02 13:15:21 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.5607
2022-03-02 13:16:03 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.2815
2022-03-02 13:16:46 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.7958
2022-03-02 13:17:28 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.4027
2022-03-02 13:18:10 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.4571
2022-03-02 13:18:52 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.2775
2022-03-02 13:19:34 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.6129
2022-03-02 13:20:17 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.3173
2022-03-02 13:20:59 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.2049
2022-03-02 13:21:41 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.2937
2022-03-02 13:22:23 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.4271
2022-03-02 13:23:05 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.5051
2022-03-02 13:23:47 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.3682
2022-03-02 13:24:29 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.3685
2022-03-02 13:25:11 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.4824
2022-03-02 13:25:53 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.4451
2022-03-02 13:26:36 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.3983
2022-03-02 13:27:18 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.6515
2022-03-02 13:28:00 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.4474
2022-03-02 13:28:42 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.0874
2022-03-02 13:29:24 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.2818
2022-03-02 13:30:07 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.4112
2022-03-02 13:30:49 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.3762
2022-03-02 13:31:31 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.1364
2022-03-02 13:32:13 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.4719
2022-03-02 13:32:55 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.4972
2022-03-02 13:33:38 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.2451
2022-03-02 13:34:20 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.2496
2022-03-02 13:35:02 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.4173
2022-03-02 13:35:45 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.3668
2022-03-02 13:36:27 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.2930
2022-03-02 13:37:09 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.5153
2022-03-02 13:37:52 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.7008
2022-03-02 13:38:34 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.3883
2022-03-02 13:39:17 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.2929
2022-03-02 13:40:00 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.4050
2022-03-02 13:40:42 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.3840
2022-03-02 13:41:25 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.3487
2022-03-02 13:41:27 - train: epoch 019, train_loss: 2.3657
2022-03-02 13:42:43 - eval: epoch: 019, acc1: 52.698%, acc5: 77.328%, test_loss: 2.0438, per_image_load_time: 0.760ms, per_image_inference_time: 0.968ms
2022-03-02 13:42:43 - until epoch: 019, best_acc1: 52.698%
2022-03-02 13:42:43 - epoch 020 lr: 0.1
2022-03-02 13:43:32 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.5376
2022-03-02 13:44:14 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.0873
2022-03-02 13:44:55 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.4441
2022-03-02 13:45:37 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.0675
2022-03-02 13:46:19 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.3261
2022-03-02 13:47:01 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.5226
2022-03-02 13:47:43 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.2606
2022-03-02 13:48:25 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.4219
2022-03-02 13:49:07 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.7473
2022-03-02 13:49:49 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.3316
2022-03-02 13:50:31 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.2489
2022-03-02 13:51:12 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.2250
2022-03-02 13:51:54 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.1780
2022-03-02 13:52:36 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.4559
2022-03-02 13:53:18 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.4604
2022-03-02 13:54:01 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.2302
2022-03-02 13:54:42 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.1065
2022-03-02 13:55:24 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.2447
2022-03-02 13:56:06 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.2540
2022-03-02 13:56:49 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.4174
2022-03-02 13:57:31 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.5352
2022-03-02 13:58:13 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.1304
2022-03-02 13:58:54 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.2853
2022-03-02 13:59:36 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.4231
2022-03-02 14:00:18 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.2776
2022-03-02 14:01:00 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.1880
2022-03-02 14:01:42 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.4363
2022-03-02 14:02:24 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.4080
2022-03-02 14:03:06 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.5621
2022-03-02 14:03:48 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.2272
2022-03-02 14:04:30 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.5116
2022-03-02 14:05:12 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.6340
2022-03-02 14:05:53 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.2443
2022-03-02 14:06:35 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.3303
2022-03-02 14:07:17 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.2201
2022-03-02 14:07:59 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.1800
2022-03-02 14:08:41 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.2451
2022-03-02 14:09:22 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.2271
2022-03-02 14:10:04 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.3970
2022-03-02 14:10:47 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.2164
2022-03-02 14:11:29 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.2931
2022-03-02 14:12:11 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.2048
2022-03-02 14:12:53 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.4272
2022-03-02 14:13:35 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.3028
2022-03-02 14:14:18 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.3923
2022-03-02 14:15:00 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.4681
2022-03-02 14:15:42 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.2135
2022-03-02 14:16:24 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.4798
2022-03-02 14:17:06 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.4571
2022-03-02 14:17:49 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.1897
2022-03-02 14:17:52 - train: epoch 020, train_loss: 2.3497
2022-03-02 14:19:06 - eval: epoch: 020, acc1: 52.694%, acc5: 77.798%, test_loss: 2.0355, per_image_load_time: 0.910ms, per_image_inference_time: 0.976ms
2022-03-02 14:19:07 - until epoch: 020, best_acc1: 52.698%
2022-03-02 14:19:07 - epoch 021 lr: 0.1
2022-03-02 14:19:55 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.1614
2022-03-02 14:20:36 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.4013
2022-03-02 14:21:18 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.9787
2022-03-02 14:22:00 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.4008
2022-03-02 14:22:41 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.2046
2022-03-02 14:23:23 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.2213
2022-03-02 14:24:05 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.1932
2022-03-02 14:24:46 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.4622
2022-03-02 14:25:28 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.3623
2022-03-02 14:26:10 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.2812
2022-03-02 14:26:52 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.2489
2022-03-02 14:27:34 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.2719
2022-03-02 14:28:15 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.2218
2022-03-02 14:28:57 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.3574
2022-03-02 14:29:39 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.1321
2022-03-02 14:30:21 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.2818
2022-03-02 14:31:03 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.3523
2022-03-02 14:31:45 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.2401
2022-03-02 14:32:26 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.4319
2022-03-02 14:33:08 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.5200
2022-03-02 14:33:50 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.1703
2022-03-02 14:34:32 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.2940
2022-03-02 14:35:14 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.2422
2022-03-02 14:35:56 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.1788
2022-03-02 14:36:38 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.2926
2022-03-02 14:37:19 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.5187
2022-03-02 14:38:01 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.2065
2022-03-02 14:38:43 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.3642
2022-03-02 14:39:25 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.3174
2022-03-02 14:40:07 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.5729
2022-03-02 14:40:49 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.4058
2022-03-02 14:41:30 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.2615
2022-03-02 14:42:12 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.8682
2022-03-02 14:42:54 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.3984
2022-03-02 14:43:36 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.2251
2022-03-02 14:44:18 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.2644
2022-03-02 14:45:00 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.3583
2022-03-02 14:45:43 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.2192
2022-03-02 14:46:25 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.3083
2022-03-02 14:47:07 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.5912
2022-03-02 14:47:49 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.1302
2022-03-02 14:48:31 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.3365
2022-03-02 14:49:13 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.2636
2022-03-02 14:49:56 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.4669
2022-03-02 14:50:38 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.5320
2022-03-02 14:51:20 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.3220
2022-03-02 14:52:02 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.4004
2022-03-02 14:52:44 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.5032
2022-03-02 14:53:27 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.0335
2022-03-02 14:54:09 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.2317
2022-03-02 14:54:11 - train: epoch 021, train_loss: 2.3407
2022-03-02 14:55:24 - eval: epoch: 021, acc1: 52.532%, acc5: 77.732%, test_loss: 2.0379, per_image_load_time: 1.381ms, per_image_inference_time: 0.956ms
2022-03-02 14:55:25 - until epoch: 021, best_acc1: 52.698%
2022-03-02 14:55:25 - epoch 022 lr: 0.1
2022-03-02 14:56:13 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.1701
2022-03-02 14:56:55 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.1999
2022-03-02 14:57:37 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.0618
2022-03-02 14:58:19 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.1950
2022-03-02 14:59:00 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.3003
2022-03-02 14:59:42 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.3963
2022-03-02 15:00:24 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.5082
2022-03-02 15:01:06 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.5684
2022-03-02 15:01:48 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.4486
2022-03-02 15:02:29 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.3208
2022-03-02 15:03:11 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.4363
2022-03-02 15:03:53 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.8963
2022-03-02 15:04:35 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.2643
2022-03-02 15:05:17 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.3763
2022-03-02 15:06:00 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.4028
2022-03-02 15:06:41 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.2360
2022-03-02 15:07:24 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.0317
2022-03-02 15:08:06 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.5188
2022-03-02 15:08:47 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.1980
2022-03-02 15:09:29 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.3277
2022-03-02 15:10:11 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.3966
2022-03-02 15:10:53 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.1625
2022-03-02 15:11:35 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.4843
2022-03-02 15:12:17 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.3794
2022-03-02 15:12:59 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.2858
2022-03-02 15:13:41 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.1153
2022-03-02 15:14:23 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.1298
2022-03-02 15:15:05 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.6197
2022-03-02 15:15:47 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.1042
2022-03-02 15:16:29 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.4751
2022-03-02 15:17:10 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.5464
2022-03-02 15:17:52 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.5132
2022-03-02 15:18:35 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.3850
2022-03-02 15:19:18 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.1805
2022-03-02 15:20:00 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.4186
2022-03-02 15:20:43 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.2801
2022-03-02 15:21:25 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.4181
2022-03-02 15:22:08 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.4554
2022-03-02 15:22:50 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.3046
2022-03-02 15:23:33 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.3815
2022-03-02 15:24:15 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.2704
2022-03-02 15:24:57 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.2153
2022-03-02 15:25:40 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.4519
2022-03-02 15:26:22 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.3862
2022-03-02 15:27:05 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.2928
2022-03-02 15:27:47 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.5495
2022-03-02 15:28:30 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.4129
2022-03-02 15:29:12 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.0938
2022-03-02 15:29:54 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.1513
2022-03-02 15:30:37 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.2098
2022-03-02 15:30:39 - train: epoch 022, train_loss: 2.3351
2022-03-02 15:31:54 - eval: epoch: 022, acc1: 52.938%, acc5: 77.896%, test_loss: 2.0164, per_image_load_time: 1.902ms, per_image_inference_time: 0.957ms
2022-03-02 15:31:55 - until epoch: 022, best_acc1: 52.938%
2022-03-02 15:31:55 - epoch 023 lr: 0.1
2022-03-02 15:32:44 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.2435
2022-03-02 15:33:25 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.0361
2022-03-02 15:34:07 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.1212
2022-03-02 15:34:49 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.3308
2022-03-02 15:35:31 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.2925
2022-03-02 15:36:13 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.1899
2022-03-02 15:36:55 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.0468
2022-03-02 15:37:37 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.1908
2022-03-02 15:38:18 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.3003
2022-03-02 15:39:00 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.1367
2022-03-02 15:39:42 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.4694
2022-03-02 15:40:24 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.1533
2022-03-02 15:41:05 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.2912
2022-03-02 15:41:47 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.3145
2022-03-02 15:42:29 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.2895
2022-03-02 15:43:11 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.3326
2022-03-02 15:43:53 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.5656
2022-03-02 15:44:35 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.2969
2022-03-02 15:45:16 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.3553
2022-03-02 15:45:58 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.9213
2022-03-02 15:46:40 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.3650
2022-03-02 15:47:23 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.0234
2022-03-02 15:48:05 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.1982
2022-03-02 15:48:47 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.3464
2022-03-02 15:49:29 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.4062
2022-03-02 15:50:12 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.3272
2022-03-02 15:50:54 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.2595
2022-03-02 15:51:36 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.2980
2022-03-02 15:52:18 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.4130
2022-03-02 15:53:01 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.4204
2022-03-02 15:53:43 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.4770
2022-03-02 15:54:25 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.4186
2022-03-02 15:55:08 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.3613
2022-03-02 15:55:50 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.4410
2022-03-02 15:56:32 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.1175
2022-03-02 15:57:14 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.1031
2022-03-02 15:57:57 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.2276
2022-03-02 15:58:39 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.2411
2022-03-02 15:59:21 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.3427
2022-03-02 16:00:03 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.3063
2022-03-02 16:00:46 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.1910
2022-03-02 16:01:28 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.2231
2022-03-02 16:02:11 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.2157
2022-03-02 16:02:53 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.1131
2022-03-02 16:03:36 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.1195
2022-03-02 16:04:18 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.5058
2022-03-02 16:05:00 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.0592
2022-03-02 16:05:43 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.2035
2022-03-02 16:06:25 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.2233
2022-03-02 16:07:08 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.3740
2022-03-02 16:07:10 - train: epoch 023, train_loss: 2.3237
2022-03-02 16:08:24 - eval: epoch: 023, acc1: 53.700%, acc5: 78.472%, test_loss: 1.9864, per_image_load_time: 1.893ms, per_image_inference_time: 0.947ms
2022-03-02 16:08:25 - until epoch: 023, best_acc1: 53.700%
2022-03-02 16:08:25 - epoch 024 lr: 0.1
2022-03-02 16:09:12 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.2317
2022-03-02 16:09:53 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.3405
2022-03-02 16:10:35 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.1858
2022-03-02 16:11:17 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.3257
2022-03-02 16:11:59 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.2246
2022-03-02 16:12:41 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.1172
2022-03-02 16:13:22 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.2100
2022-03-02 16:14:04 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.1131
2022-03-02 16:14:46 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.2854
2022-03-02 16:15:28 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.2643
2022-03-02 16:16:10 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.1151
2022-03-02 16:16:52 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.0974
2022-03-02 16:17:34 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.6485
2022-03-02 16:18:16 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.2250
2022-03-02 16:18:58 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.5414
2022-03-02 16:19:40 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.3303
2022-03-02 16:20:22 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.2598
2022-03-02 16:21:04 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.4786
2022-03-02 16:21:46 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.0267
2022-03-02 16:22:28 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.3749
2022-03-02 16:23:10 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.3075
2022-03-02 16:23:52 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.0164
2022-03-02 16:24:34 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.2930
2022-03-02 16:25:16 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.3145
2022-03-02 16:25:58 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.1834
2022-03-02 16:26:40 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.3185
2022-03-02 16:27:22 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.3344
2022-03-02 16:28:05 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.4158
2022-03-02 16:28:47 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.4035
2022-03-02 16:29:29 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.2306
2022-03-02 16:30:11 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.1298
2022-03-02 16:30:54 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.4414
2022-03-02 16:31:36 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.1397
2022-03-02 16:32:18 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.3553
2022-03-02 16:33:00 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.0781
2022-03-02 16:33:42 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.3289
2022-03-02 16:34:25 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.2236
2022-03-02 16:35:07 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.5466
2022-03-02 16:35:50 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.3775
2022-03-02 16:36:32 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.2444
2022-03-02 16:37:14 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.2136
2022-03-02 16:37:57 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.3480
2022-03-02 16:38:39 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.3041
2022-03-02 16:39:22 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.4039
2022-03-02 16:40:04 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.1709
2022-03-02 16:40:47 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.4076
2022-03-02 16:41:29 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.3203
2022-03-02 16:42:12 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.2087
2022-03-02 16:42:54 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.3289
2022-03-02 16:43:37 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.4195
2022-03-02 16:43:39 - train: epoch 024, train_loss: 2.3168
2022-03-02 16:44:55 - eval: epoch: 024, acc1: 53.548%, acc5: 78.484%, test_loss: 1.9875, per_image_load_time: 1.316ms, per_image_inference_time: 0.951ms
2022-03-02 16:44:56 - until epoch: 024, best_acc1: 53.700%
2022-03-02 16:44:56 - epoch 025 lr: 0.1
2022-03-02 16:45:43 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.1228
2022-03-02 16:46:25 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 1.9918
2022-03-02 16:47:07 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.2426
2022-03-02 16:47:49 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.3049
2022-03-02 16:48:31 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.1151
2022-03-02 16:49:13 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.2865
2022-03-02 16:49:55 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.3251
2022-03-02 16:50:38 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.2137
2022-03-02 16:51:20 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.2065
2022-03-02 16:52:02 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.2178
2022-03-02 16:52:44 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.2842
2022-03-02 16:53:27 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.3202
2022-03-02 16:54:09 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.1975
2022-03-02 16:54:51 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.3331
2022-03-02 16:55:33 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.2956
2022-03-02 16:56:15 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.0124
2022-03-02 16:56:58 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.2651
2022-03-02 16:57:40 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.0552
2022-03-02 16:58:23 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.1423
2022-03-02 16:59:05 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.3937
2022-03-02 16:59:48 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.1492
2022-03-02 17:00:30 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.1930
2022-03-02 17:01:12 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.2801
2022-03-02 17:01:55 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.0688
2022-03-02 17:02:37 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.3420
2022-03-02 17:03:20 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.3472
2022-03-02 17:04:02 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.4009
2022-03-02 17:04:45 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.2279
2022-03-02 17:05:27 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.4723
2022-03-02 17:06:10 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.4053
2022-03-02 17:06:52 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.2740
2022-03-02 17:07:35 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.4311
2022-03-02 17:08:18 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.3954
2022-03-02 17:09:00 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.4051
2022-03-02 17:09:43 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 1.9921
2022-03-02 17:10:26 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.4066
2022-03-02 17:11:08 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.2290
2022-03-02 17:11:51 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.4234
2022-03-02 17:12:34 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.5302
2022-03-02 17:13:16 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.4584
2022-03-02 17:13:59 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.4666
2022-03-02 17:14:42 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.3936
2022-03-02 17:15:24 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.1468
2022-03-02 17:16:07 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.1782
2022-03-02 17:16:50 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.2774
2022-03-02 17:17:33 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.3003
2022-03-02 17:18:16 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.1852
2022-03-02 17:18:58 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.1381
2022-03-02 17:19:41 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.2618
2022-03-02 17:20:24 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.3714
2022-03-02 17:20:27 - train: epoch 025, train_loss: 2.3076
2022-03-02 17:21:47 - eval: epoch: 025, acc1: 53.612%, acc5: 78.372%, test_loss: 1.9918, per_image_load_time: 2.137ms, per_image_inference_time: 0.958ms
2022-03-02 17:21:48 - until epoch: 025, best_acc1: 53.700%
2022-03-02 17:21:48 - epoch 026 lr: 0.1
2022-03-02 17:22:36 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.1466
2022-03-02 17:23:18 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.1094
2022-03-02 17:24:00 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.2998
2022-03-02 17:24:42 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.4557
2022-03-02 17:25:23 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.3118
2022-03-02 17:26:05 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.3316
2022-03-02 17:26:47 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.2743
2022-03-02 17:27:29 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.0990
2022-03-02 17:28:11 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.5801
2022-03-02 17:28:53 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.1972
2022-03-02 17:29:35 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.3213
2022-03-02 17:30:17 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.3469
2022-03-02 17:30:59 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.2192
2022-03-02 17:31:41 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.3736
2022-03-02 17:32:22 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.3570
2022-03-02 17:33:04 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.3989
2022-03-02 17:33:46 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.2287
2022-03-02 17:34:28 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.3430
2022-03-02 17:35:10 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.6041
2022-03-02 17:35:52 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.3508
2022-03-02 17:36:34 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.3439
2022-03-02 17:37:16 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.2977
2022-03-02 17:37:58 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.1459
2022-03-02 17:38:40 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.5977
2022-03-02 17:39:23 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.4979
2022-03-02 17:40:05 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.3328
2022-03-02 17:40:47 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.1507
2022-03-02 17:41:29 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.0915
2022-03-02 17:42:11 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.5280
2022-03-02 17:42:53 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.2320
2022-03-02 17:43:35 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.3652
2022-03-02 17:44:17 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.2797
2022-03-02 17:44:59 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.2048
2022-03-02 17:45:41 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.3660
2022-03-02 17:46:23 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.3960
2022-03-02 17:47:05 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.2921
2022-03-02 17:47:47 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.4234
2022-03-02 17:48:29 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.4757
2022-03-02 17:49:11 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.4996
2022-03-02 17:49:53 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.5062
2022-03-02 17:50:36 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.3274
2022-03-02 17:51:18 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.4345
2022-03-02 17:52:00 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.4762
2022-03-02 17:52:42 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.4070
2022-03-02 17:53:24 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.5112
2022-03-02 17:54:06 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.3325
2022-03-02 17:54:48 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.2400
2022-03-02 17:55:31 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.2048
2022-03-02 17:56:13 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.3775
2022-03-02 17:56:56 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.5260
2022-03-02 17:56:59 - train: epoch 026, train_loss: 2.3017
2022-03-02 17:58:21 - eval: epoch: 026, acc1: 53.934%, acc5: 78.908%, test_loss: 1.9603, per_image_load_time: 2.172ms, per_image_inference_time: 0.969ms
2022-03-02 17:58:21 - until epoch: 026, best_acc1: 53.934%
2022-03-02 17:58:21 - epoch 027 lr: 0.1
2022-03-02 17:59:09 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.4393
2022-03-02 17:59:51 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.2797
2022-03-02 18:00:33 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.3699
2022-03-02 18:01:15 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.3869
2022-03-02 18:01:57 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.3148
2022-03-02 18:02:38 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.5650
2022-03-02 18:03:20 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.4299
2022-03-02 18:04:02 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.2670
2022-03-02 18:04:44 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.2619
2022-03-02 18:05:26 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.3672
2022-03-02 18:06:08 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.2515
2022-03-02 18:06:50 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.5974
2022-03-02 18:07:32 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.2799
2022-03-02 18:08:14 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.4667
2022-03-02 18:08:56 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.4591
2022-03-02 18:09:38 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.3383
2022-03-02 18:10:20 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.3427
2022-03-02 18:11:02 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.3716
2022-03-02 18:11:44 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.3954
2022-03-02 18:12:26 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.2725
2022-03-02 18:13:09 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.5871
2022-03-02 18:13:51 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.5543
2022-03-02 18:14:33 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.5968
2022-03-02 18:15:15 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.3075
2022-03-02 18:15:57 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.1893
2022-03-02 18:16:39 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.4359
2022-03-02 18:17:21 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.2732
2022-03-02 18:18:03 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.3728
2022-03-02 18:18:46 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.3097
2022-03-02 18:19:28 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.0794
2022-03-02 18:20:10 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.1119
2022-03-02 18:20:52 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.0883
2022-03-02 18:21:34 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.2706
2022-03-02 18:22:16 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.2264
2022-03-02 18:22:58 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.5818
2022-03-02 18:23:41 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.1189
2022-03-02 18:24:23 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.1921
2022-03-02 18:25:05 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.0777
2022-03-02 18:25:47 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.1436
2022-03-02 18:26:29 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.4247
2022-03-02 18:27:11 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.4139
2022-03-02 18:27:54 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.3306
2022-03-02 18:28:36 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.2768
2022-03-02 18:29:18 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.3335
2022-03-02 18:30:00 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.0876
2022-03-02 18:30:42 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.1576
2022-03-02 18:31:25 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.3238
2022-03-02 18:32:07 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.5232
2022-03-02 18:32:49 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.3439
2022-03-02 18:33:32 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.0635
2022-03-02 18:33:35 - train: epoch 027, train_loss: 2.2962
2022-03-02 18:34:57 - eval: epoch: 027, acc1: 54.096%, acc5: 78.800%, test_loss: 1.9572, per_image_load_time: 2.200ms, per_image_inference_time: 0.949ms
2022-03-02 18:34:57 - until epoch: 027, best_acc1: 54.096%
2022-03-02 18:34:57 - epoch 028 lr: 0.1
2022-03-02 18:35:46 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.0804
2022-03-02 18:36:27 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.2163
2022-03-02 18:37:09 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.2817
2022-03-02 18:37:51 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.1867
2022-03-02 18:38:32 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.1723
2022-03-02 18:39:14 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.2925
2022-03-02 18:39:56 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.4980
2022-03-02 18:40:38 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.0304
2022-03-02 18:41:20 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.1103
2022-03-02 18:42:02 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.2606
2022-03-02 18:42:44 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.3885
2022-03-02 18:43:26 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.0938
2022-03-02 18:44:08 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.1976
2022-03-02 18:44:49 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.5435
2022-03-02 18:45:31 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.2408
2022-03-02 18:46:13 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.2581
2022-03-02 18:46:55 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.2075
2022-03-02 18:47:37 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.1416
2022-03-02 18:48:19 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.1719
2022-03-02 18:49:01 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.5360
2022-03-02 18:49:44 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.4653
2022-03-02 18:50:26 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.4148
2022-03-02 18:51:08 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.4617
2022-03-02 18:51:50 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.4869
2022-03-02 18:52:32 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.2559
2022-03-02 18:53:14 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.1327
2022-03-02 18:53:56 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.2656
2022-03-02 18:54:38 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.2240
2022-03-02 18:55:21 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.3246
2022-03-02 18:56:03 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.4422
2022-03-02 18:56:45 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.4948
2022-03-02 18:57:28 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.2460
2022-03-02 18:58:10 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.3375
2022-03-02 18:58:53 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.1744
2022-03-02 18:59:35 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.2471
2022-03-02 19:00:17 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.1079
2022-03-02 19:01:00 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.2159
2022-03-02 19:01:42 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.9592
2022-03-02 19:02:24 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.2673
2022-03-02 19:03:07 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.4046
2022-03-02 19:03:49 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.4316
2022-03-02 19:04:31 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.4383
2022-03-02 19:05:14 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.2146
2022-03-02 19:05:56 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.2754
2022-03-02 19:06:38 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.2550
2022-03-02 19:07:20 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.4011
2022-03-02 19:08:02 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.3047
2022-03-02 19:08:44 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.1484
2022-03-02 19:09:26 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.1901
2022-03-02 19:10:08 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.0983
2022-03-02 19:10:11 - train: epoch 028, train_loss: 2.2873
2022-03-02 19:11:35 - eval: epoch: 028, acc1: 54.430%, acc5: 79.108%, test_loss: 1.9496, per_image_load_time: 2.244ms, per_image_inference_time: 0.971ms
2022-03-02 19:11:35 - until epoch: 028, best_acc1: 54.430%
2022-03-02 19:11:35 - epoch 029 lr: 0.1
2022-03-02 19:12:24 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.2984
2022-03-02 19:13:05 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.3078
2022-03-02 19:13:47 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.4302
2022-03-02 19:14:29 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.1971
2022-03-02 19:15:11 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.2326
2022-03-02 19:15:53 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.6463
2022-03-02 19:16:35 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.9059
2022-03-02 19:17:17 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.3396
2022-03-02 19:17:59 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.1030
2022-03-02 19:18:41 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.1114
2022-03-02 19:19:24 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.1069
2022-03-02 19:20:06 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.2045
2022-03-02 19:20:48 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.2836
2022-03-02 19:21:30 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.5147
2022-03-02 19:22:12 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.3817
2022-03-02 19:22:54 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.1925
2022-03-02 19:23:37 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.3840
2022-03-02 19:24:19 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.4159
2022-03-02 19:25:01 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.1313
2022-03-02 19:25:44 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.5381
2022-03-02 19:26:26 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.1936
2022-03-02 19:27:08 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.5033
2022-03-02 19:27:51 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.1928
2022-03-02 19:28:33 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.1441
2022-03-02 19:29:16 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.2071
2022-03-02 19:29:58 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.3132
2022-03-02 19:30:40 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.3001
2022-03-02 19:31:23 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.2684
2022-03-02 19:32:05 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.1708
2022-03-02 19:32:48 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.1793
2022-03-02 19:33:30 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.2915
2022-03-02 19:34:13 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.2521
2022-03-02 19:34:55 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.2551
2022-03-02 19:35:37 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.1036
2022-03-02 19:36:20 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.4646
2022-03-02 19:37:02 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.1448
2022-03-02 19:37:45 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.1531
2022-03-02 19:38:27 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.2367
2022-03-02 19:39:10 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.1899
2022-03-02 19:39:52 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.2231
2022-03-02 19:40:35 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.2298
2022-03-02 19:41:18 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.1187
2022-03-02 19:42:00 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.2990
2022-03-02 19:42:43 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.3171
2022-03-02 19:43:25 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.4182
2022-03-02 19:44:08 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.4138
2022-03-02 19:44:50 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.1467
2022-03-02 19:45:33 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.3690
2022-03-02 19:46:16 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.7770
2022-03-02 19:46:58 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.0239
2022-03-02 19:47:01 - train: epoch 029, train_loss: 2.2824
2022-03-02 19:48:24 - eval: epoch: 029, acc1: 53.890%, acc5: 78.786%, test_loss: 1.9672, per_image_load_time: 2.217ms, per_image_inference_time: 0.974ms
2022-03-02 19:48:25 - until epoch: 029, best_acc1: 54.430%
2022-03-02 19:48:25 - epoch 030 lr: 0.1
2022-03-02 19:49:11 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.3983
2022-03-02 19:49:53 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.3400
2022-03-02 19:50:35 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.2381
2022-03-02 19:51:17 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.0021
2022-03-02 19:51:58 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.5189
2022-03-02 19:52:40 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.0585
2022-03-02 19:53:22 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.1559
2022-03-02 19:54:03 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.3526
2022-03-02 19:54:45 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.3243
2022-03-02 19:55:27 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.1045
2022-03-02 19:56:09 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.1056
2022-03-02 19:56:50 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.2646
2022-03-02 19:57:32 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.0956
2022-03-02 19:58:14 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.1238
2022-03-02 19:58:56 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.1590
2022-03-02 19:59:38 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.1896
2022-03-02 20:00:20 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.5591
2022-03-02 20:01:02 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.2990
2022-03-02 20:01:44 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.4422
2022-03-02 20:02:26 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.2891
2022-03-02 20:03:07 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.3836
2022-03-02 20:03:49 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.3165
2022-03-02 20:04:31 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.2996
2022-03-02 20:05:13 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.5330
2022-03-02 20:05:55 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.2698
2022-03-02 20:06:37 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.2829
2022-03-02 20:07:19 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.2065
2022-03-02 20:08:01 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.2715
2022-03-02 20:08:43 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.1212
2022-03-02 20:09:25 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.3431
2022-03-02 20:10:07 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.2510
2022-03-02 20:10:49 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.1255
2022-03-02 20:11:32 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.4799
2022-03-02 20:12:14 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.3376
2022-03-02 20:12:56 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.2901
2022-03-02 20:13:38 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.0896
2022-03-02 20:14:20 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.0727
2022-03-02 20:15:03 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.4605
2022-03-02 20:15:45 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.4080
2022-03-02 20:16:27 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.0914
2022-03-02 20:17:09 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.0837
2022-03-02 20:17:52 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.5213
2022-03-02 20:18:34 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.3341
2022-03-02 20:19:16 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.2520
2022-03-02 20:19:59 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.3664
2022-03-02 20:20:41 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.0809
2022-03-02 20:21:23 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.2678
2022-03-02 20:22:06 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.4103
2022-03-02 20:22:48 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.4126
2022-03-02 20:23:30 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.3947
2022-03-02 20:23:33 - train: epoch 030, train_loss: 2.2817
2022-03-02 20:24:57 - eval: epoch: 030, acc1: 54.366%, acc5: 79.138%, test_loss: 1.9397, per_image_load_time: 2.256ms, per_image_inference_time: 0.946ms
2022-03-02 20:24:57 - until epoch: 030, best_acc1: 54.430%
2022-03-02 20:24:57 - epoch 031 lr: 0.010000000000000002
2022-03-02 20:25:45 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.1038
2022-03-02 20:26:27 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.9579
2022-03-02 20:27:09 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.8325
2022-03-02 20:27:51 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.9650
2022-03-02 20:28:33 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.8314
2022-03-02 20:29:15 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.8424
2022-03-02 20:29:58 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.7984
2022-03-02 20:30:39 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.7961
2022-03-02 20:31:21 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.8088
2022-03-02 20:32:04 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.0827
2022-03-02 20:32:46 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.1353
2022-03-02 20:33:28 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.7880
2022-03-02 20:34:11 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.5274
2022-03-02 20:34:53 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.8368
2022-03-02 20:35:36 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.9578
2022-03-02 20:36:18 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.6792
2022-03-02 20:37:00 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.6644
2022-03-02 20:37:42 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.7575
2022-03-02 20:38:24 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.9338
2022-03-02 20:39:07 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.8694
2022-03-02 20:39:49 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.6857
2022-03-02 20:40:31 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.6151
2022-03-02 20:41:13 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.6018
2022-03-02 20:41:55 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.7568
2022-03-02 20:42:37 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.6642
2022-03-02 20:43:19 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.7139
2022-03-02 20:44:02 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.8135
2022-03-02 20:44:44 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.0936
2022-03-02 20:45:26 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.8341
2022-03-02 20:46:09 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.9794
2022-03-02 20:46:52 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.6129
2022-03-02 20:47:34 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.9053
2022-03-02 20:48:17 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.8378
2022-03-02 20:48:59 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.9955
2022-03-02 20:49:42 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.8326
2022-03-02 20:50:25 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.7912
2022-03-02 20:51:07 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.6714
2022-03-02 20:51:50 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.5451
2022-03-02 20:52:32 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.7207
2022-03-02 20:53:14 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.6468
2022-03-02 20:53:57 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.6991
2022-03-02 20:54:39 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.8066
2022-03-02 20:55:21 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.7018
2022-03-02 20:56:04 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.8357
2022-03-02 20:56:46 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.8373
2022-03-02 20:57:28 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.7918
2022-03-02 20:58:11 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.8247
2022-03-02 20:58:53 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.6539
2022-03-02 20:59:36 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.5915
2022-03-02 21:00:18 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.7431
2022-03-02 21:00:21 - train: epoch 031, train_loss: 1.7977
2022-03-02 21:01:44 - eval: epoch: 031, acc1: 65.778%, acc5: 86.654%, test_loss: 1.4090, per_image_load_time: 2.202ms, per_image_inference_time: 0.975ms
2022-03-02 21:01:45 - until epoch: 031, best_acc1: 65.778%
2022-03-02 21:01:45 - epoch 032 lr: 0.010000000000000002
2022-03-02 21:02:32 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.6036
2022-03-02 21:03:14 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.7479
2022-03-02 21:03:56 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.7388
2022-03-02 21:04:38 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.9793
2022-03-02 21:05:20 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.7351
2022-03-02 21:06:02 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.8214
2022-03-02 21:06:43 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.6966
2022-03-02 21:07:25 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.6246
2022-03-02 21:08:07 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.7065
2022-03-02 21:08:49 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.7713
2022-03-02 21:09:31 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.8119
2022-03-02 21:10:13 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.7057
2022-03-02 21:10:55 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.6347
2022-03-02 21:11:37 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.7800
2022-03-02 21:12:20 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.7557
2022-03-02 21:13:02 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.8220
2022-03-02 21:13:44 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.8040
2022-03-02 21:14:27 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.9507
2022-03-02 21:15:09 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.5297
2022-03-02 21:15:52 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.6495
2022-03-02 21:16:34 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.6618
2022-03-02 21:17:17 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.5641
2022-03-02 21:18:00 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.7349
2022-03-02 21:18:42 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.6286
2022-03-02 21:19:25 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.7325
2022-03-02 21:20:07 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.4752
2022-03-02 21:20:49 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.7389
2022-03-02 21:21:32 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.7327
2022-03-02 21:22:15 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.5279
2022-03-02 21:22:57 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.4229
2022-03-02 21:23:39 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.6844
2022-03-02 21:24:22 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.7057
2022-03-02 21:25:04 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.5211
2022-03-02 21:25:46 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.5492
2022-03-02 21:26:29 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.6472
2022-03-02 21:27:11 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.6957
2022-03-02 21:27:54 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.7061
2022-03-02 21:28:37 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.5405
2022-03-02 21:29:19 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.7779
2022-03-02 21:30:02 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.5807
2022-03-02 21:30:44 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.6679
2022-03-02 21:31:27 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.5711
2022-03-02 21:32:09 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.8593
2022-03-02 21:32:52 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.6300
2022-03-02 21:33:34 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.9075
2022-03-02 21:34:17 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.6807
2022-03-02 21:34:59 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.7574
2022-03-02 21:35:42 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.5718
2022-03-02 21:36:25 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.8475
2022-03-02 21:37:07 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.6795
2022-03-02 21:37:11 - train: epoch 032, train_loss: 1.6795
2022-03-02 21:38:33 - eval: epoch: 032, acc1: 66.548%, acc5: 87.334%, test_loss: 1.3637, per_image_load_time: 1.120ms, per_image_inference_time: 0.954ms
2022-03-02 21:38:34 - until epoch: 032, best_acc1: 66.548%
2022-03-02 21:38:34 - epoch 033 lr: 0.010000000000000002
2022-03-02 21:39:22 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.4776
2022-03-02 21:40:04 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.7146
2022-03-02 21:40:46 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.5583
2022-03-02 21:41:27 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.4697
2022-03-02 21:42:09 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.7524
2022-03-02 21:42:51 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.5586
2022-03-02 21:43:34 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.7368
2022-03-02 21:44:16 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.7579
2022-03-02 21:44:58 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.7634
2022-03-02 21:45:40 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.7465
2022-03-02 21:46:22 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.6029
2022-03-02 21:47:05 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.5905
2022-03-02 21:47:47 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.4902
2022-03-02 21:48:29 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.8202
2022-03-02 21:49:11 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.8577
2022-03-02 21:49:54 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.8111
2022-03-02 21:50:36 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.5327
2022-03-02 21:51:19 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.8241
2022-03-02 21:52:02 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.6606
2022-03-02 21:52:44 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.5348
2022-03-02 21:53:26 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.5890
2022-03-02 21:54:09 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.7122
2022-03-02 21:54:51 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.4499
2022-03-02 21:55:34 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.9069
2022-03-02 21:56:16 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.6294
2022-03-02 21:56:58 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.5343
2022-03-02 21:57:41 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.7869
2022-03-02 21:58:23 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.5873
2022-03-02 21:59:05 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.7443
2022-03-02 21:59:47 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.6636
2022-03-02 22:00:30 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.6339
2022-03-02 22:01:12 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.6363
2022-03-02 22:01:55 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.6729
2022-03-02 22:02:37 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.6355
2022-03-02 22:03:19 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.7707
2022-03-02 22:04:02 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.8015
2022-03-02 22:04:44 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.5247
2022-03-02 22:05:26 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.4605
2022-03-02 22:06:08 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.8123
2022-03-02 22:06:51 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.7093
2022-03-02 22:07:33 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.6214
2022-03-02 22:08:15 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.5363
2022-03-02 22:08:58 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.7760
2022-03-02 22:09:40 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.7304
2022-03-02 22:10:23 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.8203
2022-03-02 22:11:06 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.5420
2022-03-02 22:11:48 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.4857
2022-03-02 22:12:31 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.9147
2022-03-02 22:13:13 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.5842
2022-03-02 22:13:56 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.5973
2022-03-02 22:13:59 - train: epoch 033, train_loss: 1.6327
2022-03-02 22:15:23 - eval: epoch: 033, acc1: 67.058%, acc5: 87.596%, test_loss: 1.3434, per_image_load_time: 2.252ms, per_image_inference_time: 0.962ms
2022-03-02 22:15:23 - until epoch: 033, best_acc1: 67.058%
2022-03-02 22:15:23 - epoch 034 lr: 0.010000000000000002
2022-03-02 22:16:11 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.5876
2022-03-02 22:16:53 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.5783
2022-03-02 22:17:35 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.5351
2022-03-02 22:18:17 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.4061
2022-03-02 22:18:59 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.5897
2022-03-02 22:19:41 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.8344
2022-03-02 22:20:23 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.5168
2022-03-02 22:21:05 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.5540
2022-03-02 22:21:48 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.6121
2022-03-02 22:22:30 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.5904
2022-03-02 22:23:12 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.6464
2022-03-02 22:23:55 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.5400
2022-03-02 22:24:37 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.5248
2022-03-02 22:25:19 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.6842
2022-03-02 22:26:01 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.5536
2022-03-02 22:26:44 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.5535
2022-03-02 22:27:26 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.5814
2022-03-02 22:28:09 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.8323
2022-03-02 22:28:51 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.7348
2022-03-02 22:29:33 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.7026
2022-03-02 22:30:16 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.8493
2022-03-02 22:30:58 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.4866
2022-03-02 22:31:40 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.7549
2022-03-02 22:32:23 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.5081
2022-03-02 22:33:05 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.5736
2022-03-02 22:33:47 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.6939
2022-03-02 22:34:29 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.6293
2022-03-02 22:35:11 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.4969
2022-03-02 22:35:54 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.3649
2022-03-02 22:36:36 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.3737
2022-03-02 22:37:18 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.5285
2022-03-02 22:38:00 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.6139
2022-03-02 22:38:43 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.6292
2022-03-02 22:39:25 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.6819
2022-03-02 22:40:07 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.4737
2022-03-02 22:40:49 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.4741
2022-03-02 22:41:31 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.3900
2022-03-02 22:42:14 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.5278
2022-03-02 22:42:56 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.6681
2022-03-02 22:43:38 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.5271
2022-03-02 22:44:21 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.6425
2022-03-02 22:45:03 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.5350
2022-03-02 22:45:46 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.5797
2022-03-02 22:46:29 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.6221
2022-03-02 22:47:11 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.7859
2022-03-02 22:47:54 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.7266
2022-03-02 22:48:37 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.6857
2022-03-02 22:49:20 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.5431
2022-03-02 22:50:03 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.5431
2022-03-02 22:50:45 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.4537
2022-03-02 22:50:49 - train: epoch 034, train_loss: 1.6071
2022-03-02 22:52:12 - eval: epoch: 034, acc1: 67.442%, acc5: 87.808%, test_loss: 1.3270, per_image_load_time: 2.243ms, per_image_inference_time: 0.957ms
2022-03-02 22:52:13 - until epoch: 034, best_acc1: 67.442%
2022-03-02 22:52:13 - epoch 035 lr: 0.010000000000000002
2022-03-02 22:53:00 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.4816
2022-03-02 22:53:42 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.3558
2022-03-02 22:54:24 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.7976
2022-03-02 22:55:06 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.4931
2022-03-02 22:55:48 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.6686
2022-03-02 22:56:30 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.4813
2022-03-02 22:57:12 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.6116
2022-03-02 22:57:54 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.5946
2022-03-02 22:58:36 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.7066
2022-03-02 22:59:18 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.7820
2022-03-02 23:00:01 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.6813
2022-03-02 23:00:43 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.5775
2022-03-02 23:01:25 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.7039
2022-03-02 23:02:07 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.5432
2022-03-02 23:02:49 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.6638
2022-03-02 23:03:31 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.5697
2022-03-02 23:04:13 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.4276
2022-03-02 23:04:55 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.6652
2022-03-02 23:05:37 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.5849
2022-03-02 23:06:19 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.6891
2022-03-02 23:07:02 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.5134
2022-03-02 23:07:44 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.7016
2022-03-02 23:08:27 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.6365
2022-03-02 23:09:09 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.7372
2022-03-02 23:09:52 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.5482
2022-03-02 23:10:34 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.6937
2022-03-02 23:11:17 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.6426
2022-03-02 23:11:59 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.5324
2022-03-02 23:12:42 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.6702
2022-03-02 23:13:24 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.6547
2022-03-02 23:14:06 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.6169
2022-03-02 23:14:49 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.4399
2022-03-02 23:15:31 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.4979
2022-03-02 23:16:13 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.5868
2022-03-02 23:16:55 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.3484
2022-03-02 23:17:38 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.5908
2022-03-02 23:18:20 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.3659
2022-03-02 23:19:02 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.5856
2022-03-02 23:19:44 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.6909
2022-03-02 23:20:27 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.3812
2022-03-02 23:21:09 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.8148
2022-03-02 23:21:51 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.5238
2022-03-02 23:22:33 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.6279
2022-03-02 23:23:15 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.5703
2022-03-02 23:23:58 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.6370
2022-03-02 23:24:40 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.5296
2022-03-02 23:25:22 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.8028
2022-03-02 23:26:04 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.7658
2022-03-02 23:26:46 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.6581
2022-03-02 23:27:29 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.5493
2022-03-02 23:27:32 - train: epoch 035, train_loss: 1.5898
2022-03-02 23:28:54 - eval: epoch: 035, acc1: 67.672%, acc5: 87.924%, test_loss: 1.3183, per_image_load_time: 1.971ms, per_image_inference_time: 0.969ms
2022-03-02 23:28:54 - until epoch: 035, best_acc1: 67.672%
2022-03-02 23:28:54 - epoch 036 lr: 0.010000000000000002
2022-03-02 23:29:41 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.6977
2022-03-02 23:30:23 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.2712
2022-03-02 23:31:05 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.6047
2022-03-02 23:31:47 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.5541
2022-03-02 23:32:29 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.5776
2022-03-02 23:33:11 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.4658
2022-03-02 23:33:53 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.4186
2022-03-02 23:34:35 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.3416
2022-03-02 23:35:18 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.5923
2022-03-02 23:36:00 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.5486
2022-03-02 23:36:42 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.7702
2022-03-02 23:37:24 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.5908
2022-03-02 23:38:06 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.5048
2022-03-02 23:38:48 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.8247
2022-03-02 23:39:30 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.6736
2022-03-02 23:40:13 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.5873
2022-03-02 23:40:55 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.6036
2022-03-02 23:41:37 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.4754
2022-03-02 23:42:19 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.5311
2022-03-02 23:43:02 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.4471
2022-03-02 23:43:44 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.5491
2022-03-02 23:44:26 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.5175
2022-03-02 23:45:09 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.6245
2022-03-02 23:45:51 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.6699
2022-03-02 23:46:34 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.4325
2022-03-02 23:47:16 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.6398
2022-03-02 23:47:59 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.4844
2022-03-02 23:48:41 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.5364
2022-03-02 23:49:24 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.3937
2022-03-02 23:50:06 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.7347
2022-03-02 23:50:49 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.6036
2022-03-02 23:51:31 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.5418
2022-03-02 23:52:14 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.5098
2022-03-02 23:52:56 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.4797
2022-03-02 23:53:39 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.6455
2022-03-02 23:54:21 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.6707
2022-03-02 23:55:04 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.5954
2022-03-02 23:55:47 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.5451
2022-03-02 23:56:29 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.5193
2022-03-02 23:57:12 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.5814
2022-03-02 23:57:55 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.5756
2022-03-02 23:58:37 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.4977
2022-03-02 23:59:20 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.5901
2022-03-03 00:00:03 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.5914
2022-03-03 00:00:45 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.3399
2022-03-03 00:01:28 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.5156
2022-03-03 00:02:10 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.5693
2022-03-03 00:02:52 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.5069
2022-03-03 00:03:35 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.4844
2022-03-03 00:04:17 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.4851
2022-03-03 00:04:21 - train: epoch 036, train_loss: 1.5774
2022-03-03 00:05:44 - eval: epoch: 036, acc1: 67.912%, acc5: 87.936%, test_loss: 1.3134, per_image_load_time: 2.210ms, per_image_inference_time: 0.962ms
2022-03-03 00:05:45 - until epoch: 036, best_acc1: 67.912%
2022-03-03 00:05:45 - epoch 037 lr: 0.010000000000000002
2022-03-03 00:06:33 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.3645
2022-03-03 00:07:14 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.3101
2022-03-03 00:07:56 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.3992
2022-03-03 00:08:38 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.6363
2022-03-03 00:09:20 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.4737
2022-03-03 00:10:02 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.6405
2022-03-03 00:10:44 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.6068
2022-03-03 00:11:26 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.5255
2022-03-03 00:12:07 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.8556
2022-03-03 00:12:49 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.4820
2022-03-03 00:13:31 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.5722
2022-03-03 00:14:13 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.5528
2022-03-03 00:14:55 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.5483
2022-03-03 00:15:37 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.7128
2022-03-03 00:16:19 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.4625
2022-03-03 00:17:01 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.4496
2022-03-03 00:17:43 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.8172
2022-03-03 00:18:25 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.7021
2022-03-03 00:19:07 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.6254
2022-03-03 00:19:49 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.5505
2022-03-03 00:20:31 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.5957
2022-03-03 00:21:13 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.5108
2022-03-03 00:21:55 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.4568
2022-03-03 00:22:37 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.6186
2022-03-03 00:23:19 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.5079
2022-03-03 00:24:02 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.8405
2022-03-03 00:24:44 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.6155
2022-03-03 00:25:26 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.5663
2022-03-03 00:26:08 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.6248
2022-03-03 00:26:50 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.7304
2022-03-03 00:27:32 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.5373
2022-03-03 00:28:14 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.6187
2022-03-03 00:28:57 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.5160
2022-03-03 00:29:39 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.4675
2022-03-03 00:30:21 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.6134
2022-03-03 00:31:03 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.7154
2022-03-03 00:31:45 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.6561
2022-03-03 00:32:28 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.4374
2022-03-03 00:33:10 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.7328
2022-03-03 00:33:52 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.5270
2022-03-03 00:34:34 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.5275
2022-03-03 00:35:16 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.7819
2022-03-03 00:35:59 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.6145
2022-03-03 00:36:41 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.5026
2022-03-03 00:37:23 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.4930
2022-03-03 00:38:05 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.5912
2022-03-03 00:38:47 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.6294
2022-03-03 00:39:29 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.5613
2022-03-03 00:40:12 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.5326
2022-03-03 00:40:54 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.6378
2022-03-03 00:40:57 - train: epoch 037, train_loss: 1.5718
2022-03-03 00:42:18 - eval: epoch: 037, acc1: 67.654%, acc5: 87.964%, test_loss: 1.3148, per_image_load_time: 2.122ms, per_image_inference_time: 0.960ms
2022-03-03 00:42:19 - until epoch: 037, best_acc1: 67.912%
2022-03-03 00:42:19 - epoch 038 lr: 0.010000000000000002
2022-03-03 00:43:07 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.4760
2022-03-03 00:43:49 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.3593
2022-03-03 00:44:30 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.2676
2022-03-03 00:45:12 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.5158
2022-03-03 00:45:54 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.4923
2022-03-03 00:46:35 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.7557
2022-03-03 00:47:17 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.3302
2022-03-03 00:47:59 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.5328
2022-03-03 00:48:41 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.5185
2022-03-03 00:49:23 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.7265
2022-03-03 00:50:04 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.5039
2022-03-03 00:50:46 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.5923
2022-03-03 00:51:28 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.7310
2022-03-03 00:52:09 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.5839
2022-03-03 00:52:51 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.6546
2022-03-03 00:53:33 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.7150
2022-03-03 00:54:15 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.7062
2022-03-03 00:54:57 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.7318
2022-03-03 00:55:39 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.6473
2022-03-03 00:56:21 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.5798
2022-03-03 00:57:03 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.6751
2022-03-03 00:57:45 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.3904
2022-03-03 00:58:27 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.6951
2022-03-03 00:59:09 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.8454
2022-03-03 00:59:51 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.4696
2022-03-03 01:00:33 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.6276
2022-03-03 01:01:16 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.6534
2022-03-03 01:01:58 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.7438
2022-03-03 01:02:41 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.6684
2022-03-03 01:03:23 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.5150
2022-03-03 01:04:05 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.5138
2022-03-03 01:04:48 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.2720
2022-03-03 01:05:30 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.3350
2022-03-03 01:06:13 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.5479
2022-03-03 01:06:55 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.6858
2022-03-03 01:07:37 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.5962
2022-03-03 01:08:20 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.3706
2022-03-03 01:09:02 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.6900
2022-03-03 01:09:45 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.5163
2022-03-03 01:10:27 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.5107
2022-03-03 01:11:10 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.5491
2022-03-03 01:11:52 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.4635
2022-03-03 01:12:34 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.7306
2022-03-03 01:13:17 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.6213
2022-03-03 01:13:59 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.6964
2022-03-03 01:14:41 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.6751
2022-03-03 01:15:24 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.4363
2022-03-03 01:16:06 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.6003
2022-03-03 01:16:48 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.5622
2022-03-03 01:17:30 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.4330
2022-03-03 01:17:34 - train: epoch 038, train_loss: 1.5663
2022-03-03 01:18:57 - eval: epoch: 038, acc1: 67.766%, acc5: 88.132%, test_loss: 1.3130, per_image_load_time: 2.180ms, per_image_inference_time: 0.958ms
2022-03-03 01:18:57 - until epoch: 038, best_acc1: 67.912%
2022-03-03 01:18:57 - epoch 039 lr: 0.010000000000000002
2022-03-03 01:19:45 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.6746
2022-03-03 01:20:26 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.6463
2022-03-03 01:21:08 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.3151
2022-03-03 01:21:50 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.6046
2022-03-03 01:22:31 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.4840
2022-03-03 01:23:13 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.3998
2022-03-03 01:23:55 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.6940
2022-03-03 01:24:37 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.5120
2022-03-03 01:25:18 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.6978
2022-03-03 01:26:00 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.4898
2022-03-03 01:26:42 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.6419
2022-03-03 01:27:24 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.6853
2022-03-03 01:28:06 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.7848
2022-03-03 01:28:48 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.5917
2022-03-03 01:29:30 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.4995
2022-03-03 01:30:12 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.7300
2022-03-03 01:30:54 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.3465
2022-03-03 01:31:37 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.6143
2022-03-03 01:32:19 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.3119
2022-03-03 01:33:01 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.4684
2022-03-03 01:33:43 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.5960
2022-03-03 01:34:25 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.5281
2022-03-03 01:35:07 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.7327
2022-03-03 01:35:49 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.6684
2022-03-03 01:36:31 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.3689
2022-03-03 01:37:13 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.5990
2022-03-03 01:37:55 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.7263
2022-03-03 01:38:37 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.4479
2022-03-03 01:39:19 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.4142
2022-03-03 01:40:01 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.6125
2022-03-03 01:40:44 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.5500
2022-03-03 01:41:26 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.6323
2022-03-03 01:42:08 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.5990
2022-03-03 01:42:50 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.5974
2022-03-03 01:43:33 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.7654
2022-03-03 01:44:15 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.6665
2022-03-03 01:44:57 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.5530
2022-03-03 01:45:39 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.4473
2022-03-03 01:46:22 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.6222
2022-03-03 01:47:04 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.5218
2022-03-03 01:47:46 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.5489
2022-03-03 01:48:28 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.5310
2022-03-03 01:49:10 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.5407
2022-03-03 01:49:52 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.2727
2022-03-03 01:50:34 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.4083
2022-03-03 01:51:16 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.6976
2022-03-03 01:51:58 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.5974
2022-03-03 01:52:40 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.6030
2022-03-03 01:53:22 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.4158
2022-03-03 01:54:04 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.4519
2022-03-03 01:54:08 - train: epoch 039, train_loss: 1.5634
2022-03-03 01:55:29 - eval: epoch: 039, acc1: 67.730%, acc5: 88.102%, test_loss: 1.3055, per_image_load_time: 2.165ms, per_image_inference_time: 0.954ms
2022-03-03 01:55:29 - until epoch: 039, best_acc1: 67.912%
2022-03-03 01:55:29 - epoch 040 lr: 0.010000000000000002
2022-03-03 01:56:17 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.8046
2022-03-03 01:56:59 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.7858
2022-03-03 01:57:41 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.6929
2022-03-03 01:58:22 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.6417
2022-03-03 01:59:04 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.4534
2022-03-03 01:59:46 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.6441
2022-03-03 02:00:27 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.5871
2022-03-03 02:01:09 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.7816
2022-03-03 02:01:51 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.4842
2022-03-03 02:02:33 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.2676
2022-03-03 02:03:14 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.6038
2022-03-03 02:03:56 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.4801
2022-03-03 02:04:38 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.5673
2022-03-03 02:05:19 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.4604
2022-03-03 02:06:01 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.7474
2022-03-03 02:06:43 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.5558
2022-03-03 02:07:25 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.6058
2022-03-03 02:08:07 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.3816
2022-03-03 02:08:49 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.4979
2022-03-03 02:09:30 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.5853
2022-03-03 02:10:12 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.5022
2022-03-03 02:10:54 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.4183
2022-03-03 02:11:36 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.5052
2022-03-03 02:12:18 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.6277
2022-03-03 02:12:59 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.6674
2022-03-03 02:13:41 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.5200
