2022-03-03 02:14:23 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.6998
2022-03-03 02:15:05 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.5649
2022-03-03 02:15:46 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.7863
2022-03-03 02:16:28 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.6335
2022-03-03 02:17:10 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.4953
2022-03-03 02:17:52 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.7455
2022-03-03 02:18:34 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.6314
2022-03-03 02:19:16 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.5553
2022-03-03 02:19:58 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.6219
2022-03-03 02:20:40 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.6996
2022-03-03 02:21:22 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.5305
2022-03-03 02:22:04 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.3548
2022-03-03 02:22:46 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.6184
2022-03-03 02:23:27 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.6064
2022-03-03 02:24:09 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.6166
2022-03-03 02:24:51 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.4601
2022-03-03 02:25:33 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.6769
2022-03-03 02:26:15 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.6054
2022-03-03 02:26:57 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.2988
2022-03-03 02:27:39 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.5916
2022-03-03 02:28:21 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.6089
2022-03-03 02:29:03 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.5045
2022-03-03 02:29:46 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.6256
2022-03-03 02:30:28 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.6670
2022-03-03 02:30:31 - train: epoch 040, train_loss: 1.5622
2022-03-03 02:31:52 - eval: epoch: 040, acc1: 67.498%, acc5: 88.070%, test_loss: 1.3136, per_image_load_time: 1.932ms, per_image_inference_time: 0.963ms
2022-03-03 02:31:52 - until epoch: 040, best_acc1: 67.912%
2022-03-03 02:31:52 - epoch 041 lr: 0.010000000000000002
2022-03-03 02:32:40 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.7027
2022-03-03 02:33:22 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.6289
2022-03-03 02:34:03 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.4988
2022-03-03 02:34:45 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.6016
2022-03-03 02:35:27 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.3914
2022-03-03 02:36:09 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.6784
2022-03-03 02:36:50 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.4698
2022-03-03 02:37:32 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.3408
2022-03-03 02:38:14 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.4240
2022-03-03 02:38:56 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.7137
2022-03-03 02:39:38 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.5057
2022-03-03 02:40:19 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.3345
2022-03-03 02:41:01 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.5900
2022-03-03 02:41:43 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.6235
2022-03-03 02:42:25 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.8921
2022-03-03 02:43:07 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.3597
2022-03-03 02:43:48 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.5062
2022-03-03 02:44:30 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.4658
2022-03-03 02:45:12 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.5525
2022-03-03 02:45:54 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.5951
2022-03-03 02:46:36 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.4600
2022-03-03 02:47:18 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.4868
2022-03-03 02:48:00 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.3109
2022-03-03 02:48:42 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.7144
2022-03-03 02:49:24 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.4941
2022-03-03 02:50:06 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.7423
2022-03-03 02:50:48 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.7459
2022-03-03 02:51:29 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.5011
2022-03-03 02:52:11 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.7503
2022-03-03 02:52:53 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.6144
2022-03-03 02:53:35 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.7176
2022-03-03 02:54:17 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.4431
2022-03-03 02:54:59 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.4415
2022-03-03 02:55:41 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.4133
2022-03-03 02:56:23 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.6172
2022-03-03 02:57:05 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.6707
2022-03-03 02:57:47 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.6444
2022-03-03 02:58:29 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.5120
2022-03-03 02:59:11 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.5812
2022-03-03 02:59:53 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.4609
2022-03-03 03:00:35 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.6360
2022-03-03 03:01:18 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.4730
2022-03-03 03:02:00 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.6484
2022-03-03 03:02:42 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.5381
2022-03-03 03:03:24 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.5291
2022-03-03 03:04:06 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.6783
2022-03-03 03:04:48 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.7882
2022-03-03 03:05:30 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.5156
2022-03-03 03:06:12 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.6070
2022-03-03 03:06:54 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.5929
2022-03-03 03:06:58 - train: epoch 041, train_loss: 1.5617
2022-03-03 03:08:20 - eval: epoch: 041, acc1: 67.638%, acc5: 88.012%, test_loss: 1.3111, per_image_load_time: 2.194ms, per_image_inference_time: 0.975ms
2022-03-03 03:08:20 - until epoch: 041, best_acc1: 67.912%
2022-03-03 03:08:20 - epoch 042 lr: 0.010000000000000002
2022-03-03 03:09:08 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.2891
2022-03-03 03:09:50 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.4843
2022-03-03 03:10:32 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.5950
2022-03-03 03:11:13 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.2009
2022-03-03 03:11:55 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.6326
2022-03-03 03:12:37 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.3641
2022-03-03 03:13:18 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.6451
2022-03-03 03:14:00 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.4736
2022-03-03 03:14:41 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.7596
2022-03-03 03:15:23 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.7013
2022-03-03 03:16:05 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.3734
2022-03-03 03:16:47 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.5137
2022-03-03 03:17:29 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.5149
2022-03-03 03:18:10 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.9121
2022-03-03 03:18:52 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.4857
2022-03-03 03:19:34 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.5510
2022-03-03 03:20:16 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.5870
2022-03-03 03:20:58 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.5398
2022-03-03 03:21:40 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.7202
2022-03-03 03:22:22 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.4247
2022-03-03 03:23:03 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.3141
2022-03-03 03:23:45 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.4387
2022-03-03 03:24:27 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.4563
2022-03-03 03:25:09 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.8174
2022-03-03 03:25:51 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.7273
2022-03-03 03:26:33 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.6471
2022-03-03 03:27:15 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.4986
2022-03-03 03:27:57 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.5136
2022-03-03 03:28:39 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.5729
2022-03-03 03:29:21 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.5223
2022-03-03 03:30:03 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.6432
2022-03-03 03:30:45 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.6963
2022-03-03 03:31:26 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.7962
2022-03-03 03:32:08 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.3429
2022-03-03 03:32:51 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.5145
2022-03-03 03:33:33 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.6856
2022-03-03 03:34:15 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.4933
2022-03-03 03:34:57 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.3806
2022-03-03 03:35:39 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.4944
2022-03-03 03:36:21 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.5522
2022-03-03 03:37:03 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.7121
2022-03-03 03:37:45 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.6615
2022-03-03 03:38:27 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.3701
2022-03-03 03:39:09 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.4138
2022-03-03 03:39:51 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.6050
2022-03-03 03:40:33 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.7647
2022-03-03 03:41:15 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.4348
2022-03-03 03:41:58 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.7812
2022-03-03 03:42:40 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.5831
2022-03-03 03:43:23 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.5226
2022-03-03 03:43:27 - train: epoch 042, train_loss: 1.5596
2022-03-03 03:44:50 - eval: epoch: 042, acc1: 67.552%, acc5: 87.934%, test_loss: 1.3181, per_image_load_time: 2.216ms, per_image_inference_time: 0.950ms
2022-03-03 03:44:50 - until epoch: 042, best_acc1: 67.912%
2022-03-03 03:44:50 - epoch 043 lr: 0.010000000000000002
2022-03-03 03:45:37 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.5179
2022-03-03 03:46:19 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.5196
2022-03-03 03:47:01 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.2806
2022-03-03 03:47:43 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.5298
2022-03-03 03:48:24 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.5301
2022-03-03 03:49:06 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.5380
2022-03-03 03:49:48 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.6896
2022-03-03 03:50:30 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.7549
2022-03-03 03:51:12 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.4703
2022-03-03 03:51:54 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.6475
2022-03-03 03:52:36 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.5645
2022-03-03 03:53:18 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.6276
2022-03-03 03:54:00 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.6981
2022-03-03 03:54:41 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.4161
2022-03-03 03:55:23 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.3694
2022-03-03 03:56:05 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.6123
2022-03-03 03:56:48 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.5717
2022-03-03 03:57:30 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.6662
2022-03-03 03:58:12 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.6387
2022-03-03 03:58:54 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.3579
2022-03-03 03:59:36 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.5055
2022-03-03 04:00:19 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.6033
2022-03-03 04:01:01 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.7388
2022-03-03 04:01:43 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.4780
2022-03-03 04:02:26 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.7344
2022-03-03 04:03:08 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.2963
2022-03-03 04:03:50 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.5828
2022-03-03 04:04:33 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.3961
2022-03-03 04:05:15 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.6362
2022-03-03 04:05:58 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.8350
2022-03-03 04:06:40 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.7880
2022-03-03 04:07:22 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.7509
2022-03-03 04:08:05 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.5878
2022-03-03 04:08:47 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.6814
2022-03-03 04:09:30 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.5674
2022-03-03 04:10:12 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.6345
2022-03-03 04:10:55 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.5912
2022-03-03 04:11:37 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.7691
2022-03-03 04:12:20 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.7292
2022-03-03 04:13:02 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.5272
2022-03-03 04:13:45 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.8318
2022-03-03 04:14:27 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.5986
2022-03-03 04:15:09 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.6093
2022-03-03 04:15:52 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.4951
2022-03-03 04:16:34 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.6063
2022-03-03 04:17:17 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.5430
2022-03-03 04:17:59 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.5300
2022-03-03 04:18:42 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.4663
2022-03-03 04:19:25 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.4054
2022-03-03 04:20:07 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.4855
2022-03-03 04:20:11 - train: epoch 043, train_loss: 1.5576
2022-03-03 04:21:32 - eval: epoch: 043, acc1: 67.398%, acc5: 87.922%, test_loss: 1.3167, per_image_load_time: 0.963ms, per_image_inference_time: 0.962ms
2022-03-03 04:21:32 - until epoch: 043, best_acc1: 67.912%
2022-03-03 04:21:32 - epoch 044 lr: 0.010000000000000002
2022-03-03 04:22:20 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.6101
2022-03-03 04:23:02 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.5996
2022-03-03 04:23:44 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.5175
2022-03-03 04:24:25 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.2717
2022-03-03 04:25:07 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.4793
2022-03-03 04:25:49 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.3302
2022-03-03 04:26:31 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.3252
2022-03-03 04:27:12 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.5067
2022-03-03 04:27:54 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.4930
2022-03-03 04:28:36 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.5388
2022-03-03 04:29:18 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.4826
2022-03-03 04:30:00 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.4578
2022-03-03 04:30:42 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.5820
2022-03-03 04:31:24 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.3622
2022-03-03 04:32:06 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.5209
2022-03-03 04:32:48 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.4723
2022-03-03 04:33:30 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.4710
2022-03-03 04:34:12 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.5355
2022-03-03 04:34:55 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.6526
2022-03-03 04:35:37 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.5474
2022-03-03 04:36:19 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.7212
2022-03-03 04:37:01 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.6248
2022-03-03 04:37:43 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.7101
2022-03-03 04:38:25 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.7135
2022-03-03 04:39:07 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.7132
2022-03-03 04:39:49 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.6356
2022-03-03 04:40:31 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.6603
2022-03-03 04:41:13 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.3790
2022-03-03 04:41:54 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.4062
2022-03-03 04:42:37 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.5008
2022-03-03 04:43:19 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.5954
2022-03-03 04:44:01 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.2976
2022-03-03 04:44:43 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.5072
2022-03-03 04:45:25 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.7916
2022-03-03 04:46:08 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.4640
2022-03-03 04:46:50 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.6537
2022-03-03 04:47:32 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.6879
2022-03-03 04:48:14 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.3347
2022-03-03 04:48:56 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.6928
2022-03-03 04:49:38 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.7289
2022-03-03 04:50:20 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.4557
2022-03-03 04:51:03 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.5414
2022-03-03 04:51:45 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.7409
2022-03-03 04:52:27 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.3723
2022-03-03 04:53:09 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.7138
2022-03-03 04:53:51 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.5007
2022-03-03 04:54:33 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.6342
2022-03-03 04:55:15 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.7415
2022-03-03 04:55:57 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.4139
2022-03-03 04:56:39 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.5747
2022-03-03 04:56:43 - train: epoch 044, train_loss: 1.5581
2022-03-03 04:58:07 - eval: epoch: 044, acc1: 67.782%, acc5: 88.024%, test_loss: 1.3125, per_image_load_time: 1.179ms, per_image_inference_time: 0.954ms
2022-03-03 04:58:07 - until epoch: 044, best_acc1: 67.912%
2022-03-03 04:58:07 - epoch 045 lr: 0.010000000000000002
2022-03-03 04:58:56 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.4030
2022-03-03 04:59:37 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.5368
2022-03-03 05:00:19 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.5529
2022-03-03 05:01:01 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.3881
2022-03-03 05:01:43 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.6413
2022-03-03 05:02:24 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.5121
2022-03-03 05:03:06 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.2854
2022-03-03 05:03:48 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.5932
2022-03-03 05:04:30 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.4642
2022-03-03 05:05:12 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.4959
2022-03-03 05:05:54 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.6761
2022-03-03 05:06:36 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.5445
2022-03-03 05:07:18 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.6227
2022-03-03 05:08:00 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.5499
2022-03-03 05:08:42 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.6305
2022-03-03 05:09:23 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.5184
2022-03-03 05:10:05 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.4909
2022-03-03 05:10:47 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.4278
2022-03-03 05:11:29 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.6514
2022-03-03 05:12:11 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.6908
2022-03-03 05:12:53 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.5873
2022-03-03 05:13:35 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.5833
2022-03-03 05:14:17 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.3702
2022-03-03 05:14:59 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.6359
2022-03-03 05:15:41 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.6155
2022-03-03 05:16:23 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.5889
2022-03-03 05:17:05 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.4391
2022-03-03 05:17:47 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.5491
2022-03-03 05:18:29 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.6933
2022-03-03 05:19:12 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.7540
2022-03-03 05:19:54 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.5442
2022-03-03 05:20:36 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.6175
2022-03-03 05:21:18 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.4378
2022-03-03 05:22:00 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.3071
2022-03-03 05:22:42 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.7602
2022-03-03 05:23:24 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.6176
2022-03-03 05:24:06 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.5498
2022-03-03 05:24:48 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.4233
2022-03-03 05:25:30 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.5671
2022-03-03 05:26:12 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.6426
2022-03-03 05:26:53 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.4506
2022-03-03 05:27:35 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.7284
2022-03-03 05:28:18 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.6789
2022-03-03 05:28:59 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.7592
2022-03-03 05:29:42 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.4211
2022-03-03 05:30:24 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.6007
2022-03-03 05:31:06 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.5794
2022-03-03 05:31:48 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.4683
2022-03-03 05:32:30 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.4361
2022-03-03 05:33:12 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.5014
2022-03-03 05:33:16 - train: epoch 045, train_loss: 1.5531
2022-03-03 05:34:40 - eval: epoch: 045, acc1: 67.246%, acc5: 87.912%, test_loss: 1.3240, per_image_load_time: 2.241ms, per_image_inference_time: 0.957ms
2022-03-03 05:34:40 - until epoch: 045, best_acc1: 67.912%
2022-03-03 05:34:40 - epoch 046 lr: 0.010000000000000002
2022-03-03 05:35:29 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.2361
2022-03-03 05:36:10 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.3322
2022-03-03 05:36:52 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.5809
2022-03-03 05:37:34 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.7538
2022-03-03 05:38:16 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.3528
2022-03-03 05:38:57 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.6851
2022-03-03 05:39:39 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.5214
2022-03-03 05:40:21 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.6110
2022-03-03 05:41:03 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.5260
2022-03-03 05:41:44 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.3326
2022-03-03 05:42:26 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.5651
2022-03-03 05:43:08 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.5398
2022-03-03 05:43:50 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.6019
2022-03-03 05:44:32 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.6863
2022-03-03 05:45:14 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.4875
2022-03-03 05:45:56 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.5889
2022-03-03 05:46:38 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.3366
2022-03-03 05:47:20 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.5069
2022-03-03 05:48:02 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.3360
2022-03-03 05:48:44 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.6215
2022-03-03 05:49:26 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.5992
2022-03-03 05:50:08 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.3030
2022-03-03 05:50:50 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.7149
2022-03-03 05:51:33 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.5715
2022-03-03 05:52:15 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.6153
2022-03-03 05:52:57 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.6693
2022-03-03 05:53:39 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.5052
2022-03-03 05:54:21 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.3590
2022-03-03 05:55:03 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.6180
2022-03-03 05:55:46 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.3786
2022-03-03 05:56:28 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.3674
2022-03-03 05:57:10 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.7565
2022-03-03 05:57:52 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.6124
2022-03-03 05:58:34 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.5716
2022-03-03 05:59:16 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.5695
2022-03-03 05:59:58 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.5084
2022-03-03 06:00:40 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.4368
2022-03-03 06:01:23 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.6547
2022-03-03 06:02:05 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.4861
2022-03-03 06:02:47 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.4652
2022-03-03 06:03:29 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.7031
2022-03-03 06:04:11 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.4164
2022-03-03 06:04:53 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.7583
2022-03-03 06:05:35 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.6097
2022-03-03 06:06:17 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.4423
2022-03-03 06:06:59 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.6437
2022-03-03 06:07:41 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.5228
2022-03-03 06:08:23 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.6090
2022-03-03 06:09:05 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.5352
2022-03-03 06:09:47 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.5456
2022-03-03 06:09:51 - train: epoch 046, train_loss: 1.5530
2022-03-03 06:11:16 - eval: epoch: 046, acc1: 67.436%, acc5: 87.914%, test_loss: 1.3225, per_image_load_time: 1.603ms, per_image_inference_time: 0.958ms
2022-03-03 06:11:17 - until epoch: 046, best_acc1: 67.912%
2022-03-03 06:11:17 - epoch 047 lr: 0.010000000000000002
2022-03-03 06:12:04 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.4025
2022-03-03 06:12:46 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.6925
2022-03-03 06:13:28 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.3655
2022-03-03 06:14:09 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.4308
2022-03-03 06:14:51 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.5573
2022-03-03 06:15:33 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.5511
2022-03-03 06:16:14 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.6745
2022-03-03 06:16:56 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.3869
2022-03-03 06:17:38 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.7737
2022-03-03 06:18:20 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.5295
2022-03-03 06:19:01 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.6402
2022-03-03 06:19:43 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.4858
2022-03-03 06:20:25 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.6275
2022-03-03 06:21:07 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.5117
2022-03-03 06:21:49 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.5326
2022-03-03 06:22:31 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.5425
2022-03-03 06:23:13 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.3775
2022-03-03 06:23:55 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.5798
2022-03-03 06:24:37 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.4502
2022-03-03 06:25:19 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.5406
2022-03-03 06:26:01 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.7553
2022-03-03 06:26:43 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.6247
2022-03-03 06:27:25 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.5658
2022-03-03 06:28:07 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.3751
2022-03-03 06:28:49 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.5567
2022-03-03 06:29:31 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.7969
2022-03-03 06:30:13 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.5393
2022-03-03 06:30:55 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.7778
2022-03-03 06:31:37 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.5537
2022-03-03 06:32:19 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.6008
2022-03-03 06:33:01 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.5072
2022-03-03 06:33:43 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.8064
2022-03-03 06:34:25 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.4139
2022-03-03 06:35:08 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.5917
2022-03-03 06:35:50 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.7279
2022-03-03 06:36:32 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.6115
2022-03-03 06:37:15 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.7372
2022-03-03 06:37:57 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.5315
2022-03-03 06:38:39 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.4918
2022-03-03 06:39:22 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.5043
2022-03-03 06:40:04 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.5972
2022-03-03 06:40:46 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.5749
2022-03-03 06:41:29 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.4316
2022-03-03 06:42:11 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.5244
2022-03-03 06:42:54 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.6915
2022-03-03 06:43:36 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.5748
2022-03-03 06:44:18 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.6628
2022-03-03 06:45:01 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.4572
2022-03-03 06:45:43 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.6746
2022-03-03 06:46:26 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.4292
2022-03-03 06:46:29 - train: epoch 047, train_loss: 1.5522
2022-03-03 06:47:51 - eval: epoch: 047, acc1: 67.534%, acc5: 87.928%, test_loss: 1.3160, per_image_load_time: 1.759ms, per_image_inference_time: 0.961ms
2022-03-03 06:47:51 - until epoch: 047, best_acc1: 67.912%
2022-03-03 06:47:51 - epoch 048 lr: 0.010000000000000002
2022-03-03 06:48:39 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.7274
2022-03-03 06:49:21 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.9984
2022-03-03 06:50:03 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.4745
2022-03-03 06:50:44 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.5139
2022-03-03 06:51:26 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.4355
2022-03-03 06:52:08 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.6029
2022-03-03 06:52:49 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.4735
2022-03-03 06:53:31 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.5943
2022-03-03 06:54:13 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.7219
2022-03-03 06:54:55 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.6422
2022-03-03 06:55:36 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.4982
2022-03-03 06:56:18 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.6837
2022-03-03 06:57:00 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.3648
2022-03-03 06:57:42 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.4411
2022-03-03 06:58:24 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.6272
2022-03-03 06:59:06 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.4357
2022-03-03 06:59:47 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.6870
2022-03-03 07:00:29 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.4678
2022-03-03 07:01:11 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.6382
2022-03-03 07:01:53 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.8651
2022-03-03 07:02:35 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.5539
2022-03-03 07:03:17 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.8025
2022-03-03 07:03:59 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.3704
2022-03-03 07:04:41 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.6586
2022-03-03 07:05:23 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.4816
2022-03-03 07:06:05 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.8109
2022-03-03 07:06:48 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.7283
2022-03-03 07:07:30 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.5610
2022-03-03 07:08:12 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.6381
2022-03-03 07:08:54 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.6083
2022-03-03 07:09:36 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.4728
2022-03-03 07:10:18 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.3952
2022-03-03 07:11:00 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.6252
2022-03-03 07:11:43 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.4947
2022-03-03 07:12:25 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.8101
2022-03-03 07:13:07 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.5732
2022-03-03 07:13:49 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.6011
2022-03-03 07:14:31 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.4523
2022-03-03 07:15:13 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.6626
2022-03-03 07:15:55 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.4175
2022-03-03 07:16:37 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.7156
2022-03-03 07:17:19 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.5028
2022-03-03 07:18:01 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.5398
2022-03-03 07:18:44 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.3747
2022-03-03 07:19:26 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.5390
2022-03-03 07:20:08 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.4689
2022-03-03 07:20:51 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.5868
2022-03-03 07:21:33 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.9094
2022-03-03 07:22:16 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.6497
2022-03-03 07:22:58 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.4714
2022-03-03 07:23:01 - train: epoch 048, train_loss: 1.5520
2022-03-03 07:24:24 - eval: epoch: 048, acc1: 67.442%, acc5: 87.780%, test_loss: 1.3234, per_image_load_time: 1.128ms, per_image_inference_time: 0.968ms
2022-03-03 07:24:24 - until epoch: 048, best_acc1: 67.912%
2022-03-03 07:24:24 - epoch 049 lr: 0.010000000000000002
2022-03-03 07:25:12 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.6766
2022-03-03 07:25:54 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.4652
2022-03-03 07:26:35 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.5535
2022-03-03 07:27:17 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.6337
2022-03-03 07:27:59 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.4507
2022-03-03 07:28:41 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.4751
2022-03-03 07:29:23 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.6584
2022-03-03 07:30:05 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.9307
2022-03-03 07:30:47 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.4234
2022-03-03 07:31:29 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.5431
2022-03-03 07:32:11 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.4594
2022-03-03 07:32:53 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.3086
2022-03-03 07:33:35 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.7036
2022-03-03 07:34:17 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.6792
2022-03-03 07:35:00 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.3967
2022-03-03 07:35:42 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.8395
2022-03-03 07:36:24 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.5930
2022-03-03 07:37:06 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.4757
2022-03-03 07:37:48 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.4841
2022-03-03 07:38:30 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.4448
2022-03-03 07:39:12 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.4524
2022-03-03 07:39:54 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.5244
2022-03-03 07:40:36 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.4701
2022-03-03 07:41:18 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.7039
2022-03-03 07:42:00 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.6313
2022-03-03 07:42:42 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.5766
2022-03-03 07:43:24 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.3277
2022-03-03 07:44:06 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.4545
2022-03-03 07:44:48 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.6211
2022-03-03 07:45:30 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.5501
2022-03-03 07:46:12 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.5827
2022-03-03 07:46:54 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.7646
2022-03-03 07:47:37 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.5617
2022-03-03 07:48:19 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.8050
2022-03-03 07:49:01 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.6041
2022-03-03 07:49:43 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.6172
2022-03-03 07:50:25 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.5593
2022-03-03 07:51:07 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.6916
2022-03-03 07:51:49 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.7857
2022-03-03 07:52:31 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.5953
2022-03-03 07:53:13 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.5224
2022-03-03 07:53:56 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.6234
2022-03-03 07:54:38 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.7749
2022-03-03 07:55:20 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.7118
2022-03-03 07:56:02 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.5848
2022-03-03 07:56:44 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.6484
2022-03-03 07:57:26 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.7369
2022-03-03 07:58:08 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.3616
2022-03-03 07:58:51 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.4897
2022-03-03 07:59:33 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.4525
2022-03-03 07:59:36 - train: epoch 049, train_loss: 1.5481
2022-03-03 08:00:59 - eval: epoch: 049, acc1: 67.432%, acc5: 87.862%, test_loss: 1.3260, per_image_load_time: 1.603ms, per_image_inference_time: 0.963ms
2022-03-03 08:00:59 - until epoch: 049, best_acc1: 67.912%
2022-03-03 08:00:59 - epoch 050 lr: 0.010000000000000002
2022-03-03 08:01:47 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.7509
2022-03-03 08:02:29 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.5930
2022-03-03 08:03:11 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.4350
2022-03-03 08:03:53 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.3950
2022-03-03 08:04:35 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.5673
2022-03-03 08:05:17 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.6260
2022-03-03 08:06:00 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.3578
2022-03-03 08:06:42 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.3637
2022-03-03 08:07:25 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.4837
2022-03-03 08:08:07 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.7251
2022-03-03 08:08:49 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.6347
2022-03-03 08:09:32 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.5447
2022-03-03 08:10:14 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.3062
2022-03-03 08:10:56 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.6426
2022-03-03 08:11:38 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.4992
2022-03-03 08:12:21 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.5932
2022-03-03 08:13:03 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.6428
2022-03-03 08:13:45 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.5082
2022-03-03 08:14:27 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.4203
2022-03-03 08:15:10 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.5139
2022-03-03 08:15:52 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.2466
2022-03-03 08:16:34 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.5638
2022-03-03 08:17:16 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.4835
2022-03-03 08:17:59 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.6446
2022-03-03 08:18:41 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.5370
2022-03-03 08:19:23 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.4388
2022-03-03 08:20:05 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.6474
2022-03-03 08:20:47 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.8023
2022-03-03 08:21:30 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.7116
2022-03-03 08:22:12 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.6965
2022-03-03 08:22:54 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.4436
2022-03-03 08:23:37 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.5187
2022-03-03 08:24:19 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.4303
2022-03-03 08:25:01 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.5448
2022-03-03 08:25:44 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.5698
2022-03-03 08:26:26 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.5872
2022-03-03 08:27:08 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.6339
2022-03-03 08:27:50 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.4022
2022-03-03 08:28:32 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.3287
2022-03-03 08:29:15 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.6378
2022-03-03 08:29:57 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.4937
2022-03-03 08:30:39 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.5336
2022-03-03 08:31:22 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.4713
2022-03-03 08:32:04 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.4951
2022-03-03 08:32:46 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.4794
2022-03-03 08:33:29 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.5356
2022-03-03 08:34:11 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.5901
2022-03-03 08:34:53 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.3176
2022-03-03 08:35:36 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.4117
2022-03-03 08:36:19 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.4684
2022-03-03 08:36:22 - train: epoch 050, train_loss: 1.5435
2022-03-03 08:37:43 - eval: epoch: 050, acc1: 67.796%, acc5: 88.190%, test_loss: 1.3073, per_image_load_time: 1.884ms, per_image_inference_time: 0.976ms
2022-03-03 08:37:44 - until epoch: 050, best_acc1: 67.912%
2022-03-03 08:37:44 - epoch 051 lr: 0.010000000000000002
2022-03-03 08:38:32 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.7690
2022-03-03 08:39:14 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.8133
2022-03-03 08:39:56 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.7116
2022-03-03 08:40:38 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.2931
2022-03-03 08:41:20 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.5475
2022-03-03 08:42:02 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.5124
2022-03-03 08:42:45 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.4009
2022-03-03 08:43:27 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.7317
2022-03-03 08:44:09 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.3598
2022-03-03 08:44:51 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.8062
2022-03-03 08:45:33 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.7408
2022-03-03 08:46:14 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.4911
2022-03-03 08:46:56 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.1939
2022-03-03 08:47:38 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.4620
2022-03-03 08:48:20 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.5050
2022-03-03 08:49:02 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.4385
2022-03-03 08:49:44 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.5658
2022-03-03 08:50:26 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.5705
2022-03-03 08:51:08 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.3656
2022-03-03 08:51:50 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.5449
2022-03-03 08:52:32 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.4315
2022-03-03 08:53:14 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.5706
2022-03-03 08:53:56 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.6112
2022-03-03 08:54:38 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.6242
2022-03-03 08:55:20 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.4879
2022-03-03 08:56:02 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.4636
2022-03-03 08:56:44 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.5618
2022-03-03 08:57:26 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.4620
2022-03-03 08:58:08 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.4697
2022-03-03 08:58:51 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.3975
2022-03-03 08:59:33 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.4076
2022-03-03 09:00:15 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.5577
2022-03-03 09:00:57 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.5259
2022-03-03 09:01:39 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.5853
2022-03-03 09:02:21 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.5024
2022-03-03 09:03:04 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.5061
2022-03-03 09:03:46 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.6496
2022-03-03 09:04:28 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.4323
2022-03-03 09:05:10 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.5612
2022-03-03 09:05:52 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.4754
2022-03-03 09:06:35 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.6380
2022-03-03 09:07:17 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.8191
2022-03-03 09:07:59 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.4122
2022-03-03 09:08:41 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.5868
2022-03-03 09:09:24 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.2904
2022-03-03 09:10:06 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.6680
2022-03-03 09:10:48 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.5391
2022-03-03 09:11:30 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.5106
2022-03-03 09:12:12 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.6432
2022-03-03 09:12:55 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.4201
2022-03-03 09:12:58 - train: epoch 051, train_loss: 1.5430
2022-03-03 09:14:21 - eval: epoch: 051, acc1: 67.554%, acc5: 87.916%, test_loss: 1.3163, per_image_load_time: 2.034ms, per_image_inference_time: 0.971ms
2022-03-03 09:14:22 - until epoch: 051, best_acc1: 67.912%
2022-03-03 09:14:22 - epoch 052 lr: 0.010000000000000002
2022-03-03 09:15:10 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.4902
2022-03-03 09:15:52 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.5839
2022-03-03 09:16:33 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.5629
2022-03-03 09:17:15 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.6039
2022-03-03 09:17:57 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.6335
2022-03-03 09:18:38 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.4457
2022-03-03 09:19:20 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.6612
2022-03-03 09:20:02 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.5739
2022-03-03 09:20:44 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.5535
2022-03-03 09:21:25 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.7255
2022-03-03 09:22:07 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.5104
2022-03-03 09:22:49 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.4127
2022-03-03 09:23:31 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.4769
2022-03-03 09:24:13 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.8217
2022-03-03 09:24:55 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.3242
2022-03-03 09:25:37 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.3767
2022-03-03 09:26:19 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.3682
2022-03-03 09:27:01 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.3198
2022-03-03 09:27:43 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.5889
2022-03-03 09:28:25 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.5941
2022-03-03 09:29:07 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.4142
2022-03-03 09:29:49 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.5890
2022-03-03 09:30:31 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.2991
2022-03-03 09:31:13 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.4321
2022-03-03 09:31:55 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.3010
2022-03-03 09:32:37 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.4037
2022-03-03 09:33:19 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.4828
2022-03-03 09:34:02 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.5407
2022-03-03 09:34:43 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.3793
2022-03-03 09:35:26 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.5681
2022-03-03 09:36:08 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.6450
2022-03-03 09:36:50 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.6206
2022-03-03 09:37:32 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.5362
2022-03-03 09:38:14 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.5862
2022-03-03 09:38:55 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.6112
2022-03-03 09:39:37 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.6425
2022-03-03 09:40:19 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.6293
2022-03-03 09:41:01 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.4921
2022-03-03 09:41:43 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.5638
2022-03-03 09:42:25 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.7597
2022-03-03 09:43:08 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.3354
2022-03-03 09:43:50 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.4777
2022-03-03 09:44:33 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.5235
2022-03-03 09:45:15 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.7118
2022-03-03 09:45:58 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.4228
2022-03-03 09:46:40 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.6274
2022-03-03 09:47:23 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.5036
2022-03-03 09:48:05 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.3007
2022-03-03 09:48:47 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.3732
2022-03-03 09:49:30 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.5597
2022-03-03 09:49:33 - train: epoch 052, train_loss: 1.5403
2022-03-03 09:50:56 - eval: epoch: 052, acc1: 67.706%, acc5: 88.018%, test_loss: 1.3085, per_image_load_time: 2.207ms, per_image_inference_time: 0.951ms
2022-03-03 09:50:56 - until epoch: 052, best_acc1: 67.912%
2022-03-03 09:50:56 - epoch 053 lr: 0.010000000000000002
2022-03-03 09:51:44 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.5140
2022-03-03 09:52:26 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.7829
2022-03-03 09:53:08 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.5469
2022-03-03 09:53:50 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.5834
2022-03-03 09:54:32 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.6717
2022-03-03 09:55:14 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.5477
2022-03-03 09:55:57 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.4015
2022-03-03 09:56:39 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.6501
2022-03-03 09:57:21 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.5348
2022-03-03 09:58:03 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.5082
2022-03-03 09:58:46 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.4616
2022-03-03 09:59:28 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.4576
2022-03-03 10:00:10 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.6248
2022-03-03 10:00:52 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.7662
2022-03-03 10:01:34 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.5652
2022-03-03 10:02:16 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.7519
2022-03-03 10:02:58 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.6933
2022-03-03 10:03:40 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.5868
2022-03-03 10:04:22 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.2247
2022-03-03 10:05:04 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.5219
2022-03-03 10:05:46 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.6042
2022-03-03 10:06:28 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.4720
2022-03-03 10:07:09 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.5966
2022-03-03 10:07:51 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.5470
2022-03-03 10:08:33 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.7595
2022-03-03 10:09:15 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.5681
2022-03-03 10:09:57 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.6892
2022-03-03 10:10:39 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.6162
2022-03-03 10:11:22 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.3799
2022-03-03 10:12:04 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.3887
2022-03-03 10:12:46 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.6785
2022-03-03 10:13:28 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.7626
2022-03-03 10:14:10 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.5042
2022-03-03 10:14:52 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.6419
2022-03-03 10:15:34 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.5700
2022-03-03 10:16:16 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.5990
2022-03-03 10:16:59 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.7214
2022-03-03 10:17:41 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.5168
2022-03-03 10:18:23 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.6595
2022-03-03 10:19:06 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.5292
2022-03-03 10:19:48 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.5877
2022-03-03 10:20:31 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.6168
2022-03-03 10:21:13 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.7501
2022-03-03 10:21:56 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.4597
2022-03-03 10:22:38 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.6968
2022-03-03 10:23:21 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.5465
2022-03-03 10:24:04 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.7452
2022-03-03 10:24:46 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.7022
2022-03-03 10:25:29 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.4284
2022-03-03 10:26:11 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.5186
2022-03-03 10:26:15 - train: epoch 053, train_loss: 1.5362
2022-03-03 10:27:33 - eval: epoch: 053, acc1: 67.758%, acc5: 88.060%, test_loss: 1.3099, per_image_load_time: 2.032ms, per_image_inference_time: 0.950ms
2022-03-03 10:27:33 - until epoch: 053, best_acc1: 67.912%
2022-03-03 10:27:33 - epoch 054 lr: 0.010000000000000002
2022-03-03 10:28:20 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.3858
2022-03-03 10:29:02 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.8089
2022-03-03 10:29:44 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.5343
2022-03-03 10:30:25 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.3526
2022-03-03 10:31:07 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.6463
2022-03-03 10:31:49 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.3444
2022-03-03 10:32:31 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.5759
2022-03-03 10:33:13 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.7226
2022-03-03 10:33:54 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.2772
2022-03-03 10:34:36 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.3250
2022-03-03 10:35:18 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.2910
2022-03-03 10:35:59 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.7326
2022-03-03 10:36:41 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.4591
2022-03-03 10:37:24 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.5633
2022-03-03 10:38:05 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.5957
2022-03-03 10:38:47 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.1970
2022-03-03 10:39:29 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.5022
2022-03-03 10:40:11 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.4924
2022-03-03 10:40:53 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.9111
2022-03-03 10:41:35 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.6436
2022-03-03 10:42:18 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.3269
2022-03-03 10:43:00 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.4720
2022-03-03 10:43:42 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.5628
2022-03-03 10:44:25 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.3874
2022-03-03 10:45:07 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.5576
2022-03-03 10:45:50 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.6021
2022-03-03 10:46:32 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.6211
2022-03-03 10:47:15 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.8206
2022-03-03 10:47:57 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.4596
2022-03-03 10:48:40 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.5834
2022-03-03 10:49:23 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.5324
2022-03-03 10:50:05 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.7371
2022-03-03 10:50:48 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.5121
2022-03-03 10:51:30 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.6197
2022-03-03 10:52:12 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.7110
2022-03-03 10:52:55 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.4624
2022-03-03 10:53:37 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.4893
2022-03-03 10:54:20 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.6540
2022-03-03 10:55:02 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.5040
2022-03-03 10:55:44 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.4544
2022-03-03 10:56:27 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.5408
2022-03-03 10:57:09 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.5332
2022-03-03 10:57:52 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.6850
2022-03-03 10:58:34 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.4039
2022-03-03 10:59:16 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.3959
2022-03-03 10:59:59 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.5718
2022-03-03 11:00:41 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.7126
2022-03-03 11:01:23 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.7030
2022-03-03 11:02:05 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.4144
2022-03-03 11:02:47 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.5533
2022-03-03 11:02:51 - train: epoch 054, train_loss: 1.5358
2022-03-03 11:04:08 - eval: epoch: 054, acc1: 67.814%, acc5: 88.118%, test_loss: 1.3099, per_image_load_time: 1.987ms, per_image_inference_time: 0.962ms
2022-03-03 11:04:09 - until epoch: 054, best_acc1: 67.912%
2022-03-03 11:04:09 - epoch 055 lr: 0.010000000000000002
2022-03-03 11:04:56 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.5728
2022-03-03 11:05:37 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.4404
2022-03-03 11:06:19 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.4356
2022-03-03 11:07:00 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.4605
2022-03-03 11:07:42 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.3420
2022-03-03 11:08:23 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.3739
2022-03-03 11:09:05 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.6784
2022-03-03 11:09:46 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.3829
2022-03-03 11:10:28 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.4330
2022-03-03 11:11:10 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.4985
2022-03-03 11:11:52 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.5643
2022-03-03 11:12:34 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.4987
2022-03-03 11:13:15 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.6353
2022-03-03 11:13:57 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.4958
2022-03-03 11:14:40 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.5435
2022-03-03 11:15:22 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.7373
2022-03-03 11:16:04 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.5575
2022-03-03 11:16:46 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.6797
2022-03-03 11:17:28 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.5871
2022-03-03 11:18:10 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.5184
2022-03-03 11:18:52 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.3021
2022-03-03 11:19:34 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.7150
2022-03-03 11:20:16 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.4351
2022-03-03 11:20:58 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.3301
2022-03-03 11:21:40 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.5730
2022-03-03 11:22:22 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.4637
2022-03-03 11:23:04 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.3871
2022-03-03 11:23:46 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.5824
2022-03-03 11:24:28 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.6633
2022-03-03 11:25:10 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.4778
2022-03-03 11:25:52 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.6036
2022-03-03 11:26:35 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.4439
2022-03-03 11:27:16 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.3648
2022-03-03 11:27:58 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.4434
2022-03-03 11:28:40 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.2922
2022-03-03 11:29:22 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.6283
2022-03-03 11:30:04 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.4993
2022-03-03 11:30:46 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.6080
2022-03-03 11:31:28 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.8493
2022-03-03 11:32:11 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.5525
2022-03-03 11:32:53 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.5295
2022-03-03 11:33:35 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.4975
2022-03-03 11:34:17 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.6255
2022-03-03 11:34:59 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.7102
2022-03-03 11:35:41 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.5593
2022-03-03 11:36:23 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.5870
2022-03-03 11:37:06 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.3313
2022-03-03 11:37:48 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.5716
2022-03-03 11:38:30 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.5329
2022-03-03 11:39:12 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.6693
2022-03-03 11:39:16 - train: epoch 055, train_loss: 1.5300
2022-03-03 11:40:34 - eval: epoch: 055, acc1: 67.956%, acc5: 88.248%, test_loss: 1.2939, per_image_load_time: 1.672ms, per_image_inference_time: 0.952ms
2022-03-03 11:40:34 - until epoch: 055, best_acc1: 67.956%
2022-03-03 11:40:34 - epoch 056 lr: 0.010000000000000002
2022-03-03 11:41:22 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.5000
2022-03-03 11:42:04 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.6585
2022-03-03 11:42:45 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.4987
2022-03-03 11:43:27 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.4459
2022-03-03 11:44:09 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.4885
2022-03-03 11:44:51 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.5485
2022-03-03 11:45:32 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.6069
2022-03-03 11:46:14 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.7400
2022-03-03 11:46:56 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.5863
2022-03-03 11:47:38 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.4647
2022-03-03 11:48:20 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.4059
2022-03-03 11:49:02 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.4488
2022-03-03 11:49:44 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.5621
2022-03-03 11:50:26 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.4192
2022-03-03 11:51:08 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.7660
2022-03-03 11:51:49 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.3761
2022-03-03 11:52:31 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.6445
2022-03-03 11:53:13 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.7322
2022-03-03 11:53:55 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.5475
2022-03-03 11:54:37 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.5457
2022-03-03 11:55:20 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.6710
2022-03-03 11:56:02 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.6926
2022-03-03 11:56:44 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.6357
2022-03-03 11:57:26 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.4874
2022-03-03 11:58:08 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.6951
2022-03-03 11:58:50 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.4696
2022-03-03 11:59:32 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.5148
2022-03-03 12:00:14 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.4909
2022-03-03 12:00:56 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.6592
2022-03-03 12:01:39 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.6665
2022-03-03 12:02:21 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.5144
2022-03-03 12:03:03 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.3730
2022-03-03 12:03:45 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.7298
2022-03-03 12:04:28 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.4365
2022-03-03 12:05:10 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.5614
2022-03-03 12:05:52 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.3241
2022-03-03 12:06:34 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.5185
2022-03-03 12:07:16 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.3727
2022-03-03 12:07:58 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.7429
2022-03-03 12:08:40 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.4457
2022-03-03 12:09:23 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.8281
2022-03-03 12:10:05 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.5798
2022-03-03 12:10:47 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.4614
2022-03-03 12:11:29 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.6532
2022-03-03 12:12:12 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.5823
2022-03-03 12:12:54 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.6141
2022-03-03 12:13:36 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.6439
2022-03-03 12:14:18 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.7651
2022-03-03 12:15:00 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.5580
2022-03-03 12:15:42 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.6587
2022-03-03 12:15:46 - train: epoch 056, train_loss: 1.5278
2022-03-03 12:17:02 - eval: epoch: 056, acc1: 67.856%, acc5: 88.114%, test_loss: 1.3054, per_image_load_time: 1.659ms, per_image_inference_time: 0.981ms
2022-03-03 12:17:02 - until epoch: 056, best_acc1: 67.956%
2022-03-03 12:17:02 - epoch 057 lr: 0.010000000000000002
2022-03-03 12:17:49 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.6599
2022-03-03 12:18:31 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.4240
2022-03-03 12:19:13 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.4093
2022-03-03 12:19:54 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.4435
2022-03-03 12:20:36 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.2572
2022-03-03 12:21:18 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.7550
2022-03-03 12:22:00 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.2454
2022-03-03 12:22:41 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.5162
2022-03-03 12:23:23 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.5128
2022-03-03 12:24:05 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.3137
2022-03-03 12:24:47 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.5030
2022-03-03 12:25:28 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.4264
2022-03-03 12:26:10 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.5406
2022-03-03 12:26:52 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.5580
2022-03-03 12:27:34 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.6872
2022-03-03 12:28:16 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.5892
2022-03-03 12:28:58 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.7302
2022-03-03 12:29:39 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.5731
2022-03-03 12:30:21 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.4153
2022-03-03 12:31:03 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.5383
2022-03-03 12:31:45 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.5404
2022-03-03 12:32:27 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.5622
2022-03-03 12:33:08 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.5271
2022-03-03 12:33:50 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.4850
2022-03-03 12:34:32 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.4420
2022-03-03 12:35:14 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.3435
2022-03-03 12:35:56 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.2914
2022-03-03 12:36:38 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.1819
2022-03-03 12:37:20 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.6731
2022-03-03 12:38:02 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.6701
2022-03-03 12:38:44 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.7339
2022-03-03 12:39:26 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.5312
2022-03-03 12:40:08 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.5575
2022-03-03 12:40:50 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.6259
2022-03-03 12:41:32 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.6078
2022-03-03 12:42:14 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.4367
2022-03-03 12:42:55 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.4829
2022-03-03 12:43:37 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.4456
2022-03-03 12:44:19 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.6150
2022-03-03 12:45:01 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.4538
2022-03-03 12:45:44 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.5270
2022-03-03 12:46:26 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.6218
2022-03-03 12:47:08 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.4700
2022-03-03 12:47:50 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.5196
2022-03-03 12:48:33 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.7164
2022-03-03 12:49:15 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.7790
2022-03-03 12:49:57 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.4592
2022-03-03 12:50:40 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.7408
2022-03-03 12:51:22 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.6573
2022-03-03 12:52:05 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.7036
2022-03-03 12:52:08 - train: epoch 057, train_loss: 1.5256
2022-03-03 12:53:25 - eval: epoch: 057, acc1: 68.016%, acc5: 88.148%, test_loss: 1.3028, per_image_load_time: 1.942ms, per_image_inference_time: 0.975ms
2022-03-03 12:53:25 - until epoch: 057, best_acc1: 68.016%
2022-03-03 12:53:25 - epoch 058 lr: 0.010000000000000002
2022-03-03 12:54:12 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.5182
2022-03-03 12:54:54 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.3775
2022-03-03 12:55:35 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.6374
2022-03-03 12:56:17 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.5417
2022-03-03 12:56:59 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.4244
2022-03-03 12:57:41 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.6737
2022-03-03 12:58:23 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.4916
2022-03-03 12:59:05 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.4779
2022-03-03 12:59:47 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.3550
2022-03-03 13:00:29 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.6692
2022-03-03 13:01:11 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.2840
2022-03-03 13:01:53 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.4540
2022-03-03 13:02:35 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.5909
2022-03-03 13:03:17 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.5901
2022-03-03 13:03:59 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.5096
2022-03-03 13:04:41 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.4510
2022-03-03 13:05:23 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.7537
2022-03-03 13:06:05 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.5729
2022-03-03 13:06:47 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.5523
2022-03-03 13:07:30 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.6856
2022-03-03 13:08:12 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.3716
2022-03-03 13:08:55 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.2588
2022-03-03 13:09:37 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.4900
2022-03-03 13:10:19 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.5210
2022-03-03 13:11:02 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.6089
2022-03-03 13:11:44 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.4382
2022-03-03 13:12:27 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.6975
2022-03-03 13:13:09 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.2919
2022-03-03 13:13:51 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.4940
2022-03-03 13:14:34 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.6852
2022-03-03 13:15:16 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.4071
2022-03-03 13:15:58 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.3460
2022-03-03 13:16:41 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.5373
2022-03-03 13:17:23 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.4285
2022-03-03 13:18:05 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.3364
2022-03-03 13:18:47 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.4333
2022-03-03 13:19:29 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.4493
2022-03-03 13:20:12 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.4766
2022-03-03 13:20:54 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.5832
2022-03-03 13:21:36 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.6385
2022-03-03 13:22:18 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.6482
2022-03-03 13:23:01 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.3360
2022-03-03 13:23:43 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.8392
2022-03-03 13:24:26 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.3815
2022-03-03 13:25:08 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.5684
2022-03-03 13:25:50 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.4347
2022-03-03 13:26:32 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.5880
2022-03-03 13:27:15 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.7349
2022-03-03 13:27:57 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.4167
2022-03-03 13:28:39 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.5098
2022-03-03 13:28:43 - train: epoch 058, train_loss: 1.5216
2022-03-03 13:29:59 - eval: epoch: 058, acc1: 67.978%, acc5: 88.198%, test_loss: 1.2978, per_image_load_time: 1.639ms, per_image_inference_time: 0.967ms
2022-03-03 13:30:00 - until epoch: 058, best_acc1: 68.016%
2022-03-03 13:30:00 - epoch 059 lr: 0.010000000000000002
2022-03-03 13:30:47 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.6722
2022-03-03 13:31:29 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.4870
2022-03-03 13:32:10 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.4371
2022-03-03 13:32:52 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.5712
2022-03-03 13:33:34 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.7113
2022-03-03 13:34:15 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.5059
2022-03-03 13:34:57 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.4446
2022-03-03 13:35:39 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.5359
2022-03-03 13:36:20 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.5278
2022-03-03 13:37:02 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.5399
2022-03-03 13:37:44 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 1.7019
2022-03-03 13:38:26 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.3242
2022-03-03 13:39:08 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.7013
2022-03-03 13:39:50 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.6759
2022-03-03 13:40:32 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.6318
2022-03-03 13:41:14 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.4309
2022-03-03 13:41:56 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.5799
2022-03-03 13:42:38 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.5496
2022-03-03 13:43:20 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.6074
2022-03-03 13:44:02 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.5226
2022-03-03 13:44:44 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.6425
2022-03-03 13:45:26 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.7451
2022-03-03 13:46:08 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.4198
2022-03-03 13:46:50 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.6285
2022-03-03 13:47:32 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.4864
2022-03-03 13:48:14 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.4576
2022-03-03 13:48:56 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.4978
2022-03-03 13:49:39 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.6944
2022-03-03 13:50:21 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.5443
2022-03-03 13:51:03 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 2.0455
2022-03-03 13:51:45 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.3777
2022-03-03 13:52:27 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.4578
2022-03-03 13:53:09 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.5946
2022-03-03 13:53:51 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 1.8790
2022-03-03 13:54:34 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.3705
2022-03-03 13:55:16 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.4577
2022-03-03 13:55:58 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.4562
2022-03-03 13:56:40 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.5205
2022-03-03 13:57:22 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.3217
2022-03-03 13:58:05 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 1.8265
2022-03-03 13:58:47 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.5854
2022-03-03 13:59:29 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.5018
2022-03-03 14:00:11 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.6345
2022-03-03 14:00:53 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.6816
2022-03-03 14:01:36 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.5073
2022-03-03 14:02:18 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.5533
2022-03-03 14:03:00 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.4608
2022-03-03 14:03:43 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.4697
2022-03-03 14:04:25 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.6991
2022-03-03 14:05:07 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.6541
2022-03-03 14:05:10 - train: epoch 059, train_loss: 1.5200
2022-03-03 14:06:27 - eval: epoch: 059, acc1: 67.928%, acc5: 88.232%, test_loss: 1.3023, per_image_load_time: 1.947ms, per_image_inference_time: 0.965ms
2022-03-03 14:06:27 - until epoch: 059, best_acc1: 68.016%
2022-03-03 14:06:27 - epoch 060 lr: 0.010000000000000002
2022-03-03 14:07:14 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.4640
2022-03-03 14:07:56 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.6450
2022-03-03 14:08:37 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.5056
2022-03-03 14:09:19 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.5146
2022-03-03 14:10:01 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.7181
2022-03-03 14:10:42 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.6255
2022-03-03 14:11:24 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.5544
2022-03-03 14:12:05 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.6610
2022-03-03 14:12:47 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.3250
2022-03-03 14:13:29 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.2437
2022-03-03 14:14:11 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.2687
2022-03-03 14:14:52 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.4153
2022-03-03 14:15:34 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.4055
2022-03-03 14:16:16 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.5061
2022-03-03 14:16:58 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.6205
2022-03-03 14:17:40 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.4586
2022-03-03 14:18:22 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.5094
2022-03-03 14:19:03 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.5236
2022-03-03 14:19:45 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.6613
2022-03-03 14:20:27 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.4591
2022-03-03 14:21:09 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.6415
2022-03-03 14:21:51 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.5475
2022-03-03 14:22:33 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.3641
2022-03-03 14:23:15 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.4786
2022-03-03 14:23:57 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.5407
2022-03-03 14:24:38 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.6308
2022-03-03 14:25:20 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.4743
2022-03-03 14:26:02 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.4765
2022-03-03 14:26:44 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.6278
2022-03-03 14:27:26 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.8303
2022-03-03 14:28:07 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.5589
2022-03-03 14:28:49 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.4781
2022-03-03 14:29:31 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.3486
2022-03-03 14:30:13 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.5215
2022-03-03 14:30:55 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.6071
2022-03-03 14:31:37 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.4297
2022-03-03 14:32:18 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.5351
2022-03-03 14:33:00 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.5705
2022-03-03 14:33:42 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.9096
2022-03-03 14:34:24 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.5848
2022-03-03 14:35:06 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.6865
2022-03-03 14:35:48 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.6479
2022-03-03 14:36:30 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.4898
2022-03-03 14:37:11 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.6435
2022-03-03 14:37:53 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.4767
2022-03-03 14:38:35 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.4507
2022-03-03 14:39:17 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.5217
2022-03-03 14:39:59 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.3665
2022-03-03 14:40:41 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.5152
2022-03-03 14:41:23 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.5742
2022-03-03 14:41:26 - train: epoch 060, train_loss: 1.5153
2022-03-03 14:42:45 - eval: epoch: 060, acc1: 67.944%, acc5: 88.224%, test_loss: 1.2984, per_image_load_time: 1.131ms, per_image_inference_time: 0.957ms
2022-03-03 14:42:46 - until epoch: 060, best_acc1: 68.016%
2022-03-03 14:42:46 - epoch 061 lr: 0.0010000000000000002
2022-03-03 14:43:33 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.2995
2022-03-03 14:44:16 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.4330
2022-03-03 14:44:58 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.2672
2022-03-03 14:45:40 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.6204
2022-03-03 14:46:23 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.3361
2022-03-03 14:47:05 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.3794
2022-03-03 14:47:47 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.2294
2022-03-03 14:48:29 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.4219
2022-03-03 14:49:11 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.3003
2022-03-03 14:49:53 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.1376
2022-03-03 14:50:36 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.1674
2022-03-03 14:51:18 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.3232
2022-03-03 14:52:00 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.2663
2022-03-03 14:52:42 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.2381
2022-03-03 14:53:24 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.4164
2022-03-03 14:54:06 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.1540
2022-03-03 14:54:48 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.3386
2022-03-03 14:55:30 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.3142
2022-03-03 14:56:12 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.4892
2022-03-03 14:56:54 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.2743
2022-03-03 14:57:36 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.7661
2022-03-03 14:58:18 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.2242
2022-03-03 14:59:00 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.2684
2022-03-03 14:59:41 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.0748
2022-03-03 15:00:23 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.2065
2022-03-03 15:01:05 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.4436
2022-03-03 15:01:47 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.5272
2022-03-03 15:02:29 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.3487
2022-03-03 15:03:11 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.4720
2022-03-03 15:03:53 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.2396
2022-03-03 15:04:35 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.5164
2022-03-03 15:05:17 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.3954
2022-03-03 15:06:00 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.3322
2022-03-03 15:06:42 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.1249
2022-03-03 15:07:24 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.3113
2022-03-03 15:08:06 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.3495
2022-03-03 15:08:48 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.4308
2022-03-03 15:09:30 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.3644
2022-03-03 15:10:12 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.3568
2022-03-03 15:10:54 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.3291
2022-03-03 15:11:36 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.4644
2022-03-03 15:12:18 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.2705
2022-03-03 15:13:00 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.3353
2022-03-03 15:13:43 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.3295
2022-03-03 15:14:25 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.3467
2022-03-03 15:15:07 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.4567
2022-03-03 15:15:49 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.2507
2022-03-03 15:16:32 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.2929
2022-03-03 15:17:14 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.4114
2022-03-03 15:17:56 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.4179
2022-03-03 15:17:59 - train: epoch 061, train_loss: 1.3413
2022-03-03 15:19:17 - eval: epoch: 061, acc1: 71.302%, acc5: 90.108%, test_loss: 1.1483, per_image_load_time: 1.956ms, per_image_inference_time: 0.964ms
2022-03-03 15:19:17 - until epoch: 061, best_acc1: 71.302%
2022-03-03 15:19:17 - epoch 062 lr: 0.0010000000000000002
2022-03-03 15:20:04 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.2831
2022-03-03 15:20:46 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.4334
2022-03-03 15:21:27 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.2368
2022-03-03 15:22:09 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.1681
2022-03-03 15:22:51 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.2571
2022-03-03 15:23:33 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.2247
2022-03-03 15:24:15 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.5956
2022-03-03 15:24:57 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.3510
2022-03-03 15:25:39 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.1920
2022-03-03 15:26:21 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.4360
2022-03-03 15:27:04 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.1961
2022-03-03 15:27:46 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.5717
2022-03-03 15:28:28 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.3578
2022-03-03 15:29:10 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.2955
2022-03-03 15:29:52 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.3841
2022-03-03 15:30:35 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.4400
2022-03-03 15:31:17 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.4308
2022-03-03 15:31:59 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.1500
2022-03-03 15:32:41 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.3434
2022-03-03 15:33:23 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.2330
2022-03-03 15:34:05 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.3194
2022-03-03 15:34:47 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.2020
2022-03-03 15:35:29 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.2648
2022-03-03 15:36:11 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.1455
2022-03-03 15:36:53 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.3426
2022-03-03 15:37:35 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.1636
2022-03-03 15:38:17 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.2792
2022-03-03 15:38:58 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.3605
2022-03-03 15:39:40 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.4190
2022-03-03 15:40:22 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.4128
2022-03-03 15:41:04 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.3825
2022-03-03 15:41:46 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.1481
2022-03-03 15:42:28 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.4152
2022-03-03 15:43:10 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.4019
2022-03-03 15:43:52 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.2975
2022-03-03 15:44:33 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.3344
2022-03-03 15:45:15 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.2900
2022-03-03 15:45:57 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.2978
2022-03-03 15:46:39 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.2565
2022-03-03 15:47:21 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.1177
2022-03-03 15:48:03 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.4005
2022-03-03 15:48:45 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 1.1323
2022-03-03 15:49:27 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.3622
2022-03-03 15:50:09 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.2695
2022-03-03 15:50:50 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.1711
2022-03-03 15:51:33 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 1.0603
2022-03-03 15:52:15 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.3004
2022-03-03 15:52:57 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.2188
2022-03-03 15:53:39 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.1785
2022-03-03 15:54:22 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.3199
2022-03-03 15:54:26 - train: epoch 062, train_loss: 1.2966
2022-03-03 15:55:41 - eval: epoch: 062, acc1: 71.650%, acc5: 90.330%, test_loss: 1.1314, per_image_load_time: 1.861ms, per_image_inference_time: 0.974ms
2022-03-03 15:55:42 - until epoch: 062, best_acc1: 71.650%
2022-03-03 15:55:42 - epoch 063 lr: 0.0010000000000000002
2022-03-03 15:56:29 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 1.2656
2022-03-03 15:57:10 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.2053
2022-03-03 15:57:52 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.3634
2022-03-03 15:58:34 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.3071
2022-03-03 15:59:15 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.1979
2022-03-03 15:59:57 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.3677
2022-03-03 16:00:39 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.2453
2022-03-03 16:01:20 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.2922
2022-03-03 16:02:02 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.3806
2022-03-03 16:02:44 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.3391
2022-03-03 16:03:25 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.0971
2022-03-03 16:04:07 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.2513
2022-03-03 16:04:49 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.2896
2022-03-03 16:05:30 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.6008
2022-03-03 16:06:12 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.1810
2022-03-03 16:06:53 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.1755
2022-03-03 16:07:35 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.1662
2022-03-03 16:08:17 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.3153
2022-03-03 16:08:58 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.3888
2022-03-03 16:09:40 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 1.1322
2022-03-03 16:10:21 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.2230
2022-03-03 16:11:03 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.5155
2022-03-03 16:11:45 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.4048
2022-03-03 16:12:27 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.4866
2022-03-03 16:13:09 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.2273
2022-03-03 16:13:51 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.1966
2022-03-03 16:14:32 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.4228
2022-03-03 16:15:14 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.2433
2022-03-03 16:15:56 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.2765
2022-03-03 16:16:39 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.3995
2022-03-03 16:17:21 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.5250
2022-03-03 16:18:03 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.4416
2022-03-03 16:18:45 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.2797
2022-03-03 16:19:27 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.0709
2022-03-03 16:20:09 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.4036
2022-03-03 16:20:52 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.1251
2022-03-03 16:21:34 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.4387
2022-03-03 16:22:16 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.4899
2022-03-03 16:22:58 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 0.9720
2022-03-03 16:23:41 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 0.9897
2022-03-03 16:24:23 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.4077
2022-03-03 16:25:05 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.2204
2022-03-03 16:25:48 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.4622
2022-03-03 16:26:30 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.3217
2022-03-03 16:27:12 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.1638
2022-03-03 16:27:54 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.2599
2022-03-03 16:28:37 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.1300
2022-03-03 16:29:19 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.3062
2022-03-03 16:30:01 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.2341
2022-03-03 16:30:44 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.2220
2022-03-03 16:30:47 - train: epoch 063, train_loss: 1.2803
2022-03-03 16:32:01 - eval: epoch: 063, acc1: 71.780%, acc5: 90.460%, test_loss: 1.1260, per_image_load_time: 1.757ms, per_image_inference_time: 0.960ms
2022-03-03 16:32:02 - until epoch: 063, best_acc1: 71.780%
2022-03-03 16:32:02 - epoch 064 lr: 0.0010000000000000002
2022-03-03 16:32:49 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.2493
2022-03-03 16:33:31 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.3457
2022-03-03 16:34:13 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.0999
2022-03-03 16:34:54 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.3651
2022-03-03 16:35:36 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.0667
2022-03-03 16:36:18 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.4751
2022-03-03 16:37:00 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.4025
2022-03-03 16:37:41 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 1.1260
2022-03-03 16:38:23 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.2976
2022-03-03 16:39:05 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.2577
2022-03-03 16:39:47 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.1873
2022-03-03 16:40:29 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.1024
2022-03-03 16:41:11 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.3354
2022-03-03 16:41:53 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.5447
2022-03-03 16:42:35 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.3911
2022-03-03 16:43:16 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.1294
2022-03-03 16:43:58 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.0840
2022-03-03 16:44:40 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.1425
2022-03-03 16:45:22 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.2329
2022-03-03 16:46:04 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.1861
2022-03-03 16:46:46 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 1.2199
2022-03-03 16:47:28 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.3050
2022-03-03 16:48:10 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.3855
2022-03-03 16:48:52 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.2610
2022-03-03 16:49:34 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 1.1868
2022-03-03 16:50:16 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.1488
2022-03-03 16:50:58 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.2871
2022-03-03 16:51:40 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.0768
2022-03-03 16:52:22 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.5185
2022-03-03 16:53:05 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.3574
2022-03-03 16:53:47 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.1481
2022-03-03 16:54:30 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.2868
2022-03-03 16:55:12 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.1518
2022-03-03 16:55:54 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.4005
2022-03-03 16:56:37 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.2822
2022-03-03 16:57:19 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.2710
2022-03-03 16:58:02 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 1.0087
2022-03-03 16:58:44 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.4361
2022-03-03 16:59:27 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.1941
2022-03-03 17:00:09 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.0739
2022-03-03 17:00:51 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.3214
2022-03-03 17:01:34 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 1.1912
2022-03-03 17:02:16 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.5010
2022-03-03 17:02:59 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.1626
2022-03-03 17:03:41 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 1.1138
2022-03-03 17:04:23 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.3709
2022-03-03 17:05:05 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.5779
2022-03-03 17:05:48 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.2554
2022-03-03 17:06:30 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.4223
2022-03-03 17:07:12 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.1696
2022-03-03 17:07:15 - train: epoch 064, train_loss: 1.2712
2022-03-03 17:08:31 - eval: epoch: 064, acc1: 71.980%, acc5: 90.550%, test_loss: 1.1196, per_image_load_time: 1.925ms, per_image_inference_time: 0.964ms
2022-03-03 17:08:31 - until epoch: 064, best_acc1: 71.980%
2022-03-03 17:08:31 - epoch 065 lr: 0.0010000000000000002
2022-03-03 17:09:19 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.3949
2022-03-03 17:10:00 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.2473
2022-03-03 17:10:42 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.2693
2022-03-03 17:11:24 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.3272
2022-03-03 17:12:05 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.2239
2022-03-03 17:12:47 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.3117
2022-03-03 17:13:28 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.2509
2022-03-03 17:14:10 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.2077
2022-03-03 17:14:52 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.1479
2022-03-03 17:15:33 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.2388
2022-03-03 17:16:15 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.2600
2022-03-03 17:16:57 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.4253
2022-03-03 17:17:38 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.1874
2022-03-03 17:18:20 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 1.0569
2022-03-03 17:19:02 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.2486
2022-03-03 17:19:44 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.2864
2022-03-03 17:20:26 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.2422
2022-03-03 17:21:08 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.1775
2022-03-03 17:21:50 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 1.1400
2022-03-03 17:22:32 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.1835
2022-03-03 17:23:14 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.1967
2022-03-03 17:23:56 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.3802
2022-03-03 17:24:38 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.1619
2022-03-03 17:25:20 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.1908
2022-03-03 17:26:02 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.2380
2022-03-03 17:26:45 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.4234
2022-03-03 17:27:27 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.3166
2022-03-03 17:28:09 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.1789
2022-03-03 17:28:51 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.3138
2022-03-03 17:29:33 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.1084
2022-03-03 17:30:15 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.2905
2022-03-03 17:30:57 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.2774
2022-03-03 17:31:39 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.2110
2022-03-03 17:32:21 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.1563
2022-03-03 17:33:03 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.5122
2022-03-03 17:33:45 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.3188
2022-03-03 17:34:28 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.0760
2022-03-03 17:35:10 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.0547
2022-03-03 17:35:52 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.3316
2022-03-03 17:36:34 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.2955
2022-03-03 17:37:16 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.1717
2022-03-03 17:37:58 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.3691
2022-03-03 17:38:40 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.1557
2022-03-03 17:39:22 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.2674
2022-03-03 17:40:04 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.2628
2022-03-03 17:40:46 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.2331
2022-03-03 17:41:28 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.2578
2022-03-03 17:42:10 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 1.0416
2022-03-03 17:42:53 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.1492
2022-03-03 17:43:35 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.4020
2022-03-03 17:43:38 - train: epoch 065, train_loss: 1.2614
2022-03-03 17:44:55 - eval: epoch: 065, acc1: 72.040%, acc5: 90.630%, test_loss: 1.1161, per_image_load_time: 1.955ms, per_image_inference_time: 0.986ms
2022-03-03 17:44:55 - until epoch: 065, best_acc1: 72.040%
2022-03-03 17:44:55 - epoch 066 lr: 0.0010000000000000002
2022-03-03 17:45:42 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.1512
2022-03-03 17:46:24 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.3761
2022-03-03 17:47:06 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 1.0426
2022-03-03 17:47:48 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 1.0295
2022-03-03 17:48:30 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.3005
2022-03-03 17:49:12 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.2354
2022-03-03 17:49:54 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.2628
2022-03-03 17:50:35 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.5156
2022-03-03 17:51:17 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.2751
2022-03-03 17:51:59 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.3568
2022-03-03 17:52:41 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.3347
2022-03-03 17:53:23 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.3451
2022-03-03 17:54:05 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.3829
2022-03-03 17:54:47 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 1.0647
2022-03-03 17:55:29 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.2962
2022-03-03 17:56:11 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.2487
2022-03-03 17:56:53 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.1459
2022-03-03 17:57:35 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.0545
2022-03-03 17:58:17 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.4760
2022-03-03 17:58:59 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.3490
2022-03-03 17:59:42 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.2470
2022-03-03 18:00:24 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 1.2730
2022-03-03 18:01:06 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.4032
2022-03-03 18:01:48 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.1793
2022-03-03 18:02:31 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.2177
2022-03-03 18:03:13 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.1662
2022-03-03 18:03:55 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.4765
2022-03-03 18:04:37 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.3454
2022-03-03 18:05:19 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.1710
2022-03-03 18:06:02 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.2145
2022-03-03 18:06:44 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.4186
2022-03-03 18:07:26 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.1079
2022-03-03 18:08:08 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.2110
2022-03-03 18:08:51 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.4645
2022-03-03 18:09:33 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.3191
2022-03-03 18:10:15 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.3117
2022-03-03 18:10:57 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.3693
2022-03-03 18:11:39 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.1149
2022-03-03 18:12:21 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.0682
2022-03-03 18:13:03 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.4161
2022-03-03 18:13:45 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 1.0742
2022-03-03 18:14:27 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 1.0568
2022-03-03 18:15:09 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.0887
2022-03-03 18:15:51 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.1892
2022-03-03 18:16:33 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.2945
2022-03-03 18:17:15 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.4104
2022-03-03 18:17:57 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.1955
2022-03-03 18:18:39 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.1732
2022-03-03 18:19:21 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.2293
2022-03-03 18:20:03 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 1.1778
2022-03-03 18:20:07 - train: epoch 066, train_loss: 1.2551
2022-03-03 18:21:23 - eval: epoch: 066, acc1: 72.168%, acc5: 90.694%, test_loss: 1.1120, per_image_load_time: 1.885ms, per_image_inference_time: 0.961ms
2022-03-03 18:21:24 - until epoch: 066, best_acc1: 72.168%
2022-03-03 18:21:24 - epoch 067 lr: 0.0010000000000000002
2022-03-03 18:22:11 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.2114
2022-03-03 18:22:52 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.2076
2022-03-03 18:23:34 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.5460
2022-03-03 18:24:16 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.2880
2022-03-03 18:24:57 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.1621
2022-03-03 18:25:39 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 1.1574
2022-03-03 18:26:21 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.2273
2022-03-03 18:27:03 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.3559
2022-03-03 18:27:45 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.3815
2022-03-03 18:28:27 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 1.0798
2022-03-03 18:29:09 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 1.2662
2022-03-03 18:29:51 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.2297
2022-03-03 18:30:33 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.3587
2022-03-03 18:31:15 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.2710
2022-03-03 18:31:57 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.3107
2022-03-03 18:32:39 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.2847
2022-03-03 18:33:21 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.1418
2022-03-03 18:34:03 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.5070
2022-03-03 18:34:45 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.2121
2022-03-03 18:35:27 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.4679
2022-03-03 18:36:09 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 1.1485
2022-03-03 18:36:51 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.3460
2022-03-03 18:37:33 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.2119
2022-03-03 18:38:15 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 1.1682
2022-03-03 18:38:57 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 1.1492
2022-03-03 18:39:40 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.1522
2022-03-03 18:40:22 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.1247
2022-03-03 18:41:04 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.2632
2022-03-03 18:41:46 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.1907
2022-03-03 18:42:28 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.2073
2022-03-03 18:43:11 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 1.0832
2022-03-03 18:43:53 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.3845
2022-03-03 18:44:35 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.0602
2022-03-03 18:45:17 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.2765
2022-03-03 18:45:59 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.1679
2022-03-03 18:46:41 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.2229
2022-03-03 18:47:23 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.4340
2022-03-03 18:48:06 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.2244
2022-03-03 18:48:48 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 1.3211
2022-03-03 18:49:30 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.3749
2022-03-03 18:50:12 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.3365
2022-03-03 18:50:54 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.4399
2022-03-03 18:51:37 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.1104
2022-03-03 18:52:19 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.2764
2022-03-03 18:53:01 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 1.1025
2022-03-03 18:53:43 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 0.9540
2022-03-03 18:54:26 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.1589
2022-03-03 18:55:08 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.0414
2022-03-03 18:55:50 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.3441
2022-03-03 18:56:33 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.3043
2022-03-03 18:56:36 - train: epoch 067, train_loss: 1.2484
2022-03-03 18:57:52 - eval: epoch: 067, acc1: 72.066%, acc5: 90.534%, test_loss: 1.1123, per_image_load_time: 1.193ms, per_image_inference_time: 0.956ms
2022-03-03 18:57:53 - until epoch: 067, best_acc1: 72.168%
2022-03-03 18:57:53 - epoch 068 lr: 0.0010000000000000002
2022-03-03 18:58:40 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.2714
2022-03-03 18:59:22 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.2287
2022-03-03 19:00:04 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.3633
2022-03-03 19:00:46 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.2243
2022-03-03 19:01:28 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.2476
2022-03-03 19:02:11 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.2845
2022-03-03 19:02:53 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.5372
2022-03-03 19:03:35 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.2075
2022-03-03 19:04:17 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.2380
2022-03-03 19:05:00 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.2188
2022-03-03 19:05:42 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.3527
2022-03-03 19:06:24 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 1.1328
2022-03-03 19:07:06 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.1076
2022-03-03 19:07:49 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.2847
2022-03-03 19:08:31 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.3365
2022-03-03 19:09:14 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.2828
2022-03-03 19:09:56 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.4026
2022-03-03 19:10:38 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.2532
2022-03-03 19:11:21 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.3352
2022-03-03 19:12:03 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.3967
2022-03-03 19:12:45 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.2250
2022-03-03 19:13:28 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.2431
2022-03-03 19:14:10 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 1.1357
2022-03-03 19:14:52 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.3643
2022-03-03 19:15:35 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.3104
2022-03-03 19:16:17 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.2213
2022-03-03 19:16:59 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.2333
2022-03-03 19:17:42 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.3533
2022-03-03 19:18:24 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.3416
2022-03-03 19:19:06 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.5266
2022-03-03 19:19:49 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.1047
2022-03-03 19:20:31 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.4135
2022-03-03 19:21:13 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.2085
2022-03-03 19:21:56 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 1.1417
2022-03-03 19:22:38 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.2422
2022-03-03 19:23:20 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.1807
2022-03-03 19:24:03 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.2672
2022-03-03 19:24:45 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.3833
2022-03-03 19:25:27 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.3070
2022-03-03 19:26:10 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.2379
2022-03-03 19:26:52 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.1290
2022-03-03 19:27:34 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.4120
2022-03-03 19:28:16 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 1.3325
2022-03-03 19:28:59 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.2854
2022-03-03 19:29:41 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.3029
2022-03-03 19:30:23 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.2831
2022-03-03 19:31:06 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.5022
2022-03-03 19:31:48 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.3843
2022-03-03 19:32:31 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.3499
2022-03-03 19:33:13 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.2244
2022-03-03 19:33:17 - train: epoch 068, train_loss: 1.2434
2022-03-03 19:34:33 - eval: epoch: 068, acc1: 72.226%, acc5: 90.762%, test_loss: 1.1066, per_image_load_time: 1.908ms, per_image_inference_time: 0.956ms
2022-03-03 19:34:33 - until epoch: 068, best_acc1: 72.226%
2022-03-03 19:34:33 - epoch 069 lr: 0.0010000000000000002
2022-03-03 19:35:21 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.3828
2022-03-03 19:36:03 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.4315
2022-03-03 19:36:46 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.3128
2022-03-03 19:37:28 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.1226
2022-03-03 19:38:10 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.1941
2022-03-03 19:38:53 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 1.1449
2022-03-03 19:39:35 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.2023
2022-03-03 19:40:17 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.2261
2022-03-03 19:40:59 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.1844
2022-03-03 19:41:42 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.2404
2022-03-03 19:42:24 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.1767
2022-03-03 19:43:06 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.1862
2022-03-03 19:43:48 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.4773
2022-03-03 19:44:31 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.3210
2022-03-03 19:45:13 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.2920
2022-03-03 19:45:55 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.3028
2022-03-03 19:46:37 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.1632
2022-03-03 19:47:20 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.0421
2022-03-03 19:48:02 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.1688
2022-03-03 19:48:45 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 1.0675
2022-03-03 19:49:27 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.3076
2022-03-03 19:50:10 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.2772
2022-03-03 19:50:52 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.2635
2022-03-03 19:51:35 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.3875
2022-03-03 19:52:18 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.2273
2022-03-03 19:53:00 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.2778
2022-03-03 19:53:43 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.4883
2022-03-03 19:54:25 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.3433
2022-03-03 19:55:08 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 1.1207
2022-03-03 19:55:50 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.1960
2022-03-03 19:56:33 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 1.1389
2022-03-03 19:57:15 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 1.1187
2022-03-03 19:57:58 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.0901
2022-03-03 19:58:40 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 1.0645
2022-03-03 19:59:22 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 1.1397
2022-03-03 20:00:05 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.0918
2022-03-03 20:00:47 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.3084
2022-03-03 20:01:30 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.3735
2022-03-03 20:02:12 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.2500
2022-03-03 20:02:55 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.2940
2022-03-03 20:03:37 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.3690
2022-03-03 20:04:19 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.1628
2022-03-03 20:05:02 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.2479
2022-03-03 20:05:44 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.2123
2022-03-03 20:06:26 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.1887
2022-03-03 20:07:08 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.3979
2022-03-03 20:07:51 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.1854
2022-03-03 20:08:33 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.2433
2022-03-03 20:09:15 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 1.1331
2022-03-03 20:09:57 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.3262
2022-03-03 20:10:01 - train: epoch 069, train_loss: 1.2378
2022-03-03 20:11:18 - eval: epoch: 069, acc1: 72.148%, acc5: 90.626%, test_loss: 1.1077, per_image_load_time: 1.955ms, per_image_inference_time: 0.960ms
2022-03-03 20:11:18 - until epoch: 069, best_acc1: 72.226%
2022-03-03 20:11:18 - epoch 070 lr: 0.0010000000000000002
2022-03-03 20:12:06 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.3185
2022-03-03 20:12:48 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.3319
2022-03-03 20:13:30 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.3072
2022-03-03 20:14:13 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.1440
2022-03-03 20:14:55 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.2477
2022-03-03 20:15:37 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.1778
2022-03-03 20:16:19 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.1729
2022-03-03 20:17:01 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.1796
2022-03-03 20:17:43 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.2756
2022-03-03 20:18:26 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.2207
2022-03-03 20:19:08 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.5807
2022-03-03 20:19:50 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 1.0990
2022-03-03 20:20:32 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.1737
2022-03-03 20:21:14 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.2084
2022-03-03 20:21:57 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 1.1778
2022-03-03 20:22:39 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.2457
2022-03-03 20:23:21 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.3222
2022-03-03 20:24:04 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.1142
2022-03-03 20:24:46 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.1431
2022-03-03 20:25:28 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.2378
2022-03-03 20:26:11 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 1.3300
2022-03-03 20:26:53 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.3026
2022-03-03 20:27:35 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.2849
2022-03-03 20:28:17 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.3870
2022-03-03 20:28:59 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.1812
2022-03-03 20:29:41 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.2366
2022-03-03 20:30:23 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.1721
2022-03-03 20:31:06 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.3729
2022-03-03 20:31:48 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.4306
2022-03-03 20:32:30 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.1973
2022-03-03 20:33:12 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.2266
2022-03-03 20:33:55 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.3048
2022-03-03 20:34:37 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 1.1884
2022-03-03 20:35:19 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.2768
2022-03-03 20:36:01 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.2954
2022-03-03 20:36:44 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.3131
2022-03-03 20:37:26 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.3196
2022-03-03 20:38:08 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.1937
2022-03-03 20:38:50 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 1.0433
2022-03-03 20:39:33 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.0760
2022-03-03 20:40:15 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 1.2079
2022-03-03 20:40:57 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.2339
2022-03-03 20:41:39 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.2911
2022-03-03 20:42:22 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.2528
2022-03-03 20:43:04 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.2106
2022-03-03 20:43:46 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.3663
2022-03-03 20:44:29 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.3378
2022-03-03 20:45:11 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.2791
2022-03-03 20:45:53 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.3477
2022-03-03 20:46:36 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.2891
2022-03-03 20:46:39 - train: epoch 070, train_loss: 1.2347
2022-03-03 20:47:56 - eval: epoch: 070, acc1: 72.250%, acc5: 90.698%, test_loss: 1.1042, per_image_load_time: 1.982ms, per_image_inference_time: 0.950ms
2022-03-03 20:47:57 - until epoch: 070, best_acc1: 72.250%
2022-03-03 20:47:57 - epoch 071 lr: 0.0010000000000000002
2022-03-03 20:48:45 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.1008
2022-03-03 20:49:27 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.2043
2022-03-03 20:50:09 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.1037
2022-03-03 20:50:51 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.2733
2022-03-03 20:51:33 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.2251
2022-03-03 20:52:15 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.3438
2022-03-03 20:52:57 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.3813
2022-03-03 20:53:39 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.1551
2022-03-03 20:54:21 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.3263
2022-03-03 20:55:03 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.3273
2022-03-03 20:55:45 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.3582
2022-03-03 20:56:27 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.2067
2022-03-03 20:57:09 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.2335
2022-03-03 20:57:51 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.2198
2022-03-03 20:58:33 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 1.0688
2022-03-03 20:59:15 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 1.1210
2022-03-03 20:59:58 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.2207
2022-03-03 21:00:40 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.1644
2022-03-03 21:01:22 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 1.1708
2022-03-03 21:02:04 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.3294
2022-03-03 21:02:46 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.1152
2022-03-03 21:03:28 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 1.0716
2022-03-03 21:04:10 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 1.2550
2022-03-03 21:04:52 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.1781
2022-03-03 21:05:34 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.3764
2022-03-03 21:06:16 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.1249
2022-03-03 21:06:59 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.2814
2022-03-03 21:07:41 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.2906
2022-03-03 21:08:23 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.2491
2022-03-03 21:09:05 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.3049
2022-03-03 21:09:48 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.1520
2022-03-03 21:10:30 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.0707
2022-03-03 21:11:12 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.0897
2022-03-03 21:11:54 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.1316
2022-03-03 21:12:37 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.3511
2022-03-03 21:13:19 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.3621
2022-03-03 21:14:01 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.1297
2022-03-03 21:14:44 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.2083
2022-03-03 21:15:26 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.2817
2022-03-03 21:16:08 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.4630
2022-03-03 21:16:51 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.0749
2022-03-03 21:17:33 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.3291
2022-03-03 21:18:15 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.2250
2022-03-03 21:18:57 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.2288
2022-03-03 21:19:40 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.2010
2022-03-03 21:20:24 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.3768
2022-03-03 21:21:06 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.1017
2022-03-03 21:21:49 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 1.1773
2022-03-03 21:22:31 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 1.0040
2022-03-03 21:23:13 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.1453
2022-03-03 21:23:16 - train: epoch 071, train_loss: 1.2303
2022-03-03 21:24:33 - eval: epoch: 071, acc1: 72.192%, acc5: 90.716%, test_loss: 1.1022, per_image_load_time: 1.980ms, per_image_inference_time: 0.949ms
2022-03-03 21:24:34 - until epoch: 071, best_acc1: 72.250%
2022-03-03 21:24:34 - epoch 072 lr: 0.0010000000000000002
2022-03-03 21:25:22 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.3834
2022-03-03 21:26:04 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.0702
2022-03-03 21:26:46 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.1701
2022-03-03 21:27:28 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.2882
2022-03-03 21:28:10 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.1376
2022-03-03 21:28:52 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.1424
2022-03-03 21:29:34 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.1788
2022-03-03 21:30:16 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.4930
2022-03-03 21:30:58 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.0680
2022-03-03 21:31:39 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.1628
2022-03-03 21:32:21 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 1.3050
2022-03-03 21:33:03 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 1.1215
2022-03-03 21:33:45 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.2055
2022-03-03 21:34:27 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.3003
2022-03-03 21:35:09 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.1014
2022-03-03 21:35:51 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.3374
2022-03-03 21:36:33 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 1.1170
2022-03-03 21:37:15 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.1018
2022-03-03 21:37:58 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.1123
2022-03-03 21:38:40 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.3498
2022-03-03 21:39:22 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.2195
2022-03-03 21:40:04 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.2982
2022-03-03 21:40:46 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.3449
2022-03-03 21:41:28 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 1.2109
2022-03-03 21:42:10 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.0957
2022-03-03 21:42:52 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 1.0788
2022-03-03 21:43:34 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 1.2525
2022-03-03 21:44:16 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.2879
2022-03-03 21:44:58 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.2223
2022-03-03 21:45:40 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.1665
2022-03-03 21:46:22 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.3516
2022-03-03 21:47:04 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.1772
2022-03-03 21:47:46 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.1379
2022-03-03 21:48:28 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 1.3501
2022-03-03 21:49:10 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 1.2310
2022-03-03 21:49:52 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.2958
2022-03-03 21:50:34 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.3064
2022-03-03 21:51:16 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.2235
2022-03-03 21:51:58 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.3765
2022-03-03 21:52:40 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.1654
2022-03-03 21:53:22 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.3752
2022-03-03 21:54:04 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.1856
2022-03-03 21:54:46 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.2413
2022-03-03 21:55:28 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 1.1528
2022-03-03 21:56:10 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.2588
2022-03-03 21:56:53 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.1501
2022-03-03 21:57:35 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.2649
2022-03-03 21:58:17 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.3366
2022-03-03 21:58:59 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.2713
2022-03-03 21:59:42 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.1350
2022-03-03 21:59:45 - train: epoch 072, train_loss: 1.2287
2022-03-03 22:01:01 - eval: epoch: 072, acc1: 72.196%, acc5: 90.682%, test_loss: 1.1023, per_image_load_time: 1.297ms, per_image_inference_time: 0.961ms
2022-03-03 22:01:01 - until epoch: 072, best_acc1: 72.250%
2022-03-03 22:01:01 - epoch 073 lr: 0.0010000000000000002
2022-03-03 22:01:48 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.4852
2022-03-03 22:02:30 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.2421
2022-03-03 22:03:11 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.3162
2022-03-03 22:03:53 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 0.9767
2022-03-03 22:04:35 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.1344
2022-03-03 22:05:16 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.0560
2022-03-03 22:05:58 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.2437
2022-03-03 22:06:40 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.2486
2022-03-03 22:07:22 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.0269
2022-03-03 22:08:03 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 1.1646
2022-03-03 22:08:45 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.3094
2022-03-03 22:09:27 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.2597
2022-03-03 22:10:09 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.2852
2022-03-03 22:10:51 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.1136
2022-03-03 22:11:33 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.1778
2022-03-03 22:12:15 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.2189
2022-03-03 22:12:57 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.4766
2022-03-03 22:13:39 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 1.0960
2022-03-03 22:14:21 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.3269
2022-03-03 22:15:03 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.0203
2022-03-03 22:15:44 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.2026
2022-03-03 22:16:26 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.4011
2022-03-03 22:17:08 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.2831
2022-03-03 22:17:50 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.2061
2022-03-03 22:18:33 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.3480
2022-03-03 22:19:15 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 1.2474
2022-03-03 22:19:56 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.2941
2022-03-03 22:20:39 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.2816
2022-03-03 22:21:21 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.2682
2022-03-03 22:22:03 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 1.0475
2022-03-03 22:22:45 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.1537
2022-03-03 22:23:27 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 1.1295
2022-03-03 22:24:09 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 1.2265
2022-03-03 22:24:51 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 1.1493
2022-03-03 22:25:33 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.3369
2022-03-03 22:26:15 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 1.0995
2022-03-03 22:26:57 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.2350
2022-03-03 22:27:39 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.4419
2022-03-03 22:28:21 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.2387
2022-03-03 22:29:03 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.3176
2022-03-03 22:29:45 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 1.1604
2022-03-03 22:30:26 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.3863
2022-03-03 22:31:08 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.2053
2022-03-03 22:31:50 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.2145
2022-03-03 22:32:32 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.1103
2022-03-03 22:33:14 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.4028
2022-03-03 22:33:56 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.0695
2022-03-03 22:34:38 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.3504
2022-03-03 22:35:20 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.0568
2022-03-03 22:36:02 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.2946
2022-03-03 22:36:05 - train: epoch 073, train_loss: 1.2249
2022-03-03 22:37:22 - eval: epoch: 073, acc1: 72.370%, acc5: 90.796%, test_loss: 1.0991, per_image_load_time: 1.914ms, per_image_inference_time: 0.957ms
2022-03-03 22:37:22 - until epoch: 073, best_acc1: 72.370%
2022-03-03 22:37:22 - epoch 074 lr: 0.0010000000000000002
2022-03-03 22:38:09 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.1350
2022-03-03 22:38:51 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 1.2439
2022-03-03 22:39:32 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.3136
2022-03-03 22:40:14 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.4102
2022-03-03 22:40:55 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.2868
2022-03-03 22:41:37 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.2687
2022-03-03 22:42:19 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 1.2930
2022-03-03 22:43:00 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.4054
2022-03-03 22:43:42 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.1632
2022-03-03 22:44:24 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.2534
2022-03-03 22:45:05 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.3887
2022-03-03 22:45:47 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 1.2142
2022-03-03 22:46:29 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.3639
2022-03-03 22:47:10 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 1.1602
2022-03-03 22:47:52 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.2403
2022-03-03 22:48:34 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 0.9631
2022-03-03 22:49:15 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.3835
2022-03-03 22:49:57 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.6106
2022-03-03 22:50:39 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.3215
2022-03-03 22:51:20 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 1.0021
2022-03-03 22:52:02 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.1866
2022-03-03 22:52:44 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.5324
2022-03-03 22:53:26 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.3437
2022-03-03 22:54:08 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.2247
2022-03-03 22:54:50 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.0674
2022-03-03 22:55:32 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.0473
2022-03-03 22:56:14 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 1.1744
2022-03-03 22:56:56 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.4489
2022-03-03 22:57:37 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.1320
2022-03-03 22:58:19 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.2730
2022-03-03 22:59:01 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.1359
2022-03-03 22:59:43 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 1.1238
2022-03-03 23:00:25 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.2441
2022-03-03 23:01:07 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.2325
2022-03-03 23:01:49 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.0609
2022-03-03 23:02:31 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 1.3039
2022-03-03 23:03:13 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.3478
2022-03-03 23:03:55 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.1679
2022-03-03 23:04:37 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 1.0228
2022-03-03 23:05:19 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 1.1929
2022-03-03 23:06:01 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.3996
2022-03-03 23:06:43 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.2091
2022-03-03 23:07:25 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.1555
2022-03-03 23:08:07 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 1.0291
2022-03-03 23:08:50 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 1.0846
2022-03-03 23:09:32 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.2319
2022-03-03 23:10:14 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.3044
2022-03-03 23:10:56 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.2759
2022-03-03 23:11:39 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.4931
2022-03-03 23:12:21 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.1511
2022-03-03 23:12:24 - train: epoch 074, train_loss: 1.2210
2022-03-03 23:13:39 - eval: epoch: 074, acc1: 72.474%, acc5: 90.768%, test_loss: 1.0942, per_image_load_time: 1.885ms, per_image_inference_time: 0.963ms
2022-03-03 23:13:39 - until epoch: 074, best_acc1: 72.474%
2022-03-03 23:13:39 - epoch 075 lr: 0.0010000000000000002
2022-03-03 23:14:26 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 1.2643
2022-03-03 23:15:07 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 1.2064
2022-03-03 23:15:49 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.2710
2022-03-03 23:16:31 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.2868
2022-03-03 23:17:13 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.0760
2022-03-03 23:17:54 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.1114
2022-03-03 23:18:36 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.3663
2022-03-03 23:19:17 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.3232
2022-03-03 23:19:59 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.3516
2022-03-03 23:20:40 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 1.0258
2022-03-03 23:21:22 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.2732
2022-03-03 23:22:04 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.1601
2022-03-03 23:22:45 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.1520
2022-03-03 23:23:27 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.2445
2022-03-03 23:24:09 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.3630
2022-03-03 23:24:51 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 0.9513
2022-03-03 23:25:33 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 1.2024
2022-03-03 23:26:15 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.1910
2022-03-03 23:26:57 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 1.0597
2022-03-03 23:27:39 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.3102
2022-03-03 23:28:21 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.1841
2022-03-03 23:29:03 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.2197
2022-03-03 23:29:46 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 1.1423
2022-03-03 23:30:28 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.2693
2022-03-03 23:31:10 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.3177
2022-03-03 23:31:52 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.2870
2022-03-03 23:32:34 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 0.9537
2022-03-03 23:33:16 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.2584
2022-03-03 23:33:59 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.2560
2022-03-03 23:34:41 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.4149
2022-03-03 23:35:23 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.2707
2022-03-03 23:36:05 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.1936
2022-03-03 23:36:48 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.2648
2022-03-03 23:37:30 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 1.0338
2022-03-03 23:38:12 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.1321
2022-03-03 23:38:54 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.1633
2022-03-03 23:39:37 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.2564
2022-03-03 23:40:19 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 1.0878
2022-03-03 23:41:01 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.4412
2022-03-03 23:41:43 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.1089
2022-03-03 23:42:25 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 0.9565
2022-03-03 23:43:07 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.2581
2022-03-03 23:43:49 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.3703
2022-03-03 23:44:32 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 1.0571
2022-03-03 23:45:14 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.2759
2022-03-03 23:45:56 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 1.0672
2022-03-03 23:46:38 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.3587
2022-03-03 23:47:21 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.1840
2022-03-03 23:48:03 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.0659
2022-03-03 23:48:45 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.2705
2022-03-03 23:48:48 - train: epoch 075, train_loss: 1.2189
2022-03-03 23:50:05 - eval: epoch: 075, acc1: 72.362%, acc5: 90.776%, test_loss: 1.0959, per_image_load_time: 1.961ms, per_image_inference_time: 0.947ms
2022-03-03 23:50:05 - until epoch: 075, best_acc1: 72.474%
2022-03-03 23:50:05 - epoch 076 lr: 0.0010000000000000002
2022-03-03 23:50:52 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 1.0237
2022-03-03 23:51:34 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.2550
2022-03-03 23:52:15 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.2861
2022-03-03 23:52:57 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.2602
2022-03-03 23:53:39 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.2765
2022-03-03 23:54:21 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.2330
2022-03-03 23:55:03 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.2663
2022-03-03 23:55:45 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.1355
2022-03-03 23:56:27 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 1.0226
2022-03-03 23:57:09 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.0026
2022-03-03 23:57:51 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.0961
2022-03-03 23:58:33 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.1724
2022-03-03 23:59:15 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.2182
2022-03-03 23:59:57 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 1.0666
2022-03-04 00:00:39 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.1054
2022-03-04 00:01:21 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.2049
2022-03-04 00:02:03 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.2904
2022-03-04 00:02:45 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 1.0192
2022-03-04 00:03:27 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.2408
2022-03-04 00:04:09 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 1.1851
2022-03-04 00:04:51 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.3471
2022-03-04 00:05:33 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.3377
2022-03-04 00:06:15 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 1.1095
2022-03-04 00:06:57 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.4196
2022-03-04 00:07:39 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 1.1690
2022-03-04 00:08:22 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.4435
2022-03-04 00:09:04 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.2914
2022-03-04 00:09:46 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 1.1879
2022-03-04 00:10:29 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 1.2705
2022-03-04 00:11:11 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 1.0938
2022-03-04 00:11:53 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 1.2337
2022-03-04 00:12:35 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.3191
2022-03-04 00:13:17 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.1653
2022-03-04 00:13:59 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.2599
2022-03-04 00:14:42 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 1.1469
2022-03-04 00:15:25 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 1.1340
2022-03-04 00:16:07 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 1.1044
2022-03-04 00:16:50 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 1.0381
2022-03-04 00:17:32 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 1.0862
2022-03-04 00:18:15 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 1.2201
2022-03-04 00:18:58 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 1.1243
2022-03-04 00:19:41 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.4306
2022-03-04 00:20:24 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 1.3146
2022-03-04 00:21:06 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 1.0944
2022-03-04 00:21:49 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 1.1505
2022-03-04 00:22:32 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 1.1086
2022-03-04 00:23:15 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.3473
2022-03-04 00:23:58 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 0.9801
2022-03-04 00:24:41 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 1.2376
2022-03-04 00:25:24 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.2266
2022-03-04 00:25:27 - train: epoch 076, train_loss: 1.2166
2022-03-04 00:26:43 - eval: epoch: 076, acc1: 72.462%, acc5: 90.750%, test_loss: 1.0976, per_image_load_time: 1.949ms, per_image_inference_time: 0.940ms
2022-03-04 00:26:44 - until epoch: 076, best_acc1: 72.474%
2022-03-04 00:26:44 - epoch 077 lr: 0.0010000000000000002
2022-03-04 00:27:32 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 1.2890
2022-03-04 00:28:14 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 1.2972
2022-03-04 00:28:57 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 0.9612
2022-03-04 00:29:40 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.3139
2022-03-04 00:30:22 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 1.1270
2022-03-04 00:31:05 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 0.9710
2022-03-04 00:31:47 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 1.3023
2022-03-04 00:32:30 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.3233
2022-03-04 00:33:12 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.3212
2022-03-04 00:33:55 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 1.0467
2022-03-04 00:34:38 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 1.2939
2022-03-04 00:35:21 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 1.3086
2022-03-04 00:36:04 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 0.9950
2022-03-04 00:36:47 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.2423
2022-03-04 00:37:30 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 1.0799
2022-03-04 00:38:12 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 1.2271
2022-03-04 00:38:55 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.3990
2022-03-04 00:39:38 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 1.1093
2022-03-04 00:40:21 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 1.1919
2022-03-04 00:41:04 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 1.0877
2022-03-04 00:41:46 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 1.1733
2022-03-04 00:42:28 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 1.1928
2022-03-04 00:43:10 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.3584
2022-03-04 00:43:52 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 1.1031
2022-03-04 00:44:34 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.3395
2022-03-04 00:45:16 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 1.0349
2022-03-04 00:45:58 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 1.2294
2022-03-04 00:46:40 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.3587
2022-03-04 00:47:22 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.4982
2022-03-04 00:48:04 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 1.0452
2022-03-04 00:48:46 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 1.2323
2022-03-04 00:49:29 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.4418
2022-03-04 00:50:11 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 0.9997
2022-03-04 00:50:53 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.3948
2022-03-04 00:51:35 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.0278
2022-03-04 00:52:18 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 1.1363
2022-03-04 00:53:00 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 1.3183
2022-03-04 00:53:42 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 1.2313
2022-03-04 00:54:25 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 1.2667
2022-03-04 00:55:07 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 1.2234
2022-03-04 00:55:49 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 1.4215
2022-03-04 00:56:31 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.3387
2022-03-04 00:57:13 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 1.1575
2022-03-04 00:57:56 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 1.1608
2022-03-04 00:58:38 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.2547
2022-03-04 00:59:20 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 1.1354
2022-03-04 01:00:02 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 0.9877
2022-03-04 01:00:44 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 1.2183
2022-03-04 01:01:27 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.4152
2022-03-04 01:02:09 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.3730
2022-03-04 01:02:12 - train: epoch 077, train_loss: 1.2120
2022-03-04 01:03:29 - eval: epoch: 077, acc1: 72.438%, acc5: 90.820%, test_loss: 1.0955, per_image_load_time: 1.975ms, per_image_inference_time: 0.966ms
2022-03-04 01:03:30 - until epoch: 077, best_acc1: 72.474%
2022-03-04 01:03:30 - epoch 078 lr: 0.0010000000000000002
2022-03-04 01:04:17 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 1.1622
2022-03-04 01:04:59 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.3260
2022-03-04 01:05:41 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.3468
2022-03-04 01:06:23 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 1.2201
2022-03-04 01:07:05 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.1785
2022-03-04 01:07:47 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 1.1868
2022-03-04 01:08:29 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.3712
2022-03-04 01:09:11 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.3615
2022-03-04 01:09:53 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 1.2800
2022-03-04 01:10:35 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 1.2870
2022-03-04 01:11:17 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 0.9893
2022-03-04 01:11:59 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 1.0532
2022-03-04 01:12:40 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 1.2404
2022-03-04 01:13:22 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 1.0998
2022-03-04 01:14:04 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 1.1744
2022-03-04 01:14:46 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 1.2694
2022-03-04 01:15:28 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 1.1659
2022-03-04 01:16:10 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 1.0925
2022-03-04 01:16:52 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 1.0206
2022-03-04 01:17:34 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 1.0615
2022-03-04 01:18:15 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.4609
2022-03-04 01:18:57 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 1.1672
2022-03-04 01:19:39 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 1.3050
2022-03-04 01:20:21 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 1.1743
2022-03-04 01:21:03 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.0947
2022-03-04 01:21:45 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 1.1212
2022-03-04 01:22:27 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 1.1621
2022-03-04 01:23:09 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 1.2793
2022-03-04 01:23:51 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 1.0918
2022-03-04 01:24:33 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 1.2145
2022-03-04 01:25:15 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.3531
2022-03-04 01:25:57 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 1.2232
2022-03-04 01:26:39 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.3379
2022-03-04 01:27:20 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 1.2428
2022-03-04 01:28:02 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 1.2089
2022-03-04 01:28:45 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.2164
2022-03-04 01:29:27 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 1.1103
2022-03-04 01:30:09 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 1.1419
2022-03-04 01:30:51 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 1.2177
2022-03-04 01:31:33 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 1.1949
2022-03-04 01:32:15 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.2739
2022-03-04 01:32:57 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.3075
2022-03-04 01:33:39 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 1.1858
2022-03-04 01:34:20 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 1.2213
2022-03-04 01:35:02 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.3320
2022-03-04 01:35:44 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 1.0409
2022-03-04 01:36:26 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 1.3189
2022-03-04 01:37:08 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 1.5012
2022-03-04 01:37:50 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 1.2005
2022-03-04 01:38:32 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 1.1947
2022-03-04 01:38:36 - train: epoch 078, train_loss: 1.2085
2022-03-04 01:39:53 - eval: epoch: 078, acc1: 72.440%, acc5: 90.736%, test_loss: 1.0974, per_image_load_time: 1.987ms, per_image_inference_time: 0.960ms
2022-03-04 01:39:53 - until epoch: 078, best_acc1: 72.474%
2022-03-04 01:39:53 - epoch 079 lr: 0.0010000000000000002
2022-03-04 01:40:40 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 1.1047
2022-03-04 01:41:22 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 1.0929
2022-03-04 01:42:03 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 1.3027
2022-03-04 01:42:45 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 1.3465
2022-03-04 01:43:27 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 1.2638
2022-03-04 01:44:08 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 1.1044
2022-03-04 01:44:50 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 1.1358
2022-03-04 01:45:31 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 1.2438
2022-03-04 01:46:13 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 1.1215
2022-03-04 01:46:55 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 0.9805
2022-03-04 01:47:37 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 1.3422
2022-03-04 01:48:18 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.3684
2022-03-04 01:49:00 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 1.0379
2022-03-04 01:49:42 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 1.1669
2022-03-04 01:50:24 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 1.1185
2022-03-04 01:51:05 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 1.0571
2022-03-04 01:51:47 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 1.1105
2022-03-04 01:52:29 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 1.1560
2022-03-04 01:53:10 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.3023
2022-03-04 01:53:52 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.4052
2022-03-04 01:54:34 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 1.1248
2022-03-04 01:55:15 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 1.2875
2022-03-04 01:55:57 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 1.0680
2022-03-04 01:56:39 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.2406
2022-03-04 01:57:20 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 1.2169
2022-03-04 01:58:02 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 1.1749
2022-03-04 01:58:43 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 1.0482
2022-03-04 01:59:25 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 1.0714
2022-03-04 02:00:07 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 1.0786
2022-03-04 02:00:48 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.2666
2022-03-04 02:01:30 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 1.2611
2022-03-04 02:02:12 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.3958
2022-03-04 02:02:54 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 1.2294
2022-03-04 02:03:36 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 1.0021
2022-03-04 02:04:17 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.4034
2022-03-04 02:04:59 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 1.1752
2022-03-04 02:05:41 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 1.0934
2022-03-04 02:06:23 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 1.2676
2022-03-04 02:07:05 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 1.0583
2022-03-04 02:07:46 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 1.0918
2022-03-04 02:08:28 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.3668
2022-03-04 02:09:10 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.0173
2022-03-04 02:09:52 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 1.0185
2022-03-04 02:10:34 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.4260
2022-03-04 02:11:16 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.3535
2022-03-04 02:11:58 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.3430
2022-03-04 02:12:39 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 1.3205
2022-03-04 02:13:21 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.3198
2022-03-04 02:14:04 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.3755
