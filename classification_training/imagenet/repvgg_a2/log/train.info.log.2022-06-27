2022-06-27 20:53:51 - train: epoch 0049, iter [00500, 05004], lr: 0.065326, loss: 1.7281
2022-06-27 20:54:25 - train: epoch 0049, iter [00600, 05004], lr: 0.065302, loss: 1.7986
2022-06-27 20:55:00 - train: epoch 0049, iter [00700, 05004], lr: 0.065277, loss: 1.8637
2022-06-27 20:55:34 - train: epoch 0049, iter [00800, 05004], lr: 0.065252, loss: 2.1835
2022-06-27 20:56:08 - train: epoch 0049, iter [00900, 05004], lr: 0.065227, loss: 1.7124
2022-06-27 20:56:42 - train: epoch 0049, iter [01000, 05004], lr: 0.065202, loss: 1.8993
2022-06-27 20:57:16 - train: epoch 0049, iter [01100, 05004], lr: 0.065177, loss: 1.6827
2022-06-27 20:57:50 - train: epoch 0049, iter [01200, 05004], lr: 0.065152, loss: 1.6088
2022-06-27 20:58:24 - train: epoch 0049, iter [01300, 05004], lr: 0.065127, loss: 1.9761
2022-06-27 20:58:58 - train: epoch 0049, iter [01400, 05004], lr: 0.065102, loss: 1.9101
2022-06-27 20:59:32 - train: epoch 0049, iter [01500, 05004], lr: 0.065077, loss: 1.7550
2022-06-27 21:00:06 - train: epoch 0049, iter [01600, 05004], lr: 0.065052, loss: 2.0031
2022-06-27 21:00:40 - train: epoch 0049, iter [01700, 05004], lr: 0.065027, loss: 1.8677
2022-06-27 21:01:15 - train: epoch 0049, iter [01800, 05004], lr: 0.065002, loss: 1.7558
2022-06-27 21:01:49 - train: epoch 0049, iter [01900, 05004], lr: 0.064977, loss: 1.7465
2022-06-27 21:02:23 - train: epoch 0049, iter [02000, 05004], lr: 0.064952, loss: 1.7844
2022-06-27 21:02:58 - train: epoch 0049, iter [02100, 05004], lr: 0.064927, loss: 1.7429
2022-06-27 21:03:32 - train: epoch 0049, iter [02200, 05004], lr: 0.064903, loss: 1.7499
2022-06-27 21:04:06 - train: epoch 0049, iter [02300, 05004], lr: 0.064878, loss: 1.8192
2022-06-27 21:04:40 - train: epoch 0049, iter [02400, 05004], lr: 0.064853, loss: 2.0333
2022-06-27 21:05:14 - train: epoch 0049, iter [02500, 05004], lr: 0.064828, loss: 1.9824
2022-06-27 21:05:49 - train: epoch 0049, iter [02600, 05004], lr: 0.064803, loss: 1.8925
2022-06-27 21:06:22 - train: epoch 0049, iter [02700, 05004], lr: 0.064778, loss: 1.7283
2022-06-27 21:06:58 - train: epoch 0049, iter [02800, 05004], lr: 0.064753, loss: 1.7745
2022-06-27 21:07:32 - train: epoch 0049, iter [02900, 05004], lr: 0.064728, loss: 1.9939
2022-06-27 21:08:06 - train: epoch 0049, iter [03000, 05004], lr: 0.064703, loss: 1.9339
2022-06-27 21:08:40 - train: epoch 0049, iter [03100, 05004], lr: 0.064678, loss: 1.8676
2022-06-27 21:09:14 - train: epoch 0049, iter [03200, 05004], lr: 0.064653, loss: 1.8961
2022-06-27 21:09:48 - train: epoch 0049, iter [03300, 05004], lr: 0.064628, loss: 1.7612
2022-06-27 21:10:23 - train: epoch 0049, iter [03400, 05004], lr: 0.064603, loss: 2.0452
2022-06-27 21:10:57 - train: epoch 0049, iter [03500, 05004], lr: 0.064578, loss: 2.0033
2022-06-27 21:11:31 - train: epoch 0049, iter [03600, 05004], lr: 0.064553, loss: 1.9899
2022-06-27 21:12:06 - train: epoch 0049, iter [03700, 05004], lr: 0.064528, loss: 1.8172
2022-06-27 21:12:40 - train: epoch 0049, iter [03800, 05004], lr: 0.064502, loss: 2.0564
2022-06-27 21:13:14 - train: epoch 0049, iter [03900, 05004], lr: 0.064477, loss: 2.0118
2022-06-27 21:13:49 - train: epoch 0049, iter [04000, 05004], lr: 0.064452, loss: 1.7908
2022-06-27 21:14:23 - train: epoch 0049, iter [04100, 05004], lr: 0.064427, loss: 1.7007
2022-06-27 21:14:57 - train: epoch 0049, iter [04200, 05004], lr: 0.064402, loss: 1.8441
2022-06-27 21:15:32 - train: epoch 0049, iter [04300, 05004], lr: 0.064377, loss: 2.1850
2022-06-27 21:16:06 - train: epoch 0049, iter [04400, 05004], lr: 0.064352, loss: 1.8191
2022-06-27 21:16:41 - train: epoch 0049, iter [04500, 05004], lr: 0.064327, loss: 1.7987
2022-06-27 21:17:15 - train: epoch 0049, iter [04600, 05004], lr: 0.064302, loss: 1.9324
2022-06-27 21:17:49 - train: epoch 0049, iter [04700, 05004], lr: 0.064277, loss: 2.0893
2022-06-27 21:18:24 - train: epoch 0049, iter [04800, 05004], lr: 0.064252, loss: 1.7690
2022-06-27 21:18:57 - train: epoch 0049, iter [04900, 05004], lr: 0.064227, loss: 1.7482
2022-06-27 21:19:30 - train: epoch 0049, iter [05000, 05004], lr: 0.064202, loss: 1.7109
2022-06-27 21:19:31 - train: epoch 049, train_loss: 1.8428
2022-06-27 21:20:49 - eval: epoch: 049, acc1: 61.674%, acc5: 83.992%, test_loss: 1.6277, per_image_load_time: 2.303ms, per_image_inference_time: 0.415ms
2022-06-27 21:20:49 - until epoch: 049, best_acc1: 61.708%
2022-06-27 21:20:49 - epoch 050 lr: 0.064201
2022-06-27 21:21:28 - train: epoch 0050, iter [00100, 05004], lr: 0.064176, loss: 1.9351
2022-06-27 21:22:02 - train: epoch 0050, iter [00200, 05004], lr: 0.064151, loss: 1.8527
2022-06-27 21:22:36 - train: epoch 0050, iter [00300, 05004], lr: 0.064126, loss: 1.9166
2022-06-27 21:23:10 - train: epoch 0050, iter [00400, 05004], lr: 0.064100, loss: 1.7143
2022-06-27 21:23:45 - train: epoch 0050, iter [00500, 05004], lr: 0.064075, loss: 1.7454
2022-06-27 21:24:19 - train: epoch 0050, iter [00600, 05004], lr: 0.064050, loss: 1.8400
2022-06-27 21:24:54 - train: epoch 0050, iter [00700, 05004], lr: 0.064025, loss: 1.6944
2022-06-27 21:25:27 - train: epoch 0050, iter [00800, 05004], lr: 0.064000, loss: 1.5708
2022-06-27 21:26:03 - train: epoch 0050, iter [00900, 05004], lr: 0.063975, loss: 1.7509
2022-06-27 21:26:37 - train: epoch 0050, iter [01000, 05004], lr: 0.063950, loss: 2.0336
2022-06-27 21:27:11 - train: epoch 0050, iter [01100, 05004], lr: 0.063925, loss: 1.9090
2022-06-27 21:27:45 - train: epoch 0050, iter [01200, 05004], lr: 0.063900, loss: 1.7678
2022-06-27 21:28:19 - train: epoch 0050, iter [01300, 05004], lr: 0.063874, loss: 1.6105
2022-06-27 21:28:54 - train: epoch 0050, iter [01400, 05004], lr: 0.063849, loss: 1.8346
2022-06-27 21:29:28 - train: epoch 0050, iter [01500, 05004], lr: 0.063824, loss: 1.8018
2022-06-27 21:30:03 - train: epoch 0050, iter [01600, 05004], lr: 0.063799, loss: 1.7129
2022-06-27 21:30:37 - train: epoch 0050, iter [01700, 05004], lr: 0.063774, loss: 1.9446
2022-06-27 21:31:11 - train: epoch 0050, iter [01800, 05004], lr: 0.063749, loss: 1.8872
2022-06-27 21:31:46 - train: epoch 0050, iter [01900, 05004], lr: 0.063724, loss: 1.7330
2022-06-27 21:32:20 - train: epoch 0050, iter [02000, 05004], lr: 0.063698, loss: 1.8100
2022-06-27 21:32:54 - train: epoch 0050, iter [02100, 05004], lr: 0.063673, loss: 1.5037
2022-06-27 21:33:29 - train: epoch 0050, iter [02200, 05004], lr: 0.063648, loss: 1.8098
2022-06-27 21:34:03 - train: epoch 0050, iter [02300, 05004], lr: 0.063623, loss: 1.8723
2022-06-27 21:34:38 - train: epoch 0050, iter [02400, 05004], lr: 0.063598, loss: 1.7724
2022-06-27 21:35:12 - train: epoch 0050, iter [02500, 05004], lr: 0.063573, loss: 1.8540
2022-06-27 21:35:46 - train: epoch 0050, iter [02600, 05004], lr: 0.063547, loss: 1.6930
2022-06-27 21:36:21 - train: epoch 0050, iter [02700, 05004], lr: 0.063522, loss: 1.9566
2022-06-27 21:36:56 - train: epoch 0050, iter [02800, 05004], lr: 0.063497, loss: 1.9578
2022-06-27 21:37:30 - train: epoch 0050, iter [02900, 05004], lr: 0.063472, loss: 1.9699
2022-06-27 21:38:04 - train: epoch 0050, iter [03000, 05004], lr: 0.063447, loss: 1.9589
2022-06-27 21:38:39 - train: epoch 0050, iter [03100, 05004], lr: 0.063421, loss: 1.8029
2022-06-27 21:39:13 - train: epoch 0050, iter [03200, 05004], lr: 0.063396, loss: 1.8507
2022-06-27 21:39:48 - train: epoch 0050, iter [03300, 05004], lr: 0.063371, loss: 1.6967
2022-06-27 21:40:23 - train: epoch 0050, iter [03400, 05004], lr: 0.063346, loss: 1.7389
2022-06-27 21:40:57 - train: epoch 0050, iter [03500, 05004], lr: 0.063321, loss: 1.7869
2022-06-27 21:41:33 - train: epoch 0050, iter [03600, 05004], lr: 0.063295, loss: 1.9158
2022-06-27 21:42:08 - train: epoch 0050, iter [03700, 05004], lr: 0.063270, loss: 1.9045
2022-06-27 21:42:42 - train: epoch 0050, iter [03800, 05004], lr: 0.063245, loss: 1.7155
2022-06-27 21:43:17 - train: epoch 0050, iter [03900, 05004], lr: 0.063220, loss: 1.6525
2022-06-27 21:43:52 - train: epoch 0050, iter [04000, 05004], lr: 0.063194, loss: 1.9635
2022-06-27 21:44:26 - train: epoch 0050, iter [04100, 05004], lr: 0.063169, loss: 1.8281
2022-06-27 21:45:01 - train: epoch 0050, iter [04200, 05004], lr: 0.063144, loss: 1.7931
2022-06-27 21:45:36 - train: epoch 0050, iter [04300, 05004], lr: 0.063119, loss: 1.7042
2022-06-27 21:46:10 - train: epoch 0050, iter [04400, 05004], lr: 0.063094, loss: 1.7775
2022-06-27 21:46:45 - train: epoch 0050, iter [04500, 05004], lr: 0.063068, loss: 1.7311
2022-06-27 21:47:20 - train: epoch 0050, iter [04600, 05004], lr: 0.063043, loss: 1.8540
2022-06-27 21:47:54 - train: epoch 0050, iter [04700, 05004], lr: 0.063018, loss: 1.8512
2022-06-27 21:48:28 - train: epoch 0050, iter [04800, 05004], lr: 0.062992, loss: 1.5793
2022-06-27 21:49:03 - train: epoch 0050, iter [04900, 05004], lr: 0.062967, loss: 1.6701
2022-06-27 21:49:37 - train: epoch 0050, iter [05000, 05004], lr: 0.062942, loss: 1.8296
2022-06-27 21:49:38 - train: epoch 050, train_loss: 1.8322
2022-06-27 21:50:55 - eval: epoch: 050, acc1: 61.934%, acc5: 84.558%, test_loss: 1.5979, per_image_load_time: 2.533ms, per_image_inference_time: 0.414ms
2022-06-27 21:50:56 - until epoch: 050, best_acc1: 61.934%
2022-06-27 21:50:56 - epoch 051 lr: 0.062941
2022-06-27 21:51:36 - train: epoch 0051, iter [00100, 05004], lr: 0.062916, loss: 2.0405
2022-06-27 21:52:10 - train: epoch 0051, iter [00200, 05004], lr: 0.062890, loss: 2.1393
2022-06-27 21:52:44 - train: epoch 0051, iter [00300, 05004], lr: 0.062865, loss: 1.8526
2022-06-27 21:53:20 - train: epoch 0051, iter [00400, 05004], lr: 0.062840, loss: 1.5643
2022-06-27 21:53:53 - train: epoch 0051, iter [00500, 05004], lr: 0.062815, loss: 1.7805
2022-06-27 21:54:28 - train: epoch 0051, iter [00600, 05004], lr: 0.062789, loss: 1.7909
2022-06-27 21:55:02 - train: epoch 0051, iter [00700, 05004], lr: 0.062764, loss: 1.8017
2022-06-27 21:55:36 - train: epoch 0051, iter [00800, 05004], lr: 0.062739, loss: 2.1284
2022-06-27 21:56:11 - train: epoch 0051, iter [00900, 05004], lr: 0.062713, loss: 1.7183
2022-06-27 21:56:45 - train: epoch 0051, iter [01000, 05004], lr: 0.062688, loss: 2.0029
2022-06-27 21:57:20 - train: epoch 0051, iter [01100, 05004], lr: 0.062663, loss: 2.0007
2022-06-27 21:57:55 - train: epoch 0051, iter [01200, 05004], lr: 0.062637, loss: 1.7833
2022-06-27 21:58:29 - train: epoch 0051, iter [01300, 05004], lr: 0.062612, loss: 1.5654
2022-06-27 21:59:04 - train: epoch 0051, iter [01400, 05004], lr: 0.062587, loss: 1.6401
2022-06-27 21:59:38 - train: epoch 0051, iter [01500, 05004], lr: 0.062562, loss: 1.7716
2022-06-27 22:00:13 - train: epoch 0051, iter [01600, 05004], lr: 0.062536, loss: 1.5910
2022-06-27 22:00:49 - train: epoch 0051, iter [01700, 05004], lr: 0.062511, loss: 1.8937
2022-06-27 22:01:24 - train: epoch 0051, iter [01800, 05004], lr: 0.062486, loss: 1.7086
2022-06-27 22:01:58 - train: epoch 0051, iter [01900, 05004], lr: 0.062460, loss: 1.5937
2022-06-27 22:02:32 - train: epoch 0051, iter [02000, 05004], lr: 0.062435, loss: 1.8102
2022-06-27 22:03:08 - train: epoch 0051, iter [02100, 05004], lr: 0.062410, loss: 1.7494
2022-06-27 22:03:43 - train: epoch 0051, iter [02200, 05004], lr: 0.062384, loss: 1.9360
2022-06-27 22:04:18 - train: epoch 0051, iter [02300, 05004], lr: 0.062359, loss: 1.9139
2022-06-27 22:04:53 - train: epoch 0051, iter [02400, 05004], lr: 0.062334, loss: 1.8723
2022-06-27 22:05:28 - train: epoch 0051, iter [02500, 05004], lr: 0.062308, loss: 1.7973
2022-06-27 22:06:04 - train: epoch 0051, iter [02600, 05004], lr: 0.062283, loss: 1.6548
2022-06-27 22:06:38 - train: epoch 0051, iter [02700, 05004], lr: 0.062257, loss: 1.8851
2022-06-27 22:07:13 - train: epoch 0051, iter [02800, 05004], lr: 0.062232, loss: 1.8028
2022-06-27 22:07:49 - train: epoch 0051, iter [02900, 05004], lr: 0.062207, loss: 1.8635
2022-06-27 22:08:23 - train: epoch 0051, iter [03000, 05004], lr: 0.062181, loss: 1.7943
2022-06-27 22:08:58 - train: epoch 0051, iter [03100, 05004], lr: 0.062156, loss: 1.7885
2022-06-27 22:09:33 - train: epoch 0051, iter [03200, 05004], lr: 0.062131, loss: 1.8509
2022-06-27 22:10:08 - train: epoch 0051, iter [03300, 05004], lr: 0.062105, loss: 1.9217
2022-06-27 22:10:43 - train: epoch 0051, iter [03400, 05004], lr: 0.062080, loss: 1.9051
2022-06-27 22:11:18 - train: epoch 0051, iter [03500, 05004], lr: 0.062054, loss: 1.7445
2022-06-27 22:11:52 - train: epoch 0051, iter [03600, 05004], lr: 0.062029, loss: 1.6995
2022-06-27 22:12:28 - train: epoch 0051, iter [03700, 05004], lr: 0.062004, loss: 1.9023
2022-06-27 22:13:03 - train: epoch 0051, iter [03800, 05004], lr: 0.061978, loss: 1.7878
2022-06-27 22:13:38 - train: epoch 0051, iter [03900, 05004], lr: 0.061953, loss: 1.8432
2022-06-27 22:14:12 - train: epoch 0051, iter [04000, 05004], lr: 0.061927, loss: 1.8731
2022-06-27 22:14:47 - train: epoch 0051, iter [04100, 05004], lr: 0.061902, loss: 1.8957
2022-06-27 22:15:22 - train: epoch 0051, iter [04200, 05004], lr: 0.061877, loss: 2.0813
2022-06-27 22:15:57 - train: epoch 0051, iter [04300, 05004], lr: 0.061851, loss: 1.7437
2022-06-27 22:16:31 - train: epoch 0051, iter [04400, 05004], lr: 0.061826, loss: 1.9119
2022-06-27 22:17:08 - train: epoch 0051, iter [04500, 05004], lr: 0.061800, loss: 1.6765
2022-06-27 22:17:42 - train: epoch 0051, iter [04600, 05004], lr: 0.061775, loss: 1.9262
2022-06-27 22:18:17 - train: epoch 0051, iter [04700, 05004], lr: 0.061750, loss: 1.9555
2022-06-27 22:18:53 - train: epoch 0051, iter [04800, 05004], lr: 0.061724, loss: 1.8626
2022-06-27 22:19:27 - train: epoch 0051, iter [04900, 05004], lr: 0.061699, loss: 1.7934
2022-06-27 22:20:00 - train: epoch 0051, iter [05000, 05004], lr: 0.061673, loss: 1.7139
2022-06-27 22:20:02 - train: epoch 051, train_loss: 1.8236
2022-06-27 22:21:18 - eval: epoch: 051, acc1: 62.868%, acc5: 85.004%, test_loss: 1.5598, per_image_load_time: 2.502ms, per_image_inference_time: 0.442ms
2022-06-27 22:21:18 - until epoch: 051, best_acc1: 62.868%
2022-06-27 22:21:18 - epoch 052 lr: 0.061672
2022-06-27 22:21:58 - train: epoch 0052, iter [00100, 05004], lr: 0.061647, loss: 1.7117
2022-06-27 22:22:33 - train: epoch 0052, iter [00200, 05004], lr: 0.061621, loss: 1.7757
2022-06-27 22:23:07 - train: epoch 0052, iter [00300, 05004], lr: 0.061596, loss: 1.8926
2022-06-27 22:23:41 - train: epoch 0052, iter [00400, 05004], lr: 0.061570, loss: 1.8171
2022-06-27 22:24:17 - train: epoch 0052, iter [00500, 05004], lr: 0.061545, loss: 1.8389
2022-06-27 22:24:51 - train: epoch 0052, iter [00600, 05004], lr: 0.061520, loss: 1.6887
2022-06-27 22:25:25 - train: epoch 0052, iter [00700, 05004], lr: 0.061494, loss: 1.8748
2022-06-27 22:26:00 - train: epoch 0052, iter [00800, 05004], lr: 0.061469, loss: 1.8557
2022-06-27 22:26:34 - train: epoch 0052, iter [00900, 05004], lr: 0.061443, loss: 1.8438
2022-06-27 22:27:09 - train: epoch 0052, iter [01000, 05004], lr: 0.061418, loss: 2.1024
2022-06-27 22:27:43 - train: epoch 0052, iter [01100, 05004], lr: 0.061392, loss: 1.7644
2022-06-27 22:28:18 - train: epoch 0052, iter [01200, 05004], lr: 0.061367, loss: 1.6383
2022-06-27 22:28:54 - train: epoch 0052, iter [01300, 05004], lr: 0.061341, loss: 1.7152
2022-06-27 22:29:28 - train: epoch 0052, iter [01400, 05004], lr: 0.061316, loss: 2.0917
2022-06-27 22:30:02 - train: epoch 0052, iter [01500, 05004], lr: 0.061290, loss: 1.6361
2022-06-27 22:30:37 - train: epoch 0052, iter [01600, 05004], lr: 0.061265, loss: 1.5866
2022-06-27 22:31:12 - train: epoch 0052, iter [01700, 05004], lr: 0.061239, loss: 1.7288
2022-06-27 22:31:48 - train: epoch 0052, iter [01800, 05004], lr: 0.061214, loss: 1.6804
2022-06-27 22:32:21 - train: epoch 0052, iter [01900, 05004], lr: 0.061188, loss: 1.7640
2022-06-27 22:32:56 - train: epoch 0052, iter [02000, 05004], lr: 0.061163, loss: 2.0278
2022-06-27 22:33:31 - train: epoch 0052, iter [02100, 05004], lr: 0.061137, loss: 1.7096
2022-06-27 22:34:05 - train: epoch 0052, iter [02200, 05004], lr: 0.061112, loss: 1.9163
2022-06-27 22:34:40 - train: epoch 0052, iter [02300, 05004], lr: 0.061086, loss: 1.6696
2022-06-27 22:35:15 - train: epoch 0052, iter [02400, 05004], lr: 0.061061, loss: 1.6054
2022-06-27 22:35:49 - train: epoch 0052, iter [02500, 05004], lr: 0.061035, loss: 1.5718
2022-06-27 22:36:25 - train: epoch 0052, iter [02600, 05004], lr: 0.061010, loss: 1.4945
2022-06-27 22:36:59 - train: epoch 0052, iter [02700, 05004], lr: 0.060984, loss: 1.7662
2022-06-27 22:37:34 - train: epoch 0052, iter [02800, 05004], lr: 0.060959, loss: 1.8705
2022-06-27 22:38:09 - train: epoch 0052, iter [02900, 05004], lr: 0.060933, loss: 1.6122
2022-06-27 22:38:44 - train: epoch 0052, iter [03000, 05004], lr: 0.060908, loss: 1.8162
2022-06-27 22:39:19 - train: epoch 0052, iter [03100, 05004], lr: 0.060882, loss: 1.8806
2022-06-27 22:39:54 - train: epoch 0052, iter [03200, 05004], lr: 0.060857, loss: 1.9214
2022-06-27 22:40:29 - train: epoch 0052, iter [03300, 05004], lr: 0.060831, loss: 1.8675
2022-06-27 22:41:04 - train: epoch 0052, iter [03400, 05004], lr: 0.060806, loss: 1.7873
2022-06-27 22:41:38 - train: epoch 0052, iter [03500, 05004], lr: 0.060780, loss: 1.9205
2022-06-27 22:42:13 - train: epoch 0052, iter [03600, 05004], lr: 0.060755, loss: 1.9426
2022-06-27 22:42:49 - train: epoch 0052, iter [03700, 05004], lr: 0.060729, loss: 2.0454
2022-06-27 22:43:24 - train: epoch 0052, iter [03800, 05004], lr: 0.060703, loss: 1.6646
2022-06-27 22:43:59 - train: epoch 0052, iter [03900, 05004], lr: 0.060678, loss: 1.7282
2022-06-27 22:44:34 - train: epoch 0052, iter [04000, 05004], lr: 0.060652, loss: 2.0352
2022-06-27 22:45:08 - train: epoch 0052, iter [04100, 05004], lr: 0.060627, loss: 1.5395
2022-06-27 22:45:44 - train: epoch 0052, iter [04200, 05004], lr: 0.060601, loss: 1.6606
2022-06-27 22:46:17 - train: epoch 0052, iter [04300, 05004], lr: 0.060576, loss: 1.8712
2022-06-27 22:46:53 - train: epoch 0052, iter [04400, 05004], lr: 0.060550, loss: 1.8387
2022-06-27 22:47:28 - train: epoch 0052, iter [04500, 05004], lr: 0.060525, loss: 1.7578
2022-06-27 22:48:03 - train: epoch 0052, iter [04600, 05004], lr: 0.060499, loss: 1.8302
2022-06-27 22:48:38 - train: epoch 0052, iter [04700, 05004], lr: 0.060473, loss: 1.8558
2022-06-27 22:49:13 - train: epoch 0052, iter [04800, 05004], lr: 0.060448, loss: 1.6624
2022-06-27 22:49:48 - train: epoch 0052, iter [04900, 05004], lr: 0.060422, loss: 1.7247
2022-06-27 22:50:21 - train: epoch 0052, iter [05000, 05004], lr: 0.060397, loss: 1.8121
2022-06-27 22:50:22 - train: epoch 052, train_loss: 1.8142
2022-06-27 22:51:39 - eval: epoch: 052, acc1: 62.260%, acc5: 85.204%, test_loss: 1.5648, per_image_load_time: 2.520ms, per_image_inference_time: 0.445ms
2022-06-27 22:51:39 - until epoch: 052, best_acc1: 62.868%
2022-06-27 22:51:39 - epoch 053 lr: 0.060395
2022-06-27 22:52:19 - train: epoch 0053, iter [00100, 05004], lr: 0.060370, loss: 1.7812
2022-06-27 22:52:54 - train: epoch 0053, iter [00200, 05004], lr: 0.060344, loss: 2.0648
2022-06-27 22:53:27 - train: epoch 0053, iter [00300, 05004], lr: 0.060319, loss: 1.8398
2022-06-27 22:54:02 - train: epoch 0053, iter [00400, 05004], lr: 0.060293, loss: 1.8609
2022-06-27 22:54:36 - train: epoch 0053, iter [00500, 05004], lr: 0.060268, loss: 1.9923
2022-06-27 22:55:10 - train: epoch 0053, iter [00600, 05004], lr: 0.060242, loss: 1.8064
2022-06-27 22:55:44 - train: epoch 0053, iter [00700, 05004], lr: 0.060216, loss: 1.6612
2022-06-27 22:56:18 - train: epoch 0053, iter [00800, 05004], lr: 0.060191, loss: 1.8076
2022-06-27 22:56:52 - train: epoch 0053, iter [00900, 05004], lr: 0.060165, loss: 1.7292
2022-06-27 22:57:26 - train: epoch 0053, iter [01000, 05004], lr: 0.060140, loss: 1.6939
2022-06-27 22:58:00 - train: epoch 0053, iter [01100, 05004], lr: 0.060114, loss: 1.7058
2022-06-27 22:58:35 - train: epoch 0053, iter [01200, 05004], lr: 0.060088, loss: 1.7704
2022-06-27 22:59:09 - train: epoch 0053, iter [01300, 05004], lr: 0.060063, loss: 1.8058
2022-06-27 22:59:43 - train: epoch 0053, iter [01400, 05004], lr: 0.060037, loss: 2.2604
2022-06-27 23:00:17 - train: epoch 0053, iter [01500, 05004], lr: 0.060011, loss: 1.8315
2022-06-27 23:00:51 - train: epoch 0053, iter [01600, 05004], lr: 0.059986, loss: 2.0185
2022-06-27 23:01:25 - train: epoch 0053, iter [01700, 05004], lr: 0.059960, loss: 1.9875
2022-06-27 23:02:00 - train: epoch 0053, iter [01800, 05004], lr: 0.059935, loss: 1.9540
2022-06-27 23:02:34 - train: epoch 0053, iter [01900, 05004], lr: 0.059909, loss: 1.6013
2022-06-27 23:03:09 - train: epoch 0053, iter [02000, 05004], lr: 0.059883, loss: 1.8404
2022-06-27 23:03:43 - train: epoch 0053, iter [02100, 05004], lr: 0.059858, loss: 1.8744
2022-06-27 23:04:17 - train: epoch 0053, iter [02200, 05004], lr: 0.059832, loss: 1.7747
2022-06-27 23:04:52 - train: epoch 0053, iter [02300, 05004], lr: 0.059806, loss: 1.7604
2022-06-27 23:05:26 - train: epoch 0053, iter [02400, 05004], lr: 0.059781, loss: 1.7170
2022-06-27 23:06:01 - train: epoch 0053, iter [02500, 05004], lr: 0.059755, loss: 2.0372
2022-06-27 23:06:34 - train: epoch 0053, iter [02600, 05004], lr: 0.059729, loss: 1.8544
2022-06-27 23:07:09 - train: epoch 0053, iter [02700, 05004], lr: 0.059704, loss: 2.0799
2022-06-27 23:07:44 - train: epoch 0053, iter [02800, 05004], lr: 0.059678, loss: 1.9626
2022-06-27 23:08:19 - train: epoch 0053, iter [02900, 05004], lr: 0.059652, loss: 1.6171
2022-06-27 23:08:52 - train: epoch 0053, iter [03000, 05004], lr: 0.059627, loss: 1.5952
2022-06-27 23:09:26 - train: epoch 0053, iter [03100, 05004], lr: 0.059601, loss: 1.9566
2022-06-27 23:10:00 - train: epoch 0053, iter [03200, 05004], lr: 0.059575, loss: 2.0742
2022-06-27 23:10:34 - train: epoch 0053, iter [03300, 05004], lr: 0.059550, loss: 1.7575
2022-06-27 23:11:09 - train: epoch 0053, iter [03400, 05004], lr: 0.059524, loss: 1.9486
2022-06-27 23:11:43 - train: epoch 0053, iter [03500, 05004], lr: 0.059498, loss: 1.8676
2022-06-27 23:12:17 - train: epoch 0053, iter [03600, 05004], lr: 0.059473, loss: 1.8737
2022-06-27 23:12:51 - train: epoch 0053, iter [03700, 05004], lr: 0.059447, loss: 1.9057
2022-06-27 23:13:25 - train: epoch 0053, iter [03800, 05004], lr: 0.059421, loss: 1.7296
2022-06-27 23:14:00 - train: epoch 0053, iter [03900, 05004], lr: 0.059396, loss: 1.9481
2022-06-27 23:14:34 - train: epoch 0053, iter [04000, 05004], lr: 0.059370, loss: 1.8014
2022-06-27 23:15:08 - train: epoch 0053, iter [04100, 05004], lr: 0.059344, loss: 1.9027
2022-06-27 23:15:43 - train: epoch 0053, iter [04200, 05004], lr: 0.059318, loss: 1.8004
2022-06-27 23:16:17 - train: epoch 0053, iter [04300, 05004], lr: 0.059293, loss: 2.1174
2022-06-27 23:16:52 - train: epoch 0053, iter [04400, 05004], lr: 0.059267, loss: 1.7868
2022-06-27 23:17:26 - train: epoch 0053, iter [04500, 05004], lr: 0.059241, loss: 1.9424
2022-06-27 23:18:00 - train: epoch 0053, iter [04600, 05004], lr: 0.059216, loss: 1.7238
2022-06-27 23:18:35 - train: epoch 0053, iter [04700, 05004], lr: 0.059190, loss: 1.9708
2022-06-27 23:19:09 - train: epoch 0053, iter [04800, 05004], lr: 0.059164, loss: 1.9858
2022-06-27 23:19:43 - train: epoch 0053, iter [04900, 05004], lr: 0.059139, loss: 1.7055
2022-06-27 23:20:16 - train: epoch 0053, iter [05000, 05004], lr: 0.059113, loss: 1.8051
2022-06-27 23:20:17 - train: epoch 053, train_loss: 1.8041
2022-06-27 23:21:34 - eval: epoch: 053, acc1: 62.712%, acc5: 85.252%, test_loss: 1.5529, per_image_load_time: 2.551ms, per_image_inference_time: 0.397ms
2022-06-27 23:21:34 - until epoch: 053, best_acc1: 62.868%
2022-06-27 23:21:34 - epoch 054 lr: 0.059112
2022-06-27 23:22:15 - train: epoch 0054, iter [00100, 05004], lr: 0.059086, loss: 1.5893
2022-06-27 23:22:49 - train: epoch 0054, iter [00200, 05004], lr: 0.059060, loss: 2.1150
2022-06-27 23:23:23 - train: epoch 0054, iter [00300, 05004], lr: 0.059035, loss: 1.8872
2022-06-27 23:23:57 - train: epoch 0054, iter [00400, 05004], lr: 0.059009, loss: 1.6140
2022-06-27 23:24:32 - train: epoch 0054, iter [00500, 05004], lr: 0.058983, loss: 1.8149
2022-06-27 23:25:06 - train: epoch 0054, iter [00600, 05004], lr: 0.058957, loss: 1.6401
2022-06-27 23:25:41 - train: epoch 0054, iter [00700, 05004], lr: 0.058932, loss: 1.8309
2022-06-27 23:26:14 - train: epoch 0054, iter [00800, 05004], lr: 0.058906, loss: 1.8666
2022-06-27 23:26:50 - train: epoch 0054, iter [00900, 05004], lr: 0.058880, loss: 1.5287
2022-06-27 23:27:22 - train: epoch 0054, iter [01000, 05004], lr: 0.058854, loss: 1.6480
2022-06-27 23:27:57 - train: epoch 0054, iter [01100, 05004], lr: 0.058829, loss: 1.7659
2022-06-27 23:28:30 - train: epoch 0054, iter [01200, 05004], lr: 0.058803, loss: 1.8873
2022-06-27 23:29:04 - train: epoch 0054, iter [01300, 05004], lr: 0.058777, loss: 1.7335
2022-06-27 23:29:37 - train: epoch 0054, iter [01400, 05004], lr: 0.058751, loss: 1.7710
2022-06-27 23:30:10 - train: epoch 0054, iter [01500, 05004], lr: 0.058726, loss: 1.7922
2022-06-27 23:30:44 - train: epoch 0054, iter [01600, 05004], lr: 0.058700, loss: 1.5296
2022-06-27 23:31:17 - train: epoch 0054, iter [01700, 05004], lr: 0.058674, loss: 1.8968
2022-06-27 23:31:50 - train: epoch 0054, iter [01800, 05004], lr: 0.058648, loss: 1.7217
2022-06-27 23:32:24 - train: epoch 0054, iter [01900, 05004], lr: 0.058623, loss: 2.1614
2022-06-27 23:32:56 - train: epoch 0054, iter [02000, 05004], lr: 0.058597, loss: 1.9420
2022-06-27 23:33:30 - train: epoch 0054, iter [02100, 05004], lr: 0.058571, loss: 1.5229
2022-06-27 23:34:04 - train: epoch 0054, iter [02200, 05004], lr: 0.058545, loss: 1.8124
2022-06-27 23:34:37 - train: epoch 0054, iter [02300, 05004], lr: 0.058520, loss: 1.7795
2022-06-27 23:35:12 - train: epoch 0054, iter [02400, 05004], lr: 0.058494, loss: 1.6880
2022-06-27 23:35:45 - train: epoch 0054, iter [02500, 05004], lr: 0.058468, loss: 1.8783
2022-06-27 23:36:18 - train: epoch 0054, iter [02600, 05004], lr: 0.058442, loss: 1.6590
2022-06-27 23:36:53 - train: epoch 0054, iter [02700, 05004], lr: 0.058416, loss: 1.8119
2022-06-27 23:37:27 - train: epoch 0054, iter [02800, 05004], lr: 0.058391, loss: 2.1123
2022-06-27 23:37:59 - train: epoch 0054, iter [02900, 05004], lr: 0.058365, loss: 1.6221
2022-06-27 23:38:33 - train: epoch 0054, iter [03000, 05004], lr: 0.058339, loss: 1.9465
2022-06-27 23:39:07 - train: epoch 0054, iter [03100, 05004], lr: 0.058313, loss: 1.7794
2022-06-27 23:39:40 - train: epoch 0054, iter [03200, 05004], lr: 0.058287, loss: 1.9588
2022-06-27 23:40:13 - train: epoch 0054, iter [03300, 05004], lr: 0.058262, loss: 1.6693
2022-06-27 23:40:47 - train: epoch 0054, iter [03400, 05004], lr: 0.058236, loss: 1.8150
2022-06-27 23:41:20 - train: epoch 0054, iter [03500, 05004], lr: 0.058210, loss: 1.9550
2022-06-27 23:41:53 - train: epoch 0054, iter [03600, 05004], lr: 0.058184, loss: 1.7444
2022-06-27 23:42:27 - train: epoch 0054, iter [03700, 05004], lr: 0.058158, loss: 1.6332
2022-06-27 23:43:00 - train: epoch 0054, iter [03800, 05004], lr: 0.058133, loss: 1.8237
2022-06-27 23:43:34 - train: epoch 0054, iter [03900, 05004], lr: 0.058107, loss: 1.7146
2022-06-27 23:44:08 - train: epoch 0054, iter [04000, 05004], lr: 0.058081, loss: 1.6346
2022-06-27 23:44:42 - train: epoch 0054, iter [04100, 05004], lr: 0.058055, loss: 1.7959
2022-06-27 23:45:15 - train: epoch 0054, iter [04200, 05004], lr: 0.058029, loss: 1.7890
2022-06-27 23:45:49 - train: epoch 0054, iter [04300, 05004], lr: 0.058004, loss: 1.8253
2022-06-27 23:46:22 - train: epoch 0054, iter [04400, 05004], lr: 0.057978, loss: 1.5811
2022-06-27 23:46:56 - train: epoch 0054, iter [04500, 05004], lr: 0.057952, loss: 1.7160
2022-06-27 23:47:30 - train: epoch 0054, iter [04600, 05004], lr: 0.057926, loss: 1.8910
2022-06-27 23:48:04 - train: epoch 0054, iter [04700, 05004], lr: 0.057900, loss: 2.1667
2022-06-27 23:48:37 - train: epoch 0054, iter [04800, 05004], lr: 0.057874, loss: 1.9407
2022-06-27 23:49:11 - train: epoch 0054, iter [04900, 05004], lr: 0.057849, loss: 1.7124
2022-06-27 23:49:43 - train: epoch 0054, iter [05000, 05004], lr: 0.057823, loss: 1.8115
2022-06-27 23:49:44 - train: epoch 054, train_loss: 1.7969
2022-06-27 23:50:58 - eval: epoch: 054, acc1: 62.026%, acc5: 84.856%, test_loss: 1.5940, per_image_load_time: 2.399ms, per_image_inference_time: 0.405ms
2022-06-27 23:50:58 - until epoch: 054, best_acc1: 62.868%
2022-06-27 23:50:58 - epoch 055 lr: 0.057821
2022-06-27 23:51:37 - train: epoch 0055, iter [00100, 05004], lr: 0.057796, loss: 1.7036
2022-06-27 23:52:10 - train: epoch 0055, iter [00200, 05004], lr: 0.057770, loss: 1.6849
2022-06-27 23:52:44 - train: epoch 0055, iter [00300, 05004], lr: 0.057744, loss: 1.6326
2022-06-27 23:53:18 - train: epoch 0055, iter [00400, 05004], lr: 0.057718, loss: 1.6406
2022-06-27 23:53:50 - train: epoch 0055, iter [00500, 05004], lr: 0.057693, loss: 1.5238
2022-06-27 23:54:24 - train: epoch 0055, iter [00600, 05004], lr: 0.057667, loss: 1.6639
2022-06-27 23:54:57 - train: epoch 0055, iter [00700, 05004], lr: 0.057641, loss: 1.9410
2022-06-27 23:55:30 - train: epoch 0055, iter [00800, 05004], lr: 0.057615, loss: 1.5821
2022-06-27 23:56:04 - train: epoch 0055, iter [00900, 05004], lr: 0.057589, loss: 1.8650
2022-06-27 23:56:37 - train: epoch 0055, iter [01000, 05004], lr: 0.057563, loss: 1.7900
2022-06-27 23:57:11 - train: epoch 0055, iter [01100, 05004], lr: 0.057537, loss: 1.8097
2022-06-27 23:57:44 - train: epoch 0055, iter [01200, 05004], lr: 0.057512, loss: 1.7546
2022-06-27 23:58:17 - train: epoch 0055, iter [01300, 05004], lr: 0.057486, loss: 2.0389
2022-06-27 23:58:51 - train: epoch 0055, iter [01400, 05004], lr: 0.057460, loss: 1.7041
2022-06-27 23:59:24 - train: epoch 0055, iter [01500, 05004], lr: 0.057434, loss: 1.8425
2022-06-27 23:59:58 - train: epoch 0055, iter [01600, 05004], lr: 0.057408, loss: 1.8531
2022-06-28 00:00:31 - train: epoch 0055, iter [01700, 05004], lr: 0.057382, loss: 1.8470
2022-06-28 00:01:04 - train: epoch 0055, iter [01800, 05004], lr: 0.057356, loss: 1.9248
2022-06-28 00:01:38 - train: epoch 0055, iter [01900, 05004], lr: 0.057330, loss: 1.7697
2022-06-28 00:02:11 - train: epoch 0055, iter [02000, 05004], lr: 0.057305, loss: 1.7400
2022-06-28 00:02:44 - train: epoch 0055, iter [02100, 05004], lr: 0.057279, loss: 1.6580
2022-06-28 00:03:17 - train: epoch 0055, iter [02200, 05004], lr: 0.057253, loss: 1.8916
2022-06-28 00:03:51 - train: epoch 0055, iter [02300, 05004], lr: 0.057227, loss: 1.7277
2022-06-28 00:04:24 - train: epoch 0055, iter [02400, 05004], lr: 0.057201, loss: 1.6355
2022-06-28 00:04:58 - train: epoch 0055, iter [02500, 05004], lr: 0.057175, loss: 1.8090
2022-06-28 00:05:31 - train: epoch 0055, iter [02600, 05004], lr: 0.057149, loss: 1.6661
2022-06-28 00:06:03 - train: epoch 0055, iter [02700, 05004], lr: 0.057123, loss: 1.5784
2022-06-28 00:06:36 - train: epoch 0055, iter [02800, 05004], lr: 0.057097, loss: 1.7907
2022-06-28 00:07:10 - train: epoch 0055, iter [02900, 05004], lr: 0.057072, loss: 1.8709
2022-06-28 00:07:43 - train: epoch 0055, iter [03000, 05004], lr: 0.057046, loss: 1.6933
2022-06-28 00:08:16 - train: epoch 0055, iter [03100, 05004], lr: 0.057020, loss: 1.8172
2022-06-28 00:08:50 - train: epoch 0055, iter [03200, 05004], lr: 0.056994, loss: 1.7023
2022-06-28 00:09:23 - train: epoch 0055, iter [03300, 05004], lr: 0.056968, loss: 1.5882
2022-06-28 00:09:56 - train: epoch 0055, iter [03400, 05004], lr: 0.056942, loss: 1.6623
2022-06-28 00:10:30 - train: epoch 0055, iter [03500, 05004], lr: 0.056916, loss: 1.5610
2022-06-28 00:11:03 - train: epoch 0055, iter [03600, 05004], lr: 0.056890, loss: 1.8074
2022-06-28 00:11:36 - train: epoch 0055, iter [03700, 05004], lr: 0.056864, loss: 1.7696
2022-06-28 00:12:10 - train: epoch 0055, iter [03800, 05004], lr: 0.056838, loss: 1.8435
2022-06-28 00:12:43 - train: epoch 0055, iter [03900, 05004], lr: 0.056813, loss: 2.2349
2022-06-28 00:13:16 - train: epoch 0055, iter [04000, 05004], lr: 0.056787, loss: 1.7665
2022-06-28 00:13:49 - train: epoch 0055, iter [04100, 05004], lr: 0.056761, loss: 1.7355
2022-06-28 00:14:22 - train: epoch 0055, iter [04200, 05004], lr: 0.056735, loss: 1.8006
2022-06-28 00:14:57 - train: epoch 0055, iter [04300, 05004], lr: 0.056709, loss: 1.8626
2022-06-28 00:15:30 - train: epoch 0055, iter [04400, 05004], lr: 0.056683, loss: 1.9164
2022-06-28 00:16:03 - train: epoch 0055, iter [04500, 05004], lr: 0.056657, loss: 1.7683
2022-06-28 00:16:37 - train: epoch 0055, iter [04600, 05004], lr: 0.056631, loss: 1.8553
2022-06-28 00:17:10 - train: epoch 0055, iter [04700, 05004], lr: 0.056605, loss: 1.6373
2022-06-28 00:17:43 - train: epoch 0055, iter [04800, 05004], lr: 0.056579, loss: 1.8222
2022-06-28 00:18:16 - train: epoch 0055, iter [04900, 05004], lr: 0.056553, loss: 1.7687
2022-06-28 00:18:48 - train: epoch 0055, iter [05000, 05004], lr: 0.056527, loss: 1.8258
2022-06-28 00:18:50 - train: epoch 055, train_loss: 1.7848
2022-06-28 00:20:04 - eval: epoch: 055, acc1: 63.466%, acc5: 85.962%, test_loss: 1.5075, per_image_load_time: 2.433ms, per_image_inference_time: 0.406ms
2022-06-28 00:20:04 - until epoch: 055, best_acc1: 63.466%
2022-06-28 00:20:04 - epoch 056 lr: 0.056526
2022-06-28 00:20:43 - train: epoch 0056, iter [00100, 05004], lr: 0.056500, loss: 1.8084
2022-06-28 00:21:17 - train: epoch 0056, iter [00200, 05004], lr: 0.056474, loss: 1.8631
2022-06-28 00:21:50 - train: epoch 0056, iter [00300, 05004], lr: 0.056448, loss: 1.7846
2022-06-28 00:22:24 - train: epoch 0056, iter [00400, 05004], lr: 0.056423, loss: 1.7462
2022-06-28 00:22:57 - train: epoch 0056, iter [00500, 05004], lr: 0.056397, loss: 1.6821
2022-06-28 00:23:31 - train: epoch 0056, iter [00600, 05004], lr: 0.056371, loss: 1.7509
2022-06-28 00:24:04 - train: epoch 0056, iter [00700, 05004], lr: 0.056345, loss: 1.7531
2022-06-28 00:24:38 - train: epoch 0056, iter [00800, 05004], lr: 0.056319, loss: 1.9975
2022-06-28 00:25:11 - train: epoch 0056, iter [00900, 05004], lr: 0.056293, loss: 1.7830
2022-06-28 00:25:44 - train: epoch 0056, iter [01000, 05004], lr: 0.056267, loss: 1.7692
2022-06-28 00:26:18 - train: epoch 0056, iter [01100, 05004], lr: 0.056241, loss: 1.6750
2022-06-28 00:26:51 - train: epoch 0056, iter [01200, 05004], lr: 0.056215, loss: 1.6707
2022-06-28 00:27:24 - train: epoch 0056, iter [01300, 05004], lr: 0.056189, loss: 1.8634
2022-06-28 00:27:57 - train: epoch 0056, iter [01400, 05004], lr: 0.056163, loss: 1.6288
2022-06-28 00:28:31 - train: epoch 0056, iter [01500, 05004], lr: 0.056137, loss: 2.1015
2022-06-28 00:29:04 - train: epoch 0056, iter [01600, 05004], lr: 0.056111, loss: 1.6226
2022-06-28 00:29:38 - train: epoch 0056, iter [01700, 05004], lr: 0.056085, loss: 1.8935
2022-06-28 00:30:10 - train: epoch 0056, iter [01800, 05004], lr: 0.056059, loss: 2.0156
2022-06-28 00:30:43 - train: epoch 0056, iter [01900, 05004], lr: 0.056033, loss: 1.7646
2022-06-28 00:31:16 - train: epoch 0056, iter [02000, 05004], lr: 0.056007, loss: 1.6962
2022-06-28 00:31:49 - train: epoch 0056, iter [02100, 05004], lr: 0.055981, loss: 1.8072
2022-06-28 00:32:22 - train: epoch 0056, iter [02200, 05004], lr: 0.055955, loss: 1.8564
2022-06-28 00:32:55 - train: epoch 0056, iter [02300, 05004], lr: 0.055929, loss: 1.9655
2022-06-28 00:33:28 - train: epoch 0056, iter [02400, 05004], lr: 0.055903, loss: 1.7231
2022-06-28 00:34:02 - train: epoch 0056, iter [02500, 05004], lr: 0.055877, loss: 1.9255
2022-06-28 00:34:35 - train: epoch 0056, iter [02600, 05004], lr: 0.055851, loss: 1.7499
2022-06-28 00:35:08 - train: epoch 0056, iter [02700, 05004], lr: 0.055825, loss: 1.7308
2022-06-28 00:35:42 - train: epoch 0056, iter [02800, 05004], lr: 0.055799, loss: 1.7067
2022-06-28 00:36:15 - train: epoch 0056, iter [02900, 05004], lr: 0.055773, loss: 1.9386
2022-06-28 00:36:48 - train: epoch 0056, iter [03000, 05004], lr: 0.055747, loss: 1.9810
2022-06-28 00:37:21 - train: epoch 0056, iter [03100, 05004], lr: 0.055721, loss: 1.7314
2022-06-28 00:37:54 - train: epoch 0056, iter [03200, 05004], lr: 0.055696, loss: 1.5730
2022-06-28 00:38:28 - train: epoch 0056, iter [03300, 05004], lr: 0.055670, loss: 2.0980
2022-06-28 00:39:01 - train: epoch 0056, iter [03400, 05004], lr: 0.055644, loss: 1.7322
2022-06-28 00:39:34 - train: epoch 0056, iter [03500, 05004], lr: 0.055618, loss: 1.7138
2022-06-28 00:40:07 - train: epoch 0056, iter [03600, 05004], lr: 0.055592, loss: 1.5104
2022-06-28 00:40:41 - train: epoch 0056, iter [03700, 05004], lr: 0.055566, loss: 1.6855
2022-06-28 00:41:14 - train: epoch 0056, iter [03800, 05004], lr: 0.055540, loss: 1.6202
2022-06-28 00:41:48 - train: epoch 0056, iter [03900, 05004], lr: 0.055514, loss: 2.0328
2022-06-28 00:42:21 - train: epoch 0056, iter [04000, 05004], lr: 0.055488, loss: 1.7285
2022-06-28 00:42:54 - train: epoch 0056, iter [04100, 05004], lr: 0.055462, loss: 2.0208
2022-06-28 00:43:27 - train: epoch 0056, iter [04200, 05004], lr: 0.055436, loss: 1.7538
2022-06-28 00:44:01 - train: epoch 0056, iter [04300, 05004], lr: 0.055410, loss: 1.6907
2022-06-28 00:44:33 - train: epoch 0056, iter [04400, 05004], lr: 0.055384, loss: 1.9413
2022-06-28 00:45:07 - train: epoch 0056, iter [04500, 05004], lr: 0.055358, loss: 1.7173
2022-06-28 00:45:40 - train: epoch 0056, iter [04600, 05004], lr: 0.055332, loss: 1.8893
2022-06-28 00:46:14 - train: epoch 0056, iter [04700, 05004], lr: 0.055306, loss: 1.7982
2022-06-28 00:46:46 - train: epoch 0056, iter [04800, 05004], lr: 0.055279, loss: 2.0174
2022-06-28 00:47:20 - train: epoch 0056, iter [04900, 05004], lr: 0.055253, loss: 1.7421
2022-06-28 00:47:51 - train: epoch 0056, iter [05000, 05004], lr: 0.055227, loss: 1.9517
2022-06-28 00:47:52 - train: epoch 056, train_loss: 1.7734
2022-06-28 00:49:06 - eval: epoch: 056, acc1: 63.638%, acc5: 85.936%, test_loss: 1.5180, per_image_load_time: 2.439ms, per_image_inference_time: 0.383ms
2022-06-28 00:49:06 - until epoch: 056, best_acc1: 63.638%
2022-06-28 00:49:06 - epoch 057 lr: 0.055226
2022-06-28 00:49:45 - train: epoch 0057, iter [00100, 05004], lr: 0.055200, loss: 1.8700
2022-06-28 00:50:18 - train: epoch 0057, iter [00200, 05004], lr: 0.055174, loss: 1.6478
2022-06-28 00:50:52 - train: epoch 0057, iter [00300, 05004], lr: 0.055148, loss: 1.6316
2022-06-28 00:51:25 - train: epoch 0057, iter [00400, 05004], lr: 0.055122, loss: 1.7508
2022-06-28 00:51:59 - train: epoch 0057, iter [00500, 05004], lr: 0.055096, loss: 1.5517
2022-06-28 00:52:32 - train: epoch 0057, iter [00600, 05004], lr: 0.055070, loss: 1.8605
2022-06-28 00:53:06 - train: epoch 0057, iter [00700, 05004], lr: 0.055044, loss: 1.4504
2022-06-28 00:53:39 - train: epoch 0057, iter [00800, 05004], lr: 0.055018, loss: 1.7765
2022-06-28 00:54:12 - train: epoch 0057, iter [00900, 05004], lr: 0.054992, loss: 1.8582
2022-06-28 00:54:45 - train: epoch 0057, iter [01000, 05004], lr: 0.054966, loss: 1.7203
2022-06-28 00:55:19 - train: epoch 0057, iter [01100, 05004], lr: 0.054940, loss: 1.7456
2022-06-28 00:55:53 - train: epoch 0057, iter [01200, 05004], lr: 0.054914, loss: 1.6364
2022-06-28 00:56:27 - train: epoch 0057, iter [01300, 05004], lr: 0.054888, loss: 1.8076
2022-06-28 00:56:59 - train: epoch 0057, iter [01400, 05004], lr: 0.054862, loss: 1.8845
2022-06-28 00:57:32 - train: epoch 0057, iter [01500, 05004], lr: 0.054836, loss: 1.9002
2022-06-28 00:58:06 - train: epoch 0057, iter [01600, 05004], lr: 0.054810, loss: 2.0002
2022-06-28 00:58:39 - train: epoch 0057, iter [01700, 05004], lr: 0.054784, loss: 1.9036
2022-06-28 00:59:12 - train: epoch 0057, iter [01800, 05004], lr: 0.054758, loss: 1.8737
2022-06-28 00:59:45 - train: epoch 0057, iter [01900, 05004], lr: 0.054732, loss: 1.6826
2022-06-28 01:00:18 - train: epoch 0057, iter [02000, 05004], lr: 0.054706, loss: 1.7178
2022-06-28 01:00:51 - train: epoch 0057, iter [02100, 05004], lr: 0.054680, loss: 1.8544
2022-06-28 01:01:25 - train: epoch 0057, iter [02200, 05004], lr: 0.054654, loss: 1.8401
2022-06-28 01:01:59 - train: epoch 0057, iter [02300, 05004], lr: 0.054628, loss: 1.6565
2022-06-28 01:02:32 - train: epoch 0057, iter [02400, 05004], lr: 0.054602, loss: 1.7221
2022-06-28 01:03:04 - train: epoch 0057, iter [02500, 05004], lr: 0.054576, loss: 1.7906
2022-06-28 01:03:38 - train: epoch 0057, iter [02600, 05004], lr: 0.054550, loss: 1.6633
2022-06-28 01:04:11 - train: epoch 0057, iter [02700, 05004], lr: 0.054524, loss: 1.5156
2022-06-28 01:04:44 - train: epoch 0057, iter [02800, 05004], lr: 0.054497, loss: 1.3731
2022-06-28 01:05:17 - train: epoch 0057, iter [02900, 05004], lr: 0.054471, loss: 1.9158
2022-06-28 01:05:50 - train: epoch 0057, iter [03000, 05004], lr: 0.054445, loss: 1.8709
2022-06-28 01:06:23 - train: epoch 0057, iter [03100, 05004], lr: 0.054419, loss: 2.0531
2022-06-28 01:06:57 - train: epoch 0057, iter [03200, 05004], lr: 0.054393, loss: 1.9350
2022-06-28 01:07:31 - train: epoch 0057, iter [03300, 05004], lr: 0.054367, loss: 1.6871
2022-06-28 01:08:03 - train: epoch 0057, iter [03400, 05004], lr: 0.054341, loss: 1.7984
2022-06-28 01:08:37 - train: epoch 0057, iter [03500, 05004], lr: 0.054315, loss: 1.8255
2022-06-28 01:09:11 - train: epoch 0057, iter [03600, 05004], lr: 0.054289, loss: 1.6565
2022-06-28 01:09:44 - train: epoch 0057, iter [03700, 05004], lr: 0.054263, loss: 1.6364
2022-06-28 01:10:17 - train: epoch 0057, iter [03800, 05004], lr: 0.054237, loss: 1.7758
2022-06-28 01:10:51 - train: epoch 0057, iter [03900, 05004], lr: 0.054211, loss: 1.9738
2022-06-28 01:11:23 - train: epoch 0057, iter [04000, 05004], lr: 0.054185, loss: 1.6727
2022-06-28 01:11:56 - train: epoch 0057, iter [04100, 05004], lr: 0.054159, loss: 1.8818
2022-06-28 01:12:29 - train: epoch 0057, iter [04200, 05004], lr: 0.054133, loss: 1.8259
2022-06-28 01:13:03 - train: epoch 0057, iter [04300, 05004], lr: 0.054107, loss: 1.6542
2022-06-28 01:13:36 - train: epoch 0057, iter [04400, 05004], lr: 0.054080, loss: 1.7556
2022-06-28 01:14:10 - train: epoch 0057, iter [04500, 05004], lr: 0.054054, loss: 1.9058
2022-06-28 01:14:43 - train: epoch 0057, iter [04600, 05004], lr: 0.054028, loss: 1.8013
2022-06-28 01:15:16 - train: epoch 0057, iter [04700, 05004], lr: 0.054002, loss: 1.6635
2022-06-28 01:15:49 - train: epoch 0057, iter [04800, 05004], lr: 0.053976, loss: 2.0167
2022-06-28 01:16:22 - train: epoch 0057, iter [04900, 05004], lr: 0.053950, loss: 1.9943
2022-06-28 01:16:54 - train: epoch 0057, iter [05000, 05004], lr: 0.053924, loss: 1.9132
2022-06-28 01:16:55 - train: epoch 057, train_loss: 1.7656
2022-06-28 01:18:08 - eval: epoch: 057, acc1: 64.018%, acc5: 86.044%, test_loss: 1.4993, per_image_load_time: 2.392ms, per_image_inference_time: 0.405ms
2022-06-28 01:18:09 - until epoch: 057, best_acc1: 64.018%
2022-06-28 01:18:09 - epoch 058 lr: 0.053923
2022-06-28 01:18:47 - train: epoch 0058, iter [00100, 05004], lr: 0.053897, loss: 1.7897
2022-06-28 01:19:21 - train: epoch 0058, iter [00200, 05004], lr: 0.053871, loss: 1.6186
2022-06-28 01:19:55 - train: epoch 0058, iter [00300, 05004], lr: 0.053845, loss: 1.8149
2022-06-28 01:20:28 - train: epoch 0058, iter [00400, 05004], lr: 0.053819, loss: 1.6896
2022-06-28 01:21:01 - train: epoch 0058, iter [00500, 05004], lr: 0.053793, loss: 1.6216
2022-06-28 01:21:34 - train: epoch 0058, iter [00600, 05004], lr: 0.053766, loss: 1.9097
2022-06-28 01:22:08 - train: epoch 0058, iter [00700, 05004], lr: 0.053740, loss: 1.7083
2022-06-28 01:22:41 - train: epoch 0058, iter [00800, 05004], lr: 0.053714, loss: 1.5173
2022-06-28 01:23:14 - train: epoch 0058, iter [00900, 05004], lr: 0.053688, loss: 1.5680
2022-06-28 01:23:47 - train: epoch 0058, iter [01000, 05004], lr: 0.053662, loss: 1.7913
2022-06-28 01:24:20 - train: epoch 0058, iter [01100, 05004], lr: 0.053636, loss: 1.5493
2022-06-28 01:24:53 - train: epoch 0058, iter [01200, 05004], lr: 0.053610, loss: 1.6385
2022-06-28 01:25:27 - train: epoch 0058, iter [01300, 05004], lr: 0.053584, loss: 1.8921
2022-06-28 01:26:00 - train: epoch 0058, iter [01400, 05004], lr: 0.053558, loss: 1.8307
2022-06-28 01:26:33 - train: epoch 0058, iter [01500, 05004], lr: 0.053532, loss: 1.6479
2022-06-28 01:27:07 - train: epoch 0058, iter [01600, 05004], lr: 0.053506, loss: 1.7390
2022-06-28 01:27:41 - train: epoch 0058, iter [01700, 05004], lr: 0.053479, loss: 1.9821
2022-06-28 01:28:14 - train: epoch 0058, iter [01800, 05004], lr: 0.053453, loss: 1.8456
2022-06-28 01:28:47 - train: epoch 0058, iter [01900, 05004], lr: 0.053427, loss: 1.9914
2022-06-28 01:29:21 - train: epoch 0058, iter [02000, 05004], lr: 0.053401, loss: 1.9081
2022-06-28 01:29:54 - train: epoch 0058, iter [02100, 05004], lr: 0.053375, loss: 1.6723
2022-06-28 01:30:27 - train: epoch 0058, iter [02200, 05004], lr: 0.053349, loss: 1.5257
2022-06-28 01:31:01 - train: epoch 0058, iter [02300, 05004], lr: 0.053323, loss: 1.7146
2022-06-28 01:31:34 - train: epoch 0058, iter [02400, 05004], lr: 0.053297, loss: 1.8077
2022-06-28 01:32:06 - train: epoch 0058, iter [02500, 05004], lr: 0.053271, loss: 1.9295
2022-06-28 01:32:39 - train: epoch 0058, iter [02600, 05004], lr: 0.053245, loss: 1.6511
2022-06-28 01:33:12 - train: epoch 0058, iter [02700, 05004], lr: 0.053218, loss: 1.9429
2022-06-28 01:33:45 - train: epoch 0058, iter [02800, 05004], lr: 0.053192, loss: 1.5488
2022-06-28 01:34:17 - train: epoch 0058, iter [02900, 05004], lr: 0.053166, loss: 1.6013
2022-06-28 01:34:50 - train: epoch 0058, iter [03000, 05004], lr: 0.053140, loss: 1.8603
2022-06-28 01:35:23 - train: epoch 0058, iter [03100, 05004], lr: 0.053114, loss: 1.5883
2022-06-28 01:35:56 - train: epoch 0058, iter [03200, 05004], lr: 0.053088, loss: 1.5044
2022-06-28 01:36:28 - train: epoch 0058, iter [03300, 05004], lr: 0.053062, loss: 1.7676
2022-06-28 01:37:00 - train: epoch 0058, iter [03400, 05004], lr: 0.053036, loss: 1.6401
2022-06-28 01:37:33 - train: epoch 0058, iter [03500, 05004], lr: 0.053010, loss: 1.6575
2022-06-28 01:38:06 - train: epoch 0058, iter [03600, 05004], lr: 0.052983, loss: 1.7086
2022-06-28 01:38:38 - train: epoch 0058, iter [03700, 05004], lr: 0.052957, loss: 1.8149
2022-06-28 01:39:11 - train: epoch 0058, iter [03800, 05004], lr: 0.052931, loss: 1.8689
2022-06-28 01:39:45 - train: epoch 0058, iter [03900, 05004], lr: 0.052905, loss: 1.8439
2022-06-28 01:40:17 - train: epoch 0058, iter [04000, 05004], lr: 0.052879, loss: 1.9888
2022-06-28 01:40:50 - train: epoch 0058, iter [04100, 05004], lr: 0.052853, loss: 1.8566
2022-06-28 01:41:23 - train: epoch 0058, iter [04200, 05004], lr: 0.052827, loss: 1.4582
2022-06-28 01:41:55 - train: epoch 0058, iter [04300, 05004], lr: 0.052801, loss: 2.0111
2022-06-28 01:42:29 - train: epoch 0058, iter [04400, 05004], lr: 0.052775, loss: 1.5251
2022-06-28 01:43:01 - train: epoch 0058, iter [04500, 05004], lr: 0.052748, loss: 1.8611
2022-06-28 01:43:34 - train: epoch 0058, iter [04600, 05004], lr: 0.052722, loss: 1.5859
2022-06-28 01:44:07 - train: epoch 0058, iter [04700, 05004], lr: 0.052696, loss: 1.8166
2022-06-28 01:44:39 - train: epoch 0058, iter [04800, 05004], lr: 0.052670, loss: 1.8544
2022-06-28 01:45:12 - train: epoch 0058, iter [04900, 05004], lr: 0.052644, loss: 1.6535
2022-06-28 01:45:44 - train: epoch 0058, iter [05000, 05004], lr: 0.052618, loss: 1.6037
2022-06-28 01:45:45 - train: epoch 058, train_loss: 1.7537
2022-06-28 01:46:59 - eval: epoch: 058, acc1: 64.080%, acc5: 86.180%, test_loss: 1.4992, per_image_load_time: 2.128ms, per_image_inference_time: 0.413ms
2022-06-28 01:46:59 - until epoch: 058, best_acc1: 64.080%
2022-06-28 01:46:59 - epoch 059 lr: 0.052617
2022-06-28 01:47:38 - train: epoch 0059, iter [00100, 05004], lr: 0.052591, loss: 1.8915
2022-06-28 01:48:11 - train: epoch 0059, iter [00200, 05004], lr: 0.052565, loss: 1.6687
2022-06-28 01:48:43 - train: epoch 0059, iter [00300, 05004], lr: 0.052538, loss: 1.7422
2022-06-28 01:49:17 - train: epoch 0059, iter [00400, 05004], lr: 0.052512, loss: 1.8307
2022-06-28 01:49:50 - train: epoch 0059, iter [00500, 05004], lr: 0.052486, loss: 2.0376
2022-06-28 01:50:23 - train: epoch 0059, iter [00600, 05004], lr: 0.052460, loss: 1.7654
2022-06-28 01:50:56 - train: epoch 0059, iter [00700, 05004], lr: 0.052434, loss: 1.6272
2022-06-28 01:51:30 - train: epoch 0059, iter [00800, 05004], lr: 0.052408, loss: 1.7721
2022-06-28 01:52:03 - train: epoch 0059, iter [00900, 05004], lr: 0.052382, loss: 1.6857
2022-06-28 01:52:36 - train: epoch 0059, iter [01000, 05004], lr: 0.052356, loss: 1.8165
2022-06-28 01:53:09 - train: epoch 0059, iter [01100, 05004], lr: 0.052329, loss: 1.9538
2022-06-28 01:53:42 - train: epoch 0059, iter [01200, 05004], lr: 0.052303, loss: 1.4361
2022-06-28 01:54:15 - train: epoch 0059, iter [01300, 05004], lr: 0.052277, loss: 1.9688
2022-06-28 01:54:49 - train: epoch 0059, iter [01400, 05004], lr: 0.052251, loss: 1.9114
2022-06-28 01:55:22 - train: epoch 0059, iter [01500, 05004], lr: 0.052225, loss: 1.8864
2022-06-28 01:55:55 - train: epoch 0059, iter [01600, 05004], lr: 0.052199, loss: 1.5947
2022-06-28 01:56:29 - train: epoch 0059, iter [01700, 05004], lr: 0.052173, loss: 1.7292
2022-06-28 01:57:02 - train: epoch 0059, iter [01800, 05004], lr: 0.052146, loss: 1.7931
2022-06-28 01:57:35 - train: epoch 0059, iter [01900, 05004], lr: 0.052120, loss: 1.8370
2022-06-28 01:58:09 - train: epoch 0059, iter [02000, 05004], lr: 0.052094, loss: 1.6077
2022-06-28 01:58:41 - train: epoch 0059, iter [02100, 05004], lr: 0.052068, loss: 1.9982
2022-06-28 01:59:15 - train: epoch 0059, iter [02200, 05004], lr: 0.052042, loss: 1.8982
2022-06-28 01:59:48 - train: epoch 0059, iter [02300, 05004], lr: 0.052016, loss: 1.7372
2022-06-28 02:00:22 - train: epoch 0059, iter [02400, 05004], lr: 0.051990, loss: 1.8017
2022-06-28 02:00:55 - train: epoch 0059, iter [02500, 05004], lr: 0.051964, loss: 1.7695
2022-06-28 02:01:28 - train: epoch 0059, iter [02600, 05004], lr: 0.051937, loss: 1.6393
2022-06-28 02:02:01 - train: epoch 0059, iter [02700, 05004], lr: 0.051911, loss: 1.7389
2022-06-28 02:02:36 - train: epoch 0059, iter [02800, 05004], lr: 0.051885, loss: 1.9050
2022-06-28 02:03:08 - train: epoch 0059, iter [02900, 05004], lr: 0.051859, loss: 1.6889
2022-06-28 02:03:41 - train: epoch 0059, iter [03000, 05004], lr: 0.051833, loss: 2.2496
2022-06-28 02:04:15 - train: epoch 0059, iter [03100, 05004], lr: 0.051807, loss: 1.6770
2022-06-28 02:04:48 - train: epoch 0059, iter [03200, 05004], lr: 0.051781, loss: 1.7536
2022-06-28 02:05:21 - train: epoch 0059, iter [03300, 05004], lr: 0.051754, loss: 1.8435
2022-06-28 02:05:54 - train: epoch 0059, iter [03400, 05004], lr: 0.051728, loss: 2.1461
2022-06-28 02:06:27 - train: epoch 0059, iter [03500, 05004], lr: 0.051702, loss: 1.5785
2022-06-28 02:07:01 - train: epoch 0059, iter [03600, 05004], lr: 0.051676, loss: 1.7454
2022-06-28 02:07:34 - train: epoch 0059, iter [03700, 05004], lr: 0.051650, loss: 1.6818
2022-06-28 02:08:07 - train: epoch 0059, iter [03800, 05004], lr: 0.051624, loss: 1.6859
2022-06-28 02:08:41 - train: epoch 0059, iter [03900, 05004], lr: 0.051598, loss: 1.6785
2022-06-28 02:09:14 - train: epoch 0059, iter [04000, 05004], lr: 0.051571, loss: 2.1072
2022-06-28 02:09:47 - train: epoch 0059, iter [04100, 05004], lr: 0.051545, loss: 1.7882
2022-06-28 02:10:20 - train: epoch 0059, iter [04200, 05004], lr: 0.051519, loss: 1.7775
2022-06-28 02:10:54 - train: epoch 0059, iter [04300, 05004], lr: 0.051493, loss: 1.9050
2022-06-28 02:11:27 - train: epoch 0059, iter [04400, 05004], lr: 0.051467, loss: 1.9175
2022-06-28 02:12:01 - train: epoch 0059, iter [04500, 05004], lr: 0.051441, loss: 1.8998
2022-06-28 02:12:34 - train: epoch 0059, iter [04600, 05004], lr: 0.051414, loss: 1.7757
2022-06-28 02:13:08 - train: epoch 0059, iter [04700, 05004], lr: 0.051388, loss: 1.7364
2022-06-28 02:13:41 - train: epoch 0059, iter [04800, 05004], lr: 0.051362, loss: 1.6204
2022-06-28 02:14:15 - train: epoch 0059, iter [04900, 05004], lr: 0.051336, loss: 1.9094
2022-06-28 02:14:48 - train: epoch 0059, iter [05000, 05004], lr: 0.051310, loss: 1.9398
2022-06-28 02:14:49 - train: epoch 059, train_loss: 1.7434
2022-06-28 02:16:01 - eval: epoch: 059, acc1: 63.316%, acc5: 85.634%, test_loss: 1.5188, per_image_load_time: 2.384ms, per_image_inference_time: 0.363ms
2022-06-28 02:16:01 - until epoch: 059, best_acc1: 64.080%
2022-06-28 02:16:01 - epoch 060 lr: 0.051309
2022-06-28 02:16:40 - train: epoch 0060, iter [00100, 05004], lr: 0.051283, loss: 1.6576
2022-06-28 02:17:13 - train: epoch 0060, iter [00200, 05004], lr: 0.051257, loss: 1.8009
2022-06-28 02:17:47 - train: epoch 0060, iter [00300, 05004], lr: 0.051230, loss: 1.6920
2022-06-28 02:18:19 - train: epoch 0060, iter [00400, 05004], lr: 0.051204, loss: 1.7560
2022-06-28 02:18:53 - train: epoch 0060, iter [00500, 05004], lr: 0.051178, loss: 1.9052
2022-06-28 02:19:27 - train: epoch 0060, iter [00600, 05004], lr: 0.051152, loss: 1.8982
2022-06-28 02:20:00 - train: epoch 0060, iter [00700, 05004], lr: 0.051126, loss: 1.7488
2022-06-28 02:20:33 - train: epoch 0060, iter [00800, 05004], lr: 0.051100, loss: 1.9869
2022-06-28 02:21:07 - train: epoch 0060, iter [00900, 05004], lr: 0.051073, loss: 1.5127
2022-06-28 02:21:41 - train: epoch 0060, iter [01000, 05004], lr: 0.051047, loss: 1.4173
2022-06-28 02:22:14 - train: epoch 0060, iter [01100, 05004], lr: 0.051021, loss: 1.4553
2022-06-28 02:22:47 - train: epoch 0060, iter [01200, 05004], lr: 0.050995, loss: 1.6002
2022-06-28 02:23:21 - train: epoch 0060, iter [01300, 05004], lr: 0.050969, loss: 1.6328
2022-06-28 02:23:53 - train: epoch 0060, iter [01400, 05004], lr: 0.050943, loss: 1.7681
2022-06-28 02:24:27 - train: epoch 0060, iter [01500, 05004], lr: 0.050917, loss: 1.8072
2022-06-28 02:25:01 - train: epoch 0060, iter [01600, 05004], lr: 0.050890, loss: 1.7954
2022-06-28 02:25:33 - train: epoch 0060, iter [01700, 05004], lr: 0.050864, loss: 1.7519
2022-06-28 02:26:07 - train: epoch 0060, iter [01800, 05004], lr: 0.050838, loss: 1.7860
2022-06-28 02:26:40 - train: epoch 0060, iter [01900, 05004], lr: 0.050812, loss: 1.9779
2022-06-28 02:27:15 - train: epoch 0060, iter [02000, 05004], lr: 0.050786, loss: 1.5452
2022-06-28 02:27:48 - train: epoch 0060, iter [02100, 05004], lr: 0.050760, loss: 1.8277
2022-06-28 02:28:21 - train: epoch 0060, iter [02200, 05004], lr: 0.050733, loss: 1.8362
2022-06-28 02:28:55 - train: epoch 0060, iter [02300, 05004], lr: 0.050707, loss: 1.4651
2022-06-28 02:29:27 - train: epoch 0060, iter [02400, 05004], lr: 0.050681, loss: 1.6822
2022-06-28 02:30:01 - train: epoch 0060, iter [02500, 05004], lr: 0.050655, loss: 1.7758
2022-06-28 02:30:35 - train: epoch 0060, iter [02600, 05004], lr: 0.050629, loss: 1.7937
2022-06-28 02:31:08 - train: epoch 0060, iter [02700, 05004], lr: 0.050603, loss: 1.7228
2022-06-28 02:31:41 - train: epoch 0060, iter [02800, 05004], lr: 0.050577, loss: 1.6859
2022-06-28 02:32:15 - train: epoch 0060, iter [02900, 05004], lr: 0.050550, loss: 1.7884
2022-06-28 02:32:47 - train: epoch 0060, iter [03000, 05004], lr: 0.050524, loss: 1.9190
2022-06-28 02:33:21 - train: epoch 0060, iter [03100, 05004], lr: 0.050498, loss: 1.8152
2022-06-28 02:33:55 - train: epoch 0060, iter [03200, 05004], lr: 0.050472, loss: 1.7438
2022-06-28 02:34:28 - train: epoch 0060, iter [03300, 05004], lr: 0.050446, loss: 1.5024
2022-06-28 02:35:01 - train: epoch 0060, iter [03400, 05004], lr: 0.050420, loss: 1.7977
2022-06-28 02:35:34 - train: epoch 0060, iter [03500, 05004], lr: 0.050393, loss: 1.8074
2022-06-28 02:36:06 - train: epoch 0060, iter [03600, 05004], lr: 0.050367, loss: 1.8369
2022-06-28 02:36:39 - train: epoch 0060, iter [03700, 05004], lr: 0.050341, loss: 1.8717
2022-06-28 02:37:13 - train: epoch 0060, iter [03800, 05004], lr: 0.050315, loss: 1.7655
2022-06-28 02:37:47 - train: epoch 0060, iter [03900, 05004], lr: 0.050289, loss: 2.0894
2022-06-28 02:38:20 - train: epoch 0060, iter [04000, 05004], lr: 0.050263, loss: 1.7168
2022-06-28 02:38:54 - train: epoch 0060, iter [04100, 05004], lr: 0.050236, loss: 1.8645
2022-06-28 02:39:27 - train: epoch 0060, iter [04200, 05004], lr: 0.050210, loss: 1.8483
2022-06-28 02:40:01 - train: epoch 0060, iter [04300, 05004], lr: 0.050184, loss: 1.6954
2022-06-28 02:40:34 - train: epoch 0060, iter [04400, 05004], lr: 0.050158, loss: 1.7827
2022-06-28 02:41:07 - train: epoch 0060, iter [04500, 05004], lr: 0.050132, loss: 1.5701
2022-06-28 02:41:40 - train: epoch 0060, iter [04600, 05004], lr: 0.050106, loss: 1.6634
2022-06-28 02:42:14 - train: epoch 0060, iter [04700, 05004], lr: 0.050080, loss: 1.8172
2022-06-28 02:42:47 - train: epoch 0060, iter [04800, 05004], lr: 0.050053, loss: 1.5980
2022-06-28 02:43:20 - train: epoch 0060, iter [04900, 05004], lr: 0.050027, loss: 1.7127
2022-06-28 02:43:52 - train: epoch 0060, iter [05000, 05004], lr: 0.050001, loss: 1.7486
2022-06-28 02:43:53 - train: epoch 060, train_loss: 1.7334
2022-06-28 02:45:07 - eval: epoch: 060, acc1: 63.886%, acc5: 85.966%, test_loss: 1.4917, per_image_load_time: 2.480ms, per_image_inference_time: 0.396ms
2022-06-28 02:45:08 - until epoch: 060, best_acc1: 64.080%
2022-06-28 02:45:08 - epoch 061 lr: 0.050000
2022-06-28 02:45:46 - train: epoch 0061, iter [00100, 05004], lr: 0.049974, loss: 1.5815
2022-06-28 02:46:19 - train: epoch 0061, iter [00200, 05004], lr: 0.049948, loss: 1.7443
2022-06-28 02:46:52 - train: epoch 0061, iter [00300, 05004], lr: 0.049922, loss: 1.4881
2022-06-28 02:47:25 - train: epoch 0061, iter [00400, 05004], lr: 0.049895, loss: 1.8994
2022-06-28 02:47:58 - train: epoch 0061, iter [00500, 05004], lr: 0.049869, loss: 1.6709
2022-06-28 02:48:31 - train: epoch 0061, iter [00600, 05004], lr: 0.049843, loss: 1.7745
2022-06-28 02:49:04 - train: epoch 0061, iter [00700, 05004], lr: 0.049817, loss: 1.5597
2022-06-28 02:49:38 - train: epoch 0061, iter [00800, 05004], lr: 0.049791, loss: 1.8237
2022-06-28 02:50:11 - train: epoch 0061, iter [00900, 05004], lr: 0.049765, loss: 1.6867
2022-06-28 02:50:45 - train: epoch 0061, iter [01000, 05004], lr: 0.049738, loss: 1.4659
2022-06-28 02:51:18 - train: epoch 0061, iter [01100, 05004], lr: 0.049712, loss: 1.4273
2022-06-28 02:51:50 - train: epoch 0061, iter [01200, 05004], lr: 0.049686, loss: 1.6467
2022-06-28 02:52:24 - train: epoch 0061, iter [01300, 05004], lr: 0.049660, loss: 1.5857
2022-06-28 02:52:57 - train: epoch 0061, iter [01400, 05004], lr: 0.049634, loss: 1.6949
2022-06-28 02:53:31 - train: epoch 0061, iter [01500, 05004], lr: 0.049608, loss: 1.8890
2022-06-28 02:54:04 - train: epoch 0061, iter [01600, 05004], lr: 0.049581, loss: 1.5591
2022-06-28 02:54:37 - train: epoch 0061, iter [01700, 05004], lr: 0.049555, loss: 1.7164
2022-06-28 02:55:10 - train: epoch 0061, iter [01800, 05004], lr: 0.049529, loss: 1.8456
2022-06-28 02:55:43 - train: epoch 0061, iter [01900, 05004], lr: 0.049503, loss: 1.9115
2022-06-28 02:56:17 - train: epoch 0061, iter [02000, 05004], lr: 0.049477, loss: 1.5946
2022-06-28 02:56:50 - train: epoch 0061, iter [02100, 05004], lr: 0.049451, loss: 2.1898
2022-06-28 02:57:23 - train: epoch 0061, iter [02200, 05004], lr: 0.049425, loss: 1.5644
2022-06-28 02:57:56 - train: epoch 0061, iter [02300, 05004], lr: 0.049398, loss: 1.6171
2022-06-28 02:58:30 - train: epoch 0061, iter [02400, 05004], lr: 0.049372, loss: 1.5248
2022-06-28 02:59:04 - train: epoch 0061, iter [02500, 05004], lr: 0.049346, loss: 1.6377
2022-06-28 02:59:37 - train: epoch 0061, iter [02600, 05004], lr: 0.049320, loss: 1.7854
2022-06-28 03:00:11 - train: epoch 0061, iter [02700, 05004], lr: 0.049294, loss: 1.8062
2022-06-28 03:00:44 - train: epoch 0061, iter [02800, 05004], lr: 0.049268, loss: 1.7210
2022-06-28 03:01:17 - train: epoch 0061, iter [02900, 05004], lr: 0.049241, loss: 1.7784
2022-06-28 03:01:51 - train: epoch 0061, iter [03000, 05004], lr: 0.049215, loss: 1.6317
2022-06-28 03:02:25 - train: epoch 0061, iter [03100, 05004], lr: 0.049189, loss: 1.8577
2022-06-28 03:02:58 - train: epoch 0061, iter [03200, 05004], lr: 0.049163, loss: 1.8007
2022-06-28 03:03:31 - train: epoch 0061, iter [03300, 05004], lr: 0.049137, loss: 1.6415
2022-06-28 03:04:04 - train: epoch 0061, iter [03400, 05004], lr: 0.049111, loss: 1.5061
2022-06-28 03:04:37 - train: epoch 0061, iter [03500, 05004], lr: 0.049084, loss: 1.8138
2022-06-28 03:05:11 - train: epoch 0061, iter [03600, 05004], lr: 0.049058, loss: 1.6746
2022-06-28 03:05:44 - train: epoch 0061, iter [03700, 05004], lr: 0.049032, loss: 1.8419
2022-06-28 03:06:17 - train: epoch 0061, iter [03800, 05004], lr: 0.049006, loss: 1.7179
2022-06-28 03:06:50 - train: epoch 0061, iter [03900, 05004], lr: 0.048980, loss: 1.7286
2022-06-28 03:07:23 - train: epoch 0061, iter [04000, 05004], lr: 0.048954, loss: 1.7380
2022-06-28 03:07:56 - train: epoch 0061, iter [04100, 05004], lr: 0.048928, loss: 1.9629
2022-06-28 03:08:29 - train: epoch 0061, iter [04200, 05004], lr: 0.048901, loss: 1.5622
2022-06-28 03:09:03 - train: epoch 0061, iter [04300, 05004], lr: 0.048875, loss: 1.8142
2022-06-28 03:09:36 - train: epoch 0061, iter [04400, 05004], lr: 0.048849, loss: 1.7212
2022-06-28 03:10:09 - train: epoch 0061, iter [04500, 05004], lr: 0.048823, loss: 1.8072
2022-06-28 03:10:42 - train: epoch 0061, iter [04600, 05004], lr: 0.048797, loss: 1.8594
2022-06-28 03:11:15 - train: epoch 0061, iter [04700, 05004], lr: 0.048771, loss: 1.6159
2022-06-28 03:11:48 - train: epoch 0061, iter [04800, 05004], lr: 0.048744, loss: 1.7045
2022-06-28 03:12:22 - train: epoch 0061, iter [04900, 05004], lr: 0.048718, loss: 1.6915
2022-06-28 03:12:54 - train: epoch 0061, iter [05000, 05004], lr: 0.048692, loss: 1.9012
2022-06-28 03:12:55 - train: epoch 061, train_loss: 1.7223
2022-06-28 03:14:10 - eval: epoch: 061, acc1: 64.120%, acc5: 86.256%, test_loss: 1.4870, per_image_load_time: 2.475ms, per_image_inference_time: 0.389ms
2022-06-28 03:14:10 - until epoch: 061, best_acc1: 64.120%
2022-06-28 03:14:10 - epoch 062 lr: 0.048691
2022-06-28 03:14:49 - train: epoch 0062, iter [00100, 05004], lr: 0.048665, loss: 1.6938
2022-06-28 03:15:21 - train: epoch 0062, iter [00200, 05004], lr: 0.048639, loss: 1.7751
2022-06-28 03:15:55 - train: epoch 0062, iter [00300, 05004], lr: 0.048613, loss: 1.6084
2022-06-28 03:16:28 - train: epoch 0062, iter [00400, 05004], lr: 0.048587, loss: 1.6485
2022-06-28 03:17:01 - train: epoch 0062, iter [00500, 05004], lr: 0.048560, loss: 1.6655
2022-06-28 03:17:34 - train: epoch 0062, iter [00600, 05004], lr: 0.048534, loss: 1.6441
2022-06-28 03:18:07 - train: epoch 0062, iter [00700, 05004], lr: 0.048508, loss: 1.9086
2022-06-28 03:18:41 - train: epoch 0062, iter [00800, 05004], lr: 0.048482, loss: 1.6069
2022-06-28 03:19:13 - train: epoch 0062, iter [00900, 05004], lr: 0.048456, loss: 1.6669
2022-06-28 03:19:47 - train: epoch 0062, iter [01000, 05004], lr: 0.048430, loss: 1.8682
2022-06-28 03:20:20 - train: epoch 0062, iter [01100, 05004], lr: 0.048404, loss: 1.5923
2022-06-28 03:20:55 - train: epoch 0062, iter [01200, 05004], lr: 0.048377, loss: 2.0827
2022-06-28 03:21:28 - train: epoch 0062, iter [01300, 05004], lr: 0.048351, loss: 1.8130
2022-06-28 03:22:01 - train: epoch 0062, iter [01400, 05004], lr: 0.048325, loss: 1.6883
2022-06-28 03:22:34 - train: epoch 0062, iter [01500, 05004], lr: 0.048299, loss: 1.7970
2022-06-28 03:23:07 - train: epoch 0062, iter [01600, 05004], lr: 0.048273, loss: 1.6750
2022-06-28 03:23:40 - train: epoch 0062, iter [01700, 05004], lr: 0.048247, loss: 1.7716
2022-06-28 03:24:14 - train: epoch 0062, iter [01800, 05004], lr: 0.048221, loss: 1.5660
2022-06-28 03:24:47 - train: epoch 0062, iter [01900, 05004], lr: 0.048194, loss: 1.6800
2022-06-28 03:25:21 - train: epoch 0062, iter [02000, 05004], lr: 0.048168, loss: 1.5687
2022-06-28 03:25:54 - train: epoch 0062, iter [02100, 05004], lr: 0.048142, loss: 1.7914
2022-06-28 03:26:27 - train: epoch 0062, iter [02200, 05004], lr: 0.048116, loss: 1.6396
2022-06-28 03:27:01 - train: epoch 0062, iter [02300, 05004], lr: 0.048090, loss: 1.7052
2022-06-28 03:27:35 - train: epoch 0062, iter [02400, 05004], lr: 0.048064, loss: 1.7293
2022-06-28 03:28:08 - train: epoch 0062, iter [02500, 05004], lr: 0.048038, loss: 1.8494
2022-06-28 03:28:42 - train: epoch 0062, iter [02600, 05004], lr: 0.048011, loss: 1.6771
2022-06-28 03:29:15 - train: epoch 0062, iter [02700, 05004], lr: 0.047985, loss: 1.7361
2022-06-28 03:29:48 - train: epoch 0062, iter [02800, 05004], lr: 0.047959, loss: 1.8707
2022-06-28 03:30:21 - train: epoch 0062, iter [02900, 05004], lr: 0.047933, loss: 1.8372
2022-06-28 03:30:55 - train: epoch 0062, iter [03000, 05004], lr: 0.047907, loss: 1.8296
2022-06-28 03:31:29 - train: epoch 0062, iter [03100, 05004], lr: 0.047881, loss: 1.7942
2022-06-28 03:32:02 - train: epoch 0062, iter [03200, 05004], lr: 0.047855, loss: 1.5909
2022-06-28 03:32:35 - train: epoch 0062, iter [03300, 05004], lr: 0.047828, loss: 1.8889
2022-06-28 03:33:08 - train: epoch 0062, iter [03400, 05004], lr: 0.047802, loss: 1.6836
2022-06-28 03:33:41 - train: epoch 0062, iter [03500, 05004], lr: 0.047776, loss: 1.6592
2022-06-28 03:34:15 - train: epoch 0062, iter [03600, 05004], lr: 0.047750, loss: 1.8095
2022-06-28 03:34:48 - train: epoch 0062, iter [03700, 05004], lr: 0.047724, loss: 1.6711
2022-06-28 03:35:21 - train: epoch 0062, iter [03800, 05004], lr: 0.047698, loss: 1.6788
2022-06-28 03:35:55 - train: epoch 0062, iter [03900, 05004], lr: 0.047672, loss: 1.7561
2022-06-28 03:36:28 - train: epoch 0062, iter [04000, 05004], lr: 0.047646, loss: 1.5468
2022-06-28 03:37:01 - train: epoch 0062, iter [04100, 05004], lr: 0.047619, loss: 1.8045
2022-06-28 03:37:35 - train: epoch 0062, iter [04200, 05004], lr: 0.047593, loss: 1.5278
2022-06-28 03:38:08 - train: epoch 0062, iter [04300, 05004], lr: 0.047567, loss: 1.8624
2022-06-28 03:38:41 - train: epoch 0062, iter [04400, 05004], lr: 0.047541, loss: 1.6915
2022-06-28 03:39:14 - train: epoch 0062, iter [04500, 05004], lr: 0.047515, loss: 1.5247
2022-06-28 03:39:48 - train: epoch 0062, iter [04600, 05004], lr: 0.047489, loss: 1.3595
2022-06-28 03:40:22 - train: epoch 0062, iter [04700, 05004], lr: 0.047463, loss: 1.6733
2022-06-28 03:40:56 - train: epoch 0062, iter [04800, 05004], lr: 0.047436, loss: 1.6452
2022-06-28 03:41:28 - train: epoch 0062, iter [04900, 05004], lr: 0.047410, loss: 1.6636
2022-06-28 03:42:00 - train: epoch 0062, iter [05000, 05004], lr: 0.047384, loss: 1.7895
2022-06-28 03:42:01 - train: epoch 062, train_loss: 1.7103
2022-06-28 03:43:16 - eval: epoch: 062, acc1: 64.068%, acc5: 86.238%, test_loss: 1.4795, per_image_load_time: 2.486ms, per_image_inference_time: 0.398ms
2022-06-28 03:43:17 - until epoch: 062, best_acc1: 64.120%
2022-06-28 03:43:17 - epoch 063 lr: 0.047383
2022-06-28 03:43:55 - train: epoch 0063, iter [00100, 05004], lr: 0.047357, loss: 1.7208
2022-06-28 03:44:28 - train: epoch 0063, iter [00200, 05004], lr: 0.047331, loss: 1.5588
2022-06-28 03:45:02 - train: epoch 0063, iter [00300, 05004], lr: 0.047305, loss: 1.8121
2022-06-28 03:45:35 - train: epoch 0063, iter [00400, 05004], lr: 0.047279, loss: 1.6680
2022-06-28 03:46:08 - train: epoch 0063, iter [00500, 05004], lr: 0.047253, loss: 1.5673
2022-06-28 03:46:41 - train: epoch 0063, iter [00600, 05004], lr: 0.047226, loss: 1.8439
2022-06-28 03:47:14 - train: epoch 0063, iter [00700, 05004], lr: 0.047200, loss: 1.6745
2022-06-28 03:47:47 - train: epoch 0063, iter [00800, 05004], lr: 0.047174, loss: 1.6599
2022-06-28 03:48:21 - train: epoch 0063, iter [00900, 05004], lr: 0.047148, loss: 1.6765
2022-06-28 03:48:54 - train: epoch 0063, iter [01000, 05004], lr: 0.047122, loss: 1.8675
2022-06-28 03:49:26 - train: epoch 0063, iter [01100, 05004], lr: 0.047096, loss: 1.6871
2022-06-28 03:49:59 - train: epoch 0063, iter [01200, 05004], lr: 0.047070, loss: 1.6510
2022-06-28 03:50:33 - train: epoch 0063, iter [01300, 05004], lr: 0.047044, loss: 1.7060
2022-06-28 03:51:06 - train: epoch 0063, iter [01400, 05004], lr: 0.047018, loss: 2.0711
2022-06-28 03:51:39 - train: epoch 0063, iter [01500, 05004], lr: 0.046991, loss: 1.7493
2022-06-28 03:52:13 - train: epoch 0063, iter [01600, 05004], lr: 0.046965, loss: 1.5439
2022-06-28 03:52:46 - train: epoch 0063, iter [01700, 05004], lr: 0.046939, loss: 1.5774
2022-06-28 03:53:20 - train: epoch 0063, iter [01800, 05004], lr: 0.046913, loss: 1.9865
2022-06-28 03:53:52 - train: epoch 0063, iter [01900, 05004], lr: 0.046887, loss: 1.9153
2022-06-28 03:54:26 - train: epoch 0063, iter [02000, 05004], lr: 0.046861, loss: 1.5668
2022-06-28 03:54:59 - train: epoch 0063, iter [02100, 05004], lr: 0.046835, loss: 1.7049
2022-06-28 03:55:32 - train: epoch 0063, iter [02200, 05004], lr: 0.046809, loss: 2.1707
2022-06-28 03:56:05 - train: epoch 0063, iter [02300, 05004], lr: 0.046783, loss: 1.7927
2022-06-28 03:56:38 - train: epoch 0063, iter [02400, 05004], lr: 0.046756, loss: 1.8458
2022-06-28 03:57:13 - train: epoch 0063, iter [02500, 05004], lr: 0.046730, loss: 1.7473
2022-06-28 03:57:46 - train: epoch 0063, iter [02600, 05004], lr: 0.046704, loss: 1.6339
2022-06-28 03:58:19 - train: epoch 0063, iter [02700, 05004], lr: 0.046678, loss: 1.8194
2022-06-28 03:58:52 - train: epoch 0063, iter [02800, 05004], lr: 0.046652, loss: 1.6543
2022-06-28 03:59:25 - train: epoch 0063, iter [02900, 05004], lr: 0.046626, loss: 1.5828
2022-06-28 03:59:59 - train: epoch 0063, iter [03000, 05004], lr: 0.046600, loss: 1.7149
2022-06-28 04:00:31 - train: epoch 0063, iter [03100, 05004], lr: 0.046574, loss: 2.1658
2022-06-28 04:01:05 - train: epoch 0063, iter [03200, 05004], lr: 0.046548, loss: 1.7527
2022-06-28 04:01:38 - train: epoch 0063, iter [03300, 05004], lr: 0.046522, loss: 1.5888
2022-06-28 04:02:11 - train: epoch 0063, iter [03400, 05004], lr: 0.046495, loss: 1.4803
2022-06-28 04:02:45 - train: epoch 0063, iter [03500, 05004], lr: 0.046469, loss: 1.9187
2022-06-28 04:03:18 - train: epoch 0063, iter [03600, 05004], lr: 0.046443, loss: 1.6415
2022-06-28 04:03:52 - train: epoch 0063, iter [03700, 05004], lr: 0.046417, loss: 1.8202
2022-06-28 04:04:25 - train: epoch 0063, iter [03800, 05004], lr: 0.046391, loss: 1.9830
2022-06-28 04:04:58 - train: epoch 0063, iter [03900, 05004], lr: 0.046365, loss: 1.3907
2022-06-28 04:05:31 - train: epoch 0063, iter [04000, 05004], lr: 0.046339, loss: 1.4228
2022-06-28 04:06:05 - train: epoch 0063, iter [04100, 05004], lr: 0.046313, loss: 1.8295
2022-06-28 04:06:37 - train: epoch 0063, iter [04200, 05004], lr: 0.046287, loss: 1.6332
2022-06-28 04:07:11 - train: epoch 0063, iter [04300, 05004], lr: 0.046261, loss: 1.8773
2022-06-28 04:07:44 - train: epoch 0063, iter [04400, 05004], lr: 0.046235, loss: 1.6563
2022-06-28 04:08:18 - train: epoch 0063, iter [04500, 05004], lr: 0.046208, loss: 1.6696
2022-06-28 04:08:52 - train: epoch 0063, iter [04600, 05004], lr: 0.046182, loss: 1.5959
2022-06-28 04:09:24 - train: epoch 0063, iter [04700, 05004], lr: 0.046156, loss: 1.5646
2022-06-28 04:09:58 - train: epoch 0063, iter [04800, 05004], lr: 0.046130, loss: 1.6668
2022-06-28 04:10:31 - train: epoch 0063, iter [04900, 05004], lr: 0.046104, loss: 1.6141
2022-06-28 04:11:03 - train: epoch 0063, iter [05000, 05004], lr: 0.046078, loss: 1.7756
2022-06-28 04:11:04 - train: epoch 063, train_loss: 1.7003
2022-06-28 04:12:19 - eval: epoch: 063, acc1: 64.510%, acc5: 86.294%, test_loss: 1.4706, per_image_load_time: 2.449ms, per_image_inference_time: 0.392ms
2022-06-28 04:12:19 - until epoch: 063, best_acc1: 64.510%
2022-06-28 04:12:19 - epoch 064 lr: 0.046077
2022-06-28 04:12:57 - train: epoch 0064, iter [00100, 05004], lr: 0.046051, loss: 1.6165
2022-06-28 04:13:31 - train: epoch 0064, iter [00200, 05004], lr: 0.046025, loss: 1.6157
2022-06-28 04:14:04 - train: epoch 0064, iter [00300, 05004], lr: 0.045999, loss: 1.4357
2022-06-28 04:14:37 - train: epoch 0064, iter [00400, 05004], lr: 0.045973, loss: 1.6767
2022-06-28 04:15:11 - train: epoch 0064, iter [00500, 05004], lr: 0.045947, loss: 1.5186
2022-06-28 04:15:44 - train: epoch 0064, iter [00600, 05004], lr: 0.045921, loss: 1.7931
2022-06-28 04:16:17 - train: epoch 0064, iter [00700, 05004], lr: 0.045895, loss: 1.7603
2022-06-28 04:16:51 - train: epoch 0064, iter [00800, 05004], lr: 0.045868, loss: 1.4602
2022-06-28 04:17:23 - train: epoch 0064, iter [00900, 05004], lr: 0.045842, loss: 1.7644
2022-06-28 04:17:57 - train: epoch 0064, iter [01000, 05004], lr: 0.045816, loss: 1.5852
2022-06-28 04:18:30 - train: epoch 0064, iter [01100, 05004], lr: 0.045790, loss: 1.5921
2022-06-28 04:19:03 - train: epoch 0064, iter [01200, 05004], lr: 0.045764, loss: 1.4801
2022-06-28 04:19:37 - train: epoch 0064, iter [01300, 05004], lr: 0.045738, loss: 1.8239
2022-06-28 04:20:09 - train: epoch 0064, iter [01400, 05004], lr: 0.045712, loss: 2.0515
2022-06-28 04:20:43 - train: epoch 0064, iter [01500, 05004], lr: 0.045686, loss: 1.7882
2022-06-28 04:21:16 - train: epoch 0064, iter [01600, 05004], lr: 0.045660, loss: 1.6497
2022-06-28 04:21:50 - train: epoch 0064, iter [01700, 05004], lr: 0.045634, loss: 1.4984
2022-06-28 04:22:23 - train: epoch 0064, iter [01800, 05004], lr: 0.045608, loss: 1.5406
2022-06-28 04:22:56 - train: epoch 0064, iter [01900, 05004], lr: 0.045582, loss: 1.6903
2022-06-28 04:23:30 - train: epoch 0064, iter [02000, 05004], lr: 0.045556, loss: 1.5672
2022-06-28 04:24:03 - train: epoch 0064, iter [02100, 05004], lr: 0.045530, loss: 1.6906
2022-06-28 04:24:36 - train: epoch 0064, iter [02200, 05004], lr: 0.045504, loss: 1.7405
2022-06-28 04:25:09 - train: epoch 0064, iter [02300, 05004], lr: 0.045478, loss: 1.7332
2022-06-28 04:25:42 - train: epoch 0064, iter [02400, 05004], lr: 0.045451, loss: 1.6848
2022-06-28 04:26:16 - train: epoch 0064, iter [02500, 05004], lr: 0.045425, loss: 1.5844
2022-06-28 04:26:49 - train: epoch 0064, iter [02600, 05004], lr: 0.045399, loss: 1.6512
2022-06-28 04:27:22 - train: epoch 0064, iter [02700, 05004], lr: 0.045373, loss: 1.6852
2022-06-28 04:27:56 - train: epoch 0064, iter [02800, 05004], lr: 0.045347, loss: 1.5126
2022-06-28 04:28:29 - train: epoch 0064, iter [02900, 05004], lr: 0.045321, loss: 1.9655
2022-06-28 04:29:02 - train: epoch 0064, iter [03000, 05004], lr: 0.045295, loss: 1.7567
2022-06-28 04:29:35 - train: epoch 0064, iter [03100, 05004], lr: 0.045269, loss: 1.6687
2022-06-28 04:30:09 - train: epoch 0064, iter [03200, 05004], lr: 0.045243, loss: 1.6471
2022-06-28 04:30:42 - train: epoch 0064, iter [03300, 05004], lr: 0.045217, loss: 1.6442
2022-06-28 04:31:15 - train: epoch 0064, iter [03400, 05004], lr: 0.045191, loss: 1.8401
2022-06-28 04:31:49 - train: epoch 0064, iter [03500, 05004], lr: 0.045165, loss: 1.6020
2022-06-28 04:32:22 - train: epoch 0064, iter [03600, 05004], lr: 0.045139, loss: 1.6139
2022-06-28 04:32:56 - train: epoch 0064, iter [03700, 05004], lr: 0.045113, loss: 1.3547
2022-06-28 04:33:29 - train: epoch 0064, iter [03800, 05004], lr: 0.045087, loss: 1.8865
2022-06-28 04:34:02 - train: epoch 0064, iter [03900, 05004], lr: 0.045061, loss: 1.6060
2022-06-28 04:34:36 - train: epoch 0064, iter [04000, 05004], lr: 0.045035, loss: 1.5930
2022-06-28 04:35:09 - train: epoch 0064, iter [04100, 05004], lr: 0.045009, loss: 1.7684
2022-06-28 04:35:42 - train: epoch 0064, iter [04200, 05004], lr: 0.044983, loss: 1.4694
2022-06-28 04:36:15 - train: epoch 0064, iter [04300, 05004], lr: 0.044957, loss: 1.8604
2022-06-28 04:36:49 - train: epoch 0064, iter [04400, 05004], lr: 0.044931, loss: 1.6910
2022-06-28 04:37:23 - train: epoch 0064, iter [04500, 05004], lr: 0.044905, loss: 1.6306
2022-06-28 04:37:55 - train: epoch 0064, iter [04600, 05004], lr: 0.044879, loss: 1.8688
2022-06-28 04:38:29 - train: epoch 0064, iter [04700, 05004], lr: 0.044853, loss: 2.0275
2022-06-28 04:39:03 - train: epoch 0064, iter [04800, 05004], lr: 0.044827, loss: 1.7563
2022-06-28 04:39:36 - train: epoch 0064, iter [04900, 05004], lr: 0.044801, loss: 1.8416
2022-06-28 04:40:08 - train: epoch 0064, iter [05000, 05004], lr: 0.044775, loss: 1.5490
2022-06-28 04:40:09 - train: epoch 064, train_loss: 1.6902
2022-06-28 04:41:24 - eval: epoch: 064, acc1: 65.058%, acc5: 86.840%, test_loss: 1.4420, per_image_load_time: 2.464ms, per_image_inference_time: 0.421ms
2022-06-28 04:41:24 - until epoch: 064, best_acc1: 65.058%
2022-06-28 04:41:24 - epoch 065 lr: 0.044773
2022-06-28 04:42:03 - train: epoch 0065, iter [00100, 05004], lr: 0.044748, loss: 1.7224
2022-06-28 04:42:36 - train: epoch 0065, iter [00200, 05004], lr: 0.044722, loss: 1.6278
2022-06-28 04:43:10 - train: epoch 0065, iter [00300, 05004], lr: 0.044696, loss: 1.6415
2022-06-28 04:43:43 - train: epoch 0065, iter [00400, 05004], lr: 0.044670, loss: 1.8194
2022-06-28 04:44:16 - train: epoch 0065, iter [00500, 05004], lr: 0.044644, loss: 1.6202
2022-06-28 04:44:50 - train: epoch 0065, iter [00600, 05004], lr: 0.044618, loss: 1.7004
2022-06-28 04:45:23 - train: epoch 0065, iter [00700, 05004], lr: 0.044592, loss: 1.6636
2022-06-28 04:45:56 - train: epoch 0065, iter [00800, 05004], lr: 0.044565, loss: 1.5861
2022-06-28 04:46:30 - train: epoch 0065, iter [00900, 05004], lr: 0.044539, loss: 1.5877
2022-06-28 04:47:02 - train: epoch 0065, iter [01000, 05004], lr: 0.044513, loss: 1.5864
2022-06-28 04:47:35 - train: epoch 0065, iter [01100, 05004], lr: 0.044487, loss: 1.6122
2022-06-28 04:48:09 - train: epoch 0065, iter [01200, 05004], lr: 0.044461, loss: 1.9066
2022-06-28 04:48:42 - train: epoch 0065, iter [01300, 05004], lr: 0.044435, loss: 1.6234
2022-06-28 04:49:14 - train: epoch 0065, iter [01400, 05004], lr: 0.044410, loss: 1.4722
2022-06-28 04:49:47 - train: epoch 0065, iter [01500, 05004], lr: 0.044384, loss: 1.7678
2022-06-28 04:50:21 - train: epoch 0065, iter [01600, 05004], lr: 0.044358, loss: 1.6937
2022-06-28 04:50:53 - train: epoch 0065, iter [01700, 05004], lr: 0.044332, loss: 1.6033
2022-06-28 04:51:26 - train: epoch 0065, iter [01800, 05004], lr: 0.044306, loss: 1.5334
2022-06-28 04:52:00 - train: epoch 0065, iter [01900, 05004], lr: 0.044280, loss: 1.4966
2022-06-28 04:52:33 - train: epoch 0065, iter [02000, 05004], lr: 0.044254, loss: 1.6216
2022-06-28 04:53:06 - train: epoch 0065, iter [02100, 05004], lr: 0.044228, loss: 1.6064
2022-06-28 04:53:39 - train: epoch 0065, iter [02200, 05004], lr: 0.044202, loss: 1.7645
2022-06-28 04:54:12 - train: epoch 0065, iter [02300, 05004], lr: 0.044176, loss: 1.5542
2022-06-28 04:54:45 - train: epoch 0065, iter [02400, 05004], lr: 0.044150, loss: 1.5649
2022-06-28 04:55:18 - train: epoch 0065, iter [02500, 05004], lr: 0.044124, loss: 1.6208
2022-06-28 04:55:52 - train: epoch 0065, iter [02600, 05004], lr: 0.044098, loss: 1.8653
2022-06-28 04:56:25 - train: epoch 0065, iter [02700, 05004], lr: 0.044072, loss: 1.7799
2022-06-28 04:56:58 - train: epoch 0065, iter [02800, 05004], lr: 0.044046, loss: 1.5998
2022-06-28 04:57:31 - train: epoch 0065, iter [02900, 05004], lr: 0.044020, loss: 1.7816
2022-06-28 04:58:03 - train: epoch 0065, iter [03000, 05004], lr: 0.043994, loss: 1.5132
2022-06-28 04:58:36 - train: epoch 0065, iter [03100, 05004], lr: 0.043968, loss: 1.7985
2022-06-28 04:59:10 - train: epoch 0065, iter [03200, 05004], lr: 0.043942, loss: 1.7724
2022-06-28 04:59:42 - train: epoch 0065, iter [03300, 05004], lr: 0.043916, loss: 1.6751
2022-06-28 05:00:16 - train: epoch 0065, iter [03400, 05004], lr: 0.043890, loss: 1.5030
2022-06-28 05:00:50 - train: epoch 0065, iter [03500, 05004], lr: 0.043864, loss: 1.9614
2022-06-28 05:01:22 - train: epoch 0065, iter [03600, 05004], lr: 0.043838, loss: 1.7867
2022-06-28 05:01:55 - train: epoch 0065, iter [03700, 05004], lr: 0.043812, loss: 1.6135
2022-06-28 05:02:28 - train: epoch 0065, iter [03800, 05004], lr: 0.043786, loss: 1.4992
2022-06-28 05:03:02 - train: epoch 0065, iter [03900, 05004], lr: 0.043760, loss: 1.8379
2022-06-28 05:03:35 - train: epoch 0065, iter [04000, 05004], lr: 0.043734, loss: 1.7081
2022-06-28 05:04:08 - train: epoch 0065, iter [04100, 05004], lr: 0.043708, loss: 1.4867
2022-06-28 05:04:41 - train: epoch 0065, iter [04200, 05004], lr: 0.043682, loss: 1.6907
2022-06-28 05:05:15 - train: epoch 0065, iter [04300, 05004], lr: 0.043656, loss: 1.6761
2022-06-28 05:05:49 - train: epoch 0065, iter [04400, 05004], lr: 0.043630, loss: 1.7334
2022-06-28 05:06:23 - train: epoch 0065, iter [04500, 05004], lr: 0.043604, loss: 1.7640
2022-06-28 05:06:57 - train: epoch 0065, iter [04600, 05004], lr: 0.043578, loss: 1.7600
2022-06-28 05:07:32 - train: epoch 0065, iter [04700, 05004], lr: 0.043553, loss: 1.7923
2022-06-28 05:08:05 - train: epoch 0065, iter [04800, 05004], lr: 0.043527, loss: 1.4072
2022-06-28 05:08:40 - train: epoch 0065, iter [04900, 05004], lr: 0.043501, loss: 1.6408
2022-06-28 05:09:13 - train: epoch 0065, iter [05000, 05004], lr: 0.043475, loss: 1.7388
2022-06-28 05:09:14 - train: epoch 065, train_loss: 1.6785
2022-06-28 05:10:31 - eval: epoch: 065, acc1: 65.082%, acc5: 86.578%, test_loss: 1.4467, per_image_load_time: 2.543ms, per_image_inference_time: 0.420ms
2022-06-28 05:10:31 - until epoch: 065, best_acc1: 65.082%
2022-06-28 05:10:31 - epoch 066 lr: 0.043473
2022-06-28 05:11:10 - train: epoch 0066, iter [00100, 05004], lr: 0.043448, loss: 1.5114
2022-06-28 05:11:44 - train: epoch 0066, iter [00200, 05004], lr: 0.043422, loss: 1.7761
2022-06-28 05:12:19 - train: epoch 0066, iter [00300, 05004], lr: 0.043396, loss: 1.5892
2022-06-28 05:12:53 - train: epoch 0066, iter [00400, 05004], lr: 0.043370, loss: 1.4656
2022-06-28 05:13:27 - train: epoch 0066, iter [00500, 05004], lr: 0.043344, loss: 1.6879
2022-06-28 05:14:02 - train: epoch 0066, iter [00600, 05004], lr: 0.043318, loss: 1.6100
2022-06-28 05:14:36 - train: epoch 0066, iter [00700, 05004], lr: 0.043292, loss: 1.6184
2022-06-28 05:15:10 - train: epoch 0066, iter [00800, 05004], lr: 0.043266, loss: 2.0569
2022-06-28 05:15:45 - train: epoch 0066, iter [00900, 05004], lr: 0.043240, loss: 1.7723
2022-06-28 05:16:19 - train: epoch 0066, iter [01000, 05004], lr: 0.043214, loss: 1.7485
2022-06-28 05:16:54 - train: epoch 0066, iter [01100, 05004], lr: 0.043189, loss: 1.7182
2022-06-28 05:17:28 - train: epoch 0066, iter [01200, 05004], lr: 0.043163, loss: 1.8777
2022-06-28 05:18:02 - train: epoch 0066, iter [01300, 05004], lr: 0.043137, loss: 1.7968
2022-06-28 05:18:38 - train: epoch 0066, iter [01400, 05004], lr: 0.043111, loss: 1.5024
2022-06-28 05:19:12 - train: epoch 0066, iter [01500, 05004], lr: 0.043085, loss: 1.6697
2022-06-28 05:19:47 - train: epoch 0066, iter [01600, 05004], lr: 0.043059, loss: 1.6999
2022-06-28 05:20:21 - train: epoch 0066, iter [01700, 05004], lr: 0.043033, loss: 1.6181
2022-06-28 05:20:56 - train: epoch 0066, iter [01800, 05004], lr: 0.043007, loss: 1.5394
2022-06-28 05:21:31 - train: epoch 0066, iter [01900, 05004], lr: 0.042981, loss: 1.9082
2022-06-28 05:22:06 - train: epoch 0066, iter [02000, 05004], lr: 0.042955, loss: 1.7640
2022-06-28 05:22:40 - train: epoch 0066, iter [02100, 05004], lr: 0.042929, loss: 1.7203
2022-06-28 05:23:15 - train: epoch 0066, iter [02200, 05004], lr: 0.042904, loss: 1.5887
2022-06-28 05:23:50 - train: epoch 0066, iter [02300, 05004], lr: 0.042878, loss: 1.9019
2022-06-28 05:24:24 - train: epoch 0066, iter [02400, 05004], lr: 0.042852, loss: 1.5057
2022-06-28 05:24:58 - train: epoch 0066, iter [02500, 05004], lr: 0.042826, loss: 1.7295
2022-06-28 05:25:33 - train: epoch 0066, iter [02600, 05004], lr: 0.042800, loss: 1.6082
2022-06-28 05:26:08 - train: epoch 0066, iter [02700, 05004], lr: 0.042774, loss: 1.8869
2022-06-28 05:26:42 - train: epoch 0066, iter [02800, 05004], lr: 0.042748, loss: 1.6481
2022-06-28 05:27:17 - train: epoch 0066, iter [02900, 05004], lr: 0.042722, loss: 1.7539
2022-06-28 05:27:52 - train: epoch 0066, iter [03000, 05004], lr: 0.042696, loss: 1.5829
2022-06-28 05:28:27 - train: epoch 0066, iter [03100, 05004], lr: 0.042671, loss: 1.7234
2022-06-28 05:29:01 - train: epoch 0066, iter [03200, 05004], lr: 0.042645, loss: 1.4225
2022-06-28 05:29:35 - train: epoch 0066, iter [03300, 05004], lr: 0.042619, loss: 1.5725
2022-06-28 05:30:11 - train: epoch 0066, iter [03400, 05004], lr: 0.042593, loss: 1.9112
2022-06-28 05:30:45 - train: epoch 0066, iter [03500, 05004], lr: 0.042567, loss: 1.7509
2022-06-28 05:31:20 - train: epoch 0066, iter [03600, 05004], lr: 0.042541, loss: 1.7059
2022-06-28 05:31:54 - train: epoch 0066, iter [03700, 05004], lr: 0.042515, loss: 1.6138
2022-06-28 05:32:29 - train: epoch 0066, iter [03800, 05004], lr: 0.042490, loss: 1.5571
2022-06-28 05:33:03 - train: epoch 0066, iter [03900, 05004], lr: 0.042464, loss: 1.5036
2022-06-28 05:33:38 - train: epoch 0066, iter [04000, 05004], lr: 0.042438, loss: 1.8305
2022-06-28 05:34:12 - train: epoch 0066, iter [04100, 05004], lr: 0.042412, loss: 1.4424
2022-06-28 05:34:47 - train: epoch 0066, iter [04200, 05004], lr: 0.042386, loss: 1.4925
2022-06-28 05:35:22 - train: epoch 0066, iter [04300, 05004], lr: 0.042360, loss: 1.4933
2022-06-28 05:35:56 - train: epoch 0066, iter [04400, 05004], lr: 0.042334, loss: 1.5666
2022-06-28 05:36:31 - train: epoch 0066, iter [04500, 05004], lr: 0.042309, loss: 1.7860
2022-06-28 05:37:05 - train: epoch 0066, iter [04600, 05004], lr: 0.042283, loss: 1.9074
2022-06-28 05:37:40 - train: epoch 0066, iter [04700, 05004], lr: 0.042257, loss: 1.6231
2022-06-28 05:38:15 - train: epoch 0066, iter [04800, 05004], lr: 0.042231, loss: 1.5558
2022-06-28 05:38:50 - train: epoch 0066, iter [04900, 05004], lr: 0.042205, loss: 1.7301
2022-06-28 05:39:23 - train: epoch 0066, iter [05000, 05004], lr: 0.042179, loss: 1.6059
2022-06-28 05:39:24 - train: epoch 066, train_loss: 1.6671
2022-06-28 05:40:40 - eval: epoch: 066, acc1: 64.784%, acc5: 86.918%, test_loss: 1.4446, per_image_load_time: 2.532ms, per_image_inference_time: 0.386ms
2022-06-28 05:40:40 - until epoch: 066, best_acc1: 65.082%
2022-06-28 05:40:40 - epoch 067 lr: 0.042178
2022-06-28 05:41:20 - train: epoch 0067, iter [00100, 05004], lr: 0.042152, loss: 1.5621
2022-06-28 05:41:56 - train: epoch 0067, iter [00200, 05004], lr: 0.042127, loss: 1.6183
2022-06-28 05:42:29 - train: epoch 0067, iter [00300, 05004], lr: 0.042101, loss: 1.8670
2022-06-28 05:43:04 - train: epoch 0067, iter [00400, 05004], lr: 0.042075, loss: 1.6423
2022-06-28 05:43:38 - train: epoch 0067, iter [00500, 05004], lr: 0.042049, loss: 1.4318
2022-06-28 05:44:12 - train: epoch 0067, iter [00600, 05004], lr: 0.042023, loss: 1.5885
2022-06-28 05:44:46 - train: epoch 0067, iter [00700, 05004], lr: 0.041997, loss: 1.8093
2022-06-28 05:45:21 - train: epoch 0067, iter [00800, 05004], lr: 0.041972, loss: 1.6427
2022-06-28 05:45:55 - train: epoch 0067, iter [00900, 05004], lr: 0.041946, loss: 1.7444
2022-06-28 05:46:30 - train: epoch 0067, iter [01000, 05004], lr: 0.041920, loss: 1.5669
2022-06-28 05:47:04 - train: epoch 0067, iter [01100, 05004], lr: 0.041894, loss: 1.6306
2022-06-28 05:47:38 - train: epoch 0067, iter [01200, 05004], lr: 0.041868, loss: 1.6528
2022-06-28 05:48:13 - train: epoch 0067, iter [01300, 05004], lr: 0.041843, loss: 1.8566
2022-06-28 05:48:48 - train: epoch 0067, iter [01400, 05004], lr: 0.041817, loss: 1.6363
2022-06-28 05:49:22 - train: epoch 0067, iter [01500, 05004], lr: 0.041791, loss: 1.6041
2022-06-28 05:49:56 - train: epoch 0067, iter [01600, 05004], lr: 0.041765, loss: 1.6491
2022-06-28 05:50:31 - train: epoch 0067, iter [01700, 05004], lr: 0.041739, loss: 1.5453
2022-06-28 05:51:05 - train: epoch 0067, iter [01800, 05004], lr: 0.041714, loss: 1.8575
2022-06-28 05:51:40 - train: epoch 0067, iter [01900, 05004], lr: 0.041688, loss: 1.6063
2022-06-28 05:52:14 - train: epoch 0067, iter [02000, 05004], lr: 0.041662, loss: 1.6727
2022-06-28 05:52:49 - train: epoch 0067, iter [02100, 05004], lr: 0.041636, loss: 1.4663
2022-06-28 05:53:24 - train: epoch 0067, iter [02200, 05004], lr: 0.041610, loss: 1.6611
2022-06-28 05:53:59 - train: epoch 0067, iter [02300, 05004], lr: 0.041585, loss: 1.5903
2022-06-28 05:54:34 - train: epoch 0067, iter [02400, 05004], lr: 0.041559, loss: 1.6075
2022-06-28 05:55:07 - train: epoch 0067, iter [02500, 05004], lr: 0.041533, loss: 1.6147
2022-06-28 05:55:42 - train: epoch 0067, iter [02600, 05004], lr: 0.041507, loss: 1.5480
2022-06-28 05:56:17 - train: epoch 0067, iter [02700, 05004], lr: 0.041481, loss: 1.5800
2022-06-28 05:56:51 - train: epoch 0067, iter [02800, 05004], lr: 0.041456, loss: 1.7903
2022-06-28 05:57:26 - train: epoch 0067, iter [02900, 05004], lr: 0.041430, loss: 1.6456
2022-06-28 05:58:00 - train: epoch 0067, iter [03000, 05004], lr: 0.041404, loss: 1.5138
2022-06-28 05:58:35 - train: epoch 0067, iter [03100, 05004], lr: 0.041378, loss: 1.4977
2022-06-28 05:59:09 - train: epoch 0067, iter [03200, 05004], lr: 0.041353, loss: 1.7324
2022-06-28 05:59:44 - train: epoch 0067, iter [03300, 05004], lr: 0.041327, loss: 1.4891
2022-06-28 06:00:19 - train: epoch 0067, iter [03400, 05004], lr: 0.041301, loss: 1.5668
2022-06-28 06:00:53 - train: epoch 0067, iter [03500, 05004], lr: 0.041275, loss: 1.5640
2022-06-28 06:01:29 - train: epoch 0067, iter [03600, 05004], lr: 0.041250, loss: 1.7945
2022-06-28 06:02:03 - train: epoch 0067, iter [03700, 05004], lr: 0.041224, loss: 1.8182
2022-06-28 06:02:37 - train: epoch 0067, iter [03800, 05004], lr: 0.041198, loss: 1.5543
2022-06-28 06:03:12 - train: epoch 0067, iter [03900, 05004], lr: 0.041172, loss: 1.7598
2022-06-28 06:03:47 - train: epoch 0067, iter [04000, 05004], lr: 0.041147, loss: 1.7697
2022-06-28 06:04:21 - train: epoch 0067, iter [04100, 05004], lr: 0.041121, loss: 1.7692
2022-06-28 06:04:56 - train: epoch 0067, iter [04200, 05004], lr: 0.041095, loss: 1.9050
2022-06-28 06:05:30 - train: epoch 0067, iter [04300, 05004], lr: 0.041069, loss: 1.4320
2022-06-28 06:06:06 - train: epoch 0067, iter [04400, 05004], lr: 0.041044, loss: 1.7684
2022-06-28 06:06:40 - train: epoch 0067, iter [04500, 05004], lr: 0.041018, loss: 1.6387
2022-06-28 06:07:15 - train: epoch 0067, iter [04600, 05004], lr: 0.040992, loss: 1.3319
2022-06-28 06:07:50 - train: epoch 0067, iter [04700, 05004], lr: 0.040966, loss: 1.6134
2022-06-28 06:08:25 - train: epoch 0067, iter [04800, 05004], lr: 0.040941, loss: 1.3860
2022-06-28 06:09:00 - train: epoch 0067, iter [04900, 05004], lr: 0.040915, loss: 1.6611
2022-06-28 06:09:32 - train: epoch 0067, iter [05000, 05004], lr: 0.040889, loss: 1.8224
2022-06-28 06:09:34 - train: epoch 067, train_loss: 1.6540
2022-06-28 06:10:50 - eval: epoch: 067, acc1: 65.418%, acc5: 87.048%, test_loss: 1.4197, per_image_load_time: 2.547ms, per_image_inference_time: 0.421ms
2022-06-28 06:10:51 - until epoch: 067, best_acc1: 65.418%
2022-06-28 06:10:51 - epoch 068 lr: 0.040888
2022-06-28 06:11:30 - train: epoch 0068, iter [00100, 05004], lr: 0.040863, loss: 1.6886
2022-06-28 06:12:04 - train: epoch 0068, iter [00200, 05004], lr: 0.040837, loss: 1.6877
2022-06-28 06:12:39 - train: epoch 0068, iter [00300, 05004], lr: 0.040811, loss: 1.7066
2022-06-28 06:13:13 - train: epoch 0068, iter [00400, 05004], lr: 0.040785, loss: 1.5104
2022-06-28 06:13:47 - train: epoch 0068, iter [00500, 05004], lr: 0.040760, loss: 1.5127
2022-06-28 06:14:22 - train: epoch 0068, iter [00600, 05004], lr: 0.040734, loss: 1.6308
2022-06-28 06:14:56 - train: epoch 0068, iter [00700, 05004], lr: 0.040708, loss: 1.9278
2022-06-28 06:15:30 - train: epoch 0068, iter [00800, 05004], lr: 0.040683, loss: 1.5359
2022-06-28 06:16:04 - train: epoch 0068, iter [00900, 05004], lr: 0.040657, loss: 1.4992
2022-06-28 06:16:39 - train: epoch 0068, iter [01000, 05004], lr: 0.040631, loss: 1.6561
2022-06-28 06:17:13 - train: epoch 0068, iter [01100, 05004], lr: 0.040605, loss: 1.9260
2022-06-28 06:17:48 - train: epoch 0068, iter [01200, 05004], lr: 0.040580, loss: 1.6592
2022-06-28 06:18:23 - train: epoch 0068, iter [01300, 05004], lr: 0.040554, loss: 1.4768
2022-06-28 06:18:58 - train: epoch 0068, iter [01400, 05004], lr: 0.040528, loss: 1.6343
2022-06-28 06:19:31 - train: epoch 0068, iter [01500, 05004], lr: 0.040503, loss: 1.7529
2022-06-28 06:20:07 - train: epoch 0068, iter [01600, 05004], lr: 0.040477, loss: 1.7226
2022-06-28 06:20:41 - train: epoch 0068, iter [01700, 05004], lr: 0.040451, loss: 1.7861
2022-06-28 06:21:16 - train: epoch 0068, iter [01800, 05004], lr: 0.040426, loss: 1.8104
2022-06-28 06:21:50 - train: epoch 0068, iter [01900, 05004], lr: 0.040400, loss: 1.7201
2022-06-28 06:22:25 - train: epoch 0068, iter [02000, 05004], lr: 0.040374, loss: 1.5864
2022-06-28 06:22:59 - train: epoch 0068, iter [02100, 05004], lr: 0.040349, loss: 1.6837
2022-06-28 06:23:34 - train: epoch 0068, iter [02200, 05004], lr: 0.040323, loss: 1.7063
2022-06-28 06:24:08 - train: epoch 0068, iter [02300, 05004], lr: 0.040297, loss: 1.4876
2022-06-28 06:24:42 - train: epoch 0068, iter [02400, 05004], lr: 0.040272, loss: 1.6314
2022-06-28 06:25:17 - train: epoch 0068, iter [02500, 05004], lr: 0.040246, loss: 1.6193
2022-06-28 06:25:52 - train: epoch 0068, iter [02600, 05004], lr: 0.040220, loss: 1.5660
2022-06-28 06:26:27 - train: epoch 0068, iter [02700, 05004], lr: 0.040195, loss: 1.7512
2022-06-28 06:27:01 - train: epoch 0068, iter [02800, 05004], lr: 0.040169, loss: 1.6965
2022-06-28 06:27:35 - train: epoch 0068, iter [02900, 05004], lr: 0.040143, loss: 1.7906
2022-06-28 06:28:09 - train: epoch 0068, iter [03000, 05004], lr: 0.040118, loss: 1.8400
2022-06-28 06:28:44 - train: epoch 0068, iter [03100, 05004], lr: 0.040092, loss: 1.3924
2022-06-28 06:29:19 - train: epoch 0068, iter [03200, 05004], lr: 0.040066, loss: 1.9040
2022-06-28 06:29:54 - train: epoch 0068, iter [03300, 05004], lr: 0.040041, loss: 1.6628
2022-06-28 06:30:28 - train: epoch 0068, iter [03400, 05004], lr: 0.040015, loss: 1.4759
2022-06-28 06:31:02 - train: epoch 0068, iter [03500, 05004], lr: 0.039990, loss: 1.7598
2022-06-28 06:31:36 - train: epoch 0068, iter [03600, 05004], lr: 0.039964, loss: 1.5515
2022-06-28 06:32:12 - train: epoch 0068, iter [03700, 05004], lr: 0.039938, loss: 1.7485
2022-06-28 06:32:46 - train: epoch 0068, iter [03800, 05004], lr: 0.039913, loss: 1.7277
2022-06-28 06:33:21 - train: epoch 0068, iter [03900, 05004], lr: 0.039887, loss: 1.6851
2022-06-28 06:33:56 - train: epoch 0068, iter [04000, 05004], lr: 0.039861, loss: 1.6701
2022-06-28 06:34:31 - train: epoch 0068, iter [04100, 05004], lr: 0.039836, loss: 1.3886
2022-06-28 06:35:06 - train: epoch 0068, iter [04200, 05004], lr: 0.039810, loss: 1.8089
2022-06-28 06:35:39 - train: epoch 0068, iter [04300, 05004], lr: 0.039785, loss: 1.6894
2022-06-28 06:36:15 - train: epoch 0068, iter [04400, 05004], lr: 0.039759, loss: 1.5147
2022-06-28 06:36:49 - train: epoch 0068, iter [04500, 05004], lr: 0.039733, loss: 1.6629
2022-06-28 06:37:24 - train: epoch 0068, iter [04600, 05004], lr: 0.039708, loss: 1.7303
2022-06-28 06:37:58 - train: epoch 0068, iter [04700, 05004], lr: 0.039682, loss: 1.9130
2022-06-28 06:38:32 - train: epoch 0068, iter [04800, 05004], lr: 0.039657, loss: 1.8489
2022-06-28 06:39:08 - train: epoch 0068, iter [04900, 05004], lr: 0.039631, loss: 1.8402
2022-06-28 06:39:41 - train: epoch 0068, iter [05000, 05004], lr: 0.039605, loss: 1.6188
2022-06-28 06:39:42 - train: epoch 068, train_loss: 1.6440
2022-06-28 06:40:58 - eval: epoch: 068, acc1: 65.672%, acc5: 87.094%, test_loss: 1.4098, per_image_load_time: 2.509ms, per_image_inference_time: 0.412ms
2022-06-28 06:40:58 - until epoch: 068, best_acc1: 65.672%
2022-06-28 06:40:58 - epoch 069 lr: 0.039604
2022-06-28 06:41:38 - train: epoch 0069, iter [00100, 05004], lr: 0.039579, loss: 1.8940
2022-06-28 06:42:12 - train: epoch 0069, iter [00200, 05004], lr: 0.039553, loss: 1.9140
2022-06-28 06:42:47 - train: epoch 0069, iter [00300, 05004], lr: 0.039528, loss: 1.6470
2022-06-28 06:43:22 - train: epoch 0069, iter [00400, 05004], lr: 0.039502, loss: 1.5251
2022-06-28 06:43:56 - train: epoch 0069, iter [00500, 05004], lr: 0.039477, loss: 1.5126
2022-06-28 06:44:31 - train: epoch 0069, iter [00600, 05004], lr: 0.039451, loss: 1.4012
2022-06-28 06:45:06 - train: epoch 0069, iter [00700, 05004], lr: 0.039425, loss: 1.5775
2022-06-28 06:45:41 - train: epoch 0069, iter [00800, 05004], lr: 0.039400, loss: 1.6165
2022-06-28 06:46:15 - train: epoch 0069, iter [00900, 05004], lr: 0.039374, loss: 1.4794
2022-06-28 06:46:49 - train: epoch 0069, iter [01000, 05004], lr: 0.039349, loss: 1.4934
2022-06-28 06:47:24 - train: epoch 0069, iter [01100, 05004], lr: 0.039323, loss: 1.5999
2022-06-28 06:47:58 - train: epoch 0069, iter [01200, 05004], lr: 0.039298, loss: 1.5836
2022-06-28 06:48:34 - train: epoch 0069, iter [01300, 05004], lr: 0.039272, loss: 1.9085
2022-06-28 06:49:08 - train: epoch 0069, iter [01400, 05004], lr: 0.039246, loss: 1.6612
2022-06-28 06:49:42 - train: epoch 0069, iter [01500, 05004], lr: 0.039221, loss: 1.6233
2022-06-28 06:50:17 - train: epoch 0069, iter [01600, 05004], lr: 0.039195, loss: 1.6875
2022-06-28 06:50:52 - train: epoch 0069, iter [01700, 05004], lr: 0.039170, loss: 1.5505
2022-06-28 06:51:26 - train: epoch 0069, iter [01800, 05004], lr: 0.039144, loss: 1.4307
2022-06-28 06:52:01 - train: epoch 0069, iter [01900, 05004], lr: 0.039119, loss: 1.6688
2022-06-28 06:52:36 - train: epoch 0069, iter [02000, 05004], lr: 0.039093, loss: 1.5065
2022-06-28 06:53:10 - train: epoch 0069, iter [02100, 05004], lr: 0.039068, loss: 1.6444
2022-06-28 06:53:45 - train: epoch 0069, iter [02200, 05004], lr: 0.039042, loss: 1.7058
2022-06-28 06:54:19 - train: epoch 0069, iter [02300, 05004], lr: 0.039017, loss: 1.5846
2022-06-28 06:54:54 - train: epoch 0069, iter [02400, 05004], lr: 0.038991, loss: 1.7659
2022-06-28 06:55:28 - train: epoch 0069, iter [02500, 05004], lr: 0.038966, loss: 1.5595
2022-06-28 06:56:02 - train: epoch 0069, iter [02600, 05004], lr: 0.038940, loss: 1.7400
2022-06-28 06:56:37 - train: epoch 0069, iter [02700, 05004], lr: 0.038915, loss: 1.9017
2022-06-28 06:57:12 - train: epoch 0069, iter [02800, 05004], lr: 0.038889, loss: 1.6214
2022-06-28 06:57:46 - train: epoch 0069, iter [02900, 05004], lr: 0.038864, loss: 1.5251
2022-06-28 06:58:21 - train: epoch 0069, iter [03000, 05004], lr: 0.038838, loss: 1.5924
2022-06-28 06:58:56 - train: epoch 0069, iter [03100, 05004], lr: 0.038813, loss: 1.5673
2022-06-28 06:59:31 - train: epoch 0069, iter [03200, 05004], lr: 0.038787, loss: 1.4837
2022-06-28 07:00:05 - train: epoch 0069, iter [03300, 05004], lr: 0.038762, loss: 1.5369
2022-06-28 07:00:40 - train: epoch 0069, iter [03400, 05004], lr: 0.038736, loss: 1.4478
2022-06-28 07:01:15 - train: epoch 0069, iter [03500, 05004], lr: 0.038711, loss: 1.5705
2022-06-28 07:01:49 - train: epoch 0069, iter [03600, 05004], lr: 0.038685, loss: 1.5397
2022-06-28 07:02:24 - train: epoch 0069, iter [03700, 05004], lr: 0.038660, loss: 1.8111
2022-06-28 07:03:00 - train: epoch 0069, iter [03800, 05004], lr: 0.038634, loss: 1.6049
2022-06-28 07:03:33 - train: epoch 0069, iter [03900, 05004], lr: 0.038609, loss: 1.5908
2022-06-28 07:04:09 - train: epoch 0069, iter [04000, 05004], lr: 0.038583, loss: 1.7490
2022-06-28 07:04:43 - train: epoch 0069, iter [04100, 05004], lr: 0.038558, loss: 1.7483
2022-06-28 07:05:18 - train: epoch 0069, iter [04200, 05004], lr: 0.038532, loss: 1.4020
2022-06-28 07:05:52 - train: epoch 0069, iter [04300, 05004], lr: 0.038507, loss: 1.7194
2022-06-28 07:06:27 - train: epoch 0069, iter [04400, 05004], lr: 0.038481, loss: 1.6415
2022-06-28 07:07:02 - train: epoch 0069, iter [04500, 05004], lr: 0.038456, loss: 1.6799
2022-06-28 07:07:36 - train: epoch 0069, iter [04600, 05004], lr: 0.038431, loss: 1.9435
2022-06-28 07:08:10 - train: epoch 0069, iter [04700, 05004], lr: 0.038405, loss: 1.7000
2022-06-28 07:08:46 - train: epoch 0069, iter [04800, 05004], lr: 0.038380, loss: 1.6142
2022-06-28 07:09:21 - train: epoch 0069, iter [04900, 05004], lr: 0.038354, loss: 1.5064
2022-06-28 07:09:54 - train: epoch 0069, iter [05000, 05004], lr: 0.038329, loss: 1.7383
2022-06-28 07:09:55 - train: epoch 069, train_loss: 1.6293
2022-06-28 07:11:12 - eval: epoch: 069, acc1: 64.702%, acc5: 86.418%, test_loss: 1.4592, per_image_load_time: 2.524ms, per_image_inference_time: 0.420ms
2022-06-28 07:11:12 - until epoch: 069, best_acc1: 65.672%
2022-06-28 07:11:12 - epoch 070 lr: 0.038327
2022-06-28 07:11:51 - train: epoch 0070, iter [00100, 05004], lr: 0.038302, loss: 1.7167
2022-06-28 07:12:25 - train: epoch 0070, iter [00200, 05004], lr: 0.038277, loss: 1.6087
2022-06-28 07:13:00 - train: epoch 0070, iter [00300, 05004], lr: 0.038251, loss: 1.7508
2022-06-28 07:13:34 - train: epoch 0070, iter [00400, 05004], lr: 0.038226, loss: 1.6055
2022-06-28 07:14:09 - train: epoch 0070, iter [00500, 05004], lr: 0.038201, loss: 1.6836
2022-06-28 07:14:43 - train: epoch 0070, iter [00600, 05004], lr: 0.038175, loss: 1.6067
2022-06-28 07:15:17 - train: epoch 0070, iter [00700, 05004], lr: 0.038150, loss: 1.5813
2022-06-28 07:15:52 - train: epoch 0070, iter [00800, 05004], lr: 0.038124, loss: 1.5763
2022-06-28 07:16:26 - train: epoch 0070, iter [00900, 05004], lr: 0.038099, loss: 1.7387
2022-06-28 07:17:00 - train: epoch 0070, iter [01000, 05004], lr: 0.038074, loss: 1.5459
2022-06-28 07:17:35 - train: epoch 0070, iter [01100, 05004], lr: 0.038048, loss: 2.0489
2022-06-28 07:18:10 - train: epoch 0070, iter [01200, 05004], lr: 0.038023, loss: 1.3716
2022-06-28 07:18:45 - train: epoch 0070, iter [01300, 05004], lr: 0.037997, loss: 1.5443
2022-06-28 07:19:19 - train: epoch 0070, iter [01400, 05004], lr: 0.037972, loss: 1.6049
2022-06-28 07:19:53 - train: epoch 0070, iter [01500, 05004], lr: 0.037947, loss: 1.5182
2022-06-28 07:20:28 - train: epoch 0070, iter [01600, 05004], lr: 0.037921, loss: 1.6076
2022-06-28 07:21:03 - train: epoch 0070, iter [01700, 05004], lr: 0.037896, loss: 1.7045
2022-06-28 07:21:37 - train: epoch 0070, iter [01800, 05004], lr: 0.037870, loss: 1.5198
2022-06-28 07:22:12 - train: epoch 0070, iter [01900, 05004], lr: 0.037845, loss: 1.6319
2022-06-28 07:22:47 - train: epoch 0070, iter [02000, 05004], lr: 0.037820, loss: 1.6206
2022-06-28 07:23:22 - train: epoch 0070, iter [02100, 05004], lr: 0.037794, loss: 1.6382
2022-06-28 07:23:57 - train: epoch 0070, iter [02200, 05004], lr: 0.037769, loss: 1.6549
2022-06-28 07:24:31 - train: epoch 0070, iter [02300, 05004], lr: 0.037744, loss: 1.5930
2022-06-28 07:25:07 - train: epoch 0070, iter [02400, 05004], lr: 0.037718, loss: 1.6990
2022-06-28 07:25:41 - train: epoch 0070, iter [02500, 05004], lr: 0.037693, loss: 1.6890
2022-06-28 07:26:16 - train: epoch 0070, iter [02600, 05004], lr: 0.037667, loss: 1.5953
2022-06-28 07:26:50 - train: epoch 0070, iter [02700, 05004], lr: 0.037642, loss: 1.6805
2022-06-28 07:27:24 - train: epoch 0070, iter [02800, 05004], lr: 0.037617, loss: 1.6725
2022-06-28 07:27:59 - train: epoch 0070, iter [02900, 05004], lr: 0.037591, loss: 1.8110
2022-06-28 07:28:33 - train: epoch 0070, iter [03000, 05004], lr: 0.037566, loss: 1.5702
2022-06-28 07:29:08 - train: epoch 0070, iter [03100, 05004], lr: 0.037541, loss: 1.5849
2022-06-28 07:29:43 - train: epoch 0070, iter [03200, 05004], lr: 0.037515, loss: 1.8067
2022-06-28 07:30:17 - train: epoch 0070, iter [03300, 05004], lr: 0.037490, loss: 1.5851
2022-06-28 07:30:52 - train: epoch 0070, iter [03400, 05004], lr: 0.037465, loss: 1.7204
2022-06-28 07:31:27 - train: epoch 0070, iter [03500, 05004], lr: 0.037439, loss: 1.6972
2022-06-28 07:32:02 - train: epoch 0070, iter [03600, 05004], lr: 0.037414, loss: 1.6795
2022-06-28 07:32:37 - train: epoch 0070, iter [03700, 05004], lr: 0.037389, loss: 1.5956
2022-06-28 07:33:11 - train: epoch 0070, iter [03800, 05004], lr: 0.037364, loss: 1.6199
2022-06-28 07:33:46 - train: epoch 0070, iter [03900, 05004], lr: 0.037338, loss: 1.4634
2022-06-28 07:34:21 - train: epoch 0070, iter [04000, 05004], lr: 0.037313, loss: 1.4370
2022-06-28 07:34:55 - train: epoch 0070, iter [04100, 05004], lr: 0.037288, loss: 1.4889
2022-06-28 07:35:29 - train: epoch 0070, iter [04200, 05004], lr: 0.037262, loss: 1.6632
2022-06-28 07:36:04 - train: epoch 0070, iter [04300, 05004], lr: 0.037237, loss: 1.7099
2022-06-28 07:36:38 - train: epoch 0070, iter [04400, 05004], lr: 0.037212, loss: 1.6379
2022-06-28 07:37:13 - train: epoch 0070, iter [04500, 05004], lr: 0.037186, loss: 1.7109
2022-06-28 07:37:48 - train: epoch 0070, iter [04600, 05004], lr: 0.037161, loss: 1.7806
2022-06-28 07:38:23 - train: epoch 0070, iter [04700, 05004], lr: 0.037136, loss: 1.5912
2022-06-28 07:38:57 - train: epoch 0070, iter [04800, 05004], lr: 0.037111, loss: 1.7227
2022-06-28 07:39:32 - train: epoch 0070, iter [04900, 05004], lr: 0.037085, loss: 1.7547
2022-06-28 07:40:05 - train: epoch 0070, iter [05000, 05004], lr: 0.037060, loss: 1.6327
2022-06-28 07:40:06 - train: epoch 070, train_loss: 1.6193
2022-06-28 07:41:23 - eval: epoch: 070, acc1: 66.240%, acc5: 87.572%, test_loss: 1.3793, per_image_load_time: 2.572ms, per_image_inference_time: 0.395ms
2022-06-28 07:41:24 - until epoch: 070, best_acc1: 66.240%
2022-06-28 07:41:24 - epoch 071 lr: 0.037059
2022-06-28 07:42:03 - train: epoch 0071, iter [00100, 05004], lr: 0.037034, loss: 1.4027
2022-06-28 07:42:38 - train: epoch 0071, iter [00200, 05004], lr: 0.037009, loss: 1.4119
2022-06-28 07:43:12 - train: epoch 0071, iter [00300, 05004], lr: 0.036983, loss: 1.5263
2022-06-28 07:43:47 - train: epoch 0071, iter [00400, 05004], lr: 0.036958, loss: 1.6861
2022-06-28 07:44:21 - train: epoch 0071, iter [00500, 05004], lr: 0.036933, loss: 1.6150
2022-06-28 07:44:56 - train: epoch 0071, iter [00600, 05004], lr: 0.036908, loss: 1.7027
2022-06-28 07:45:30 - train: epoch 0071, iter [00700, 05004], lr: 0.036882, loss: 1.6737
2022-06-28 07:46:04 - train: epoch 0071, iter [00800, 05004], lr: 0.036857, loss: 1.5492
2022-06-28 07:46:39 - train: epoch 0071, iter [00900, 05004], lr: 0.036832, loss: 1.7887
2022-06-28 07:47:13 - train: epoch 0071, iter [01000, 05004], lr: 0.036807, loss: 1.7494
2022-06-28 07:47:48 - train: epoch 0071, iter [01100, 05004], lr: 0.036781, loss: 1.7647
2022-06-28 07:48:22 - train: epoch 0071, iter [01200, 05004], lr: 0.036756, loss: 1.5362
2022-06-28 07:48:57 - train: epoch 0071, iter [01300, 05004], lr: 0.036731, loss: 1.6602
2022-06-28 07:49:31 - train: epoch 0071, iter [01400, 05004], lr: 0.036706, loss: 1.6516
2022-06-28 07:50:06 - train: epoch 0071, iter [01500, 05004], lr: 0.036680, loss: 1.3319
2022-06-28 07:50:41 - train: epoch 0071, iter [01600, 05004], lr: 0.036655, loss: 1.4032
2022-06-28 07:51:15 - train: epoch 0071, iter [01700, 05004], lr: 0.036630, loss: 1.6543
2022-06-28 07:51:50 - train: epoch 0071, iter [01800, 05004], lr: 0.036605, loss: 1.6038
2022-06-28 07:52:24 - train: epoch 0071, iter [01900, 05004], lr: 0.036580, loss: 1.5690
2022-06-28 07:52:59 - train: epoch 0071, iter [02000, 05004], lr: 0.036554, loss: 1.6837
2022-06-28 07:53:34 - train: epoch 0071, iter [02100, 05004], lr: 0.036529, loss: 1.4399
2022-06-28 07:54:08 - train: epoch 0071, iter [02200, 05004], lr: 0.036504, loss: 1.4346
2022-06-28 07:54:43 - train: epoch 0071, iter [02300, 05004], lr: 0.036479, loss: 1.5702
2022-06-28 07:55:18 - train: epoch 0071, iter [02400, 05004], lr: 0.036454, loss: 1.5811
2022-06-28 07:55:52 - train: epoch 0071, iter [02500, 05004], lr: 0.036428, loss: 1.8882
2022-06-28 07:56:27 - train: epoch 0071, iter [02600, 05004], lr: 0.036403, loss: 1.4597
2022-06-28 07:57:02 - train: epoch 0071, iter [02700, 05004], lr: 0.036378, loss: 1.6563
2022-06-28 07:57:37 - train: epoch 0071, iter [02800, 05004], lr: 0.036353, loss: 1.7103
2022-06-28 07:58:12 - train: epoch 0071, iter [02900, 05004], lr: 0.036328, loss: 1.5990
2022-06-28 07:58:47 - train: epoch 0071, iter [03000, 05004], lr: 0.036303, loss: 1.6996
2022-06-28 07:59:21 - train: epoch 0071, iter [03100, 05004], lr: 0.036277, loss: 1.6196
2022-06-28 07:59:56 - train: epoch 0071, iter [03200, 05004], lr: 0.036252, loss: 1.4434
2022-06-28 08:00:31 - train: epoch 0071, iter [03300, 05004], lr: 0.036227, loss: 1.4713
2022-06-28 08:01:06 - train: epoch 0071, iter [03400, 05004], lr: 0.036202, loss: 1.5189
2022-06-28 08:01:41 - train: epoch 0071, iter [03500, 05004], lr: 0.036177, loss: 1.7312
2022-06-28 08:02:15 - train: epoch 0071, iter [03600, 05004], lr: 0.036152, loss: 1.7585
2022-06-28 08:02:49 - train: epoch 0071, iter [03700, 05004], lr: 0.036127, loss: 1.5489
2022-06-28 08:03:24 - train: epoch 0071, iter [03800, 05004], lr: 0.036101, loss: 1.5412
2022-06-28 08:03:58 - train: epoch 0071, iter [03900, 05004], lr: 0.036076, loss: 1.7128
2022-06-28 08:04:33 - train: epoch 0071, iter [04000, 05004], lr: 0.036051, loss: 1.8977
2022-06-28 08:05:08 - train: epoch 0071, iter [04100, 05004], lr: 0.036026, loss: 1.5717
2022-06-28 08:05:43 - train: epoch 0071, iter [04200, 05004], lr: 0.036001, loss: 1.7353
2022-06-28 08:06:17 - train: epoch 0071, iter [04300, 05004], lr: 0.035976, loss: 1.6365
2022-06-28 08:06:52 - train: epoch 0071, iter [04400, 05004], lr: 0.035951, loss: 1.7051
2022-06-28 08:07:27 - train: epoch 0071, iter [04500, 05004], lr: 0.035926, loss: 1.6072
2022-06-28 08:08:02 - train: epoch 0071, iter [04600, 05004], lr: 0.035901, loss: 1.5848
2022-06-28 08:08:37 - train: epoch 0071, iter [04700, 05004], lr: 0.035875, loss: 1.5374
2022-06-28 08:09:12 - train: epoch 0071, iter [04800, 05004], lr: 0.035850, loss: 1.4783
2022-06-28 08:09:46 - train: epoch 0071, iter [04900, 05004], lr: 0.035825, loss: 1.4489
2022-06-28 08:10:19 - train: epoch 0071, iter [05000, 05004], lr: 0.035800, loss: 1.5104
2022-06-28 08:10:20 - train: epoch 071, train_loss: 1.6047
2022-06-28 08:11:37 - eval: epoch: 071, acc1: 66.318%, acc5: 87.418%, test_loss: 1.3881, per_image_load_time: 2.524ms, per_image_inference_time: 0.400ms
2022-06-28 08:11:37 - until epoch: 071, best_acc1: 66.318%
2022-06-28 08:11:37 - epoch 072 lr: 0.035799
2022-06-28 08:12:17 - train: epoch 0072, iter [00100, 05004], lr: 0.035774, loss: 1.7230
2022-06-28 08:12:51 - train: epoch 0072, iter [00200, 05004], lr: 0.035749, loss: 1.4605
2022-06-28 08:13:26 - train: epoch 0072, iter [00300, 05004], lr: 0.035724, loss: 1.3472
2022-06-28 08:14:00 - train: epoch 0072, iter [00400, 05004], lr: 0.035699, loss: 1.5055
2022-06-28 08:14:34 - train: epoch 0072, iter [00500, 05004], lr: 0.035674, loss: 1.4172
2022-06-28 08:15:09 - train: epoch 0072, iter [00600, 05004], lr: 0.035649, loss: 1.4470
2022-06-28 08:15:43 - train: epoch 0072, iter [00700, 05004], lr: 0.035624, loss: 1.5919
2022-06-28 08:16:18 - train: epoch 0072, iter [00800, 05004], lr: 0.035599, loss: 1.7535
2022-06-28 08:16:52 - train: epoch 0072, iter [00900, 05004], lr: 0.035574, loss: 1.3846
2022-06-28 08:17:27 - train: epoch 0072, iter [01000, 05004], lr: 0.035549, loss: 1.4873
2022-06-28 08:18:02 - train: epoch 0072, iter [01100, 05004], lr: 0.035524, loss: 1.6760
2022-06-28 08:18:36 - train: epoch 0072, iter [01200, 05004], lr: 0.035499, loss: 1.5158
2022-06-28 08:19:11 - train: epoch 0072, iter [01300, 05004], lr: 0.035474, loss: 1.6109
2022-06-28 08:19:45 - train: epoch 0072, iter [01400, 05004], lr: 0.035448, loss: 1.6377
2022-06-28 08:20:20 - train: epoch 0072, iter [01500, 05004], lr: 0.035423, loss: 1.6193
2022-06-28 08:20:55 - train: epoch 0072, iter [01600, 05004], lr: 0.035398, loss: 1.6452
2022-06-28 08:21:29 - train: epoch 0072, iter [01700, 05004], lr: 0.035373, loss: 1.4173
2022-06-28 08:22:04 - train: epoch 0072, iter [01800, 05004], lr: 0.035348, loss: 1.3802
2022-06-28 08:22:38 - train: epoch 0072, iter [01900, 05004], lr: 0.035323, loss: 1.4358
2022-06-28 08:23:12 - train: epoch 0072, iter [02000, 05004], lr: 0.035298, loss: 1.6926
2022-06-28 08:23:48 - train: epoch 0072, iter [02100, 05004], lr: 0.035273, loss: 1.6851
2022-06-28 08:24:22 - train: epoch 0072, iter [02200, 05004], lr: 0.035248, loss: 1.7416
2022-06-28 08:24:57 - train: epoch 0072, iter [02300, 05004], lr: 0.035223, loss: 1.7552
2022-06-28 08:25:31 - train: epoch 0072, iter [02400, 05004], lr: 0.035198, loss: 1.5791
2022-06-28 08:26:06 - train: epoch 0072, iter [02500, 05004], lr: 0.035173, loss: 1.4613
2022-06-28 08:26:39 - train: epoch 0072, iter [02600, 05004], lr: 0.035148, loss: 1.4368
2022-06-28 08:27:14 - train: epoch 0072, iter [02700, 05004], lr: 0.035123, loss: 1.6606
2022-06-28 08:27:48 - train: epoch 0072, iter [02800, 05004], lr: 0.035098, loss: 1.6847
2022-06-28 08:28:23 - train: epoch 0072, iter [02900, 05004], lr: 0.035074, loss: 1.4248
2022-06-28 08:28:58 - train: epoch 0072, iter [03000, 05004], lr: 0.035049, loss: 1.5888
2022-06-28 08:29:33 - train: epoch 0072, iter [03100, 05004], lr: 0.035024, loss: 1.6723
2022-06-28 08:30:08 - train: epoch 0072, iter [03200, 05004], lr: 0.034999, loss: 1.5089
2022-06-28 08:30:42 - train: epoch 0072, iter [03300, 05004], lr: 0.034974, loss: 1.6016
2022-06-28 08:31:17 - train: epoch 0072, iter [03400, 05004], lr: 0.034949, loss: 1.7459
2022-06-28 08:31:51 - train: epoch 0072, iter [03500, 05004], lr: 0.034924, loss: 1.6925
2022-06-28 08:32:25 - train: epoch 0072, iter [03600, 05004], lr: 0.034899, loss: 1.5582
2022-06-28 08:33:00 - train: epoch 0072, iter [03700, 05004], lr: 0.034874, loss: 1.7881
2022-06-28 08:33:35 - train: epoch 0072, iter [03800, 05004], lr: 0.034849, loss: 1.6119
2022-06-28 08:34:09 - train: epoch 0072, iter [03900, 05004], lr: 0.034824, loss: 1.6663
2022-06-28 08:34:44 - train: epoch 0072, iter [04000, 05004], lr: 0.034799, loss: 1.6337
2022-06-28 08:35:18 - train: epoch 0072, iter [04100, 05004], lr: 0.034774, loss: 1.8200
2022-06-28 08:35:54 - train: epoch 0072, iter [04200, 05004], lr: 0.034749, loss: 1.5913
2022-06-28 08:36:28 - train: epoch 0072, iter [04300, 05004], lr: 0.034724, loss: 1.6151
2022-06-28 08:37:03 - train: epoch 0072, iter [04400, 05004], lr: 0.034699, loss: 1.4337
2022-06-28 08:37:38 - train: epoch 0072, iter [04500, 05004], lr: 0.034675, loss: 1.8077
2022-06-28 08:38:12 - train: epoch 0072, iter [04600, 05004], lr: 0.034650, loss: 1.4857
2022-06-28 08:38:46 - train: epoch 0072, iter [04700, 05004], lr: 0.034625, loss: 1.6093
2022-06-28 08:39:22 - train: epoch 0072, iter [04800, 05004], lr: 0.034600, loss: 1.9064
2022-06-28 08:39:56 - train: epoch 0072, iter [04900, 05004], lr: 0.034575, loss: 1.6361
2022-06-28 08:40:30 - train: epoch 0072, iter [05000, 05004], lr: 0.034550, loss: 1.6222
2022-06-28 08:40:31 - train: epoch 072, train_loss: 1.5944
2022-06-28 08:41:47 - eval: epoch: 072, acc1: 66.534%, acc5: 87.606%, test_loss: 1.3813, per_image_load_time: 2.566ms, per_image_inference_time: 0.397ms
2022-06-28 08:41:48 - until epoch: 072, best_acc1: 66.534%
2022-06-28 08:41:48 - epoch 073 lr: 0.034549
2022-06-28 08:42:27 - train: epoch 0073, iter [00100, 05004], lr: 0.034524, loss: 1.8142
2022-06-28 08:43:02 - train: epoch 0073, iter [00200, 05004], lr: 0.034499, loss: 1.6365
2022-06-28 08:43:36 - train: epoch 0073, iter [00300, 05004], lr: 0.034475, loss: 1.7387
2022-06-28 08:44:11 - train: epoch 0073, iter [00400, 05004], lr: 0.034450, loss: 1.2389
2022-06-28 08:44:45 - train: epoch 0073, iter [00500, 05004], lr: 0.034425, loss: 1.4108
2022-06-28 08:45:20 - train: epoch 0073, iter [00600, 05004], lr: 0.034400, loss: 1.4358
2022-06-28 08:45:55 - train: epoch 0073, iter [00700, 05004], lr: 0.034375, loss: 1.5852
2022-06-28 08:46:30 - train: epoch 0073, iter [00800, 05004], lr: 0.034350, loss: 1.4274
2022-06-28 08:47:05 - train: epoch 0073, iter [00900, 05004], lr: 0.034325, loss: 1.3526
2022-06-28 08:47:39 - train: epoch 0073, iter [01000, 05004], lr: 0.034301, loss: 1.4596
2022-06-28 08:48:14 - train: epoch 0073, iter [01100, 05004], lr: 0.034276, loss: 1.6331
2022-06-28 08:48:48 - train: epoch 0073, iter [01200, 05004], lr: 0.034251, loss: 1.5211
2022-06-28 08:49:23 - train: epoch 0073, iter [01300, 05004], lr: 0.034226, loss: 1.6607
2022-06-28 08:49:58 - train: epoch 0073, iter [01400, 05004], lr: 0.034201, loss: 1.4567
2022-06-28 08:50:33 - train: epoch 0073, iter [01500, 05004], lr: 0.034176, loss: 1.5831
2022-06-28 08:51:06 - train: epoch 0073, iter [01600, 05004], lr: 0.034152, loss: 1.6457
2022-06-28 08:51:41 - train: epoch 0073, iter [01700, 05004], lr: 0.034127, loss: 1.7779
2022-06-28 08:52:16 - train: epoch 0073, iter [01800, 05004], lr: 0.034102, loss: 1.3250
2022-06-28 08:52:50 - train: epoch 0073, iter [01900, 05004], lr: 0.034077, loss: 1.5348
2022-06-28 08:53:24 - train: epoch 0073, iter [02000, 05004], lr: 0.034052, loss: 1.3516
2022-06-28 08:53:59 - train: epoch 0073, iter [02100, 05004], lr: 0.034028, loss: 1.6962
2022-06-28 08:54:34 - train: epoch 0073, iter [02200, 05004], lr: 0.034003, loss: 1.7390
2022-06-28 08:55:08 - train: epoch 0073, iter [02300, 05004], lr: 0.033978, loss: 1.6270
2022-06-28 08:55:43 - train: epoch 0073, iter [02400, 05004], lr: 0.033953, loss: 1.4985
2022-06-28 08:56:18 - train: epoch 0073, iter [02500, 05004], lr: 0.033929, loss: 1.9294
2022-06-28 08:56:52 - train: epoch 0073, iter [02600, 05004], lr: 0.033904, loss: 1.5915
2022-06-28 08:57:27 - train: epoch 0073, iter [02700, 05004], lr: 0.033879, loss: 1.6041
2022-06-28 08:58:02 - train: epoch 0073, iter [02800, 05004], lr: 0.033854, loss: 1.5530
2022-06-28 08:58:36 - train: epoch 0073, iter [02900, 05004], lr: 0.033829, loss: 1.7910
2022-06-28 08:59:11 - train: epoch 0073, iter [03000, 05004], lr: 0.033805, loss: 1.4976
2022-06-28 08:59:44 - train: epoch 0073, iter [03100, 05004], lr: 0.033780, loss: 1.5516
2022-06-28 09:00:19 - train: epoch 0073, iter [03200, 05004], lr: 0.033755, loss: 1.5371
2022-06-28 09:00:54 - train: epoch 0073, iter [03300, 05004], lr: 0.033730, loss: 1.6215
2022-06-28 09:01:29 - train: epoch 0073, iter [03400, 05004], lr: 0.033706, loss: 1.5917
2022-06-28 09:02:03 - train: epoch 0073, iter [03500, 05004], lr: 0.033681, loss: 1.6774
2022-06-28 09:02:38 - train: epoch 0073, iter [03600, 05004], lr: 0.033656, loss: 1.4880
2022-06-28 09:03:13 - train: epoch 0073, iter [03700, 05004], lr: 0.033632, loss: 1.6663
2022-06-28 09:03:47 - train: epoch 0073, iter [03800, 05004], lr: 0.033607, loss: 1.7963
2022-06-28 09:04:22 - train: epoch 0073, iter [03900, 05004], lr: 0.033582, loss: 1.6709
2022-06-28 09:04:57 - train: epoch 0073, iter [04000, 05004], lr: 0.033557, loss: 1.6772
2022-06-28 09:05:31 - train: epoch 0073, iter [04100, 05004], lr: 0.033533, loss: 1.6542
2022-06-28 09:06:06 - train: epoch 0073, iter [04200, 05004], lr: 0.033508, loss: 1.7794
2022-06-28 09:06:40 - train: epoch 0073, iter [04300, 05004], lr: 0.033483, loss: 1.5829
2022-06-28 09:07:15 - train: epoch 0073, iter [04400, 05004], lr: 0.033459, loss: 1.7185
2022-06-28 09:07:50 - train: epoch 0073, iter [04500, 05004], lr: 0.033434, loss: 1.4974
2022-06-28 09:08:24 - train: epoch 0073, iter [04600, 05004], lr: 0.033409, loss: 1.8180
2022-06-28 09:08:59 - train: epoch 0073, iter [04700, 05004], lr: 0.033385, loss: 1.3335
2022-06-28 09:09:33 - train: epoch 0073, iter [04800, 05004], lr: 0.033360, loss: 1.6383
2022-06-28 09:10:08 - train: epoch 0073, iter [04900, 05004], lr: 0.033335, loss: 1.4283
2022-06-28 09:10:41 - train: epoch 0073, iter [05000, 05004], lr: 0.033311, loss: 1.6993
2022-06-28 09:10:42 - train: epoch 073, train_loss: 1.5806
2022-06-28 09:11:59 - eval: epoch: 073, acc1: 66.578%, acc5: 87.604%, test_loss: 1.3607, per_image_load_time: 2.565ms, per_image_inference_time: 0.442ms
2022-06-28 09:12:00 - until epoch: 073, best_acc1: 66.578%
2022-06-28 09:12:00 - epoch 074 lr: 0.033309
2022-06-28 09:12:40 - train: epoch 0074, iter [00100, 05004], lr: 0.033285, loss: 1.4523
2022-06-28 09:13:14 - train: epoch 0074, iter [00200, 05004], lr: 0.033260, loss: 1.5528
2022-06-28 09:13:49 - train: epoch 0074, iter [00300, 05004], lr: 0.033236, loss: 1.6609
2022-06-28 09:14:22 - train: epoch 0074, iter [00400, 05004], lr: 0.033211, loss: 1.7437
2022-06-28 09:14:57 - train: epoch 0074, iter [00500, 05004], lr: 0.033186, loss: 1.4921
2022-06-28 09:15:31 - train: epoch 0074, iter [00600, 05004], lr: 0.033162, loss: 1.6110
2022-06-28 09:16:06 - train: epoch 0074, iter [00700, 05004], lr: 0.033137, loss: 1.4584
2022-06-28 09:16:40 - train: epoch 0074, iter [00800, 05004], lr: 0.033113, loss: 1.6096
2022-06-28 09:17:14 - train: epoch 0074, iter [00900, 05004], lr: 0.033088, loss: 1.4961
2022-06-28 09:17:50 - train: epoch 0074, iter [01000, 05004], lr: 0.033063, loss: 1.7528
2022-06-28 09:18:24 - train: epoch 0074, iter [01100, 05004], lr: 0.033039, loss: 1.6168
2022-06-28 09:18:59 - train: epoch 0074, iter [01200, 05004], lr: 0.033014, loss: 1.4707
2022-06-28 09:19:33 - train: epoch 0074, iter [01300, 05004], lr: 0.032989, loss: 1.6513
2022-06-28 09:20:07 - train: epoch 0074, iter [01400, 05004], lr: 0.032965, loss: 1.5340
2022-06-28 09:20:41 - train: epoch 0074, iter [01500, 05004], lr: 0.032940, loss: 1.7047
2022-06-28 09:21:15 - train: epoch 0074, iter [01600, 05004], lr: 0.032916, loss: 1.3982
2022-06-28 09:21:50 - train: epoch 0074, iter [01700, 05004], lr: 0.032891, loss: 1.6118
2022-06-28 09:22:24 - train: epoch 0074, iter [01800, 05004], lr: 0.032867, loss: 1.9549
2022-06-28 09:22:59 - train: epoch 0074, iter [01900, 05004], lr: 0.032842, loss: 1.6333
2022-06-28 09:23:34 - train: epoch 0074, iter [02000, 05004], lr: 0.032817, loss: 1.3100
2022-06-28 09:24:08 - train: epoch 0074, iter [02100, 05004], lr: 0.032793, loss: 1.5791
2022-06-28 09:24:43 - train: epoch 0074, iter [02200, 05004], lr: 0.032768, loss: 1.8529
2022-06-28 09:25:17 - train: epoch 0074, iter [02300, 05004], lr: 0.032744, loss: 1.6480
2022-06-28 09:25:51 - train: epoch 0074, iter [02400, 05004], lr: 0.032719, loss: 1.5089
2022-06-28 09:26:26 - train: epoch 0074, iter [02500, 05004], lr: 0.032695, loss: 1.5164
2022-06-28 09:27:00 - train: epoch 0074, iter [02600, 05004], lr: 0.032670, loss: 1.3265
2022-06-28 09:27:35 - train: epoch 0074, iter [02700, 05004], lr: 0.032646, loss: 1.4736
2022-06-28 09:28:10 - train: epoch 0074, iter [02800, 05004], lr: 0.032621, loss: 1.9160
2022-06-28 09:28:44 - train: epoch 0074, iter [02900, 05004], lr: 0.032597, loss: 1.5718
2022-06-28 09:29:18 - train: epoch 0074, iter [03000, 05004], lr: 0.032572, loss: 1.6374
2022-06-28 09:29:53 - train: epoch 0074, iter [03100, 05004], lr: 0.032547, loss: 1.6050
2022-06-28 09:30:27 - train: epoch 0074, iter [03200, 05004], lr: 0.032523, loss: 1.4727
2022-06-28 09:31:02 - train: epoch 0074, iter [03300, 05004], lr: 0.032498, loss: 1.6195
2022-06-28 09:31:36 - train: epoch 0074, iter [03400, 05004], lr: 0.032474, loss: 1.7094
2022-06-28 09:32:11 - train: epoch 0074, iter [03500, 05004], lr: 0.032449, loss: 1.4165
2022-06-28 09:32:46 - train: epoch 0074, iter [03600, 05004], lr: 0.032425, loss: 1.5742
2022-06-28 09:33:20 - train: epoch 0074, iter [03700, 05004], lr: 0.032400, loss: 1.7965
2022-06-28 09:33:55 - train: epoch 0074, iter [03800, 05004], lr: 0.032376, loss: 1.5962
2022-06-28 09:34:29 - train: epoch 0074, iter [03900, 05004], lr: 0.032352, loss: 1.3875
2022-06-28 09:35:05 - train: epoch 0074, iter [04000, 05004], lr: 0.032327, loss: 1.4695
2022-06-28 09:35:39 - train: epoch 0074, iter [04100, 05004], lr: 0.032303, loss: 1.7421
2022-06-28 09:36:13 - train: epoch 0074, iter [04200, 05004], lr: 0.032278, loss: 1.5459
2022-06-28 09:36:48 - train: epoch 0074, iter [04300, 05004], lr: 0.032254, loss: 1.6057
2022-06-28 09:37:23 - train: epoch 0074, iter [04400, 05004], lr: 0.032229, loss: 1.3827
2022-06-28 09:37:57 - train: epoch 0074, iter [04500, 05004], lr: 0.032205, loss: 1.3781
2022-06-28 09:38:31 - train: epoch 0074, iter [04600, 05004], lr: 0.032180, loss: 1.6454
2022-06-28 09:39:05 - train: epoch 0074, iter [04700, 05004], lr: 0.032156, loss: 1.5894
2022-06-28 09:39:40 - train: epoch 0074, iter [04800, 05004], lr: 0.032131, loss: 1.5339
2022-06-28 09:40:13 - train: epoch 0074, iter [04900, 05004], lr: 0.032107, loss: 1.9809
2022-06-28 09:40:47 - train: epoch 0074, iter [05000, 05004], lr: 0.032083, loss: 1.5164
2022-06-28 09:40:48 - train: epoch 074, train_loss: 1.5666
2022-06-28 09:42:04 - eval: epoch: 074, acc1: 66.988%, acc5: 87.946%, test_loss: 1.3567, per_image_load_time: 2.585ms, per_image_inference_time: 0.365ms
2022-06-28 09:42:05 - until epoch: 074, best_acc1: 66.988%
2022-06-28 09:42:05 - epoch 075 lr: 0.032081
2022-06-28 09:42:45 - train: epoch 0075, iter [00100, 05004], lr: 0.032057, loss: 1.6546
2022-06-28 09:43:19 - train: epoch 0075, iter [00200, 05004], lr: 0.032033, loss: 1.5413
2022-06-28 09:43:53 - train: epoch 0075, iter [00300, 05004], lr: 0.032008, loss: 1.4864
2022-06-28 09:44:27 - train: epoch 0075, iter [00400, 05004], lr: 0.031984, loss: 1.5864
2022-06-28 09:45:02 - train: epoch 0075, iter [00500, 05004], lr: 0.031960, loss: 1.3972
2022-06-28 09:45:37 - train: epoch 0075, iter [00600, 05004], lr: 0.031935, loss: 1.3970
2022-06-28 09:46:11 - train: epoch 0075, iter [00700, 05004], lr: 0.031911, loss: 1.7363
2022-06-28 09:46:46 - train: epoch 0075, iter [00800, 05004], lr: 0.031886, loss: 1.6395
2022-06-28 09:47:19 - train: epoch 0075, iter [00900, 05004], lr: 0.031862, loss: 1.6490
2022-06-28 09:47:54 - train: epoch 0075, iter [01000, 05004], lr: 0.031838, loss: 1.3801
2022-06-28 09:48:28 - train: epoch 0075, iter [01100, 05004], lr: 0.031813, loss: 1.5406
2022-06-28 09:49:03 - train: epoch 0075, iter [01200, 05004], lr: 0.031789, loss: 1.4166
2022-06-28 09:49:37 - train: epoch 0075, iter [01300, 05004], lr: 0.031765, loss: 1.3701
2022-06-28 09:50:12 - train: epoch 0075, iter [01400, 05004], lr: 0.031740, loss: 1.5871
2022-06-28 09:50:47 - train: epoch 0075, iter [01500, 05004], lr: 0.031716, loss: 1.6811
2022-06-28 09:51:20 - train: epoch 0075, iter [01600, 05004], lr: 0.031691, loss: 1.1158
2022-06-28 09:51:55 - train: epoch 0075, iter [01700, 05004], lr: 0.031667, loss: 1.5518
2022-06-28 09:52:29 - train: epoch 0075, iter [01800, 05004], lr: 0.031643, loss: 1.5649
2022-06-28 09:53:04 - train: epoch 0075, iter [01900, 05004], lr: 0.031618, loss: 1.3750
2022-06-28 09:53:38 - train: epoch 0075, iter [02000, 05004], lr: 0.031594, loss: 1.5544
2022-06-28 09:54:13 - train: epoch 0075, iter [02100, 05004], lr: 0.031570, loss: 1.4074
2022-06-28 09:54:47 - train: epoch 0075, iter [02200, 05004], lr: 0.031546, loss: 1.6707
2022-06-28 09:55:21 - train: epoch 0075, iter [02300, 05004], lr: 0.031521, loss: 1.4903
2022-06-28 09:55:55 - train: epoch 0075, iter [02400, 05004], lr: 0.031497, loss: 1.5889
2022-06-28 09:56:31 - train: epoch 0075, iter [02500, 05004], lr: 0.031473, loss: 1.7092
2022-06-28 09:57:05 - train: epoch 0075, iter [02600, 05004], lr: 0.031448, loss: 1.5968
2022-06-28 09:57:40 - train: epoch 0075, iter [02700, 05004], lr: 0.031424, loss: 1.4039
2022-06-28 09:58:13 - train: epoch 0075, iter [02800, 05004], lr: 0.031400, loss: 1.5461
2022-06-28 09:58:48 - train: epoch 0075, iter [02900, 05004], lr: 0.031375, loss: 1.6966
2022-06-28 09:59:22 - train: epoch 0075, iter [03000, 05004], lr: 0.031351, loss: 1.7188
2022-06-28 09:59:57 - train: epoch 0075, iter [03100, 05004], lr: 0.031327, loss: 1.6358
2022-06-28 10:00:31 - train: epoch 0075, iter [03200, 05004], lr: 0.031303, loss: 1.5539
2022-06-28 10:01:06 - train: epoch 0075, iter [03300, 05004], lr: 0.031278, loss: 1.6298
2022-06-28 10:01:40 - train: epoch 0075, iter [03400, 05004], lr: 0.031254, loss: 1.4193
2022-06-28 10:02:15 - train: epoch 0075, iter [03500, 05004], lr: 0.031230, loss: 1.4671
2022-06-28 10:02:50 - train: epoch 0075, iter [03600, 05004], lr: 0.031206, loss: 1.5664
2022-06-28 10:03:24 - train: epoch 0075, iter [03700, 05004], lr: 0.031181, loss: 1.5385
2022-06-28 10:03:59 - train: epoch 0075, iter [03800, 05004], lr: 0.031157, loss: 1.5097
2022-06-28 10:04:33 - train: epoch 0075, iter [03900, 05004], lr: 0.031133, loss: 1.7109
2022-06-28 10:05:07 - train: epoch 0075, iter [04000, 05004], lr: 0.031109, loss: 1.3984
2022-06-28 10:05:42 - train: epoch 0075, iter [04100, 05004], lr: 0.031085, loss: 1.2164
2022-06-28 10:06:16 - train: epoch 0075, iter [04200, 05004], lr: 0.031060, loss: 1.7632
2022-06-28 10:06:51 - train: epoch 0075, iter [04300, 05004], lr: 0.031036, loss: 1.8703
2022-06-28 10:07:26 - train: epoch 0075, iter [04400, 05004], lr: 0.031012, loss: 1.3457
2022-06-28 10:08:00 - train: epoch 0075, iter [04500, 05004], lr: 0.030988, loss: 1.5548
2022-06-28 10:08:35 - train: epoch 0075, iter [04600, 05004], lr: 0.030964, loss: 1.3621
2022-06-28 10:09:10 - train: epoch 0075, iter [04700, 05004], lr: 0.030939, loss: 1.7564
2022-06-28 10:09:45 - train: epoch 0075, iter [04800, 05004], lr: 0.030915, loss: 1.6206
2022-06-28 10:10:20 - train: epoch 0075, iter [04900, 05004], lr: 0.030891, loss: 1.4073
2022-06-28 10:10:52 - train: epoch 0075, iter [05000, 05004], lr: 0.030867, loss: 1.5328
2022-06-28 10:10:53 - train: epoch 075, train_loss: 1.5555
2022-06-28 10:12:10 - eval: epoch: 075, acc1: 67.004%, acc5: 88.066%, test_loss: 1.3477, per_image_load_time: 2.550ms, per_image_inference_time: 0.401ms
2022-06-28 10:12:10 - until epoch: 075, best_acc1: 67.004%
2022-06-28 10:12:10 - epoch 076 lr: 0.030866
2022-06-28 10:12:51 - train: epoch 0076, iter [00100, 05004], lr: 0.030842, loss: 1.2851
2022-06-28 10:13:24 - train: epoch 0076, iter [00200, 05004], lr: 0.030818, loss: 1.4477
2022-06-28 10:13:59 - train: epoch 0076, iter [00300, 05004], lr: 0.030793, loss: 1.5670
2022-06-28 10:14:33 - train: epoch 0076, iter [00400, 05004], lr: 0.030769, loss: 1.5018
2022-06-28 10:15:08 - train: epoch 0076, iter [00500, 05004], lr: 0.030745, loss: 1.5809
2022-06-28 10:15:42 - train: epoch 0076, iter [00600, 05004], lr: 0.030721, loss: 1.4768
2022-06-28 10:16:17 - train: epoch 0076, iter [00700, 05004], lr: 0.030697, loss: 1.6220
2022-06-28 10:16:52 - train: epoch 0076, iter [00800, 05004], lr: 0.030673, loss: 1.4448
2022-06-28 10:17:26 - train: epoch 0076, iter [00900, 05004], lr: 0.030649, loss: 1.4700
2022-06-28 10:18:01 - train: epoch 0076, iter [01000, 05004], lr: 0.030624, loss: 1.3899
2022-06-28 10:18:35 - train: epoch 0076, iter [01100, 05004], lr: 0.030600, loss: 1.2784
2022-06-28 10:19:10 - train: epoch 0076, iter [01200, 05004], lr: 0.030576, loss: 1.5101
2022-06-28 10:19:45 - train: epoch 0076, iter [01300, 05004], lr: 0.030552, loss: 1.5294
2022-06-28 10:20:19 - train: epoch 0076, iter [01400, 05004], lr: 0.030528, loss: 1.3537
2022-06-28 10:20:53 - train: epoch 0076, iter [01500, 05004], lr: 0.030504, loss: 1.4843
2022-06-28 10:21:29 - train: epoch 0076, iter [01600, 05004], lr: 0.030480, loss: 1.4236
2022-06-28 10:22:03 - train: epoch 0076, iter [01700, 05004], lr: 0.030456, loss: 1.5000
2022-06-28 10:22:37 - train: epoch 0076, iter [01800, 05004], lr: 0.030432, loss: 1.4308
2022-06-28 10:23:11 - train: epoch 0076, iter [01900, 05004], lr: 0.030408, loss: 1.6732
2022-06-28 10:23:45 - train: epoch 0076, iter [02000, 05004], lr: 0.030384, loss: 1.6104
2022-06-28 10:24:20 - train: epoch 0076, iter [02100, 05004], lr: 0.030359, loss: 1.6181
2022-06-28 10:24:54 - train: epoch 0076, iter [02200, 05004], lr: 0.030335, loss: 1.6698
2022-06-28 10:25:29 - train: epoch 0076, iter [02300, 05004], lr: 0.030311, loss: 1.4846
2022-06-28 10:26:05 - train: epoch 0076, iter [02400, 05004], lr: 0.030287, loss: 1.6990
2022-06-28 10:26:38 - train: epoch 0076, iter [02500, 05004], lr: 0.030263, loss: 1.4945
2022-06-28 10:27:13 - train: epoch 0076, iter [02600, 05004], lr: 0.030239, loss: 1.7510
2022-06-28 10:27:46 - train: epoch 0076, iter [02700, 05004], lr: 0.030215, loss: 1.5940
2022-06-28 10:28:21 - train: epoch 0076, iter [02800, 05004], lr: 0.030191, loss: 1.5445
2022-06-28 10:28:56 - train: epoch 0076, iter [02900, 05004], lr: 0.030167, loss: 1.5804
2022-06-28 10:29:30 - train: epoch 0076, iter [03000, 05004], lr: 0.030143, loss: 1.4467
2022-06-28 10:30:04 - train: epoch 0076, iter [03100, 05004], lr: 0.030119, loss: 1.6498
2022-06-28 10:30:38 - train: epoch 0076, iter [03200, 05004], lr: 0.030095, loss: 1.6175
2022-06-28 10:31:13 - train: epoch 0076, iter [03300, 05004], lr: 0.030071, loss: 1.4729
2022-06-28 10:31:48 - train: epoch 0076, iter [03400, 05004], lr: 0.030047, loss: 1.4501
2022-06-28 10:32:22 - train: epoch 0076, iter [03500, 05004], lr: 0.030023, loss: 1.4092
2022-06-28 10:32:57 - train: epoch 0076, iter [03600, 05004], lr: 0.029999, loss: 1.3058
2022-06-28 10:33:31 - train: epoch 0076, iter [03700, 05004], lr: 0.029975, loss: 1.5029
2022-06-28 10:34:07 - train: epoch 0076, iter [03800, 05004], lr: 0.029951, loss: 1.3873
2022-06-28 10:34:41 - train: epoch 0076, iter [03900, 05004], lr: 0.029927, loss: 1.4904
2022-06-28 10:35:15 - train: epoch 0076, iter [04000, 05004], lr: 0.029903, loss: 1.4578
2022-06-28 10:35:50 - train: epoch 0076, iter [04100, 05004], lr: 0.029879, loss: 1.5970
2022-06-28 10:36:25 - train: epoch 0076, iter [04200, 05004], lr: 0.029855, loss: 1.6507
2022-06-28 10:36:59 - train: epoch 0076, iter [04300, 05004], lr: 0.029832, loss: 1.5676
2022-06-28 10:37:34 - train: epoch 0076, iter [04400, 05004], lr: 0.029808, loss: 1.5475
2022-06-28 10:38:08 - train: epoch 0076, iter [04500, 05004], lr: 0.029784, loss: 1.5427
2022-06-28 10:38:43 - train: epoch 0076, iter [04600, 05004], lr: 0.029760, loss: 1.5300
2022-06-28 10:39:17 - train: epoch 0076, iter [04700, 05004], lr: 0.029736, loss: 1.6762
2022-06-28 10:39:51 - train: epoch 0076, iter [04800, 05004], lr: 0.029712, loss: 1.3393
2022-06-28 10:40:26 - train: epoch 0076, iter [04900, 05004], lr: 0.029688, loss: 1.4312
2022-06-28 10:40:59 - train: epoch 0076, iter [05000, 05004], lr: 0.029664, loss: 1.5959
2022-06-28 10:41:00 - train: epoch 076, train_loss: 1.5403
2022-06-28 10:42:17 - eval: epoch: 076, acc1: 67.094%, acc5: 88.052%, test_loss: 1.3486, per_image_load_time: 2.525ms, per_image_inference_time: 0.402ms
2022-06-28 10:42:17 - until epoch: 076, best_acc1: 67.094%
2022-06-28 10:42:17 - epoch 077 lr: 0.029663
2022-06-28 10:42:58 - train: epoch 0077, iter [00100, 05004], lr: 0.029639, loss: 1.7326
2022-06-28 10:43:32 - train: epoch 0077, iter [00200, 05004], lr: 0.029615, loss: 1.5554
2022-06-28 10:44:05 - train: epoch 0077, iter [00300, 05004], lr: 0.029592, loss: 1.3264
2022-06-28 10:44:40 - train: epoch 0077, iter [00400, 05004], lr: 0.029568, loss: 1.6076
2022-06-28 10:45:14 - train: epoch 0077, iter [00500, 05004], lr: 0.029544, loss: 1.3151
2022-06-28 10:45:48 - train: epoch 0077, iter [00600, 05004], lr: 0.029520, loss: 1.2532
2022-06-28 10:46:21 - train: epoch 0077, iter [00700, 05004], lr: 0.029496, loss: 1.5896
2022-06-28 10:46:56 - train: epoch 0077, iter [00800, 05004], lr: 0.029472, loss: 1.5746
2022-06-28 10:47:31 - train: epoch 0077, iter [00900, 05004], lr: 0.029448, loss: 1.6841
2022-06-28 10:48:05 - train: epoch 0077, iter [01000, 05004], lr: 0.029424, loss: 1.3013
2022-06-28 10:48:39 - train: epoch 0077, iter [01100, 05004], lr: 0.029401, loss: 1.6854
2022-06-28 10:49:14 - train: epoch 0077, iter [01200, 05004], lr: 0.029377, loss: 1.5985
2022-06-28 10:49:49 - train: epoch 0077, iter [01300, 05004], lr: 0.029353, loss: 1.1737
2022-06-28 10:50:23 - train: epoch 0077, iter [01400, 05004], lr: 0.029329, loss: 1.7120
2022-06-28 10:50:58 - train: epoch 0077, iter [01500, 05004], lr: 0.029305, loss: 1.3753
2022-06-28 10:51:32 - train: epoch 0077, iter [01600, 05004], lr: 0.029282, loss: 1.5281
2022-06-28 10:52:06 - train: epoch 0077, iter [01700, 05004], lr: 0.029258, loss: 1.7930
2022-06-28 10:52:41 - train: epoch 0077, iter [01800, 05004], lr: 0.029234, loss: 1.3930
2022-06-28 10:53:14 - train: epoch 0077, iter [01900, 05004], lr: 0.029210, loss: 1.4811
2022-06-28 10:53:48 - train: epoch 0077, iter [02000, 05004], lr: 0.029186, loss: 1.3706
2022-06-28 10:54:22 - train: epoch 0077, iter [02100, 05004], lr: 0.029163, loss: 1.6020
2022-06-28 10:54:57 - train: epoch 0077, iter [02200, 05004], lr: 0.029139, loss: 1.5388
2022-06-28 10:55:32 - train: epoch 0077, iter [02300, 05004], lr: 0.029115, loss: 1.5756
2022-06-28 10:56:07 - train: epoch 0077, iter [02400, 05004], lr: 0.029091, loss: 1.5225
2022-06-28 10:56:39 - train: epoch 0077, iter [02500, 05004], lr: 0.029067, loss: 1.7401
2022-06-28 10:57:12 - train: epoch 0077, iter [02600, 05004], lr: 0.029044, loss: 1.2737
2022-06-28 10:57:46 - train: epoch 0077, iter [02700, 05004], lr: 0.029020, loss: 1.5258
2022-06-28 10:58:19 - train: epoch 0077, iter [02800, 05004], lr: 0.028996, loss: 1.7457
2022-06-28 10:58:52 - train: epoch 0077, iter [02900, 05004], lr: 0.028973, loss: 1.7661
2022-06-28 10:59:26 - train: epoch 0077, iter [03000, 05004], lr: 0.028949, loss: 1.4307
2022-06-28 10:59:59 - train: epoch 0077, iter [03100, 05004], lr: 0.028925, loss: 1.5056
2022-06-28 11:00:32 - train: epoch 0077, iter [03200, 05004], lr: 0.028901, loss: 1.6128
2022-06-28 11:01:05 - train: epoch 0077, iter [03300, 05004], lr: 0.028878, loss: 1.3956
2022-06-28 11:01:38 - train: epoch 0077, iter [03400, 05004], lr: 0.028854, loss: 1.8157
2022-06-28 11:02:11 - train: epoch 0077, iter [03500, 05004], lr: 0.028830, loss: 1.3479
2022-06-28 11:02:45 - train: epoch 0077, iter [03600, 05004], lr: 0.028807, loss: 1.4306
2022-06-28 11:03:18 - train: epoch 0077, iter [03700, 05004], lr: 0.028783, loss: 1.5465
2022-06-28 11:03:51 - train: epoch 0077, iter [03800, 05004], lr: 0.028759, loss: 1.5130
2022-06-28 11:04:25 - train: epoch 0077, iter [03900, 05004], lr: 0.028735, loss: 1.5261
2022-06-28 11:04:58 - train: epoch 0077, iter [04000, 05004], lr: 0.028712, loss: 1.5203
2022-06-28 11:05:31 - train: epoch 0077, iter [04100, 05004], lr: 0.028688, loss: 1.6991
2022-06-28 11:06:04 - train: epoch 0077, iter [04200, 05004], lr: 0.028664, loss: 1.7048
2022-06-28 11:06:37 - train: epoch 0077, iter [04300, 05004], lr: 0.028641, loss: 1.4239
2022-06-28 11:07:10 - train: epoch 0077, iter [04400, 05004], lr: 0.028617, loss: 1.4500
2022-06-28 11:07:43 - train: epoch 0077, iter [04500, 05004], lr: 0.028594, loss: 1.6492
2022-06-28 11:08:16 - train: epoch 0077, iter [04600, 05004], lr: 0.028570, loss: 1.3907
2022-06-28 11:08:49 - train: epoch 0077, iter [04700, 05004], lr: 0.028546, loss: 1.3737
2022-06-28 11:09:22 - train: epoch 0077, iter [04800, 05004], lr: 0.028523, loss: 1.4450
2022-06-28 11:09:56 - train: epoch 0077, iter [04900, 05004], lr: 0.028499, loss: 1.6625
2022-06-28 11:10:28 - train: epoch 0077, iter [05000, 05004], lr: 0.028475, loss: 1.7863
2022-06-28 11:10:29 - train: epoch 077, train_loss: 1.5251
2022-06-28 11:11:43 - eval: epoch: 077, acc1: 67.098%, acc5: 88.174%, test_loss: 1.3420, per_image_load_time: 2.424ms, per_image_inference_time: 0.405ms
2022-06-28 11:11:44 - until epoch: 077, best_acc1: 67.098%
2022-06-28 11:11:44 - epoch 078 lr: 0.028474
2022-06-28 11:12:22 - train: epoch 0078, iter [00100, 05004], lr: 0.028451, loss: 1.4928
2022-06-28 11:12:55 - train: epoch 0078, iter [00200, 05004], lr: 0.028427, loss: 1.6390
2022-06-28 11:13:28 - train: epoch 0078, iter [00300, 05004], lr: 0.028404, loss: 1.5240
2022-06-28 11:14:02 - train: epoch 0078, iter [00400, 05004], lr: 0.028380, loss: 1.4190
2022-06-28 11:14:35 - train: epoch 0078, iter [00500, 05004], lr: 0.028356, loss: 1.4004
2022-06-28 11:15:08 - train: epoch 0078, iter [00600, 05004], lr: 0.028333, loss: 1.5661
2022-06-28 11:15:41 - train: epoch 0078, iter [00700, 05004], lr: 0.028309, loss: 1.5828
2022-06-28 11:16:15 - train: epoch 0078, iter [00800, 05004], lr: 0.028286, loss: 1.7460
2022-06-28 11:16:48 - train: epoch 0078, iter [00900, 05004], lr: 0.028262, loss: 1.6450
2022-06-28 11:17:23 - train: epoch 0078, iter [01000, 05004], lr: 0.028239, loss: 1.6052
2022-06-28 11:17:55 - train: epoch 0078, iter [01100, 05004], lr: 0.028215, loss: 1.3097
2022-06-28 11:18:29 - train: epoch 0078, iter [01200, 05004], lr: 0.028192, loss: 1.2909
2022-06-28 11:19:01 - train: epoch 0078, iter [01300, 05004], lr: 0.028168, loss: 1.5338
2022-06-28 11:19:36 - train: epoch 0078, iter [01400, 05004], lr: 0.028144, loss: 1.4044
2022-06-28 11:20:08 - train: epoch 0078, iter [01500, 05004], lr: 0.028121, loss: 1.5040
2022-06-28 11:20:42 - train: epoch 0078, iter [01600, 05004], lr: 0.028097, loss: 1.6350
2022-06-28 11:21:16 - train: epoch 0078, iter [01700, 05004], lr: 0.028074, loss: 1.5575
2022-06-28 11:21:50 - train: epoch 0078, iter [01800, 05004], lr: 0.028050, loss: 1.4810
2022-06-28 11:22:23 - train: epoch 0078, iter [01900, 05004], lr: 0.028027, loss: 1.2482
2022-06-28 11:22:56 - train: epoch 0078, iter [02000, 05004], lr: 0.028003, loss: 1.4779
2022-06-28 11:23:30 - train: epoch 0078, iter [02100, 05004], lr: 0.027980, loss: 1.7595
2022-06-28 11:24:04 - train: epoch 0078, iter [02200, 05004], lr: 0.027956, loss: 1.4864
2022-06-28 11:24:38 - train: epoch 0078, iter [02300, 05004], lr: 0.027933, loss: 1.6596
2022-06-28 11:25:11 - train: epoch 0078, iter [02400, 05004], lr: 0.027909, loss: 1.4398
2022-06-28 11:25:45 - train: epoch 0078, iter [02500, 05004], lr: 0.027886, loss: 1.5021
2022-06-28 11:26:20 - train: epoch 0078, iter [02600, 05004], lr: 0.027863, loss: 1.4924
2022-06-28 11:26:54 - train: epoch 0078, iter [02700, 05004], lr: 0.027839, loss: 1.4886
2022-06-28 11:27:26 - train: epoch 0078, iter [02800, 05004], lr: 0.027816, loss: 1.4435
2022-06-28 11:28:00 - train: epoch 0078, iter [02900, 05004], lr: 0.027792, loss: 1.4127
2022-06-28 11:28:34 - train: epoch 0078, iter [03000, 05004], lr: 0.027769, loss: 1.4019
2022-06-28 11:29:07 - train: epoch 0078, iter [03100, 05004], lr: 0.027745, loss: 1.6444
2022-06-28 11:29:41 - train: epoch 0078, iter [03200, 05004], lr: 0.027722, loss: 1.5901
2022-06-28 11:30:14 - train: epoch 0078, iter [03300, 05004], lr: 0.027699, loss: 1.6516
2022-06-28 11:30:49 - train: epoch 0078, iter [03400, 05004], lr: 0.027675, loss: 1.4643
2022-06-28 11:31:22 - train: epoch 0078, iter [03500, 05004], lr: 0.027652, loss: 1.4465
2022-06-28 11:31:55 - train: epoch 0078, iter [03600, 05004], lr: 0.027628, loss: 1.5942
2022-06-28 11:32:29 - train: epoch 0078, iter [03700, 05004], lr: 0.027605, loss: 1.3906
2022-06-28 11:33:03 - train: epoch 0078, iter [03800, 05004], lr: 0.027582, loss: 1.5065
2022-06-28 11:33:37 - train: epoch 0078, iter [03900, 05004], lr: 0.027558, loss: 1.5955
2022-06-28 11:34:10 - train: epoch 0078, iter [04000, 05004], lr: 0.027535, loss: 1.5070
2022-06-28 11:34:44 - train: epoch 0078, iter [04100, 05004], lr: 0.027511, loss: 1.6252
2022-06-28 11:35:17 - train: epoch 0078, iter [04200, 05004], lr: 0.027488, loss: 1.7666
2022-06-28 11:35:52 - train: epoch 0078, iter [04300, 05004], lr: 0.027465, loss: 1.5766
2022-06-28 11:36:25 - train: epoch 0078, iter [04400, 05004], lr: 0.027441, loss: 1.4332
2022-06-28 11:36:58 - train: epoch 0078, iter [04500, 05004], lr: 0.027418, loss: 1.5825
2022-06-28 11:37:32 - train: epoch 0078, iter [04600, 05004], lr: 0.027395, loss: 1.2581
2022-06-28 11:38:06 - train: epoch 0078, iter [04700, 05004], lr: 0.027371, loss: 1.6798
2022-06-28 11:38:39 - train: epoch 0078, iter [04800, 05004], lr: 0.027348, loss: 1.8058
2022-06-28 11:39:13 - train: epoch 0078, iter [04900, 05004], lr: 0.027325, loss: 1.5452
2022-06-28 11:39:46 - train: epoch 0078, iter [05000, 05004], lr: 0.027301, loss: 1.5069
2022-06-28 11:39:47 - train: epoch 078, train_loss: 1.5098
2022-06-28 11:41:02 - eval: epoch: 078, acc1: 67.740%, acc5: 88.458%, test_loss: 1.3158, per_image_load_time: 2.489ms, per_image_inference_time: 0.384ms
2022-06-28 11:41:03 - until epoch: 078, best_acc1: 67.740%
2022-06-28 11:41:03 - epoch 079 lr: 0.027300
2022-06-28 11:41:42 - train: epoch 0079, iter [00100, 05004], lr: 0.027277, loss: 1.4741
2022-06-28 11:42:15 - train: epoch 0079, iter [00200, 05004], lr: 0.027254, loss: 1.4380
2022-06-28 11:42:48 - train: epoch 0079, iter [00300, 05004], lr: 0.027231, loss: 1.5553
2022-06-28 11:43:22 - train: epoch 0079, iter [00400, 05004], lr: 0.027207, loss: 1.7085
2022-06-28 11:43:56 - train: epoch 0079, iter [00500, 05004], lr: 0.027184, loss: 1.5812
2022-06-28 11:44:30 - train: epoch 0079, iter [00600, 05004], lr: 0.027161, loss: 1.3491
2022-06-28 11:45:04 - train: epoch 0079, iter [00700, 05004], lr: 0.027137, loss: 1.4720
2022-06-28 11:45:37 - train: epoch 0079, iter [00800, 05004], lr: 0.027114, loss: 1.5771
2022-06-28 11:46:11 - train: epoch 0079, iter [00900, 05004], lr: 0.027091, loss: 1.3688
2022-06-28 11:46:45 - train: epoch 0079, iter [01000, 05004], lr: 0.027068, loss: 1.4081
2022-06-28 11:47:18 - train: epoch 0079, iter [01100, 05004], lr: 0.027044, loss: 1.6901
2022-06-28 11:47:52 - train: epoch 0079, iter [01200, 05004], lr: 0.027021, loss: 1.7515
2022-06-28 11:48:25 - train: epoch 0079, iter [01300, 05004], lr: 0.026998, loss: 1.3279
2022-06-28 11:48:59 - train: epoch 0079, iter [01400, 05004], lr: 0.026975, loss: 1.4314
2022-06-28 11:49:33 - train: epoch 0079, iter [01500, 05004], lr: 0.026952, loss: 1.3287
2022-06-28 11:50:06 - train: epoch 0079, iter [01600, 05004], lr: 0.026928, loss: 1.2795
2022-06-28 11:50:40 - train: epoch 0079, iter [01700, 05004], lr: 0.026905, loss: 1.3884
2022-06-28 11:51:14 - train: epoch 0079, iter [01800, 05004], lr: 0.026882, loss: 1.4248
2022-06-28 11:51:48 - train: epoch 0079, iter [01900, 05004], lr: 0.026859, loss: 1.5455
2022-06-28 11:52:21 - train: epoch 0079, iter [02000, 05004], lr: 0.026836, loss: 1.6735
2022-06-28 11:52:54 - train: epoch 0079, iter [02100, 05004], lr: 0.026812, loss: 1.3247
2022-06-28 11:53:29 - train: epoch 0079, iter [02200, 05004], lr: 0.026789, loss: 1.5514
2022-06-28 11:54:02 - train: epoch 0079, iter [02300, 05004], lr: 0.026766, loss: 1.4199
2022-06-28 11:54:36 - train: epoch 0079, iter [02400, 05004], lr: 0.026743, loss: 1.4256
2022-06-28 11:55:09 - train: epoch 0079, iter [02500, 05004], lr: 0.026720, loss: 1.4109
2022-06-28 11:55:42 - train: epoch 0079, iter [02600, 05004], lr: 0.026697, loss: 1.4364
2022-06-28 11:56:17 - train: epoch 0079, iter [02700, 05004], lr: 0.026673, loss: 1.2660
2022-06-28 11:56:49 - train: epoch 0079, iter [02800, 05004], lr: 0.026650, loss: 1.3846
2022-06-28 11:57:23 - train: epoch 0079, iter [02900, 05004], lr: 0.026627, loss: 1.3269
2022-06-28 11:57:58 - train: epoch 0079, iter [03000, 05004], lr: 0.026604, loss: 1.4557
2022-06-28 11:58:31 - train: epoch 0079, iter [03100, 05004], lr: 0.026581, loss: 1.5629
2022-06-28 11:59:04 - train: epoch 0079, iter [03200, 05004], lr: 0.026558, loss: 1.7961
2022-06-28 11:59:37 - train: epoch 0079, iter [03300, 05004], lr: 0.026535, loss: 1.5173
2022-06-28 12:00:11 - train: epoch 0079, iter [03400, 05004], lr: 0.026512, loss: 1.3928
2022-06-28 12:00:45 - train: epoch 0079, iter [03500, 05004], lr: 0.026489, loss: 1.6865
2022-06-28 12:01:18 - train: epoch 0079, iter [03600, 05004], lr: 0.026465, loss: 1.3983
2022-06-28 12:01:52 - train: epoch 0079, iter [03700, 05004], lr: 0.026442, loss: 1.4386
2022-06-28 12:02:26 - train: epoch 0079, iter [03800, 05004], lr: 0.026419, loss: 1.5560
2022-06-28 12:03:00 - train: epoch 0079, iter [03900, 05004], lr: 0.026396, loss: 1.3171
2022-06-28 12:03:33 - train: epoch 0079, iter [04000, 05004], lr: 0.026373, loss: 1.3743
2022-06-28 12:04:06 - train: epoch 0079, iter [04100, 05004], lr: 0.026350, loss: 1.6766
2022-06-28 12:04:40 - train: epoch 0079, iter [04200, 05004], lr: 0.026327, loss: 1.3978
2022-06-28 12:05:14 - train: epoch 0079, iter [04300, 05004], lr: 0.026304, loss: 1.1524
2022-06-28 12:05:47 - train: epoch 0079, iter [04400, 05004], lr: 0.026281, loss: 1.6810
2022-06-28 12:06:21 - train: epoch 0079, iter [04500, 05004], lr: 0.026258, loss: 1.8687
2022-06-28 12:06:54 - train: epoch 0079, iter [04600, 05004], lr: 0.026235, loss: 1.7778
2022-06-28 12:07:29 - train: epoch 0079, iter [04700, 05004], lr: 0.026212, loss: 1.4810
2022-06-28 12:08:02 - train: epoch 0079, iter [04800, 05004], lr: 0.026189, loss: 1.6657
2022-06-28 12:08:36 - train: epoch 0079, iter [04900, 05004], lr: 0.026166, loss: 1.6698
2022-06-28 12:09:08 - train: epoch 0079, iter [05000, 05004], lr: 0.026143, loss: 1.2709
2022-06-28 12:09:09 - train: epoch 079, train_loss: 1.4975
2022-06-28 12:10:25 - eval: epoch: 079, acc1: 67.552%, acc5: 88.504%, test_loss: 1.3180, per_image_load_time: 2.480ms, per_image_inference_time: 0.394ms
2022-06-28 12:10:25 - until epoch: 079, best_acc1: 67.740%
2022-06-28 12:10:25 - epoch 080 lr: 0.026142
2022-06-28 12:11:04 - train: epoch 0080, iter [00100, 05004], lr: 0.026119, loss: 1.3385
2022-06-28 12:11:38 - train: epoch 0080, iter [00200, 05004], lr: 0.026096, loss: 1.5205
2022-06-28 12:12:12 - train: epoch 0080, iter [00300, 05004], lr: 0.026073, loss: 1.5453
2022-06-28 12:12:45 - train: epoch 0080, iter [00400, 05004], lr: 0.026050, loss: 1.2987
2022-06-28 12:13:20 - train: epoch 0080, iter [00500, 05004], lr: 0.026027, loss: 1.5599
2022-06-28 12:13:53 - train: epoch 0080, iter [00600, 05004], lr: 0.026004, loss: 1.3359
2022-06-28 12:14:27 - train: epoch 0080, iter [00700, 05004], lr: 0.025981, loss: 1.3504
2022-06-28 12:15:01 - train: epoch 0080, iter [00800, 05004], lr: 0.025958, loss: 1.3361
2022-06-28 12:15:34 - train: epoch 0080, iter [00900, 05004], lr: 0.025935, loss: 1.4734
2022-06-28 12:16:08 - train: epoch 0080, iter [01000, 05004], lr: 0.025912, loss: 1.4245
2022-06-28 12:16:42 - train: epoch 0080, iter [01100, 05004], lr: 0.025890, loss: 1.5232
2022-06-28 12:17:16 - train: epoch 0080, iter [01200, 05004], lr: 0.025867, loss: 1.4116
2022-06-28 12:17:49 - train: epoch 0080, iter [01300, 05004], lr: 0.025844, loss: 1.4110
2022-06-28 12:18:21 - train: epoch 0080, iter [01400, 05004], lr: 0.025821, loss: 1.4479
2022-06-28 12:18:54 - train: epoch 0080, iter [01500, 05004], lr: 0.025798, loss: 1.3316
2022-06-28 12:19:28 - train: epoch 0080, iter [01600, 05004], lr: 0.025775, loss: 1.4815
2022-06-28 12:20:01 - train: epoch 0080, iter [01700, 05004], lr: 0.025752, loss: 1.5087
2022-06-28 12:20:33 - train: epoch 0080, iter [01800, 05004], lr: 0.025729, loss: 1.6343
2022-06-28 12:21:07 - train: epoch 0080, iter [01900, 05004], lr: 0.025706, loss: 1.4352
2022-06-28 12:21:41 - train: epoch 0080, iter [02000, 05004], lr: 0.025684, loss: 1.5278
2022-06-28 12:22:14 - train: epoch 0080, iter [02100, 05004], lr: 0.025661, loss: 1.6792
2022-06-28 12:22:47 - train: epoch 0080, iter [02200, 05004], lr: 0.025638, loss: 1.4867
2022-06-28 12:23:20 - train: epoch 0080, iter [02300, 05004], lr: 0.025615, loss: 1.4112
2022-06-28 12:23:53 - train: epoch 0080, iter [02400, 05004], lr: 0.025592, loss: 1.4302
2022-06-28 12:24:26 - train: epoch 0080, iter [02500, 05004], lr: 0.025569, loss: 1.3948
2022-06-28 12:24:59 - train: epoch 0080, iter [02600, 05004], lr: 0.025547, loss: 1.3981
2022-06-28 12:25:33 - train: epoch 0080, iter [02700, 05004], lr: 0.025524, loss: 1.6946
2022-06-28 12:26:07 - train: epoch 0080, iter [02800, 05004], lr: 0.025501, loss: 1.6122
2022-06-28 12:26:40 - train: epoch 0080, iter [02900, 05004], lr: 0.025478, loss: 1.4506
2022-06-28 12:27:14 - train: epoch 0080, iter [03000, 05004], lr: 0.025455, loss: 1.4686
2022-06-28 12:27:48 - train: epoch 0080, iter [03100, 05004], lr: 0.025433, loss: 1.6953
2022-06-28 12:28:22 - train: epoch 0080, iter [03200, 05004], lr: 0.025410, loss: 1.1333
2022-06-28 12:28:55 - train: epoch 0080, iter [03300, 05004], lr: 0.025387, loss: 1.5096
2022-06-28 12:29:29 - train: epoch 0080, iter [03400, 05004], lr: 0.025364, loss: 1.3757
2022-06-28 12:30:03 - train: epoch 0080, iter [03500, 05004], lr: 0.025341, loss: 1.3666
2022-06-28 12:30:37 - train: epoch 0080, iter [03600, 05004], lr: 0.025319, loss: 1.5372
2022-06-28 12:31:11 - train: epoch 0080, iter [03700, 05004], lr: 0.025296, loss: 1.4685
2022-06-28 12:31:45 - train: epoch 0080, iter [03800, 05004], lr: 0.025273, loss: 1.6890
2022-06-28 12:32:18 - train: epoch 0080, iter [03900, 05004], lr: 0.025251, loss: 1.3735
2022-06-28 12:32:52 - train: epoch 0080, iter [04000, 05004], lr: 0.025228, loss: 1.6541
2022-06-28 12:33:26 - train: epoch 0080, iter [04100, 05004], lr: 0.025205, loss: 1.6441
2022-06-28 12:34:00 - train: epoch 0080, iter [04200, 05004], lr: 0.025182, loss: 1.4522
2022-06-28 12:34:33 - train: epoch 0080, iter [04300, 05004], lr: 0.025160, loss: 1.6083
2022-06-28 12:35:07 - train: epoch 0080, iter [04400, 05004], lr: 0.025137, loss: 1.4854
2022-06-28 12:35:41 - train: epoch 0080, iter [04500, 05004], lr: 0.025114, loss: 1.4244
2022-06-28 12:36:14 - train: epoch 0080, iter [04600, 05004], lr: 0.025092, loss: 1.6227
2022-06-28 12:36:48 - train: epoch 0080, iter [04700, 05004], lr: 0.025069, loss: 1.5236
2022-06-28 12:37:22 - train: epoch 0080, iter [04800, 05004], lr: 0.025046, loss: 1.4761
2022-06-28 12:37:55 - train: epoch 0080, iter [04900, 05004], lr: 0.025024, loss: 1.6635
2022-06-28 12:38:29 - train: epoch 0080, iter [05000, 05004], lr: 0.025001, loss: 1.3158
2022-06-28 12:38:30 - train: epoch 080, train_loss: 1.4857
2022-06-28 12:39:44 - eval: epoch: 080, acc1: 68.158%, acc5: 88.796%, test_loss: 1.2970, per_image_load_time: 2.472ms, per_image_inference_time: 0.385ms
2022-06-28 12:39:45 - until epoch: 080, best_acc1: 68.158%
2022-06-28 12:39:45 - epoch 081 lr: 0.025000
2022-06-28 12:40:24 - train: epoch 0081, iter [00100, 05004], lr: 0.024977, loss: 1.3061
2022-06-28 12:40:57 - train: epoch 0081, iter [00200, 05004], lr: 0.024955, loss: 1.5182
2022-06-28 12:41:30 - train: epoch 0081, iter [00300, 05004], lr: 0.024932, loss: 1.4924
2022-06-28 12:42:04 - train: epoch 0081, iter [00400, 05004], lr: 0.024909, loss: 1.6173
2022-06-28 12:42:39 - train: epoch 0081, iter [00500, 05004], lr: 0.024887, loss: 1.5109
2022-06-28 12:43:12 - train: epoch 0081, iter [00600, 05004], lr: 0.024864, loss: 1.4437
2022-06-28 12:43:46 - train: epoch 0081, iter [00700, 05004], lr: 0.024842, loss: 1.3273
2022-06-28 12:44:20 - train: epoch 0081, iter [00800, 05004], lr: 0.024819, loss: 1.6130
2022-06-28 12:44:53 - train: epoch 0081, iter [00900, 05004], lr: 0.024796, loss: 1.3711
2022-06-28 12:45:27 - train: epoch 0081, iter [01000, 05004], lr: 0.024774, loss: 1.3329
2022-06-28 12:46:00 - train: epoch 0081, iter [01100, 05004], lr: 0.024751, loss: 1.3697
2022-06-28 12:46:33 - train: epoch 0081, iter [01200, 05004], lr: 0.024729, loss: 1.4505
2022-06-28 12:47:07 - train: epoch 0081, iter [01300, 05004], lr: 0.024706, loss: 1.3687
2022-06-28 12:47:40 - train: epoch 0081, iter [01400, 05004], lr: 0.024684, loss: 1.1469
2022-06-28 12:48:14 - train: epoch 0081, iter [01500, 05004], lr: 0.024661, loss: 1.5420
2022-06-28 12:48:48 - train: epoch 0081, iter [01600, 05004], lr: 0.024638, loss: 1.3769
2022-06-28 12:49:21 - train: epoch 0081, iter [01700, 05004], lr: 0.024616, loss: 1.5324
2022-06-28 12:49:56 - train: epoch 0081, iter [01800, 05004], lr: 0.024593, loss: 1.2706
2022-06-28 12:50:30 - train: epoch 0081, iter [01900, 05004], lr: 0.024571, loss: 1.4098
2022-06-28 12:51:02 - train: epoch 0081, iter [02000, 05004], lr: 0.024548, loss: 1.5174
2022-06-28 12:51:37 - train: epoch 0081, iter [02100, 05004], lr: 0.024526, loss: 1.4274
2022-06-28 12:52:10 - train: epoch 0081, iter [02200, 05004], lr: 0.024503, loss: 1.4438
2022-06-28 12:52:43 - train: epoch 0081, iter [02300, 05004], lr: 0.024481, loss: 1.3997
2022-06-28 12:53:17 - train: epoch 0081, iter [02400, 05004], lr: 0.024458, loss: 1.5282
2022-06-28 12:53:51 - train: epoch 0081, iter [02500, 05004], lr: 0.024436, loss: 1.6627
2022-06-28 12:54:26 - train: epoch 0081, iter [02600, 05004], lr: 0.024413, loss: 1.6220
2022-06-28 12:54:59 - train: epoch 0081, iter [02700, 05004], lr: 0.024391, loss: 1.4422
2022-06-28 12:55:32 - train: epoch 0081, iter [02800, 05004], lr: 0.024368, loss: 1.4053
2022-06-28 12:56:06 - train: epoch 0081, iter [02900, 05004], lr: 0.024346, loss: 1.3135
2022-06-28 12:56:39 - train: epoch 0081, iter [03000, 05004], lr: 0.024323, loss: 1.5426
2022-06-28 12:57:14 - train: epoch 0081, iter [03100, 05004], lr: 0.024301, loss: 1.4050
2022-06-28 12:57:47 - train: epoch 0081, iter [03200, 05004], lr: 0.024279, loss: 1.4719
2022-06-28 12:58:21 - train: epoch 0081, iter [03300, 05004], lr: 0.024256, loss: 1.5177
2022-06-28 12:58:55 - train: epoch 0081, iter [03400, 05004], lr: 0.024234, loss: 1.6426
2022-06-28 12:59:29 - train: epoch 0081, iter [03500, 05004], lr: 0.024211, loss: 1.5676
2022-06-28 13:00:02 - train: epoch 0081, iter [03600, 05004], lr: 0.024189, loss: 1.3218
2022-06-28 13:00:36 - train: epoch 0081, iter [03700, 05004], lr: 0.024167, loss: 1.5863
2022-06-28 13:01:09 - train: epoch 0081, iter [03800, 05004], lr: 0.024144, loss: 1.3349
2022-06-28 13:01:43 - train: epoch 0081, iter [03900, 05004], lr: 0.024122, loss: 1.5116
2022-06-28 13:02:16 - train: epoch 0081, iter [04000, 05004], lr: 0.024099, loss: 1.2663
2022-06-28 13:02:51 - train: epoch 0081, iter [04100, 05004], lr: 0.024077, loss: 1.6302
2022-06-28 13:03:24 - train: epoch 0081, iter [04200, 05004], lr: 0.024055, loss: 1.5282
2022-06-28 13:03:59 - train: epoch 0081, iter [04300, 05004], lr: 0.024032, loss: 1.4690
2022-06-28 13:04:32 - train: epoch 0081, iter [04400, 05004], lr: 0.024010, loss: 1.5707
2022-06-28 13:05:05 - train: epoch 0081, iter [04500, 05004], lr: 0.023988, loss: 1.5367
2022-06-28 13:05:39 - train: epoch 0081, iter [04600, 05004], lr: 0.023965, loss: 1.4207
2022-06-28 13:06:13 - train: epoch 0081, iter [04700, 05004], lr: 0.023943, loss: 1.5176
2022-06-28 13:06:46 - train: epoch 0081, iter [04800, 05004], lr: 0.023921, loss: 1.4712
2022-06-28 13:07:20 - train: epoch 0081, iter [04900, 05004], lr: 0.023898, loss: 1.4931
2022-06-28 13:07:53 - train: epoch 0081, iter [05000, 05004], lr: 0.023876, loss: 1.2785
2022-06-28 13:07:54 - train: epoch 081, train_loss: 1.4694
2022-06-28 13:09:09 - eval: epoch: 081, acc1: 68.538%, acc5: 88.564%, test_loss: 1.2970, per_image_load_time: 2.504ms, per_image_inference_time: 0.404ms
2022-06-28 13:09:10 - until epoch: 081, best_acc1: 68.538%
2022-06-28 13:09:10 - epoch 082 lr: 0.023875
2022-06-28 13:09:48 - train: epoch 0082, iter [00100, 05004], lr: 0.023853, loss: 1.1849
2022-06-28 13:10:22 - train: epoch 0082, iter [00200, 05004], lr: 0.023830, loss: 1.2645
2022-06-28 13:10:56 - train: epoch 0082, iter [00300, 05004], lr: 0.023808, loss: 1.4362
2022-06-28 13:11:30 - train: epoch 0082, iter [00400, 05004], lr: 0.023786, loss: 1.5409
2022-06-28 13:12:04 - train: epoch 0082, iter [00500, 05004], lr: 0.023764, loss: 1.5165
2022-06-28 13:12:37 - train: epoch 0082, iter [00600, 05004], lr: 0.023741, loss: 1.3756
2022-06-28 13:13:11 - train: epoch 0082, iter [00700, 05004], lr: 0.023719, loss: 1.6266
2022-06-28 13:13:44 - train: epoch 0082, iter [00800, 05004], lr: 0.023697, loss: 1.4845
2022-06-28 13:14:18 - train: epoch 0082, iter [00900, 05004], lr: 0.023675, loss: 1.6549
2022-06-28 13:14:52 - train: epoch 0082, iter [01000, 05004], lr: 0.023652, loss: 1.5687
2022-06-28 13:15:26 - train: epoch 0082, iter [01100, 05004], lr: 0.023630, loss: 1.6079
2022-06-28 13:15:59 - train: epoch 0082, iter [01200, 05004], lr: 0.023608, loss: 1.4334
2022-06-28 13:16:33 - train: epoch 0082, iter [01300, 05004], lr: 0.023586, loss: 1.7089
2022-06-28 13:17:07 - train: epoch 0082, iter [01400, 05004], lr: 0.023564, loss: 1.3694
2022-06-28 13:17:41 - train: epoch 0082, iter [01500, 05004], lr: 0.023541, loss: 1.3375
2022-06-28 13:18:16 - train: epoch 0082, iter [01600, 05004], lr: 0.023519, loss: 1.5201
2022-06-28 13:18:50 - train: epoch 0082, iter [01700, 05004], lr: 0.023497, loss: 1.3424
2022-06-28 13:19:24 - train: epoch 0082, iter [01800, 05004], lr: 0.023475, loss: 1.3495
2022-06-28 13:19:57 - train: epoch 0082, iter [01900, 05004], lr: 0.023453, loss: 1.3427
2022-06-28 13:20:32 - train: epoch 0082, iter [02000, 05004], lr: 0.023430, loss: 1.2701
2022-06-28 13:21:06 - train: epoch 0082, iter [02100, 05004], lr: 0.023408, loss: 1.4116
2022-06-28 13:21:40 - train: epoch 0082, iter [02200, 05004], lr: 0.023386, loss: 1.5574
2022-06-28 13:22:14 - train: epoch 0082, iter [02300, 05004], lr: 0.023364, loss: 1.4616
2022-06-28 13:22:47 - train: epoch 0082, iter [02400, 05004], lr: 0.023342, loss: 1.5683
2022-06-28 13:23:20 - train: epoch 0082, iter [02500, 05004], lr: 0.023320, loss: 1.4320
2022-06-28 13:23:54 - train: epoch 0082, iter [02600, 05004], lr: 0.023298, loss: 1.3819
2022-06-28 13:24:28 - train: epoch 0082, iter [02700, 05004], lr: 0.023275, loss: 1.3884
2022-06-28 13:25:02 - train: epoch 0082, iter [02800, 05004], lr: 0.023253, loss: 1.2181
2022-06-28 13:25:35 - train: epoch 0082, iter [02900, 05004], lr: 0.023231, loss: 1.2417
2022-06-28 13:26:09 - train: epoch 0082, iter [03000, 05004], lr: 0.023209, loss: 1.3971
2022-06-28 13:26:43 - train: epoch 0082, iter [03100, 05004], lr: 0.023187, loss: 1.3833
2022-06-28 13:27:16 - train: epoch 0082, iter [03200, 05004], lr: 0.023165, loss: 1.6793
2022-06-28 13:27:50 - train: epoch 0082, iter [03300, 05004], lr: 0.023143, loss: 1.3969
2022-06-28 13:28:24 - train: epoch 0082, iter [03400, 05004], lr: 0.023121, loss: 1.3766
2022-06-28 13:28:58 - train: epoch 0082, iter [03500, 05004], lr: 0.023099, loss: 1.3671
2022-06-28 13:29:31 - train: epoch 0082, iter [03600, 05004], lr: 0.023077, loss: 1.2987
2022-06-28 13:30:05 - train: epoch 0082, iter [03700, 05004], lr: 0.023055, loss: 1.2387
2022-06-28 13:30:38 - train: epoch 0082, iter [03800, 05004], lr: 0.023033, loss: 1.6055
2022-06-28 13:31:13 - train: epoch 0082, iter [03900, 05004], lr: 0.023011, loss: 1.5067
2022-06-28 13:31:46 - train: epoch 0082, iter [04000, 05004], lr: 0.022989, loss: 1.5429
2022-06-28 13:32:20 - train: epoch 0082, iter [04100, 05004], lr: 0.022967, loss: 1.4635
2022-06-28 13:32:53 - train: epoch 0082, iter [04200, 05004], lr: 0.022945, loss: 1.7270
2022-06-28 13:33:28 - train: epoch 0082, iter [04300, 05004], lr: 0.022923, loss: 1.2713
2022-06-28 13:34:01 - train: epoch 0082, iter [04400, 05004], lr: 0.022901, loss: 1.4005
2022-06-28 13:34:35 - train: epoch 0082, iter [04500, 05004], lr: 0.022879, loss: 1.5659
2022-06-28 13:35:08 - train: epoch 0082, iter [04600, 05004], lr: 0.022857, loss: 1.4227
2022-06-28 13:35:42 - train: epoch 0082, iter [04700, 05004], lr: 0.022835, loss: 1.3852
2022-06-28 13:36:15 - train: epoch 0082, iter [04800, 05004], lr: 0.022813, loss: 1.4044
2022-06-28 13:36:49 - train: epoch 0082, iter [04900, 05004], lr: 0.022791, loss: 1.4786
2022-06-28 13:37:21 - train: epoch 0082, iter [05000, 05004], lr: 0.022769, loss: 1.4268
2022-06-28 13:37:22 - train: epoch 082, train_loss: 1.4539
2022-06-28 13:38:37 - eval: epoch: 082, acc1: 68.966%, acc5: 89.136%, test_loss: 1.2664, per_image_load_time: 2.453ms, per_image_inference_time: 0.396ms
2022-06-28 13:38:37 - until epoch: 082, best_acc1: 68.966%
2022-06-28 13:38:37 - epoch 083 lr: 0.022768
2022-06-28 13:39:16 - train: epoch 0083, iter [00100, 05004], lr: 0.022746, loss: 1.3490
2022-06-28 13:39:50 - train: epoch 0083, iter [00200, 05004], lr: 0.022724, loss: 1.3685
2022-06-28 13:40:23 - train: epoch 0083, iter [00300, 05004], lr: 0.022702, loss: 1.5354
2022-06-28 13:40:57 - train: epoch 0083, iter [00400, 05004], lr: 0.022680, loss: 1.5768
2022-06-28 13:41:32 - train: epoch 0083, iter [00500, 05004], lr: 0.022658, loss: 1.4066
2022-06-28 13:42:04 - train: epoch 0083, iter [00600, 05004], lr: 0.022637, loss: 1.3241
2022-06-28 13:42:38 - train: epoch 0083, iter [00700, 05004], lr: 0.022615, loss: 1.4060
2022-06-28 13:43:12 - train: epoch 0083, iter [00800, 05004], lr: 0.022593, loss: 1.3686
2022-06-28 13:43:45 - train: epoch 0083, iter [00900, 05004], lr: 0.022571, loss: 1.4900
2022-06-28 13:44:18 - train: epoch 0083, iter [01000, 05004], lr: 0.022549, loss: 1.6391
2022-06-28 13:44:51 - train: epoch 0083, iter [01100, 05004], lr: 0.022527, loss: 1.3593
2022-06-28 13:45:24 - train: epoch 0083, iter [01200, 05004], lr: 0.022505, loss: 1.3656
2022-06-28 13:45:58 - train: epoch 0083, iter [01300, 05004], lr: 0.022483, loss: 1.3029
2022-06-28 13:46:31 - train: epoch 0083, iter [01400, 05004], lr: 0.022462, loss: 1.5252
2022-06-28 13:47:05 - train: epoch 0083, iter [01500, 05004], lr: 0.022440, loss: 1.2585
2022-06-28 13:47:39 - train: epoch 0083, iter [01600, 05004], lr: 0.022418, loss: 1.4437
2022-06-28 13:48:12 - train: epoch 0083, iter [01700, 05004], lr: 0.022396, loss: 1.5914
2022-06-28 13:48:45 - train: epoch 0083, iter [01800, 05004], lr: 0.022374, loss: 1.6434
2022-06-28 13:49:18 - train: epoch 0083, iter [01900, 05004], lr: 0.022353, loss: 1.2834
2022-06-28 13:49:53 - train: epoch 0083, iter [02000, 05004], lr: 0.022331, loss: 1.1753
2022-06-28 13:50:26 - train: epoch 0083, iter [02100, 05004], lr: 0.022309, loss: 1.4002
2022-06-28 13:51:00 - train: epoch 0083, iter [02200, 05004], lr: 0.022287, loss: 1.4234
2022-06-28 13:51:34 - train: epoch 0083, iter [02300, 05004], lr: 0.022265, loss: 1.4958
2022-06-28 13:52:07 - train: epoch 0083, iter [02400, 05004], lr: 0.022244, loss: 1.4220
2022-06-28 13:52:40 - train: epoch 0083, iter [02500, 05004], lr: 0.022222, loss: 1.4635
2022-06-28 13:53:14 - train: epoch 0083, iter [02600, 05004], lr: 0.022200, loss: 1.4184
2022-06-28 13:53:47 - train: epoch 0083, iter [02700, 05004], lr: 0.022178, loss: 1.3670
2022-06-28 13:54:21 - train: epoch 0083, iter [02800, 05004], lr: 0.022157, loss: 1.3730
2022-06-28 13:54:54 - train: epoch 0083, iter [02900, 05004], lr: 0.022135, loss: 1.4767
2022-06-28 13:55:27 - train: epoch 0083, iter [03000, 05004], lr: 0.022113, loss: 1.6310
2022-06-28 13:56:01 - train: epoch 0083, iter [03100, 05004], lr: 0.022092, loss: 1.2604
2022-06-28 13:56:34 - train: epoch 0083, iter [03200, 05004], lr: 0.022070, loss: 1.2278
2022-06-28 13:57:08 - train: epoch 0083, iter [03300, 05004], lr: 0.022048, loss: 1.6143
2022-06-28 13:57:42 - train: epoch 0083, iter [03400, 05004], lr: 0.022026, loss: 1.5561
2022-06-28 13:58:16 - train: epoch 0083, iter [03500, 05004], lr: 0.022005, loss: 1.5147
2022-06-28 13:58:50 - train: epoch 0083, iter [03600, 05004], lr: 0.021983, loss: 1.4923
2022-06-28 13:59:23 - train: epoch 0083, iter [03700, 05004], lr: 0.021961, loss: 1.4220
2022-06-28 13:59:56 - train: epoch 0083, iter [03800, 05004], lr: 0.021940, loss: 1.3082
2022-06-28 14:00:29 - train: epoch 0083, iter [03900, 05004], lr: 0.021918, loss: 1.4309
2022-06-28 14:01:04 - train: epoch 0083, iter [04000, 05004], lr: 0.021897, loss: 1.6318
2022-06-28 14:01:36 - train: epoch 0083, iter [04100, 05004], lr: 0.021875, loss: 1.4620
2022-06-28 14:02:10 - train: epoch 0083, iter [04200, 05004], lr: 0.021853, loss: 1.6918
2022-06-28 14:02:44 - train: epoch 0083, iter [04300, 05004], lr: 0.021832, loss: 1.5308
2022-06-28 14:03:17 - train: epoch 0083, iter [04400, 05004], lr: 0.021810, loss: 1.3336
2022-06-28 14:03:52 - train: epoch 0083, iter [04500, 05004], lr: 0.021788, loss: 1.4402
2022-06-28 14:04:25 - train: epoch 0083, iter [04600, 05004], lr: 0.021767, loss: 1.5247
2022-06-28 14:05:00 - train: epoch 0083, iter [04700, 05004], lr: 0.021745, loss: 1.5780
2022-06-28 14:05:32 - train: epoch 0083, iter [04800, 05004], lr: 0.021724, loss: 1.5276
2022-06-28 14:06:06 - train: epoch 0083, iter [04900, 05004], lr: 0.021702, loss: 1.7005
2022-06-28 14:06:40 - train: epoch 0083, iter [05000, 05004], lr: 0.021681, loss: 1.5318
2022-06-28 14:06:41 - train: epoch 083, train_loss: 1.4386
2022-06-28 14:07:56 - eval: epoch: 083, acc1: 68.798%, acc5: 88.866%, test_loss: 1.2720, per_image_load_time: 2.511ms, per_image_inference_time: 0.387ms
2022-06-28 14:07:56 - until epoch: 083, best_acc1: 68.966%
2022-06-28 14:07:56 - epoch 084 lr: 0.021679
2022-06-28 14:08:36 - train: epoch 0084, iter [00100, 05004], lr: 0.021658, loss: 1.4965
2022-06-28 14:09:09 - train: epoch 0084, iter [00200, 05004], lr: 0.021637, loss: 1.4652
2022-06-28 14:09:43 - train: epoch 0084, iter [00300, 05004], lr: 0.021615, loss: 1.2298
2022-06-28 14:10:18 - train: epoch 0084, iter [00400, 05004], lr: 0.021594, loss: 1.3553
2022-06-28 14:10:52 - train: epoch 0084, iter [00500, 05004], lr: 0.021572, loss: 1.3987
2022-06-28 14:11:26 - train: epoch 0084, iter [00600, 05004], lr: 0.021550, loss: 1.6590
2022-06-28 14:11:59 - train: epoch 0084, iter [00700, 05004], lr: 0.021529, loss: 1.4543
2022-06-28 14:12:33 - train: epoch 0084, iter [00800, 05004], lr: 0.021507, loss: 1.4641
2022-06-28 14:13:07 - train: epoch 0084, iter [00900, 05004], lr: 0.021486, loss: 1.3630
2022-06-28 14:13:40 - train: epoch 0084, iter [01000, 05004], lr: 0.021464, loss: 1.5459
2022-06-28 14:14:14 - train: epoch 0084, iter [01100, 05004], lr: 0.021443, loss: 1.3493
2022-06-28 14:14:48 - train: epoch 0084, iter [01200, 05004], lr: 0.021422, loss: 1.6261
2022-06-28 14:15:21 - train: epoch 0084, iter [01300, 05004], lr: 0.021400, loss: 1.4515
2022-06-28 14:15:55 - train: epoch 0084, iter [01400, 05004], lr: 0.021379, loss: 1.4825
2022-06-28 14:16:29 - train: epoch 0084, iter [01500, 05004], lr: 0.021357, loss: 1.4121
2022-06-28 14:17:02 - train: epoch 0084, iter [01600, 05004], lr: 0.021336, loss: 1.3061
2022-06-28 14:17:37 - train: epoch 0084, iter [01700, 05004], lr: 0.021314, loss: 1.4890
2022-06-28 14:18:10 - train: epoch 0084, iter [01800, 05004], lr: 0.021293, loss: 1.4520
2022-06-28 14:18:44 - train: epoch 0084, iter [01900, 05004], lr: 0.021271, loss: 1.4393
2022-06-28 14:19:18 - train: epoch 0084, iter [02000, 05004], lr: 0.021250, loss: 1.4386
2022-06-28 14:19:52 - train: epoch 0084, iter [02100, 05004], lr: 0.021229, loss: 1.2894
2022-06-28 14:20:25 - train: epoch 0084, iter [02200, 05004], lr: 0.021207, loss: 1.2364
2022-06-28 14:21:00 - train: epoch 0084, iter [02300, 05004], lr: 0.021186, loss: 1.3964
2022-06-28 14:21:34 - train: epoch 0084, iter [02400, 05004], lr: 0.021165, loss: 1.3156
2022-06-28 14:22:08 - train: epoch 0084, iter [02500, 05004], lr: 0.021143, loss: 1.5379
2022-06-28 14:22:43 - train: epoch 0084, iter [02600, 05004], lr: 0.021122, loss: 1.5029
2022-06-28 14:23:17 - train: epoch 0084, iter [02700, 05004], lr: 0.021100, loss: 1.4845
2022-06-28 14:23:50 - train: epoch 0084, iter [02800, 05004], lr: 0.021079, loss: 1.5236
2022-06-28 14:24:24 - train: epoch 0084, iter [02900, 05004], lr: 0.021058, loss: 1.3509
2022-06-28 14:24:59 - train: epoch 0084, iter [03000, 05004], lr: 0.021036, loss: 1.4128
2022-06-28 14:25:32 - train: epoch 0084, iter [03100, 05004], lr: 0.021015, loss: 1.3598
2022-06-28 14:26:07 - train: epoch 0084, iter [03200, 05004], lr: 0.020994, loss: 1.3780
2022-06-28 14:26:41 - train: epoch 0084, iter [03300, 05004], lr: 0.020973, loss: 1.4912
2022-06-28 14:27:15 - train: epoch 0084, iter [03400, 05004], lr: 0.020951, loss: 1.3964
2022-06-28 14:27:49 - train: epoch 0084, iter [03500, 05004], lr: 0.020930, loss: 1.3171
2022-06-28 14:28:23 - train: epoch 0084, iter [03600, 05004], lr: 0.020909, loss: 1.4554
2022-06-28 14:28:56 - train: epoch 0084, iter [03700, 05004], lr: 0.020887, loss: 1.5384
2022-06-28 14:29:31 - train: epoch 0084, iter [03800, 05004], lr: 0.020866, loss: 1.6695
2022-06-28 14:30:04 - train: epoch 0084, iter [03900, 05004], lr: 0.020845, loss: 1.5437
2022-06-28 14:30:38 - train: epoch 0084, iter [04000, 05004], lr: 0.020824, loss: 1.4580
2022-06-28 14:31:12 - train: epoch 0084, iter [04100, 05004], lr: 0.020802, loss: 1.4537
2022-06-28 14:31:45 - train: epoch 0084, iter [04200, 05004], lr: 0.020781, loss: 1.5180
2022-06-28 14:32:19 - train: epoch 0084, iter [04300, 05004], lr: 0.020760, loss: 1.5050
2022-06-28 14:32:53 - train: epoch 0084, iter [04400, 05004], lr: 0.020739, loss: 1.7249
2022-06-28 14:33:27 - train: epoch 0084, iter [04500, 05004], lr: 0.020718, loss: 1.3165
2022-06-28 14:34:01 - train: epoch 0084, iter [04600, 05004], lr: 0.020696, loss: 1.4291
2022-06-28 14:34:34 - train: epoch 0084, iter [04700, 05004], lr: 0.020675, loss: 1.6786
2022-06-28 14:35:08 - train: epoch 0084, iter [04800, 05004], lr: 0.020654, loss: 1.3178
2022-06-28 14:35:42 - train: epoch 0084, iter [04900, 05004], lr: 0.020633, loss: 1.2928
2022-06-28 14:36:15 - train: epoch 0084, iter [05000, 05004], lr: 0.020612, loss: 1.4640
2022-06-28 14:36:16 - train: epoch 084, train_loss: 1.4231
2022-06-28 14:37:30 - eval: epoch: 084, acc1: 68.776%, acc5: 89.000%, test_loss: 1.2663, per_image_load_time: 2.066ms, per_image_inference_time: 0.369ms
2022-06-28 14:37:30 - until epoch: 084, best_acc1: 68.966%
2022-06-28 14:37:30 - epoch 085 lr: 0.020611
2022-06-28 14:38:09 - train: epoch 0085, iter [00100, 05004], lr: 0.020590, loss: 1.2985
2022-06-28 14:38:42 - train: epoch 0085, iter [00200, 05004], lr: 0.020568, loss: 1.3183
2022-06-28 14:39:16 - train: epoch 0085, iter [00300, 05004], lr: 0.020547, loss: 1.5434
2022-06-28 14:39:49 - train: epoch 0085, iter [00400, 05004], lr: 0.020526, loss: 1.3082
2022-06-28 14:40:23 - train: epoch 0085, iter [00500, 05004], lr: 0.020505, loss: 1.5806
2022-06-28 14:40:56 - train: epoch 0085, iter [00600, 05004], lr: 0.020484, loss: 1.1406
2022-06-28 14:41:30 - train: epoch 0085, iter [00700, 05004], lr: 0.020463, loss: 1.2814
2022-06-28 14:42:05 - train: epoch 0085, iter [00800, 05004], lr: 0.020442, loss: 1.1684
2022-06-28 14:42:37 - train: epoch 0085, iter [00900, 05004], lr: 0.020421, loss: 1.3084
2022-06-28 14:43:11 - train: epoch 0085, iter [01000, 05004], lr: 0.020400, loss: 1.5288
2022-06-28 14:43:44 - train: epoch 0085, iter [01100, 05004], lr: 0.020378, loss: 1.4868
2022-06-28 14:44:18 - train: epoch 0085, iter [01200, 05004], lr: 0.020357, loss: 1.3971
2022-06-28 14:44:51 - train: epoch 0085, iter [01300, 05004], lr: 0.020336, loss: 1.3380
2022-06-28 14:45:26 - train: epoch 0085, iter [01400, 05004], lr: 0.020315, loss: 1.3980
2022-06-28 14:46:00 - train: epoch 0085, iter [01500, 05004], lr: 0.020294, loss: 1.3491
2022-06-28 14:46:34 - train: epoch 0085, iter [01600, 05004], lr: 0.020273, loss: 1.3750
2022-06-28 14:47:08 - train: epoch 0085, iter [01700, 05004], lr: 0.020252, loss: 1.7179
2022-06-28 14:47:42 - train: epoch 0085, iter [01800, 05004], lr: 0.020231, loss: 1.1606
2022-06-28 14:48:16 - train: epoch 0085, iter [01900, 05004], lr: 0.020210, loss: 1.3304
2022-06-28 14:48:49 - train: epoch 0085, iter [02000, 05004], lr: 0.020189, loss: 1.4731
2022-06-28 14:49:22 - train: epoch 0085, iter [02100, 05004], lr: 0.020168, loss: 1.0647
2022-06-28 14:49:58 - train: epoch 0085, iter [02200, 05004], lr: 0.020147, loss: 1.5759
2022-06-28 14:50:32 - train: epoch 0085, iter [02300, 05004], lr: 0.020126, loss: 1.1958
2022-06-28 14:51:05 - train: epoch 0085, iter [02400, 05004], lr: 0.020105, loss: 1.5618
2022-06-28 14:51:40 - train: epoch 0085, iter [02500, 05004], lr: 0.020084, loss: 1.7401
2022-06-28 14:52:14 - train: epoch 0085, iter [02600, 05004], lr: 0.020063, loss: 1.4535
2022-06-28 14:52:48 - train: epoch 0085, iter [02700, 05004], lr: 0.020042, loss: 1.2473
2022-06-28 14:53:21 - train: epoch 0085, iter [02800, 05004], lr: 0.020021, loss: 1.6764
2022-06-28 14:53:56 - train: epoch 0085, iter [02900, 05004], lr: 0.020000, loss: 1.5089
2022-06-28 14:54:30 - train: epoch 0085, iter [03000, 05004], lr: 0.019979, loss: 1.5823
2022-06-28 14:55:05 - train: epoch 0085, iter [03100, 05004], lr: 0.019959, loss: 1.3923
2022-06-28 14:55:39 - train: epoch 0085, iter [03200, 05004], lr: 0.019938, loss: 1.2979
2022-06-28 14:56:12 - train: epoch 0085, iter [03300, 05004], lr: 0.019917, loss: 1.3935
2022-06-28 14:56:47 - train: epoch 0085, iter [03400, 05004], lr: 0.019896, loss: 1.5578
2022-06-28 14:57:21 - train: epoch 0085, iter [03500, 05004], lr: 0.019875, loss: 1.4866
2022-06-28 14:57:54 - train: epoch 0085, iter [03600, 05004], lr: 0.019854, loss: 1.4741
2022-06-28 14:58:29 - train: epoch 0085, iter [03700, 05004], lr: 0.019833, loss: 1.3020
2022-06-28 14:59:03 - train: epoch 0085, iter [03800, 05004], lr: 0.019812, loss: 1.4038
2022-06-28 14:59:37 - train: epoch 0085, iter [03900, 05004], lr: 0.019792, loss: 1.3385
2022-06-28 15:00:11 - train: epoch 0085, iter [04000, 05004], lr: 0.019771, loss: 1.7248
2022-06-28 15:00:45 - train: epoch 0085, iter [04100, 05004], lr: 0.019750, loss: 1.3569
2022-06-28 15:01:19 - train: epoch 0085, iter [04200, 05004], lr: 0.019729, loss: 1.5668
2022-06-28 15:01:53 - train: epoch 0085, iter [04300, 05004], lr: 0.019708, loss: 1.3216
2022-06-28 15:02:27 - train: epoch 0085, iter [04400, 05004], lr: 0.019687, loss: 1.4069
2022-06-28 15:03:01 - train: epoch 0085, iter [04500, 05004], lr: 0.019667, loss: 1.6035
2022-06-28 15:03:34 - train: epoch 0085, iter [04600, 05004], lr: 0.019646, loss: 1.4641
2022-06-28 15:04:09 - train: epoch 0085, iter [04700, 05004], lr: 0.019625, loss: 1.7071
2022-06-28 15:04:43 - train: epoch 0085, iter [04800, 05004], lr: 0.019604, loss: 1.2189
2022-06-28 15:05:17 - train: epoch 0085, iter [04900, 05004], lr: 0.019584, loss: 1.3800
2022-06-28 15:05:50 - train: epoch 0085, iter [05000, 05004], lr: 0.019563, loss: 1.3709
2022-06-28 15:05:51 - train: epoch 085, train_loss: 1.4101
2022-06-28 15:07:07 - eval: epoch: 085, acc1: 69.464%, acc5: 89.400%, test_loss: 1.2423, per_image_load_time: 2.493ms, per_image_inference_time: 0.396ms
2022-06-28 15:07:08 - until epoch: 085, best_acc1: 69.464%
2022-06-28 15:07:08 - epoch 086 lr: 0.019562
2022-06-28 15:07:47 - train: epoch 0086, iter [00100, 05004], lr: 0.019541, loss: 1.1870
2022-06-28 15:08:20 - train: epoch 0086, iter [00200, 05004], lr: 0.019520, loss: 1.2809
2022-06-28 15:08:53 - train: epoch 0086, iter [00300, 05004], lr: 0.019500, loss: 1.5372
2022-06-28 15:09:27 - train: epoch 0086, iter [00400, 05004], lr: 0.019479, loss: 1.4092
2022-06-28 15:10:01 - train: epoch 0086, iter [00500, 05004], lr: 0.019458, loss: 1.3845
2022-06-28 15:10:35 - train: epoch 0086, iter [00600, 05004], lr: 0.019438, loss: 1.3382
2022-06-28 15:11:09 - train: epoch 0086, iter [00700, 05004], lr: 0.019417, loss: 1.2761
2022-06-28 15:11:43 - train: epoch 0086, iter [00800, 05004], lr: 0.019396, loss: 1.5510
2022-06-28 15:12:16 - train: epoch 0086, iter [00900, 05004], lr: 0.019375, loss: 1.5343
2022-06-28 15:12:50 - train: epoch 0086, iter [01000, 05004], lr: 0.019355, loss: 1.4339
2022-06-28 15:13:25 - train: epoch 0086, iter [01100, 05004], lr: 0.019334, loss: 1.4278
2022-06-28 15:13:58 - train: epoch 0086, iter [01200, 05004], lr: 0.019313, loss: 1.2474
2022-06-28 15:14:32 - train: epoch 0086, iter [01300, 05004], lr: 0.019293, loss: 1.4598
2022-06-28 15:15:06 - train: epoch 0086, iter [01400, 05004], lr: 0.019272, loss: 1.3416
2022-06-28 15:15:40 - train: epoch 0086, iter [01500, 05004], lr: 0.019252, loss: 1.2435
2022-06-28 15:16:14 - train: epoch 0086, iter [01600, 05004], lr: 0.019231, loss: 1.4662
2022-06-28 15:16:48 - train: epoch 0086, iter [01700, 05004], lr: 0.019210, loss: 1.2844
2022-06-28 15:17:22 - train: epoch 0086, iter [01800, 05004], lr: 0.019190, loss: 1.5078
2022-06-28 15:17:55 - train: epoch 0086, iter [01900, 05004], lr: 0.019169, loss: 1.4979
2022-06-28 15:18:29 - train: epoch 0086, iter [02000, 05004], lr: 0.019149, loss: 1.3626
2022-06-28 15:19:03 - train: epoch 0086, iter [02100, 05004], lr: 0.019128, loss: 1.2027
2022-06-28 15:19:38 - train: epoch 0086, iter [02200, 05004], lr: 0.019107, loss: 1.4509
2022-06-28 15:20:13 - train: epoch 0086, iter [02300, 05004], lr: 0.019087, loss: 1.3563
2022-06-28 15:20:46 - train: epoch 0086, iter [02400, 05004], lr: 0.019066, loss: 1.1240
2022-06-28 15:21:19 - train: epoch 0086, iter [02500, 05004], lr: 0.019046, loss: 1.3334
2022-06-28 15:21:54 - train: epoch 0086, iter [02600, 05004], lr: 0.019025, loss: 1.4868
2022-06-28 15:22:28 - train: epoch 0086, iter [02700, 05004], lr: 0.019005, loss: 1.3693
2022-06-28 15:23:01 - train: epoch 0086, iter [02800, 05004], lr: 0.018984, loss: 1.3975
2022-06-28 15:23:35 - train: epoch 0086, iter [02900, 05004], lr: 0.018964, loss: 1.2148
2022-06-28 15:24:09 - train: epoch 0086, iter [03000, 05004], lr: 0.018943, loss: 1.4541
2022-06-28 15:24:42 - train: epoch 0086, iter [03100, 05004], lr: 0.018923, loss: 1.3416
2022-06-28 15:25:17 - train: epoch 0086, iter [03200, 05004], lr: 0.018902, loss: 1.4949
2022-06-28 15:25:51 - train: epoch 0086, iter [03300, 05004], lr: 0.018882, loss: 1.4802
2022-06-28 15:26:25 - train: epoch 0086, iter [03400, 05004], lr: 0.018861, loss: 1.3963
2022-06-28 15:26:59 - train: epoch 0086, iter [03500, 05004], lr: 0.018841, loss: 1.4573
2022-06-28 15:27:33 - train: epoch 0086, iter [03600, 05004], lr: 0.018820, loss: 1.4189
2022-06-28 15:28:07 - train: epoch 0086, iter [03700, 05004], lr: 0.018800, loss: 1.4274
2022-06-28 15:28:41 - train: epoch 0086, iter [03800, 05004], lr: 0.018779, loss: 1.4390
2022-06-28 15:29:15 - train: epoch 0086, iter [03900, 05004], lr: 0.018759, loss: 1.4628
2022-06-28 15:29:48 - train: epoch 0086, iter [04000, 05004], lr: 0.018739, loss: 1.4850
2022-06-28 15:30:22 - train: epoch 0086, iter [04100, 05004], lr: 0.018718, loss: 1.4074
2022-06-28 15:30:57 - train: epoch 0086, iter [04200, 05004], lr: 0.018698, loss: 1.3965
2022-06-28 15:31:31 - train: epoch 0086, iter [04300, 05004], lr: 0.018677, loss: 1.5522
2022-06-28 15:32:05 - train: epoch 0086, iter [04400, 05004], lr: 0.018657, loss: 1.3283
2022-06-28 15:32:38 - train: epoch 0086, iter [04500, 05004], lr: 0.018637, loss: 1.2817
2022-06-28 15:33:13 - train: epoch 0086, iter [04600, 05004], lr: 0.018616, loss: 1.3329
2022-06-28 15:33:48 - train: epoch 0086, iter [04700, 05004], lr: 0.018596, loss: 1.3680
2022-06-28 15:34:22 - train: epoch 0086, iter [04800, 05004], lr: 0.018575, loss: 1.4537
2022-06-28 15:34:56 - train: epoch 0086, iter [04900, 05004], lr: 0.018555, loss: 1.3630
2022-06-28 15:35:28 - train: epoch 0086, iter [05000, 05004], lr: 0.018535, loss: 1.3931
2022-06-28 15:35:29 - train: epoch 086, train_loss: 1.3911
2022-06-28 15:36:45 - eval: epoch: 086, acc1: 69.362%, acc5: 89.276%, test_loss: 1.2520, per_image_load_time: 2.525ms, per_image_inference_time: 0.383ms
2022-06-28 15:36:45 - until epoch: 086, best_acc1: 69.464%
2022-06-28 15:36:45 - epoch 087 lr: 0.018534
2022-06-28 15:37:24 - train: epoch 0087, iter [00100, 05004], lr: 0.018514, loss: 1.3981
2022-06-28 15:37:58 - train: epoch 0087, iter [00200, 05004], lr: 0.018493, loss: 1.1916
2022-06-28 15:38:32 - train: epoch 0087, iter [00300, 05004], lr: 0.018473, loss: 1.5479
2022-06-28 15:39:05 - train: epoch 0087, iter [00400, 05004], lr: 0.018453, loss: 1.4788
2022-06-28 15:39:40 - train: epoch 0087, iter [00500, 05004], lr: 0.018432, loss: 1.1116
2022-06-28 15:40:14 - train: epoch 0087, iter [00600, 05004], lr: 0.018412, loss: 1.3883
2022-06-28 15:40:47 - train: epoch 0087, iter [00700, 05004], lr: 0.018392, loss: 1.1497
2022-06-28 15:41:21 - train: epoch 0087, iter [00800, 05004], lr: 0.018372, loss: 1.3974
2022-06-28 15:41:55 - train: epoch 0087, iter [00900, 05004], lr: 0.018351, loss: 1.4100
2022-06-28 15:42:29 - train: epoch 0087, iter [01000, 05004], lr: 0.018331, loss: 1.3496
2022-06-28 15:43:03 - train: epoch 0087, iter [01100, 05004], lr: 0.018311, loss: 1.4496
2022-06-28 15:43:37 - train: epoch 0087, iter [01200, 05004], lr: 0.018291, loss: 1.4731
2022-06-28 15:44:10 - train: epoch 0087, iter [01300, 05004], lr: 0.018270, loss: 1.6171
2022-06-28 15:44:45 - train: epoch 0087, iter [01400, 05004], lr: 0.018250, loss: 1.3960
2022-06-28 15:45:20 - train: epoch 0087, iter [01500, 05004], lr: 0.018230, loss: 1.3070
2022-06-28 15:45:52 - train: epoch 0087, iter [01600, 05004], lr: 0.018210, loss: 1.2531
2022-06-28 15:46:28 - train: epoch 0087, iter [01700, 05004], lr: 0.018190, loss: 1.3969
2022-06-28 15:47:01 - train: epoch 0087, iter [01800, 05004], lr: 0.018169, loss: 1.4331
2022-06-28 15:47:35 - train: epoch 0087, iter [01900, 05004], lr: 0.018149, loss: 1.4121
2022-06-28 15:48:09 - train: epoch 0087, iter [02000, 05004], lr: 0.018129, loss: 1.2986
2022-06-28 15:48:43 - train: epoch 0087, iter [02100, 05004], lr: 0.018109, loss: 1.4587
2022-06-28 15:49:18 - train: epoch 0087, iter [02200, 05004], lr: 0.018089, loss: 1.4118
2022-06-28 15:49:53 - train: epoch 0087, iter [02300, 05004], lr: 0.018069, loss: 1.4101
2022-06-28 15:50:26 - train: epoch 0087, iter [02400, 05004], lr: 0.018049, loss: 1.3580
2022-06-28 15:51:01 - train: epoch 0087, iter [02500, 05004], lr: 0.018028, loss: 1.3961
2022-06-28 15:51:35 - train: epoch 0087, iter [02600, 05004], lr: 0.018008, loss: 1.3995
2022-06-28 15:52:09 - train: epoch 0087, iter [02700, 05004], lr: 0.017988, loss: 1.2765
2022-06-28 15:52:44 - train: epoch 0087, iter [02800, 05004], lr: 0.017968, loss: 1.2631
2022-06-28 15:53:17 - train: epoch 0087, iter [02900, 05004], lr: 0.017948, loss: 1.1918
2022-06-28 15:53:52 - train: epoch 0087, iter [03000, 05004], lr: 0.017928, loss: 1.4601
2022-06-28 15:54:27 - train: epoch 0087, iter [03100, 05004], lr: 0.017908, loss: 1.4125
2022-06-28 15:55:01 - train: epoch 0087, iter [03200, 05004], lr: 0.017888, loss: 1.4260
2022-06-28 15:55:35 - train: epoch 0087, iter [03300, 05004], lr: 0.017868, loss: 1.4444
2022-06-28 15:56:09 - train: epoch 0087, iter [03400, 05004], lr: 0.017848, loss: 1.4053
2022-06-28 15:56:43 - train: epoch 0087, iter [03500, 05004], lr: 0.017828, loss: 1.1908
2022-06-28 15:57:18 - train: epoch 0087, iter [03600, 05004], lr: 0.017808, loss: 1.1420
2022-06-28 15:57:52 - train: epoch 0087, iter [03700, 05004], lr: 0.017788, loss: 1.3409
2022-06-28 15:58:26 - train: epoch 0087, iter [03800, 05004], lr: 0.017768, loss: 1.3857
2022-06-28 15:59:00 - train: epoch 0087, iter [03900, 05004], lr: 0.017748, loss: 1.3156
2022-06-28 15:59:34 - train: epoch 0087, iter [04000, 05004], lr: 0.017728, loss: 1.4832
2022-06-28 16:00:09 - train: epoch 0087, iter [04100, 05004], lr: 0.017708, loss: 1.5266
2022-06-28 16:00:43 - train: epoch 0087, iter [04200, 05004], lr: 0.017688, loss: 1.4787
2022-06-28 16:01:18 - train: epoch 0087, iter [04300, 05004], lr: 0.017668, loss: 1.3383
2022-06-28 16:01:52 - train: epoch 0087, iter [04400, 05004], lr: 0.017648, loss: 1.4277
2022-06-28 16:02:26 - train: epoch 0087, iter [04500, 05004], lr: 0.017628, loss: 1.4790
2022-06-28 16:03:00 - train: epoch 0087, iter [04600, 05004], lr: 0.017608, loss: 1.4946
2022-06-28 16:03:34 - train: epoch 0087, iter [04700, 05004], lr: 0.017588, loss: 1.3883
2022-06-28 16:04:09 - train: epoch 0087, iter [04800, 05004], lr: 0.017568, loss: 1.3218
2022-06-28 16:04:43 - train: epoch 0087, iter [04900, 05004], lr: 0.017548, loss: 1.3759
2022-06-28 16:05:16 - train: epoch 0087, iter [05000, 05004], lr: 0.017528, loss: 1.4676
2022-06-28 16:05:18 - train: epoch 087, train_loss: 1.3746
2022-06-28 16:06:32 - eval: epoch: 087, acc1: 69.652%, acc5: 89.218%, test_loss: 1.2454, per_image_load_time: 1.606ms, per_image_inference_time: 0.404ms
2022-06-28 16:06:33 - until epoch: 087, best_acc1: 69.652%
2022-06-28 16:06:33 - epoch 088 lr: 0.017527
2022-06-28 16:07:13 - train: epoch 0088, iter [00100, 05004], lr: 0.017508, loss: 1.2086
2022-06-28 16:07:45 - train: epoch 0088, iter [00200, 05004], lr: 0.017488, loss: 1.2586
2022-06-28 16:08:20 - train: epoch 0088, iter [00300, 05004], lr: 0.017468, loss: 1.4133
2022-06-28 16:08:54 - train: epoch 0088, iter [00400, 05004], lr: 0.017448, loss: 1.2938
2022-06-28 16:09:27 - train: epoch 0088, iter [00500, 05004], lr: 0.017428, loss: 1.2652
2022-06-28 16:10:02 - train: epoch 0088, iter [00600, 05004], lr: 0.017408, loss: 1.3722
2022-06-28 16:10:36 - train: epoch 0088, iter [00700, 05004], lr: 0.017389, loss: 1.2804
2022-06-28 16:11:09 - train: epoch 0088, iter [00800, 05004], lr: 0.017369, loss: 1.2921
2022-06-28 16:11:43 - train: epoch 0088, iter [00900, 05004], lr: 0.017349, loss: 1.4598
2022-06-28 16:12:17 - train: epoch 0088, iter [01000, 05004], lr: 0.017329, loss: 1.2914
2022-06-28 16:12:51 - train: epoch 0088, iter [01100, 05004], lr: 0.017309, loss: 1.3517
2022-06-28 16:13:25 - train: epoch 0088, iter [01200, 05004], lr: 0.017290, loss: 1.3047
2022-06-28 16:14:00 - train: epoch 0088, iter [01300, 05004], lr: 0.017270, loss: 1.4232
2022-06-28 16:14:34 - train: epoch 0088, iter [01400, 05004], lr: 0.017250, loss: 1.2141
2022-06-28 16:15:08 - train: epoch 0088, iter [01500, 05004], lr: 0.017230, loss: 1.3119
2022-06-28 16:15:43 - train: epoch 0088, iter [01600, 05004], lr: 0.017210, loss: 1.0996
2022-06-28 16:16:16 - train: epoch 0088, iter [01700, 05004], lr: 0.017191, loss: 1.3657
2022-06-28 16:16:50 - train: epoch 0088, iter [01800, 05004], lr: 0.017171, loss: 1.2712
2022-06-28 16:17:24 - train: epoch 0088, iter [01900, 05004], lr: 0.017151, loss: 1.4702
2022-06-28 16:17:58 - train: epoch 0088, iter [02000, 05004], lr: 0.017132, loss: 1.2175
2022-06-28 16:18:32 - train: epoch 0088, iter [02100, 05004], lr: 0.017112, loss: 1.3509
2022-06-28 16:19:06 - train: epoch 0088, iter [02200, 05004], lr: 0.017092, loss: 1.3653
2022-06-28 16:19:40 - train: epoch 0088, iter [02300, 05004], lr: 0.017072, loss: 1.1941
2022-06-28 16:20:15 - train: epoch 0088, iter [02400, 05004], lr: 0.017053, loss: 1.4534
2022-06-28 16:20:48 - train: epoch 0088, iter [02500, 05004], lr: 0.017033, loss: 1.3938
2022-06-28 16:21:23 - train: epoch 0088, iter [02600, 05004], lr: 0.017013, loss: 1.4229
2022-06-28 16:21:57 - train: epoch 0088, iter [02700, 05004], lr: 0.016994, loss: 1.4410
2022-06-28 16:22:31 - train: epoch 0088, iter [02800, 05004], lr: 0.016974, loss: 1.4161
2022-06-28 16:23:06 - train: epoch 0088, iter [02900, 05004], lr: 0.016955, loss: 1.3032
2022-06-28 16:23:40 - train: epoch 0088, iter [03000, 05004], lr: 0.016935, loss: 1.4494
2022-06-28 16:24:14 - train: epoch 0088, iter [03100, 05004], lr: 0.016915, loss: 1.3344
2022-06-28 16:24:48 - train: epoch 0088, iter [03200, 05004], lr: 0.016896, loss: 1.2541
2022-06-28 16:25:23 - train: epoch 0088, iter [03300, 05004], lr: 0.016876, loss: 1.2128
2022-06-28 16:25:57 - train: epoch 0088, iter [03400, 05004], lr: 0.016856, loss: 1.2696
2022-06-28 16:26:31 - train: epoch 0088, iter [03500, 05004], lr: 0.016837, loss: 1.2531
2022-06-28 16:27:06 - train: epoch 0088, iter [03600, 05004], lr: 0.016817, loss: 1.3976
2022-06-28 16:27:40 - train: epoch 0088, iter [03700, 05004], lr: 0.016798, loss: 1.5001
2022-06-28 16:28:15 - train: epoch 0088, iter [03800, 05004], lr: 0.016778, loss: 1.3157
2022-06-28 16:28:48 - train: epoch 0088, iter [03900, 05004], lr: 0.016759, loss: 1.5239
2022-06-28 16:29:23 - train: epoch 0088, iter [04000, 05004], lr: 0.016739, loss: 1.2037
2022-06-28 16:29:57 - train: epoch 0088, iter [04100, 05004], lr: 0.016720, loss: 1.4405
2022-06-28 16:30:30 - train: epoch 0088, iter [04200, 05004], lr: 0.016700, loss: 1.4157
2022-06-28 16:31:04 - train: epoch 0088, iter [04300, 05004], lr: 0.016681, loss: 1.2523
2022-06-28 16:31:38 - train: epoch 0088, iter [04400, 05004], lr: 0.016661, loss: 1.2208
2022-06-28 16:32:13 - train: epoch 0088, iter [04500, 05004], lr: 0.016642, loss: 1.3930
2022-06-28 16:32:47 - train: epoch 0088, iter [04600, 05004], lr: 0.016622, loss: 1.5691
2022-06-28 16:33:22 - train: epoch 0088, iter [04700, 05004], lr: 0.016603, loss: 1.4097
2022-06-28 16:33:56 - train: epoch 0088, iter [04800, 05004], lr: 0.016583, loss: 1.2745
2022-06-28 16:34:31 - train: epoch 0088, iter [04900, 05004], lr: 0.016564, loss: 1.1323
2022-06-28 16:35:04 - train: epoch 0088, iter [05000, 05004], lr: 0.016544, loss: 1.3663
2022-06-28 16:35:05 - train: epoch 088, train_loss: 1.3593
2022-06-28 16:36:21 - eval: epoch: 088, acc1: 70.268%, acc5: 89.682%, test_loss: 1.2062, per_image_load_time: 2.551ms, per_image_inference_time: 0.370ms
2022-06-28 16:36:21 - until epoch: 088, best_acc1: 70.268%
2022-06-28 16:36:21 - epoch 089 lr: 0.016543
2022-06-28 16:37:00 - train: epoch 0089, iter [00100, 05004], lr: 0.016524, loss: 1.5215
2022-06-28 16:37:34 - train: epoch 0089, iter [00200, 05004], lr: 0.016505, loss: 1.0544
2022-06-28 16:38:08 - train: epoch 0089, iter [00300, 05004], lr: 0.016485, loss: 1.4194
2022-06-28 16:38:42 - train: epoch 0089, iter [00400, 05004], lr: 0.016466, loss: 1.2028
2022-06-28 16:39:17 - train: epoch 0089, iter [00500, 05004], lr: 0.016446, loss: 1.2952
2022-06-28 16:39:51 - train: epoch 0089, iter [00600, 05004], lr: 0.016427, loss: 1.2552
2022-06-28 16:40:24 - train: epoch 0089, iter [00700, 05004], lr: 0.016408, loss: 1.3950
2022-06-28 16:40:58 - train: epoch 0089, iter [00800, 05004], lr: 0.016388, loss: 1.4867
2022-06-28 16:41:32 - train: epoch 0089, iter [00900, 05004], lr: 0.016369, loss: 1.4130
2022-06-28 16:42:06 - train: epoch 0089, iter [01000, 05004], lr: 0.016350, loss: 1.4976
2022-06-28 16:42:40 - train: epoch 0089, iter [01100, 05004], lr: 0.016330, loss: 1.2692
2022-06-28 16:43:14 - train: epoch 0089, iter [01200, 05004], lr: 0.016311, loss: 1.4871
2022-06-28 16:43:48 - train: epoch 0089, iter [01300, 05004], lr: 0.016292, loss: 1.3657
2022-06-28 16:44:23 - train: epoch 0089, iter [01400, 05004], lr: 0.016272, loss: 1.4160
2022-06-28 16:44:57 - train: epoch 0089, iter [01500, 05004], lr: 0.016253, loss: 1.3721
2022-06-28 16:45:31 - train: epoch 0089, iter [01600, 05004], lr: 0.016234, loss: 1.2810
2022-06-28 16:46:06 - train: epoch 0089, iter [01700, 05004], lr: 0.016214, loss: 1.3461
2022-06-28 16:46:40 - train: epoch 0089, iter [01800, 05004], lr: 0.016195, loss: 1.3178
2022-06-28 16:47:13 - train: epoch 0089, iter [01900, 05004], lr: 0.016176, loss: 1.2456
2022-06-28 16:47:49 - train: epoch 0089, iter [02000, 05004], lr: 0.016157, loss: 1.2921
2022-06-28 16:48:22 - train: epoch 0089, iter [02100, 05004], lr: 0.016137, loss: 1.4438
2022-06-28 16:48:56 - train: epoch 0089, iter [02200, 05004], lr: 0.016118, loss: 1.5087
2022-06-28 16:49:31 - train: epoch 0089, iter [02300, 05004], lr: 0.016099, loss: 1.1936
2022-06-28 16:50:04 - train: epoch 0089, iter [02400, 05004], lr: 0.016080, loss: 1.4682
2022-06-28 16:50:39 - train: epoch 0089, iter [02500, 05004], lr: 0.016060, loss: 1.3207
2022-06-28 16:51:13 - train: epoch 0089, iter [02600, 05004], lr: 0.016041, loss: 1.3860
2022-06-28 16:51:47 - train: epoch 0089, iter [02700, 05004], lr: 0.016022, loss: 1.3001
2022-06-28 16:52:22 - train: epoch 0089, iter [02800, 05004], lr: 0.016003, loss: 1.4135
2022-06-28 16:52:56 - train: epoch 0089, iter [02900, 05004], lr: 0.015984, loss: 1.2743
2022-06-28 16:53:31 - train: epoch 0089, iter [03000, 05004], lr: 0.015964, loss: 1.4131
2022-06-28 16:54:06 - train: epoch 0089, iter [03100, 05004], lr: 0.015945, loss: 1.4008
2022-06-28 16:54:39 - train: epoch 0089, iter [03200, 05004], lr: 0.015926, loss: 1.5139
2022-06-28 16:55:13 - train: epoch 0089, iter [03300, 05004], lr: 0.015907, loss: 1.5687
2022-06-28 16:55:47 - train: epoch 0089, iter [03400, 05004], lr: 0.015888, loss: 1.2332
2022-06-28 16:56:21 - train: epoch 0089, iter [03500, 05004], lr: 0.015869, loss: 1.1065
2022-06-28 16:56:55 - train: epoch 0089, iter [03600, 05004], lr: 0.015850, loss: 1.3797
2022-06-28 16:57:30 - train: epoch 0089, iter [03700, 05004], lr: 0.015831, loss: 1.5713
2022-06-28 16:58:04 - train: epoch 0089, iter [03800, 05004], lr: 0.015811, loss: 1.3304
2022-06-28 16:58:39 - train: epoch 0089, iter [03900, 05004], lr: 0.015792, loss: 1.4676
2022-06-28 16:59:13 - train: epoch 0089, iter [04000, 05004], lr: 0.015773, loss: 1.1335
2022-06-28 16:59:47 - train: epoch 0089, iter [04100, 05004], lr: 0.015754, loss: 1.5313
2022-06-28 17:00:21 - train: epoch 0089, iter [04200, 05004], lr: 0.015735, loss: 1.3081
2022-06-28 17:00:56 - train: epoch 0089, iter [04300, 05004], lr: 0.015716, loss: 1.3567
2022-06-28 17:01:30 - train: epoch 0089, iter [04400, 05004], lr: 0.015697, loss: 1.3437
2022-06-28 17:02:04 - train: epoch 0089, iter [04500, 05004], lr: 0.015678, loss: 1.1968
2022-06-28 17:02:39 - train: epoch 0089, iter [04600, 05004], lr: 0.015659, loss: 1.3364
2022-06-28 17:03:14 - train: epoch 0089, iter [04700, 05004], lr: 0.015640, loss: 1.2938
2022-06-28 17:03:48 - train: epoch 0089, iter [04800, 05004], lr: 0.015621, loss: 1.3346
2022-06-28 17:04:22 - train: epoch 0089, iter [04900, 05004], lr: 0.015602, loss: 1.3376
2022-06-28 17:04:56 - train: epoch 0089, iter [05000, 05004], lr: 0.015583, loss: 1.1362
2022-06-28 17:04:57 - train: epoch 089, train_loss: 1.3430
2022-06-28 17:06:13 - eval: epoch: 089, acc1: 70.608%, acc5: 90.028%, test_loss: 1.1838, per_image_load_time: 2.578ms, per_image_inference_time: 0.365ms
2022-06-28 17:06:14 - until epoch: 089, best_acc1: 70.608%
2022-06-28 17:06:14 - epoch 090 lr: 0.015582
2022-06-28 17:06:53 - train: epoch 0090, iter [00100, 05004], lr: 0.015563, loss: 1.3471
2022-06-28 17:07:27 - train: epoch 0090, iter [00200, 05004], lr: 0.015544, loss: 1.2892
2022-06-28 17:08:01 - train: epoch 0090, iter [00300, 05004], lr: 0.015525, loss: 1.2428
2022-06-28 17:08:36 - train: epoch 0090, iter [00400, 05004], lr: 0.015506, loss: 1.2063
2022-06-28 17:09:10 - train: epoch 0090, iter [00500, 05004], lr: 0.015488, loss: 1.2802
2022-06-28 17:09:45 - train: epoch 0090, iter [00600, 05004], lr: 0.015469, loss: 1.3771
2022-06-28 17:10:19 - train: epoch 0090, iter [00700, 05004], lr: 0.015450, loss: 1.3892
2022-06-28 17:10:54 - train: epoch 0090, iter [00800, 05004], lr: 0.015431, loss: 1.3316
2022-06-28 17:11:29 - train: epoch 0090, iter [00900, 05004], lr: 0.015412, loss: 1.2363
2022-06-28 17:12:03 - train: epoch 0090, iter [01000, 05004], lr: 0.015393, loss: 1.1619
2022-06-28 17:12:38 - train: epoch 0090, iter [01100, 05004], lr: 0.015374, loss: 1.4513
2022-06-28 17:13:12 - train: epoch 0090, iter [01200, 05004], lr: 0.015355, loss: 1.2995
2022-06-28 17:13:46 - train: epoch 0090, iter [01300, 05004], lr: 0.015336, loss: 1.4527
2022-06-28 17:14:20 - train: epoch 0090, iter [01400, 05004], lr: 0.015318, loss: 1.1790
2022-06-28 17:14:54 - train: epoch 0090, iter [01500, 05004], lr: 0.015299, loss: 1.6147
2022-06-28 17:15:29 - train: epoch 0090, iter [01600, 05004], lr: 0.015280, loss: 1.2865
2022-06-28 17:16:03 - train: epoch 0090, iter [01700, 05004], lr: 0.015261, loss: 1.1139
2022-06-28 17:16:39 - train: epoch 0090, iter [01800, 05004], lr: 0.015242, loss: 1.1833
2022-06-28 17:17:13 - train: epoch 0090, iter [01900, 05004], lr: 0.015223, loss: 1.1584
2022-06-28 17:17:47 - train: epoch 0090, iter [02000, 05004], lr: 0.015205, loss: 1.4651
2022-06-28 17:18:22 - train: epoch 0090, iter [02100, 05004], lr: 0.015186, loss: 1.4738
2022-06-28 17:18:56 - train: epoch 0090, iter [02200, 05004], lr: 0.015167, loss: 1.3878
2022-06-28 17:19:32 - train: epoch 0090, iter [02300, 05004], lr: 0.015148, loss: 1.5195
2022-06-28 17:20:06 - train: epoch 0090, iter [02400, 05004], lr: 0.015130, loss: 1.2997
2022-06-28 17:20:41 - train: epoch 0090, iter [02500, 05004], lr: 0.015111, loss: 1.3917
2022-06-28 17:21:14 - train: epoch 0090, iter [02600, 05004], lr: 0.015092, loss: 1.3836
2022-06-28 17:21:48 - train: epoch 0090, iter [02700, 05004], lr: 0.015073, loss: 1.3229
2022-06-28 17:22:22 - train: epoch 0090, iter [02800, 05004], lr: 0.015055, loss: 1.3188
2022-06-28 17:22:57 - train: epoch 0090, iter [02900, 05004], lr: 0.015036, loss: 1.3285
2022-06-28 17:23:31 - train: epoch 0090, iter [03000, 05004], lr: 0.015017, loss: 1.3283
2022-06-28 17:24:06 - train: epoch 0090, iter [03100, 05004], lr: 0.014999, loss: 1.2155
2022-06-28 17:24:40 - train: epoch 0090, iter [03200, 05004], lr: 0.014980, loss: 1.1651
2022-06-28 17:25:15 - train: epoch 0090, iter [03300, 05004], lr: 0.014961, loss: 1.5136
2022-06-28 17:25:49 - train: epoch 0090, iter [03400, 05004], lr: 0.014943, loss: 1.1798
2022-06-28 17:26:23 - train: epoch 0090, iter [03500, 05004], lr: 0.014924, loss: 1.2959
2022-06-28 17:26:58 - train: epoch 0090, iter [03600, 05004], lr: 0.014905, loss: 1.1388
2022-06-28 17:27:33 - train: epoch 0090, iter [03700, 05004], lr: 0.014887, loss: 1.3604
2022-06-28 17:28:07 - train: epoch 0090, iter [03800, 05004], lr: 0.014868, loss: 1.3093
2022-06-28 17:28:42 - train: epoch 0090, iter [03900, 05004], lr: 0.014849, loss: 1.0244
2022-06-28 17:29:17 - train: epoch 0090, iter [04000, 05004], lr: 0.014831, loss: 1.3609
2022-06-28 17:29:51 - train: epoch 0090, iter [04100, 05004], lr: 0.014812, loss: 1.6341
2022-06-28 17:30:27 - train: epoch 0090, iter [04200, 05004], lr: 0.014794, loss: 1.4958
2022-06-28 17:31:01 - train: epoch 0090, iter [04300, 05004], lr: 0.014775, loss: 1.3310
2022-06-28 17:31:35 - train: epoch 0090, iter [04400, 05004], lr: 0.014757, loss: 1.2619
2022-06-28 17:32:10 - train: epoch 0090, iter [04500, 05004], lr: 0.014738, loss: 1.2421
2022-06-28 17:32:45 - train: epoch 0090, iter [04600, 05004], lr: 0.014719, loss: 1.2888
2022-06-28 17:33:19 - train: epoch 0090, iter [04700, 05004], lr: 0.014701, loss: 1.2101
2022-06-28 17:33:53 - train: epoch 0090, iter [04800, 05004], lr: 0.014682, loss: 1.4224
2022-06-28 17:34:28 - train: epoch 0090, iter [04900, 05004], lr: 0.014664, loss: 1.2585
2022-06-28 17:35:02 - train: epoch 0090, iter [05000, 05004], lr: 0.014645, loss: 1.3085
2022-06-28 17:35:03 - train: epoch 090, train_loss: 1.3255
2022-06-28 17:36:20 - eval: epoch: 090, acc1: 70.482%, acc5: 89.820%, test_loss: 1.1919, per_image_load_time: 2.586ms, per_image_inference_time: 0.405ms
2022-06-28 17:36:21 - until epoch: 090, best_acc1: 70.608%
2022-06-28 17:36:21 - epoch 091 lr: 0.014644
2022-06-28 17:37:01 - train: epoch 0091, iter [00100, 05004], lr: 0.014626, loss: 1.1951
2022-06-28 17:37:35 - train: epoch 0091, iter [00200, 05004], lr: 0.014608, loss: 1.2785
2022-06-28 17:38:10 - train: epoch 0091, iter [00300, 05004], lr: 0.014589, loss: 1.2919
2022-06-28 17:38:45 - train: epoch 0091, iter [00400, 05004], lr: 0.014571, loss: 1.1777
2022-06-28 17:39:20 - train: epoch 0091, iter [00500, 05004], lr: 0.014552, loss: 1.2115
2022-06-28 17:39:54 - train: epoch 0091, iter [00600, 05004], lr: 0.014534, loss: 1.3253
2022-06-28 17:40:29 - train: epoch 0091, iter [00700, 05004], lr: 0.014515, loss: 1.2858
2022-06-28 17:41:03 - train: epoch 0091, iter [00800, 05004], lr: 0.014497, loss: 1.1296
2022-06-28 17:41:39 - train: epoch 0091, iter [00900, 05004], lr: 0.014479, loss: 1.3212
2022-06-28 17:42:13 - train: epoch 0091, iter [01000, 05004], lr: 0.014460, loss: 1.2116
2022-06-28 17:42:48 - train: epoch 0091, iter [01100, 05004], lr: 0.014442, loss: 1.2000
2022-06-28 17:43:23 - train: epoch 0091, iter [01200, 05004], lr: 0.014423, loss: 1.3223
2022-06-28 17:43:58 - train: epoch 0091, iter [01300, 05004], lr: 0.014405, loss: 1.1841
2022-06-28 17:44:33 - train: epoch 0091, iter [01400, 05004], lr: 0.014387, loss: 1.2894
2022-06-28 17:45:07 - train: epoch 0091, iter [01500, 05004], lr: 0.014368, loss: 1.3697
2022-06-28 17:45:42 - train: epoch 0091, iter [01600, 05004], lr: 0.014350, loss: 1.3367
2022-06-28 17:46:17 - train: epoch 0091, iter [01700, 05004], lr: 0.014332, loss: 1.3393
2022-06-28 17:46:52 - train: epoch 0091, iter [01800, 05004], lr: 0.014313, loss: 1.3419
2022-06-28 17:47:27 - train: epoch 0091, iter [01900, 05004], lr: 0.014295, loss: 1.1950
2022-06-28 17:48:02 - train: epoch 0091, iter [02000, 05004], lr: 0.014277, loss: 1.3255
2022-06-28 17:48:37 - train: epoch 0091, iter [02100, 05004], lr: 0.014258, loss: 1.1007
2022-06-28 17:49:11 - train: epoch 0091, iter [02200, 05004], lr: 0.014240, loss: 1.3902
2022-06-28 17:49:46 - train: epoch 0091, iter [02300, 05004], lr: 0.014222, loss: 1.3128
2022-06-28 17:50:21 - train: epoch 0091, iter [02400, 05004], lr: 0.014204, loss: 1.2870
2022-06-28 17:50:55 - train: epoch 0091, iter [02500, 05004], lr: 0.014185, loss: 1.2648
2022-06-28 17:51:31 - train: epoch 0091, iter [02600, 05004], lr: 0.014167, loss: 1.3326
2022-06-28 17:52:06 - train: epoch 0091, iter [02700, 05004], lr: 0.014149, loss: 1.1215
2022-06-28 17:52:40 - train: epoch 0091, iter [02800, 05004], lr: 0.014131, loss: 1.2748
2022-06-28 17:53:16 - train: epoch 0091, iter [02900, 05004], lr: 0.014112, loss: 1.3439
2022-06-28 17:53:51 - train: epoch 0091, iter [03000, 05004], lr: 0.014094, loss: 1.5296
2022-06-28 17:54:25 - train: epoch 0091, iter [03100, 05004], lr: 0.014076, loss: 1.1666
2022-06-28 17:55:00 - train: epoch 0091, iter [03200, 05004], lr: 0.014058, loss: 1.4636
2022-06-28 17:55:35 - train: epoch 0091, iter [03300, 05004], lr: 0.014040, loss: 1.2134
2022-06-28 17:56:09 - train: epoch 0091, iter [03400, 05004], lr: 0.014021, loss: 1.2899
2022-06-28 17:56:44 - train: epoch 0091, iter [03500, 05004], lr: 0.014003, loss: 1.3057
2022-06-28 17:57:19 - train: epoch 0091, iter [03600, 05004], lr: 0.013985, loss: 1.2578
2022-06-28 17:57:55 - train: epoch 0091, iter [03700, 05004], lr: 0.013967, loss: 1.3721
2022-06-28 17:58:29 - train: epoch 0091, iter [03800, 05004], lr: 0.013949, loss: 1.3088
2022-06-28 17:59:04 - train: epoch 0091, iter [03900, 05004], lr: 0.013931, loss: 1.2345
2022-06-28 17:59:39 - train: epoch 0091, iter [04000, 05004], lr: 0.013913, loss: 1.3211
2022-06-28 18:00:13 - train: epoch 0091, iter [04100, 05004], lr: 0.013894, loss: 1.2971
2022-06-28 18:00:48 - train: epoch 0091, iter [04200, 05004], lr: 0.013876, loss: 1.3899
2022-06-28 18:01:23 - train: epoch 0091, iter [04300, 05004], lr: 0.013858, loss: 1.4345
2022-06-28 18:01:58 - train: epoch 0091, iter [04400, 05004], lr: 0.013840, loss: 1.1377
2022-06-28 18:02:33 - train: epoch 0091, iter [04500, 05004], lr: 0.013822, loss: 1.1966
2022-06-28 18:03:07 - train: epoch 0091, iter [04600, 05004], lr: 0.013804, loss: 1.2828
2022-06-28 18:03:42 - train: epoch 0091, iter [04700, 05004], lr: 0.013786, loss: 1.4778
2022-06-28 18:04:17 - train: epoch 0091, iter [04800, 05004], lr: 0.013768, loss: 1.3404
2022-06-28 18:04:51 - train: epoch 0091, iter [04900, 05004], lr: 0.013750, loss: 1.2804
2022-06-28 18:05:24 - train: epoch 0091, iter [05000, 05004], lr: 0.013732, loss: 1.5200
2022-06-28 18:05:25 - train: epoch 091, train_loss: 1.3071
2022-06-28 18:06:41 - eval: epoch: 091, acc1: 70.938%, acc5: 90.194%, test_loss: 1.1760, per_image_load_time: 1.358ms, per_image_inference_time: 0.420ms
2022-06-28 18:06:42 - until epoch: 091, best_acc1: 70.938%
2022-06-28 18:06:42 - epoch 092 lr: 0.013731
2022-06-28 18:07:22 - train: epoch 0092, iter [00100, 05004], lr: 0.013713, loss: 1.2742
2022-06-28 18:07:58 - train: epoch 0092, iter [00200, 05004], lr: 0.013695, loss: 1.3516
2022-06-28 18:08:32 - train: epoch 0092, iter [00300, 05004], lr: 0.013677, loss: 1.3173
2022-06-28 18:09:06 - train: epoch 0092, iter [00400, 05004], lr: 0.013659, loss: 1.1620
2022-06-28 18:09:39 - train: epoch 0092, iter [00500, 05004], lr: 0.013641, loss: 1.3836
2022-06-28 18:10:12 - train: epoch 0092, iter [00600, 05004], lr: 0.013623, loss: 1.3002
2022-06-28 18:10:47 - train: epoch 0092, iter [00700, 05004], lr: 0.013605, loss: 1.2753
2022-06-28 18:11:21 - train: epoch 0092, iter [00800, 05004], lr: 0.013588, loss: 1.3775
2022-06-28 18:11:56 - train: epoch 0092, iter [00900, 05004], lr: 0.013570, loss: 1.3119
2022-06-28 18:12:32 - train: epoch 0092, iter [01000, 05004], lr: 0.013552, loss: 1.5671
2022-06-28 18:13:06 - train: epoch 0092, iter [01100, 05004], lr: 0.013534, loss: 1.1516
2022-06-28 18:13:41 - train: epoch 0092, iter [01200, 05004], lr: 0.013516, loss: 1.2177
2022-06-28 18:14:16 - train: epoch 0092, iter [01300, 05004], lr: 0.013498, loss: 1.1800
2022-06-28 18:14:51 - train: epoch 0092, iter [01400, 05004], lr: 0.013480, loss: 1.2273
2022-06-28 18:15:26 - train: epoch 0092, iter [01500, 05004], lr: 0.013462, loss: 1.2710
2022-06-28 18:16:01 - train: epoch 0092, iter [01600, 05004], lr: 0.013444, loss: 1.3330
2022-06-28 18:16:35 - train: epoch 0092, iter [01700, 05004], lr: 0.013427, loss: 1.3564
2022-06-28 18:17:10 - train: epoch 0092, iter [01800, 05004], lr: 0.013409, loss: 1.2321
2022-06-28 18:17:45 - train: epoch 0092, iter [01900, 05004], lr: 0.013391, loss: 1.2219
2022-06-28 18:18:19 - train: epoch 0092, iter [02000, 05004], lr: 0.013373, loss: 1.2264
2022-06-28 18:18:54 - train: epoch 0092, iter [02100, 05004], lr: 0.013355, loss: 1.3100
2022-06-28 18:19:29 - train: epoch 0092, iter [02200, 05004], lr: 0.013338, loss: 1.3122
2022-06-28 18:20:04 - train: epoch 0092, iter [02300, 05004], lr: 0.013320, loss: 1.4456
2022-06-28 18:20:39 - train: epoch 0092, iter [02400, 05004], lr: 0.013302, loss: 1.1594
2022-06-28 18:21:14 - train: epoch 0092, iter [02500, 05004], lr: 0.013284, loss: 1.2968
2022-06-28 18:21:49 - train: epoch 0092, iter [02600, 05004], lr: 0.013266, loss: 1.3801
2022-06-28 18:22:24 - train: epoch 0092, iter [02700, 05004], lr: 0.013249, loss: 1.2560
2022-06-28 18:22:59 - train: epoch 0092, iter [02800, 05004], lr: 0.013231, loss: 1.1846
2022-06-28 18:23:34 - train: epoch 0092, iter [02900, 05004], lr: 0.013213, loss: 1.4166
2022-06-28 18:24:08 - train: epoch 0092, iter [03000, 05004], lr: 0.013196, loss: 1.1820
2022-06-28 18:24:42 - train: epoch 0092, iter [03100, 05004], lr: 0.013178, loss: 1.2274
2022-06-28 18:25:18 - train: epoch 0092, iter [03200, 05004], lr: 0.013160, loss: 1.2151
2022-06-28 18:25:51 - train: epoch 0092, iter [03300, 05004], lr: 0.013142, loss: 1.2536
2022-06-28 18:26:28 - train: epoch 0092, iter [03400, 05004], lr: 0.013125, loss: 1.4554
2022-06-28 18:27:02 - train: epoch 0092, iter [03500, 05004], lr: 0.013107, loss: 1.1598
2022-06-28 18:27:37 - train: epoch 0092, iter [03600, 05004], lr: 0.013090, loss: 1.2363
2022-06-28 18:28:11 - train: epoch 0092, iter [03700, 05004], lr: 0.013072, loss: 1.1237
2022-06-28 18:28:46 - train: epoch 0092, iter [03800, 05004], lr: 0.013054, loss: 1.4516
2022-06-28 18:29:21 - train: epoch 0092, iter [03900, 05004], lr: 0.013037, loss: 1.3754
2022-06-28 18:29:56 - train: epoch 0092, iter [04000, 05004], lr: 0.013019, loss: 1.4067
2022-06-28 18:30:31 - train: epoch 0092, iter [04100, 05004], lr: 0.013001, loss: 1.1380
2022-06-28 18:31:06 - train: epoch 0092, iter [04200, 05004], lr: 0.012984, loss: 1.2504
2022-06-28 18:31:40 - train: epoch 0092, iter [04300, 05004], lr: 0.012966, loss: 1.2197
2022-06-28 18:32:15 - train: epoch 0092, iter [04400, 05004], lr: 0.012949, loss: 1.1928
2022-06-28 18:32:51 - train: epoch 0092, iter [04500, 05004], lr: 0.012931, loss: 1.2423
2022-06-28 18:33:25 - train: epoch 0092, iter [04600, 05004], lr: 0.012914, loss: 1.2586
2022-06-28 18:34:00 - train: epoch 0092, iter [04700, 05004], lr: 0.012896, loss: 0.9096
2022-06-28 18:34:35 - train: epoch 0092, iter [04800, 05004], lr: 0.012878, loss: 1.2111
2022-06-28 18:35:09 - train: epoch 0092, iter [04900, 05004], lr: 0.012861, loss: 1.2081
2022-06-28 18:35:43 - train: epoch 0092, iter [05000, 05004], lr: 0.012843, loss: 1.4800
2022-06-28 18:35:44 - train: epoch 092, train_loss: 1.2887
2022-06-28 18:37:02 - eval: epoch: 092, acc1: 71.018%, acc5: 90.378%, test_loss: 1.1708, per_image_load_time: 2.574ms, per_image_inference_time: 0.396ms
2022-06-28 18:37:02 - until epoch: 092, best_acc1: 71.018%
2022-06-28 18:37:02 - epoch 093 lr: 0.012843
2022-06-28 18:37:42 - train: epoch 0093, iter [00100, 05004], lr: 0.012825, loss: 1.1584
2022-06-28 18:38:17 - train: epoch 0093, iter [00200, 05004], lr: 0.012808, loss: 1.1903
2022-06-28 18:38:51 - train: epoch 0093, iter [00300, 05004], lr: 0.012790, loss: 1.3176
2022-06-28 18:39:26 - train: epoch 0093, iter [00400, 05004], lr: 0.012773, loss: 1.2330
2022-06-28 18:40:01 - train: epoch 0093, iter [00500, 05004], lr: 0.012755, loss: 1.3650
2022-06-28 18:40:35 - train: epoch 0093, iter [00600, 05004], lr: 0.012738, loss: 1.1809
2022-06-28 18:41:10 - train: epoch 0093, iter [00700, 05004], lr: 0.012720, loss: 1.2823
2022-06-28 18:41:44 - train: epoch 0093, iter [00800, 05004], lr: 0.012703, loss: 1.1552
2022-06-28 18:42:19 - train: epoch 0093, iter [00900, 05004], lr: 0.012686, loss: 1.2413
2022-06-28 18:42:53 - train: epoch 0093, iter [01000, 05004], lr: 0.012668, loss: 1.0918
2022-06-28 18:43:27 - train: epoch 0093, iter [01100, 05004], lr: 0.012651, loss: 1.2243
2022-06-28 18:44:02 - train: epoch 0093, iter [01200, 05004], lr: 0.012633, loss: 1.1550
2022-06-28 18:44:37 - train: epoch 0093, iter [01300, 05004], lr: 0.012616, loss: 1.3166
2022-06-28 18:45:12 - train: epoch 0093, iter [01400, 05004], lr: 0.012599, loss: 1.2420
2022-06-28 18:45:46 - train: epoch 0093, iter [01500, 05004], lr: 0.012581, loss: 1.4982
2022-06-28 18:46:21 - train: epoch 0093, iter [01600, 05004], lr: 0.012564, loss: 1.2788
2022-06-28 18:46:55 - train: epoch 0093, iter [01700, 05004], lr: 0.012547, loss: 1.2083
2022-06-28 18:47:29 - train: epoch 0093, iter [01800, 05004], lr: 0.012529, loss: 1.3042
2022-06-28 18:48:03 - train: epoch 0093, iter [01900, 05004], lr: 0.012512, loss: 1.1797
2022-06-28 18:48:38 - train: epoch 0093, iter [02000, 05004], lr: 0.012495, loss: 1.1621
2022-06-28 18:49:13 - train: epoch 0093, iter [02100, 05004], lr: 0.012477, loss: 1.1618
2022-06-28 18:49:48 - train: epoch 0093, iter [02200, 05004], lr: 0.012460, loss: 1.4024
2022-06-28 18:50:23 - train: epoch 0093, iter [02300, 05004], lr: 0.012443, loss: 1.3004
2022-06-28 18:50:57 - train: epoch 0093, iter [02400, 05004], lr: 0.012426, loss: 1.1078
2022-06-28 18:51:33 - train: epoch 0093, iter [02500, 05004], lr: 0.012408, loss: 1.2658
2022-06-28 18:52:08 - train: epoch 0093, iter [02600, 05004], lr: 0.012391, loss: 1.5241
2022-06-28 18:52:42 - train: epoch 0093, iter [02700, 05004], lr: 0.012374, loss: 1.1772
2022-06-28 18:53:18 - train: epoch 0093, iter [02800, 05004], lr: 0.012357, loss: 1.2450
2022-06-28 18:53:52 - train: epoch 0093, iter [02900, 05004], lr: 0.012339, loss: 1.1928
2022-06-28 18:54:27 - train: epoch 0093, iter [03000, 05004], lr: 0.012322, loss: 1.1493
2022-06-28 18:55:02 - train: epoch 0093, iter [03100, 05004], lr: 0.012305, loss: 1.4518
2022-06-28 18:55:37 - train: epoch 0093, iter [03200, 05004], lr: 0.012288, loss: 1.1361
2022-06-28 18:56:12 - train: epoch 0093, iter [03300, 05004], lr: 0.012271, loss: 1.2531
2022-06-28 18:56:46 - train: epoch 0093, iter [03400, 05004], lr: 0.012254, loss: 1.5116
2022-06-28 18:57:21 - train: epoch 0093, iter [03500, 05004], lr: 0.012236, loss: 1.1112
2022-06-28 18:57:56 - train: epoch 0093, iter [03600, 05004], lr: 0.012219, loss: 1.4114
2022-06-28 18:58:30 - train: epoch 0093, iter [03700, 05004], lr: 0.012202, loss: 1.1982
2022-06-28 18:59:05 - train: epoch 0093, iter [03800, 05004], lr: 0.012185, loss: 1.0313
2022-06-28 18:59:40 - train: epoch 0093, iter [03900, 05004], lr: 0.012168, loss: 1.2659
2022-06-28 19:00:15 - train: epoch 0093, iter [04000, 05004], lr: 0.012151, loss: 1.4870
2022-06-28 19:00:50 - train: epoch 0093, iter [04100, 05004], lr: 0.012134, loss: 1.3850
2022-06-28 19:01:25 - train: epoch 0093, iter [04200, 05004], lr: 0.012117, loss: 1.2925
2022-06-28 19:01:59 - train: epoch 0093, iter [04300, 05004], lr: 0.012100, loss: 1.4465
2022-06-28 19:02:35 - train: epoch 0093, iter [04400, 05004], lr: 0.012083, loss: 1.2065
2022-06-28 19:03:09 - train: epoch 0093, iter [04500, 05004], lr: 0.012065, loss: 1.2885
2022-06-28 19:03:44 - train: epoch 0093, iter [04600, 05004], lr: 0.012048, loss: 1.1204
2022-06-28 19:04:19 - train: epoch 0093, iter [04700, 05004], lr: 0.012031, loss: 1.6339
2022-06-28 19:04:53 - train: epoch 0093, iter [04800, 05004], lr: 0.012014, loss: 1.3105
2022-06-28 19:05:28 - train: epoch 0093, iter [04900, 05004], lr: 0.011997, loss: 1.4162
2022-06-28 19:06:01 - train: epoch 0093, iter [05000, 05004], lr: 0.011980, loss: 1.2346
2022-06-28 19:06:02 - train: epoch 093, train_loss: 1.2715
2022-06-28 19:07:19 - eval: epoch: 093, acc1: 71.808%, acc5: 90.552%, test_loss: 1.1384, per_image_load_time: 1.348ms, per_image_inference_time: 0.407ms
2022-06-28 19:07:20 - until epoch: 093, best_acc1: 71.808%
2022-06-28 19:07:20 - epoch 094 lr: 0.011980
2022-06-28 19:07:59 - train: epoch 0094, iter [00100, 05004], lr: 0.011963, loss: 1.3149
2022-06-28 19:08:34 - train: epoch 0094, iter [00200, 05004], lr: 0.011946, loss: 1.2515
2022-06-28 19:09:08 - train: epoch 0094, iter [00300, 05004], lr: 0.011929, loss: 1.3726
2022-06-28 19:09:43 - train: epoch 0094, iter [00400, 05004], lr: 0.011912, loss: 1.3587
2022-06-28 19:10:17 - train: epoch 0094, iter [00500, 05004], lr: 0.011895, loss: 1.1990
2022-06-28 19:10:51 - train: epoch 0094, iter [00600, 05004], lr: 0.011878, loss: 1.0351
2022-06-28 19:11:26 - train: epoch 0094, iter [00700, 05004], lr: 0.011861, loss: 1.4078
2022-06-28 19:11:59 - train: epoch 0094, iter [00800, 05004], lr: 0.011844, loss: 1.1540
2022-06-28 19:12:34 - train: epoch 0094, iter [00900, 05004], lr: 0.011827, loss: 1.1439
2022-06-28 19:13:08 - train: epoch 0094, iter [01000, 05004], lr: 0.011810, loss: 1.3210
2022-06-28 19:13:44 - train: epoch 0094, iter [01100, 05004], lr: 0.011793, loss: 1.4052
2022-06-28 19:14:18 - train: epoch 0094, iter [01200, 05004], lr: 0.011777, loss: 1.1797
2022-06-28 19:14:53 - train: epoch 0094, iter [01300, 05004], lr: 0.011760, loss: 1.3398
2022-06-28 19:15:27 - train: epoch 0094, iter [01400, 05004], lr: 0.011743, loss: 1.1758
2022-06-28 19:16:02 - train: epoch 0094, iter [01500, 05004], lr: 0.011726, loss: 1.3032
2022-06-28 19:16:36 - train: epoch 0094, iter [01600, 05004], lr: 0.011709, loss: 1.6901
2022-06-28 19:17:10 - train: epoch 0094, iter [01700, 05004], lr: 0.011692, loss: 1.1725
2022-06-28 19:17:45 - train: epoch 0094, iter [01800, 05004], lr: 0.011676, loss: 1.1382
2022-06-28 19:18:19 - train: epoch 0094, iter [01900, 05004], lr: 0.011659, loss: 1.2326
2022-06-28 19:18:54 - train: epoch 0094, iter [02000, 05004], lr: 0.011642, loss: 0.9732
2022-06-28 19:19:29 - train: epoch 0094, iter [02100, 05004], lr: 0.011625, loss: 1.3806
2022-06-28 19:20:04 - train: epoch 0094, iter [02200, 05004], lr: 0.011608, loss: 1.1215
2022-06-28 19:20:39 - train: epoch 0094, iter [02300, 05004], lr: 0.011592, loss: 1.0765
2022-06-28 19:21:13 - train: epoch 0094, iter [02400, 05004], lr: 0.011575, loss: 1.2693
2022-06-28 19:21:48 - train: epoch 0094, iter [02500, 05004], lr: 0.011558, loss: 1.1689
2022-06-28 19:22:22 - train: epoch 0094, iter [02600, 05004], lr: 0.011542, loss: 1.1150
2022-06-28 19:22:57 - train: epoch 0094, iter [02700, 05004], lr: 0.011525, loss: 1.2241
2022-06-28 19:23:32 - train: epoch 0094, iter [02800, 05004], lr: 0.011508, loss: 1.2511
2022-06-28 19:24:07 - train: epoch 0094, iter [02900, 05004], lr: 0.011491, loss: 1.2702
2022-06-28 19:24:42 - train: epoch 0094, iter [03000, 05004], lr: 0.011475, loss: 1.1048
2022-06-28 19:25:17 - train: epoch 0094, iter [03100, 05004], lr: 0.011458, loss: 1.4385
2022-06-28 19:25:51 - train: epoch 0094, iter [03200, 05004], lr: 0.011441, loss: 1.2314
2022-06-28 19:26:26 - train: epoch 0094, iter [03300, 05004], lr: 0.011425, loss: 1.1474
2022-06-28 19:27:01 - train: epoch 0094, iter [03400, 05004], lr: 0.011408, loss: 1.4002
2022-06-28 19:27:36 - train: epoch 0094, iter [03500, 05004], lr: 0.011391, loss: 1.4366
2022-06-28 19:28:10 - train: epoch 0094, iter [03600, 05004], lr: 0.011375, loss: 1.3072
2022-06-28 19:28:45 - train: epoch 0094, iter [03700, 05004], lr: 0.011358, loss: 1.3991
2022-06-28 19:29:20 - train: epoch 0094, iter [03800, 05004], lr: 0.011342, loss: 1.1588
2022-06-28 19:29:55 - train: epoch 0094, iter [03900, 05004], lr: 0.011325, loss: 1.2809
2022-06-28 19:30:29 - train: epoch 0094, iter [04000, 05004], lr: 0.011309, loss: 1.4491
2022-06-28 19:31:04 - train: epoch 0094, iter [04100, 05004], lr: 0.011292, loss: 1.5162
2022-06-28 19:31:38 - train: epoch 0094, iter [04200, 05004], lr: 0.011275, loss: 1.0329
2022-06-28 19:32:15 - train: epoch 0094, iter [04300, 05004], lr: 0.011259, loss: 1.3853
2022-06-28 19:32:48 - train: epoch 0094, iter [04400, 05004], lr: 0.011242, loss: 1.4116
2022-06-28 19:33:23 - train: epoch 0094, iter [04500, 05004], lr: 0.011226, loss: 1.1890
2022-06-28 19:33:57 - train: epoch 0094, iter [04600, 05004], lr: 0.011209, loss: 1.2980
2022-06-28 19:34:32 - train: epoch 0094, iter [04700, 05004], lr: 0.011193, loss: 1.2873
2022-06-28 19:35:07 - train: epoch 0094, iter [04800, 05004], lr: 0.011176, loss: 1.3128
2022-06-28 19:35:41 - train: epoch 0094, iter [04900, 05004], lr: 0.011160, loss: 1.1899
2022-06-28 19:36:14 - train: epoch 0094, iter [05000, 05004], lr: 0.011143, loss: 1.1835
2022-06-28 19:36:15 - train: epoch 094, train_loss: 1.2557
2022-06-28 19:37:32 - eval: epoch: 094, acc1: 71.308%, acc5: 90.194%, test_loss: 1.1636, per_image_load_time: 2.480ms, per_image_inference_time: 0.407ms
2022-06-28 19:37:32 - until epoch: 094, best_acc1: 71.808%
2022-06-28 19:37:32 - epoch 095 lr: 0.011143
2022-06-28 19:38:13 - train: epoch 0095, iter [00100, 05004], lr: 0.011126, loss: 1.0493
2022-06-28 19:38:48 - train: epoch 0095, iter [00200, 05004], lr: 0.011110, loss: 1.4102
2022-06-28 19:39:22 - train: epoch 0095, iter [00300, 05004], lr: 0.011093, loss: 1.1732
2022-06-28 19:39:57 - train: epoch 0095, iter [00400, 05004], lr: 0.011077, loss: 1.3085
2022-06-28 19:40:31 - train: epoch 0095, iter [00500, 05004], lr: 0.011061, loss: 1.2180
2022-06-28 19:41:06 - train: epoch 0095, iter [00600, 05004], lr: 0.011044, loss: 1.2139
2022-06-28 19:41:40 - train: epoch 0095, iter [00700, 05004], lr: 0.011028, loss: 1.3591
2022-06-28 19:42:15 - train: epoch 0095, iter [00800, 05004], lr: 0.011011, loss: 1.4111
2022-06-28 19:42:50 - train: epoch 0095, iter [00900, 05004], lr: 0.010995, loss: 1.3896
2022-06-28 19:43:24 - train: epoch 0095, iter [01000, 05004], lr: 0.010979, loss: 1.3065
2022-06-28 19:43:59 - train: epoch 0095, iter [01100, 05004], lr: 0.010962, loss: 1.0751
2022-06-28 19:44:34 - train: epoch 0095, iter [01200, 05004], lr: 0.010946, loss: 1.0506
2022-06-28 19:45:09 - train: epoch 0095, iter [01300, 05004], lr: 0.010930, loss: 1.1560
2022-06-28 19:45:44 - train: epoch 0095, iter [01400, 05004], lr: 0.010913, loss: 1.2734
2022-06-28 19:46:17 - train: epoch 0095, iter [01500, 05004], lr: 0.010897, loss: 1.2435
2022-06-28 19:46:52 - train: epoch 0095, iter [01600, 05004], lr: 0.010881, loss: 0.9168
2022-06-28 19:47:27 - train: epoch 0095, iter [01700, 05004], lr: 0.010864, loss: 1.2852
2022-06-28 19:48:02 - train: epoch 0095, iter [01800, 05004], lr: 0.010848, loss: 1.3331
2022-06-28 19:48:37 - train: epoch 0095, iter [01900, 05004], lr: 0.010832, loss: 1.2110
2022-06-28 19:49:12 - train: epoch 0095, iter [02000, 05004], lr: 0.010816, loss: 1.2389
2022-06-28 19:49:46 - train: epoch 0095, iter [02100, 05004], lr: 0.010799, loss: 1.2277
2022-06-28 19:50:21 - train: epoch 0095, iter [02200, 05004], lr: 0.010783, loss: 0.9326
2022-06-28 19:50:56 - train: epoch 0095, iter [02300, 05004], lr: 0.010767, loss: 1.1668
2022-06-28 19:51:31 - train: epoch 0095, iter [02400, 05004], lr: 0.010751, loss: 1.2941
2022-06-28 19:52:05 - train: epoch 0095, iter [02500, 05004], lr: 0.010734, loss: 1.1610
2022-06-28 19:52:41 - train: epoch 0095, iter [02600, 05004], lr: 0.010718, loss: 1.2796
2022-06-28 19:53:15 - train: epoch 0095, iter [02700, 05004], lr: 0.010702, loss: 1.2647
2022-06-28 19:53:50 - train: epoch 0095, iter [02800, 05004], lr: 0.010686, loss: 1.1015
2022-06-28 19:54:25 - train: epoch 0095, iter [02900, 05004], lr: 0.010670, loss: 1.1830
2022-06-28 19:55:00 - train: epoch 0095, iter [03000, 05004], lr: 0.010654, loss: 1.4061
2022-06-28 19:55:34 - train: epoch 0095, iter [03100, 05004], lr: 0.010638, loss: 1.4522
2022-06-28 19:56:09 - train: epoch 0095, iter [03200, 05004], lr: 0.010621, loss: 1.2642
2022-06-28 19:56:44 - train: epoch 0095, iter [03300, 05004], lr: 0.010605, loss: 1.3320
2022-06-28 19:57:19 - train: epoch 0095, iter [03400, 05004], lr: 0.010589, loss: 1.1700
2022-06-28 19:57:54 - train: epoch 0095, iter [03500, 05004], lr: 0.010573, loss: 1.1522
2022-06-28 19:58:29 - train: epoch 0095, iter [03600, 05004], lr: 0.010557, loss: 1.1721
2022-06-28 19:59:03 - train: epoch 0095, iter [03700, 05004], lr: 0.010541, loss: 1.1531
2022-06-28 19:59:38 - train: epoch 0095, iter [03800, 05004], lr: 0.010525, loss: 0.9531
2022-06-28 20:00:13 - train: epoch 0095, iter [03900, 05004], lr: 0.010509, loss: 1.3204
2022-06-28 20:00:47 - train: epoch 0095, iter [04000, 05004], lr: 0.010493, loss: 1.1442
2022-06-28 20:01:22 - train: epoch 0095, iter [04100, 05004], lr: 0.010477, loss: 1.2794
2022-06-28 20:01:57 - train: epoch 0095, iter [04200, 05004], lr: 0.010461, loss: 1.0804
2022-06-28 20:02:32 - train: epoch 0095, iter [04300, 05004], lr: 0.010445, loss: 1.3469
2022-06-28 20:03:07 - train: epoch 0095, iter [04400, 05004], lr: 0.010429, loss: 1.3524
2022-06-28 20:03:42 - train: epoch 0095, iter [04500, 05004], lr: 0.010413, loss: 1.1201
2022-06-28 20:04:17 - train: epoch 0095, iter [04600, 05004], lr: 0.010397, loss: 1.3323
2022-06-28 20:04:52 - train: epoch 0095, iter [04700, 05004], lr: 0.010381, loss: 1.2423
2022-06-28 20:05:26 - train: epoch 0095, iter [04800, 05004], lr: 0.010365, loss: 1.3392
2022-06-28 20:06:01 - train: epoch 0095, iter [04900, 05004], lr: 0.010349, loss: 1.2126
2022-06-28 20:06:35 - train: epoch 0095, iter [05000, 05004], lr: 0.010333, loss: 1.0053
2022-06-28 20:06:36 - train: epoch 095, train_loss: 1.2388
2022-06-28 20:07:53 - eval: epoch: 095, acc1: 71.880%, acc5: 90.572%, test_loss: 1.1312, per_image_load_time: 2.529ms, per_image_inference_time: 0.411ms
2022-06-28 20:07:53 - until epoch: 095, best_acc1: 71.880%
2022-06-28 20:07:53 - epoch 096 lr: 0.010332
2022-06-28 20:08:32 - train: epoch 0096, iter [00100, 05004], lr: 0.010316, loss: 1.2367
2022-06-28 20:09:08 - train: epoch 0096, iter [00200, 05004], lr: 0.010301, loss: 1.2213
2022-06-28 20:09:42 - train: epoch 0096, iter [00300, 05004], lr: 0.010285, loss: 1.1968
2022-06-28 20:10:17 - train: epoch 0096, iter [00400, 05004], lr: 0.010269, loss: 1.1166
2022-06-28 20:10:51 - train: epoch 0096, iter [00500, 05004], lr: 0.010253, loss: 1.2076
2022-06-28 20:11:26 - train: epoch 0096, iter [00600, 05004], lr: 0.010237, loss: 1.2790
2022-06-28 20:12:00 - train: epoch 0096, iter [00700, 05004], lr: 0.010221, loss: 1.0449
2022-06-28 20:12:35 - train: epoch 0096, iter [00800, 05004], lr: 0.010205, loss: 1.0984
2022-06-28 20:13:09 - train: epoch 0096, iter [00900, 05004], lr: 0.010189, loss: 1.2257
2022-06-28 20:13:45 - train: epoch 0096, iter [01000, 05004], lr: 0.010174, loss: 1.1438
2022-06-28 20:14:19 - train: epoch 0096, iter [01100, 05004], lr: 0.010158, loss: 1.2264
2022-06-28 20:14:54 - train: epoch 0096, iter [01200, 05004], lr: 0.010142, loss: 1.2701
2022-06-28 20:15:28 - train: epoch 0096, iter [01300, 05004], lr: 0.010126, loss: 1.2238
2022-06-28 20:16:03 - train: epoch 0096, iter [01400, 05004], lr: 0.010110, loss: 1.1942
2022-06-28 20:16:37 - train: epoch 0096, iter [01500, 05004], lr: 0.010095, loss: 1.1398
2022-06-28 20:17:12 - train: epoch 0096, iter [01600, 05004], lr: 0.010079, loss: 0.9540
2022-06-28 20:17:46 - train: epoch 0096, iter [01700, 05004], lr: 0.010063, loss: 1.2099
2022-06-28 20:18:21 - train: epoch 0096, iter [01800, 05004], lr: 0.010047, loss: 1.3422
2022-06-28 20:18:55 - train: epoch 0096, iter [01900, 05004], lr: 0.010032, loss: 1.2866
2022-06-28 20:19:30 - train: epoch 0096, iter [02000, 05004], lr: 0.010016, loss: 1.1799
2022-06-28 20:20:06 - train: epoch 0096, iter [02100, 05004], lr: 0.010000, loss: 1.2020
2022-06-28 20:20:40 - train: epoch 0096, iter [02200, 05004], lr: 0.009985, loss: 1.0731
2022-06-28 20:21:15 - train: epoch 0096, iter [02300, 05004], lr: 0.009969, loss: 1.1429
2022-06-28 20:21:49 - train: epoch 0096, iter [02400, 05004], lr: 0.009953, loss: 1.0716
2022-06-28 20:22:24 - train: epoch 0096, iter [02500, 05004], lr: 0.009938, loss: 1.1604
2022-06-28 20:22:58 - train: epoch 0096, iter [02600, 05004], lr: 0.009922, loss: 1.1959
2022-06-28 20:23:32 - train: epoch 0096, iter [02700, 05004], lr: 0.009906, loss: 1.2374
2022-06-28 20:24:08 - train: epoch 0096, iter [02800, 05004], lr: 0.009891, loss: 1.4011
2022-06-28 20:24:42 - train: epoch 0096, iter [02900, 05004], lr: 0.009875, loss: 1.1583
2022-06-28 20:25:17 - train: epoch 0096, iter [03000, 05004], lr: 0.009860, loss: 1.2402
2022-06-28 20:25:52 - train: epoch 0096, iter [03100, 05004], lr: 0.009844, loss: 1.2837
2022-06-28 20:26:27 - train: epoch 0096, iter [03200, 05004], lr: 0.009828, loss: 1.1860
2022-06-28 20:27:02 - train: epoch 0096, iter [03300, 05004], lr: 0.009813, loss: 1.3612
2022-06-28 20:27:36 - train: epoch 0096, iter [03400, 05004], lr: 0.009797, loss: 1.0111
2022-06-28 20:28:11 - train: epoch 0096, iter [03500, 05004], lr: 0.009782, loss: 1.1131
2022-06-28 20:28:46 - train: epoch 0096, iter [03600, 05004], lr: 0.009766, loss: 1.0581
2022-06-28 20:29:21 - train: epoch 0096, iter [03700, 05004], lr: 0.009751, loss: 1.2471
2022-06-28 20:29:55 - train: epoch 0096, iter [03800, 05004], lr: 0.009735, loss: 1.1434
2022-06-28 20:30:31 - train: epoch 0096, iter [03900, 05004], lr: 0.009720, loss: 1.1207
2022-06-28 20:31:06 - train: epoch 0096, iter [04000, 05004], lr: 0.009704, loss: 1.0546
2022-06-28 20:31:41 - train: epoch 0096, iter [04100, 05004], lr: 0.009689, loss: 1.1667
2022-06-28 20:32:16 - train: epoch 0096, iter [04200, 05004], lr: 0.009673, loss: 1.2533
2022-06-28 20:32:50 - train: epoch 0096, iter [04300, 05004], lr: 0.009658, loss: 0.9058
2022-06-28 20:33:25 - train: epoch 0096, iter [04400, 05004], lr: 0.009642, loss: 1.1210
2022-06-28 20:34:00 - train: epoch 0096, iter [04500, 05004], lr: 0.009627, loss: 1.1398
2022-06-28 20:34:34 - train: epoch 0096, iter [04600, 05004], lr: 0.009611, loss: 1.1115
2022-06-28 20:35:09 - train: epoch 0096, iter [04700, 05004], lr: 0.009596, loss: 1.2027
2022-06-28 20:35:44 - train: epoch 0096, iter [04800, 05004], lr: 0.009581, loss: 1.2688
2022-06-28 20:36:19 - train: epoch 0096, iter [04900, 05004], lr: 0.009565, loss: 1.4345
2022-06-28 20:36:52 - train: epoch 0096, iter [05000, 05004], lr: 0.009550, loss: 1.0847
2022-06-28 20:36:54 - train: epoch 096, train_loss: 1.2170
2022-06-28 20:38:11 - eval: epoch: 096, acc1: 72.154%, acc5: 90.890%, test_loss: 1.1197, per_image_load_time: 2.560ms, per_image_inference_time: 0.388ms
2022-06-28 20:38:11 - until epoch: 096, best_acc1: 72.154%
2022-06-28 20:38:11 - epoch 097 lr: 0.009549
2022-06-28 20:38:51 - train: epoch 0097, iter [00100, 05004], lr: 0.009534, loss: 1.2676
2022-06-28 20:39:25 - train: epoch 0097, iter [00200, 05004], lr: 0.009518, loss: 1.0047
2022-06-28 20:40:00 - train: epoch 0097, iter [00300, 05004], lr: 0.009503, loss: 1.2579
2022-06-28 20:40:35 - train: epoch 0097, iter [00400, 05004], lr: 0.009488, loss: 1.4434
2022-06-28 20:41:09 - train: epoch 0097, iter [00500, 05004], lr: 0.009472, loss: 1.2253
2022-06-28 20:41:44 - train: epoch 0097, iter [00600, 05004], lr: 0.009457, loss: 1.3431
2022-06-28 20:42:19 - train: epoch 0097, iter [00700, 05004], lr: 0.009442, loss: 1.2080
2022-06-28 20:42:53 - train: epoch 0097, iter [00800, 05004], lr: 0.009426, loss: 1.1047
2022-06-28 20:43:27 - train: epoch 0097, iter [00900, 05004], lr: 0.009411, loss: 1.1014
2022-06-28 20:44:02 - train: epoch 0097, iter [01000, 05004], lr: 0.009396, loss: 1.1564
2022-06-28 20:44:37 - train: epoch 0097, iter [01100, 05004], lr: 0.009381, loss: 1.0570
2022-06-28 20:45:11 - train: epoch 0097, iter [01200, 05004], lr: 0.009365, loss: 1.1626
2022-06-28 20:45:46 - train: epoch 0097, iter [01300, 05004], lr: 0.009350, loss: 1.0609
2022-06-28 20:46:21 - train: epoch 0097, iter [01400, 05004], lr: 0.009335, loss: 1.1918
2022-06-28 20:46:55 - train: epoch 0097, iter [01500, 05004], lr: 0.009320, loss: 1.2138
2022-06-28 20:47:31 - train: epoch 0097, iter [01600, 05004], lr: 0.009305, loss: 1.1511
2022-06-28 20:48:06 - train: epoch 0097, iter [01700, 05004], lr: 0.009289, loss: 1.1174
2022-06-28 20:48:41 - train: epoch 0097, iter [01800, 05004], lr: 0.009274, loss: 1.2791
2022-06-28 20:49:15 - train: epoch 0097, iter [01900, 05004], lr: 0.009259, loss: 1.1976
2022-06-28 20:49:50 - train: epoch 0097, iter [02000, 05004], lr: 0.009244, loss: 1.2023
2022-06-28 20:50:25 - train: epoch 0097, iter [02100, 05004], lr: 0.009229, loss: 1.2577
2022-06-28 20:50:59 - train: epoch 0097, iter [02200, 05004], lr: 0.009214, loss: 1.3981
2022-06-28 20:51:34 - train: epoch 0097, iter [02300, 05004], lr: 0.009198, loss: 1.1063
2022-06-28 20:52:09 - train: epoch 0097, iter [02400, 05004], lr: 0.009183, loss: 1.2078
2022-06-28 20:52:43 - train: epoch 0097, iter [02500, 05004], lr: 0.009168, loss: 1.2473
2022-06-28 20:53:18 - train: epoch 0097, iter [02600, 05004], lr: 0.009153, loss: 1.3206
