2022-06-28 20:53:52 - train: epoch 0097, iter [02700, 05004], lr: 0.009138, loss: 1.0299
2022-06-28 20:54:28 - train: epoch 0097, iter [02800, 05004], lr: 0.009123, loss: 1.1183
2022-06-28 20:55:02 - train: epoch 0097, iter [02900, 05004], lr: 0.009108, loss: 1.3905
2022-06-28 20:55:37 - train: epoch 0097, iter [03000, 05004], lr: 0.009093, loss: 1.2304
2022-06-28 20:56:11 - train: epoch 0097, iter [03100, 05004], lr: 0.009078, loss: 1.1732
2022-06-28 20:56:46 - train: epoch 0097, iter [03200, 05004], lr: 0.009063, loss: 1.3798
2022-06-28 20:57:20 - train: epoch 0097, iter [03300, 05004], lr: 0.009048, loss: 1.2909
2022-06-28 20:57:56 - train: epoch 0097, iter [03400, 05004], lr: 0.009033, loss: 1.4172
2022-06-28 20:58:30 - train: epoch 0097, iter [03500, 05004], lr: 0.009018, loss: 1.3020
2022-06-28 20:59:05 - train: epoch 0097, iter [03600, 05004], lr: 0.009003, loss: 1.1527
2022-06-28 20:59:39 - train: epoch 0097, iter [03700, 05004], lr: 0.008988, loss: 1.0226
2022-06-28 21:00:14 - train: epoch 0097, iter [03800, 05004], lr: 0.008973, loss: 1.0107
2022-06-28 21:00:48 - train: epoch 0097, iter [03900, 05004], lr: 0.008958, loss: 1.2332
2022-06-28 21:01:22 - train: epoch 0097, iter [04000, 05004], lr: 0.008943, loss: 1.2940
2022-06-28 21:01:57 - train: epoch 0097, iter [04100, 05004], lr: 0.008928, loss: 1.1868
2022-06-28 21:02:31 - train: epoch 0097, iter [04200, 05004], lr: 0.008913, loss: 1.1857
2022-06-28 21:03:06 - train: epoch 0097, iter [04300, 05004], lr: 0.008898, loss: 1.0714
2022-06-28 21:03:41 - train: epoch 0097, iter [04400, 05004], lr: 0.008883, loss: 1.1702
2022-06-28 21:04:16 - train: epoch 0097, iter [04500, 05004], lr: 0.008869, loss: 1.1479
2022-06-28 21:04:50 - train: epoch 0097, iter [04600, 05004], lr: 0.008854, loss: 1.4124
2022-06-28 21:05:25 - train: epoch 0097, iter [04700, 05004], lr: 0.008839, loss: 1.1815
2022-06-28 21:06:01 - train: epoch 0097, iter [04800, 05004], lr: 0.008824, loss: 1.2682
2022-06-28 21:06:35 - train: epoch 0097, iter [04900, 05004], lr: 0.008809, loss: 1.3885
2022-06-28 21:07:08 - train: epoch 0097, iter [05000, 05004], lr: 0.008794, loss: 1.2547
2022-06-28 21:07:09 - train: epoch 097, train_loss: 1.1998
2022-06-28 21:08:26 - eval: epoch: 097, acc1: 72.316%, acc5: 91.006%, test_loss: 1.1032, per_image_load_time: 1.704ms, per_image_inference_time: 0.431ms
2022-06-28 21:08:26 - until epoch: 097, best_acc1: 72.316%
2022-06-28 21:08:26 - epoch 098 lr: 0.008794
2022-06-28 21:09:06 - train: epoch 0098, iter [00100, 05004], lr: 0.008779, loss: 1.3511
2022-06-28 21:09:41 - train: epoch 0098, iter [00200, 05004], lr: 0.008764, loss: 1.3007
2022-06-28 21:10:16 - train: epoch 0098, iter [00300, 05004], lr: 0.008749, loss: 1.4167
2022-06-28 21:10:50 - train: epoch 0098, iter [00400, 05004], lr: 0.008735, loss: 0.9862
2022-06-28 21:11:24 - train: epoch 0098, iter [00500, 05004], lr: 0.008720, loss: 1.1760
2022-06-28 21:11:59 - train: epoch 0098, iter [00600, 05004], lr: 0.008705, loss: 1.2776
2022-06-28 21:12:34 - train: epoch 0098, iter [00700, 05004], lr: 0.008690, loss: 1.1186
2022-06-28 21:13:09 - train: epoch 0098, iter [00800, 05004], lr: 0.008676, loss: 1.2195
2022-06-28 21:13:43 - train: epoch 0098, iter [00900, 05004], lr: 0.008661, loss: 1.2023
2022-06-28 21:14:18 - train: epoch 0098, iter [01000, 05004], lr: 0.008646, loss: 1.0309
2022-06-28 21:14:53 - train: epoch 0098, iter [01100, 05004], lr: 0.008631, loss: 0.9416
2022-06-28 21:15:28 - train: epoch 0098, iter [01200, 05004], lr: 0.008617, loss: 1.0543
2022-06-28 21:16:03 - train: epoch 0098, iter [01300, 05004], lr: 0.008602, loss: 1.2522
2022-06-28 21:16:37 - train: epoch 0098, iter [01400, 05004], lr: 0.008587, loss: 1.1642
2022-06-28 21:17:11 - train: epoch 0098, iter [01500, 05004], lr: 0.008573, loss: 1.1380
2022-06-28 21:17:47 - train: epoch 0098, iter [01600, 05004], lr: 0.008558, loss: 1.1350
2022-06-28 21:18:21 - train: epoch 0098, iter [01700, 05004], lr: 0.008543, loss: 1.2122
2022-06-28 21:18:56 - train: epoch 0098, iter [01800, 05004], lr: 0.008529, loss: 1.1180
2022-06-28 21:19:31 - train: epoch 0098, iter [01900, 05004], lr: 0.008514, loss: 1.0535
2022-06-28 21:20:06 - train: epoch 0098, iter [02000, 05004], lr: 0.008500, loss: 1.3064
2022-06-28 21:20:41 - train: epoch 0098, iter [02100, 05004], lr: 0.008485, loss: 1.1495
2022-06-28 21:21:16 - train: epoch 0098, iter [02200, 05004], lr: 0.008470, loss: 1.1756
2022-06-28 21:21:50 - train: epoch 0098, iter [02300, 05004], lr: 0.008456, loss: 1.1356
2022-06-28 21:22:25 - train: epoch 0098, iter [02400, 05004], lr: 0.008441, loss: 1.1071
2022-06-28 21:23:00 - train: epoch 0098, iter [02500, 05004], lr: 0.008427, loss: 1.2986
2022-06-28 21:23:34 - train: epoch 0098, iter [02600, 05004], lr: 0.008412, loss: 1.1106
2022-06-28 21:24:09 - train: epoch 0098, iter [02700, 05004], lr: 0.008398, loss: 1.2489
2022-06-28 21:24:43 - train: epoch 0098, iter [02800, 05004], lr: 0.008383, loss: 1.1005
2022-06-28 21:25:19 - train: epoch 0098, iter [02900, 05004], lr: 0.008369, loss: 1.1370
2022-06-28 21:25:54 - train: epoch 0098, iter [03000, 05004], lr: 0.008354, loss: 1.0900
2022-06-28 21:26:29 - train: epoch 0098, iter [03100, 05004], lr: 0.008340, loss: 1.1111
2022-06-28 21:27:04 - train: epoch 0098, iter [03200, 05004], lr: 0.008325, loss: 0.9793
2022-06-28 21:27:39 - train: epoch 0098, iter [03300, 05004], lr: 0.008311, loss: 1.0262
2022-06-28 21:28:13 - train: epoch 0098, iter [03400, 05004], lr: 0.008296, loss: 1.2559
2022-06-28 21:28:48 - train: epoch 0098, iter [03500, 05004], lr: 0.008282, loss: 1.2463
2022-06-28 21:29:22 - train: epoch 0098, iter [03600, 05004], lr: 0.008268, loss: 1.3410
2022-06-28 21:29:57 - train: epoch 0098, iter [03700, 05004], lr: 0.008253, loss: 1.2922
2022-06-28 21:30:32 - train: epoch 0098, iter [03800, 05004], lr: 0.008239, loss: 1.2204
2022-06-28 21:31:07 - train: epoch 0098, iter [03900, 05004], lr: 0.008224, loss: 1.1308
2022-06-28 21:31:42 - train: epoch 0098, iter [04000, 05004], lr: 0.008210, loss: 1.1975
2022-06-28 21:32:17 - train: epoch 0098, iter [04100, 05004], lr: 0.008196, loss: 1.3392
2022-06-28 21:32:52 - train: epoch 0098, iter [04200, 05004], lr: 0.008181, loss: 1.1691
2022-06-28 21:33:27 - train: epoch 0098, iter [04300, 05004], lr: 0.008167, loss: 0.9969
2022-06-28 21:34:01 - train: epoch 0098, iter [04400, 05004], lr: 0.008153, loss: 1.4108
2022-06-28 21:34:36 - train: epoch 0098, iter [04500, 05004], lr: 0.008138, loss: 1.3884
2022-06-28 21:35:12 - train: epoch 0098, iter [04600, 05004], lr: 0.008124, loss: 1.3294
2022-06-28 21:35:46 - train: epoch 0098, iter [04700, 05004], lr: 0.008110, loss: 1.1920
2022-06-28 21:36:21 - train: epoch 0098, iter [04800, 05004], lr: 0.008096, loss: 1.0293
2022-06-28 21:36:56 - train: epoch 0098, iter [04900, 05004], lr: 0.008081, loss: 1.2874
2022-06-28 21:37:29 - train: epoch 0098, iter [05000, 05004], lr: 0.008067, loss: 1.3249
2022-06-28 21:37:30 - train: epoch 098, train_loss: 1.1787
2022-06-28 21:38:47 - eval: epoch: 098, acc1: 72.532%, acc5: 91.006%, test_loss: 1.1030, per_image_load_time: 2.535ms, per_image_inference_time: 0.398ms
2022-06-28 21:38:48 - until epoch: 098, best_acc1: 72.532%
2022-06-28 21:38:48 - epoch 099 lr: 0.008066
2022-06-28 21:39:27 - train: epoch 0099, iter [00100, 05004], lr: 0.008052, loss: 0.8856
2022-06-28 21:40:01 - train: epoch 0099, iter [00200, 05004], lr: 0.008038, loss: 1.0613
2022-06-28 21:40:36 - train: epoch 0099, iter [00300, 05004], lr: 0.008024, loss: 1.1188
2022-06-28 21:41:12 - train: epoch 0099, iter [00400, 05004], lr: 0.008010, loss: 1.2262
2022-06-28 21:41:46 - train: epoch 0099, iter [00500, 05004], lr: 0.007995, loss: 1.2030
2022-06-28 21:42:21 - train: epoch 0099, iter [00600, 05004], lr: 0.007981, loss: 1.1235
2022-06-28 21:42:57 - train: epoch 0099, iter [00700, 05004], lr: 0.007967, loss: 1.1079
2022-06-28 21:43:31 - train: epoch 0099, iter [00800, 05004], lr: 0.007953, loss: 1.1649
2022-06-28 21:44:05 - train: epoch 0099, iter [00900, 05004], lr: 0.007939, loss: 1.0099
2022-06-28 21:44:40 - train: epoch 0099, iter [01000, 05004], lr: 0.007925, loss: 1.1648
2022-06-28 21:45:15 - train: epoch 0099, iter [01100, 05004], lr: 0.007910, loss: 1.1821
2022-06-28 21:45:50 - train: epoch 0099, iter [01200, 05004], lr: 0.007896, loss: 1.0769
2022-06-28 21:46:26 - train: epoch 0099, iter [01300, 05004], lr: 0.007882, loss: 1.2390
2022-06-28 21:47:01 - train: epoch 0099, iter [01400, 05004], lr: 0.007868, loss: 1.1527
2022-06-28 21:47:36 - train: epoch 0099, iter [01500, 05004], lr: 0.007854, loss: 0.9774
2022-06-28 21:48:10 - train: epoch 0099, iter [01600, 05004], lr: 0.007840, loss: 1.4110
2022-06-28 21:48:45 - train: epoch 0099, iter [01700, 05004], lr: 0.007826, loss: 1.0398
2022-06-28 21:49:20 - train: epoch 0099, iter [01800, 05004], lr: 0.007812, loss: 1.1673
2022-06-28 21:49:55 - train: epoch 0099, iter [01900, 05004], lr: 0.007798, loss: 1.1472
2022-06-28 21:50:31 - train: epoch 0099, iter [02000, 05004], lr: 0.007784, loss: 0.9601
2022-06-28 21:51:06 - train: epoch 0099, iter [02100, 05004], lr: 0.007770, loss: 1.0821
2022-06-28 21:51:41 - train: epoch 0099, iter [02200, 05004], lr: 0.007756, loss: 1.1363
2022-06-28 21:52:16 - train: epoch 0099, iter [02300, 05004], lr: 0.007742, loss: 1.2668
2022-06-28 21:52:52 - train: epoch 0099, iter [02400, 05004], lr: 0.007728, loss: 1.3233
2022-06-28 21:53:26 - train: epoch 0099, iter [02500, 05004], lr: 0.007714, loss: 1.1063
2022-06-28 21:54:01 - train: epoch 0099, iter [02600, 05004], lr: 0.007700, loss: 0.9734
2022-06-28 21:54:37 - train: epoch 0099, iter [02700, 05004], lr: 0.007686, loss: 1.1971
2022-06-28 21:55:12 - train: epoch 0099, iter [02800, 05004], lr: 0.007672, loss: 1.2521
2022-06-28 21:55:47 - train: epoch 0099, iter [02900, 05004], lr: 0.007658, loss: 1.1410
2022-06-28 21:56:22 - train: epoch 0099, iter [03000, 05004], lr: 0.007644, loss: 1.3392
2022-06-28 21:56:56 - train: epoch 0099, iter [03100, 05004], lr: 0.007630, loss: 1.0767
2022-06-28 21:57:32 - train: epoch 0099, iter [03200, 05004], lr: 0.007616, loss: 1.2932
2022-06-28 21:58:06 - train: epoch 0099, iter [03300, 05004], lr: 0.007603, loss: 1.0648
2022-06-28 21:58:42 - train: epoch 0099, iter [03400, 05004], lr: 0.007589, loss: 1.2009
2022-06-28 21:59:16 - train: epoch 0099, iter [03500, 05004], lr: 0.007575, loss: 1.1728
2022-06-28 21:59:51 - train: epoch 0099, iter [03600, 05004], lr: 0.007561, loss: 1.0879
2022-06-28 22:00:26 - train: epoch 0099, iter [03700, 05004], lr: 0.007547, loss: 1.0560
2022-06-28 22:01:01 - train: epoch 0099, iter [03800, 05004], lr: 0.007533, loss: 1.2880
2022-06-28 22:01:37 - train: epoch 0099, iter [03900, 05004], lr: 0.007520, loss: 1.2408
2022-06-28 22:02:11 - train: epoch 0099, iter [04000, 05004], lr: 0.007506, loss: 1.1988
2022-06-28 22:02:45 - train: epoch 0099, iter [04100, 05004], lr: 0.007492, loss: 1.0892
2022-06-28 22:03:20 - train: epoch 0099, iter [04200, 05004], lr: 0.007478, loss: 1.1600
2022-06-28 22:03:55 - train: epoch 0099, iter [04300, 05004], lr: 0.007465, loss: 1.1650
2022-06-28 22:04:30 - train: epoch 0099, iter [04400, 05004], lr: 0.007451, loss: 1.2053
2022-06-28 22:05:04 - train: epoch 0099, iter [04500, 05004], lr: 0.007437, loss: 1.3089
2022-06-28 22:05:39 - train: epoch 0099, iter [04600, 05004], lr: 0.007423, loss: 1.2613
2022-06-28 22:06:14 - train: epoch 0099, iter [04700, 05004], lr: 0.007410, loss: 1.0842
2022-06-28 22:06:49 - train: epoch 0099, iter [04800, 05004], lr: 0.007396, loss: 1.3288
2022-06-28 22:07:23 - train: epoch 0099, iter [04900, 05004], lr: 0.007382, loss: 1.1098
2022-06-28 22:07:58 - train: epoch 0099, iter [05000, 05004], lr: 0.007369, loss: 1.1581
2022-06-28 22:07:59 - train: epoch 099, train_loss: 1.1618
2022-06-28 22:09:16 - eval: epoch: 099, acc1: 73.018%, acc5: 91.098%, test_loss: 1.0846, per_image_load_time: 2.484ms, per_image_inference_time: 0.413ms
2022-06-28 22:09:17 - until epoch: 099, best_acc1: 73.018%
2022-06-28 22:09:17 - epoch 100 lr: 0.007368
2022-06-28 22:09:57 - train: epoch 0100, iter [00100, 05004], lr: 0.007354, loss: 1.1417
2022-06-28 22:10:31 - train: epoch 0100, iter [00200, 05004], lr: 0.007341, loss: 1.1906
2022-06-28 22:11:06 - train: epoch 0100, iter [00300, 05004], lr: 0.007327, loss: 1.1135
2022-06-28 22:11:40 - train: epoch 0100, iter [00400, 05004], lr: 0.007313, loss: 0.8922
2022-06-28 22:12:14 - train: epoch 0100, iter [00500, 05004], lr: 0.007300, loss: 1.2330
2022-06-28 22:12:48 - train: epoch 0100, iter [00600, 05004], lr: 0.007286, loss: 1.3905
2022-06-28 22:13:23 - train: epoch 0100, iter [00700, 05004], lr: 0.007273, loss: 0.9428
2022-06-28 22:13:57 - train: epoch 0100, iter [00800, 05004], lr: 0.007259, loss: 1.1859
2022-06-28 22:14:31 - train: epoch 0100, iter [00900, 05004], lr: 0.007245, loss: 0.9406
2022-06-28 22:15:06 - train: epoch 0100, iter [01000, 05004], lr: 0.007232, loss: 1.1793
2022-06-28 22:15:40 - train: epoch 0100, iter [01100, 05004], lr: 0.007218, loss: 1.0875
2022-06-28 22:16:15 - train: epoch 0100, iter [01200, 05004], lr: 0.007205, loss: 1.1722
2022-06-28 22:16:49 - train: epoch 0100, iter [01300, 05004], lr: 0.007191, loss: 1.2280
2022-06-28 22:17:23 - train: epoch 0100, iter [01400, 05004], lr: 0.007178, loss: 1.1517
2022-06-28 22:17:57 - train: epoch 0100, iter [01500, 05004], lr: 0.007164, loss: 1.1874
2022-06-28 22:18:32 - train: epoch 0100, iter [01600, 05004], lr: 0.007151, loss: 1.0735
2022-06-28 22:19:06 - train: epoch 0100, iter [01700, 05004], lr: 0.007137, loss: 1.1757
2022-06-28 22:19:41 - train: epoch 0100, iter [01800, 05004], lr: 0.007124, loss: 1.1581
2022-06-28 22:20:15 - train: epoch 0100, iter [01900, 05004], lr: 0.007110, loss: 1.1019
2022-06-28 22:20:50 - train: epoch 0100, iter [02000, 05004], lr: 0.007097, loss: 1.0691
2022-06-28 22:21:24 - train: epoch 0100, iter [02100, 05004], lr: 0.007084, loss: 1.1628
2022-06-28 22:21:59 - train: epoch 0100, iter [02200, 05004], lr: 0.007070, loss: 1.1872
2022-06-28 22:22:34 - train: epoch 0100, iter [02300, 05004], lr: 0.007057, loss: 1.0903
2022-06-28 22:23:09 - train: epoch 0100, iter [02400, 05004], lr: 0.007043, loss: 1.1418
2022-06-28 22:23:44 - train: epoch 0100, iter [02500, 05004], lr: 0.007030, loss: 1.0491
2022-06-28 22:24:18 - train: epoch 0100, iter [02600, 05004], lr: 0.007017, loss: 1.1944
2022-06-28 22:24:53 - train: epoch 0100, iter [02700, 05004], lr: 0.007003, loss: 1.1128
2022-06-28 22:25:27 - train: epoch 0100, iter [02800, 05004], lr: 0.006990, loss: 1.1303
2022-06-28 22:26:01 - train: epoch 0100, iter [02900, 05004], lr: 0.006977, loss: 1.2328
2022-06-28 22:26:36 - train: epoch 0100, iter [03000, 05004], lr: 0.006963, loss: 1.0174
2022-06-28 22:27:10 - train: epoch 0100, iter [03100, 05004], lr: 0.006950, loss: 1.1505
2022-06-28 22:27:43 - train: epoch 0100, iter [03200, 05004], lr: 0.006937, loss: 1.1736
2022-06-28 22:28:18 - train: epoch 0100, iter [03300, 05004], lr: 0.006923, loss: 1.2568
2022-06-28 22:28:52 - train: epoch 0100, iter [03400, 05004], lr: 0.006910, loss: 1.2930
2022-06-28 22:29:27 - train: epoch 0100, iter [03500, 05004], lr: 0.006897, loss: 1.0375
2022-06-28 22:30:02 - train: epoch 0100, iter [03600, 05004], lr: 0.006884, loss: 1.2052
2022-06-28 22:30:36 - train: epoch 0100, iter [03700, 05004], lr: 0.006870, loss: 1.1636
2022-06-28 22:31:11 - train: epoch 0100, iter [03800, 05004], lr: 0.006857, loss: 1.1685
2022-06-28 22:31:45 - train: epoch 0100, iter [03900, 05004], lr: 0.006844, loss: 1.2282
2022-06-28 22:32:19 - train: epoch 0100, iter [04000, 05004], lr: 0.006831, loss: 1.0190
2022-06-28 22:32:54 - train: epoch 0100, iter [04100, 05004], lr: 0.006817, loss: 1.2128
2022-06-28 22:33:29 - train: epoch 0100, iter [04200, 05004], lr: 0.006804, loss: 1.1579
2022-06-28 22:34:03 - train: epoch 0100, iter [04300, 05004], lr: 0.006791, loss: 1.1441
2022-06-28 22:34:39 - train: epoch 0100, iter [04400, 05004], lr: 0.006778, loss: 1.2370
2022-06-28 22:35:12 - train: epoch 0100, iter [04500, 05004], lr: 0.006765, loss: 1.0909
2022-06-28 22:35:47 - train: epoch 0100, iter [04600, 05004], lr: 0.006752, loss: 0.9722
2022-06-28 22:36:21 - train: epoch 0100, iter [04700, 05004], lr: 0.006739, loss: 1.1511
2022-06-28 22:36:56 - train: epoch 0100, iter [04800, 05004], lr: 0.006725, loss: 0.8887
2022-06-28 22:37:31 - train: epoch 0100, iter [04900, 05004], lr: 0.006712, loss: 1.1208
2022-06-28 22:38:03 - train: epoch 0100, iter [05000, 05004], lr: 0.006699, loss: 1.2530
2022-06-28 22:38:05 - train: epoch 100, train_loss: 1.1420
2022-06-28 22:39:21 - eval: epoch: 100, acc1: 73.118%, acc5: 91.212%, test_loss: 1.0824, per_image_load_time: 2.103ms, per_image_inference_time: 0.434ms
2022-06-28 22:39:22 - until epoch: 100, best_acc1: 73.118%
2022-06-28 22:39:22 - epoch 101 lr: 0.006699
2022-06-28 22:40:01 - train: epoch 0101, iter [00100, 05004], lr: 0.006686, loss: 0.9611
2022-06-28 22:40:36 - train: epoch 0101, iter [00200, 05004], lr: 0.006673, loss: 1.0101
2022-06-28 22:41:11 - train: epoch 0101, iter [00300, 05004], lr: 0.006660, loss: 0.8882
2022-06-28 22:41:46 - train: epoch 0101, iter [00400, 05004], lr: 0.006647, loss: 1.0171
2022-06-28 22:42:20 - train: epoch 0101, iter [00500, 05004], lr: 0.006633, loss: 1.0357
2022-06-28 22:42:55 - train: epoch 0101, iter [00600, 05004], lr: 0.006620, loss: 1.1452
2022-06-28 22:43:29 - train: epoch 0101, iter [00700, 05004], lr: 0.006607, loss: 1.1818
2022-06-28 22:44:04 - train: epoch 0101, iter [00800, 05004], lr: 0.006594, loss: 1.1907
2022-06-28 22:44:38 - train: epoch 0101, iter [00900, 05004], lr: 0.006581, loss: 1.1950
2022-06-28 22:45:12 - train: epoch 0101, iter [01000, 05004], lr: 0.006569, loss: 1.2750
2022-06-28 22:45:47 - train: epoch 0101, iter [01100, 05004], lr: 0.006556, loss: 1.1189
2022-06-28 22:46:21 - train: epoch 0101, iter [01200, 05004], lr: 0.006543, loss: 1.0270
2022-06-28 22:46:57 - train: epoch 0101, iter [01300, 05004], lr: 0.006530, loss: 1.2802
2022-06-28 22:47:31 - train: epoch 0101, iter [01400, 05004], lr: 0.006517, loss: 1.0455
2022-06-28 22:48:05 - train: epoch 0101, iter [01500, 05004], lr: 0.006504, loss: 1.0357
2022-06-28 22:48:40 - train: epoch 0101, iter [01600, 05004], lr: 0.006491, loss: 0.9452
2022-06-28 22:49:15 - train: epoch 0101, iter [01700, 05004], lr: 0.006478, loss: 1.0215
2022-06-28 22:49:49 - train: epoch 0101, iter [01800, 05004], lr: 0.006465, loss: 1.2916
2022-06-28 22:50:25 - train: epoch 0101, iter [01900, 05004], lr: 0.006452, loss: 0.9461
2022-06-28 22:50:58 - train: epoch 0101, iter [02000, 05004], lr: 0.006440, loss: 0.9199
2022-06-28 22:51:33 - train: epoch 0101, iter [02100, 05004], lr: 0.006427, loss: 1.2163
2022-06-28 22:52:09 - train: epoch 0101, iter [02200, 05004], lr: 0.006414, loss: 0.9682
2022-06-28 22:52:43 - train: epoch 0101, iter [02300, 05004], lr: 0.006401, loss: 1.0289
2022-06-28 22:53:17 - train: epoch 0101, iter [02400, 05004], lr: 0.006388, loss: 1.0767
2022-06-28 22:53:52 - train: epoch 0101, iter [02500, 05004], lr: 0.006375, loss: 1.0619
2022-06-28 22:54:27 - train: epoch 0101, iter [02600, 05004], lr: 0.006363, loss: 1.1824
2022-06-28 22:55:02 - train: epoch 0101, iter [02700, 05004], lr: 0.006350, loss: 1.2839
2022-06-28 22:55:36 - train: epoch 0101, iter [02800, 05004], lr: 0.006337, loss: 1.1669
2022-06-28 22:56:11 - train: epoch 0101, iter [02900, 05004], lr: 0.006324, loss: 1.3183
2022-06-28 22:56:45 - train: epoch 0101, iter [03000, 05004], lr: 0.006312, loss: 1.2962
2022-06-28 22:57:19 - train: epoch 0101, iter [03100, 05004], lr: 0.006299, loss: 1.2562
2022-06-28 22:57:55 - train: epoch 0101, iter [03200, 05004], lr: 0.006286, loss: 1.2094
2022-06-28 22:58:29 - train: epoch 0101, iter [03300, 05004], lr: 0.006274, loss: 1.0874
2022-06-28 22:59:03 - train: epoch 0101, iter [03400, 05004], lr: 0.006261, loss: 1.1559
2022-06-28 22:59:38 - train: epoch 0101, iter [03500, 05004], lr: 0.006248, loss: 1.0003
2022-06-28 23:00:13 - train: epoch 0101, iter [03600, 05004], lr: 0.006236, loss: 1.1328
2022-06-28 23:00:48 - train: epoch 0101, iter [03700, 05004], lr: 0.006223, loss: 1.3131
2022-06-28 23:01:22 - train: epoch 0101, iter [03800, 05004], lr: 0.006210, loss: 1.1445
2022-06-28 23:01:57 - train: epoch 0101, iter [03900, 05004], lr: 0.006198, loss: 1.2423
2022-06-28 23:02:31 - train: epoch 0101, iter [04000, 05004], lr: 0.006185, loss: 1.2647
2022-06-28 23:03:05 - train: epoch 0101, iter [04100, 05004], lr: 0.006172, loss: 1.0664
2022-06-28 23:03:40 - train: epoch 0101, iter [04200, 05004], lr: 0.006160, loss: 1.1636
2022-06-28 23:04:15 - train: epoch 0101, iter [04300, 05004], lr: 0.006147, loss: 1.1005
2022-06-28 23:04:50 - train: epoch 0101, iter [04400, 05004], lr: 0.006135, loss: 1.0093
2022-06-28 23:05:24 - train: epoch 0101, iter [04500, 05004], lr: 0.006122, loss: 1.4303
2022-06-28 23:05:59 - train: epoch 0101, iter [04600, 05004], lr: 0.006110, loss: 1.1170
2022-06-28 23:06:33 - train: epoch 0101, iter [04700, 05004], lr: 0.006097, loss: 1.1243
2022-06-28 23:07:08 - train: epoch 0101, iter [04800, 05004], lr: 0.006085, loss: 1.1648
2022-06-28 23:07:42 - train: epoch 0101, iter [04900, 05004], lr: 0.006072, loss: 1.0804
2022-06-28 23:08:15 - train: epoch 0101, iter [05000, 05004], lr: 0.006060, loss: 1.0948
2022-06-28 23:08:16 - train: epoch 101, train_loss: 1.1212
2022-06-28 23:09:33 - eval: epoch: 101, acc1: 73.460%, acc5: 91.544%, test_loss: 1.0638, per_image_load_time: 2.215ms, per_image_inference_time: 0.415ms
2022-06-28 23:09:33 - until epoch: 101, best_acc1: 73.460%
2022-06-28 23:09:33 - epoch 102 lr: 0.006059
2022-06-28 23:10:13 - train: epoch 0102, iter [00100, 05004], lr: 0.006047, loss: 1.1213
2022-06-28 23:10:47 - train: epoch 0102, iter [00200, 05004], lr: 0.006034, loss: 1.0103
2022-06-28 23:11:22 - train: epoch 0102, iter [00300, 05004], lr: 0.006022, loss: 1.0576
2022-06-28 23:11:56 - train: epoch 0102, iter [00400, 05004], lr: 0.006009, loss: 1.0455
2022-06-28 23:12:30 - train: epoch 0102, iter [00500, 05004], lr: 0.005997, loss: 1.1114
2022-06-28 23:13:05 - train: epoch 0102, iter [00600, 05004], lr: 0.005984, loss: 0.9871
2022-06-28 23:13:40 - train: epoch 0102, iter [00700, 05004], lr: 0.005972, loss: 1.0091
2022-06-28 23:14:13 - train: epoch 0102, iter [00800, 05004], lr: 0.005960, loss: 1.0944
2022-06-28 23:14:48 - train: epoch 0102, iter [00900, 05004], lr: 0.005947, loss: 0.9919
2022-06-28 23:15:22 - train: epoch 0102, iter [01000, 05004], lr: 0.005935, loss: 1.1553
2022-06-28 23:15:57 - train: epoch 0102, iter [01100, 05004], lr: 0.005923, loss: 0.9709
2022-06-28 23:16:32 - train: epoch 0102, iter [01200, 05004], lr: 0.005910, loss: 1.2725
2022-06-28 23:17:06 - train: epoch 0102, iter [01300, 05004], lr: 0.005898, loss: 1.1429
2022-06-28 23:17:40 - train: epoch 0102, iter [01400, 05004], lr: 0.005886, loss: 1.1342
2022-06-28 23:18:15 - train: epoch 0102, iter [01500, 05004], lr: 0.005873, loss: 1.0377
2022-06-28 23:18:49 - train: epoch 0102, iter [01600, 05004], lr: 0.005861, loss: 1.2279
2022-06-28 23:19:24 - train: epoch 0102, iter [01700, 05004], lr: 0.005849, loss: 1.0516
2022-06-28 23:19:59 - train: epoch 0102, iter [01800, 05004], lr: 0.005836, loss: 1.1917
2022-06-28 23:20:33 - train: epoch 0102, iter [01900, 05004], lr: 0.005824, loss: 1.0821
2022-06-28 23:21:08 - train: epoch 0102, iter [02000, 05004], lr: 0.005812, loss: 1.0427
2022-06-28 23:21:42 - train: epoch 0102, iter [02100, 05004], lr: 0.005800, loss: 1.0009
2022-06-28 23:22:17 - train: epoch 0102, iter [02200, 05004], lr: 0.005787, loss: 1.0505
2022-06-28 23:22:51 - train: epoch 0102, iter [02300, 05004], lr: 0.005775, loss: 1.1537
2022-06-28 23:23:26 - train: epoch 0102, iter [02400, 05004], lr: 0.005763, loss: 1.2063
2022-06-28 23:24:00 - train: epoch 0102, iter [02500, 05004], lr: 0.005751, loss: 1.1448
2022-06-28 23:24:35 - train: epoch 0102, iter [02600, 05004], lr: 0.005739, loss: 1.0625
2022-06-28 23:25:09 - train: epoch 0102, iter [02700, 05004], lr: 0.005727, loss: 1.4195
2022-06-28 23:25:44 - train: epoch 0102, iter [02800, 05004], lr: 0.005714, loss: 1.2013
2022-06-28 23:26:18 - train: epoch 0102, iter [02900, 05004], lr: 0.005702, loss: 1.1556
2022-06-28 23:26:53 - train: epoch 0102, iter [03000, 05004], lr: 0.005690, loss: 1.0147
2022-06-28 23:27:27 - train: epoch 0102, iter [03100, 05004], lr: 0.005678, loss: 1.0501
2022-06-28 23:28:03 - train: epoch 0102, iter [03200, 05004], lr: 0.005666, loss: 0.9568
2022-06-28 23:28:37 - train: epoch 0102, iter [03300, 05004], lr: 0.005654, loss: 1.0308
2022-06-28 23:29:11 - train: epoch 0102, iter [03400, 05004], lr: 0.005642, loss: 1.1334
2022-06-28 23:29:47 - train: epoch 0102, iter [03500, 05004], lr: 0.005630, loss: 1.0258
2022-06-28 23:30:21 - train: epoch 0102, iter [03600, 05004], lr: 0.005618, loss: 1.2443
2022-06-28 23:30:56 - train: epoch 0102, iter [03700, 05004], lr: 0.005606, loss: 1.2017
2022-06-28 23:31:30 - train: epoch 0102, iter [03800, 05004], lr: 0.005594, loss: 1.0799
2022-06-28 23:32:04 - train: epoch 0102, iter [03900, 05004], lr: 0.005582, loss: 1.2296
2022-06-28 23:32:40 - train: epoch 0102, iter [04000, 05004], lr: 0.005570, loss: 0.9142
2022-06-28 23:33:14 - train: epoch 0102, iter [04100, 05004], lr: 0.005558, loss: 1.0708
2022-06-28 23:33:49 - train: epoch 0102, iter [04200, 05004], lr: 0.005546, loss: 1.3105
2022-06-28 23:34:23 - train: epoch 0102, iter [04300, 05004], lr: 0.005534, loss: 0.9868
2022-06-28 23:34:57 - train: epoch 0102, iter [04400, 05004], lr: 0.005522, loss: 0.9629
2022-06-28 23:35:32 - train: epoch 0102, iter [04500, 05004], lr: 0.005510, loss: 1.1660
2022-06-28 23:36:07 - train: epoch 0102, iter [04600, 05004], lr: 0.005498, loss: 1.3253
2022-06-28 23:36:42 - train: epoch 0102, iter [04700, 05004], lr: 0.005486, loss: 1.1163
2022-06-28 23:37:16 - train: epoch 0102, iter [04800, 05004], lr: 0.005474, loss: 1.2726
2022-06-28 23:37:52 - train: epoch 0102, iter [04900, 05004], lr: 0.005462, loss: 1.2435
2022-06-28 23:38:24 - train: epoch 0102, iter [05000, 05004], lr: 0.005450, loss: 0.9358
2022-06-28 23:38:26 - train: epoch 102, train_loss: 1.1001
2022-06-28 23:39:42 - eval: epoch: 102, acc1: 73.800%, acc5: 91.618%, test_loss: 1.0482, per_image_load_time: 2.261ms, per_image_inference_time: 0.433ms
2022-06-28 23:39:42 - until epoch: 102, best_acc1: 73.800%
2022-06-28 23:39:42 - epoch 103 lr: 0.005450
2022-06-28 23:40:23 - train: epoch 0103, iter [00100, 05004], lr: 0.005438, loss: 1.0273
2022-06-28 23:40:58 - train: epoch 0103, iter [00200, 05004], lr: 0.005426, loss: 1.1711
2022-06-28 23:41:32 - train: epoch 0103, iter [00300, 05004], lr: 0.005414, loss: 0.9876
2022-06-28 23:42:06 - train: epoch 0103, iter [00400, 05004], lr: 0.005402, loss: 1.2263
2022-06-28 23:42:41 - train: epoch 0103, iter [00500, 05004], lr: 0.005390, loss: 1.0271
2022-06-28 23:43:16 - train: epoch 0103, iter [00600, 05004], lr: 0.005379, loss: 0.9154
2022-06-28 23:43:50 - train: epoch 0103, iter [00700, 05004], lr: 0.005367, loss: 1.0868
2022-06-28 23:44:25 - train: epoch 0103, iter [00800, 05004], lr: 0.005355, loss: 1.0304
2022-06-28 23:45:01 - train: epoch 0103, iter [00900, 05004], lr: 0.005343, loss: 1.0632
2022-06-28 23:45:35 - train: epoch 0103, iter [01000, 05004], lr: 0.005332, loss: 1.0463
2022-06-28 23:46:10 - train: epoch 0103, iter [01100, 05004], lr: 0.005320, loss: 1.0234
2022-06-28 23:46:45 - train: epoch 0103, iter [01200, 05004], lr: 0.005308, loss: 1.0403
2022-06-28 23:47:19 - train: epoch 0103, iter [01300, 05004], lr: 0.005296, loss: 1.2186
2022-06-28 23:47:54 - train: epoch 0103, iter [01400, 05004], lr: 0.005285, loss: 1.2123
2022-06-28 23:48:28 - train: epoch 0103, iter [01500, 05004], lr: 0.005273, loss: 1.2804
2022-06-28 23:49:03 - train: epoch 0103, iter [01600, 05004], lr: 0.005261, loss: 0.7661
2022-06-28 23:49:38 - train: epoch 0103, iter [01700, 05004], lr: 0.005250, loss: 1.1251
2022-06-28 23:50:13 - train: epoch 0103, iter [01800, 05004], lr: 0.005238, loss: 0.9786
2022-06-28 23:50:47 - train: epoch 0103, iter [01900, 05004], lr: 0.005226, loss: 1.0296
2022-06-28 23:51:22 - train: epoch 0103, iter [02000, 05004], lr: 0.005215, loss: 0.8928
2022-06-28 23:51:56 - train: epoch 0103, iter [02100, 05004], lr: 0.005203, loss: 1.1215
2022-06-28 23:52:31 - train: epoch 0103, iter [02200, 05004], lr: 0.005191, loss: 1.1104
2022-06-28 23:53:05 - train: epoch 0103, iter [02300, 05004], lr: 0.005180, loss: 1.1485
2022-06-28 23:53:39 - train: epoch 0103, iter [02400, 05004], lr: 0.005168, loss: 1.1268
2022-06-28 23:54:15 - train: epoch 0103, iter [02500, 05004], lr: 0.005157, loss: 1.0683
2022-06-28 23:54:48 - train: epoch 0103, iter [02600, 05004], lr: 0.005145, loss: 1.0537
2022-06-28 23:55:23 - train: epoch 0103, iter [02700, 05004], lr: 0.005133, loss: 1.1338
2022-06-28 23:55:58 - train: epoch 0103, iter [02800, 05004], lr: 0.005122, loss: 0.8173
2022-06-28 23:56:33 - train: epoch 0103, iter [02900, 05004], lr: 0.005110, loss: 1.0821
2022-06-28 23:57:08 - train: epoch 0103, iter [03000, 05004], lr: 0.005099, loss: 1.1912
2022-06-28 23:57:43 - train: epoch 0103, iter [03100, 05004], lr: 0.005087, loss: 1.0532
2022-06-28 23:58:17 - train: epoch 0103, iter [03200, 05004], lr: 0.005076, loss: 1.0756
2022-06-28 23:58:52 - train: epoch 0103, iter [03300, 05004], lr: 0.005064, loss: 1.0429
2022-06-28 23:59:27 - train: epoch 0103, iter [03400, 05004], lr: 0.005053, loss: 1.1096
2022-06-29 00:00:02 - train: epoch 0103, iter [03500, 05004], lr: 0.005042, loss: 1.1716
2022-06-29 00:00:37 - train: epoch 0103, iter [03600, 05004], lr: 0.005030, loss: 0.9975
2022-06-29 00:01:12 - train: epoch 0103, iter [03700, 05004], lr: 0.005019, loss: 1.0316
2022-06-29 00:01:46 - train: epoch 0103, iter [03800, 05004], lr: 0.005007, loss: 1.0432
2022-06-29 00:02:21 - train: epoch 0103, iter [03900, 05004], lr: 0.004996, loss: 1.0768
2022-06-29 00:02:56 - train: epoch 0103, iter [04000, 05004], lr: 0.004984, loss: 1.0503
2022-06-29 00:03:31 - train: epoch 0103, iter [04100, 05004], lr: 0.004973, loss: 1.0222
2022-06-29 00:04:05 - train: epoch 0103, iter [04200, 05004], lr: 0.004962, loss: 1.0711
2022-06-29 00:04:40 - train: epoch 0103, iter [04300, 05004], lr: 0.004950, loss: 1.1986
2022-06-29 00:05:16 - train: epoch 0103, iter [04400, 05004], lr: 0.004939, loss: 1.1000
2022-06-29 00:05:49 - train: epoch 0103, iter [04500, 05004], lr: 0.004928, loss: 1.2554
2022-06-29 00:06:24 - train: epoch 0103, iter [04600, 05004], lr: 0.004916, loss: 1.2739
2022-06-29 00:06:59 - train: epoch 0103, iter [04700, 05004], lr: 0.004905, loss: 1.0042
2022-06-29 00:07:34 - train: epoch 0103, iter [04800, 05004], lr: 0.004894, loss: 1.3184
2022-06-29 00:08:08 - train: epoch 0103, iter [04900, 05004], lr: 0.004882, loss: 0.9933
2022-06-29 00:08:41 - train: epoch 0103, iter [05000, 05004], lr: 0.004871, loss: 1.1044
2022-06-29 00:08:42 - train: epoch 103, train_loss: 1.0832
2022-06-29 00:09:59 - eval: epoch: 103, acc1: 74.068%, acc5: 91.694%, test_loss: 1.0420, per_image_load_time: 1.883ms, per_image_inference_time: 0.454ms
2022-06-29 00:10:00 - until epoch: 103, best_acc1: 74.068%
2022-06-29 00:10:00 - epoch 104 lr: 0.004871
2022-06-29 00:10:40 - train: epoch 0104, iter [00100, 05004], lr: 0.004859, loss: 1.0319
2022-06-29 00:11:14 - train: epoch 0104, iter [00200, 05004], lr: 0.004848, loss: 0.9302
2022-06-29 00:11:49 - train: epoch 0104, iter [00300, 05004], lr: 0.004837, loss: 1.1663
2022-06-29 00:12:22 - train: epoch 0104, iter [00400, 05004], lr: 0.004826, loss: 1.0733
2022-06-29 00:12:57 - train: epoch 0104, iter [00500, 05004], lr: 0.004815, loss: 0.9431
2022-06-29 00:13:32 - train: epoch 0104, iter [00600, 05004], lr: 0.004803, loss: 1.1937
2022-06-29 00:14:07 - train: epoch 0104, iter [00700, 05004], lr: 0.004792, loss: 1.1957
2022-06-29 00:14:41 - train: epoch 0104, iter [00800, 05004], lr: 0.004781, loss: 1.0293
2022-06-29 00:15:16 - train: epoch 0104, iter [00900, 05004], lr: 0.004770, loss: 0.9323
2022-06-29 00:15:51 - train: epoch 0104, iter [01000, 05004], lr: 0.004759, loss: 0.9344
2022-06-29 00:16:25 - train: epoch 0104, iter [01100, 05004], lr: 0.004748, loss: 1.0025
2022-06-29 00:17:00 - train: epoch 0104, iter [01200, 05004], lr: 0.004736, loss: 0.9372
2022-06-29 00:17:35 - train: epoch 0104, iter [01300, 05004], lr: 0.004725, loss: 1.1102
2022-06-29 00:18:10 - train: epoch 0104, iter [01400, 05004], lr: 0.004714, loss: 1.0504
2022-06-29 00:18:45 - train: epoch 0104, iter [01500, 05004], lr: 0.004703, loss: 1.2404
2022-06-29 00:19:20 - train: epoch 0104, iter [01600, 05004], lr: 0.004692, loss: 0.9004
2022-06-29 00:19:55 - train: epoch 0104, iter [01700, 05004], lr: 0.004681, loss: 1.0113
2022-06-29 00:20:30 - train: epoch 0104, iter [01800, 05004], lr: 0.004670, loss: 1.1727
2022-06-29 00:21:05 - train: epoch 0104, iter [01900, 05004], lr: 0.004659, loss: 0.8507
2022-06-29 00:21:40 - train: epoch 0104, iter [02000, 05004], lr: 0.004648, loss: 1.0411
2022-06-29 00:22:14 - train: epoch 0104, iter [02100, 05004], lr: 0.004637, loss: 0.9422
2022-06-29 00:22:50 - train: epoch 0104, iter [02200, 05004], lr: 0.004626, loss: 1.1692
2022-06-29 00:23:24 - train: epoch 0104, iter [02300, 05004], lr: 0.004615, loss: 1.0845
2022-06-29 00:23:59 - train: epoch 0104, iter [02400, 05004], lr: 0.004604, loss: 1.3179
2022-06-29 00:24:34 - train: epoch 0104, iter [02500, 05004], lr: 0.004593, loss: 1.0831
2022-06-29 00:25:08 - train: epoch 0104, iter [02600, 05004], lr: 0.004582, loss: 0.9817
2022-06-29 00:25:44 - train: epoch 0104, iter [02700, 05004], lr: 0.004571, loss: 1.1472
2022-06-29 00:26:18 - train: epoch 0104, iter [02800, 05004], lr: 0.004560, loss: 1.1104
2022-06-29 00:26:54 - train: epoch 0104, iter [02900, 05004], lr: 0.004549, loss: 1.1529
2022-06-29 00:27:28 - train: epoch 0104, iter [03000, 05004], lr: 0.004538, loss: 1.1198
2022-06-29 00:28:03 - train: epoch 0104, iter [03100, 05004], lr: 0.004528, loss: 0.9590
2022-06-29 00:28:38 - train: epoch 0104, iter [03200, 05004], lr: 0.004517, loss: 1.0885
2022-06-29 00:29:13 - train: epoch 0104, iter [03300, 05004], lr: 0.004506, loss: 1.2949
2022-06-29 00:29:48 - train: epoch 0104, iter [03400, 05004], lr: 0.004495, loss: 0.9444
2022-06-29 00:30:23 - train: epoch 0104, iter [03500, 05004], lr: 0.004484, loss: 1.0544
2022-06-29 00:30:57 - train: epoch 0104, iter [03600, 05004], lr: 0.004473, loss: 1.0037
2022-06-29 00:31:32 - train: epoch 0104, iter [03700, 05004], lr: 0.004463, loss: 1.1247
2022-06-29 00:32:07 - train: epoch 0104, iter [03800, 05004], lr: 0.004452, loss: 0.8426
2022-06-29 00:32:42 - train: epoch 0104, iter [03900, 05004], lr: 0.004441, loss: 0.9935
2022-06-29 00:33:16 - train: epoch 0104, iter [04000, 05004], lr: 0.004430, loss: 0.8920
2022-06-29 00:33:51 - train: epoch 0104, iter [04100, 05004], lr: 0.004419, loss: 1.0141
2022-06-29 00:34:25 - train: epoch 0104, iter [04200, 05004], lr: 0.004409, loss: 1.0600
2022-06-29 00:35:00 - train: epoch 0104, iter [04300, 05004], lr: 0.004398, loss: 0.9635
2022-06-29 00:35:35 - train: epoch 0104, iter [04400, 05004], lr: 0.004387, loss: 1.0911
2022-06-29 00:36:10 - train: epoch 0104, iter [04500, 05004], lr: 0.004377, loss: 1.2059
2022-06-29 00:36:44 - train: epoch 0104, iter [04600, 05004], lr: 0.004366, loss: 1.0052
2022-06-29 00:37:19 - train: epoch 0104, iter [04700, 05004], lr: 0.004355, loss: 0.9657
2022-06-29 00:37:53 - train: epoch 0104, iter [04800, 05004], lr: 0.004344, loss: 1.1384
2022-06-29 00:38:29 - train: epoch 0104, iter [04900, 05004], lr: 0.004334, loss: 1.1058
2022-06-29 00:39:02 - train: epoch 0104, iter [05000, 05004], lr: 0.004323, loss: 1.1403
2022-06-29 00:39:03 - train: epoch 104, train_loss: 1.0625
2022-06-29 00:40:20 - eval: epoch: 104, acc1: 74.206%, acc5: 91.888%, test_loss: 1.0340, per_image_load_time: 1.998ms, per_image_inference_time: 0.432ms
2022-06-29 00:40:21 - until epoch: 104, best_acc1: 74.206%
2022-06-29 00:40:21 - epoch 105 lr: 0.004323
2022-06-29 00:41:01 - train: epoch 0105, iter [00100, 05004], lr: 0.004312, loss: 1.0587
2022-06-29 00:41:35 - train: epoch 0105, iter [00200, 05004], lr: 0.004301, loss: 1.1841
2022-06-29 00:42:09 - train: epoch 0105, iter [00300, 05004], lr: 0.004291, loss: 1.1517
2022-06-29 00:42:43 - train: epoch 0105, iter [00400, 05004], lr: 0.004280, loss: 1.0218
2022-06-29 00:43:18 - train: epoch 0105, iter [00500, 05004], lr: 0.004270, loss: 1.0252
2022-06-29 00:43:52 - train: epoch 0105, iter [00600, 05004], lr: 0.004259, loss: 1.1722
2022-06-29 00:44:27 - train: epoch 0105, iter [00700, 05004], lr: 0.004249, loss: 0.9308
2022-06-29 00:45:01 - train: epoch 0105, iter [00800, 05004], lr: 0.004238, loss: 1.0515
2022-06-29 00:45:35 - train: epoch 0105, iter [00900, 05004], lr: 0.004227, loss: 1.0121
2022-06-29 00:46:11 - train: epoch 0105, iter [01000, 05004], lr: 0.004217, loss: 1.0458
2022-06-29 00:46:45 - train: epoch 0105, iter [01100, 05004], lr: 0.004206, loss: 1.2307
2022-06-29 00:47:20 - train: epoch 0105, iter [01200, 05004], lr: 0.004196, loss: 1.0742
2022-06-29 00:47:55 - train: epoch 0105, iter [01300, 05004], lr: 0.004185, loss: 0.9342
2022-06-29 00:48:29 - train: epoch 0105, iter [01400, 05004], lr: 0.004175, loss: 1.1112
2022-06-29 00:49:04 - train: epoch 0105, iter [01500, 05004], lr: 0.004165, loss: 1.0860
2022-06-29 00:49:38 - train: epoch 0105, iter [01600, 05004], lr: 0.004154, loss: 0.8336
2022-06-29 00:50:13 - train: epoch 0105, iter [01700, 05004], lr: 0.004144, loss: 1.0637
2022-06-29 00:50:49 - train: epoch 0105, iter [01800, 05004], lr: 0.004133, loss: 1.0170
2022-06-29 00:51:23 - train: epoch 0105, iter [01900, 05004], lr: 0.004123, loss: 1.0690
2022-06-29 00:51:58 - train: epoch 0105, iter [02000, 05004], lr: 0.004112, loss: 0.9608
2022-06-29 00:52:32 - train: epoch 0105, iter [02100, 05004], lr: 0.004102, loss: 1.0580
2022-06-29 00:53:07 - train: epoch 0105, iter [02200, 05004], lr: 0.004092, loss: 1.2260
2022-06-29 00:53:42 - train: epoch 0105, iter [02300, 05004], lr: 0.004081, loss: 1.0577
2022-06-29 00:54:17 - train: epoch 0105, iter [02400, 05004], lr: 0.004071, loss: 1.0025
2022-06-29 00:54:52 - train: epoch 0105, iter [02500, 05004], lr: 0.004061, loss: 0.8505
2022-06-29 00:55:28 - train: epoch 0105, iter [02600, 05004], lr: 0.004050, loss: 0.9132
2022-06-29 00:56:02 - train: epoch 0105, iter [02700, 05004], lr: 0.004040, loss: 1.2467
2022-06-29 00:56:37 - train: epoch 0105, iter [02800, 05004], lr: 0.004030, loss: 1.1963
2022-06-29 00:57:11 - train: epoch 0105, iter [02900, 05004], lr: 0.004019, loss: 1.0886
2022-06-29 00:57:46 - train: epoch 0105, iter [03000, 05004], lr: 0.004009, loss: 0.9780
2022-06-29 00:58:21 - train: epoch 0105, iter [03100, 05004], lr: 0.003999, loss: 1.1059
2022-06-29 00:58:56 - train: epoch 0105, iter [03200, 05004], lr: 0.003989, loss: 1.2580
2022-06-29 00:59:32 - train: epoch 0105, iter [03300, 05004], lr: 0.003978, loss: 0.9665
2022-06-29 01:00:06 - train: epoch 0105, iter [03400, 05004], lr: 0.003968, loss: 1.0252
2022-06-29 01:00:41 - train: epoch 0105, iter [03500, 05004], lr: 0.003958, loss: 1.2051
2022-06-29 01:01:16 - train: epoch 0105, iter [03600, 05004], lr: 0.003948, loss: 1.1006
2022-06-29 01:01:51 - train: epoch 0105, iter [03700, 05004], lr: 0.003938, loss: 0.9939
2022-06-29 01:02:26 - train: epoch 0105, iter [03800, 05004], lr: 0.003927, loss: 1.1546
2022-06-29 01:03:01 - train: epoch 0105, iter [03900, 05004], lr: 0.003917, loss: 1.0897
2022-06-29 01:03:35 - train: epoch 0105, iter [04000, 05004], lr: 0.003907, loss: 0.9137
2022-06-29 01:04:11 - train: epoch 0105, iter [04100, 05004], lr: 0.003897, loss: 0.9464
2022-06-29 01:04:45 - train: epoch 0105, iter [04200, 05004], lr: 0.003887, loss: 0.9627
2022-06-29 01:05:20 - train: epoch 0105, iter [04300, 05004], lr: 0.003877, loss: 1.2061
2022-06-29 01:05:55 - train: epoch 0105, iter [04400, 05004], lr: 0.003867, loss: 1.0033
2022-06-29 01:06:30 - train: epoch 0105, iter [04500, 05004], lr: 0.003857, loss: 1.1866
2022-06-29 01:07:05 - train: epoch 0105, iter [04600, 05004], lr: 0.003847, loss: 1.1525
2022-06-29 01:07:39 - train: epoch 0105, iter [04700, 05004], lr: 0.003837, loss: 1.0629
2022-06-29 01:08:14 - train: epoch 0105, iter [04800, 05004], lr: 0.003826, loss: 1.0891
2022-06-29 01:08:49 - train: epoch 0105, iter [04900, 05004], lr: 0.003816, loss: 0.9933
2022-06-29 01:09:22 - train: epoch 0105, iter [05000, 05004], lr: 0.003806, loss: 0.8902
2022-06-29 01:09:23 - train: epoch 105, train_loss: 1.0444
2022-06-29 01:10:40 - eval: epoch: 105, acc1: 74.394%, acc5: 91.934%, test_loss: 1.0212, per_image_load_time: 2.525ms, per_image_inference_time: 0.422ms
2022-06-29 01:10:41 - until epoch: 105, best_acc1: 74.394%
2022-06-29 01:10:41 - epoch 106 lr: 0.003806
2022-06-29 01:11:20 - train: epoch 0106, iter [00100, 05004], lr: 0.003796, loss: 0.8555
2022-06-29 01:11:55 - train: epoch 0106, iter [00200, 05004], lr: 0.003786, loss: 0.9100
2022-06-29 01:12:29 - train: epoch 0106, iter [00300, 05004], lr: 0.003776, loss: 0.8260
2022-06-29 01:13:04 - train: epoch 0106, iter [00400, 05004], lr: 0.003766, loss: 1.0612
2022-06-29 01:13:38 - train: epoch 0106, iter [00500, 05004], lr: 0.003756, loss: 1.1922
2022-06-29 01:14:12 - train: epoch 0106, iter [00600, 05004], lr: 0.003746, loss: 0.9729
2022-06-29 01:14:46 - train: epoch 0106, iter [00700, 05004], lr: 0.003736, loss: 0.9931
2022-06-29 01:15:21 - train: epoch 0106, iter [00800, 05004], lr: 0.003726, loss: 0.8110
2022-06-29 01:15:55 - train: epoch 0106, iter [00900, 05004], lr: 0.003716, loss: 1.1980
2022-06-29 01:16:29 - train: epoch 0106, iter [01000, 05004], lr: 0.003707, loss: 1.1905
2022-06-29 01:17:05 - train: epoch 0106, iter [01100, 05004], lr: 0.003697, loss: 0.8300
2022-06-29 01:17:39 - train: epoch 0106, iter [01200, 05004], lr: 0.003687, loss: 0.8712
2022-06-29 01:18:14 - train: epoch 0106, iter [01300, 05004], lr: 0.003677, loss: 0.9903
2022-06-29 01:18:48 - train: epoch 0106, iter [01400, 05004], lr: 0.003667, loss: 1.0155
2022-06-29 01:19:23 - train: epoch 0106, iter [01500, 05004], lr: 0.003657, loss: 1.0302
2022-06-29 01:19:57 - train: epoch 0106, iter [01600, 05004], lr: 0.003647, loss: 1.0088
2022-06-29 01:20:31 - train: epoch 0106, iter [01700, 05004], lr: 0.003638, loss: 0.9867
2022-06-29 01:21:06 - train: epoch 0106, iter [01800, 05004], lr: 0.003628, loss: 0.8917
2022-06-29 01:21:41 - train: epoch 0106, iter [01900, 05004], lr: 0.003618, loss: 0.9092
2022-06-29 01:22:15 - train: epoch 0106, iter [02000, 05004], lr: 0.003608, loss: 1.0208
2022-06-29 01:22:50 - train: epoch 0106, iter [02100, 05004], lr: 0.003599, loss: 0.9916
2022-06-29 01:23:24 - train: epoch 0106, iter [02200, 05004], lr: 0.003589, loss: 1.0454
2022-06-29 01:23:59 - train: epoch 0106, iter [02300, 05004], lr: 0.003579, loss: 0.8960
2022-06-29 01:24:34 - train: epoch 0106, iter [02400, 05004], lr: 0.003569, loss: 1.0407
2022-06-29 01:25:08 - train: epoch 0106, iter [02500, 05004], lr: 0.003560, loss: 1.0426
2022-06-29 01:25:44 - train: epoch 0106, iter [02600, 05004], lr: 0.003550, loss: 0.7586
2022-06-29 01:26:19 - train: epoch 0106, iter [02700, 05004], lr: 0.003540, loss: 0.9977
2022-06-29 01:26:53 - train: epoch 0106, iter [02800, 05004], lr: 0.003531, loss: 0.9915
2022-06-29 01:27:28 - train: epoch 0106, iter [02900, 05004], lr: 0.003521, loss: 0.8620
2022-06-29 01:28:02 - train: epoch 0106, iter [03000, 05004], lr: 0.003511, loss: 1.1169
2022-06-29 01:28:37 - train: epoch 0106, iter [03100, 05004], lr: 0.003502, loss: 1.0071
2022-06-29 01:29:12 - train: epoch 0106, iter [03200, 05004], lr: 0.003492, loss: 0.8638
2022-06-29 01:29:47 - train: epoch 0106, iter [03300, 05004], lr: 0.003483, loss: 1.0751
2022-06-29 01:30:21 - train: epoch 0106, iter [03400, 05004], lr: 0.003473, loss: 0.9496
2022-06-29 01:30:57 - train: epoch 0106, iter [03500, 05004], lr: 0.003463, loss: 0.9847
2022-06-29 01:31:31 - train: epoch 0106, iter [03600, 05004], lr: 0.003454, loss: 0.9322
2022-06-29 01:32:06 - train: epoch 0106, iter [03700, 05004], lr: 0.003444, loss: 1.1728
2022-06-29 01:32:41 - train: epoch 0106, iter [03800, 05004], lr: 0.003435, loss: 1.2605
2022-06-29 01:33:16 - train: epoch 0106, iter [03900, 05004], lr: 0.003425, loss: 1.0889
2022-06-29 01:33:51 - train: epoch 0106, iter [04000, 05004], lr: 0.003416, loss: 0.9079
2022-06-29 01:34:25 - train: epoch 0106, iter [04100, 05004], lr: 0.003406, loss: 1.0718
2022-06-29 01:35:01 - train: epoch 0106, iter [04200, 05004], lr: 0.003397, loss: 0.8608
2022-06-29 01:35:35 - train: epoch 0106, iter [04300, 05004], lr: 0.003387, loss: 0.7633
2022-06-29 01:36:10 - train: epoch 0106, iter [04400, 05004], lr: 0.003378, loss: 1.0560
2022-06-29 01:36:45 - train: epoch 0106, iter [04500, 05004], lr: 0.003368, loss: 1.1868
2022-06-29 01:37:20 - train: epoch 0106, iter [04600, 05004], lr: 0.003359, loss: 1.2524
2022-06-29 01:37:54 - train: epoch 0106, iter [04700, 05004], lr: 0.003350, loss: 0.9466
2022-06-29 01:38:29 - train: epoch 0106, iter [04800, 05004], lr: 0.003340, loss: 1.0938
2022-06-29 01:39:04 - train: epoch 0106, iter [04900, 05004], lr: 0.003331, loss: 1.0120
2022-06-29 01:39:37 - train: epoch 0106, iter [05000, 05004], lr: 0.003321, loss: 1.0311
2022-06-29 01:39:38 - train: epoch 106, train_loss: 1.0239
2022-06-29 01:40:55 - eval: epoch: 106, acc1: 74.550%, acc5: 92.142%, test_loss: 1.0128, per_image_load_time: 2.521ms, per_image_inference_time: 0.414ms
2022-06-29 01:40:55 - until epoch: 106, best_acc1: 74.550%
2022-06-29 01:40:55 - epoch 107 lr: 0.003321
2022-06-29 01:41:36 - train: epoch 0107, iter [00100, 05004], lr: 0.003312, loss: 0.8576
2022-06-29 01:42:10 - train: epoch 0107, iter [00200, 05004], lr: 0.003302, loss: 1.0073
2022-06-29 01:42:44 - train: epoch 0107, iter [00300, 05004], lr: 0.003293, loss: 1.0711
2022-06-29 01:43:19 - train: epoch 0107, iter [00400, 05004], lr: 0.003284, loss: 0.9511
2022-06-29 01:43:54 - train: epoch 0107, iter [00500, 05004], lr: 0.003274, loss: 0.8424
2022-06-29 01:44:28 - train: epoch 0107, iter [00600, 05004], lr: 0.003265, loss: 0.8352
2022-06-29 01:45:02 - train: epoch 0107, iter [00700, 05004], lr: 0.003256, loss: 0.9029
2022-06-29 01:45:38 - train: epoch 0107, iter [00800, 05004], lr: 0.003246, loss: 0.8113
2022-06-29 01:46:12 - train: epoch 0107, iter [00900, 05004], lr: 0.003237, loss: 1.1623
2022-06-29 01:46:46 - train: epoch 0107, iter [01000, 05004], lr: 0.003228, loss: 0.8773
2022-06-29 01:47:20 - train: epoch 0107, iter [01100, 05004], lr: 0.003219, loss: 1.0692
2022-06-29 01:47:56 - train: epoch 0107, iter [01200, 05004], lr: 0.003209, loss: 0.9278
2022-06-29 01:48:30 - train: epoch 0107, iter [01300, 05004], lr: 0.003200, loss: 0.9072
2022-06-29 01:49:04 - train: epoch 0107, iter [01400, 05004], lr: 0.003191, loss: 1.0113
2022-06-29 01:49:39 - train: epoch 0107, iter [01500, 05004], lr: 0.003182, loss: 0.9526
2022-06-29 01:50:13 - train: epoch 0107, iter [01600, 05004], lr: 0.003173, loss: 0.9161
2022-06-29 01:50:48 - train: epoch 0107, iter [01700, 05004], lr: 0.003163, loss: 0.9047
2022-06-29 01:51:23 - train: epoch 0107, iter [01800, 05004], lr: 0.003154, loss: 1.1371
2022-06-29 01:51:58 - train: epoch 0107, iter [01900, 05004], lr: 0.003145, loss: 1.0804
2022-06-29 01:52:32 - train: epoch 0107, iter [02000, 05004], lr: 0.003136, loss: 0.8753
2022-06-29 01:53:07 - train: epoch 0107, iter [02100, 05004], lr: 0.003127, loss: 1.0476
2022-06-29 01:53:42 - train: epoch 0107, iter [02200, 05004], lr: 0.003118, loss: 0.9763
2022-06-29 01:54:16 - train: epoch 0107, iter [02300, 05004], lr: 0.003109, loss: 0.9678
2022-06-29 01:54:50 - train: epoch 0107, iter [02400, 05004], lr: 0.003100, loss: 1.0013
2022-06-29 01:55:25 - train: epoch 0107, iter [02500, 05004], lr: 0.003091, loss: 1.0889
2022-06-29 01:56:00 - train: epoch 0107, iter [02600, 05004], lr: 0.003082, loss: 1.1189
2022-06-29 01:56:35 - train: epoch 0107, iter [02700, 05004], lr: 0.003073, loss: 1.0445
2022-06-29 01:57:11 - train: epoch 0107, iter [02800, 05004], lr: 0.003064, loss: 1.0682
2022-06-29 01:57:45 - train: epoch 0107, iter [02900, 05004], lr: 0.003054, loss: 0.9838
2022-06-29 01:58:21 - train: epoch 0107, iter [03000, 05004], lr: 0.003046, loss: 1.0669
2022-06-29 01:58:55 - train: epoch 0107, iter [03100, 05004], lr: 0.003037, loss: 1.1186
2022-06-29 01:59:30 - train: epoch 0107, iter [03200, 05004], lr: 0.003028, loss: 0.9828
2022-06-29 02:00:05 - train: epoch 0107, iter [03300, 05004], lr: 0.003019, loss: 1.1105
2022-06-29 02:00:40 - train: epoch 0107, iter [03400, 05004], lr: 0.003010, loss: 0.9039
2022-06-29 02:01:14 - train: epoch 0107, iter [03500, 05004], lr: 0.003001, loss: 0.8942
2022-06-29 02:01:49 - train: epoch 0107, iter [03600, 05004], lr: 0.002992, loss: 0.9721
2022-06-29 02:02:24 - train: epoch 0107, iter [03700, 05004], lr: 0.002983, loss: 1.0110
2022-06-29 02:02:59 - train: epoch 0107, iter [03800, 05004], lr: 0.002974, loss: 0.9245
2022-06-29 02:03:33 - train: epoch 0107, iter [03900, 05004], lr: 0.002965, loss: 0.8452
2022-06-29 02:04:08 - train: epoch 0107, iter [04000, 05004], lr: 0.002956, loss: 1.0436
2022-06-29 02:04:43 - train: epoch 0107, iter [04100, 05004], lr: 0.002947, loss: 0.9316
2022-06-29 02:05:18 - train: epoch 0107, iter [04200, 05004], lr: 0.002939, loss: 1.0792
2022-06-29 02:05:52 - train: epoch 0107, iter [04300, 05004], lr: 0.002930, loss: 1.0092
2022-06-29 02:06:28 - train: epoch 0107, iter [04400, 05004], lr: 0.002921, loss: 0.8826
2022-06-29 02:07:02 - train: epoch 0107, iter [04500, 05004], lr: 0.002912, loss: 1.0030
2022-06-29 02:07:37 - train: epoch 0107, iter [04600, 05004], lr: 0.002903, loss: 0.8985
2022-06-29 02:08:12 - train: epoch 0107, iter [04700, 05004], lr: 0.002895, loss: 1.0082
2022-06-29 02:08:47 - train: epoch 0107, iter [04800, 05004], lr: 0.002886, loss: 1.0858
2022-06-29 02:09:22 - train: epoch 0107, iter [04900, 05004], lr: 0.002877, loss: 0.8740
2022-06-29 02:09:55 - train: epoch 0107, iter [05000, 05004], lr: 0.002868, loss: 0.8704
2022-06-29 02:09:56 - train: epoch 107, train_loss: 1.0060
2022-06-29 02:11:13 - eval: epoch: 107, acc1: 74.766%, acc5: 92.174%, test_loss: 1.0061, per_image_load_time: 2.544ms, per_image_inference_time: 0.404ms
2022-06-29 02:11:13 - until epoch: 107, best_acc1: 74.766%
2022-06-29 02:11:13 - epoch 108 lr: 0.002868
2022-06-29 02:11:53 - train: epoch 0108, iter [00100, 05004], lr: 0.002859, loss: 0.8718
2022-06-29 02:12:27 - train: epoch 0108, iter [00200, 05004], lr: 0.002850, loss: 0.9016
2022-06-29 02:13:01 - train: epoch 0108, iter [00300, 05004], lr: 0.002842, loss: 0.9588
2022-06-29 02:13:36 - train: epoch 0108, iter [00400, 05004], lr: 0.002833, loss: 1.1347
2022-06-29 02:14:10 - train: epoch 0108, iter [00500, 05004], lr: 0.002824, loss: 0.8606
2022-06-29 02:14:46 - train: epoch 0108, iter [00600, 05004], lr: 0.002816, loss: 1.0276
2022-06-29 02:15:20 - train: epoch 0108, iter [00700, 05004], lr: 0.002807, loss: 1.0129
2022-06-29 02:15:55 - train: epoch 0108, iter [00800, 05004], lr: 0.002798, loss: 0.9804
2022-06-29 02:16:29 - train: epoch 0108, iter [00900, 05004], lr: 0.002790, loss: 1.1546
2022-06-29 02:17:04 - train: epoch 0108, iter [01000, 05004], lr: 0.002781, loss: 0.9779
2022-06-29 02:17:38 - train: epoch 0108, iter [01100, 05004], lr: 0.002773, loss: 1.0386
2022-06-29 02:18:12 - train: epoch 0108, iter [01200, 05004], lr: 0.002764, loss: 1.0206
2022-06-29 02:18:48 - train: epoch 0108, iter [01300, 05004], lr: 0.002755, loss: 0.9669
2022-06-29 02:19:22 - train: epoch 0108, iter [01400, 05004], lr: 0.002747, loss: 1.0130
2022-06-29 02:19:57 - train: epoch 0108, iter [01500, 05004], lr: 0.002738, loss: 1.0817
2022-06-29 02:20:32 - train: epoch 0108, iter [01600, 05004], lr: 0.002730, loss: 0.9844
2022-06-29 02:21:07 - train: epoch 0108, iter [01700, 05004], lr: 0.002721, loss: 1.1083
2022-06-29 02:21:42 - train: epoch 0108, iter [01800, 05004], lr: 0.002713, loss: 1.0054
2022-06-29 02:22:17 - train: epoch 0108, iter [01900, 05004], lr: 0.002704, loss: 0.9645
2022-06-29 02:22:52 - train: epoch 0108, iter [02000, 05004], lr: 0.002696, loss: 1.0674
2022-06-29 02:23:26 - train: epoch 0108, iter [02100, 05004], lr: 0.002687, loss: 0.9930
2022-06-29 02:24:00 - train: epoch 0108, iter [02200, 05004], lr: 0.002679, loss: 1.0142
2022-06-29 02:24:35 - train: epoch 0108, iter [02300, 05004], lr: 0.002671, loss: 0.9493
2022-06-29 02:25:10 - train: epoch 0108, iter [02400, 05004], lr: 0.002662, loss: 1.1389
2022-06-29 02:25:45 - train: epoch 0108, iter [02500, 05004], lr: 0.002654, loss: 1.3192
2022-06-29 02:26:20 - train: epoch 0108, iter [02600, 05004], lr: 0.002645, loss: 1.0507
2022-06-29 02:26:54 - train: epoch 0108, iter [02700, 05004], lr: 0.002637, loss: 1.0902
2022-06-29 02:27:29 - train: epoch 0108, iter [02800, 05004], lr: 0.002628, loss: 0.9351
2022-06-29 02:28:03 - train: epoch 0108, iter [02900, 05004], lr: 0.002620, loss: 1.0089
2022-06-29 02:28:38 - train: epoch 0108, iter [03000, 05004], lr: 0.002612, loss: 0.9964
2022-06-29 02:29:14 - train: epoch 0108, iter [03100, 05004], lr: 0.002603, loss: 0.8844
2022-06-29 02:29:49 - train: epoch 0108, iter [03200, 05004], lr: 0.002595, loss: 0.8872
2022-06-29 02:30:23 - train: epoch 0108, iter [03300, 05004], lr: 0.002587, loss: 0.9154
2022-06-29 02:30:59 - train: epoch 0108, iter [03400, 05004], lr: 0.002579, loss: 1.1098
2022-06-29 02:31:34 - train: epoch 0108, iter [03500, 05004], lr: 0.002570, loss: 0.9514
2022-06-29 02:32:09 - train: epoch 0108, iter [03600, 05004], lr: 0.002562, loss: 0.9967
2022-06-29 02:32:43 - train: epoch 0108, iter [03700, 05004], lr: 0.002554, loss: 0.9150
2022-06-29 02:33:19 - train: epoch 0108, iter [03800, 05004], lr: 0.002545, loss: 0.9404
2022-06-29 02:33:53 - train: epoch 0108, iter [03900, 05004], lr: 0.002537, loss: 1.1144
2022-06-29 02:34:28 - train: epoch 0108, iter [04000, 05004], lr: 0.002529, loss: 0.9856
2022-06-29 02:35:03 - train: epoch 0108, iter [04100, 05004], lr: 0.002521, loss: 0.9912
2022-06-29 02:35:37 - train: epoch 0108, iter [04200, 05004], lr: 0.002513, loss: 0.9830
2022-06-29 02:36:12 - train: epoch 0108, iter [04300, 05004], lr: 0.002504, loss: 0.9530
2022-06-29 02:36:48 - train: epoch 0108, iter [04400, 05004], lr: 0.002496, loss: 0.9992
2022-06-29 02:37:23 - train: epoch 0108, iter [04500, 05004], lr: 0.002488, loss: 1.0192
2022-06-29 02:37:57 - train: epoch 0108, iter [04600, 05004], lr: 0.002480, loss: 1.0001
2022-06-29 02:38:32 - train: epoch 0108, iter [04700, 05004], lr: 0.002472, loss: 0.9710
2022-06-29 02:39:07 - train: epoch 0108, iter [04800, 05004], lr: 0.002464, loss: 0.9407
2022-06-29 02:39:42 - train: epoch 0108, iter [04900, 05004], lr: 0.002456, loss: 1.0227
2022-06-29 02:40:15 - train: epoch 0108, iter [05000, 05004], lr: 0.002447, loss: 1.0509
2022-06-29 02:40:16 - train: epoch 108, train_loss: 0.9883
2022-06-29 02:41:34 - eval: epoch: 108, acc1: 74.804%, acc5: 92.288%, test_loss: 1.0001, per_image_load_time: 2.587ms, per_image_inference_time: 0.413ms
2022-06-29 02:41:34 - until epoch: 108, best_acc1: 74.804%
2022-06-29 02:41:34 - epoch 109 lr: 0.002447
2022-06-29 02:42:14 - train: epoch 0109, iter [00100, 05004], lr: 0.002439, loss: 0.9915
2022-06-29 02:42:49 - train: epoch 0109, iter [00200, 05004], lr: 0.002431, loss: 1.1524
2022-06-29 02:43:23 - train: epoch 0109, iter [00300, 05004], lr: 0.002423, loss: 0.8940
2022-06-29 02:43:58 - train: epoch 0109, iter [00400, 05004], lr: 0.002415, loss: 0.9870
2022-06-29 02:44:32 - train: epoch 0109, iter [00500, 05004], lr: 0.002407, loss: 1.0607
2022-06-29 02:45:07 - train: epoch 0109, iter [00600, 05004], lr: 0.002399, loss: 0.9651
2022-06-29 02:45:41 - train: epoch 0109, iter [00700, 05004], lr: 0.002391, loss: 0.8778
2022-06-29 02:46:16 - train: epoch 0109, iter [00800, 05004], lr: 0.002383, loss: 1.0592
2022-06-29 02:46:51 - train: epoch 0109, iter [00900, 05004], lr: 0.002375, loss: 0.9640
2022-06-29 02:47:26 - train: epoch 0109, iter [01000, 05004], lr: 0.002367, loss: 0.7657
2022-06-29 02:48:00 - train: epoch 0109, iter [01100, 05004], lr: 0.002359, loss: 0.8916
2022-06-29 02:48:34 - train: epoch 0109, iter [01200, 05004], lr: 0.002351, loss: 0.9333
2022-06-29 02:49:09 - train: epoch 0109, iter [01300, 05004], lr: 0.002343, loss: 0.7986
2022-06-29 02:49:44 - train: epoch 0109, iter [01400, 05004], lr: 0.002335, loss: 0.9334
2022-06-29 02:50:18 - train: epoch 0109, iter [01500, 05004], lr: 0.002327, loss: 1.0856
2022-06-29 02:50:54 - train: epoch 0109, iter [01600, 05004], lr: 0.002320, loss: 0.9164
2022-06-29 02:51:28 - train: epoch 0109, iter [01700, 05004], lr: 0.002312, loss: 1.2024
2022-06-29 02:52:03 - train: epoch 0109, iter [01800, 05004], lr: 0.002304, loss: 0.9754
2022-06-29 02:52:38 - train: epoch 0109, iter [01900, 05004], lr: 0.002296, loss: 0.9525
2022-06-29 02:53:12 - train: epoch 0109, iter [02000, 05004], lr: 0.002288, loss: 1.0738
2022-06-29 02:53:47 - train: epoch 0109, iter [02100, 05004], lr: 0.002280, loss: 0.9574
2022-06-29 02:54:21 - train: epoch 0109, iter [02200, 05004], lr: 0.002272, loss: 0.9359
2022-06-29 02:54:57 - train: epoch 0109, iter [02300, 05004], lr: 0.002265, loss: 0.8833
2022-06-29 02:55:31 - train: epoch 0109, iter [02400, 05004], lr: 0.002257, loss: 0.9910
2022-06-29 02:56:06 - train: epoch 0109, iter [02500, 05004], lr: 0.002249, loss: 0.9446
2022-06-29 02:56:41 - train: epoch 0109, iter [02600, 05004], lr: 0.002241, loss: 1.1044
2022-06-29 02:57:15 - train: epoch 0109, iter [02700, 05004], lr: 0.002234, loss: 0.9206
2022-06-29 02:57:51 - train: epoch 0109, iter [02800, 05004], lr: 0.002226, loss: 1.0194
2022-06-29 02:58:25 - train: epoch 0109, iter [02900, 05004], lr: 0.002218, loss: 0.8758
2022-06-29 02:58:59 - train: epoch 0109, iter [03000, 05004], lr: 0.002211, loss: 0.7955
2022-06-29 02:59:34 - train: epoch 0109, iter [03100, 05004], lr: 0.002203, loss: 1.0216
2022-06-29 03:00:08 - train: epoch 0109, iter [03200, 05004], lr: 0.002195, loss: 0.8077
2022-06-29 03:00:44 - train: epoch 0109, iter [03300, 05004], lr: 0.002188, loss: 0.9995
2022-06-29 03:01:19 - train: epoch 0109, iter [03400, 05004], lr: 0.002180, loss: 0.9658
2022-06-29 03:01:53 - train: epoch 0109, iter [03500, 05004], lr: 0.002172, loss: 0.9021
2022-06-29 03:02:28 - train: epoch 0109, iter [03600, 05004], lr: 0.002165, loss: 0.9305
2022-06-29 03:03:03 - train: epoch 0109, iter [03700, 05004], lr: 0.002157, loss: 0.7313
2022-06-29 03:03:37 - train: epoch 0109, iter [03800, 05004], lr: 0.002149, loss: 0.9823
2022-06-29 03:04:11 - train: epoch 0109, iter [03900, 05004], lr: 0.002142, loss: 1.0258
2022-06-29 03:04:46 - train: epoch 0109, iter [04000, 05004], lr: 0.002134, loss: 0.9502
2022-06-29 03:05:21 - train: epoch 0109, iter [04100, 05004], lr: 0.002127, loss: 0.9350
2022-06-29 03:05:56 - train: epoch 0109, iter [04200, 05004], lr: 0.002119, loss: 1.0781
2022-06-29 03:06:31 - train: epoch 0109, iter [04300, 05004], lr: 0.002112, loss: 0.8964
2022-06-29 03:07:06 - train: epoch 0109, iter [04400, 05004], lr: 0.002104, loss: 0.9449
2022-06-29 03:07:41 - train: epoch 0109, iter [04500, 05004], lr: 0.002097, loss: 0.9306
2022-06-29 03:08:16 - train: epoch 0109, iter [04600, 05004], lr: 0.002089, loss: 0.9755
2022-06-29 03:08:51 - train: epoch 0109, iter [04700, 05004], lr: 0.002082, loss: 0.8817
2022-06-29 03:09:26 - train: epoch 0109, iter [04800, 05004], lr: 0.002074, loss: 0.9776
2022-06-29 03:10:01 - train: epoch 0109, iter [04900, 05004], lr: 0.002067, loss: 1.0135
2022-06-29 03:10:34 - train: epoch 0109, iter [05000, 05004], lr: 0.002059, loss: 0.7941
2022-06-29 03:10:35 - train: epoch 109, train_loss: 0.9698
2022-06-29 03:11:52 - eval: epoch: 109, acc1: 75.124%, acc5: 92.352%, test_loss: 0.9914, per_image_load_time: 2.513ms, per_image_inference_time: 0.409ms
2022-06-29 03:11:53 - until epoch: 109, best_acc1: 75.124%
2022-06-29 03:11:53 - epoch 110 lr: 0.002059
2022-06-29 03:12:33 - train: epoch 0110, iter [00100, 05004], lr: 0.002052, loss: 0.7687
2022-06-29 03:13:07 - train: epoch 0110, iter [00200, 05004], lr: 0.002044, loss: 0.8013
2022-06-29 03:13:41 - train: epoch 0110, iter [00300, 05004], lr: 0.002037, loss: 1.1553
2022-06-29 03:14:16 - train: epoch 0110, iter [00400, 05004], lr: 0.002029, loss: 0.9311
2022-06-29 03:14:51 - train: epoch 0110, iter [00500, 05004], lr: 0.002022, loss: 0.9192
2022-06-29 03:15:25 - train: epoch 0110, iter [00600, 05004], lr: 0.002015, loss: 1.0107
2022-06-29 03:16:00 - train: epoch 0110, iter [00700, 05004], lr: 0.002007, loss: 0.9455
2022-06-29 03:16:34 - train: epoch 0110, iter [00800, 05004], lr: 0.002000, loss: 1.0602
2022-06-29 03:17:09 - train: epoch 0110, iter [00900, 05004], lr: 0.001993, loss: 0.9120
2022-06-29 03:17:43 - train: epoch 0110, iter [01000, 05004], lr: 0.001985, loss: 0.8570
2022-06-29 03:18:18 - train: epoch 0110, iter [01100, 05004], lr: 0.001978, loss: 1.2320
2022-06-29 03:18:53 - train: epoch 0110, iter [01200, 05004], lr: 0.001971, loss: 0.8997
2022-06-29 03:19:28 - train: epoch 0110, iter [01300, 05004], lr: 0.001964, loss: 0.9241
2022-06-29 03:20:03 - train: epoch 0110, iter [01400, 05004], lr: 0.001956, loss: 0.7934
2022-06-29 03:20:36 - train: epoch 0110, iter [01500, 05004], lr: 0.001949, loss: 1.0472
2022-06-29 03:21:12 - train: epoch 0110, iter [01600, 05004], lr: 0.001942, loss: 1.0786
2022-06-29 03:21:47 - train: epoch 0110, iter [01700, 05004], lr: 0.001935, loss: 0.8677
2022-06-29 03:22:22 - train: epoch 0110, iter [01800, 05004], lr: 0.001927, loss: 0.9876
2022-06-29 03:22:56 - train: epoch 0110, iter [01900, 05004], lr: 0.001920, loss: 1.0496
2022-06-29 03:23:32 - train: epoch 0110, iter [02000, 05004], lr: 0.001913, loss: 0.9559
2022-06-29 03:24:06 - train: epoch 0110, iter [02100, 05004], lr: 0.001906, loss: 1.0852
2022-06-29 03:24:41 - train: epoch 0110, iter [02200, 05004], lr: 0.001899, loss: 0.9945
2022-06-29 03:25:16 - train: epoch 0110, iter [02300, 05004], lr: 0.001892, loss: 0.9023
2022-06-29 03:25:50 - train: epoch 0110, iter [02400, 05004], lr: 0.001884, loss: 0.9017
2022-06-29 03:26:25 - train: epoch 0110, iter [02500, 05004], lr: 0.001877, loss: 0.9082
2022-06-29 03:27:00 - train: epoch 0110, iter [02600, 05004], lr: 0.001870, loss: 1.0461
2022-06-29 03:27:35 - train: epoch 0110, iter [02700, 05004], lr: 0.001863, loss: 0.9069
2022-06-29 03:28:09 - train: epoch 0110, iter [02800, 05004], lr: 0.001856, loss: 1.0168
2022-06-29 03:28:44 - train: epoch 0110, iter [02900, 05004], lr: 0.001849, loss: 1.0573
2022-06-29 03:29:19 - train: epoch 0110, iter [03000, 05004], lr: 0.001842, loss: 0.8509
2022-06-29 03:29:54 - train: epoch 0110, iter [03100, 05004], lr: 0.001835, loss: 0.9718
2022-06-29 03:30:29 - train: epoch 0110, iter [03200, 05004], lr: 0.001828, loss: 1.1407
2022-06-29 03:31:03 - train: epoch 0110, iter [03300, 05004], lr: 0.001821, loss: 0.9110
2022-06-29 03:31:38 - train: epoch 0110, iter [03400, 05004], lr: 0.001814, loss: 0.8038
2022-06-29 03:32:12 - train: epoch 0110, iter [03500, 05004], lr: 0.001807, loss: 1.2655
2022-06-29 03:32:46 - train: epoch 0110, iter [03600, 05004], lr: 0.001800, loss: 1.1162
2022-06-29 03:33:22 - train: epoch 0110, iter [03700, 05004], lr: 0.001793, loss: 1.0405
2022-06-29 03:33:56 - train: epoch 0110, iter [03800, 05004], lr: 0.001786, loss: 0.9057
2022-06-29 03:34:30 - train: epoch 0110, iter [03900, 05004], lr: 0.001779, loss: 1.1867
2022-06-29 03:35:05 - train: epoch 0110, iter [04000, 05004], lr: 0.001772, loss: 0.9391
2022-06-29 03:35:40 - train: epoch 0110, iter [04100, 05004], lr: 0.001765, loss: 0.9414
2022-06-29 03:36:15 - train: epoch 0110, iter [04200, 05004], lr: 0.001759, loss: 1.0005
2022-06-29 03:36:50 - train: epoch 0110, iter [04300, 05004], lr: 0.001752, loss: 0.8762
2022-06-29 03:37:24 - train: epoch 0110, iter [04400, 05004], lr: 0.001745, loss: 1.0382
2022-06-29 03:37:59 - train: epoch 0110, iter [04500, 05004], lr: 0.001738, loss: 0.9132
2022-06-29 03:38:34 - train: epoch 0110, iter [04600, 05004], lr: 0.001731, loss: 0.7829
2022-06-29 03:39:09 - train: epoch 0110, iter [04700, 05004], lr: 0.001724, loss: 1.0692
2022-06-29 03:39:43 - train: epoch 0110, iter [04800, 05004], lr: 0.001718, loss: 1.1833
2022-06-29 03:40:19 - train: epoch 0110, iter [04900, 05004], lr: 0.001711, loss: 0.9076
2022-06-29 03:40:52 - train: epoch 0110, iter [05000, 05004], lr: 0.001704, loss: 0.9442
2022-06-29 03:40:53 - train: epoch 110, train_loss: 0.9537
2022-06-29 03:42:10 - eval: epoch: 110, acc1: 75.352%, acc5: 92.450%, test_loss: 0.9837, per_image_load_time: 2.331ms, per_image_inference_time: 0.434ms
2022-06-29 03:42:10 - until epoch: 110, best_acc1: 75.352%
2022-06-29 03:42:10 - epoch 111 lr: 0.001704
2022-06-29 03:42:49 - train: epoch 0111, iter [00100, 05004], lr: 0.001697, loss: 0.7797
2022-06-29 03:43:25 - train: epoch 0111, iter [00200, 05004], lr: 0.001690, loss: 0.9140
2022-06-29 03:43:59 - train: epoch 0111, iter [00300, 05004], lr: 0.001683, loss: 1.0081
2022-06-29 03:44:33 - train: epoch 0111, iter [00400, 05004], lr: 0.001677, loss: 0.9156
2022-06-29 03:45:07 - train: epoch 0111, iter [00500, 05004], lr: 0.001670, loss: 0.8416
2022-06-29 03:45:42 - train: epoch 0111, iter [00600, 05004], lr: 0.001663, loss: 1.1372
2022-06-29 03:46:16 - train: epoch 0111, iter [00700, 05004], lr: 0.001657, loss: 1.0478
2022-06-29 03:46:50 - train: epoch 0111, iter [00800, 05004], lr: 0.001650, loss: 0.9226
2022-06-29 03:47:25 - train: epoch 0111, iter [00900, 05004], lr: 0.001643, loss: 0.9033
2022-06-29 03:48:00 - train: epoch 0111, iter [01000, 05004], lr: 0.001637, loss: 0.8283
2022-06-29 03:48:34 - train: epoch 0111, iter [01100, 05004], lr: 0.001630, loss: 0.9437
2022-06-29 03:49:09 - train: epoch 0111, iter [01200, 05004], lr: 0.001623, loss: 0.8709
2022-06-29 03:49:44 - train: epoch 0111, iter [01300, 05004], lr: 0.001617, loss: 0.9288
2022-06-29 03:50:18 - train: epoch 0111, iter [01400, 05004], lr: 0.001610, loss: 0.7838
2022-06-29 03:50:54 - train: epoch 0111, iter [01500, 05004], lr: 0.001604, loss: 0.8929
2022-06-29 03:51:28 - train: epoch 0111, iter [01600, 05004], lr: 0.001597, loss: 0.7835
2022-06-29 03:52:03 - train: epoch 0111, iter [01700, 05004], lr: 0.001591, loss: 0.9012
2022-06-29 03:52:38 - train: epoch 0111, iter [01800, 05004], lr: 0.001584, loss: 0.8409
2022-06-29 03:53:12 - train: epoch 0111, iter [01900, 05004], lr: 0.001577, loss: 0.8590
2022-06-29 03:53:47 - train: epoch 0111, iter [02000, 05004], lr: 0.001571, loss: 0.9694
2022-06-29 03:54:22 - train: epoch 0111, iter [02100, 05004], lr: 0.001564, loss: 1.2118
2022-06-29 03:54:57 - train: epoch 0111, iter [02200, 05004], lr: 0.001558, loss: 0.9961
2022-06-29 03:55:31 - train: epoch 0111, iter [02300, 05004], lr: 0.001551, loss: 0.8481
2022-06-29 03:56:06 - train: epoch 0111, iter [02400, 05004], lr: 0.001545, loss: 0.9784
2022-06-29 03:56:41 - train: epoch 0111, iter [02500, 05004], lr: 0.001539, loss: 0.9653
2022-06-29 03:57:15 - train: epoch 0111, iter [02600, 05004], lr: 0.001532, loss: 0.7855
2022-06-29 03:57:50 - train: epoch 0111, iter [02700, 05004], lr: 0.001526, loss: 1.0616
2022-06-29 03:58:24 - train: epoch 0111, iter [02800, 05004], lr: 0.001519, loss: 0.8938
2022-06-29 03:58:59 - train: epoch 0111, iter [02900, 05004], lr: 0.001513, loss: 0.8120
2022-06-29 03:59:34 - train: epoch 0111, iter [03000, 05004], lr: 0.001507, loss: 1.0879
2022-06-29 04:00:09 - train: epoch 0111, iter [03100, 05004], lr: 0.001500, loss: 0.9631
2022-06-29 04:00:43 - train: epoch 0111, iter [03200, 05004], lr: 0.001494, loss: 1.0293
2022-06-29 04:01:17 - train: epoch 0111, iter [03300, 05004], lr: 0.001487, loss: 1.1284
2022-06-29 04:01:52 - train: epoch 0111, iter [03400, 05004], lr: 0.001481, loss: 1.1681
2022-06-29 04:02:27 - train: epoch 0111, iter [03500, 05004], lr: 0.001475, loss: 0.9710
2022-06-29 04:03:02 - train: epoch 0111, iter [03600, 05004], lr: 0.001469, loss: 0.8557
2022-06-29 04:03:36 - train: epoch 0111, iter [03700, 05004], lr: 0.001462, loss: 0.8551
2022-06-29 04:04:11 - train: epoch 0111, iter [03800, 05004], lr: 0.001456, loss: 0.7652
2022-06-29 04:04:45 - train: epoch 0111, iter [03900, 05004], lr: 0.001450, loss: 0.7946
2022-06-29 04:05:20 - train: epoch 0111, iter [04000, 05004], lr: 0.001443, loss: 0.9329
2022-06-29 04:05:55 - train: epoch 0111, iter [04100, 05004], lr: 0.001437, loss: 1.1393
2022-06-29 04:06:31 - train: epoch 0111, iter [04200, 05004], lr: 0.001431, loss: 0.9484
2022-06-29 04:07:05 - train: epoch 0111, iter [04300, 05004], lr: 0.001425, loss: 0.9158
2022-06-29 04:07:40 - train: epoch 0111, iter [04400, 05004], lr: 0.001419, loss: 0.8567
2022-06-29 04:08:14 - train: epoch 0111, iter [04500, 05004], lr: 0.001412, loss: 0.9418
2022-06-29 04:08:49 - train: epoch 0111, iter [04600, 05004], lr: 0.001406, loss: 0.9384
2022-06-29 04:09:24 - train: epoch 0111, iter [04700, 05004], lr: 0.001400, loss: 0.9594
2022-06-29 04:09:58 - train: epoch 0111, iter [04800, 05004], lr: 0.001394, loss: 0.9526
2022-06-29 04:10:34 - train: epoch 0111, iter [04900, 05004], lr: 0.001388, loss: 0.8814
2022-06-29 04:11:07 - train: epoch 0111, iter [05000, 05004], lr: 0.001382, loss: 1.0602
2022-06-29 04:11:08 - train: epoch 111, train_loss: 0.9388
2022-06-29 04:12:26 - eval: epoch: 111, acc1: 75.436%, acc5: 92.610%, test_loss: 0.9756, per_image_load_time: 2.359ms, per_image_inference_time: 0.423ms
2022-06-29 04:12:26 - until epoch: 111, best_acc1: 75.436%
2022-06-29 04:12:26 - epoch 112 lr: 0.001381
2022-06-29 04:13:06 - train: epoch 0112, iter [00100, 05004], lr: 0.001375, loss: 1.0352
2022-06-29 04:13:40 - train: epoch 0112, iter [00200, 05004], lr: 0.001369, loss: 0.9119
2022-06-29 04:14:14 - train: epoch 0112, iter [00300, 05004], lr: 0.001363, loss: 0.8083
2022-06-29 04:14:49 - train: epoch 0112, iter [00400, 05004], lr: 0.001357, loss: 0.7672
2022-06-29 04:15:23 - train: epoch 0112, iter [00500, 05004], lr: 0.001351, loss: 1.0862
2022-06-29 04:15:58 - train: epoch 0112, iter [00600, 05004], lr: 0.001345, loss: 0.9796
2022-06-29 04:16:32 - train: epoch 0112, iter [00700, 05004], lr: 0.001339, loss: 0.9813
2022-06-29 04:17:07 - train: epoch 0112, iter [00800, 05004], lr: 0.001333, loss: 0.8377
2022-06-29 04:17:41 - train: epoch 0112, iter [00900, 05004], lr: 0.001327, loss: 0.7507
2022-06-29 04:18:15 - train: epoch 0112, iter [01000, 05004], lr: 0.001321, loss: 0.9784
2022-06-29 04:18:51 - train: epoch 0112, iter [01100, 05004], lr: 0.001315, loss: 0.8811
2022-06-29 04:19:26 - train: epoch 0112, iter [01200, 05004], lr: 0.001309, loss: 1.0400
2022-06-29 04:20:00 - train: epoch 0112, iter [01300, 05004], lr: 0.001303, loss: 0.9120
2022-06-29 04:20:35 - train: epoch 0112, iter [01400, 05004], lr: 0.001297, loss: 0.8991
2022-06-29 04:21:09 - train: epoch 0112, iter [01500, 05004], lr: 0.001291, loss: 1.0088
2022-06-29 04:21:44 - train: epoch 0112, iter [01600, 05004], lr: 0.001286, loss: 0.9560
2022-06-29 04:22:19 - train: epoch 0112, iter [01700, 05004], lr: 0.001280, loss: 0.8389
2022-06-29 04:22:52 - train: epoch 0112, iter [01800, 05004], lr: 0.001274, loss: 0.7501
2022-06-29 04:23:28 - train: epoch 0112, iter [01900, 05004], lr: 0.001268, loss: 0.7841
2022-06-29 04:24:02 - train: epoch 0112, iter [02000, 05004], lr: 0.001262, loss: 0.8563
2022-06-29 04:24:36 - train: epoch 0112, iter [02100, 05004], lr: 0.001256, loss: 0.8495
2022-06-29 04:25:10 - train: epoch 0112, iter [02200, 05004], lr: 0.001250, loss: 0.8009
2022-06-29 04:25:45 - train: epoch 0112, iter [02300, 05004], lr: 0.001245, loss: 0.7831
2022-06-29 04:26:18 - train: epoch 0112, iter [02400, 05004], lr: 0.001239, loss: 0.7231
2022-06-29 04:26:52 - train: epoch 0112, iter [02500, 05004], lr: 0.001233, loss: 0.9534
2022-06-29 04:27:26 - train: epoch 0112, iter [02600, 05004], lr: 0.001227, loss: 0.8127
2022-06-29 04:28:01 - train: epoch 0112, iter [02700, 05004], lr: 0.001221, loss: 1.0326
2022-06-29 04:28:34 - train: epoch 0112, iter [02800, 05004], lr: 0.001216, loss: 0.9959
2022-06-29 04:29:09 - train: epoch 0112, iter [02900, 05004], lr: 0.001210, loss: 1.1145
2022-06-29 04:29:44 - train: epoch 0112, iter [03000, 05004], lr: 0.001204, loss: 1.0208
2022-06-29 04:30:18 - train: epoch 0112, iter [03100, 05004], lr: 0.001199, loss: 0.9449
2022-06-29 04:30:52 - train: epoch 0112, iter [03200, 05004], lr: 0.001193, loss: 0.8649
2022-06-29 04:31:26 - train: epoch 0112, iter [03300, 05004], lr: 0.001187, loss: 0.9529
2022-06-29 04:32:01 - train: epoch 0112, iter [03400, 05004], lr: 0.001182, loss: 0.9546
2022-06-29 04:32:34 - train: epoch 0112, iter [03500, 05004], lr: 0.001176, loss: 0.8146
2022-06-29 04:33:09 - train: epoch 0112, iter [03600, 05004], lr: 0.001170, loss: 0.9029
2022-06-29 04:33:43 - train: epoch 0112, iter [03700, 05004], lr: 0.001165, loss: 0.9358
2022-06-29 04:34:17 - train: epoch 0112, iter [03800, 05004], lr: 0.001159, loss: 0.9141
2022-06-29 04:34:52 - train: epoch 0112, iter [03900, 05004], lr: 0.001153, loss: 0.8347
2022-06-29 04:35:26 - train: epoch 0112, iter [04000, 05004], lr: 0.001148, loss: 0.9420
2022-06-29 04:36:00 - train: epoch 0112, iter [04100, 05004], lr: 0.001142, loss: 0.8064
2022-06-29 04:36:35 - train: epoch 0112, iter [04200, 05004], lr: 0.001137, loss: 0.8031
2022-06-29 04:37:09 - train: epoch 0112, iter [04300, 05004], lr: 0.001131, loss: 0.8457
2022-06-29 04:37:44 - train: epoch 0112, iter [04400, 05004], lr: 0.001126, loss: 1.1902
2022-06-29 04:38:18 - train: epoch 0112, iter [04500, 05004], lr: 0.001120, loss: 0.8005
2022-06-29 04:38:53 - train: epoch 0112, iter [04600, 05004], lr: 0.001115, loss: 1.0628
2022-06-29 04:39:27 - train: epoch 0112, iter [04700, 05004], lr: 0.001109, loss: 0.8047
2022-06-29 04:40:01 - train: epoch 0112, iter [04800, 05004], lr: 0.001104, loss: 0.8817
2022-06-29 04:40:36 - train: epoch 0112, iter [04900, 05004], lr: 0.001098, loss: 0.8983
2022-06-29 04:41:09 - train: epoch 0112, iter [05000, 05004], lr: 0.001093, loss: 0.8106
2022-06-29 04:41:10 - train: epoch 112, train_loss: 0.9257
2022-06-29 04:42:25 - eval: epoch: 112, acc1: 75.540%, acc5: 92.624%, test_loss: 0.9691, per_image_load_time: 2.503ms, per_image_inference_time: 0.398ms
2022-06-29 04:42:26 - until epoch: 112, best_acc1: 75.540%
2022-06-29 04:42:26 - epoch 113 lr: 0.001093
2022-06-29 04:43:06 - train: epoch 0113, iter [00100, 05004], lr: 0.001087, loss: 1.0866
2022-06-29 04:43:39 - train: epoch 0113, iter [00200, 05004], lr: 0.001082, loss: 0.8955
2022-06-29 04:44:13 - train: epoch 0113, iter [00300, 05004], lr: 0.001076, loss: 0.7808
2022-06-29 04:44:47 - train: epoch 0113, iter [00400, 05004], lr: 0.001071, loss: 0.9765
2022-06-29 04:45:21 - train: epoch 0113, iter [00500, 05004], lr: 0.001066, loss: 1.0790
2022-06-29 04:45:55 - train: epoch 0113, iter [00600, 05004], lr: 0.001060, loss: 0.8761
2022-06-29 04:46:29 - train: epoch 0113, iter [00700, 05004], lr: 0.001055, loss: 0.7858
2022-06-29 04:47:02 - train: epoch 0113, iter [00800, 05004], lr: 0.001050, loss: 0.9079
2022-06-29 04:47:37 - train: epoch 0113, iter [00900, 05004], lr: 0.001044, loss: 0.6997
2022-06-29 04:48:10 - train: epoch 0113, iter [01000, 05004], lr: 0.001039, loss: 0.9723
2022-06-29 04:48:45 - train: epoch 0113, iter [01100, 05004], lr: 0.001034, loss: 0.8897
2022-06-29 04:49:20 - train: epoch 0113, iter [01200, 05004], lr: 0.001028, loss: 1.0537
2022-06-29 04:49:53 - train: epoch 0113, iter [01300, 05004], lr: 0.001023, loss: 0.7544
2022-06-29 04:50:27 - train: epoch 0113, iter [01400, 05004], lr: 0.001018, loss: 1.1001
2022-06-29 04:51:01 - train: epoch 0113, iter [01500, 05004], lr: 0.001013, loss: 1.0252
2022-06-29 04:51:36 - train: epoch 0113, iter [01600, 05004], lr: 0.001007, loss: 0.8558
2022-06-29 04:52:09 - train: epoch 0113, iter [01700, 05004], lr: 0.001002, loss: 0.8106
2022-06-29 04:52:44 - train: epoch 0113, iter [01800, 05004], lr: 0.000997, loss: 0.8109
2022-06-29 04:53:20 - train: epoch 0113, iter [01900, 05004], lr: 0.000992, loss: 0.9056
2022-06-29 04:53:52 - train: epoch 0113, iter [02000, 05004], lr: 0.000987, loss: 0.9197
2022-06-29 04:54:27 - train: epoch 0113, iter [02100, 05004], lr: 0.000981, loss: 1.0667
2022-06-29 04:55:01 - train: epoch 0113, iter [02200, 05004], lr: 0.000976, loss: 0.9441
2022-06-29 04:55:36 - train: epoch 0113, iter [02300, 05004], lr: 0.000971, loss: 0.9981
2022-06-29 04:56:10 - train: epoch 0113, iter [02400, 05004], lr: 0.000966, loss: 1.0233
2022-06-29 04:56:45 - train: epoch 0113, iter [02500, 05004], lr: 0.000961, loss: 0.9964
2022-06-29 04:57:19 - train: epoch 0113, iter [02600, 05004], lr: 0.000956, loss: 0.7829
2022-06-29 04:57:53 - train: epoch 0113, iter [02700, 05004], lr: 0.000951, loss: 0.9006
2022-06-29 04:58:27 - train: epoch 0113, iter [02800, 05004], lr: 0.000946, loss: 0.9518
2022-06-29 04:59:03 - train: epoch 0113, iter [02900, 05004], lr: 0.000941, loss: 0.9737
2022-06-29 04:59:37 - train: epoch 0113, iter [03000, 05004], lr: 0.000935, loss: 0.8668
2022-06-29 05:00:11 - train: epoch 0113, iter [03100, 05004], lr: 0.000930, loss: 0.7671
2022-06-29 05:00:46 - train: epoch 0113, iter [03200, 05004], lr: 0.000925, loss: 0.7630
2022-06-29 05:01:20 - train: epoch 0113, iter [03300, 05004], lr: 0.000920, loss: 1.0331
2022-06-29 05:01:54 - train: epoch 0113, iter [03400, 05004], lr: 0.000915, loss: 0.8242
2022-06-29 05:02:29 - train: epoch 0113, iter [03500, 05004], lr: 0.000910, loss: 0.9113
2022-06-29 05:03:03 - train: epoch 0113, iter [03600, 05004], lr: 0.000906, loss: 1.1016
2022-06-29 05:03:37 - train: epoch 0113, iter [03700, 05004], lr: 0.000901, loss: 0.7293
2022-06-29 05:04:12 - train: epoch 0113, iter [03800, 05004], lr: 0.000896, loss: 0.9385
2022-06-29 05:04:47 - train: epoch 0113, iter [03900, 05004], lr: 0.000891, loss: 1.0441
2022-06-29 05:05:21 - train: epoch 0113, iter [04000, 05004], lr: 0.000886, loss: 0.8649
2022-06-29 05:05:55 - train: epoch 0113, iter [04100, 05004], lr: 0.000881, loss: 0.9526
2022-06-29 05:06:30 - train: epoch 0113, iter [04200, 05004], lr: 0.000876, loss: 1.0352
2022-06-29 05:07:04 - train: epoch 0113, iter [04300, 05004], lr: 0.000871, loss: 0.8201
2022-06-29 05:07:38 - train: epoch 0113, iter [04400, 05004], lr: 0.000866, loss: 0.7785
2022-06-29 05:08:13 - train: epoch 0113, iter [04500, 05004], lr: 0.000861, loss: 0.8549
2022-06-29 05:08:47 - train: epoch 0113, iter [04600, 05004], lr: 0.000857, loss: 1.1400
2022-06-29 05:09:22 - train: epoch 0113, iter [04700, 05004], lr: 0.000852, loss: 0.8299
2022-06-29 05:09:56 - train: epoch 0113, iter [04800, 05004], lr: 0.000847, loss: 1.1927
2022-06-29 05:10:31 - train: epoch 0113, iter [04900, 05004], lr: 0.000842, loss: 0.8386
2022-06-29 05:11:05 - train: epoch 0113, iter [05000, 05004], lr: 0.000837, loss: 0.9644
2022-06-29 05:11:06 - train: epoch 113, train_loss: 0.9122
2022-06-29 05:12:22 - eval: epoch: 113, acc1: 75.626%, acc5: 92.654%, test_loss: 0.9653, per_image_load_time: 2.518ms, per_image_inference_time: 0.414ms
2022-06-29 05:12:22 - until epoch: 113, best_acc1: 75.626%
2022-06-29 05:12:22 - epoch 114 lr: 0.000837
2022-06-29 05:13:01 - train: epoch 0114, iter [00100, 05004], lr: 0.000832, loss: 1.0801
2022-06-29 05:13:36 - train: epoch 0114, iter [00200, 05004], lr: 0.000828, loss: 0.8733
2022-06-29 05:14:09 - train: epoch 0114, iter [00300, 05004], lr: 0.000823, loss: 0.9377
2022-06-29 05:14:43 - train: epoch 0114, iter [00400, 05004], lr: 0.000818, loss: 1.0340
2022-06-29 05:15:16 - train: epoch 0114, iter [00500, 05004], lr: 0.000814, loss: 0.9229
2022-06-29 05:15:50 - train: epoch 0114, iter [00600, 05004], lr: 0.000809, loss: 0.9494
2022-06-29 05:16:24 - train: epoch 0114, iter [00700, 05004], lr: 0.000804, loss: 0.8623
2022-06-29 05:16:58 - train: epoch 0114, iter [00800, 05004], lr: 0.000800, loss: 0.9089
2022-06-29 05:17:32 - train: epoch 0114, iter [00900, 05004], lr: 0.000795, loss: 0.8361
2022-06-29 05:18:06 - train: epoch 0114, iter [01000, 05004], lr: 0.000790, loss: 0.9593
2022-06-29 05:18:40 - train: epoch 0114, iter [01100, 05004], lr: 0.000786, loss: 0.8443
2022-06-29 05:19:14 - train: epoch 0114, iter [01200, 05004], lr: 0.000781, loss: 0.9203
2022-06-29 05:19:48 - train: epoch 0114, iter [01300, 05004], lr: 0.000776, loss: 0.6911
2022-06-29 05:20:22 - train: epoch 0114, iter [01400, 05004], lr: 0.000772, loss: 0.9006
2022-06-29 05:20:56 - train: epoch 0114, iter [01500, 05004], lr: 0.000767, loss: 0.8690
2022-06-29 05:21:31 - train: epoch 0114, iter [01600, 05004], lr: 0.000763, loss: 0.7827
2022-06-29 05:22:04 - train: epoch 0114, iter [01700, 05004], lr: 0.000758, loss: 0.7476
2022-06-29 05:22:39 - train: epoch 0114, iter [01800, 05004], lr: 0.000754, loss: 0.9521
2022-06-29 05:23:13 - train: epoch 0114, iter [01900, 05004], lr: 0.000749, loss: 0.6817
2022-06-29 05:23:47 - train: epoch 0114, iter [02000, 05004], lr: 0.000745, loss: 1.0363
2022-06-29 05:24:21 - train: epoch 0114, iter [02100, 05004], lr: 0.000740, loss: 0.9011
2022-06-29 05:24:56 - train: epoch 0114, iter [02200, 05004], lr: 0.000736, loss: 0.9734
2022-06-29 05:25:30 - train: epoch 0114, iter [02300, 05004], lr: 0.000731, loss: 0.9678
2022-06-29 05:26:04 - train: epoch 0114, iter [02400, 05004], lr: 0.000727, loss: 0.9381
2022-06-29 05:26:39 - train: epoch 0114, iter [02500, 05004], lr: 0.000722, loss: 0.8011
2022-06-29 05:27:13 - train: epoch 0114, iter [02600, 05004], lr: 0.000718, loss: 0.7689
2022-06-29 05:27:48 - train: epoch 0114, iter [02700, 05004], lr: 0.000713, loss: 0.8705
2022-06-29 05:28:21 - train: epoch 0114, iter [02800, 05004], lr: 0.000709, loss: 1.0216
2022-06-29 05:28:56 - train: epoch 0114, iter [02900, 05004], lr: 0.000705, loss: 0.8059
2022-06-29 05:29:29 - train: epoch 0114, iter [03000, 05004], lr: 0.000700, loss: 0.8072
2022-06-29 05:30:04 - train: epoch 0114, iter [03100, 05004], lr: 0.000696, loss: 1.0732
2022-06-29 05:30:38 - train: epoch 0114, iter [03200, 05004], lr: 0.000692, loss: 0.9397
2022-06-29 05:31:13 - train: epoch 0114, iter [03300, 05004], lr: 0.000687, loss: 1.1324
2022-06-29 05:31:47 - train: epoch 0114, iter [03400, 05004], lr: 0.000683, loss: 0.7499
2022-06-29 05:32:21 - train: epoch 0114, iter [03500, 05004], lr: 0.000679, loss: 0.7979
2022-06-29 05:32:56 - train: epoch 0114, iter [03600, 05004], lr: 0.000674, loss: 0.8963
2022-06-29 05:33:30 - train: epoch 0114, iter [03700, 05004], lr: 0.000670, loss: 0.8598
2022-06-29 05:34:04 - train: epoch 0114, iter [03800, 05004], lr: 0.000666, loss: 0.8987
2022-06-29 05:34:38 - train: epoch 0114, iter [03900, 05004], lr: 0.000662, loss: 0.8666
2022-06-29 05:35:13 - train: epoch 0114, iter [04000, 05004], lr: 0.000657, loss: 0.7786
2022-06-29 05:35:47 - train: epoch 0114, iter [04100, 05004], lr: 0.000653, loss: 0.8986
2022-06-29 05:36:22 - train: epoch 0114, iter [04200, 05004], lr: 0.000649, loss: 0.9420
2022-06-29 05:36:55 - train: epoch 0114, iter [04300, 05004], lr: 0.000645, loss: 0.7874
2022-06-29 05:37:30 - train: epoch 0114, iter [04400, 05004], lr: 0.000641, loss: 0.9295
2022-06-29 05:38:04 - train: epoch 0114, iter [04500, 05004], lr: 0.000636, loss: 0.9894
2022-06-29 05:38:38 - train: epoch 0114, iter [04600, 05004], lr: 0.000632, loss: 0.9211
2022-06-29 05:39:12 - train: epoch 0114, iter [04700, 05004], lr: 0.000628, loss: 0.9288
2022-06-29 05:39:47 - train: epoch 0114, iter [04800, 05004], lr: 0.000624, loss: 1.1059
2022-06-29 05:40:21 - train: epoch 0114, iter [04900, 05004], lr: 0.000620, loss: 0.9089
2022-06-29 05:40:54 - train: epoch 0114, iter [05000, 05004], lr: 0.000616, loss: 0.9728
2022-06-29 05:40:55 - train: epoch 114, train_loss: 0.9001
2022-06-29 05:42:10 - eval: epoch: 114, acc1: 75.806%, acc5: 92.682%, test_loss: 0.9594, per_image_load_time: 2.520ms, per_image_inference_time: 0.411ms
2022-06-29 05:42:11 - until epoch: 114, best_acc1: 75.806%
2022-06-29 05:42:11 - epoch 115 lr: 0.000616
2022-06-29 05:42:50 - train: epoch 0115, iter [00100, 05004], lr: 0.000611, loss: 0.8400
2022-06-29 05:43:24 - train: epoch 0115, iter [00200, 05004], lr: 0.000607, loss: 0.9686
2022-06-29 05:43:58 - train: epoch 0115, iter [00300, 05004], lr: 0.000603, loss: 1.0445
2022-06-29 05:44:31 - train: epoch 0115, iter [00400, 05004], lr: 0.000599, loss: 0.7371
2022-06-29 05:45:06 - train: epoch 0115, iter [00500, 05004], lr: 0.000595, loss: 0.8954
2022-06-29 05:45:39 - train: epoch 0115, iter [00600, 05004], lr: 0.000591, loss: 0.8989
2022-06-29 05:46:13 - train: epoch 0115, iter [00700, 05004], lr: 0.000587, loss: 0.8024
2022-06-29 05:46:48 - train: epoch 0115, iter [00800, 05004], lr: 0.000583, loss: 0.7842
2022-06-29 05:47:22 - train: epoch 0115, iter [00900, 05004], lr: 0.000579, loss: 0.9333
2022-06-29 05:47:56 - train: epoch 0115, iter [01000, 05004], lr: 0.000575, loss: 0.8126
2022-06-29 05:48:30 - train: epoch 0115, iter [01100, 05004], lr: 0.000571, loss: 0.8354
2022-06-29 05:49:04 - train: epoch 0115, iter [01200, 05004], lr: 0.000567, loss: 0.9434
2022-06-29 05:49:38 - train: epoch 0115, iter [01300, 05004], lr: 0.000564, loss: 0.9204
2022-06-29 05:50:12 - train: epoch 0115, iter [01400, 05004], lr: 0.000560, loss: 0.8720
2022-06-29 05:50:47 - train: epoch 0115, iter [01500, 05004], lr: 0.000556, loss: 1.1094
2022-06-29 05:51:19 - train: epoch 0115, iter [01600, 05004], lr: 0.000552, loss: 0.7907
2022-06-29 05:51:53 - train: epoch 0115, iter [01700, 05004], lr: 0.000548, loss: 0.7748
2022-06-29 05:52:27 - train: epoch 0115, iter [01800, 05004], lr: 0.000544, loss: 0.8026
2022-06-29 05:53:00 - train: epoch 0115, iter [01900, 05004], lr: 0.000540, loss: 0.8406
2022-06-29 05:53:35 - train: epoch 0115, iter [02000, 05004], lr: 0.000536, loss: 0.9931
2022-06-29 05:54:10 - train: epoch 0115, iter [02100, 05004], lr: 0.000533, loss: 0.8533
2022-06-29 05:54:44 - train: epoch 0115, iter [02200, 05004], lr: 0.000529, loss: 1.0227
2022-06-29 05:55:18 - train: epoch 0115, iter [02300, 05004], lr: 0.000525, loss: 1.0174
2022-06-29 05:55:52 - train: epoch 0115, iter [02400, 05004], lr: 0.000521, loss: 0.8451
2022-06-29 05:56:26 - train: epoch 0115, iter [02500, 05004], lr: 0.000518, loss: 0.8077
2022-06-29 05:57:00 - train: epoch 0115, iter [02600, 05004], lr: 0.000514, loss: 0.8556
2022-06-29 05:57:34 - train: epoch 0115, iter [02700, 05004], lr: 0.000510, loss: 0.8231
2022-06-29 05:58:08 - train: epoch 0115, iter [02800, 05004], lr: 0.000506, loss: 0.9152
2022-06-29 05:58:42 - train: epoch 0115, iter [02900, 05004], lr: 0.000503, loss: 0.7655
2022-06-29 05:59:18 - train: epoch 0115, iter [03000, 05004], lr: 0.000499, loss: 0.7528
2022-06-29 05:59:54 - train: epoch 0115, iter [03100, 05004], lr: 0.000495, loss: 0.9100
2022-06-29 06:00:27 - train: epoch 0115, iter [03200, 05004], lr: 0.000492, loss: 0.9446
2022-06-29 06:01:01 - train: epoch 0115, iter [03300, 05004], lr: 0.000488, loss: 0.8391
2022-06-29 06:01:34 - train: epoch 0115, iter [03400, 05004], lr: 0.000484, loss: 0.8967
2022-06-29 06:02:08 - train: epoch 0115, iter [03500, 05004], lr: 0.000481, loss: 0.8628
2022-06-29 06:02:43 - train: epoch 0115, iter [03600, 05004], lr: 0.000477, loss: 0.7893
2022-06-29 06:03:18 - train: epoch 0115, iter [03700, 05004], lr: 0.000473, loss: 0.8288
2022-06-29 06:03:53 - train: epoch 0115, iter [03800, 05004], lr: 0.000470, loss: 0.9549
2022-06-29 06:04:26 - train: epoch 0115, iter [03900, 05004], lr: 0.000466, loss: 0.8465
2022-06-29 06:05:01 - train: epoch 0115, iter [04000, 05004], lr: 0.000463, loss: 0.9762
2022-06-29 06:05:36 - train: epoch 0115, iter [04100, 05004], lr: 0.000459, loss: 0.8676
2022-06-29 06:06:09 - train: epoch 0115, iter [04200, 05004], lr: 0.000456, loss: 0.7894
2022-06-29 06:06:43 - train: epoch 0115, iter [04300, 05004], lr: 0.000452, loss: 0.9293
2022-06-29 06:07:17 - train: epoch 0115, iter [04400, 05004], lr: 0.000449, loss: 0.9861
2022-06-29 06:07:51 - train: epoch 0115, iter [04500, 05004], lr: 0.000445, loss: 0.8090
2022-06-29 06:08:25 - train: epoch 0115, iter [04600, 05004], lr: 0.000442, loss: 0.9337
2022-06-29 06:08:59 - train: epoch 0115, iter [04700, 05004], lr: 0.000438, loss: 0.8569
2022-06-29 06:09:32 - train: epoch 0115, iter [04800, 05004], lr: 0.000435, loss: 0.8175
2022-06-29 06:10:08 - train: epoch 0115, iter [04900, 05004], lr: 0.000431, loss: 0.9399
2022-06-29 06:10:40 - train: epoch 0115, iter [05000, 05004], lr: 0.000428, loss: 0.9314
2022-06-29 06:10:42 - train: epoch 115, train_loss: 0.8915
2022-06-29 06:11:57 - eval: epoch: 115, acc1: 75.852%, acc5: 92.822%, test_loss: 0.9552, per_image_load_time: 2.498ms, per_image_inference_time: 0.394ms
2022-06-29 06:11:57 - until epoch: 115, best_acc1: 75.852%
2022-06-29 06:11:57 - epoch 116 lr: 0.000428
2022-06-29 06:12:37 - train: epoch 0116, iter [00100, 05004], lr: 0.000424, loss: 0.8098
2022-06-29 06:13:10 - train: epoch 0116, iter [00200, 05004], lr: 0.000421, loss: 0.9373
2022-06-29 06:13:45 - train: epoch 0116, iter [00300, 05004], lr: 0.000418, loss: 0.6791
2022-06-29 06:14:18 - train: epoch 0116, iter [00400, 05004], lr: 0.000414, loss: 0.8580
2022-06-29 06:14:52 - train: epoch 0116, iter [00500, 05004], lr: 0.000411, loss: 0.9876
2022-06-29 06:15:27 - train: epoch 0116, iter [00600, 05004], lr: 0.000408, loss: 0.8926
2022-06-29 06:16:01 - train: epoch 0116, iter [00700, 05004], lr: 0.000404, loss: 0.9672
2022-06-29 06:16:34 - train: epoch 0116, iter [00800, 05004], lr: 0.000401, loss: 0.9648
2022-06-29 06:17:09 - train: epoch 0116, iter [00900, 05004], lr: 0.000398, loss: 1.0349
2022-06-29 06:17:43 - train: epoch 0116, iter [01000, 05004], lr: 0.000394, loss: 0.8935
2022-06-29 06:18:17 - train: epoch 0116, iter [01100, 05004], lr: 0.000391, loss: 0.7535
2022-06-29 06:18:51 - train: epoch 0116, iter [01200, 05004], lr: 0.000388, loss: 0.8431
2022-06-29 06:19:25 - train: epoch 0116, iter [01300, 05004], lr: 0.000385, loss: 1.0634
2022-06-29 06:19:59 - train: epoch 0116, iter [01400, 05004], lr: 0.000381, loss: 0.9646
2022-06-29 06:20:34 - train: epoch 0116, iter [01500, 05004], lr: 0.000378, loss: 0.7695
2022-06-29 06:21:08 - train: epoch 0116, iter [01600, 05004], lr: 0.000375, loss: 0.9093
2022-06-29 06:21:43 - train: epoch 0116, iter [01700, 05004], lr: 0.000372, loss: 0.8762
2022-06-29 06:22:17 - train: epoch 0116, iter [01800, 05004], lr: 0.000368, loss: 1.0558
2022-06-29 06:22:51 - train: epoch 0116, iter [01900, 05004], lr: 0.000365, loss: 0.7806
2022-06-29 06:23:26 - train: epoch 0116, iter [02000, 05004], lr: 0.000362, loss: 0.8309
2022-06-29 06:24:00 - train: epoch 0116, iter [02100, 05004], lr: 0.000359, loss: 0.9882
2022-06-29 06:24:34 - train: epoch 0116, iter [02200, 05004], lr: 0.000356, loss: 0.9101
2022-06-29 06:25:09 - train: epoch 0116, iter [02300, 05004], lr: 0.000353, loss: 1.0377
2022-06-29 06:25:43 - train: epoch 0116, iter [02400, 05004], lr: 0.000350, loss: 0.9904
2022-06-29 06:26:17 - train: epoch 0116, iter [02500, 05004], lr: 0.000347, loss: 0.7132
2022-06-29 06:26:51 - train: epoch 0116, iter [02600, 05004], lr: 0.000344, loss: 0.8509
2022-06-29 06:27:26 - train: epoch 0116, iter [02700, 05004], lr: 0.000341, loss: 0.9685
2022-06-29 06:28:00 - train: epoch 0116, iter [02800, 05004], lr: 0.000337, loss: 0.9801
2022-06-29 06:28:35 - train: epoch 0116, iter [02900, 05004], lr: 0.000334, loss: 1.0581
2022-06-29 06:29:09 - train: epoch 0116, iter [03000, 05004], lr: 0.000331, loss: 0.9140
2022-06-29 06:29:43 - train: epoch 0116, iter [03100, 05004], lr: 0.000328, loss: 0.9316
2022-06-29 06:30:18 - train: epoch 0116, iter [03200, 05004], lr: 0.000325, loss: 0.9617
2022-06-29 06:30:54 - train: epoch 0116, iter [03300, 05004], lr: 0.000322, loss: 1.1351
2022-06-29 06:31:26 - train: epoch 0116, iter [03400, 05004], lr: 0.000320, loss: 0.9569
2022-06-29 06:32:01 - train: epoch 0116, iter [03500, 05004], lr: 0.000317, loss: 0.9543
2022-06-29 06:32:35 - train: epoch 0116, iter [03600, 05004], lr: 0.000314, loss: 1.0400
2022-06-29 06:33:10 - train: epoch 0116, iter [03700, 05004], lr: 0.000311, loss: 0.8201
2022-06-29 06:33:44 - train: epoch 0116, iter [03800, 05004], lr: 0.000308, loss: 0.7505
2022-06-29 06:34:18 - train: epoch 0116, iter [03900, 05004], lr: 0.000305, loss: 0.9313
2022-06-29 06:34:51 - train: epoch 0116, iter [04000, 05004], lr: 0.000302, loss: 0.8188
2022-06-29 06:35:26 - train: epoch 0116, iter [04100, 05004], lr: 0.000299, loss: 1.0417
2022-06-29 06:36:00 - train: epoch 0116, iter [04200, 05004], lr: 0.000296, loss: 0.8280
2022-06-29 06:36:35 - train: epoch 0116, iter [04300, 05004], lr: 0.000293, loss: 0.9036
2022-06-29 06:37:08 - train: epoch 0116, iter [04400, 05004], lr: 0.000291, loss: 0.8096
2022-06-29 06:37:44 - train: epoch 0116, iter [04500, 05004], lr: 0.000288, loss: 0.8757
2022-06-29 06:38:18 - train: epoch 0116, iter [04600, 05004], lr: 0.000285, loss: 1.0702
2022-06-29 06:38:52 - train: epoch 0116, iter [04700, 05004], lr: 0.000282, loss: 0.8892
2022-06-29 06:39:26 - train: epoch 0116, iter [04800, 05004], lr: 0.000280, loss: 0.8987
2022-06-29 06:40:01 - train: epoch 0116, iter [04900, 05004], lr: 0.000277, loss: 0.7118
2022-06-29 06:40:34 - train: epoch 0116, iter [05000, 05004], lr: 0.000274, loss: 0.8092
2022-06-29 06:40:35 - train: epoch 116, train_loss: 0.8839
2022-06-29 06:41:52 - eval: epoch: 116, acc1: 75.890%, acc5: 92.832%, test_loss: 0.9545, per_image_load_time: 2.547ms, per_image_inference_time: 0.404ms
2022-06-29 06:41:53 - until epoch: 116, best_acc1: 75.890%
2022-06-29 06:41:53 - epoch 117 lr: 0.000274
2022-06-29 06:42:33 - train: epoch 0117, iter [00100, 05004], lr: 0.000271, loss: 1.1291
2022-06-29 06:43:07 - train: epoch 0117, iter [00200, 05004], lr: 0.000268, loss: 0.8754
2022-06-29 06:43:41 - train: epoch 0117, iter [00300, 05004], lr: 0.000266, loss: 1.0154
2022-06-29 06:44:16 - train: epoch 0117, iter [00400, 05004], lr: 0.000263, loss: 0.7903
2022-06-29 06:44:50 - train: epoch 0117, iter [00500, 05004], lr: 0.000260, loss: 0.7805
2022-06-29 06:45:25 - train: epoch 0117, iter [00600, 05004], lr: 0.000258, loss: 0.9515
2022-06-29 06:45:59 - train: epoch 0117, iter [00700, 05004], lr: 0.000255, loss: 0.8922
2022-06-29 06:46:34 - train: epoch 0117, iter [00800, 05004], lr: 0.000252, loss: 0.7944
2022-06-29 06:47:08 - train: epoch 0117, iter [00900, 05004], lr: 0.000250, loss: 0.7338
2022-06-29 06:47:43 - train: epoch 0117, iter [01000, 05004], lr: 0.000247, loss: 0.8033
2022-06-29 06:48:18 - train: epoch 0117, iter [01100, 05004], lr: 0.000245, loss: 0.8652
2022-06-29 06:48:53 - train: epoch 0117, iter [01200, 05004], lr: 0.000242, loss: 0.9186
2022-06-29 06:49:27 - train: epoch 0117, iter [01300, 05004], lr: 0.000240, loss: 1.0374
2022-06-29 06:50:03 - train: epoch 0117, iter [01400, 05004], lr: 0.000237, loss: 1.0121
2022-06-29 06:50:37 - train: epoch 0117, iter [01500, 05004], lr: 0.000234, loss: 0.8435
2022-06-29 06:51:12 - train: epoch 0117, iter [01600, 05004], lr: 0.000232, loss: 0.7272
2022-06-29 06:51:47 - train: epoch 0117, iter [01700, 05004], lr: 0.000229, loss: 0.8566
2022-06-29 06:52:21 - train: epoch 0117, iter [01800, 05004], lr: 0.000227, loss: 0.8688
2022-06-29 06:52:56 - train: epoch 0117, iter [01900, 05004], lr: 0.000224, loss: 0.7783
2022-06-29 06:53:30 - train: epoch 0117, iter [02000, 05004], lr: 0.000222, loss: 0.8868
2022-06-29 06:54:05 - train: epoch 0117, iter [02100, 05004], lr: 0.000219, loss: 0.8541
2022-06-29 06:54:40 - train: epoch 0117, iter [02200, 05004], lr: 0.000217, loss: 0.8044
2022-06-29 06:55:15 - train: epoch 0117, iter [02300, 05004], lr: 0.000215, loss: 0.8284
2022-06-29 06:55:50 - train: epoch 0117, iter [02400, 05004], lr: 0.000212, loss: 0.7506
2022-06-29 06:56:25 - train: epoch 0117, iter [02500, 05004], lr: 0.000210, loss: 0.9889
2022-06-29 06:56:59 - train: epoch 0117, iter [02600, 05004], lr: 0.000207, loss: 0.8519
2022-06-29 06:57:35 - train: epoch 0117, iter [02700, 05004], lr: 0.000205, loss: 0.7229
2022-06-29 06:58:10 - train: epoch 0117, iter [02800, 05004], lr: 0.000203, loss: 0.7744
2022-06-29 06:58:44 - train: epoch 0117, iter [02900, 05004], lr: 0.000200, loss: 0.8483
2022-06-29 06:59:19 - train: epoch 0117, iter [03000, 05004], lr: 0.000198, loss: 0.6701
2022-06-29 06:59:54 - train: epoch 0117, iter [03100, 05004], lr: 0.000196, loss: 0.8662
2022-06-29 07:00:28 - train: epoch 0117, iter [03200, 05004], lr: 0.000193, loss: 0.8799
2022-06-29 07:01:03 - train: epoch 0117, iter [03300, 05004], lr: 0.000191, loss: 0.8270
2022-06-29 07:01:39 - train: epoch 0117, iter [03400, 05004], lr: 0.000189, loss: 0.8151
2022-06-29 07:02:13 - train: epoch 0117, iter [03500, 05004], lr: 0.000187, loss: 0.9876
2022-06-29 07:02:48 - train: epoch 0117, iter [03600, 05004], lr: 0.000184, loss: 1.0805
2022-06-29 07:03:23 - train: epoch 0117, iter [03700, 05004], lr: 0.000182, loss: 0.8370
2022-06-29 07:03:58 - train: epoch 0117, iter [03800, 05004], lr: 0.000180, loss: 0.8306
2022-06-29 07:04:32 - train: epoch 0117, iter [03900, 05004], lr: 0.000178, loss: 0.7249
2022-06-29 07:05:08 - train: epoch 0117, iter [04000, 05004], lr: 0.000175, loss: 0.8347
2022-06-29 07:05:42 - train: epoch 0117, iter [04100, 05004], lr: 0.000173, loss: 0.8739
2022-06-29 07:06:16 - train: epoch 0117, iter [04200, 05004], lr: 0.000171, loss: 0.8958
2022-06-29 07:06:52 - train: epoch 0117, iter [04300, 05004], lr: 0.000169, loss: 0.8112
2022-06-29 07:07:27 - train: epoch 0117, iter [04400, 05004], lr: 0.000167, loss: 0.7646
2022-06-29 07:08:01 - train: epoch 0117, iter [04500, 05004], lr: 0.000165, loss: 0.8568
2022-06-29 07:08:36 - train: epoch 0117, iter [04600, 05004], lr: 0.000163, loss: 0.8316
2022-06-29 07:09:11 - train: epoch 0117, iter [04700, 05004], lr: 0.000160, loss: 0.9174
2022-06-29 07:09:47 - train: epoch 0117, iter [04800, 05004], lr: 0.000158, loss: 0.9065
2022-06-29 07:10:21 - train: epoch 0117, iter [04900, 05004], lr: 0.000156, loss: 1.0357
2022-06-29 07:10:54 - train: epoch 0117, iter [05000, 05004], lr: 0.000154, loss: 0.9812
2022-06-29 07:10:56 - train: epoch 117, train_loss: 0.8775
2022-06-29 07:12:13 - eval: epoch: 117, acc1: 75.870%, acc5: 92.780%, test_loss: 0.9523, per_image_load_time: 2.473ms, per_image_inference_time: 0.444ms
2022-06-29 07:12:13 - until epoch: 117, best_acc1: 75.890%
2022-06-29 07:12:13 - epoch 118 lr: 0.000154
2022-06-29 07:12:54 - train: epoch 0118, iter [00100, 05004], lr: 0.000152, loss: 0.8059
2022-06-29 07:13:28 - train: epoch 0118, iter [00200, 05004], lr: 0.000150, loss: 0.7480
2022-06-29 07:14:02 - train: epoch 0118, iter [00300, 05004], lr: 0.000148, loss: 0.9412
2022-06-29 07:14:37 - train: epoch 0118, iter [00400, 05004], lr: 0.000146, loss: 0.9932
2022-06-29 07:15:12 - train: epoch 0118, iter [00500, 05004], lr: 0.000144, loss: 0.7772
2022-06-29 07:15:46 - train: epoch 0118, iter [00600, 05004], lr: 0.000142, loss: 1.0057
2022-06-29 07:16:19 - train: epoch 0118, iter [00700, 05004], lr: 0.000140, loss: 0.9096
2022-06-29 07:16:54 - train: epoch 0118, iter [00800, 05004], lr: 0.000138, loss: 0.8957
2022-06-29 07:17:30 - train: epoch 0118, iter [00900, 05004], lr: 0.000136, loss: 0.8854
2022-06-29 07:18:04 - train: epoch 0118, iter [01000, 05004], lr: 0.000134, loss: 1.1352
2022-06-29 07:18:39 - train: epoch 0118, iter [01100, 05004], lr: 0.000132, loss: 0.9612
2022-06-29 07:19:13 - train: epoch 0118, iter [01200, 05004], lr: 0.000130, loss: 1.0324
2022-06-29 07:19:47 - train: epoch 0118, iter [01300, 05004], lr: 0.000129, loss: 0.9914
2022-06-29 07:20:22 - train: epoch 0118, iter [01400, 05004], lr: 0.000127, loss: 0.7929
2022-06-29 07:20:57 - train: epoch 0118, iter [01500, 05004], lr: 0.000125, loss: 0.8089
2022-06-29 07:21:32 - train: epoch 0118, iter [01600, 05004], lr: 0.000123, loss: 0.7910
2022-06-29 07:22:07 - train: epoch 0118, iter [01700, 05004], lr: 0.000121, loss: 0.9328
2022-06-29 07:22:41 - train: epoch 0118, iter [01800, 05004], lr: 0.000119, loss: 0.7870
2022-06-29 07:23:16 - train: epoch 0118, iter [01900, 05004], lr: 0.000118, loss: 1.0483
2022-06-29 07:23:50 - train: epoch 0118, iter [02000, 05004], lr: 0.000116, loss: 0.7754
2022-06-29 07:24:25 - train: epoch 0118, iter [02100, 05004], lr: 0.000114, loss: 0.8788
2022-06-29 07:25:00 - train: epoch 0118, iter [02200, 05004], lr: 0.000112, loss: 0.8286
2022-06-29 07:25:35 - train: epoch 0118, iter [02300, 05004], lr: 0.000111, loss: 0.8858
2022-06-29 07:26:10 - train: epoch 0118, iter [02400, 05004], lr: 0.000109, loss: 1.0574
2022-06-29 07:26:45 - train: epoch 0118, iter [02500, 05004], lr: 0.000107, loss: 0.9931
2022-06-29 07:27:20 - train: epoch 0118, iter [02600, 05004], lr: 0.000105, loss: 0.8473
2022-06-29 07:27:55 - train: epoch 0118, iter [02700, 05004], lr: 0.000104, loss: 0.9263
2022-06-29 07:28:30 - train: epoch 0118, iter [02800, 05004], lr: 0.000102, loss: 0.7979
2022-06-29 07:29:04 - train: epoch 0118, iter [02900, 05004], lr: 0.000100, loss: 0.8657
2022-06-29 07:29:40 - train: epoch 0118, iter [03000, 05004], lr: 0.000099, loss: 0.8302
2022-06-29 07:30:14 - train: epoch 0118, iter [03100, 05004], lr: 0.000097, loss: 1.0262
2022-06-29 07:30:49 - train: epoch 0118, iter [03200, 05004], lr: 0.000095, loss: 0.8451
2022-06-29 07:31:24 - train: epoch 0118, iter [03300, 05004], lr: 0.000094, loss: 0.9042
2022-06-29 07:31:59 - train: epoch 0118, iter [03400, 05004], lr: 0.000092, loss: 0.8352
2022-06-29 07:32:34 - train: epoch 0118, iter [03500, 05004], lr: 0.000091, loss: 0.8561
2022-06-29 07:33:09 - train: epoch 0118, iter [03600, 05004], lr: 0.000089, loss: 0.9540
2022-06-29 07:33:44 - train: epoch 0118, iter [03700, 05004], lr: 0.000088, loss: 0.9939
2022-06-29 07:34:18 - train: epoch 0118, iter [03800, 05004], lr: 0.000086, loss: 0.8412
2022-06-29 07:34:53 - train: epoch 0118, iter [03900, 05004], lr: 0.000084, loss: 0.9331
2022-06-29 07:35:27 - train: epoch 0118, iter [04000, 05004], lr: 0.000083, loss: 0.8873
2022-06-29 07:36:02 - train: epoch 0118, iter [04100, 05004], lr: 0.000081, loss: 0.9782
2022-06-29 07:36:37 - train: epoch 0118, iter [04200, 05004], lr: 0.000080, loss: 0.9263
2022-06-29 07:37:12 - train: epoch 0118, iter [04300, 05004], lr: 0.000079, loss: 0.6750
2022-06-29 07:37:47 - train: epoch 0118, iter [04400, 05004], lr: 0.000077, loss: 0.8878
2022-06-29 07:38:22 - train: epoch 0118, iter [04500, 05004], lr: 0.000076, loss: 0.8223
2022-06-29 07:38:56 - train: epoch 0118, iter [04600, 05004], lr: 0.000074, loss: 0.9699
2022-06-29 07:39:30 - train: epoch 0118, iter [04700, 05004], lr: 0.000073, loss: 0.7607
2022-06-29 07:40:05 - train: epoch 0118, iter [04800, 05004], lr: 0.000071, loss: 0.7645
2022-06-29 07:40:40 - train: epoch 0118, iter [04900, 05004], lr: 0.000070, loss: 0.7937
2022-06-29 07:41:14 - train: epoch 0118, iter [05000, 05004], lr: 0.000069, loss: 0.8618
2022-06-29 07:41:15 - train: epoch 118, train_loss: 0.8738
2022-06-29 07:42:32 - eval: epoch: 118, acc1: 76.008%, acc5: 92.832%, test_loss: 0.9514, per_image_load_time: 2.493ms, per_image_inference_time: 0.409ms
2022-06-29 07:42:33 - until epoch: 118, best_acc1: 76.008%
2022-06-29 07:42:33 - epoch 119 lr: 0.000069
2022-06-29 07:43:12 - train: epoch 0119, iter [00100, 05004], lr: 0.000067, loss: 0.9873
2022-06-29 07:43:47 - train: epoch 0119, iter [00200, 05004], lr: 0.000066, loss: 0.9076
2022-06-29 07:44:22 - train: epoch 0119, iter [00300, 05004], lr: 0.000064, loss: 1.0013
2022-06-29 07:44:56 - train: epoch 0119, iter [00400, 05004], lr: 0.000063, loss: 0.8318
2022-06-29 07:45:30 - train: epoch 0119, iter [00500, 05004], lr: 0.000062, loss: 0.7406
2022-06-29 07:46:05 - train: epoch 0119, iter [00600, 05004], lr: 0.000061, loss: 0.9209
2022-06-29 07:46:40 - train: epoch 0119, iter [00700, 05004], lr: 0.000059, loss: 0.8813
2022-06-29 07:47:14 - train: epoch 0119, iter [00800, 05004], lr: 0.000058, loss: 0.9696
2022-06-29 07:47:49 - train: epoch 0119, iter [00900, 05004], lr: 0.000057, loss: 1.0075
2022-06-29 07:48:24 - train: epoch 0119, iter [01000, 05004], lr: 0.000056, loss: 0.8034
2022-06-29 07:48:59 - train: epoch 0119, iter [01100, 05004], lr: 0.000054, loss: 1.1080
2022-06-29 07:49:33 - train: epoch 0119, iter [01200, 05004], lr: 0.000053, loss: 1.0149
2022-06-29 07:50:09 - train: epoch 0119, iter [01300, 05004], lr: 0.000052, loss: 0.7863
2022-06-29 07:50:43 - train: epoch 0119, iter [01400, 05004], lr: 0.000051, loss: 0.8699
2022-06-29 07:51:18 - train: epoch 0119, iter [01500, 05004], lr: 0.000050, loss: 0.8458
2022-06-29 07:51:53 - train: epoch 0119, iter [01600, 05004], lr: 0.000048, loss: 0.9202
2022-06-29 07:52:28 - train: epoch 0119, iter [01700, 05004], lr: 0.000047, loss: 0.8754
2022-06-29 07:53:03 - train: epoch 0119, iter [01800, 05004], lr: 0.000046, loss: 0.8095
2022-06-29 07:53:37 - train: epoch 0119, iter [01900, 05004], lr: 0.000045, loss: 0.9265
2022-06-29 07:54:12 - train: epoch 0119, iter [02000, 05004], lr: 0.000044, loss: 0.7054
2022-06-29 07:54:47 - train: epoch 0119, iter [02100, 05004], lr: 0.000043, loss: 0.9817
2022-06-29 07:55:21 - train: epoch 0119, iter [02200, 05004], lr: 0.000042, loss: 0.9666
2022-06-29 07:55:56 - train: epoch 0119, iter [02300, 05004], lr: 0.000041, loss: 0.7954
2022-06-29 07:56:31 - train: epoch 0119, iter [02400, 05004], lr: 0.000040, loss: 0.8158
2022-06-29 07:57:06 - train: epoch 0119, iter [02500, 05004], lr: 0.000039, loss: 1.0014
2022-06-29 07:57:41 - train: epoch 0119, iter [02600, 05004], lr: 0.000038, loss: 0.9067
2022-06-29 07:58:16 - train: epoch 0119, iter [02700, 05004], lr: 0.000037, loss: 0.8996
2022-06-29 07:58:51 - train: epoch 0119, iter [02800, 05004], lr: 0.000036, loss: 0.9924
2022-06-29 07:59:26 - train: epoch 0119, iter [02900, 05004], lr: 0.000035, loss: 0.8677
2022-06-29 08:00:00 - train: epoch 0119, iter [03000, 05004], lr: 0.000034, loss: 0.7623
2022-06-29 08:00:36 - train: epoch 0119, iter [03100, 05004], lr: 0.000033, loss: 0.8654
2022-06-29 08:01:09 - train: epoch 0119, iter [03200, 05004], lr: 0.000032, loss: 0.7826
2022-06-29 08:01:44 - train: epoch 0119, iter [03300, 05004], lr: 0.000031, loss: 0.8265
2022-06-29 08:02:19 - train: epoch 0119, iter [03400, 05004], lr: 0.000030, loss: 0.8674
2022-06-29 08:02:54 - train: epoch 0119, iter [03500, 05004], lr: 0.000029, loss: 0.8087
2022-06-29 08:03:29 - train: epoch 0119, iter [03600, 05004], lr: 0.000028, loss: 1.0671
2022-06-29 08:04:03 - train: epoch 0119, iter [03700, 05004], lr: 0.000027, loss: 0.9035
2022-06-29 08:04:39 - train: epoch 0119, iter [03800, 05004], lr: 0.000026, loss: 0.7858
2022-06-29 08:05:13 - train: epoch 0119, iter [03900, 05004], lr: 0.000026, loss: 1.2717
2022-06-29 08:05:48 - train: epoch 0119, iter [04000, 05004], lr: 0.000025, loss: 0.8078
2022-06-29 08:06:23 - train: epoch 0119, iter [04100, 05004], lr: 0.000024, loss: 1.1060
2022-06-29 08:06:58 - train: epoch 0119, iter [04200, 05004], lr: 0.000023, loss: 0.8026
2022-06-29 08:07:33 - train: epoch 0119, iter [04300, 05004], lr: 0.000022, loss: 0.8388
2022-06-29 08:08:08 - train: epoch 0119, iter [04400, 05004], lr: 0.000022, loss: 0.9555
2022-06-29 08:08:43 - train: epoch 0119, iter [04500, 05004], lr: 0.000021, loss: 0.8378
2022-06-29 08:09:18 - train: epoch 0119, iter [04600, 05004], lr: 0.000020, loss: 0.9208
2022-06-29 08:09:52 - train: epoch 0119, iter [04700, 05004], lr: 0.000019, loss: 0.7351
2022-06-29 08:10:27 - train: epoch 0119, iter [04800, 05004], lr: 0.000019, loss: 0.9278
2022-06-29 08:11:02 - train: epoch 0119, iter [04900, 05004], lr: 0.000018, loss: 1.0622
2022-06-29 08:11:36 - train: epoch 0119, iter [05000, 05004], lr: 0.000017, loss: 0.9280
2022-06-29 08:11:37 - train: epoch 119, train_loss: 0.8703
2022-06-29 08:12:54 - eval: epoch: 119, acc1: 75.998%, acc5: 92.790%, test_loss: 0.9522, per_image_load_time: 2.536ms, per_image_inference_time: 0.418ms
2022-06-29 08:12:54 - until epoch: 119, best_acc1: 76.008%
2022-06-29 08:12:54 - epoch 120 lr: 0.000017
2022-06-29 08:13:34 - train: epoch 0120, iter [00100, 05004], lr: 0.000016, loss: 0.8881
2022-06-29 08:14:08 - train: epoch 0120, iter [00200, 05004], lr: 0.000016, loss: 0.9590
2022-06-29 08:14:43 - train: epoch 0120, iter [00300, 05004], lr: 0.000015, loss: 0.9557
2022-06-29 08:15:18 - train: epoch 0120, iter [00400, 05004], lr: 0.000015, loss: 0.8982
2022-06-29 08:15:52 - train: epoch 0120, iter [00500, 05004], lr: 0.000014, loss: 0.7633
2022-06-29 08:16:26 - train: epoch 0120, iter [00600, 05004], lr: 0.000013, loss: 0.8211
2022-06-29 08:17:00 - train: epoch 0120, iter [00700, 05004], lr: 0.000013, loss: 0.9903
2022-06-29 08:17:35 - train: epoch 0120, iter [00800, 05004], lr: 0.000012, loss: 0.8273
2022-06-29 08:18:10 - train: epoch 0120, iter [00900, 05004], lr: 0.000012, loss: 0.8329
2022-06-29 08:18:44 - train: epoch 0120, iter [01000, 05004], lr: 0.000011, loss: 0.8094
2022-06-29 08:19:17 - train: epoch 0120, iter [01100, 05004], lr: 0.000010, loss: 0.7818
2022-06-29 08:19:50 - train: epoch 0120, iter [01200, 05004], lr: 0.000010, loss: 0.9521
2022-06-29 08:20:25 - train: epoch 0120, iter [01300, 05004], lr: 0.000009, loss: 0.8436
2022-06-29 08:20:58 - train: epoch 0120, iter [01400, 05004], lr: 0.000009, loss: 0.8882
2022-06-29 08:21:32 - train: epoch 0120, iter [01500, 05004], lr: 0.000008, loss: 0.7824
2022-06-29 08:22:05 - train: epoch 0120, iter [01600, 05004], lr: 0.000008, loss: 0.8112
2022-06-29 08:22:39 - train: epoch 0120, iter [01700, 05004], lr: 0.000007, loss: 0.8611
2022-06-29 08:23:13 - train: epoch 0120, iter [01800, 05004], lr: 0.000007, loss: 0.8481
2022-06-29 08:23:47 - train: epoch 0120, iter [01900, 05004], lr: 0.000007, loss: 0.9021
2022-06-29 08:24:21 - train: epoch 0120, iter [02000, 05004], lr: 0.000006, loss: 0.8713
2022-06-29 08:24:56 - train: epoch 0120, iter [02100, 05004], lr: 0.000006, loss: 0.9941
2022-06-29 08:25:29 - train: epoch 0120, iter [02200, 05004], lr: 0.000005, loss: 0.9026
2022-06-29 08:26:03 - train: epoch 0120, iter [02300, 05004], lr: 0.000005, loss: 0.8158
2022-06-29 08:26:37 - train: epoch 0120, iter [02400, 05004], lr: 0.000005, loss: 0.8705
2022-06-29 08:27:11 - train: epoch 0120, iter [02500, 05004], lr: 0.000004, loss: 0.8975
2022-06-29 08:27:46 - train: epoch 0120, iter [02600, 05004], lr: 0.000004, loss: 0.8358
2022-06-29 08:28:19 - train: epoch 0120, iter [02700, 05004], lr: 0.000004, loss: 1.0611
2022-06-29 08:28:53 - train: epoch 0120, iter [02800, 05004], lr: 0.000003, loss: 0.8658
2022-06-29 08:29:28 - train: epoch 0120, iter [02900, 05004], lr: 0.000003, loss: 0.6509
2022-06-29 08:30:00 - train: epoch 0120, iter [03000, 05004], lr: 0.000003, loss: 0.7186
2022-06-29 08:30:34 - train: epoch 0120, iter [03100, 05004], lr: 0.000002, loss: 0.7323
2022-06-29 08:31:09 - train: epoch 0120, iter [03200, 05004], lr: 0.000002, loss: 0.9024
2022-06-29 08:31:43 - train: epoch 0120, iter [03300, 05004], lr: 0.000002, loss: 0.7463
2022-06-29 08:32:16 - train: epoch 0120, iter [03400, 05004], lr: 0.000002, loss: 0.7648
2022-06-29 08:32:49 - train: epoch 0120, iter [03500, 05004], lr: 0.000002, loss: 0.9114
2022-06-29 08:33:23 - train: epoch 0120, iter [03600, 05004], lr: 0.000001, loss: 0.8900
2022-06-29 08:33:56 - train: epoch 0120, iter [03700, 05004], lr: 0.000001, loss: 0.7343
2022-06-29 08:34:30 - train: epoch 0120, iter [03800, 05004], lr: 0.000001, loss: 0.8206
2022-06-29 08:35:03 - train: epoch 0120, iter [03900, 05004], lr: 0.000001, loss: 0.7771
2022-06-29 08:35:36 - train: epoch 0120, iter [04000, 05004], lr: 0.000001, loss: 0.8127
2022-06-29 08:36:10 - train: epoch 0120, iter [04100, 05004], lr: 0.000001, loss: 0.9284
2022-06-29 08:36:44 - train: epoch 0120, iter [04200, 05004], lr: 0.000000, loss: 1.0511
2022-06-29 08:37:17 - train: epoch 0120, iter [04300, 05004], lr: 0.000000, loss: 0.9400
2022-06-29 08:37:52 - train: epoch 0120, iter [04400, 05004], lr: 0.000000, loss: 0.8800
2022-06-29 08:38:26 - train: epoch 0120, iter [04500, 05004], lr: 0.000000, loss: 0.9358
2022-06-29 08:38:59 - train: epoch 0120, iter [04600, 05004], lr: 0.000000, loss: 0.8650
2022-06-29 08:39:33 - train: epoch 0120, iter [04700, 05004], lr: 0.000000, loss: 0.9781
2022-06-29 08:40:06 - train: epoch 0120, iter [04800, 05004], lr: 0.000000, loss: 1.0708
2022-06-29 08:40:39 - train: epoch 0120, iter [04900, 05004], lr: 0.000000, loss: 0.9066
2022-06-29 08:41:12 - train: epoch 0120, iter [05000, 05004], lr: 0.000000, loss: 0.8334
2022-06-29 08:41:13 - train: epoch 120, train_loss: 0.8692
2022-06-29 08:42:27 - eval: epoch: 120, acc1: 76.034%, acc5: 92.840%, test_loss: 0.9515, per_image_load_time: 1.908ms, per_image_inference_time: 0.427ms
2022-06-29 08:42:28 - until epoch: 120, best_acc1: 76.034%
2022-06-29 08:42:28 - train done. model: RepVGG_A2, train time: 59.801 hours, best_acc1: 76.034%
