2022-02-20 05:39:36 - network: darknet53
2022-02-20 05:39:36 - num_classes: 1000
2022-02-20 05:39:36 - input_image_size: 256
2022-02-20 05:39:36 - scale: 1.1428571428571428
2022-02-20 05:39:36 - trained_model_path: 
2022-02-20 05:39:36 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-20 05:39:36 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f8fac034400>
2022-02-20 05:39:36 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f8fac0346d0>
2022-02-20 05:39:36 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f8fac034700>
2022-02-20 05:39:36 - seed: 0
2022-02-20 05:39:36 - batch_size: 256
2022-02-20 05:39:36 - num_workers: 16
2022-02-20 05:39:36 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-20 05:39:36 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-20 05:39:36 - epochs: 100
2022-02-20 05:39:36 - print_interval: 100
2022-02-20 05:39:36 - distributed: True
2022-02-20 05:39:36 - sync_bn: False
2022-02-20 05:39:36 - apex: True
2022-02-20 05:39:36 - gpus_type: NVIDIA GeForce RTX 3090
2022-02-20 05:39:36 - gpus_num: 2
2022-02-20 05:39:36 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f8f862ef870>
2022-02-20 05:39:41 - --------------------parameters--------------------
2022-02-20 05:39:41 - name: conv1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: conv1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: conv1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: conv2.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: conv2.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: conv2.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block1.0.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block1.0.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block1.0.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block1.0.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block1.0.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block1.0.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: conv3.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: conv3.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: conv3.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block2.0.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block2.0.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block2.0.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block2.0.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block2.0.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block2.0.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block2.1.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block2.1.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block2.1.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block2.1.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block2.1.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block2.1.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: conv4.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: conv4.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: conv4.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.0.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.0.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.0.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.0.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.0.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.0.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.1.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.1.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.1.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.1.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.1.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.1.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.2.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.2.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.2.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.2.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.2.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.2.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.3.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.3.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.3.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.3.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.3.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.3.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.4.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.4.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.4.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.4.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.4.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.4.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.5.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.5.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.5.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.5.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.5.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.5.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.6.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.6.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.6.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.6.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.6.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.6.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.7.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.7.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.7.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block3.7.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block3.7.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block3.7.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: conv5.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: conv5.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: conv5.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.0.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.0.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.0.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.0.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.0.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.0.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.1.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.1.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.1.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.1.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.1.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.1.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.2.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.2.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.2.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.2.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.2.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.2.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.3.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.3.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.3.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.3.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.3.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.3.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.4.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.4.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.4.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.4.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.4.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.4.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.5.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.5.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.5.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.5.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.5.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.5.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.6.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.6.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.6.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.6.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.6.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.6.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.7.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.7.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.7.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block4.7.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block4.7.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block4.7.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: conv6.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: conv6.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: conv6.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block5.0.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block5.0.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block5.0.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block5.0.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block5.0.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block5.0.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block5.1.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block5.1.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block5.1.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block5.1.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block5.1.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block5.1.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block5.2.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block5.2.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block5.2.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block5.2.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block5.2.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block5.2.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block5.3.conv.0.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block5.3.conv.0.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block5.3.conv.0.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: block5.3.conv.1.layer.0.weight, grad: True
2022-02-20 05:39:41 - name: block5.3.conv.1.layer.1.weight, grad: True
2022-02-20 05:39:41 - name: block5.3.conv.1.layer.1.bias, grad: True
2022-02-20 05:39:41 - name: fc.weight, grad: True
2022-02-20 05:39:41 - name: fc.bias, grad: True
2022-02-20 05:39:41 - --------------------buffers--------------------
2022-02-20 05:39:41 - name: conv1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: conv1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: conv2.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: conv2.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block1.0.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block1.0.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block1.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block1.0.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block1.0.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block1.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: conv3.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: conv3.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block2.0.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block2.0.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block2.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block2.0.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block2.0.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block2.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block2.1.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block2.1.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block2.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block2.1.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block2.1.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block2.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: conv4.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: conv4.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: conv4.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.0.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.0.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.0.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.0.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.1.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.1.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.1.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.1.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.2.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.2.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.2.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.2.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.3.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.3.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.3.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.3.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.4.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.4.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.4.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.4.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.5.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.5.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.5.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.5.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.6.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.6.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.6.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.6.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.7.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.7.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block3.7.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block3.7.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block3.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: conv5.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: conv5.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: conv5.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.0.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.0.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.0.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.0.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.1.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.1.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.1.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.1.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.2.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.2.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.2.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.2.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.3.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.3.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.3.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.3.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.4.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.4.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.4.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.4.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.5.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.5.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.5.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.5.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.6.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.6.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.6.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.6.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.7.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.7.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block4.7.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block4.7.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block4.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: conv6.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: conv6.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: conv6.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block5.0.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block5.0.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block5.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block5.0.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block5.0.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block5.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block5.1.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block5.1.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block5.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block5.1.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block5.1.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block5.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block5.2.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block5.2.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block5.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block5.2.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block5.2.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block5.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block5.3.conv.0.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block5.3.conv.0.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block5.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - name: block5.3.conv.1.layer.1.running_mean, grad: False
2022-02-20 05:39:41 - name: block5.3.conv.1.layer.1.running_var, grad: False
2022-02-20 05:39:41 - name: block5.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:39:41 - epoch 001 lr: 0.1
2022-02-20 05:40:20 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9498
2022-02-20 05:40:53 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.9310
2022-02-20 05:41:25 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.9230
2022-02-20 05:41:57 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.8873
2022-02-20 05:42:29 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.8688
2022-02-20 05:43:01 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.8453
2022-02-20 05:43:33 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.8026
2022-02-20 05:44:06 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.7954
2022-02-20 05:44:38 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.7280
2022-02-20 05:45:10 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.7025
2022-02-20 05:45:42 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.7123
2022-02-20 05:46:15 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 6.6684
2022-02-20 05:46:47 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 6.6592
2022-02-20 05:47:20 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 6.6143
2022-02-20 05:47:52 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 6.5850
2022-02-20 05:48:25 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 6.6106
2022-02-20 05:48:58 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 6.5080
2022-02-20 05:49:31 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 6.4109
2022-02-20 05:50:04 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 6.5423
2022-02-20 05:50:36 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 6.4166
2022-02-20 05:51:09 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 6.3970
2022-02-20 05:51:42 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 6.3788
2022-02-20 05:52:14 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 6.3478
2022-02-20 05:52:47 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 6.3084
2022-02-20 05:53:20 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 6.3255
2022-02-20 05:53:53 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 6.2916
2022-02-20 05:54:27 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 6.2070
2022-02-20 05:54:59 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 6.1585
2022-02-20 05:55:32 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 6.0789
2022-02-20 05:56:05 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.9253
2022-02-20 05:56:38 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.9903
2022-02-20 05:57:11 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 5.9734
2022-02-20 05:57:44 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 5.7703
2022-02-20 05:58:17 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 5.9002
2022-02-20 05:58:50 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 5.6370
2022-02-20 05:59:23 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 5.7193
2022-02-20 05:59:56 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 5.6411
2022-02-20 06:00:30 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 5.5051
2022-02-20 06:01:03 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 5.4393
2022-02-20 06:01:36 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 5.3642
2022-02-20 06:02:09 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 5.5167
2022-02-20 06:02:42 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 5.3447
2022-02-20 06:03:16 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 5.2718
2022-02-20 06:03:49 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 5.1684
2022-02-20 06:04:23 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 5.3619
2022-02-20 06:04:57 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 5.2502
2022-02-20 06:05:31 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.9358
2022-02-20 06:06:05 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 5.2496
2022-02-20 06:06:39 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 5.1497
2022-02-20 06:07:13 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.9236
2022-02-20 06:07:14 - train: epoch 001, train_loss: 6.1365
2022-02-20 06:08:31 - eval: epoch: 001, acc1: 10.442%, acc5: 26.342%, test_loss: 4.9988, per_image_load_time: 2.379ms, per_image_inference_time: 0.588ms
2022-02-20 06:08:31 - until epoch: 001, best_acc1: 10.442%
2022-02-20 06:08:31 - epoch 002 lr: 0.1
2022-02-20 06:09:10 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.9832
2022-02-20 06:09:42 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.7716
2022-02-20 06:10:13 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 5.1002
2022-02-20 06:10:46 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.8876
2022-02-20 06:11:18 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.7413
2022-02-20 06:11:50 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.7253
2022-02-20 06:12:23 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.9239
2022-02-20 06:12:55 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 4.5646
2022-02-20 06:13:28 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 4.3348
2022-02-20 06:14:01 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.7581
2022-02-20 06:14:33 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.6749
2022-02-20 06:15:06 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.4909
2022-02-20 06:15:39 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.5383
2022-02-20 06:16:12 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.6745
2022-02-20 06:16:44 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.5303
2022-02-20 06:17:17 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 4.3230
2022-02-20 06:17:50 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.3903
2022-02-20 06:18:23 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.4211
2022-02-20 06:18:56 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 4.2943
2022-02-20 06:19:29 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 4.0774
2022-02-20 06:20:02 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 4.3065
2022-02-20 06:20:35 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 4.0426
2022-02-20 06:21:08 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 4.2098
2022-02-20 06:21:42 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 4.0099
2022-02-20 06:22:15 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 4.0454
2022-02-20 06:22:48 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.9487
2022-02-20 06:23:21 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 4.1895
2022-02-20 06:23:54 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.9819
2022-02-20 06:24:27 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.9661
2022-02-20 06:25:00 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.8252
2022-02-20 06:25:34 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.9038
2022-02-20 06:26:07 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.9518
2022-02-20 06:26:40 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.8544
2022-02-20 06:27:13 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 4.0412
2022-02-20 06:27:47 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.8566
2022-02-20 06:28:20 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.8146
2022-02-20 06:28:53 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.9379
2022-02-20 06:29:27 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.6770
2022-02-20 06:30:00 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.7850
2022-02-20 06:30:34 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.7533
2022-02-20 06:31:07 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.8235
2022-02-20 06:31:41 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.6794
2022-02-20 06:32:15 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.6304
2022-02-20 06:32:48 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.6498
2022-02-20 06:33:22 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.5757
2022-02-20 06:33:57 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.6316
2022-02-20 06:34:31 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.6389
2022-02-20 06:35:06 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.6419
2022-02-20 06:35:41 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.4717
2022-02-20 06:36:16 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.4497
2022-02-20 06:36:18 - train: epoch 002, train_loss: 4.1598
2022-02-20 06:37:35 - eval: epoch: 002, acc1: 29.286%, acc5: 54.926%, test_loss: 3.3754, per_image_load_time: 1.577ms, per_image_inference_time: 0.568ms
2022-02-20 06:37:36 - until epoch: 002, best_acc1: 29.286%
2022-02-20 06:37:36 - epoch 003 lr: 0.1
2022-02-20 06:38:14 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.6103
2022-02-20 06:38:46 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.4838
2022-02-20 06:39:18 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.4552
2022-02-20 06:39:50 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.4468
2022-02-20 06:40:22 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.5795
2022-02-20 06:40:55 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.3362
2022-02-20 06:41:27 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.6650
2022-02-20 06:42:00 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.5076
2022-02-20 06:42:33 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.3215
2022-02-20 06:43:06 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.3895
2022-02-20 06:43:38 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.3145
2022-02-20 06:44:11 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.2909
2022-02-20 06:44:44 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.3109
2022-02-20 06:45:17 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.2076
2022-02-20 06:45:51 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.6270
2022-02-20 06:46:23 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.3089
2022-02-20 06:46:56 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.2372
2022-02-20 06:47:29 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.1625
2022-02-20 06:48:02 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.4246
2022-02-20 06:48:35 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.5662
2022-02-20 06:49:08 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.4297
2022-02-20 06:49:41 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.6232
2022-02-20 06:50:14 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.3201
2022-02-20 06:50:47 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.1922
2022-02-20 06:51:20 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.3775
2022-02-20 06:51:53 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.2769
2022-02-20 06:52:26 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.5164
2022-02-20 06:52:59 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.0247
2022-02-20 06:53:32 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.2712
2022-02-20 06:54:05 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.4333
2022-02-20 06:54:38 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.3231
2022-02-20 06:55:11 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.2468
2022-02-20 06:55:44 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.2028
2022-02-20 06:56:17 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.3140
2022-02-20 06:56:51 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.9468
2022-02-20 06:57:24 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.0688
2022-02-20 06:57:57 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.1614
2022-02-20 06:58:31 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.2321
2022-02-20 06:59:04 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.3243
2022-02-20 06:59:37 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.1388
2022-02-20 07:00:10 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.0898
2022-02-20 07:00:44 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.9822
2022-02-20 07:01:17 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.7204
2022-02-20 07:01:51 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.0163
2022-02-20 07:02:25 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.0226
2022-02-20 07:02:59 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.0594
2022-02-20 07:03:33 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.9762
2022-02-20 07:04:08 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.1832
2022-02-20 07:04:43 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.0377
2022-02-20 07:05:18 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.0791
2022-02-20 07:05:19 - train: epoch 003, train_loss: 3.2426
2022-02-20 07:06:37 - eval: epoch: 003, acc1: 38.136%, acc5: 64.594%, test_loss: 2.8745, per_image_load_time: 2.415ms, per_image_inference_time: 0.580ms
2022-02-20 07:06:38 - until epoch: 003, best_acc1: 38.136%
2022-02-20 07:06:38 - epoch 004 lr: 0.1
2022-02-20 07:07:16 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.0741
2022-02-20 07:07:48 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.9169
2022-02-20 07:08:20 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.9844
2022-02-20 07:08:53 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.8557
2022-02-20 07:09:25 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.7667
2022-02-20 07:09:57 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.1869
2022-02-20 07:10:29 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.0112
2022-02-20 07:11:01 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.8429
2022-02-20 07:11:34 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.5566
2022-02-20 07:12:06 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.9295
2022-02-20 07:12:39 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.0613
2022-02-20 07:13:11 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.6856
2022-02-20 07:13:44 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.8133
2022-02-20 07:14:17 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.0005
2022-02-20 07:14:50 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.0374
2022-02-20 07:15:23 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.8045
2022-02-20 07:15:56 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.9569
2022-02-20 07:16:29 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.0908
2022-02-20 07:17:02 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 2.9318
2022-02-20 07:17:35 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.9070
2022-02-20 07:18:08 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.9338
2022-02-20 07:18:40 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.8789
2022-02-20 07:19:13 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.6774
2022-02-20 07:19:46 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.8692
2022-02-20 07:20:19 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.7504
2022-02-20 07:20:52 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.9831
2022-02-20 07:21:25 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.6781
2022-02-20 07:21:57 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.8064
2022-02-20 07:22:31 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.8639
2022-02-20 07:23:03 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.8002
2022-02-20 07:23:36 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.9688
2022-02-20 07:24:09 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.7300
2022-02-20 07:24:42 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.9588
2022-02-20 07:25:15 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.8684
2022-02-20 07:25:48 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.6988
2022-02-20 07:26:21 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.6291
2022-02-20 07:26:54 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.7410
2022-02-20 07:27:27 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.7040
2022-02-20 07:28:01 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.7233
2022-02-20 07:28:34 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.3748
2022-02-20 07:29:07 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.7345
2022-02-20 07:29:41 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.5742
2022-02-20 07:30:14 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.6501
2022-02-20 07:30:48 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.5659
2022-02-20 07:31:22 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.2623
2022-02-20 07:31:56 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.7871
2022-02-20 07:32:31 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.6618
2022-02-20 07:33:05 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.6569
2022-02-20 07:33:40 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.7709
2022-02-20 07:34:15 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.7645
2022-02-20 07:34:16 - train: epoch 004, train_loss: 2.8521
2022-02-20 07:35:32 - eval: epoch: 004, acc1: 44.532%, acc5: 71.026%, test_loss: 2.4671, per_image_load_time: 2.367ms, per_image_inference_time: 0.596ms
2022-02-20 07:35:34 - until epoch: 004, best_acc1: 44.532%
2022-02-20 07:35:34 - epoch 005 lr: 0.1
2022-02-20 07:36:11 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.7779
2022-02-20 07:36:43 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.8044
2022-02-20 07:37:15 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.9505
2022-02-20 07:37:47 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.6254
2022-02-20 07:38:20 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.4742
2022-02-20 07:38:52 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.7631
2022-02-20 07:39:24 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.7475
2022-02-20 07:39:57 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 2.9359
2022-02-20 07:40:29 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.5974
2022-02-20 07:41:02 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.6800
2022-02-20 07:41:34 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.6814
2022-02-20 07:42:06 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.6619
2022-02-20 07:42:39 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.6252
2022-02-20 07:43:12 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.7692
2022-02-20 07:43:44 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.5512
2022-02-20 07:44:17 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.4668
2022-02-20 07:44:50 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.5299
2022-02-20 07:45:22 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.7312
2022-02-20 07:45:55 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.5740
2022-02-20 07:46:28 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.7380
2022-02-20 07:47:01 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.3917
2022-02-20 07:47:34 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.5758
2022-02-20 07:48:07 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.4606
2022-02-20 07:48:40 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.5850
2022-02-20 07:49:14 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.6483
2022-02-20 07:49:47 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.7854
2022-02-20 07:50:20 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.7226
2022-02-20 07:50:53 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.6431
2022-02-20 07:51:26 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.5611
2022-02-20 07:51:59 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.6111
2022-02-20 07:52:32 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.6995
2022-02-20 07:53:05 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.7286
2022-02-20 07:53:39 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.4028
2022-02-20 07:54:12 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.4652
2022-02-20 07:54:45 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.6980
2022-02-20 07:55:18 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.6065
2022-02-20 07:55:51 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.5788
2022-02-20 07:56:25 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.4137
2022-02-20 07:56:58 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.8700
2022-02-20 07:57:32 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.5931
2022-02-20 07:58:05 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.5619
2022-02-20 07:58:39 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.6969
2022-02-20 07:59:13 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.5109
2022-02-20 07:59:46 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.6516
2022-02-20 08:00:20 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.6211
2022-02-20 08:00:55 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.5285
2022-02-20 08:01:29 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.3664
2022-02-20 08:02:04 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.4210
2022-02-20 08:02:39 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.6720
2022-02-20 08:03:14 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.4556
2022-02-20 08:03:15 - train: epoch 005, train_loss: 2.6373
2022-02-20 08:04:31 - eval: epoch: 005, acc1: 47.888%, acc5: 74.360%, test_loss: 2.2654, per_image_load_time: 1.381ms, per_image_inference_time: 0.607ms
2022-02-20 08:04:32 - until epoch: 005, best_acc1: 47.888%
2022-02-20 08:04:32 - epoch 006 lr: 0.1
2022-02-20 08:05:10 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.4990
2022-02-20 08:05:42 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.5263
2022-02-20 08:06:14 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.3770
2022-02-20 08:06:46 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.6197
2022-02-20 08:07:18 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.5142
2022-02-20 08:07:50 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.6575
2022-02-20 08:08:22 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.7306
2022-02-20 08:08:54 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.4254
2022-02-20 08:09:26 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.4542
2022-02-20 08:09:59 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.5172
2022-02-20 08:10:31 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.3859
2022-02-20 08:11:04 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.5813
2022-02-20 08:11:36 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.7253
2022-02-20 08:12:09 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.7026
2022-02-20 08:12:41 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.6778
2022-02-20 08:13:14 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.3404
2022-02-20 08:13:47 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.6990
2022-02-20 08:14:19 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.6649
2022-02-20 08:14:53 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.4480
2022-02-20 08:15:25 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.6145
2022-02-20 08:15:58 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.5874
2022-02-20 08:16:31 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.4708
2022-02-20 08:17:04 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.2439
2022-02-20 08:17:37 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.4855
2022-02-20 08:18:09 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.8888
2022-02-20 08:18:42 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.4100
2022-02-20 08:19:15 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.5852
2022-02-20 08:19:48 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.3358
2022-02-20 08:20:21 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.6817
2022-02-20 08:20:54 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.5200
2022-02-20 08:21:27 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.2351
2022-02-20 08:22:00 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.4429
2022-02-20 08:22:33 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.3850
2022-02-20 08:23:06 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.6208
2022-02-20 08:23:39 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.5811
2022-02-20 08:24:12 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.5206
2022-02-20 08:24:45 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.5290
2022-02-20 08:25:19 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.2879
2022-02-20 08:25:52 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.4128
2022-02-20 08:26:25 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.7035
2022-02-20 08:26:58 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.3882
2022-02-20 08:27:32 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.3278
2022-02-20 08:28:05 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.5292
2022-02-20 08:28:39 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.4503
2022-02-20 08:29:12 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.5232
2022-02-20 08:29:46 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.3643
2022-02-20 08:30:21 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.4585
2022-02-20 08:30:55 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.4481
2022-02-20 08:31:30 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.5214
2022-02-20 08:32:04 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.2537
2022-02-20 08:32:06 - train: epoch 006, train_loss: 2.5029
2022-02-20 08:33:25 - eval: epoch: 006, acc1: 49.644%, acc5: 75.036%, test_loss: 2.1922, per_image_load_time: 2.539ms, per_image_inference_time: 0.550ms
2022-02-20 08:33:26 - until epoch: 006, best_acc1: 49.644%
2022-02-20 08:33:26 - epoch 007 lr: 0.1
2022-02-20 08:34:04 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.3559
2022-02-20 08:34:36 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.5881
2022-02-20 08:35:09 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.7494
2022-02-20 08:35:41 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.4889
2022-02-20 08:36:14 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.3926
2022-02-20 08:36:47 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.5936
2022-02-20 08:37:20 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.4945
2022-02-20 08:37:53 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.4340
2022-02-20 08:38:25 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.5178
2022-02-20 08:38:58 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.5192
2022-02-20 08:39:31 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.3936
2022-02-20 08:40:03 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.4247
2022-02-20 08:40:36 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.4047
2022-02-20 08:41:09 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.4941
2022-02-20 08:41:42 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.5600
2022-02-20 08:42:15 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.4354
2022-02-20 08:42:48 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.4977
2022-02-20 08:43:20 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.3152
2022-02-20 08:43:54 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.4798
2022-02-20 08:44:27 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.2049
2022-02-20 08:45:00 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.6068
2022-02-20 08:45:33 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.2655
2022-02-20 08:46:06 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.5189
2022-02-20 08:46:39 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.4506
2022-02-20 08:47:12 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.4089
2022-02-20 08:47:45 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.3409
2022-02-20 08:48:19 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.2413
2022-02-20 08:48:52 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.3874
2022-02-20 08:49:25 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.2842
2022-02-20 08:49:58 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.5725
2022-02-20 08:50:32 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.3127
2022-02-20 08:51:05 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.3204
2022-02-20 08:51:38 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.7586
2022-02-20 08:52:12 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.2860
2022-02-20 08:52:45 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.4626
2022-02-20 08:53:18 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.2782
2022-02-20 08:53:51 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.3722
2022-02-20 08:54:24 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.5860
2022-02-20 08:54:58 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.3004
2022-02-20 08:55:31 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.5317
2022-02-20 08:56:04 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.3089
2022-02-20 08:56:38 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.2561
2022-02-20 08:57:12 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.5293
2022-02-20 08:57:45 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.2963
2022-02-20 08:58:19 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.5344
2022-02-20 08:58:53 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.4253
2022-02-20 08:59:28 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.4963
2022-02-20 09:00:03 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.4492
2022-02-20 09:00:38 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.3264
2022-02-20 09:01:12 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.3794
2022-02-20 09:01:14 - train: epoch 007, train_loss: 2.4092
2022-02-20 09:02:31 - eval: epoch: 007, acc1: 50.928%, acc5: 77.082%, test_loss: 2.0966, per_image_load_time: 2.431ms, per_image_inference_time: 0.558ms
2022-02-20 09:02:32 - until epoch: 007, best_acc1: 50.928%
2022-02-20 09:02:32 - epoch 008 lr: 0.1
2022-02-20 09:03:09 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.3008
2022-02-20 09:03:41 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.5053
2022-02-20 09:04:14 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.1253
2022-02-20 09:04:46 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.1577
2022-02-20 09:05:19 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.2776
2022-02-20 09:05:51 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.3033
2022-02-20 09:06:24 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.5963
2022-02-20 09:06:56 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.2221
2022-02-20 09:07:29 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.3867
2022-02-20 09:08:02 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.3669
2022-02-20 09:08:34 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.2123
2022-02-20 09:09:07 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.2254
2022-02-20 09:09:40 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.3589
2022-02-20 09:10:13 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.3336
2022-02-20 09:10:45 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.3666
2022-02-20 09:11:18 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.4272
2022-02-20 09:11:51 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.3119
2022-02-20 09:12:24 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.4511
2022-02-20 09:12:57 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.1635
2022-02-20 09:13:30 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.3306
2022-02-20 09:14:03 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.3185
2022-02-20 09:14:36 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.1954
2022-02-20 09:15:09 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.3864
2022-02-20 09:15:42 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.2902
2022-02-20 09:16:15 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.2890
2022-02-20 09:16:48 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.4290
2022-02-20 09:17:22 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.5053
2022-02-20 09:17:55 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.4052
2022-02-20 09:18:28 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.3850
2022-02-20 09:19:01 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.3544
2022-02-20 09:19:34 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.2506
2022-02-20 09:20:08 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.5212
2022-02-20 09:20:41 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.5605
2022-02-20 09:21:14 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.4793
2022-02-20 09:21:47 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.3100
2022-02-20 09:22:20 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.5029
2022-02-20 09:22:53 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.2704
2022-02-20 09:23:26 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.2036
2022-02-20 09:23:59 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.4720
2022-02-20 09:24:33 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.7031
2022-02-20 09:25:06 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.3237
2022-02-20 09:25:39 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.2358
2022-02-20 09:26:13 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.0738
2022-02-20 09:26:46 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.3089
2022-02-20 09:27:20 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.5377
2022-02-20 09:27:54 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.4681
2022-02-20 09:28:29 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.2222
2022-02-20 09:29:03 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.4486
2022-02-20 09:29:38 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.3862
2022-02-20 09:30:13 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.2724
2022-02-20 09:30:14 - train: epoch 008, train_loss: 2.3409
2022-02-20 09:31:30 - eval: epoch: 008, acc1: 53.730%, acc5: 79.156%, test_loss: 1.9631, per_image_load_time: 2.240ms, per_image_inference_time: 0.614ms
2022-02-20 09:31:31 - until epoch: 008, best_acc1: 53.730%
2022-02-20 09:31:31 - epoch 009 lr: 0.1
2022-02-20 09:32:09 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.0561
2022-02-20 09:32:42 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.0882
2022-02-20 09:33:14 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.0419
2022-02-20 09:33:46 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.3801
2022-02-20 09:34:18 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.2478
2022-02-20 09:34:51 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.2922
2022-02-20 09:35:23 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.2560
2022-02-20 09:35:56 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.1775
2022-02-20 09:36:28 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.1804
2022-02-20 09:37:00 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.0958
2022-02-20 09:37:33 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.5652
2022-02-20 09:38:06 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.3936
2022-02-20 09:38:38 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.3957
2022-02-20 09:39:11 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.0926
2022-02-20 09:39:44 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.2717
2022-02-20 09:40:17 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.3058
2022-02-20 09:40:49 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.5105
2022-02-20 09:41:22 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.1338
2022-02-20 09:41:55 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.0893
2022-02-20 09:42:28 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.0687
2022-02-20 09:43:01 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.3235
2022-02-20 09:43:34 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.3668
2022-02-20 09:44:07 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.0661
2022-02-20 09:44:40 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.3013
2022-02-20 09:45:13 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.1432
2022-02-20 09:45:46 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.2272
2022-02-20 09:46:19 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.3360
2022-02-20 09:46:52 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.4108
2022-02-20 09:47:25 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 1.9356
2022-02-20 09:47:58 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.1112
2022-02-20 09:48:31 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.3792
2022-02-20 09:49:04 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.3273
2022-02-20 09:49:37 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.2573
2022-02-20 09:50:10 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.4678
2022-02-20 09:50:43 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.4339
2022-02-20 09:51:16 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.1279
2022-02-20 09:51:50 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.4884
2022-02-20 09:52:23 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.4366
2022-02-20 09:52:56 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 1.9723
2022-02-20 09:53:30 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.4690
2022-02-20 09:54:03 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.2387
2022-02-20 09:54:37 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.2239
2022-02-20 09:55:11 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.3749
2022-02-20 09:55:45 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.2062
2022-02-20 09:56:19 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.2641
2022-02-20 09:56:53 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.3847
2022-02-20 09:57:28 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.4167
2022-02-20 09:58:03 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.4852
2022-02-20 09:58:38 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.3871
2022-02-20 09:59:12 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.1128
2022-02-20 09:59:14 - train: epoch 009, train_loss: 2.2914
2022-02-20 10:00:31 - eval: epoch: 009, acc1: 53.354%, acc5: 78.484%, test_loss: 1.9898, per_image_load_time: 2.422ms, per_image_inference_time: 0.574ms
2022-02-20 10:00:32 - until epoch: 009, best_acc1: 53.730%
2022-02-20 10:00:32 - epoch 010 lr: 0.1
2022-02-20 10:01:10 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.1676
2022-02-20 10:01:42 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.3172
2022-02-20 10:02:15 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.2699
2022-02-20 10:02:47 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.3283
2022-02-20 10:03:20 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.1821
2022-02-20 10:03:52 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.3260
2022-02-20 10:04:24 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.3357
2022-02-20 10:04:57 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.1934
2022-02-20 10:05:29 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.0564
2022-02-20 10:06:02 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.0517
2022-02-20 10:06:35 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.2582
2022-02-20 10:07:07 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.1375
2022-02-20 10:07:40 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.1632
2022-02-20 10:08:13 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.2778
2022-02-20 10:08:46 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.0039
2022-02-20 10:09:19 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.3455
2022-02-20 10:09:52 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.4805
2022-02-20 10:10:24 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.2772
2022-02-20 10:10:57 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.3591
2022-02-20 10:11:30 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.3137
2022-02-20 10:12:02 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.1634
2022-02-20 10:12:35 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.3937
2022-02-20 10:13:08 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.4851
2022-02-20 10:13:41 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.3346
2022-02-20 10:14:14 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.2142
2022-02-20 10:14:47 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.2910
2022-02-20 10:15:20 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 1.9098
2022-02-20 10:15:53 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.2097
2022-02-20 10:16:26 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.2685
2022-02-20 10:16:59 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.2359
2022-02-20 10:17:32 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.4484
2022-02-20 10:18:06 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.2023
2022-02-20 10:18:39 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.4379
2022-02-20 10:19:12 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.4176
2022-02-20 10:19:45 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.4915
2022-02-20 10:20:18 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.4717
2022-02-20 10:20:52 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.2760
2022-02-20 10:21:25 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.2991
2022-02-20 10:21:59 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 1.8386
2022-02-20 10:22:32 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.0804
2022-02-20 10:23:06 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.1742
2022-02-20 10:23:39 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.3268
2022-02-20 10:24:12 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.3678
2022-02-20 10:24:46 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.2705
2022-02-20 10:25:20 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.1371
2022-02-20 10:25:54 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.3350
2022-02-20 10:26:28 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.2245
2022-02-20 10:27:03 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.2227
2022-02-20 10:27:38 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.1054
2022-02-20 10:28:12 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 1.9954
2022-02-20 10:28:14 - train: epoch 010, train_loss: 2.2506
2022-02-20 10:29:32 - eval: epoch: 010, acc1: 54.898%, acc5: 79.988%, test_loss: 1.9033, per_image_load_time: 2.460ms, per_image_inference_time: 0.566ms
2022-02-20 10:29:33 - until epoch: 010, best_acc1: 54.898%
2022-02-20 10:29:33 - epoch 011 lr: 0.1
2022-02-20 10:30:11 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.0336
2022-02-20 10:30:43 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.3772
2022-02-20 10:31:16 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.0345
2022-02-20 10:31:48 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.2472
2022-02-20 10:32:21 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.1965
2022-02-20 10:32:53 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.1932
2022-02-20 10:33:26 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.3080
2022-02-20 10:33:58 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.2650
2022-02-20 10:34:31 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.4359
2022-02-20 10:35:03 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.1658
2022-02-20 10:35:36 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.2538
2022-02-20 10:36:09 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.5987
2022-02-20 10:36:42 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.4430
2022-02-20 10:37:14 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.3670
2022-02-20 10:37:47 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.1777
2022-02-20 10:38:20 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.3597
2022-02-20 10:38:53 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.2172
2022-02-20 10:39:27 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.1352
2022-02-20 10:40:00 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 1.9877
2022-02-20 10:40:32 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.3779
2022-02-20 10:41:05 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.2679
2022-02-20 10:41:39 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.0940
2022-02-20 10:42:11 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.3846
2022-02-20 10:42:45 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 1.9535
2022-02-20 10:43:17 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.4805
2022-02-20 10:43:51 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.1914
2022-02-20 10:44:24 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.2844
2022-02-20 10:44:57 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 1.8258
2022-02-20 10:45:30 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.2856
2022-02-20 10:46:03 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.4633
2022-02-20 10:46:36 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.2309
2022-02-20 10:47:10 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.1144
2022-02-20 10:47:43 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.2452
2022-02-20 10:48:16 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.1954
2022-02-20 10:48:49 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.1466
2022-02-20 10:49:23 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.1703
2022-02-20 10:49:56 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.3638
2022-02-20 10:50:29 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 1.9901
2022-02-20 10:51:02 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.2592
2022-02-20 10:51:36 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.0365
2022-02-20 10:52:09 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 1.9791
2022-02-20 10:52:43 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.1440
2022-02-20 10:53:17 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.2581
2022-02-20 10:53:51 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.1460
2022-02-20 10:54:25 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 1.9715
2022-02-20 10:54:59 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.0055
2022-02-20 10:55:34 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.0284
2022-02-20 10:56:08 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.0114
2022-02-20 10:56:43 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 1.9232
2022-02-20 10:57:18 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.2269
2022-02-20 10:57:19 - train: epoch 011, train_loss: 2.2198
2022-02-20 10:58:35 - eval: epoch: 011, acc1: 55.130%, acc5: 80.192%, test_loss: 1.8871, per_image_load_time: 2.338ms, per_image_inference_time: 0.588ms
2022-02-20 10:58:37 - until epoch: 011, best_acc1: 55.130%
2022-02-20 10:58:37 - epoch 012 lr: 0.1
2022-02-20 10:59:15 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.0662
2022-02-20 10:59:47 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.0925
2022-02-20 11:00:19 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.0989
2022-02-20 11:00:51 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.2356
2022-02-20 11:01:24 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.3740
2022-02-20 11:01:56 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 1.9688
2022-02-20 11:02:29 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.0265
2022-02-20 11:03:02 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.1776
2022-02-20 11:03:34 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.2880
2022-02-20 11:04:07 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.1074
2022-02-20 11:04:39 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.4479
2022-02-20 11:05:11 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.0497
2022-02-20 11:05:43 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.1465
2022-02-20 11:06:16 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.4126
2022-02-20 11:06:48 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.0831
2022-02-20 11:07:21 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.0806
2022-02-20 11:07:53 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.0333
2022-02-20 11:08:25 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.2718
2022-02-20 11:08:58 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.2323
2022-02-20 11:09:30 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.2931
2022-02-20 11:10:03 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.2430
2022-02-20 11:10:35 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.3287
2022-02-20 11:11:08 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.2232
2022-02-20 11:11:40 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.1902
2022-02-20 11:12:13 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 1.9554
2022-02-20 11:12:46 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.0866
2022-02-20 11:13:18 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.0961
2022-02-20 11:13:51 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.2336
2022-02-20 11:14:24 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.0358
2022-02-20 11:14:56 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.0659
2022-02-20 11:15:29 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.2909
2022-02-20 11:16:02 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 1.9626
2022-02-20 11:16:34 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.1678
2022-02-20 11:17:07 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.1918
2022-02-20 11:17:39 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.4002
2022-02-20 11:18:12 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.2169
2022-02-20 11:18:45 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.1684
2022-02-20 11:19:18 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.1927
2022-02-20 11:19:51 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.1977
2022-02-20 11:20:24 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.0627
2022-02-20 11:20:57 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.1534
2022-02-20 11:21:30 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.0716
2022-02-20 11:22:03 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.2464
2022-02-20 11:22:37 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.0466
2022-02-20 11:23:10 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.1558
2022-02-20 11:23:44 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.4626
2022-02-20 11:24:19 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.0472
2022-02-20 11:24:53 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.3929
2022-02-20 11:25:28 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.1820
2022-02-20 11:26:02 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 1.8637
2022-02-20 11:26:04 - train: epoch 012, train_loss: 2.1939
2022-02-20 11:27:22 - eval: epoch: 012, acc1: 56.182%, acc5: 80.892%, test_loss: 1.8533, per_image_load_time: 2.453ms, per_image_inference_time: 0.569ms
2022-02-20 11:27:23 - until epoch: 012, best_acc1: 56.182%

