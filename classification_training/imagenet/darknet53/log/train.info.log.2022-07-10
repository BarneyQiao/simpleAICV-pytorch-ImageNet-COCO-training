2022-07-10 22:38:35 - network: darknet53
2022-07-10 22:38:35 - num_classes: 1000
2022-07-10 22:38:35 - input_image_size: 256
2022-07-10 22:38:35 - scale: 1.1428571428571428
2022-07-10 22:38:35 - trained_model_path: 
2022-07-10 22:38:35 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-10 22:38:35 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-10 22:38:35 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f7937ca3400>
2022-07-10 22:38:35 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f7937ca36d0>
2022-07-10 22:38:35 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f7937ca3700>
2022-07-10 22:38:35 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f7937ca3760>
2022-07-10 22:38:35 - seed: 0
2022-07-10 22:38:35 - batch_size: 256
2022-07-10 22:38:35 - num_workers: 16
2022-07-10 22:38:35 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-07-10 22:38:35 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-07-10 22:38:35 - epochs: 100
2022-07-10 22:38:35 - print_interval: 100
2022-07-10 22:38:35 - accumulation_steps: 1
2022-07-10 22:38:35 - sync_bn: False
2022-07-10 22:38:35 - apex: True
2022-07-10 22:38:35 - use_ema_model: False
2022-07-10 22:38:35 - ema_model_decay: 0.9999
2022-07-10 22:38:35 - gpus_type: NVIDIA RTX A5000
2022-07-10 22:38:35 - gpus_num: 2
2022-07-10 22:38:35 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f78fcca35b0>
2022-07-10 22:38:35 - --------------------parameters--------------------
2022-07-10 22:38:35 - name: conv1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: conv1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: conv1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: conv2.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: conv2.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: conv2.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block1.0.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block1.0.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block1.0.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block1.0.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block1.0.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block1.0.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: conv3.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: conv3.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: conv3.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block2.0.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block2.0.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block2.0.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block2.0.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block2.0.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block2.0.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block2.1.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block2.1.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block2.1.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block2.1.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block2.1.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block2.1.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: conv4.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: conv4.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: conv4.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.0.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.0.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.0.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.0.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.0.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.0.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.1.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.1.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.1.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.1.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.1.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.1.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.2.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.2.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.2.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.2.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.2.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.2.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.3.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.3.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.3.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.3.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.3.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.3.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.4.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.4.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.4.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.4.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.4.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.4.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.5.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.5.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.5.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.5.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.5.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.5.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.6.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.6.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.6.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.6.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.6.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.6.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.7.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.7.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.7.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block3.7.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block3.7.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block3.7.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: conv5.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: conv5.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: conv5.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.0.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.0.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.0.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.0.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.0.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.0.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.1.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.1.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.1.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.1.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.1.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.1.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.2.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.2.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.2.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.2.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.2.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.2.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.3.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.3.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.3.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.3.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.3.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.3.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.4.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.4.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.4.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.4.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.4.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.4.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.5.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.5.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.5.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.5.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.5.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.5.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.6.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.6.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.6.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.6.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.6.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.6.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.7.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.7.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.7.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block4.7.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block4.7.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block4.7.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: conv6.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: conv6.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: conv6.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block5.0.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block5.0.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block5.0.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block5.0.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block5.0.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block5.0.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block5.1.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block5.1.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block5.1.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block5.1.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block5.1.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block5.1.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block5.2.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block5.2.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block5.2.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block5.2.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block5.2.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block5.2.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block5.3.conv.0.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block5.3.conv.0.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block5.3.conv.0.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: block5.3.conv.1.layer.0.weight, grad: True
2022-07-10 22:38:35 - name: block5.3.conv.1.layer.1.weight, grad: True
2022-07-10 22:38:35 - name: block5.3.conv.1.layer.1.bias, grad: True
2022-07-10 22:38:35 - name: fc.weight, grad: True
2022-07-10 22:38:35 - name: fc.bias, grad: True
2022-07-10 22:38:35 - --------------------buffers--------------------
2022-07-10 22:38:35 - name: conv1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: conv1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: conv2.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: conv2.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block1.0.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block1.0.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block1.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block1.0.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block1.0.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block1.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: conv3.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: conv3.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block2.0.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block2.0.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block2.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block2.0.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block2.0.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block2.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block2.1.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block2.1.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block2.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block2.1.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block2.1.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block2.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: conv4.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: conv4.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: conv4.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.0.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.0.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.0.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.0.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.1.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.1.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.1.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.1.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.2.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.2.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.2.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.2.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.3.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.3.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.3.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.3.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.4.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.4.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.4.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.4.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.5.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.5.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.5.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.5.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.6.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.6.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.6.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.6.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.7.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.7.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block3.7.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block3.7.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block3.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: conv5.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: conv5.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: conv5.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.0.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.0.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.0.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.0.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.1.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.1.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.1.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.1.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.2.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.2.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.2.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.2.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.3.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.3.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.3.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.3.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.4.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.4.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.4.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.4.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.5.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.5.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.5.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.5.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.6.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.6.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.6.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.6.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.7.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.7.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block4.7.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block4.7.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block4.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: conv6.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: conv6.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: conv6.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block5.0.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block5.0.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block5.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block5.0.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block5.0.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block5.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block5.1.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block5.1.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block5.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block5.1.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block5.1.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block5.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block5.2.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block5.2.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block5.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block5.2.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block5.2.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block5.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block5.3.conv.0.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block5.3.conv.0.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block5.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - name: block5.3.conv.1.layer.1.running_mean, grad: False
2022-07-10 22:38:35 - name: block5.3.conv.1.layer.1.running_var, grad: False
2022-07-10 22:38:35 - name: block5.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-07-10 22:38:35 - -----------no weight decay layers--------------
2022-07-10 22:38:35 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block1.0.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block1.0.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block1.0.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block1.0.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block2.0.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block2.0.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block2.0.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block2.0.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block2.1.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block2.1.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block2.1.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block2.1.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv4.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv4.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.0.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.0.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.0.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.0.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.1.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.1.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.1.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.1.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.2.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.2.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.2.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.2.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.3.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.3.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.3.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.3.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.4.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.4.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.4.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.4.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.5.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.5.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.5.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.5.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.6.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.6.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.6.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.6.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.7.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.7.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.7.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.7.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv5.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv5.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.0.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.0.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.0.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.0.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.1.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.1.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.1.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.1.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.2.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.2.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.2.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.2.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.3.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.3.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.3.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.3.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.4.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.4.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.4.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.4.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.5.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.5.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.5.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.5.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.6.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.6.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.6.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.6.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.7.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.7.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.7.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.7.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv6.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv6.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.0.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.0.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.0.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.0.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.1.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.1.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.1.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.1.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.2.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.2.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.2.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.2.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.3.conv.0.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.3.conv.0.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.3.conv.1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.3.conv.1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 22:38:35 - -------------weight decay layers---------------
2022-07-10 22:38:35 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block1.0.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block1.0.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block2.0.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block2.0.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block2.1.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block2.1.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv4.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.0.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.0.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.1.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.1.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.2.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.2.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.3.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.3.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.4.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.4.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.5.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.5.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.6.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.6.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.7.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block3.7.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv5.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.0.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.0.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.1.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.1.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.2.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.2.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.3.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.3.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.4.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.4.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.5.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.5.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.6.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.6.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.7.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block4.7.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: conv6.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.0.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.0.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.1.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.1.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.2.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.2.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.3.conv.0.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: block5.3.conv.1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-10 22:38:35 - epoch 001 lr: 0.100000
2022-07-10 22:39:14 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 7.0399
2022-07-10 22:39:47 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.9293
2022-07-10 22:40:20 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.9254
2022-07-10 22:40:53 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.8946
2022-07-10 22:41:27 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.8475
2022-07-10 22:42:00 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.7688
2022-07-10 22:42:33 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.8498
2022-07-10 22:43:06 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.7710
2022-07-10 22:43:39 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.7501
2022-07-10 22:44:12 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.6809
2022-07-10 22:44:45 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.7936
2022-07-10 22:45:18 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 6.6810
2022-07-10 22:45:51 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 6.8070
2022-07-10 22:46:24 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 6.6052
2022-07-10 22:46:58 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 6.5001
2022-07-10 22:47:30 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 6.5491
2022-07-10 22:48:03 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 6.4039
2022-07-10 22:48:36 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 6.4018
2022-07-10 22:49:10 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 6.3438
2022-07-10 22:49:43 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 6.3093
2022-07-10 22:50:16 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 6.3126
2022-07-10 22:50:49 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 6.2521
2022-07-10 22:51:22 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 6.2308
2022-07-10 22:51:55 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 6.1980
2022-07-10 22:52:28 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 6.1474
2022-07-10 22:53:02 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 6.1169
2022-07-10 22:53:35 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 6.2070
2022-07-10 22:54:08 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 5.9998
2022-07-10 22:54:41 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 5.9790
2022-07-10 22:55:14 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.8968
2022-07-10 22:55:47 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.8186
2022-07-10 22:56:20 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 5.8393
2022-07-10 22:56:54 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 5.7312
2022-07-10 22:57:27 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 5.6544
2022-07-10 22:58:00 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 5.4628
2022-07-10 22:58:33 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 5.6107
2022-07-10 22:59:06 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 5.6273
2022-07-10 22:59:39 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 5.4616
2022-07-10 23:00:12 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 5.3938
2022-07-10 23:00:45 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 5.2936
2022-07-10 23:01:18 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 5.3415
2022-07-10 23:01:52 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 5.1742
2022-07-10 23:02:25 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 5.1845
2022-07-10 23:02:58 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.9701
2022-07-10 23:03:31 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 5.0950
2022-07-10 23:04:05 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 5.1868
2022-07-10 23:04:38 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.8557
2022-07-10 23:05:11 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 5.1807
2022-07-10 23:05:44 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 5.0048
2022-07-10 23:06:16 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.8739
2022-07-10 23:06:18 - train: epoch 001, train_loss: 6.0624
2022-07-10 23:07:33 - eval: epoch: 001, acc1: 11.522%, acc5: 28.518%, test_loss: 5.0169, per_image_load_time: 1.763ms, per_image_inference_time: 0.596ms
2022-07-10 23:07:33 - until epoch: 001, best_acc1: 11.522%
2022-07-10 23:07:33 - epoch 002 lr: 0.100000
2022-07-10 23:08:13 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.8625
2022-07-10 23:08:45 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.6263
2022-07-10 23:09:17 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.9136
2022-07-10 23:09:49 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.7113
2022-07-10 23:10:22 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.5395
2022-07-10 23:10:55 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.4942
2022-07-10 23:11:27 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.7125
2022-07-10 23:12:00 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 4.3823
2022-07-10 23:12:33 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 4.1812
2022-07-10 23:13:06 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.6270
2022-07-10 23:13:39 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.4784
2022-07-10 23:14:12 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.3304
2022-07-10 23:14:46 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.3385
2022-07-10 23:15:19 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.4538
2022-07-10 23:15:52 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.3141
2022-07-10 23:16:24 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 4.1974
2022-07-10 23:16:57 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.3095
2022-07-10 23:17:30 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.2988
2022-07-10 23:18:03 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 4.0892
2022-07-10 23:18:36 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.9132
2022-07-10 23:19:09 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 4.1782
2022-07-10 23:19:42 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.8342
2022-07-10 23:20:15 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 4.0764
2022-07-10 23:20:48 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.8292
2022-07-10 23:21:21 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.8366
2022-07-10 23:21:54 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.7499
2022-07-10 23:22:27 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.9932
2022-07-10 23:23:00 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.9704
2022-07-10 23:23:33 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.8328
2022-07-10 23:24:05 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.7555
2022-07-10 23:24:38 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.8101
2022-07-10 23:25:11 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.7433
2022-07-10 23:25:44 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.7706
2022-07-10 23:26:17 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.8684
2022-07-10 23:26:51 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.8018
2022-07-10 23:27:24 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.6688
2022-07-10 23:27:57 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.7736
2022-07-10 23:28:30 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.4549
2022-07-10 23:29:04 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.6541
2022-07-10 23:29:37 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.6172
2022-07-10 23:30:10 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.7433
2022-07-10 23:30:43 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.5275
2022-07-10 23:31:16 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.5677
2022-07-10 23:31:49 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.4162
2022-07-10 23:32:22 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.4403
2022-07-10 23:32:55 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.5267
2022-07-10 23:33:28 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.5228
2022-07-10 23:34:02 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.4898
2022-07-10 23:34:35 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.2988
2022-07-10 23:35:07 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.3406
2022-07-10 23:35:09 - train: epoch 002, train_loss: 4.0129
2022-07-10 23:36:24 - eval: epoch: 002, acc1: 30.846%, acc5: 56.696%, test_loss: nan, per_image_load_time: 1.136ms, per_image_inference_time: 0.576ms
2022-07-10 23:36:25 - until epoch: 002, best_acc1: 30.846%
2022-07-10 23:36:25 - epoch 003 lr: 0.100000
2022-07-10 23:37:04 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.4482
2022-07-10 23:37:37 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.4545
2022-07-10 23:38:09 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.2425
2022-07-10 23:38:41 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.4261
2022-07-10 23:39:14 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.3589
2022-07-10 23:39:46 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.1455
2022-07-10 23:40:19 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.5738
2022-07-10 23:40:52 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.4605
2022-07-10 23:41:25 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.1351
2022-07-10 23:41:57 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.2895
2022-07-10 23:42:29 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.2533
2022-07-10 23:43:02 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.0150
2022-07-10 23:43:35 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.1513
2022-07-10 23:44:08 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.1919
2022-07-10 23:44:41 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.4194
2022-07-10 23:45:14 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.1658
2022-07-10 23:45:48 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.1275
2022-07-10 23:46:21 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.0131
2022-07-10 23:46:54 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.2574
2022-07-10 23:47:27 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.4288
2022-07-10 23:48:00 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.4596
2022-07-10 23:48:33 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.5808
2022-07-10 23:49:06 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.1110
2022-07-10 23:49:39 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.0948
2022-07-10 23:50:12 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.1518
2022-07-10 23:50:45 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.2188
2022-07-10 23:51:18 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.3635
2022-07-10 23:51:51 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 2.9807
2022-07-10 23:52:24 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.0930
2022-07-10 23:52:57 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.2865
2022-07-10 23:53:30 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.2876
2022-07-10 23:54:04 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.1257
2022-07-10 23:54:37 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.0219
2022-07-10 23:55:10 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.1824
2022-07-10 23:55:44 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.8301
2022-07-10 23:56:17 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 2.9137
2022-07-10 23:56:50 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.0019
2022-07-10 23:57:24 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.1732
2022-07-10 23:57:57 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.1860
2022-07-10 23:58:30 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.0448
2022-07-10 23:59:03 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 2.9911
2022-07-10 23:59:36 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.9396
2022-07-11 00:00:09 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.6947
2022-07-11 00:00:43 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.9082
2022-07-11 00:01:17 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 2.9688
2022-07-11 00:01:50 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 2.9272
2022-07-11 00:02:23 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.6934
2022-07-11 00:02:56 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.0271
2022-07-11 00:03:29 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.0321
2022-07-11 00:04:01 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 2.9385
2022-07-11 00:04:03 - train: epoch 003, train_loss: 3.1243
2022-07-11 00:05:18 - eval: epoch: 003, acc1: 39.822%, acc5: 66.260%, test_loss: 3.1238, per_image_load_time: 1.097ms, per_image_inference_time: 0.574ms
2022-07-11 00:05:19 - until epoch: 003, best_acc1: 39.822%
2022-07-11 00:05:19 - epoch 004 lr: 0.100000
2022-07-11 00:05:58 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 2.9595
2022-07-11 00:06:30 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.7375
2022-07-11 00:07:03 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.8581
2022-07-11 00:07:36 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.7971
2022-07-11 00:08:09 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.6815
2022-07-11 00:08:41 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 2.9976
2022-07-11 00:09:14 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 2.7533
2022-07-11 00:09:47 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.6499
2022-07-11 00:10:20 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.4267
2022-07-11 00:10:53 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.8001
2022-07-11 00:11:26 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 2.8927
2022-07-11 00:11:59 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.5587
2022-07-11 00:12:32 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.6622
2022-07-11 00:13:05 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 2.8466
2022-07-11 00:13:38 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 2.8425
2022-07-11 00:14:11 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.7325
2022-07-11 00:14:44 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.9084
2022-07-11 00:15:18 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 2.9420
2022-07-11 00:15:51 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 2.7800
2022-07-11 00:16:24 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.7886
2022-07-11 00:16:57 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.8601
2022-07-11 00:17:30 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.7480
2022-07-11 00:18:03 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.5610
2022-07-11 00:18:36 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.6330
2022-07-11 00:19:10 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.6150
2022-07-11 00:19:43 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.7623
2022-07-11 00:20:16 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.5208
2022-07-11 00:20:49 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.6563
2022-07-11 00:21:22 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.6763
2022-07-11 00:21:55 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.6302
2022-07-11 00:22:29 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.8227
2022-07-11 00:23:02 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.6272
2022-07-11 00:23:35 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.8171
2022-07-11 00:24:07 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.7176
2022-07-11 00:24:40 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.6750
2022-07-11 00:25:13 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.5627
2022-07-11 00:25:47 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.6363
2022-07-11 00:26:21 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.5697
2022-07-11 00:26:54 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.6460
2022-07-11 00:27:27 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.3019
2022-07-11 00:28:00 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.5944
2022-07-11 00:28:33 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.5626
2022-07-11 00:29:06 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.3981
2022-07-11 00:29:39 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.5045
2022-07-11 00:30:12 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.1633
2022-07-11 00:30:45 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.6134
2022-07-11 00:31:18 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.5073
2022-07-11 00:31:51 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.5609
2022-07-11 00:32:24 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.7122
2022-07-11 00:32:57 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.6232
2022-07-11 00:32:58 - train: epoch 004, train_loss: 2.7284
2022-07-11 00:34:13 - eval: epoch: 004, acc1: 46.860%, acc5: 72.786%, test_loss: 2.5312, per_image_load_time: 0.844ms, per_image_inference_time: 0.594ms
2022-07-11 00:34:14 - until epoch: 004, best_acc1: 46.860%
2022-07-11 00:34:14 - epoch 005 lr: 0.100000
2022-07-11 00:34:53 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.6956
2022-07-11 00:35:26 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.6102
2022-07-11 00:35:59 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.8085
2022-07-11 00:36:32 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.5251
2022-07-11 00:37:05 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.3106
2022-07-11 00:37:38 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.5988
2022-07-11 00:38:10 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.6201
2022-07-11 00:38:43 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 2.6881
2022-07-11 00:39:16 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.5558
2022-07-11 00:39:48 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.5875
2022-07-11 00:40:21 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.6477
2022-07-11 00:40:54 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.5697
2022-07-11 00:41:27 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.5300
2022-07-11 00:42:00 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.7219
2022-07-11 00:42:33 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.4072
2022-07-11 00:43:06 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.3813
2022-07-11 00:43:39 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.3858
2022-07-11 00:44:12 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.6058
2022-07-11 00:44:45 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.4783
2022-07-11 00:45:18 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.5527
2022-07-11 00:45:51 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.3126
2022-07-11 00:46:24 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.2097
2022-07-11 00:46:58 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.2069
2022-07-11 00:47:31 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.4248
2022-07-11 00:48:04 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.5606
2022-07-11 00:48:38 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.7158
2022-07-11 00:49:11 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.5454
2022-07-11 00:49:44 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.5225
2022-07-11 00:50:17 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.3265
2022-07-11 00:50:50 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.4495
2022-07-11 00:51:23 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.5774
2022-07-11 00:51:56 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.6673
2022-07-11 00:52:30 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.3432
2022-07-11 00:53:03 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.3473
2022-07-11 00:53:36 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.3948
2022-07-11 00:54:09 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.4646
2022-07-11 00:54:42 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.3501
2022-07-11 00:55:15 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.3046
2022-07-11 00:55:48 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.8075
2022-07-11 00:56:22 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.4059
2022-07-11 00:56:55 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.3548
2022-07-11 00:57:28 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.5003
2022-07-11 00:58:02 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.3181
2022-07-11 00:58:35 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.5445
2022-07-11 00:59:08 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.4394
2022-07-11 00:59:41 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.3990
2022-07-11 01:00:14 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.2625
2022-07-11 01:00:48 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.2692
2022-07-11 01:01:21 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.5928
2022-07-11 01:01:53 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.3205
2022-07-11 01:01:55 - train: epoch 005, train_loss: 2.5079
2022-07-11 01:03:10 - eval: epoch: 005, acc1: 50.572%, acc5: 75.970%, test_loss: 2.1691, per_image_load_time: 1.202ms, per_image_inference_time: 0.585ms
2022-07-11 01:03:11 - until epoch: 005, best_acc1: 50.572%
2022-07-11 01:03:11 - epoch 006 lr: 0.100000
2022-07-11 01:03:51 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.2956
2022-07-11 01:04:23 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.4679
2022-07-11 01:04:57 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.2162
2022-07-11 01:05:30 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.4899
2022-07-11 01:06:03 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.4644
2022-07-11 01:06:36 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.4086
2022-07-11 01:07:10 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.4569
2022-07-11 01:07:42 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.3846
2022-07-11 01:08:15 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.1970
2022-07-11 01:08:48 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.4195
2022-07-11 01:09:21 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.3032
2022-07-11 01:09:54 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.4369
2022-07-11 01:10:27 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.5657
2022-07-11 01:11:00 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.5301
2022-07-11 01:11:33 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.5822
2022-07-11 01:12:06 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.1500
2022-07-11 01:12:40 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.4070
2022-07-11 01:13:13 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.5742
2022-07-11 01:13:46 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.2880
2022-07-11 01:14:18 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.5059
2022-07-11 01:14:52 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.5014
2022-07-11 01:15:25 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.2703
2022-07-11 01:15:58 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.2272
2022-07-11 01:16:31 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.4237
2022-07-11 01:17:04 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.6409
2022-07-11 01:17:37 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.2801
2022-07-11 01:18:10 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.5110
2022-07-11 01:18:44 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.2604
2022-07-11 01:19:17 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.6084
2022-07-11 01:19:50 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.4467
2022-07-11 01:20:23 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.1426
2022-07-11 01:20:56 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.3672
2022-07-11 01:21:29 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.1970
2022-07-11 01:22:02 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.4953
2022-07-11 01:22:36 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.5100
2022-07-11 01:23:09 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.3383
2022-07-11 01:23:42 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.3888
2022-07-11 01:24:15 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.1739
2022-07-11 01:24:48 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.2676
2022-07-11 01:25:21 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.6145
2022-07-11 01:25:54 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.2452
2022-07-11 01:26:28 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.1295
2022-07-11 01:27:01 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.2649
2022-07-11 01:27:34 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.3348
2022-07-11 01:28:07 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.3319
2022-07-11 01:28:40 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.2141
2022-07-11 01:29:13 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.3252
2022-07-11 01:29:46 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.3318
2022-07-11 01:30:19 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.4428
2022-07-11 01:30:52 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.1828
2022-07-11 01:30:53 - train: epoch 006, train_loss: 2.3727
2022-07-11 01:32:08 - eval: epoch: 006, acc1: 51.558%, acc5: 77.098%, test_loss: 2.1420, per_image_load_time: 1.581ms, per_image_inference_time: 0.599ms
2022-07-11 01:32:08 - until epoch: 006, best_acc1: 51.558%
2022-07-11 01:32:08 - epoch 007 lr: 0.100000
2022-07-11 01:32:47 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.1019
2022-07-11 01:33:20 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.4386
2022-07-11 01:33:53 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.5854
2022-07-11 01:34:26 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.4184
2022-07-11 01:34:58 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.2547
2022-07-11 01:35:31 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.5140
2022-07-11 01:36:04 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.3263
2022-07-11 01:36:37 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.2765
2022-07-11 01:37:10 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.2891
2022-07-11 01:37:42 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.3984
2022-07-11 01:38:15 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.2375
2022-07-11 01:38:48 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.3866
2022-07-11 01:39:21 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.2634
2022-07-11 01:39:54 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.2518
2022-07-11 01:40:27 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.4925
2022-07-11 01:41:00 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.2564
2022-07-11 01:41:33 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.3528
2022-07-11 01:42:06 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.2918
2022-07-11 01:42:39 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.3657
2022-07-11 01:43:12 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.1206
2022-07-11 01:43:46 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.4238
2022-07-11 01:44:18 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.0794
2022-07-11 01:44:52 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.3138
2022-07-11 01:45:24 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.2621
2022-07-11 01:45:58 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.2971
2022-07-11 01:46:31 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.3126
2022-07-11 01:47:04 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.1166
2022-07-11 01:47:37 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.2392
2022-07-11 01:48:10 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.2189
2022-07-11 01:48:43 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.3719
2022-07-11 01:49:16 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.1238
2022-07-11 01:49:49 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.2176
2022-07-11 01:50:22 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.6110
2022-07-11 01:50:55 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.1567
2022-07-11 01:51:29 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.3865
2022-07-11 01:52:02 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.0852
2022-07-11 01:52:35 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.3055
2022-07-11 01:53:07 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.4608
2022-07-11 01:53:40 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.2031
2022-07-11 01:54:13 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.4365
2022-07-11 01:54:46 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.1815
2022-07-11 01:55:19 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.2371
2022-07-11 01:55:52 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.3910
2022-07-11 01:56:25 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.1200
2022-07-11 01:56:58 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.4550
2022-07-11 01:57:31 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.3813
2022-07-11 01:58:05 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.4326
2022-07-11 01:58:38 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.5476
2022-07-11 01:59:11 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.3158
2022-07-11 01:59:44 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.2942
2022-07-11 01:59:45 - train: epoch 007, train_loss: 2.2764
2022-07-11 02:01:00 - eval: epoch: 007, acc1: 54.202%, acc5: 79.208%, test_loss: 1.9493, per_image_load_time: 1.645ms, per_image_inference_time: 0.586ms
2022-07-11 02:01:01 - until epoch: 007, best_acc1: 54.202%
2022-07-11 02:01:01 - epoch 008 lr: 0.100000
2022-07-11 02:01:40 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.2041
2022-07-11 02:02:12 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.3188
2022-07-11 02:02:45 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 1.9261
2022-07-11 02:03:18 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 1.9904
2022-07-11 02:03:50 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.1562
2022-07-11 02:04:23 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.1779
2022-07-11 02:04:56 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.5406
2022-07-11 02:05:29 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.0876
2022-07-11 02:06:02 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.2676
2022-07-11 02:06:35 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.1898
2022-07-11 02:07:08 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.0633
2022-07-11 02:07:41 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.0976
2022-07-11 02:08:14 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.2631
2022-07-11 02:08:47 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.1955
2022-07-11 02:09:20 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.2062
2022-07-11 02:09:53 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.3407
2022-07-11 02:10:26 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.2021
2022-07-11 02:10:59 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.3199
2022-07-11 02:11:31 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.0385
2022-07-11 02:12:05 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.1247
2022-07-11 02:12:38 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.2313
2022-07-11 02:13:11 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.1813
2022-07-11 02:13:44 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.3251
2022-07-11 02:14:17 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.0738
2022-07-11 02:14:50 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.2362
2022-07-11 02:15:24 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.3150
2022-07-11 02:15:57 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.2881
2022-07-11 02:16:30 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.2586
2022-07-11 02:17:03 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.2002
2022-07-11 02:17:36 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.2892
2022-07-11 02:18:09 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.1838
2022-07-11 02:18:43 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.4325
2022-07-11 02:19:16 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.3200
2022-07-11 02:19:49 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.3798
2022-07-11 02:20:22 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.1461
2022-07-11 02:20:56 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.3190
2022-07-11 02:21:29 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.0871
2022-07-11 02:22:02 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.1380
2022-07-11 02:22:36 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.2602
2022-07-11 02:23:10 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.5887
2022-07-11 02:23:43 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.2127
2022-07-11 02:24:16 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.1370
2022-07-11 02:24:49 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 1.9034
2022-07-11 02:25:22 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.1159
2022-07-11 02:25:56 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.4043
2022-07-11 02:26:29 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.3186
2022-07-11 02:27:02 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.0297
2022-07-11 02:27:36 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.2897
2022-07-11 02:28:09 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.2403
2022-07-11 02:28:42 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.1398
2022-07-11 02:28:43 - train: epoch 008, train_loss: 2.2084
2022-07-11 02:29:59 - eval: epoch: 008, acc1: 55.830%, acc5: 80.462%, test_loss: 1.8743, per_image_load_time: 2.279ms, per_image_inference_time: 0.579ms
2022-07-11 02:30:00 - until epoch: 008, best_acc1: 55.830%
2022-07-11 02:30:00 - epoch 009 lr: 0.100000
2022-07-11 02:30:38 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 1.9088
2022-07-11 02:31:11 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.0394
2022-07-11 02:31:43 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 1.8374
2022-07-11 02:32:16 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.3283
2022-07-11 02:32:49 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.0903
2022-07-11 02:33:22 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.1278
2022-07-11 02:33:55 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.0861
2022-07-11 02:34:28 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.0002
2022-07-11 02:35:01 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.0754
2022-07-11 02:35:34 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 1.9384
2022-07-11 02:36:07 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.4739
2022-07-11 02:36:40 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.1890
2022-07-11 02:37:13 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.3919
2022-07-11 02:37:46 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 1.9509
2022-07-11 02:38:19 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.0693
2022-07-11 02:38:52 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.1118
2022-07-11 02:39:26 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.3097
2022-07-11 02:39:59 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.1117
2022-07-11 02:40:32 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 1.9871
2022-07-11 02:41:05 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 1.8551
2022-07-11 02:41:38 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.1772
2022-07-11 02:42:11 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.3588
2022-07-11 02:42:45 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 1.9125
2022-07-11 02:43:17 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.1612
2022-07-11 02:43:51 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.1133
2022-07-11 02:44:24 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.1350
2022-07-11 02:44:57 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.0976
2022-07-11 02:45:30 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.1917
2022-07-11 02:46:03 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 1.9297
2022-07-11 02:46:36 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.0403
2022-07-11 02:47:09 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.2358
2022-07-11 02:47:43 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.1937
2022-07-11 02:48:16 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.1500
2022-07-11 02:48:49 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.2715
2022-07-11 02:49:22 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.3169
2022-07-11 02:49:56 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.1256
2022-07-11 02:50:29 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.3905
2022-07-11 02:51:02 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.3968
2022-07-11 02:51:35 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 1.8432
2022-07-11 02:52:08 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.3626
2022-07-11 02:52:41 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.0952
2022-07-11 02:53:15 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.0611
2022-07-11 02:53:48 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.2346
2022-07-11 02:54:21 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.1280
2022-07-11 02:54:54 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.0535
2022-07-11 02:55:27 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.2070
2022-07-11 02:56:00 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.2350
2022-07-11 02:56:34 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.3436
2022-07-11 02:57:07 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.1570
2022-07-11 02:57:39 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.0931
2022-07-11 02:57:41 - train: epoch 009, train_loss: 2.1583
2022-07-11 02:58:56 - eval: epoch: 009, acc1: 56.184%, acc5: 80.868%, test_loss: 1.8545, per_image_load_time: 1.932ms, per_image_inference_time: 0.581ms
2022-07-11 02:58:57 - until epoch: 009, best_acc1: 56.184%
2022-07-11 02:58:57 - epoch 010 lr: 0.100000
2022-07-11 02:59:36 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 1.9835
2022-07-11 03:00:09 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.1480
2022-07-11 03:00:42 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.0831
2022-07-11 03:01:15 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.0953
2022-07-11 03:01:48 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 1.9858
2022-07-11 03:02:21 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.1907
2022-07-11 03:02:54 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.2041
2022-07-11 03:03:27 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.0424
2022-07-11 03:04:00 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 1.9909
2022-07-11 03:04:32 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 1.9395
2022-07-11 03:05:05 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.0191
2022-07-11 03:05:38 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 1.9187
2022-07-11 03:06:11 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 1.9995
2022-07-11 03:06:44 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.1640
2022-07-11 03:07:17 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 1.8809
2022-07-11 03:07:50 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.1372
2022-07-11 03:08:23 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.2931
2022-07-11 03:08:57 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.2261
2022-07-11 03:09:30 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.1624
2022-07-11 03:10:03 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.2860
2022-07-11 03:10:36 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.0277
2022-07-11 03:11:09 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.3013
2022-07-11 03:11:42 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.3256
2022-07-11 03:12:15 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.2757
2022-07-11 03:12:48 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.1350
2022-07-11 03:13:21 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.2700
2022-07-11 03:13:54 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 1.8498
2022-07-11 03:14:27 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.1570
2022-07-11 03:15:00 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.1539
2022-07-11 03:15:33 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.1355
2022-07-11 03:16:06 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.3814
2022-07-11 03:16:39 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.0954
2022-07-11 03:17:12 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.2480
2022-07-11 03:17:45 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.3177
2022-07-11 03:18:18 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.3839
2022-07-11 03:18:52 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.3539
2022-07-11 03:19:25 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.1189
2022-07-11 03:19:58 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.1326
2022-07-11 03:20:31 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 1.7219
2022-07-11 03:21:04 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.0740
2022-07-11 03:21:38 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 1.8926
2022-07-11 03:22:11 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.1239
2022-07-11 03:22:44 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.0779
2022-07-11 03:23:17 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.0707
2022-07-11 03:23:50 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.0288
2022-07-11 03:24:24 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.2174
2022-07-11 03:24:57 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.2107
2022-07-11 03:25:30 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.1089
2022-07-11 03:26:03 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 1.9926
2022-07-11 03:26:36 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 1.8877
2022-07-11 03:26:37 - train: epoch 010, train_loss: 2.1185
2022-07-11 03:27:52 - eval: epoch: 010, acc1: 56.684%, acc5: 81.148%, test_loss: 1.8277, per_image_load_time: 2.126ms, per_image_inference_time: 0.597ms
2022-07-11 03:27:53 - until epoch: 010, best_acc1: 56.684%
2022-07-11 03:27:53 - epoch 011 lr: 0.100000
2022-07-11 03:28:32 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.0169
2022-07-11 03:29:05 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.1831
2022-07-11 03:29:38 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 1.8512
2022-07-11 03:30:11 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.1166
2022-07-11 03:30:44 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.0428
2022-07-11 03:31:17 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.0469
2022-07-11 03:31:50 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.0769
2022-07-11 03:32:23 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.0809
2022-07-11 03:32:56 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.2449
2022-07-11 03:33:29 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.0263
2022-07-11 03:34:02 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.1911
2022-07-11 03:34:35 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.3407
2022-07-11 03:35:09 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.2974
2022-07-11 03:35:42 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.1301
2022-07-11 03:36:15 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 1.9832
2022-07-11 03:36:48 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.1356
2022-07-11 03:37:21 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.0864
2022-07-11 03:37:54 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.0219
2022-07-11 03:38:27 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 1.9467
2022-07-11 03:39:00 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.2479
2022-07-11 03:39:33 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.0555
2022-07-11 03:40:06 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.0465
2022-07-11 03:40:39 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.2779
2022-07-11 03:41:12 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 1.8260
2022-07-11 03:41:45 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.2955
2022-07-11 03:42:19 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 1.9734
2022-07-11 03:42:52 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.0852
2022-07-11 03:43:25 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 1.7466
2022-07-11 03:43:59 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.1778
2022-07-11 03:44:32 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.3551
2022-07-11 03:45:05 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.2009
2022-07-11 03:45:39 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 1.9791
2022-07-11 03:46:12 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.1409
2022-07-11 03:46:45 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 1.9673
2022-07-11 03:47:18 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.1098
2022-07-11 03:47:52 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.1274
2022-07-11 03:48:25 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.1716
2022-07-11 03:48:58 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 1.8226
2022-07-11 03:49:31 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.2070
2022-07-11 03:50:04 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 1.9423
2022-07-11 03:50:37 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 1.9064
2022-07-11 03:51:10 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.0179
2022-07-11 03:51:44 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.0259
2022-07-11 03:52:17 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 1.9523
2022-07-11 03:52:50 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 1.9132
2022-07-11 03:53:24 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.0109
2022-07-11 03:53:57 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 1.9224
2022-07-11 03:54:30 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 1.9045
2022-07-11 03:55:04 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 1.8265
2022-07-11 03:55:36 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.0423
2022-07-11 03:55:38 - train: epoch 011, train_loss: 2.0866
2022-07-11 03:56:54 - eval: epoch: 011, acc1: 57.658%, acc5: 82.072%, test_loss: 1.7908, per_image_load_time: 1.013ms, per_image_inference_time: 0.590ms
2022-07-11 03:56:54 - until epoch: 011, best_acc1: 57.658%
2022-07-11 03:56:54 - epoch 012 lr: 0.100000
2022-07-11 03:57:34 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 1.8694
2022-07-11 03:58:07 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 1.9050
2022-07-11 03:58:40 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.0781
2022-07-11 03:59:12 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.1233
2022-07-11 03:59:45 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.2203
2022-07-11 04:00:18 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 1.7975
2022-07-11 04:00:51 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 1.8286
2022-07-11 04:01:24 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.0143
2022-07-11 04:01:57 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.1362
2022-07-11 04:02:30 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.0124
2022-07-11 04:03:03 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.3871
2022-07-11 04:03:36 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 1.8934
2022-07-11 04:04:09 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 1.9324
2022-07-11 04:04:42 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.3202
2022-07-11 04:05:16 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 1.8999
2022-07-11 04:05:49 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 1.9883
2022-07-11 04:06:22 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 1.8494
2022-07-11 04:06:55 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.1510
2022-07-11 04:07:28 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 1.9970
2022-07-11 04:08:01 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.1927
2022-07-11 04:08:34 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.0831
2022-07-11 04:09:08 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.1797
2022-07-11 04:09:41 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.1459
2022-07-11 04:10:14 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.0906
2022-07-11 04:10:47 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 1.8540
2022-07-11 04:11:20 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.0196
2022-07-11 04:11:54 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.0523
2022-07-11 04:12:27 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.1105
2022-07-11 04:13:00 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 1.8601
2022-07-11 04:13:33 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.0632
2022-07-11 04:14:06 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.3082
2022-07-11 04:14:40 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 1.8000
2022-07-11 04:15:13 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.0727
2022-07-11 04:15:46 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 1.9246
2022-07-11 04:16:19 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.0519
2022-07-11 04:16:52 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.0688
2022-07-11 04:17:25 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 1.9757
2022-07-11 04:17:58 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.0625
2022-07-11 04:18:31 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 1.9657
2022-07-11 04:19:04 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.0314
2022-07-11 04:19:38 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 1.9461
2022-07-11 04:20:11 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 1.9289
2022-07-11 04:20:44 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.1600
2022-07-11 04:21:17 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 1.9548
2022-07-11 04:21:51 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 1.9503
2022-07-11 04:22:24 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.3653
2022-07-11 04:22:58 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.0230
2022-07-11 04:23:31 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.1881
2022-07-11 04:24:05 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.0177
2022-07-11 04:24:37 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 1.7511
2022-07-11 04:24:39 - train: epoch 012, train_loss: 2.0598
2022-07-11 04:25:54 - eval: epoch: 012, acc1: 58.702%, acc5: 82.556%, test_loss: 1.7312, per_image_load_time: 2.117ms, per_image_inference_time: 0.576ms
2022-07-11 04:25:54 - until epoch: 012, best_acc1: 58.702%
2022-07-11 04:25:54 - epoch 013 lr: 0.100000
2022-07-11 04:26:34 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 1.8256
2022-07-11 04:27:07 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.0175
2022-07-11 04:27:40 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 1.9679
2022-07-11 04:28:12 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 1.8380
2022-07-11 04:28:45 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.0444
2022-07-11 04:29:17 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.1770
2022-07-11 04:29:50 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.0391
2022-07-11 04:30:23 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.0790
2022-07-11 04:30:56 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.0323
2022-07-11 04:31:29 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.0454
2022-07-11 04:32:02 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.1067
2022-07-11 04:32:36 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.1523
2022-07-11 04:33:09 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.0959
2022-07-11 04:33:42 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 1.9064
2022-07-11 04:34:15 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.0923
2022-07-11 04:34:48 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 1.6995
2022-07-11 04:35:21 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 1.9870
2022-07-11 04:35:55 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.0855
2022-07-11 04:36:28 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 1.9624
2022-07-11 04:37:00 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.2490
2022-07-11 04:37:34 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.3049
2022-07-11 04:38:07 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.0100
2022-07-11 04:38:40 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.0944
2022-07-11 04:39:13 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.0487
2022-07-11 04:39:46 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 1.9329
2022-07-11 04:40:19 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 1.9991
2022-07-11 04:40:52 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 1.8931
2022-07-11 04:41:25 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.0031
2022-07-11 04:41:58 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.2086
2022-07-11 04:42:31 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 1.9727
2022-07-11 04:43:04 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 1.9562
2022-07-11 04:43:37 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.1093
2022-07-11 04:44:10 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 1.8491
2022-07-11 04:44:43 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.0769
2022-07-11 04:45:16 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 1.8794
2022-07-11 04:45:49 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.4181
2022-07-11 04:46:22 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 1.8984
2022-07-11 04:46:55 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.2401
2022-07-11 04:47:28 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.1036
2022-07-11 04:48:01 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.1347
2022-07-11 04:48:35 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.0438
2022-07-11 04:49:08 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.0486
2022-07-11 04:49:41 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 1.9244
2022-07-11 04:50:14 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.0435
2022-07-11 04:50:47 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.0146
2022-07-11 04:51:21 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.1288
2022-07-11 04:51:54 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.0947
2022-07-11 04:52:27 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.0964
2022-07-11 04:53:00 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.0621
2022-07-11 04:53:33 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.2431
2022-07-11 04:53:35 - train: epoch 013, train_loss: 2.0385
2022-07-11 04:54:49 - eval: epoch: 013, acc1: 59.166%, acc5: 83.094%, test_loss: 1.7129, per_image_load_time: 1.277ms, per_image_inference_time: 0.591ms
2022-07-11 04:54:50 - until epoch: 013, best_acc1: 59.166%
2022-07-11 04:54:50 - epoch 014 lr: 0.100000
2022-07-11 04:55:29 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.0257
2022-07-11 04:56:02 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.1063
2022-07-11 04:56:35 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 1.7345
2022-07-11 04:57:08 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 1.8622
2022-07-11 04:57:41 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 1.9586
2022-07-11 04:58:14 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.0050
2022-07-11 04:58:47 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.0447
2022-07-11 04:59:20 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.0175
2022-07-11 04:59:53 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 1.9981
2022-07-11 05:00:26 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.1322
2022-07-11 05:00:59 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 1.9532
2022-07-11 05:01:32 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.0265
2022-07-11 05:02:05 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.0010
2022-07-11 05:02:38 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.0564
2022-07-11 05:03:11 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.1501
2022-07-11 05:03:44 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 1.9128
2022-07-11 05:04:17 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.2184
2022-07-11 05:04:51 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.2020
2022-07-11 05:05:24 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 1.8738
2022-07-11 05:05:57 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 1.8688
2022-07-11 05:06:30 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.1550
2022-07-11 05:07:04 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 1.9619
2022-07-11 05:07:37 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.0998
2022-07-11 05:08:10 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.0920
2022-07-11 05:08:44 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 1.8906
2022-07-11 05:09:17 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 1.9973
2022-07-11 05:09:51 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.0028
2022-07-11 05:10:24 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.2469
2022-07-11 05:10:57 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 1.9075
2022-07-11 05:11:30 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.1005
2022-07-11 05:12:03 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 1.7497
2022-07-11 05:12:36 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.0356
2022-07-11 05:13:10 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 1.8452
2022-07-11 05:13:43 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 1.9658
2022-07-11 05:14:16 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 1.8348
2022-07-11 05:14:49 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 1.9889
2022-07-11 05:15:22 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 1.9954
2022-07-11 05:15:55 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.1802
2022-07-11 05:16:28 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 1.9328
2022-07-11 05:17:02 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.1255
2022-07-11 05:17:35 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 1.8917
2022-07-11 05:18:08 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 1.9337
2022-07-11 05:18:41 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 1.8825
2022-07-11 05:19:15 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 1.8202
2022-07-11 05:19:47 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 1.9825
2022-07-11 05:20:21 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 1.9460
2022-07-11 05:20:54 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 1.8951
2022-07-11 05:21:28 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 1.9778
2022-07-11 05:22:01 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 1.7711
2022-07-11 05:22:34 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.0791
2022-07-11 05:22:35 - train: epoch 014, train_loss: 2.0191
2022-07-11 05:23:50 - eval: epoch: 014, acc1: 58.290%, acc5: 82.712%, test_loss: 1.7431, per_image_load_time: 2.092ms, per_image_inference_time: 0.597ms
2022-07-11 05:23:51 - until epoch: 014, best_acc1: 59.166%
2022-07-11 05:23:51 - epoch 015 lr: 0.100000
2022-07-11 05:24:30 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 1.7793
2022-07-11 05:25:03 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.1948
2022-07-11 05:25:36 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.1808
2022-07-11 05:26:09 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.0041
2022-07-11 05:26:42 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 1.9566
2022-07-11 05:27:15 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.1026
2022-07-11 05:27:48 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 1.9282
2022-07-11 05:28:21 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 1.7945
2022-07-11 05:28:55 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 1.9916
2022-07-11 05:29:28 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.0190
2022-07-11 05:30:01 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 1.8243
2022-07-11 05:30:34 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.0424
2022-07-11 05:31:08 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.3362
2022-07-11 05:31:41 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 1.9618
2022-07-11 05:32:14 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 1.6626
2022-07-11 05:32:47 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 1.9890
2022-07-11 05:33:20 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.1338
2022-07-11 05:33:53 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 1.9389
2022-07-11 05:34:25 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 1.7863
2022-07-11 05:34:58 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 1.9016
2022-07-11 05:35:31 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 1.9089
2022-07-11 05:36:04 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.2103
2022-07-11 05:36:37 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 1.9008
2022-07-11 05:37:10 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.0936
2022-07-11 05:37:44 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.1494
2022-07-11 05:38:17 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 1.7256
2022-07-11 05:38:50 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.0679
2022-07-11 05:39:23 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.1187
2022-07-11 05:39:56 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 1.9873
2022-07-11 05:40:29 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 1.8126
2022-07-11 05:41:02 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 1.9768
2022-07-11 05:41:36 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 1.8883
2022-07-11 05:42:09 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 1.7173
2022-07-11 05:42:42 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.1250
2022-07-11 05:43:16 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.1646
2022-07-11 05:43:49 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.1525
2022-07-11 05:44:22 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 1.9734
2022-07-11 05:44:55 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.0093
2022-07-11 05:45:28 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.1074
2022-07-11 05:46:02 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.0694
2022-07-11 05:46:35 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.2748
2022-07-11 05:47:08 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 1.7780
2022-07-11 05:47:41 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 1.9759
2022-07-11 05:48:15 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.0581
2022-07-11 05:48:48 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.0299
2022-07-11 05:49:21 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 1.9614
2022-07-11 05:49:55 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.0458
2022-07-11 05:50:28 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 1.9261
2022-07-11 05:51:01 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 1.9565
2022-07-11 05:51:33 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.0644
2022-07-11 05:51:35 - train: epoch 015, train_loss: 2.0039
2022-07-11 05:52:50 - eval: epoch: 015, acc1: 59.076%, acc5: 82.712%, test_loss: 1.7178, per_image_load_time: 1.009ms, per_image_inference_time: 0.591ms
2022-07-11 05:52:50 - until epoch: 015, best_acc1: 59.166%
2022-07-11 05:52:50 - epoch 016 lr: 0.100000
2022-07-11 05:53:30 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 1.9843
2022-07-11 05:54:03 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 1.7270
2022-07-11 05:54:36 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 1.9979
2022-07-11 05:55:09 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.1819
2022-07-11 05:55:42 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 1.8475
2022-07-11 05:56:15 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.0198
2022-07-11 05:56:48 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 1.7382
2022-07-11 05:57:21 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 1.9973
2022-07-11 05:57:53 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.0472
2022-07-11 05:58:26 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 1.8510
2022-07-11 05:58:59 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 1.8695
2022-07-11 05:59:32 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 1.8913
2022-07-11 06:00:05 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.0730
2022-07-11 06:00:38 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 1.9197
2022-07-11 06:01:11 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.0570
2022-07-11 06:01:44 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.1452
2022-07-11 06:02:17 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 1.9428
2022-07-11 06:02:50 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 1.9186
2022-07-11 06:03:22 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 1.9345
2022-07-11 06:03:55 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 1.6652
2022-07-11 06:04:28 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.1669
2022-07-11 06:05:01 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.0969
2022-07-11 06:05:34 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.1944
2022-07-11 06:06:07 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.0353
2022-07-11 06:06:40 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 1.8817
2022-07-11 06:07:13 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.1406
2022-07-11 06:07:45 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 1.9190
2022-07-11 06:08:18 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 1.8861
2022-07-11 06:08:51 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.0299
2022-07-11 06:09:24 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.2307
2022-07-11 06:09:57 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.1700
2022-07-11 06:10:29 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.1271
2022-07-11 06:11:03 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.0208
2022-07-11 06:11:36 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 1.9106
2022-07-11 06:12:09 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.0847
2022-07-11 06:12:42 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 1.9632
2022-07-11 06:13:16 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.0190
2022-07-11 06:13:49 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.2557
2022-07-11 06:14:22 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.0531
2022-07-11 06:14:55 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.0583
2022-07-11 06:15:28 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 1.9106
2022-07-11 06:16:01 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 1.9934
2022-07-11 06:16:35 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 1.8701
2022-07-11 06:17:08 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 1.8650
2022-07-11 06:17:41 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.0511
2022-07-11 06:18:14 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 1.9133
2022-07-11 06:18:47 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.2166
2022-07-11 06:19:20 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 1.9522
2022-07-11 06:19:53 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.0257
2022-07-11 06:20:26 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.0110
2022-07-11 06:20:27 - train: epoch 016, train_loss: 1.9896
2022-07-11 06:21:42 - eval: epoch: 016, acc1: 59.418%, acc5: 83.182%, test_loss: 1.6999, per_image_load_time: 1.909ms, per_image_inference_time: 0.593ms
2022-07-11 06:21:43 - until epoch: 016, best_acc1: 59.418%
2022-07-11 06:21:43 - epoch 017 lr: 0.100000
2022-07-11 06:22:22 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 1.8271
2022-07-11 06:22:55 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 1.9737
2022-07-11 06:23:28 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.2683
2022-07-11 06:24:01 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 1.7612
2022-07-11 06:24:34 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 1.9750
2022-07-11 06:25:08 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.2996
2022-07-11 06:25:41 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.0151
2022-07-11 06:26:14 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 1.9672
2022-07-11 06:26:48 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 1.9761
2022-07-11 06:27:21 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 1.7728
2022-07-11 06:27:54 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.2279
2022-07-11 06:28:27 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.1486
2022-07-11 06:29:01 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 1.9866
2022-07-11 06:29:34 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.0552
2022-07-11 06:30:07 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 1.5927
2022-07-11 06:30:40 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 1.8372
2022-07-11 06:31:14 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 1.8875
2022-07-11 06:31:47 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 1.9484
2022-07-11 06:32:20 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 1.9155
2022-07-11 06:32:54 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.1592
2022-07-11 06:33:27 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.0285
2022-07-11 06:34:00 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 1.8717
2022-07-11 06:34:33 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 1.8696
2022-07-11 06:35:06 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 1.9597
2022-07-11 06:35:39 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.2674
2022-07-11 06:36:13 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 1.8645
2022-07-11 06:36:46 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 1.9543
2022-07-11 06:37:19 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.2853
2022-07-11 06:37:52 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.2887
2022-07-11 06:38:25 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 1.8710
2022-07-11 06:38:58 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.0570
2022-07-11 06:39:31 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 1.8422
2022-07-11 06:40:04 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.0879
2022-07-11 06:40:37 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 1.7027
2022-07-11 06:41:10 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 1.9285
2022-07-11 06:41:43 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.2097
2022-07-11 06:42:17 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 1.8776
2022-07-11 06:42:50 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.0581
2022-07-11 06:43:23 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 1.7946
2022-07-11 06:43:57 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 1.9528
2022-07-11 06:44:30 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.0572
2022-07-11 06:45:03 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 1.9310
2022-07-11 06:45:36 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.0704
2022-07-11 06:46:10 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 1.9221
2022-07-11 06:46:43 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.0780
2022-07-11 06:47:17 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.0663
2022-07-11 06:47:50 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.1550
2022-07-11 06:48:24 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.1913
2022-07-11 06:48:57 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 1.8380
2022-07-11 06:49:30 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 1.7735
2022-07-11 06:49:31 - train: epoch 017, train_loss: 1.9782
2022-07-11 06:50:46 - eval: epoch: 017, acc1: 60.526%, acc5: 84.012%, test_loss: 1.6540, per_image_load_time: 1.680ms, per_image_inference_time: 0.612ms
2022-07-11 06:50:47 - until epoch: 017, best_acc1: 60.526%
2022-07-11 06:50:47 - epoch 018 lr: 0.100000
2022-07-11 06:51:25 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 1.9009
2022-07-11 06:51:58 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 1.9082
2022-07-11 06:52:32 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.1164
2022-07-11 06:53:05 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.1679
2022-07-11 06:53:38 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 1.9401
2022-07-11 06:54:11 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.0240
2022-07-11 06:54:44 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 1.7231
2022-07-11 06:55:17 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 1.9770
2022-07-11 06:55:50 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.0030
2022-07-11 06:56:23 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 1.8211
2022-07-11 06:56:56 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.1113
2022-07-11 06:57:30 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 1.9703
2022-07-11 06:58:03 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.3519
2022-07-11 06:58:37 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 1.8607
2022-07-11 06:59:10 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.0797
2022-07-11 06:59:43 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 1.9973
2022-07-11 07:00:16 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 1.9254
2022-07-11 07:00:50 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 1.7780
2022-07-11 07:01:23 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 1.9839
2022-07-11 07:01:56 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.1981
2022-07-11 07:02:30 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.1691
2022-07-11 07:03:03 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 1.9334
2022-07-11 07:03:36 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.0197
2022-07-11 07:04:10 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 1.8300
2022-07-11 07:04:43 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 1.6829
2022-07-11 07:05:16 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 1.8271
2022-07-11 07:05:49 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.1390
2022-07-11 07:06:22 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 1.7166
2022-07-11 07:06:56 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 1.9764
2022-07-11 07:07:29 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 1.9273
2022-07-11 07:08:02 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.3601
2022-07-11 07:08:35 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 1.8852
2022-07-11 07:09:09 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 1.9044
2022-07-11 07:09:42 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 1.9233
2022-07-11 07:10:16 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.1858
2022-07-11 07:10:49 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.0055
2022-07-11 07:11:22 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.2589
2022-07-11 07:11:55 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.0541
2022-07-11 07:12:29 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.0556
2022-07-11 07:13:02 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 1.8458
2022-07-11 07:13:35 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 1.9834
2022-07-11 07:14:08 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.0274
2022-07-11 07:14:41 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 1.8932
2022-07-11 07:15:14 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.1704
2022-07-11 07:15:47 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 1.8153
2022-07-11 07:16:21 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 1.8132
2022-07-11 07:16:54 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.2363
2022-07-11 07:17:27 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.0601
2022-07-11 07:18:01 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 1.9450
2022-07-11 07:18:33 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.0843
2022-07-11 07:18:35 - train: epoch 018, train_loss: 1.9674
2022-07-11 07:19:49 - eval: epoch: 018, acc1: 59.688%, acc5: 83.276%, test_loss: 1.6874, per_image_load_time: 1.697ms, per_image_inference_time: 0.596ms
2022-07-11 07:19:50 - until epoch: 018, best_acc1: 60.526%
2022-07-11 07:19:50 - epoch 019 lr: 0.100000
2022-07-11 07:20:29 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 1.7107
2022-07-11 07:21:02 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.0927
2022-07-11 07:21:35 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.1605
2022-07-11 07:22:07 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 1.7784
2022-07-11 07:22:40 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.0102
2022-07-11 07:23:13 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 1.8090
2022-07-11 07:23:45 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 1.8136
2022-07-11 07:24:18 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.0560
2022-07-11 07:24:51 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 1.9737
2022-07-11 07:25:24 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 1.9678
2022-07-11 07:25:56 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 1.7969
2022-07-11 07:26:29 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 1.9787
2022-07-11 07:27:02 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.1273
2022-07-11 07:27:35 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 1.9232
2022-07-11 07:28:08 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.2576
2022-07-11 07:28:41 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 1.9275
2022-07-11 07:29:14 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.0136
2022-07-11 07:29:47 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 1.9670
2022-07-11 07:30:20 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.0850
2022-07-11 07:30:53 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 1.8783
2022-07-11 07:31:26 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 1.7732
2022-07-11 07:31:59 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 1.9725
2022-07-11 07:32:32 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 1.9870
2022-07-11 07:33:05 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.0371
2022-07-11 07:33:38 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 1.8172
2022-07-11 07:34:11 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.0159
2022-07-11 07:34:44 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.0501
2022-07-11 07:35:17 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.1150
2022-07-11 07:35:51 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 1.9685
2022-07-11 07:36:24 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.3008
2022-07-11 07:36:57 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.1506
2022-07-11 07:37:30 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.7324
2022-07-11 07:38:03 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 1.8997
2022-07-11 07:38:36 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.0524
2022-07-11 07:39:09 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.0221
2022-07-11 07:39:42 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 1.7738
2022-07-11 07:40:16 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 1.9726
2022-07-11 07:40:49 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.0605
2022-07-11 07:41:22 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 1.8451
2022-07-11 07:41:55 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 1.8064
2022-07-11 07:42:28 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.0015
2022-07-11 07:43:02 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 1.9277
2022-07-11 07:43:35 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 1.8121
2022-07-11 07:44:08 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.1979
2022-07-11 07:44:41 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.2595
2022-07-11 07:45:14 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 1.8765
2022-07-11 07:45:47 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 1.8342
2022-07-11 07:46:21 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 1.9613
2022-07-11 07:46:54 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.0946
2022-07-11 07:47:27 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 1.9206
2022-07-11 07:47:28 - train: epoch 019, train_loss: 1.9617
2022-07-11 07:48:44 - eval: epoch: 019, acc1: 61.048%, acc5: 83.958%, test_loss: 1.6294, per_image_load_time: 2.003ms, per_image_inference_time: 0.592ms
2022-07-11 07:48:44 - until epoch: 019, best_acc1: 61.048%
2022-07-11 07:48:44 - epoch 020 lr: 0.100000
2022-07-11 07:49:23 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.0410
2022-07-11 07:49:55 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 1.7008
2022-07-11 07:50:28 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 1.9541
2022-07-11 07:51:01 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 1.7042
2022-07-11 07:51:34 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 1.8291
2022-07-11 07:52:07 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.1119
2022-07-11 07:52:40 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 1.7270
2022-07-11 07:53:13 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.0571
2022-07-11 07:53:46 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.3124
2022-07-11 07:54:19 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.0737
2022-07-11 07:54:52 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 1.8508
2022-07-11 07:55:25 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 1.6220
2022-07-11 07:55:58 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 1.7879
2022-07-11 07:56:32 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.0900
2022-07-11 07:57:05 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.0926
2022-07-11 07:57:38 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 1.8097
2022-07-11 07:58:11 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 1.7374
2022-07-11 07:58:44 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 1.9877
2022-07-11 07:59:17 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 1.7408
2022-07-11 07:59:51 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 1.9788
2022-07-11 08:00:24 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.0802
2022-07-11 08:00:57 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 1.7762
2022-07-11 08:01:30 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 1.9414
2022-07-11 08:02:03 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.1426
2022-07-11 08:02:36 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 1.9461
2022-07-11 08:03:09 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 1.8114
2022-07-11 08:03:42 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.0927
2022-07-11 08:04:16 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 1.8700
2022-07-11 08:04:49 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.1242
2022-07-11 08:05:22 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 1.9545
2022-07-11 08:05:55 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.0650
2022-07-11 08:06:28 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.1640
2022-07-11 08:07:02 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 1.7661
2022-07-11 08:07:35 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 1.9425
2022-07-11 08:08:08 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 1.9124
2022-07-11 08:08:41 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 1.9808
2022-07-11 08:09:14 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 1.9016
2022-07-11 08:09:47 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 1.8990
2022-07-11 08:10:20 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.0681
2022-07-11 08:10:52 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 1.8052
2022-07-11 08:11:26 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 1.8891
2022-07-11 08:11:59 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 1.9352
2022-07-11 08:12:32 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 1.9033
2022-07-11 08:13:05 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 1.9234
2022-07-11 08:13:38 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.0468
2022-07-11 08:14:12 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.0426
2022-07-11 08:14:45 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 1.8740
2022-07-11 08:15:18 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 1.9933
2022-07-11 08:15:51 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.0177
2022-07-11 08:16:23 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 1.8385
2022-07-11 08:16:25 - train: epoch 020, train_loss: 1.9480
2022-07-11 08:17:40 - eval: epoch: 020, acc1: 60.612%, acc5: 83.838%, test_loss: 1.6408, per_image_load_time: 1.871ms, per_image_inference_time: 0.598ms
2022-07-11 08:17:40 - until epoch: 020, best_acc1: 61.048%
2022-07-11 08:17:40 - epoch 021 lr: 0.100000
2022-07-11 08:18:20 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 1.8051
2022-07-11 08:18:53 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 1.9851
2022-07-11 08:19:25 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.6653
2022-07-11 08:19:57 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.0192
2022-07-11 08:20:30 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 1.8686
2022-07-11 08:21:02 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 1.7728
2022-07-11 08:21:35 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 1.7292
2022-07-11 08:22:07 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.1002
2022-07-11 08:22:40 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 1.9095
2022-07-11 08:23:13 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 1.8351
2022-07-11 08:23:46 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 1.7548
2022-07-11 08:24:19 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 1.8483
2022-07-11 08:24:51 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 1.7821
2022-07-11 08:25:24 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 1.7866
2022-07-11 08:25:57 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 1.7305
2022-07-11 08:26:30 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.0471
2022-07-11 08:27:03 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 1.8915
2022-07-11 08:27:36 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 1.8697
2022-07-11 08:28:09 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.0052
2022-07-11 08:28:42 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.1842
2022-07-11 08:29:15 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 1.7656
2022-07-11 08:29:48 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 1.8495
2022-07-11 08:30:21 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 1.7146
2022-07-11 08:30:54 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 1.8731
2022-07-11 08:31:27 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 1.9721
2022-07-11 08:32:00 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.2675
2022-07-11 08:32:33 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 1.8388
2022-07-11 08:33:07 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 1.9217
2022-07-11 08:33:40 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 1.7916
2022-07-11 08:34:13 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.1586
2022-07-11 08:34:46 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 1.9621
2022-07-11 08:35:19 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 1.8898
2022-07-11 08:35:52 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.2928
2022-07-11 08:36:25 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.1218
2022-07-11 08:36:58 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 1.8096
2022-07-11 08:37:31 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 1.8681
2022-07-11 08:38:05 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 1.9308
2022-07-11 08:38:38 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 1.8077
2022-07-11 08:39:11 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 1.8849
2022-07-11 08:39:44 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.1779
2022-07-11 08:40:18 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 1.9341
2022-07-11 08:40:51 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 1.8625
2022-07-11 08:41:24 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 1.9965
2022-07-11 08:41:57 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.0190
2022-07-11 08:42:30 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.1026
2022-07-11 08:43:03 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 1.8303
2022-07-11 08:43:36 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.1193
2022-07-11 08:44:09 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.1145
2022-07-11 08:44:43 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 1.7148
2022-07-11 08:45:15 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 1.9420
2022-07-11 08:45:17 - train: epoch 021, train_loss: 1.9430
2022-07-11 08:46:31 - eval: epoch: 021, acc1: 60.292%, acc5: 83.890%, test_loss: 1.6585, per_image_load_time: 1.658ms, per_image_inference_time: 0.612ms
2022-07-11 08:46:32 - until epoch: 021, best_acc1: 61.048%
2022-07-11 08:46:32 - epoch 022 lr: 0.100000
2022-07-11 08:47:11 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 1.7644
2022-07-11 08:47:44 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 1.7329
2022-07-11 08:48:17 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 1.5944
2022-07-11 08:48:50 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 1.8681
2022-07-11 08:49:23 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 1.9393
2022-07-11 08:49:55 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.0699
2022-07-11 08:50:28 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 1.9925
2022-07-11 08:51:01 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.0135
2022-07-11 08:51:34 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.1031
2022-07-11 08:52:07 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 1.9715
2022-07-11 08:52:40 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 1.9683
2022-07-11 08:53:13 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.6646
2022-07-11 08:53:46 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 1.8842
2022-07-11 08:54:19 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 1.9594
2022-07-11 08:54:52 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 1.8598
2022-07-11 08:55:26 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 1.7511
2022-07-11 08:55:59 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 1.7149
2022-07-11 08:56:32 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.1075
2022-07-11 08:57:05 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 1.7149
2022-07-11 08:57:38 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 1.9987
2022-07-11 08:58:11 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.0571
2022-07-11 08:58:45 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 1.7825
2022-07-11 08:59:18 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.1536
2022-07-11 08:59:51 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.0489
2022-07-11 09:00:24 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 1.9267
2022-07-11 09:00:58 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 1.6860
2022-07-11 09:01:31 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 1.7702
2022-07-11 09:02:05 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.1896
2022-07-11 09:02:38 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 1.8193
2022-07-11 09:03:11 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.0512
2022-07-11 09:03:44 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.0234
2022-07-11 09:04:17 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.0835
2022-07-11 09:04:50 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 1.9232
2022-07-11 09:05:24 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 1.6960
2022-07-11 09:05:57 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.0513
2022-07-11 09:06:30 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 1.9614
2022-07-11 09:07:03 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.1087
2022-07-11 09:07:37 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.1101
2022-07-11 09:08:10 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 1.8535
2022-07-11 09:08:43 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.0764
2022-07-11 09:09:16 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 1.8083
2022-07-11 09:09:50 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 1.9002
2022-07-11 09:10:23 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.1898
2022-07-11 09:10:56 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 1.8888
2022-07-11 09:11:30 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 1.9249
2022-07-11 09:12:03 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.1539
2022-07-11 09:12:36 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.0350
2022-07-11 09:13:09 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 1.7344
2022-07-11 09:13:42 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 1.7705
2022-07-11 09:14:14 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 1.8525
2022-07-11 09:14:16 - train: epoch 022, train_loss: 1.9368
2022-07-11 09:15:30 - eval: epoch: 022, acc1: 61.098%, acc5: 84.208%, test_loss: 1.6324, per_image_load_time: 1.975ms, per_image_inference_time: 0.600ms
2022-07-11 09:15:31 - until epoch: 022, best_acc1: 61.098%
2022-07-11 09:15:31 - epoch 023 lr: 0.100000
2022-07-11 09:16:10 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 1.7259
2022-07-11 09:16:43 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 1.5686
2022-07-11 09:17:16 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 1.7685
2022-07-11 09:17:49 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.0082
2022-07-11 09:18:21 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 1.9039
2022-07-11 09:18:54 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 1.8707
2022-07-11 09:19:27 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 1.7277
2022-07-11 09:20:00 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 1.8102
2022-07-11 09:20:33 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 1.8941
2022-07-11 09:21:06 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 1.9235
2022-07-11 09:21:39 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.0318
2022-07-11 09:22:12 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 1.7980
2022-07-11 09:22:46 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 1.9306
2022-07-11 09:23:19 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 1.9360
2022-07-11 09:23:51 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 1.7105
2022-07-11 09:24:25 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 1.9904
2022-07-11 09:24:58 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.0481
2022-07-11 09:25:31 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 1.8382
2022-07-11 09:26:04 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.0224
2022-07-11 09:26:37 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.5420
2022-07-11 09:27:10 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.0889
2022-07-11 09:27:43 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.6825
2022-07-11 09:28:16 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 1.7923
2022-07-11 09:28:49 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 1.9368
2022-07-11 09:29:22 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 1.9649
2022-07-11 09:29:56 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 1.9254
2022-07-11 09:30:29 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 1.7956
2022-07-11 09:31:02 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 1.9087
2022-07-11 09:31:35 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.0643
2022-07-11 09:32:09 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.1705
2022-07-11 09:32:42 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.0515
2022-07-11 09:33:15 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.1101
2022-07-11 09:33:48 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.0274
2022-07-11 09:34:21 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 1.9229
2022-07-11 09:34:55 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 1.8223
2022-07-11 09:35:28 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 1.6821
2022-07-11 09:36:01 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 1.9249
2022-07-11 09:36:34 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 1.9664
2022-07-11 09:37:07 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 1.9301
2022-07-11 09:37:40 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 1.8035
2022-07-11 09:38:13 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 1.7786
2022-07-11 09:38:46 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 1.8205
2022-07-11 09:39:19 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 1.5737
2022-07-11 09:39:51 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 1.7207
2022-07-11 09:40:25 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 1.8293
2022-07-11 09:40:58 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.0868
2022-07-11 09:41:31 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 1.7367
2022-07-11 09:42:04 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 1.8536
2022-07-11 09:42:37 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 1.9421
2022-07-11 09:43:10 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.0645
2022-07-11 09:43:11 - train: epoch 023, train_loss: 1.9295
2022-07-11 09:44:27 - eval: epoch: 023, acc1: 60.702%, acc5: 84.008%, test_loss: 1.6565, per_image_load_time: 1.867ms, per_image_inference_time: 0.592ms
2022-07-11 09:44:27 - until epoch: 023, best_acc1: 61.098%
2022-07-11 09:44:27 - epoch 024 lr: 0.100000
2022-07-11 09:45:06 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 1.8033
2022-07-11 09:45:39 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 1.9843
2022-07-11 09:46:12 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 1.9153
2022-07-11 09:46:45 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 1.9989
2022-07-11 09:47:18 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 1.9250
2022-07-11 09:47:51 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 1.8043
2022-07-11 09:48:24 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 1.7627
2022-07-11 09:48:57 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 1.6991
2022-07-11 09:49:30 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 1.8525
2022-07-11 09:50:03 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 1.9220
2022-07-11 09:50:36 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.5420
2022-07-11 09:51:09 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 1.8199
2022-07-11 09:51:42 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.1137
2022-07-11 09:52:15 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 1.8702
2022-07-11 09:52:48 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.1036
2022-07-11 09:53:20 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.0541
2022-07-11 09:53:54 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 1.9486
2022-07-11 09:54:27 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.1505
2022-07-11 09:55:00 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 1.6729
2022-07-11 09:55:33 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 1.9948
2022-07-11 09:56:06 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 1.8774
2022-07-11 09:56:40 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 1.7622
2022-07-11 09:57:13 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 1.9455
2022-07-11 09:57:46 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 1.9129
2022-07-11 09:58:19 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 1.7871
2022-07-11 09:58:52 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 1.9790
2022-07-11 09:59:25 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.0569
2022-07-11 09:59:58 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.0726
2022-07-11 10:00:31 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 1.9921
2022-07-11 10:01:04 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 1.7912
2022-07-11 10:01:38 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 1.8849
2022-07-11 10:02:11 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.0149
2022-07-11 10:02:44 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.6020
2022-07-11 10:03:18 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 1.8773
2022-07-11 10:03:51 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 1.8140
2022-07-11 10:04:24 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.0213
2022-07-11 10:04:57 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 1.8230
2022-07-11 10:05:31 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.1509
2022-07-11 10:06:04 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 1.9977
2022-07-11 10:06:37 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 1.9784
2022-07-11 10:07:11 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 1.8919
2022-07-11 10:07:44 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 1.9396
2022-07-11 10:08:17 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 1.8451
2022-07-11 10:08:50 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.0519
2022-07-11 10:09:24 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 1.8414
2022-07-11 10:09:57 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 1.9534
2022-07-11 10:10:30 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 1.8874
2022-07-11 10:11:03 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 1.9001
2022-07-11 10:11:36 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.0336
2022-07-11 10:12:09 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 1.9507
2022-07-11 10:12:10 - train: epoch 024, train_loss: 1.9255
2022-07-11 10:13:25 - eval: epoch: 024, acc1: 60.678%, acc5: 84.128%, test_loss: 1.6422, per_image_load_time: 1.655ms, per_image_inference_time: 0.589ms
2022-07-11 10:13:25 - until epoch: 024, best_acc1: 61.098%
2022-07-11 10:13:25 - epoch 025 lr: 0.100000
2022-07-11 10:14:05 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 1.7888
2022-07-11 10:14:37 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 1.7223
2022-07-11 10:15:10 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 1.7982
2022-07-11 10:15:42 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 1.7921
2022-07-11 10:16:15 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 1.8028
2022-07-11 10:16:47 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.0025
2022-07-11 10:17:19 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 1.9409
2022-07-11 10:17:52 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 1.9063
2022-07-11 10:18:25 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 1.7684
2022-07-11 10:18:58 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 1.9675
2022-07-11 10:19:32 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 1.8967
2022-07-11 10:20:04 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 1.9563
2022-07-11 10:20:37 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 1.8766
2022-07-11 10:21:10 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 1.9082
2022-07-11 10:21:43 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 1.9204
2022-07-11 10:22:16 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 1.6644
2022-07-11 10:22:49 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 1.7853
2022-07-11 10:23:22 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 1.6455
2022-07-11 10:23:55 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 1.8099
2022-07-11 10:24:29 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 1.8841
2022-07-11 10:25:02 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 1.7638
2022-07-11 10:25:35 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 1.8795
2022-07-11 10:26:08 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 1.8976
2022-07-11 10:26:41 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 1.7743
2022-07-11 10:27:14 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 1.9072
2022-07-11 10:27:48 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.0643
2022-07-11 10:28:21 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.0070
2022-07-11 10:28:53 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 1.8851
2022-07-11 10:29:26 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.0597
2022-07-11 10:29:59 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.1274
2022-07-11 10:30:32 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 1.7674
2022-07-11 10:31:05 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.1555
2022-07-11 10:31:38 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 1.8067
2022-07-11 10:32:11 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.1367
2022-07-11 10:32:44 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 1.6745
2022-07-11 10:33:17 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 1.9950
2022-07-11 10:33:50 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 1.9443
2022-07-11 10:34:23 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 1.9967
2022-07-11 10:34:56 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.0847
2022-07-11 10:35:29 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.0644
2022-07-11 10:36:02 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.2584
2022-07-11 10:36:35 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 1.8631
2022-07-11 10:37:08 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 1.8104
2022-07-11 10:37:41 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 1.8248
2022-07-11 10:38:14 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 1.8553
2022-07-11 10:38:47 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.0155
2022-07-11 10:39:20 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 1.8648
2022-07-11 10:39:53 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 1.6422
2022-07-11 10:40:26 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 1.9527
2022-07-11 10:40:58 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.1120
2022-07-11 10:41:00 - train: epoch 025, train_loss: 1.9183
2022-07-11 10:42:14 - eval: epoch: 025, acc1: 61.628%, acc5: 84.596%, test_loss: 1.6008, per_image_load_time: 2.304ms, per_image_inference_time: 0.588ms
2022-07-11 10:42:15 - until epoch: 025, best_acc1: 61.628%
2022-07-11 10:42:15 - epoch 026 lr: 0.100000
2022-07-11 10:42:55 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 1.6992
2022-07-11 10:43:27 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.6958
2022-07-11 10:43:59 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 1.9366
2022-07-11 10:44:32 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 1.9436
2022-07-11 10:45:04 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 1.9634
2022-07-11 10:45:37 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.0319
2022-07-11 10:46:10 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 1.8141
2022-07-11 10:46:43 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 1.6646
2022-07-11 10:47:16 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.2210
2022-07-11 10:47:48 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 1.7493
2022-07-11 10:48:21 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 1.7702
2022-07-11 10:48:54 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 1.8279
2022-07-11 10:49:27 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 1.7573
2022-07-11 10:50:00 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 1.9616
2022-07-11 10:50:32 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 1.8724
2022-07-11 10:51:05 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.0350
2022-07-11 10:51:38 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 1.8150
2022-07-11 10:52:11 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 1.9856
2022-07-11 10:52:44 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.2206
2022-07-11 10:53:17 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.0082
2022-07-11 10:53:50 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 1.9228
2022-07-11 10:54:23 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 1.8033
2022-07-11 10:54:56 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 1.8486
2022-07-11 10:55:29 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.2268
2022-07-11 10:56:01 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.0603
2022-07-11 10:56:35 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 1.9483
2022-07-11 10:57:07 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 1.8351
2022-07-11 10:57:40 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 1.8464
2022-07-11 10:58:13 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 1.9800
2022-07-11 10:58:46 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 1.8677
2022-07-11 10:59:19 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 1.9424
2022-07-11 10:59:52 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 1.9328
2022-07-11 11:00:25 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 1.8881
2022-07-11 11:00:57 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.0866
2022-07-11 11:01:30 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.1350
2022-07-11 11:02:02 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 1.7468
2022-07-11 11:02:35 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.0542
2022-07-11 11:03:08 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.1167
2022-07-11 11:03:40 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.0949
2022-07-11 11:04:13 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.1963
2022-07-11 11:04:46 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 1.8859
2022-07-11 11:05:18 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.0665
2022-07-11 11:05:51 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.0157
2022-07-11 11:06:24 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.0450
2022-07-11 11:06:57 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.0744
2022-07-11 11:07:29 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 1.8764
2022-07-11 11:08:03 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 1.8257
2022-07-11 11:08:35 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 1.9219
2022-07-11 11:09:08 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 1.8537
2022-07-11 11:09:40 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.1986
2022-07-11 11:09:42 - train: epoch 026, train_loss: 1.9140
2022-07-11 11:10:57 - eval: epoch: 026, acc1: 61.628%, acc5: 84.830%, test_loss: 1.5888, per_image_load_time: 2.105ms, per_image_inference_time: 0.577ms
2022-07-11 11:10:57 - until epoch: 026, best_acc1: 61.628%
2022-07-11 11:10:57 - epoch 027 lr: 0.100000
2022-07-11 11:11:37 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 1.9503
2022-07-11 11:12:09 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 1.9466
2022-07-11 11:12:41 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 1.8947
2022-07-11 11:13:13 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.0566
2022-07-11 11:13:45 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 1.9378
2022-07-11 11:14:17 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.1336
2022-07-11 11:14:49 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.1866
2022-07-11 11:15:21 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 1.9484
2022-07-11 11:15:52 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 1.8855
2022-07-11 11:16:24 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 1.9759
2022-07-11 11:16:57 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 1.7985
2022-07-11 11:17:28 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.1029
2022-07-11 11:18:00 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 1.8521
2022-07-11 11:18:32 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.1216
2022-07-11 11:19:03 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.0694
2022-07-11 11:19:35 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 1.9481
2022-07-11 11:20:07 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 1.9401
2022-07-11 11:20:39 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 1.9044
2022-07-11 11:21:11 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 1.9856
2022-07-11 11:21:43 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 1.8557
2022-07-11 11:22:14 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.1955
2022-07-11 11:22:47 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.1638
2022-07-11 11:23:19 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.1191
2022-07-11 11:23:51 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.0067
2022-07-11 11:24:24 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 1.6750
2022-07-11 11:24:56 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.1706
2022-07-11 11:25:29 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 1.9073
2022-07-11 11:26:01 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.1081
2022-07-11 11:26:34 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 1.9682
2022-07-11 11:27:07 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 1.8005
2022-07-11 11:27:39 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 1.9406
2022-07-11 11:28:12 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 1.8050
2022-07-11 11:28:44 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.0008
2022-07-11 11:29:17 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 1.8766
2022-07-11 11:29:50 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.2204
2022-07-11 11:30:22 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 1.6429
2022-07-11 11:30:55 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 1.8428
2022-07-11 11:31:28 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 1.6970
2022-07-11 11:32:00 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 1.7659
2022-07-11 11:32:33 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 1.9467
2022-07-11 11:33:06 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 1.8293
2022-07-11 11:33:39 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 1.9697
2022-07-11 11:34:11 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 1.8809
2022-07-11 11:34:44 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.0140
2022-07-11 11:35:16 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 1.7088
2022-07-11 11:35:49 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 1.9841
2022-07-11 11:36:21 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 1.9247
2022-07-11 11:36:54 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.0519
2022-07-11 11:37:27 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 1.9776
2022-07-11 11:37:59 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.6448
2022-07-11 11:38:00 - train: epoch 027, train_loss: 1.9096
2022-07-11 11:39:16 - eval: epoch: 027, acc1: 61.928%, acc5: 84.786%, test_loss: 1.5878, per_image_load_time: 2.310ms, per_image_inference_time: 0.577ms
2022-07-11 11:39:16 - until epoch: 027, best_acc1: 61.928%
2022-07-11 11:39:16 - epoch 028 lr: 0.100000
2022-07-11 11:39:56 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 1.6287
2022-07-11 11:40:28 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 1.7714
2022-07-11 11:40:59 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 1.7651
2022-07-11 11:41:32 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 1.7889
2022-07-11 11:42:03 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 1.7500
2022-07-11 11:42:35 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.0295
2022-07-11 11:43:07 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.1135
2022-07-11 11:43:39 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.5207
2022-07-11 11:44:11 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 1.8274
2022-07-11 11:44:43 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 1.9524
2022-07-11 11:45:15 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.0506
2022-07-11 11:45:46 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 1.7468
2022-07-11 11:46:18 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 1.8390
2022-07-11 11:46:50 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.1218
2022-07-11 11:47:22 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 1.8632
2022-07-11 11:47:54 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 1.8554
2022-07-11 11:48:25 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 1.8413
2022-07-11 11:48:57 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 1.8034
2022-07-11 11:49:29 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 1.8454
2022-07-11 11:50:01 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 1.9751
2022-07-11 11:50:32 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 1.9930
2022-07-11 11:51:04 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.0425
2022-07-11 11:51:36 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.1238
2022-07-11 11:52:08 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.0537
2022-07-11 11:52:40 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.0396
2022-07-11 11:53:12 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 1.7835
2022-07-11 11:53:44 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 1.9957
2022-07-11 11:54:16 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 1.9006
2022-07-11 11:54:49 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 1.9075
2022-07-11 11:55:21 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 1.9337
2022-07-11 11:55:53 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.1048
2022-07-11 11:56:25 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 1.8937
2022-07-11 11:56:56 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 1.9846
2022-07-11 11:57:29 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 1.9365
2022-07-11 11:58:00 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 1.9093
2022-07-11 11:58:32 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 1.7666
2022-07-11 11:59:04 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 1.7833
2022-07-11 11:59:36 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.6659
2022-07-11 12:00:08 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 1.8914
2022-07-11 12:00:40 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.0534
2022-07-11 12:01:13 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.0269
2022-07-11 12:01:45 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.0144
2022-07-11 12:02:17 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 1.8722
2022-07-11 12:02:49 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 1.9305
2022-07-11 12:03:22 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 1.8144
2022-07-11 12:03:54 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.0543
2022-07-11 12:04:27 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 1.8193
2022-07-11 12:04:59 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 1.7753
2022-07-11 12:05:31 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 1.7825
2022-07-11 12:06:03 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 1.8695
2022-07-11 12:06:05 - train: epoch 028, train_loss: 1.9029
2022-07-11 12:07:20 - eval: epoch: 028, acc1: 61.412%, acc5: 84.466%, test_loss: 1.6117, per_image_load_time: 2.347ms, per_image_inference_time: 0.603ms
2022-07-11 12:07:21 - until epoch: 028, best_acc1: 61.928%
2022-07-11 12:07:21 - epoch 029 lr: 0.100000
2022-07-11 12:08:00 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 1.8714
2022-07-11 12:08:32 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 1.8558
2022-07-11 12:09:05 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 1.9925
2022-07-11 12:09:37 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 1.7893
2022-07-11 12:10:09 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 1.8511
2022-07-11 12:10:41 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.1451
2022-07-11 12:11:13 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.4553
2022-07-11 12:11:45 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.0359
2022-07-11 12:12:17 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 1.8030
2022-07-11 12:12:49 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 1.7375
2022-07-11 12:13:21 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 1.8656
2022-07-11 12:13:54 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.0522
2022-07-11 12:14:26 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 1.8123
2022-07-11 12:14:58 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.0899
2022-07-11 12:15:30 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 1.9093
2022-07-11 12:16:02 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 1.9010
2022-07-11 12:16:34 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.0225
2022-07-11 12:17:06 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.0627
2022-07-11 12:17:38 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 1.7091
2022-07-11 12:18:10 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 1.9933
2022-07-11 12:18:42 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 1.9066
2022-07-11 12:19:15 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 1.9753
2022-07-11 12:19:47 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 1.8730
2022-07-11 12:20:20 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 1.7348
2022-07-11 12:20:52 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 1.8677
2022-07-11 12:21:24 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 1.9913
2022-07-11 12:21:57 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 1.7610
2022-07-11 12:22:29 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 1.7430
2022-07-11 12:23:01 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 1.8818
2022-07-11 12:23:33 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 1.9471
2022-07-11 12:24:05 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 1.8273
2022-07-11 12:24:38 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.0459
2022-07-11 12:25:10 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 1.9819
2022-07-11 12:25:42 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 1.7115
2022-07-11 12:26:15 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.0580
2022-07-11 12:26:47 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 1.8397
2022-07-11 12:27:19 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 1.8852
2022-07-11 12:27:52 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 1.9573
2022-07-11 12:28:24 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 1.6991
2022-07-11 12:28:57 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 1.8922
2022-07-11 12:29:29 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 1.8046
2022-07-11 12:30:02 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 1.7377
2022-07-11 12:30:34 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 1.9736
2022-07-11 12:31:06 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 1.9707
2022-07-11 12:31:39 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.0312
2022-07-11 12:32:11 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.0717
2022-07-11 12:32:44 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 1.7095
2022-07-11 12:33:16 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 1.9724
2022-07-11 12:33:48 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.2892
2022-07-11 12:34:21 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.6911
2022-07-11 12:34:22 - train: epoch 029, train_loss: 1.9002
2022-07-11 12:35:37 - eval: epoch: 029, acc1: 61.074%, acc5: 84.358%, test_loss: 1.6201, per_image_load_time: 2.258ms, per_image_inference_time: 0.584ms
2022-07-11 12:35:37 - until epoch: 029, best_acc1: 61.928%
2022-07-11 12:35:37 - epoch 030 lr: 0.100000
2022-07-11 12:36:17 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.0905
2022-07-11 12:36:50 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 1.8897
2022-07-11 12:37:21 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 1.7199
2022-07-11 12:37:53 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 1.6823
2022-07-11 12:38:26 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.1462
2022-07-11 12:38:58 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 1.7481
2022-07-11 12:39:30 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 1.7997
2022-07-11 12:40:02 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.1660
2022-07-11 12:40:34 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 1.8864
2022-07-11 12:41:06 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 1.5718
2022-07-11 12:41:39 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 1.7549
2022-07-11 12:42:11 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 1.9438
2022-07-11 12:42:43 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 1.7218
2022-07-11 12:43:15 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 1.8708
2022-07-11 12:43:47 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 1.8458
2022-07-11 12:44:19 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 1.8128
2022-07-11 12:44:51 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.1483
2022-07-11 12:45:23 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 1.9500
2022-07-11 12:45:56 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.0893
2022-07-11 12:46:28 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.0011
2022-07-11 12:47:00 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.0189
2022-07-11 12:47:32 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 1.8617
2022-07-11 12:48:04 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 1.8857
2022-07-11 12:48:36 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.0485
2022-07-11 12:49:08 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 1.8359
2022-07-11 12:49:40 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 1.9768
2022-07-11 12:50:12 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 1.8425
2022-07-11 12:50:45 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 1.8595
2022-07-11 12:51:17 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 1.9090
2022-07-11 12:51:49 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.1013
2022-07-11 12:52:21 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 1.8049
2022-07-11 12:52:53 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 1.8964
2022-07-11 12:53:26 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.0863
2022-07-11 12:53:58 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.0046
2022-07-11 12:54:30 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 1.9530
2022-07-11 12:55:03 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 1.8224
2022-07-11 12:55:35 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 1.8534
2022-07-11 12:56:07 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 1.9554
2022-07-11 12:56:40 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 1.9394
2022-07-11 12:57:12 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 1.8626
2022-07-11 12:57:44 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 1.8367
2022-07-11 12:58:16 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.0746
2022-07-11 12:58:49 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 1.8706
2022-07-11 12:59:21 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 1.9095
2022-07-11 12:59:54 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.1130
2022-07-11 13:00:26 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 1.5590
2022-07-11 13:00:58 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 1.9210
2022-07-11 13:01:30 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 1.9132
2022-07-11 13:02:03 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.0583
2022-07-11 13:02:35 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 1.9178
2022-07-11 13:02:37 - train: epoch 030, train_loss: 1.8994
2022-07-11 13:03:52 - eval: epoch: 030, acc1: 61.432%, acc5: 84.566%, test_loss: 1.6109, per_image_load_time: 2.365ms, per_image_inference_time: 0.594ms
2022-07-11 13:03:53 - until epoch: 030, best_acc1: 61.928%
2022-07-11 13:03:53 - epoch 031 lr: 0.010000
2022-07-11 13:04:32 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 1.8167
2022-07-11 13:05:04 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.5576
2022-07-11 13:05:37 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.6085
2022-07-11 13:06:09 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.7052
2022-07-11 13:06:42 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.4427
2022-07-11 13:07:14 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.4457
2022-07-11 13:07:47 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.3943
2022-07-11 13:08:20 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.4526
2022-07-11 13:08:52 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.4319
2022-07-11 13:09:25 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 1.6893
2022-07-11 13:09:58 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 1.6602
2022-07-11 13:10:30 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.4800
2022-07-11 13:11:03 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.3077
2022-07-11 13:11:35 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.4646
2022-07-11 13:12:08 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.4813
2022-07-11 13:12:41 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.2833
2022-07-11 13:13:13 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.3426
2022-07-11 13:13:46 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.4000
2022-07-11 13:14:19 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.4833
2022-07-11 13:14:51 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.4158
2022-07-11 13:15:23 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.2647
2022-07-11 13:15:56 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.1984
2022-07-11 13:16:28 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.2517
2022-07-11 13:17:01 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.3591
2022-07-11 13:17:33 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.2962
2022-07-11 13:18:06 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.3931
2022-07-11 13:18:38 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.4022
2022-07-11 13:19:11 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.6855
2022-07-11 13:19:43 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.4693
2022-07-11 13:20:15 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.3996
2022-07-11 13:20:48 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.2856
2022-07-11 13:21:20 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.5029
2022-07-11 13:21:53 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.3879
2022-07-11 13:22:26 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.5180
2022-07-11 13:22:58 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.4561
2022-07-11 13:23:31 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.4729
2022-07-11 13:24:03 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.4081
2022-07-11 13:24:36 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.3039
2022-07-11 13:25:08 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.2362
2022-07-11 13:25:41 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.3453
2022-07-11 13:26:14 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.3361
2022-07-11 13:26:46 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.4206
2022-07-11 13:27:19 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.3148
2022-07-11 13:27:51 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.5207
2022-07-11 13:28:24 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.4604
2022-07-11 13:28:56 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.3734
2022-07-11 13:29:29 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.4095
2022-07-11 13:30:02 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.3362
2022-07-11 13:30:34 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.2931
2022-07-11 13:31:06 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.2964
2022-07-11 13:31:08 - train: epoch 031, train_loss: 1.4240
2022-07-11 13:32:23 - eval: epoch: 031, acc1: 71.966%, acc5: 90.624%, test_loss: 1.1241, per_image_load_time: 2.352ms, per_image_inference_time: 0.581ms
2022-07-11 13:32:23 - until epoch: 031, best_acc1: 71.966%
2022-07-11 13:32:23 - epoch 032 lr: 0.010000
2022-07-11 13:33:03 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.2886
2022-07-11 13:33:36 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.3871
2022-07-11 13:34:08 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.3764
2022-07-11 13:34:40 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.4059
2022-07-11 13:35:13 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.4353
2022-07-11 13:35:45 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.3027
2022-07-11 13:36:18 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.3373
2022-07-11 13:36:50 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.2597
2022-07-11 13:37:23 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.3272
2022-07-11 13:37:55 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.4768
2022-07-11 13:38:28 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.4315
2022-07-11 13:39:01 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.2399
2022-07-11 13:39:33 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.1982
2022-07-11 13:40:06 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.4613
2022-07-11 13:40:38 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.3025
2022-07-11 13:41:11 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.3955
2022-07-11 13:41:43 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.5473
2022-07-11 13:42:16 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.5506
2022-07-11 13:42:48 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.1698
2022-07-11 13:43:21 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.3074
2022-07-11 13:43:53 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.2968
2022-07-11 13:44:26 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.1764
2022-07-11 13:44:58 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.3913
2022-07-11 13:45:31 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.2327
2022-07-11 13:46:03 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.3903
2022-07-11 13:46:36 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.1328
2022-07-11 13:47:08 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.2619
2022-07-11 13:47:41 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.3828
2022-07-11 13:48:14 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.0851
2022-07-11 13:48:47 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.1314
2022-07-11 13:49:19 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.2955
2022-07-11 13:49:52 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.2409
2022-07-11 13:50:25 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.1689
2022-07-11 13:50:57 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.0645
2022-07-11 13:51:30 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.2782
2022-07-11 13:52:03 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.3926
2022-07-11 13:52:36 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.3361
2022-07-11 13:53:09 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.1727
2022-07-11 13:53:41 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.4223
2022-07-11 13:54:14 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.1586
2022-07-11 13:54:47 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.2844
2022-07-11 13:55:20 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.1734
2022-07-11 13:55:52 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.4750
2022-07-11 13:56:25 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.2638
2022-07-11 13:56:57 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.4989
2022-07-11 13:57:30 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.2055
2022-07-11 13:58:03 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.4120
2022-07-11 13:58:35 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.1259
2022-07-11 13:59:08 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.3970
2022-07-11 13:59:40 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.2984
2022-07-11 13:59:42 - train: epoch 032, train_loss: 1.2971
2022-07-11 14:00:57 - eval: epoch: 032, acc1: 72.910%, acc5: 91.168%, test_loss: 1.0815, per_image_load_time: 2.318ms, per_image_inference_time: 0.579ms
2022-07-11 14:00:57 - until epoch: 032, best_acc1: 72.910%
2022-07-11 14:00:57 - epoch 033 lr: 0.010000
2022-07-11 14:01:37 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.1060
2022-07-11 14:02:09 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.4144
2022-07-11 14:02:42 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.1466
2022-07-11 14:03:14 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.1516
2022-07-11 14:03:47 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.3426
2022-07-11 14:04:20 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.1061
2022-07-11 14:04:52 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.3415
2022-07-11 14:05:25 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.2253
2022-07-11 14:05:57 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.3249
2022-07-11 14:06:30 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.3456
2022-07-11 14:07:03 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.1718
2022-07-11 14:07:35 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.1761
2022-07-11 14:08:08 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.0827
2022-07-11 14:08:40 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.3670
2022-07-11 14:09:13 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.4844
2022-07-11 14:09:45 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.2765
2022-07-11 14:10:18 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.1315
2022-07-11 14:10:50 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.4927
2022-07-11 14:11:22 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.2676
2022-07-11 14:11:55 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.1069
2022-07-11 14:12:27 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.2663
2022-07-11 14:13:00 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.3373
2022-07-11 14:13:33 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.2244
2022-07-11 14:14:06 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.5246
2022-07-11 14:14:38 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.1895
2022-07-11 14:15:11 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.0646
2022-07-11 14:15:43 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.3569
2022-07-11 14:16:16 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.2000
2022-07-11 14:16:48 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.3796
2022-07-11 14:17:21 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.2916
2022-07-11 14:17:53 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.2547
2022-07-11 14:18:26 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.2398
2022-07-11 14:18:59 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.3649
2022-07-11 14:19:31 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.1643
2022-07-11 14:20:04 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.2479
2022-07-11 14:20:37 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.4067
2022-07-11 14:21:10 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.1740
2022-07-11 14:21:43 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.1246
2022-07-11 14:22:15 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.3121
2022-07-11 14:22:48 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.3431
2022-07-11 14:23:21 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.2861
2022-07-11 14:23:54 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.1639
2022-07-11 14:24:27 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.3986
2022-07-11 14:24:59 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.2472
2022-07-11 14:25:32 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.4166
2022-07-11 14:26:05 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.2375
2022-07-11 14:26:38 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.0637
2022-07-11 14:27:11 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.4867
2022-07-11 14:27:44 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.0939
2022-07-11 14:28:16 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.2190
2022-07-11 14:28:18 - train: epoch 033, train_loss: 1.2459
2022-07-11 14:29:32 - eval: epoch: 033, acc1: 73.504%, acc5: 91.486%, test_loss: 1.0579, per_image_load_time: 2.290ms, per_image_inference_time: 0.578ms
2022-07-11 14:29:33 - until epoch: 033, best_acc1: 73.504%
2022-07-11 14:29:33 - epoch 034 lr: 0.010000
2022-07-11 14:30:12 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.1259
2022-07-11 14:30:44 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.1809
2022-07-11 14:31:17 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.2364
2022-07-11 14:31:50 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.0359
2022-07-11 14:32:23 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.2130
2022-07-11 14:32:56 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.3974
2022-07-11 14:33:29 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.2453
2022-07-11 14:34:01 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.1717
2022-07-11 14:34:34 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.1911
2022-07-11 14:35:06 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.1761
2022-07-11 14:35:39 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.2105
2022-07-11 14:36:12 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.2207
2022-07-11 14:36:44 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.1712
2022-07-11 14:37:17 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.1828
2022-07-11 14:37:50 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.1299
2022-07-11 14:38:23 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.1552
2022-07-11 14:38:56 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.2138
2022-07-11 14:39:29 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.3708
2022-07-11 14:40:02 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.2948
2022-07-11 14:40:35 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.3054
2022-07-11 14:41:07 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.3980
2022-07-11 14:41:40 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.1156
2022-07-11 14:42:13 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.3562
2022-07-11 14:42:46 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.1347
2022-07-11 14:43:19 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.1908
2022-07-11 14:43:52 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.2513
2022-07-11 14:44:24 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.2448
2022-07-11 14:44:57 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.0177
2022-07-11 14:45:30 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.0162
2022-07-11 14:46:03 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.0304
2022-07-11 14:46:36 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.0666
2022-07-11 14:47:09 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.2186
2022-07-11 14:47:42 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.0929
2022-07-11 14:48:14 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.2435
2022-07-11 14:48:47 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.0842
2022-07-11 14:49:20 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.0472
2022-07-11 14:49:53 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.0983
2022-07-11 14:50:25 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.1762
2022-07-11 14:50:58 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.3278
2022-07-11 14:51:31 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.0846
2022-07-11 14:52:04 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.2596
2022-07-11 14:52:38 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.1677
2022-07-11 14:53:10 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.2136
2022-07-11 14:53:43 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.2526
2022-07-11 14:54:16 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.4467
2022-07-11 14:54:49 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.2949
2022-07-11 14:55:22 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.1128
2022-07-11 14:55:54 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.0987
2022-07-11 14:56:27 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.1718
2022-07-11 14:57:00 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.0518
2022-07-11 14:57:01 - train: epoch 034, train_loss: 1.2153
2022-07-11 14:58:16 - eval: epoch: 034, acc1: 73.754%, acc5: 91.666%, test_loss: 1.0454, per_image_load_time: 2.227ms, per_image_inference_time: 0.569ms
2022-07-11 14:58:17 - until epoch: 034, best_acc1: 73.754%
2022-07-11 14:58:17 - epoch 035 lr: 0.010000
2022-07-11 14:58:57 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.0878
2022-07-11 14:59:29 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 0.8916
2022-07-11 15:00:02 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.3444
2022-07-11 15:00:34 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.0818
2022-07-11 15:01:06 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.3472
2022-07-11 15:01:39 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.1204
2022-07-11 15:02:11 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.1829
2022-07-11 15:02:43 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.1520
2022-07-11 15:03:15 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.3273
2022-07-11 15:03:47 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.3275
2022-07-11 15:04:20 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.3530
2022-07-11 15:04:53 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.1156
2022-07-11 15:05:25 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.2691
2022-07-11 15:05:57 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.2028
2022-07-11 15:06:30 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.2533
2022-07-11 15:07:03 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.1436
2022-07-11 15:07:36 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 0.9941
2022-07-11 15:08:08 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.2089
2022-07-11 15:08:41 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.2550
2022-07-11 15:09:14 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.1391
2022-07-11 15:09:46 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.1798
2022-07-11 15:10:19 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.2589
2022-07-11 15:10:51 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.3116
2022-07-11 15:11:24 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.2203
2022-07-11 15:11:57 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.1033
2022-07-11 15:12:29 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.2948
2022-07-11 15:13:02 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.2925
2022-07-11 15:13:34 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.0704
2022-07-11 15:14:07 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.1836
2022-07-11 15:14:40 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.2868
2022-07-11 15:15:12 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.2339
2022-07-11 15:15:45 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.0728
2022-07-11 15:16:18 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.1488
2022-07-11 15:16:51 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.1824
2022-07-11 15:17:23 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.0515
2022-07-11 15:17:56 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.1477
2022-07-11 15:18:28 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.1176
2022-07-11 15:19:01 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.2296
2022-07-11 15:19:34 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.3062
2022-07-11 15:20:07 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.0427
2022-07-11 15:20:40 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.4196
2022-07-11 15:21:13 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.1439
2022-07-11 15:21:46 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.2719
2022-07-11 15:22:19 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.2299
2022-07-11 15:22:52 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.3133
2022-07-11 15:23:24 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.1094
2022-07-11 15:23:57 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.3949
2022-07-11 15:24:30 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.3412
2022-07-11 15:25:03 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.2449
2022-07-11 15:25:35 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.1538
2022-07-11 15:25:37 - train: epoch 035, train_loss: 1.1913
2022-07-11 15:26:52 - eval: epoch: 035, acc1: 73.844%, acc5: 91.638%, test_loss: 1.0409, per_image_load_time: 2.327ms, per_image_inference_time: 0.585ms
2022-07-11 15:26:52 - until epoch: 035, best_acc1: 73.844%
2022-07-11 15:26:52 - epoch 036 lr: 0.010000
2022-07-11 15:27:31 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.2836
2022-07-11 15:28:04 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 0.9535
2022-07-11 15:28:36 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.1355
2022-07-11 15:29:08 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.1483
2022-07-11 15:29:40 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.1601
2022-07-11 15:30:13 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.0969
2022-07-11 15:30:46 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.0235
2022-07-11 15:31:18 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 0.9891
2022-07-11 15:31:51 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.1976
2022-07-11 15:32:23 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.0826
2022-07-11 15:32:56 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.1807
2022-07-11 15:33:29 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.1430
2022-07-11 15:34:02 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.2415
2022-07-11 15:34:35 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.3201
2022-07-11 15:35:08 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.2892
2022-07-11 15:35:41 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.1513
2022-07-11 15:36:14 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.2316
2022-07-11 15:36:47 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.0881
2022-07-11 15:37:20 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.1817
2022-07-11 15:37:52 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.2344
2022-07-11 15:38:25 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.2273
2022-07-11 15:38:58 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.1127
2022-07-11 15:39:31 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.3653
2022-07-11 15:40:03 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.3173
2022-07-11 15:40:36 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.0038
2022-07-11 15:41:09 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.1906
2022-07-11 15:41:42 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.0799
2022-07-11 15:42:14 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.0829
2022-07-11 15:42:48 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.0112
2022-07-11 15:43:20 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.2832
2022-07-11 15:43:53 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.1387
2022-07-11 15:44:25 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.2216
2022-07-11 15:44:58 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.1051
2022-07-11 15:45:31 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.1007
2022-07-11 15:46:04 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.0947
2022-07-11 15:46:37 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.2885
2022-07-11 15:47:10 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.1476
2022-07-11 15:47:43 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.0451
2022-07-11 15:48:16 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.1317
2022-07-11 15:48:48 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.2233
2022-07-11 15:49:21 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.2003
2022-07-11 15:49:53 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.1081
2022-07-11 15:50:26 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.1145
2022-07-11 15:50:59 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.2294
2022-07-11 15:51:32 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 0.9521
2022-07-11 15:52:05 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.0355
2022-07-11 15:52:38 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.1572
2022-07-11 15:53:10 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.1715
2022-07-11 15:53:43 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.1927
2022-07-11 15:54:15 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.1374
2022-07-11 15:54:17 - train: epoch 036, train_loss: 1.1734
2022-07-11 15:55:31 - eval: epoch: 036, acc1: 73.936%, acc5: 91.662%, test_loss: 1.0397, per_image_load_time: 1.582ms, per_image_inference_time: 0.572ms
2022-07-11 15:55:32 - until epoch: 036, best_acc1: 73.936%
2022-07-11 15:55:32 - epoch 037 lr: 0.010000
2022-07-11 15:56:11 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 0.9320
2022-07-11 15:56:43 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 0.9044
2022-07-11 15:57:16 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.1278
2022-07-11 15:57:48 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.2867
2022-07-11 15:58:21 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.0505
2022-07-11 15:58:54 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.1554
2022-07-11 15:59:27 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.1847
2022-07-11 15:59:59 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.0817
2022-07-11 16:00:32 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.4656
2022-07-11 16:01:05 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.1150
2022-07-11 16:01:37 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.1591
2022-07-11 16:02:09 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.1695
2022-07-11 16:02:42 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.1458
2022-07-11 16:03:14 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.1921
2022-07-11 16:03:47 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.1371
2022-07-11 16:04:19 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.0667
2022-07-11 16:04:52 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.4358
2022-07-11 16:05:25 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.2999
2022-07-11 16:05:57 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.2820
2022-07-11 16:06:30 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.1550
2022-07-11 16:07:03 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.1183
2022-07-11 16:07:35 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.1195
2022-07-11 16:08:07 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.0409
2022-07-11 16:08:40 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.1586
2022-07-11 16:09:13 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.0775
2022-07-11 16:09:46 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.3876
2022-07-11 16:10:18 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.2205
2022-07-11 16:10:51 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.2580
2022-07-11 16:11:23 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.1053
2022-07-11 16:11:56 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.3333
2022-07-11 16:12:29 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.1012
2022-07-11 16:13:02 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.2223
2022-07-11 16:13:34 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.0600
2022-07-11 16:14:07 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.0532
2022-07-11 16:14:40 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.2015
2022-07-11 16:15:13 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.2745
2022-07-11 16:15:46 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.2983
2022-07-11 16:16:19 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.0448
2022-07-11 16:16:53 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.2885
2022-07-11 16:17:25 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.0473
2022-07-11 16:17:58 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.2071
2022-07-11 16:18:31 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.3469
2022-07-11 16:19:04 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.2440
2022-07-11 16:19:36 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.0751
2022-07-11 16:20:09 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.1587
2022-07-11 16:20:42 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.2220
2022-07-11 16:21:15 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.1730
2022-07-11 16:21:47 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.1901
2022-07-11 16:22:20 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.2117
2022-07-11 16:22:53 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.1702
2022-07-11 16:22:54 - train: epoch 037, train_loss: 1.1656
2022-07-11 16:24:09 - eval: epoch: 037, acc1: 73.722%, acc5: 91.798%, test_loss: 1.0405, per_image_load_time: 2.302ms, per_image_inference_time: 0.590ms
2022-07-11 16:24:09 - until epoch: 037, best_acc1: 73.936%
2022-07-11 16:24:09 - epoch 038 lr: 0.010000
2022-07-11 16:24:49 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.1319
2022-07-11 16:25:22 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 0.9369
2022-07-11 16:25:55 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 0.8489
2022-07-11 16:26:27 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.0491
2022-07-11 16:27:00 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.0444
2022-07-11 16:27:32 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.2189
2022-07-11 16:28:04 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.0690
2022-07-11 16:28:37 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 0.9929
2022-07-11 16:29:09 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.1227
2022-07-11 16:29:41 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.3269
2022-07-11 16:30:14 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.0651
2022-07-11 16:30:46 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.1601
2022-07-11 16:31:19 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.2578
2022-07-11 16:31:51 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.2590
2022-07-11 16:32:24 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.1416
2022-07-11 16:32:56 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.2310
2022-07-11 16:33:29 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.3055
2022-07-11 16:34:01 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.3639
2022-07-11 16:34:34 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.2094
2022-07-11 16:35:07 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.1656
2022-07-11 16:35:39 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.1342
2022-07-11 16:36:12 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.0467
2022-07-11 16:36:45 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.2636
2022-07-11 16:37:18 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.3055
2022-07-11 16:37:50 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.1525
2022-07-11 16:38:23 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.2427
2022-07-11 16:38:56 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.3104
2022-07-11 16:39:29 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.2759
2022-07-11 16:40:02 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.2125
2022-07-11 16:40:34 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.1263
2022-07-11 16:41:06 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.1318
2022-07-11 16:41:39 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 0.9133
2022-07-11 16:42:12 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 0.9815
2022-07-11 16:42:45 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.1215
2022-07-11 16:43:18 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.3479
2022-07-11 16:43:51 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.2603
2022-07-11 16:44:23 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 0.9537
2022-07-11 16:44:56 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.1819
2022-07-11 16:45:29 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.1007
2022-07-11 16:46:02 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.1151
2022-07-11 16:46:34 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.1823
2022-07-11 16:47:07 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.0780
2022-07-11 16:47:40 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.2903
2022-07-11 16:48:12 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.2131
2022-07-11 16:48:45 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.1982
2022-07-11 16:49:18 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.2740
2022-07-11 16:49:50 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.0707
2022-07-11 16:50:23 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.1573
2022-07-11 16:50:55 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.1956
2022-07-11 16:51:27 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.0083
2022-07-11 16:51:29 - train: epoch 038, train_loss: 1.1560
2022-07-11 16:52:44 - eval: epoch: 038, acc1: 73.872%, acc5: 91.664%, test_loss: 1.0428, per_image_load_time: 2.333ms, per_image_inference_time: 0.590ms
2022-07-11 16:52:44 - until epoch: 038, best_acc1: 73.936%
2022-07-11 16:52:44 - epoch 039 lr: 0.010000
2022-07-11 16:53:24 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.1607
2022-07-11 16:53:56 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.3023
2022-07-11 16:54:29 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.0154
2022-07-11 16:55:02 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.1292
2022-07-11 16:55:34 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.0412
2022-07-11 16:56:07 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.0658
2022-07-11 16:56:39 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.2645
2022-07-11 16:57:12 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.0930
2022-07-11 16:57:45 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.2785
2022-07-11 16:58:17 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.1332
2022-07-11 16:58:50 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.1609
2022-07-11 16:59:23 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.0614
2022-07-11 16:59:57 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.2999
2022-07-11 17:00:29 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.1823
2022-07-11 17:01:02 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.1186
2022-07-11 17:01:35 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.1995
2022-07-11 17:02:08 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 0.9687
2022-07-11 17:02:41 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.1155
2022-07-11 17:03:14 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 0.9018
2022-07-11 17:03:46 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.0611
2022-07-11 17:04:19 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.1376
2022-07-11 17:04:52 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.1677
2022-07-11 17:05:24 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.2924
2022-07-11 17:05:57 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.1646
2022-07-11 17:06:30 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.0909
2022-07-11 17:07:02 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.1178
2022-07-11 17:07:35 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.2896
2022-07-11 17:08:08 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.0410
2022-07-11 17:08:41 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.0119
2022-07-11 17:09:14 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.2739
2022-07-11 17:09:47 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 0.9988
2022-07-11 17:10:19 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.1275
2022-07-11 17:10:52 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.1931
2022-07-11 17:11:24 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.2759
2022-07-11 17:11:57 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.2352
2022-07-11 17:12:30 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.1909
2022-07-11 17:13:03 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.1745
2022-07-11 17:13:35 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.0324
2022-07-11 17:14:08 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.2107
2022-07-11 17:14:41 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.1676
2022-07-11 17:15:13 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.2239
2022-07-11 17:15:46 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.1429
2022-07-11 17:16:19 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.1152
2022-07-11 17:16:51 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 0.9450
2022-07-11 17:17:24 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.0450
2022-07-11 17:17:57 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.3505
2022-07-11 17:18:30 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.1129
2022-07-11 17:19:03 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.2546
2022-07-11 17:19:35 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 0.9341
2022-07-11 17:20:08 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.0411
2022-07-11 17:20:09 - train: epoch 039, train_loss: 1.1491
2022-07-11 17:21:23 - eval: epoch: 039, acc1: 73.702%, acc5: 91.698%, test_loss: 1.0448, per_image_load_time: 2.128ms, per_image_inference_time: 0.589ms
2022-07-11 17:21:24 - until epoch: 039, best_acc1: 73.936%
2022-07-11 17:21:24 - epoch 040 lr: 0.010000
2022-07-11 17:22:03 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.3053
2022-07-11 17:22:36 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.3459
2022-07-11 17:23:08 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.2117
2022-07-11 17:23:40 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.1076
2022-07-11 17:24:12 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.0654
2022-07-11 17:24:44 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.2527
2022-07-11 17:25:17 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.2095
2022-07-11 17:25:49 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.2525
2022-07-11 17:26:22 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.0061
2022-07-11 17:26:54 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 0.8602
2022-07-11 17:27:26 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.0486
2022-07-11 17:27:59 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.0833
2022-07-11 17:28:31 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.0640
2022-07-11 17:29:04 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 0.9972
2022-07-11 17:29:36 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.3196
2022-07-11 17:30:09 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.1836
2022-07-11 17:30:42 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.1621
2022-07-11 17:31:14 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.0238
2022-07-11 17:31:47 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.1855
2022-07-11 17:32:20 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.1514
2022-07-11 17:32:52 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.0336
2022-07-11 17:33:25 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.0395
2022-07-11 17:33:58 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.0373
2022-07-11 17:34:30 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.1694
2022-07-11 17:35:03 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.2263
2022-07-11 17:35:35 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.0863
2022-07-11 17:36:08 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.1987
2022-07-11 17:36:40 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.1297
2022-07-11 17:37:13 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.3808
2022-07-11 17:37:46 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.3563
2022-07-11 17:38:19 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 0.9693
2022-07-11 17:38:51 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.2994
2022-07-11 17:39:24 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.1295
2022-07-11 17:39:57 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.1627
2022-07-11 17:40:29 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.1995
2022-07-11 17:41:02 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.2348
2022-07-11 17:41:35 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.1528
2022-07-11 17:42:08 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 0.9827
2022-07-11 17:42:41 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.2261
2022-07-11 17:43:14 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.2554
2022-07-11 17:43:46 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.2318
2022-07-11 17:44:19 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.0138
2022-07-11 17:44:52 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.2829
2022-07-11 17:45:24 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.1814
2022-07-11 17:45:57 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 0.9590
2022-07-11 17:46:30 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.1896
2022-07-11 17:47:03 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.0508
2022-07-11 17:47:36 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.1463
2022-07-11 17:48:09 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.2625
2022-07-11 17:48:41 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.3186
2022-07-11 17:48:43 - train: epoch 040, train_loss: 1.1457
2022-07-11 17:49:58 - eval: epoch: 040, acc1: 73.616%, acc5: 91.638%, test_loss: 1.0455, per_image_load_time: 2.341ms, per_image_inference_time: 0.581ms
2022-07-11 17:49:58 - until epoch: 040, best_acc1: 73.936%
2022-07-11 17:49:58 - epoch 041 lr: 0.010000
2022-07-11 17:50:38 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.2143
2022-07-11 17:51:10 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.1862
2022-07-11 17:51:43 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.0774
2022-07-11 17:52:15 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.1239
2022-07-11 17:52:47 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 0.9366
2022-07-11 17:53:19 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.2111
2022-07-11 17:53:52 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.0534
2022-07-11 17:54:24 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 0.8937
2022-07-11 17:54:57 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 0.9963
2022-07-11 17:55:29 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.2934
2022-07-11 17:56:02 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.0351
2022-07-11 17:56:34 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 0.9528
2022-07-11 17:57:07 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.1427
2022-07-11 17:57:40 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.2716
2022-07-11 17:58:13 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.4043
2022-07-11 17:58:45 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 0.9584
2022-07-11 17:59:18 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.1810
2022-07-11 17:59:50 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.1062
2022-07-11 18:00:23 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.2521
2022-07-11 18:00:55 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.1006
2022-07-11 18:01:28 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.1245
2022-07-11 18:02:00 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.0534
2022-07-11 18:02:33 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 0.9834
2022-07-11 18:03:05 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.3315
2022-07-11 18:03:38 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.0519
2022-07-11 18:04:11 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.3246
2022-07-11 18:04:44 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.2723
2022-07-11 18:05:16 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.0469
2022-07-11 18:05:49 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.2393
2022-07-11 18:06:22 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.2563
2022-07-11 18:06:55 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.3566
2022-07-11 18:07:28 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.0626
2022-07-11 18:08:00 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.0737
2022-07-11 18:08:33 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.0502
2022-07-11 18:09:06 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.2498
2022-07-11 18:09:39 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.1854
2022-07-11 18:10:11 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.1509
2022-07-11 18:10:44 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.0124
2022-07-11 18:11:17 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.2031
2022-07-11 18:11:50 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.0645
2022-07-11 18:12:22 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.1262
2022-07-11 18:12:55 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.1329
2022-07-11 18:13:28 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.2540
2022-07-11 18:14:00 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.0504
2022-07-11 18:14:33 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.1821
2022-07-11 18:15:06 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.2060
2022-07-11 18:15:39 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.3493
2022-07-11 18:16:11 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.1719
2022-07-11 18:16:44 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.1868
2022-07-11 18:17:17 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.2130
2022-07-11 18:17:18 - train: epoch 041, train_loss: 1.1455
2022-07-11 18:18:32 - eval: epoch: 041, acc1: 73.452%, acc5: 91.588%, test_loss: 1.0562, per_image_load_time: 1.697ms, per_image_inference_time: 0.565ms
2022-07-11 18:18:33 - until epoch: 041, best_acc1: 73.936%
2022-07-11 18:18:33 - epoch 042 lr: 0.010000
2022-07-11 18:19:12 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 0.8758
2022-07-11 18:19:44 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.2678
2022-07-11 18:20:17 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.0885
2022-07-11 18:20:50 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 0.8855
2022-07-11 18:21:23 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.1666
2022-07-11 18:21:55 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.0662
2022-07-11 18:22:28 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.0502
2022-07-11 18:23:01 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.0441
2022-07-11 18:23:33 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.3181
2022-07-11 18:24:06 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.2114
2022-07-11 18:24:39 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 0.9537
2022-07-11 18:25:11 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.0780
2022-07-11 18:25:44 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.1630
2022-07-11 18:26:18 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.3808
2022-07-11 18:26:51 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.0328
2022-07-11 18:27:24 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.2440
2022-07-11 18:27:56 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.0742
2022-07-11 18:28:29 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.1220
2022-07-11 18:29:02 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.2668
2022-07-11 18:29:34 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.0339
2022-07-11 18:30:07 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 0.9185
2022-07-11 18:30:40 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.0604
2022-07-11 18:31:13 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.0902
2022-07-11 18:31:45 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.3469
2022-07-11 18:32:18 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.2384
2022-07-11 18:32:51 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.2088
2022-07-11 18:33:24 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.0172
2022-07-11 18:33:57 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.0938
2022-07-11 18:34:30 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.2113
2022-07-11 18:35:03 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.0948
2022-07-11 18:35:36 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.0952
2022-07-11 18:36:09 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.1833
2022-07-11 18:36:42 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.3048
2022-07-11 18:37:15 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 0.9438
2022-07-11 18:37:47 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.0398
2022-07-11 18:38:20 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.3316
2022-07-11 18:38:53 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.0917
2022-07-11 18:39:26 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 0.9888
2022-07-11 18:39:58 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.1042
2022-07-11 18:40:31 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.1973
2022-07-11 18:41:04 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.2197
2022-07-11 18:41:36 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.2520
2022-07-11 18:42:09 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.0096
2022-07-11 18:42:42 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.0477
2022-07-11 18:43:15 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.1172
2022-07-11 18:43:47 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.3548
2022-07-11 18:44:20 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.0707
2022-07-11 18:44:53 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.2681
2022-07-11 18:45:26 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.2438
2022-07-11 18:45:58 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.1425
2022-07-11 18:46:00 - train: epoch 042, train_loss: 1.1410
2022-07-11 18:47:15 - eval: epoch: 042, acc1: 73.740%, acc5: 91.714%, test_loss: 1.0513, per_image_load_time: 1.624ms, per_image_inference_time: 0.582ms
2022-07-11 18:47:16 - until epoch: 042, best_acc1: 73.936%
2022-07-11 18:47:16 - epoch 043 lr: 0.010000
2022-07-11 18:47:55 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.2020
2022-07-11 18:48:28 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.0967
2022-07-11 18:49:00 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 0.8872
2022-07-11 18:49:32 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.0422
2022-07-11 18:50:05 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.2145
2022-07-11 18:50:37 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.1238
2022-07-11 18:51:09 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.2408
2022-07-11 18:51:42 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.2729
2022-07-11 18:52:14 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.1189
2022-07-11 18:52:47 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.2351
2022-07-11 18:53:19 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.2618
2022-07-11 18:53:52 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.2997
2022-07-11 18:54:25 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.2053
2022-07-11 18:54:57 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.1468
2022-07-11 18:55:30 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.0420
2022-07-11 18:56:02 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.2383
2022-07-11 18:56:35 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.1436
2022-07-11 18:57:08 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.1645
2022-07-11 18:57:41 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.2087
2022-07-11 18:58:13 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 0.9716
2022-07-11 18:58:46 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.1709
2022-07-11 18:59:18 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.1248
2022-07-11 18:59:51 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.3845
2022-07-11 19:00:24 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.0131
2022-07-11 19:00:57 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.2815
2022-07-11 19:01:29 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 0.9028
2022-07-11 19:02:02 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.1428
2022-07-11 19:02:35 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.1288
2022-07-11 19:03:08 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.1880
2022-07-11 19:03:41 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.3448
2022-07-11 19:04:14 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.3820
2022-07-11 19:04:47 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.2618
2022-07-11 19:05:19 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.1708
2022-07-11 19:05:52 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.1989
2022-07-11 19:06:25 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.1838
2022-07-11 19:06:57 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.1614
2022-07-11 19:07:30 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.1294
2022-07-11 19:08:03 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.1377
2022-07-11 19:08:36 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.2837
2022-07-11 19:09:09 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.1536
2022-07-11 19:09:41 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.4204
2022-07-11 19:10:14 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.1749
2022-07-11 19:10:47 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.1818
2022-07-11 19:11:20 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.0692
2022-07-11 19:11:53 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.1249
2022-07-11 19:12:26 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.1632
2022-07-11 19:12:59 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.0481
2022-07-11 19:13:32 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.0291
2022-07-11 19:14:05 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.0671
2022-07-11 19:14:37 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.1036
2022-07-11 19:14:38 - train: epoch 043, train_loss: 1.1386
2022-07-11 19:15:53 - eval: epoch: 043, acc1: 73.300%, acc5: 91.640%, test_loss: 1.0624, per_image_load_time: 2.102ms, per_image_inference_time: 0.575ms
2022-07-11 19:15:54 - until epoch: 043, best_acc1: 73.936%
2022-07-11 19:15:54 - epoch 044 lr: 0.010000
2022-07-11 19:16:34 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.1124
2022-07-11 19:17:07 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.2031
2022-07-11 19:17:39 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.1959
2022-07-11 19:18:12 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 0.8954
2022-07-11 19:18:44 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.0411
2022-07-11 19:19:17 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 0.9152
2022-07-11 19:19:50 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 0.8365
2022-07-11 19:20:23 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.0841
2022-07-11 19:20:56 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.0780
2022-07-11 19:21:28 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.0748
2022-07-11 19:22:01 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.1282
2022-07-11 19:22:34 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.0770
2022-07-11 19:23:07 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.1328
2022-07-11 19:23:39 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.0364
2022-07-11 19:24:12 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.1021
2022-07-11 19:24:44 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 0.9826
2022-07-11 19:25:17 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 0.9986
2022-07-11 19:25:50 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.1524
2022-07-11 19:26:23 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.3610
2022-07-11 19:26:55 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 0.9931
2022-07-11 19:27:28 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.2396
2022-07-11 19:28:01 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.1306
2022-07-11 19:28:33 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.2306
2022-07-11 19:29:06 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.1331
2022-07-11 19:29:39 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.1372
2022-07-11 19:30:12 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.1033
2022-07-11 19:30:45 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.1856
2022-07-11 19:31:17 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.0283
2022-07-11 19:31:50 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.0194
2022-07-11 19:32:23 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 0.9986
2022-07-11 19:32:55 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.1844
2022-07-11 19:33:28 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 0.9402
2022-07-11 19:34:01 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.0669
2022-07-11 19:34:33 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.3359
2022-07-11 19:35:06 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.0772
2022-07-11 19:35:39 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.3623
2022-07-11 19:36:12 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.2038
2022-07-11 19:36:45 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 0.8990
2022-07-11 19:37:18 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.3578
2022-07-11 19:37:51 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.2577
2022-07-11 19:38:23 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.0363
2022-07-11 19:38:56 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.2079
2022-07-11 19:39:29 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.3697
2022-07-11 19:40:02 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.0395
2022-07-11 19:40:35 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.2647
2022-07-11 19:41:07 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.3017
2022-07-11 19:41:40 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.1646
2022-07-11 19:42:13 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.3132
2022-07-11 19:42:45 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 0.9933
2022-07-11 19:43:17 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.2708
2022-07-11 19:43:19 - train: epoch 044, train_loss: 1.1388
2022-07-11 19:44:34 - eval: epoch: 044, acc1: 73.244%, acc5: 91.548%, test_loss: 1.0674, per_image_load_time: 2.222ms, per_image_inference_time: 0.591ms
2022-07-11 19:44:35 - until epoch: 044, best_acc1: 73.936%
2022-07-11 19:44:35 - epoch 045 lr: 0.010000
2022-07-11 19:45:14 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 0.9206
2022-07-11 19:45:47 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.1446
2022-07-11 19:46:20 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.1814
2022-07-11 19:46:53 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.0095
2022-07-11 19:47:26 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.2372
2022-07-11 19:47:58 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.1033
2022-07-11 19:48:31 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 0.9117
2022-07-11 19:49:04 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.0373
2022-07-11 19:49:36 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.0727
2022-07-11 19:50:09 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.0441
2022-07-11 19:50:42 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.2579
2022-07-11 19:51:14 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.1900
2022-07-11 19:51:47 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.1578
2022-07-11 19:52:20 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.1465
2022-07-11 19:52:53 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.2990
2022-07-11 19:53:25 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.1036
2022-07-11 19:53:58 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.0780
2022-07-11 19:54:31 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.0195
2022-07-11 19:55:03 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.1956
2022-07-11 19:55:36 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.1169
2022-07-11 19:56:09 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.1772
2022-07-11 19:56:42 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.1872
2022-07-11 19:57:14 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.0532
2022-07-11 19:57:47 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.3246
2022-07-11 19:58:19 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.1955
2022-07-11 19:58:52 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.0618
2022-07-11 19:59:25 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.0593
2022-07-11 19:59:58 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.1746
2022-07-11 20:00:30 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.1588
2022-07-11 20:01:03 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.3080
2022-07-11 20:01:36 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.1272
2022-07-11 20:02:08 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.2788
2022-07-11 20:02:41 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.2468
2022-07-11 20:03:14 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 0.9512
2022-07-11 20:03:46 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.3234
2022-07-11 20:04:19 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.2172
2022-07-11 20:04:52 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.1074
2022-07-11 20:05:24 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 0.9602
2022-07-11 20:05:57 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.2880
2022-07-11 20:06:29 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.3086
2022-07-11 20:07:02 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.0264
2022-07-11 20:07:34 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.2475
2022-07-11 20:08:07 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.3348
2022-07-11 20:08:40 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.2905
2022-07-11 20:09:13 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.1163
2022-07-11 20:09:46 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.2334
2022-07-11 20:10:19 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.2442
2022-07-11 20:10:52 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.0486
2022-07-11 20:11:24 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.0909
2022-07-11 20:11:57 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.1687
2022-07-11 20:11:58 - train: epoch 045, train_loss: 1.1349
2022-07-11 20:13:14 - eval: epoch: 045, acc1: 73.236%, acc5: 91.584%, test_loss: 1.0676, per_image_load_time: 2.356ms, per_image_inference_time: 0.583ms
2022-07-11 20:13:14 - until epoch: 045, best_acc1: 73.936%
2022-07-11 20:13:14 - epoch 046 lr: 0.010000
2022-07-11 20:13:54 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 0.9297
2022-07-11 20:14:26 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 0.9985
2022-07-11 20:14:58 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.1390
2022-07-11 20:15:31 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.2289
2022-07-11 20:16:03 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 0.8719
2022-07-11 20:16:35 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.2531
2022-07-11 20:17:07 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.0408
2022-07-11 20:17:39 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.1780
2022-07-11 20:18:11 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.2183
2022-07-11 20:18:44 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 0.9333
2022-07-11 20:19:16 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.1546
2022-07-11 20:19:48 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.1597
2022-07-11 20:20:21 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.0624
2022-07-11 20:20:53 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.3102
2022-07-11 20:21:26 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.1267
2022-07-11 20:21:59 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.1061
2022-07-11 20:22:31 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.0068
2022-07-11 20:23:04 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.2392
2022-07-11 20:23:36 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 0.9273
2022-07-11 20:24:08 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.1865
2022-07-11 20:24:41 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.2697
2022-07-11 20:25:13 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 0.9470
2022-07-11 20:25:46 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.2041
2022-07-11 20:26:18 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.1343
2022-07-11 20:26:51 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.2765
2022-07-11 20:27:23 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.2882
2022-07-11 20:27:56 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.0350
2022-07-11 20:28:28 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 0.9420
2022-07-11 20:29:00 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.1858
2022-07-11 20:29:33 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 0.9371
2022-07-11 20:30:06 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 0.9573
2022-07-11 20:30:38 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.4202
2022-07-11 20:31:11 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.1290
2022-07-11 20:31:44 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.2401
2022-07-11 20:32:16 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.0730
2022-07-11 20:32:49 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.0469
2022-07-11 20:33:21 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.0827
2022-07-11 20:33:54 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.2950
2022-07-11 20:34:27 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.2580
2022-07-11 20:34:59 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 0.9527
2022-07-11 20:35:32 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.2234
2022-07-11 20:36:04 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.0835
2022-07-11 20:36:37 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.3366
2022-07-11 20:37:10 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.1495
2022-07-11 20:37:43 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.1016
2022-07-11 20:38:15 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.1857
2022-07-11 20:38:48 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.0089
2022-07-11 20:39:21 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.1000
2022-07-11 20:39:53 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.2353
2022-07-11 20:40:25 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.1165
2022-07-11 20:40:27 - train: epoch 046, train_loss: 1.1312
2022-07-11 20:41:43 - eval: epoch: 046, acc1: 73.316%, acc5: 91.348%, test_loss: 1.0745, per_image_load_time: 2.005ms, per_image_inference_time: 0.558ms
2022-07-11 20:41:43 - until epoch: 046, best_acc1: 73.936%
2022-07-11 20:41:43 - epoch 047 lr: 0.010000
2022-07-11 20:42:23 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 0.9973
2022-07-11 20:42:55 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.2253
2022-07-11 20:43:27 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 0.9457
2022-07-11 20:43:59 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.0462
2022-07-11 20:44:32 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.2431
2022-07-11 20:45:04 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.1184
2022-07-11 20:45:36 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.1997
2022-07-11 20:46:08 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 0.9739
2022-07-11 20:46:41 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.2419
2022-07-11 20:47:13 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.1944
2022-07-11 20:47:46 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.1479
2022-07-11 20:48:18 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.0282
2022-07-11 20:48:51 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.0142
2022-07-11 20:49:23 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.1614
2022-07-11 20:49:56 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.1001
2022-07-11 20:50:28 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 0.9783
2022-07-11 20:51:01 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 0.9485
2022-07-11 20:51:33 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.0917
2022-07-11 20:52:06 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.0248
2022-07-11 20:52:38 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.0768
2022-07-11 20:53:11 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.2383
2022-07-11 20:53:44 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.2253
2022-07-11 20:54:16 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.1302
2022-07-11 20:54:49 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 0.9881
2022-07-11 20:55:22 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.1200
2022-07-11 20:55:54 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.3661
2022-07-11 20:56:27 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.1751
2022-07-11 20:57:00 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.2580
2022-07-11 20:57:33 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.1625
2022-07-11 20:58:06 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.1889
2022-07-11 20:58:39 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.0173
2022-07-11 20:59:12 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.3809
2022-07-11 20:59:45 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.0438
2022-07-11 21:00:17 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.0996
2022-07-11 21:00:50 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.2305
2022-07-11 21:01:23 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.1733
2022-07-11 21:01:55 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.2800
2022-07-11 21:02:27 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.1938
2022-07-11 21:03:00 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.1670
2022-07-11 21:03:33 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.2838
2022-07-11 21:04:06 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.1454
2022-07-11 21:04:38 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.1643
2022-07-11 21:05:11 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.0045
2022-07-11 21:05:44 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.1035
2022-07-11 21:06:17 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.3112
2022-07-11 21:06:49 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.1797
2022-07-11 21:07:22 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.2682
2022-07-11 21:07:54 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.0148
2022-07-11 21:08:27 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.2535
2022-07-11 21:08:59 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.0801
2022-07-11 21:09:01 - train: epoch 047, train_loss: 1.1310
2022-07-11 21:10:16 - eval: epoch: 047, acc1: 73.270%, acc5: 91.562%, test_loss: 1.0687, per_image_load_time: 2.096ms, per_image_inference_time: 0.582ms
2022-07-11 21:10:17 - until epoch: 047, best_acc1: 73.936%
2022-07-11 21:10:17 - epoch 048 lr: 0.010000
2022-07-11 21:10:57 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.3313
2022-07-11 21:11:29 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.4489
2022-07-11 21:12:02 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.0879
2022-07-11 21:12:35 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.0192
2022-07-11 21:13:07 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.0277
2022-07-11 21:13:40 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.1306
2022-07-11 21:14:12 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.1316
2022-07-11 21:14:45 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.1686
2022-07-11 21:15:17 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.2165
2022-07-11 21:15:50 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.1878
2022-07-11 21:16:23 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.1230
2022-07-11 21:16:55 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.2051
2022-07-11 21:17:28 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 0.8995
2022-07-11 21:18:01 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 0.9722
2022-07-11 21:18:33 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.1548
2022-07-11 21:19:06 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.0780
2022-07-11 21:19:38 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.2034
2022-07-11 21:20:11 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.1183
2022-07-11 21:20:44 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.1560
2022-07-11 21:21:16 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.3922
2022-07-11 21:21:49 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.1388
2022-07-11 21:22:22 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.2132
2022-07-11 21:22:54 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.0502
2022-07-11 21:23:27 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.2483
2022-07-11 21:24:00 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.1034
2022-07-11 21:24:32 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.4071
2022-07-11 21:25:05 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.2685
2022-07-11 21:25:38 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.0338
2022-07-11 21:26:11 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.3224
2022-07-11 21:26:44 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.0680
2022-07-11 21:27:16 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.0491
2022-07-11 21:27:49 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.0163
2022-07-11 21:28:22 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.2294
2022-07-11 21:28:54 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.1077
2022-07-11 21:29:27 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.4130
2022-07-11 21:30:00 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.2198
2022-07-11 21:30:33 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.1065
2022-07-11 21:31:06 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.0461
2022-07-11 21:31:39 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.3781
2022-07-11 21:32:11 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.0302
2022-07-11 21:32:44 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.2738
2022-07-11 21:33:16 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.0795
2022-07-11 21:33:49 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.1411
2022-07-11 21:34:21 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.0402
2022-07-11 21:34:54 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.1350
2022-07-11 21:35:27 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.0387
2022-07-11 21:36:00 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.0972
2022-07-11 21:36:32 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.3378
2022-07-11 21:37:05 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.2925
2022-07-11 21:37:37 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.1467
2022-07-11 21:37:39 - train: epoch 048, train_loss: 1.1317
2022-07-11 21:38:53 - eval: epoch: 048, acc1: 73.216%, acc5: 91.560%, test_loss: 1.0736, per_image_load_time: 1.931ms, per_image_inference_time: 0.597ms
2022-07-11 21:38:54 - until epoch: 048, best_acc1: 73.936%
2022-07-11 21:38:54 - epoch 049 lr: 0.010000
2022-07-11 21:39:34 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.2554
2022-07-11 21:40:06 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.1217
2022-07-11 21:40:38 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.0349
2022-07-11 21:41:11 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.1185
2022-07-11 21:41:43 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.0754
2022-07-11 21:42:15 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 0.9934
2022-07-11 21:42:48 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.1060
2022-07-11 21:43:20 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.3637
2022-07-11 21:43:52 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.0358
2022-07-11 21:44:24 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.1405
2022-07-11 21:44:57 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.0229
2022-07-11 21:45:29 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 0.8935
2022-07-11 21:46:01 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.2015
2022-07-11 21:46:34 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.1552
2022-07-11 21:47:06 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.0306
2022-07-11 21:47:38 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.1962
2022-07-11 21:48:11 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.2148
2022-07-11 21:48:43 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.0230
2022-07-11 21:49:15 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.0746
2022-07-11 21:49:46 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.0763
2022-07-11 21:50:18 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.1010
2022-07-11 21:50:50 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.1338
2022-07-11 21:51:22 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.0669
2022-07-11 21:51:55 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.3452
2022-07-11 21:52:27 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.1246
2022-07-11 21:52:59 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.1005
2022-07-11 21:53:32 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.0305
2022-07-11 21:54:04 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.0967
2022-07-11 21:54:36 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.2441
2022-07-11 21:55:08 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.2969
2022-07-11 21:55:41 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.2158
2022-07-11 21:56:14 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.2288
2022-07-11 21:56:46 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.1743
2022-07-11 21:57:19 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.3159
2022-07-11 21:57:51 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.2336
2022-07-11 21:58:24 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.3115
2022-07-11 21:58:56 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.2011
2022-07-11 21:59:29 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.2839
2022-07-11 22:00:02 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.2802
2022-07-11 22:00:34 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.1759
2022-07-11 22:01:07 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.0358
2022-07-11 22:01:40 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.2219
2022-07-11 22:02:12 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.4219
2022-07-11 22:02:45 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.1636
2022-07-11 22:03:17 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.0722
2022-07-11 22:03:49 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.3153
2022-07-11 22:04:22 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.3389
2022-07-11 22:04:55 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 0.9901
2022-07-11 22:05:27 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 0.9468
2022-07-11 22:06:00 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.1031
2022-07-11 22:06:01 - train: epoch 049, train_loss: 1.1266
2022-07-11 22:07:16 - eval: epoch: 049, acc1: 73.504%, acc5: 91.442%, test_loss: 1.0654, per_image_load_time: 2.323ms, per_image_inference_time: 0.582ms
2022-07-11 22:07:17 - until epoch: 049, best_acc1: 73.936%
2022-07-11 22:07:17 - epoch 050 lr: 0.010000
2022-07-11 22:07:56 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.2735
2022-07-11 22:08:29 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.0243
2022-07-11 22:09:01 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.0661
2022-07-11 22:09:34 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.0082
2022-07-11 22:10:07 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.0998
2022-07-11 22:10:39 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.2844
2022-07-11 22:11:12 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 0.9273
2022-07-11 22:11:45 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 0.9907
2022-07-11 22:12:18 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.0626
2022-07-11 22:12:51 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.1861
2022-07-11 22:13:23 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.2218
2022-07-11 22:13:56 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.0880
2022-07-11 22:14:28 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.0114
2022-07-11 22:15:01 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.1217
2022-07-11 22:15:34 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.1639
2022-07-11 22:16:06 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.0903
2022-07-11 22:16:39 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.1377
2022-07-11 22:17:12 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.0796
2022-07-11 22:17:44 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.0764
2022-07-11 22:18:17 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.1318
2022-07-11 22:18:50 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 0.9226
2022-07-11 22:19:22 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.2239
2022-07-11 22:19:55 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.0796
2022-07-11 22:20:28 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.0689
2022-07-11 22:21:01 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.2061
2022-07-11 22:21:33 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.1426
2022-07-11 22:22:06 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.1649
2022-07-11 22:22:39 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.2208
2022-07-11 22:23:12 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.2067
2022-07-11 22:23:45 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.2400
2022-07-11 22:24:17 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.0810
2022-07-11 22:24:50 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.0605
2022-07-11 22:25:23 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.0623
2022-07-11 22:25:55 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.0141
2022-07-11 22:26:28 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.1341
2022-07-11 22:27:00 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.1636
2022-07-11 22:27:33 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.1201
2022-07-11 22:28:05 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 0.9584
2022-07-11 22:28:38 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.0124
2022-07-11 22:29:10 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.3110
2022-07-11 22:29:43 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.0252
2022-07-11 22:30:16 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.0864
2022-07-11 22:30:48 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.0319
2022-07-11 22:31:21 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.1590
2022-07-11 22:31:54 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.0325
2022-07-11 22:32:27 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.1739
2022-07-11 22:32:59 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.1679
2022-07-11 22:33:33 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 0.9891
2022-07-11 22:34:06 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.0735
2022-07-11 22:34:38 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.0218
2022-07-11 22:34:40 - train: epoch 050, train_loss: 1.1240
2022-07-11 22:35:54 - eval: epoch: 050, acc1: 73.334%, acc5: 91.512%, test_loss: 1.0711, per_image_load_time: 2.219ms, per_image_inference_time: 0.574ms
2022-07-11 22:35:55 - until epoch: 050, best_acc1: 73.936%
2022-07-11 22:35:55 - epoch 051 lr: 0.010000
2022-07-11 22:36:34 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.1760
2022-07-11 22:37:07 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.3493
2022-07-11 22:37:40 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.1883
2022-07-11 22:38:12 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.0412
