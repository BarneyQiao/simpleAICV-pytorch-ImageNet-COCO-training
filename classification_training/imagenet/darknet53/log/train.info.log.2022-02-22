2022-02-22 08:07:49 - epoch 013 lr: 0.1
2022-02-22 08:08:29 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.0167
2022-02-22 08:09:01 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.0719
2022-02-22 08:09:33 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.2119
2022-02-22 08:10:04 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 1.8536
2022-02-22 08:10:36 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.1913
2022-02-22 08:11:08 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.0936
2022-02-22 08:11:40 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.2297
2022-02-22 08:12:13 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.1546
2022-02-22 08:12:45 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.1111
2022-02-22 08:13:18 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.1051
2022-02-22 08:13:50 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.2360
2022-02-22 08:14:23 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.2020
2022-02-22 08:14:55 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 1.9527
2022-02-22 08:15:28 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.2272
2022-02-22 08:16:00 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.2542
2022-02-22 08:16:32 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.1058
2022-02-22 08:17:05 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.1893
2022-02-22 08:17:37 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.0492
2022-02-22 08:18:10 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.1688
2022-02-22 08:18:42 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.2953
2022-02-22 08:19:15 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.3717
2022-02-22 08:19:47 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.0542
2022-02-22 08:20:20 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.2764
2022-02-22 08:20:52 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.2516
2022-02-22 08:21:24 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.0640
2022-02-22 08:21:57 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.2028
2022-02-22 08:22:30 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.2443
2022-02-22 08:23:03 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.4470
2022-02-22 08:23:35 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.1847
2022-02-22 08:24:08 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 1.9945
2022-02-22 08:24:41 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 1.8936
2022-02-22 08:25:13 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.2722
2022-02-22 08:25:46 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.0235
2022-02-22 08:26:19 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.1848
2022-02-22 08:26:52 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.1176
2022-02-22 08:27:25 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.2040
2022-02-22 08:27:58 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 1.9555
2022-02-22 08:28:31 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.2834
2022-02-22 08:29:04 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.3036
2022-02-22 08:29:37 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 1.9898
2022-02-22 08:30:10 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.0586
2022-02-22 08:30:44 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.2318
2022-02-22 08:31:17 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.1243
2022-02-22 08:31:50 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.1260
2022-02-22 08:32:24 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.0674
2022-02-22 08:32:58 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.1841
2022-02-22 08:33:31 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.2778
2022-02-22 08:34:05 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.1413
2022-02-22 08:34:39 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.1491
2022-02-22 08:35:13 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.2900
2022-02-22 08:35:14 - train: epoch 013, train_loss: 2.1736
2022-02-22 08:36:33 - eval: epoch: 013, acc1: 56.550%, acc5: 81.006%, test_loss: 1.8331, per_image_load_time: 1.934ms, per_image_inference_time: 0.558ms
2022-02-22 08:36:34 - until epoch: 013, best_acc1: 56.550%
2022-02-22 08:36:34 - epoch 014 lr: 0.1
2022-02-22 08:37:12 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.2409
2022-02-22 08:37:44 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.1749
2022-02-22 08:38:16 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.0382
2022-02-22 08:38:48 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 1.9825
2022-02-22 08:39:20 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.0517
2022-02-22 08:39:53 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.1837
2022-02-22 08:40:25 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 1.8832
2022-02-22 08:40:57 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.0245
2022-02-22 08:41:30 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 1.9313
2022-02-22 08:42:02 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.1883
2022-02-22 08:42:35 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.0872
2022-02-22 08:43:07 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.1042
2022-02-22 08:43:40 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.1962
2022-02-22 08:44:13 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.1196
2022-02-22 08:44:46 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.2724
2022-02-22 08:45:18 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 1.8484
2022-02-22 08:45:51 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.2884
2022-02-22 08:46:24 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.2327
2022-02-22 08:46:57 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 1.9779
2022-02-22 08:47:30 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.0849
2022-02-22 08:48:03 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.1171
2022-02-22 08:48:36 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.0649
2022-02-22 08:49:09 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.1866
2022-02-22 08:49:42 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.1843
2022-02-22 08:50:15 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 1.9932
2022-02-22 08:50:48 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.2648
2022-02-22 08:51:21 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.2575
2022-02-22 08:51:54 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.2688
2022-02-22 08:52:27 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.1519
2022-02-22 08:53:00 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.2331
2022-02-22 08:53:33 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 1.9950
2022-02-22 08:54:06 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.2217
2022-02-22 08:54:39 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.0187
2022-02-22 08:55:13 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.0853
2022-02-22 08:55:46 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.0281
2022-02-22 08:56:19 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.0189
2022-02-22 08:56:52 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.0522
2022-02-22 08:57:26 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.2524
2022-02-22 08:57:59 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.0619
2022-02-22 08:58:33 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.2657
2022-02-22 08:59:06 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.1558
2022-02-22 08:59:40 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.1336
2022-02-22 09:00:13 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.1199
2022-02-22 09:00:47 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.2558
2022-02-22 09:01:21 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 1.9625
2022-02-22 09:01:55 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.1947
2022-02-22 09:02:30 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.0337
2022-02-22 09:03:05 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.1229
2022-02-22 09:03:40 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.1485
2022-02-22 09:04:15 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.2146
2022-02-22 09:04:16 - train: epoch 014, train_loss: 2.1507
2022-02-22 09:05:34 - eval: epoch: 014, acc1: 56.796%, acc5: 81.428%, test_loss: 1.8219, per_image_load_time: 2.141ms, per_image_inference_time: 0.573ms
2022-02-22 09:05:35 - until epoch: 014, best_acc1: 56.796%
2022-02-22 09:05:35 - epoch 015 lr: 0.1
2022-02-22 09:06:13 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.0416
2022-02-22 09:06:45 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.2170
2022-02-22 09:07:17 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.2940
2022-02-22 09:07:48 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.1222
2022-02-22 09:08:20 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 1.9313
2022-02-22 09:08:52 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.2348
2022-02-22 09:09:24 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.3351
2022-02-22 09:09:56 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 1.9708
2022-02-22 09:10:29 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.3310
2022-02-22 09:11:01 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.1734
2022-02-22 09:11:33 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.0292
2022-02-22 09:12:05 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.1662
2022-02-22 09:12:37 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.2482
2022-02-22 09:13:10 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.2829
2022-02-22 09:13:42 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.2438
2022-02-22 09:14:15 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.1547
2022-02-22 09:14:47 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.1741
2022-02-22 09:15:19 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.2102
2022-02-22 09:15:52 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.1125
2022-02-22 09:16:25 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.0531
2022-02-22 09:16:58 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.2991
2022-02-22 09:17:30 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.2594
2022-02-22 09:18:03 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.1250
2022-02-22 09:18:36 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.1828
2022-02-22 09:19:09 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.4621
2022-02-22 09:19:42 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 1.9634
2022-02-22 09:20:15 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.2204
2022-02-22 09:20:48 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.1520
2022-02-22 09:21:21 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.1411
2022-02-22 09:21:54 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 1.9672
2022-02-22 09:22:27 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.1584
2022-02-22 09:23:00 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.2159
2022-02-22 09:23:33 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 1.9526
2022-02-22 09:24:06 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.0755
2022-02-22 09:24:39 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.3794
2022-02-22 09:25:12 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.0744
2022-02-22 09:25:45 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 1.8864
2022-02-22 09:26:19 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.1413
2022-02-22 09:26:52 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.1653
2022-02-22 09:27:25 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.3672
2022-02-22 09:27:58 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.2084
2022-02-22 09:28:32 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.1636
2022-02-22 09:29:05 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.0045
2022-02-22 09:29:39 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.1225
2022-02-22 09:30:12 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.0764
2022-02-22 09:30:47 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.2661
2022-02-22 09:31:21 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.1180
2022-02-22 09:31:56 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.3189
2022-02-22 09:32:31 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.0285
2022-02-22 09:33:06 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.0963
2022-02-22 09:33:07 - train: epoch 015, train_loss: 2.1376
2022-02-22 09:34:26 - eval: epoch: 015, acc1: 56.270%, acc5: 80.764%, test_loss: 1.8563, per_image_load_time: 2.507ms, per_image_inference_time: 0.552ms
2022-02-22 09:34:27 - until epoch: 015, best_acc1: 56.796%
2022-02-22 09:34:27 - epoch 016 lr: 0.1
2022-02-22 09:35:05 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.1179
2022-02-22 09:35:38 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.1666
2022-02-22 09:36:10 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.0650
2022-02-22 09:36:43 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.2166
2022-02-22 09:37:15 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 1.8879
2022-02-22 09:37:48 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.0513
2022-02-22 09:38:21 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 1.8010
2022-02-22 09:38:53 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.1452
2022-02-22 09:39:26 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.3080
2022-02-22 09:39:58 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 1.9995
2022-02-22 09:40:31 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 1.8813
2022-02-22 09:41:04 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 1.9603
2022-02-22 09:41:36 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.1666
2022-02-22 09:42:09 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 1.9302
2022-02-22 09:42:42 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.2972
2022-02-22 09:43:14 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.1742
2022-02-22 09:43:47 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.4396
2022-02-22 09:44:19 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.4930
2022-02-22 09:44:52 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.0984
2022-02-22 09:45:25 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 1.9256
2022-02-22 09:45:58 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.3387
2022-02-22 09:46:31 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.1538
2022-02-22 09:47:03 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.2710
2022-02-22 09:47:36 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.1665
2022-02-22 09:48:09 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.0911
2022-02-22 09:48:42 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.1639
2022-02-22 09:49:15 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.0340
2022-02-22 09:49:48 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 1.8758
2022-02-22 09:50:21 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 1.9713
2022-02-22 09:50:55 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.3290
2022-02-22 09:51:28 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.3215
2022-02-22 09:52:00 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.1401
2022-02-22 09:52:33 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.4598
2022-02-22 09:53:06 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.1197
2022-02-22 09:53:39 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 1.9895
2022-02-22 09:54:12 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 1.9947
2022-02-22 09:54:45 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.2205
2022-02-22 09:55:17 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.5703
2022-02-22 09:55:51 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.0559
2022-02-22 09:56:24 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.1736
2022-02-22 09:56:57 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.0123
2022-02-22 09:57:30 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.0648
2022-02-22 09:58:03 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 1.9762
2022-02-22 09:58:37 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 1.9322
2022-02-22 09:59:11 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.0030
2022-02-22 09:59:45 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 1.9628
2022-02-22 10:00:19 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.2791
2022-02-22 10:00:54 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.1206
2022-02-22 10:01:29 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.1943
2022-02-22 10:02:04 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.1535
2022-02-22 10:02:06 - train: epoch 016, train_loss: 2.1248
2022-02-22 10:03:23 - eval: epoch: 016, acc1: 57.102%, acc5: 81.650%, test_loss: 1.8029, per_image_load_time: 2.305ms, per_image_inference_time: 0.579ms
2022-02-22 10:03:24 - until epoch: 016, best_acc1: 57.102%
2022-02-22 10:03:24 - epoch 017 lr: 0.1
2022-02-22 10:04:02 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 1.9540
2022-02-22 10:04:34 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.2152
2022-02-22 10:05:05 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.3132
2022-02-22 10:05:37 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.1204
2022-02-22 10:06:08 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.1842
2022-02-22 10:06:41 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.3398
2022-02-22 10:07:12 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.0016
2022-02-22 10:07:44 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.2315
2022-02-22 10:08:17 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.0793
2022-02-22 10:08:49 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 1.9993
2022-02-22 10:09:22 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.3265
2022-02-22 10:09:55 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 1.9401
2022-02-22 10:10:27 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.2508
2022-02-22 10:11:00 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.0192
2022-02-22 10:11:33 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 1.8754
2022-02-22 10:12:05 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 1.9771
2022-02-22 10:12:38 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.1403
2022-02-22 10:13:11 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.0417
2022-02-22 10:13:44 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.1067
2022-02-22 10:14:17 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.0444
2022-02-22 10:14:50 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.1599
2022-02-22 10:15:22 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 1.9759
2022-02-22 10:15:55 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.2674
2022-02-22 10:16:28 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.1308
2022-02-22 10:17:01 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.3520
2022-02-22 10:17:33 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.0333
2022-02-22 10:18:05 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.2719
2022-02-22 10:18:38 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.1223
2022-02-22 10:19:10 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.3573
2022-02-22 10:19:43 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 1.9157
2022-02-22 10:20:16 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.3727
2022-02-22 10:20:49 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 1.9422
2022-02-22 10:21:21 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.4054
2022-02-22 10:21:54 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 1.9816
2022-02-22 10:22:27 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.0653
2022-02-22 10:23:00 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.1478
2022-02-22 10:23:33 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.0292
2022-02-22 10:24:06 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.2341
2022-02-22 10:24:39 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 1.9068
2022-02-22 10:25:12 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.0952
2022-02-22 10:25:45 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.1329
2022-02-22 10:26:19 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.0106
2022-02-22 10:26:52 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.1625
2022-02-22 10:27:25 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.3171
2022-02-22 10:27:58 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.2696
2022-02-22 10:28:32 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.0819
2022-02-22 10:29:07 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.2503
2022-02-22 10:29:41 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.1612
2022-02-22 10:30:16 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.0499
2022-02-22 10:30:51 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 1.9343
2022-02-22 10:30:52 - train: epoch 017, train_loss: 2.1117
2022-02-22 10:32:08 - eval: epoch: 017, acc1: 57.966%, acc5: 82.220%, test_loss: 1.7567, per_image_load_time: 2.282ms, per_image_inference_time: 0.589ms
2022-02-22 10:32:09 - until epoch: 017, best_acc1: 57.966%
2022-02-22 10:32:09 - epoch 018 lr: 0.1
2022-02-22 10:32:47 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.0586
2022-02-22 10:33:19 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.1761
2022-02-22 10:33:51 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.1493
2022-02-22 10:34:23 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.1542
2022-02-22 10:34:55 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.1427
2022-02-22 10:35:28 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.1524
2022-02-22 10:36:01 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 1.9091
2022-02-22 10:36:33 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.1646
2022-02-22 10:37:05 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.2270
2022-02-22 10:37:38 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 1.9352
2022-02-22 10:38:11 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.3922
2022-02-22 10:38:43 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.1674
2022-02-22 10:39:16 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.2939
2022-02-22 10:39:49 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.1579
2022-02-22 10:40:22 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.2310
2022-02-22 10:40:55 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 1.8457
2022-02-22 10:41:28 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.2357
2022-02-22 10:42:01 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.0260
2022-02-22 10:42:34 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.0294
2022-02-22 10:43:07 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.3590
2022-02-22 10:43:40 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.2696
2022-02-22 10:44:13 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.1730
2022-02-22 10:44:46 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.2876
2022-02-22 10:45:19 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.0904
2022-02-22 10:45:52 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 1.9789
2022-02-22 10:46:25 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.0540
2022-02-22 10:46:58 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.2210
2022-02-22 10:47:31 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.0572
2022-02-22 10:48:04 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.0739
2022-02-22 10:48:37 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.1396
2022-02-22 10:49:10 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.4816
2022-02-22 10:49:43 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.2126
2022-02-22 10:50:16 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.0411
2022-02-22 10:50:49 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.2711
2022-02-22 10:51:22 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.1713
2022-02-22 10:51:56 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.1468
2022-02-22 10:52:29 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.4200
2022-02-22 10:53:03 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.3023
2022-02-22 10:53:36 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.1835
2022-02-22 10:54:10 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.2300
2022-02-22 10:54:43 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.3151
2022-02-22 10:55:17 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.0163
2022-02-22 10:55:50 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.0349
2022-02-22 10:56:24 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.0015
2022-02-22 10:56:59 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.1614
2022-02-22 10:57:33 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.0690
2022-02-22 10:58:08 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.2064
2022-02-22 10:58:43 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.0750
2022-02-22 10:59:18 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.0177
2022-02-22 10:59:53 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.2219
2022-02-22 10:59:54 - train: epoch 018, train_loss: 2.1024
2022-02-22 11:01:11 - eval: epoch: 018, acc1: 56.802%, acc5: 81.118%, test_loss: 1.8285, per_image_load_time: 2.433ms, per_image_inference_time: 0.570ms
2022-02-22 11:01:12 - until epoch: 018, best_acc1: 57.966%
2022-02-22 11:01:12 - epoch 019 lr: 0.1
2022-02-22 11:01:51 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 1.7116
2022-02-22 11:02:23 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.0925
2022-02-22 11:02:55 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.2327
2022-02-22 11:03:27 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.2080
2022-02-22 11:04:00 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 1.9890
2022-02-22 11:04:32 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.1981
2022-02-22 11:05:04 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 1.9687
2022-02-22 11:05:36 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.1405
2022-02-22 11:06:09 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.2567
2022-02-22 11:06:41 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.1646
2022-02-22 11:07:14 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 1.9676
2022-02-22 11:07:47 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.1254
2022-02-22 11:08:19 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.0254
2022-02-22 11:08:52 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.0076
2022-02-22 11:09:25 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.4045
2022-02-22 11:09:57 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 1.9464
2022-02-22 11:10:30 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.2690
2022-02-22 11:11:03 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.2031
2022-02-22 11:11:36 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.2724
2022-02-22 11:12:09 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.0879
2022-02-22 11:12:43 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 1.9879
2022-02-22 11:13:16 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 1.9808
2022-02-22 11:13:49 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.0514
2022-02-22 11:14:21 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.1807
2022-02-22 11:14:55 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.0400
2022-02-22 11:15:28 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.0977
2022-02-22 11:16:01 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.0995
2022-02-22 11:16:34 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.0048
2022-02-22 11:17:07 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.3466
2022-02-22 11:17:40 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.3055
2022-02-22 11:18:13 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.1382
2022-02-22 11:18:46 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.9725
2022-02-22 11:19:19 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 1.9921
2022-02-22 11:19:52 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.0428
2022-02-22 11:20:26 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.2563
2022-02-22 11:20:59 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 1.7499
2022-02-22 11:21:32 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.0224
2022-02-22 11:22:05 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.2468
2022-02-22 11:22:39 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.0042
2022-02-22 11:23:12 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.2041
2022-02-22 11:23:46 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.3602
2022-02-22 11:24:19 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.1089
2022-02-22 11:24:53 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 1.9479
2022-02-22 11:25:26 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.1801
2022-02-22 11:26:00 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.3175
2022-02-22 11:26:34 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 1.9594
2022-02-22 11:27:09 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.1330
2022-02-22 11:27:44 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.1498
2022-02-22 11:28:19 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 1.9779
2022-02-22 11:28:53 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.1495
2022-02-22 11:28:55 - train: epoch 019, train_loss: 2.0911
2022-02-22 11:30:12 - eval: epoch: 019, acc1: 57.334%, acc5: 81.702%, test_loss: 1.8006, per_image_load_time: 2.419ms, per_image_inference_time: 0.589ms
2022-02-22 11:30:13 - until epoch: 019, best_acc1: 57.966%
2022-02-22 11:30:13 - epoch 020 lr: 0.1
2022-02-22 11:30:51 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.0778
2022-02-22 11:31:24 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 1.9292
2022-02-22 11:31:56 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.0122
2022-02-22 11:32:29 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.0005
2022-02-22 11:33:01 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.0419
2022-02-22 11:33:33 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.2198
2022-02-22 11:34:06 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 1.6482
2022-02-22 11:34:38 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.2799
2022-02-22 11:35:11 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.1418
2022-02-22 11:35:44 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.0107
2022-02-22 11:36:17 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.0474
2022-02-22 11:36:50 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 1.9340
2022-02-22 11:37:23 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 1.9540
2022-02-22 11:37:56 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.0240
2022-02-22 11:38:29 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 1.9909
2022-02-22 11:39:02 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.2126
2022-02-22 11:39:35 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.0514
2022-02-22 11:40:08 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.0475
2022-02-22 11:40:40 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 1.9372
2022-02-22 11:41:13 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.0345
2022-02-22 11:41:46 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.3908
2022-02-22 11:42:19 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 1.9066
2022-02-22 11:42:52 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 1.9993
2022-02-22 11:43:25 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.2774
2022-02-22 11:43:58 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.0025
2022-02-22 11:44:31 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.0140
2022-02-22 11:45:03 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.0656
2022-02-22 11:45:36 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.1280
2022-02-22 11:46:09 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.2431
2022-02-22 11:46:42 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.1849
2022-02-22 11:47:15 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.2346
2022-02-22 11:47:48 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.2909
2022-02-22 11:48:21 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.0481
2022-02-22 11:48:54 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.2265
2022-02-22 11:49:27 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.0911
2022-02-22 11:50:01 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.0723
2022-02-22 11:50:33 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.0677
2022-02-22 11:51:07 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.1251
2022-02-22 11:51:39 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.2443
2022-02-22 11:52:12 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.2454
2022-02-22 11:52:46 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 1.9832
2022-02-22 11:53:19 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.0506
2022-02-22 11:53:52 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.2416
2022-02-22 11:54:26 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 1.9394
2022-02-22 11:55:00 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.0704
2022-02-22 11:55:34 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.1247
2022-02-22 11:56:08 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 1.9866
2022-02-22 11:56:43 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 1.9654
2022-02-22 11:57:18 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.3112
2022-02-22 11:57:53 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 1.8804
2022-02-22 11:57:55 - train: epoch 020, train_loss: 2.0827
2022-02-22 11:59:10 - eval: epoch: 020, acc1: 58.260%, acc5: 82.286%, test_loss: 1.7456, per_image_load_time: 1.697ms, per_image_inference_time: 0.602ms
2022-02-22 11:59:12 - until epoch: 020, best_acc1: 58.260%
2022-02-22 11:59:12 - epoch 021 lr: 0.1
2022-02-22 11:59:50 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.0703
2022-02-22 12:00:22 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.0826
2022-02-22 12:00:54 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.9712
2022-02-22 12:01:26 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 1.9112
2022-02-22 12:01:58 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 1.9669
2022-02-22 12:02:31 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.0154
2022-02-22 12:03:03 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 1.9749
2022-02-22 12:03:36 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.0540
2022-02-22 12:04:09 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.0389
2022-02-22 12:04:42 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 1.9512
2022-02-22 12:05:14 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 1.9992
2022-02-22 12:05:47 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 1.8525
2022-02-22 12:06:20 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.0323
2022-02-22 12:06:53 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.0403
2022-02-22 12:07:26 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.0655
2022-02-22 12:07:59 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.1490
2022-02-22 12:08:32 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.2395
2022-02-22 12:09:04 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.1576
2022-02-22 12:09:37 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.4942
2022-02-22 12:10:10 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.1679
2022-02-22 12:10:43 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.0315
2022-02-22 12:11:16 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.1591
2022-02-22 12:11:49 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.1436
2022-02-22 12:12:22 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 1.8857
2022-02-22 12:12:55 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.1574
2022-02-22 12:13:29 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.3761
2022-02-22 12:14:02 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.0387
2022-02-22 12:14:35 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 1.9607
2022-02-22 12:15:08 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.1785
2022-02-22 12:15:41 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.1213
2022-02-22 12:16:14 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 1.9257
2022-02-22 12:16:47 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 1.9255
2022-02-22 12:17:21 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.1375
2022-02-22 12:17:54 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.2358
2022-02-22 12:18:27 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.3360
2022-02-22 12:19:00 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.0583
2022-02-22 12:19:34 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.2222
2022-02-22 12:20:07 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.1769
2022-02-22 12:20:40 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.0861
2022-02-22 12:21:14 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.2232
2022-02-22 12:21:47 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.0383
2022-02-22 12:22:20 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.2402
2022-02-22 12:22:54 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.0471
2022-02-22 12:23:28 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.0522
2022-02-22 12:24:02 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.1426
2022-02-22 12:24:36 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 1.8968
2022-02-22 12:25:11 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.3083
2022-02-22 12:25:46 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.1917
2022-02-22 12:26:21 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 1.8148
2022-02-22 12:26:55 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.0762
2022-02-22 12:26:57 - train: epoch 021, train_loss: 2.0746
2022-02-22 12:28:12 - eval: epoch: 021, acc1: 56.554%, acc5: 81.334%, test_loss: 1.8151, per_image_load_time: 1.272ms, per_image_inference_time: 0.583ms
2022-02-22 12:28:13 - until epoch: 021, best_acc1: 58.260%
2022-02-22 12:28:13 - epoch 022 lr: 0.1
2022-02-22 12:28:53 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 1.6776
2022-02-22 12:29:25 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 1.9725
2022-02-22 12:29:57 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 1.9176
2022-02-22 12:30:29 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 1.9960
2022-02-22 12:31:01 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.0450
2022-02-22 12:31:34 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.1149
2022-02-22 12:32:06 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.3542
2022-02-22 12:32:38 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.1614
2022-02-22 12:33:11 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.2894
2022-02-22 12:33:43 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 1.9487
2022-02-22 12:34:16 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 1.9111
2022-02-22 12:34:49 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.7026
2022-02-22 12:35:22 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.0541
2022-02-22 12:35:54 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 1.9734
2022-02-22 12:36:27 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.0246
2022-02-22 12:36:59 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.0145
2022-02-22 12:37:31 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 1.9652
2022-02-22 12:38:04 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.5283
2022-02-22 12:38:37 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.1561
2022-02-22 12:39:09 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 1.9709
2022-02-22 12:39:42 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.1571
2022-02-22 12:40:15 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 1.9082
2022-02-22 12:40:47 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.2414
2022-02-22 12:41:20 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.1376
2022-02-22 12:41:53 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.0089
2022-02-22 12:42:26 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 1.7769
2022-02-22 12:42:58 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 1.9500
2022-02-22 12:43:31 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.1261
2022-02-22 12:44:04 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 1.9217
2022-02-22 12:44:37 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.1122
2022-02-22 12:45:09 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.4134
2022-02-22 12:45:42 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.0236
2022-02-22 12:46:15 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.1341
2022-02-22 12:46:48 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 1.9976
2022-02-22 12:47:21 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.0870
2022-02-22 12:47:54 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.2791
2022-02-22 12:48:27 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.3116
2022-02-22 12:49:00 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.1565
2022-02-22 12:49:33 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.1065
2022-02-22 12:50:06 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.0704
2022-02-22 12:50:40 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 1.9483
2022-02-22 12:51:13 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.2168
2022-02-22 12:51:46 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.1173
2022-02-22 12:52:20 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 1.8431
2022-02-22 12:52:54 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.0643
2022-02-22 12:53:28 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.1941
2022-02-22 12:54:02 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.0439
2022-02-22 12:54:36 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.0283
2022-02-22 12:55:11 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.1342
2022-02-22 12:55:46 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.0486
2022-02-22 12:55:48 - train: epoch 022, train_loss: 2.0692
2022-02-22 12:57:05 - eval: epoch: 022, acc1: 57.060%, acc5: 81.550%, test_loss: 1.8018, per_image_load_time: 1.875ms, per_image_inference_time: 0.594ms
2022-02-22 12:57:06 - until epoch: 022, best_acc1: 58.260%
2022-02-22 12:57:06 - epoch 023 lr: 0.1
2022-02-22 12:57:44 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 1.9095
2022-02-22 12:58:16 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 1.8620
2022-02-22 12:58:48 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 1.9078
2022-02-22 12:59:19 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 1.9771
2022-02-22 12:59:51 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.0639
2022-02-22 13:00:23 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.0260
2022-02-22 13:00:55 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 1.8161
2022-02-22 13:01:27 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.0704
2022-02-22 13:01:59 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.2234
2022-02-22 13:02:31 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 1.9184
2022-02-22 13:03:04 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 1.9584
2022-02-22 13:03:36 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.0465
2022-02-22 13:04:08 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.1306
2022-02-22 13:04:41 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 1.9984
2022-02-22 13:05:13 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 1.9406
2022-02-22 13:05:46 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.1131
2022-02-22 13:06:18 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 1.9935
2022-02-22 13:06:50 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 1.9851
2022-02-22 13:07:23 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.0668
2022-02-22 13:07:55 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.7787
2022-02-22 13:08:28 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.0200
2022-02-22 13:09:01 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.9657
2022-02-22 13:09:33 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.1589
2022-02-22 13:10:06 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.2397
2022-02-22 13:10:39 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 1.9701
2022-02-22 13:11:12 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.1071
2022-02-22 13:11:45 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.1135
2022-02-22 13:12:18 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.1564
2022-02-22 13:12:51 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.0305
2022-02-22 13:13:24 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.1975
2022-02-22 13:13:57 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.2399
2022-02-22 13:14:30 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.2392
2022-02-22 13:15:03 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.0416
2022-02-22 13:15:36 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.2004
2022-02-22 13:16:09 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 1.9917
2022-02-22 13:16:42 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.0736
2022-02-22 13:17:15 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 1.9704
2022-02-22 13:17:48 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.1578
2022-02-22 13:18:21 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.1164
2022-02-22 13:18:55 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 1.9802
2022-02-22 13:19:28 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 1.8360
2022-02-22 13:20:02 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.0075
2022-02-22 13:20:35 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.0748
2022-02-22 13:21:09 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.0352
2022-02-22 13:21:43 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 1.8610
2022-02-22 13:22:17 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.0781
2022-02-22 13:22:51 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 1.9656
2022-02-22 13:23:26 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.0481
2022-02-22 13:24:01 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 1.7900
2022-02-22 13:24:36 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 1.9922
2022-02-22 13:24:37 - train: epoch 023, train_loss: 2.0628
2022-02-22 13:25:52 - eval: epoch: 023, acc1: 57.890%, acc5: 82.098%, test_loss: 1.7711, per_image_load_time: 2.245ms, per_image_inference_time: 0.590ms
2022-02-22 13:25:53 - until epoch: 023, best_acc1: 58.260%
2022-02-22 13:25:53 - epoch 024 lr: 0.1
2022-02-22 13:26:32 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 1.9158
2022-02-22 13:27:04 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 1.8589
2022-02-22 13:27:35 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.0936
2022-02-22 13:28:07 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.2278
2022-02-22 13:28:40 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.2153
2022-02-22 13:29:12 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.0711
2022-02-22 13:29:44 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.0221
2022-02-22 13:30:16 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 1.9458
2022-02-22 13:30:49 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.1402
2022-02-22 13:31:21 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.1639
2022-02-22 13:31:54 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.7480
2022-02-22 13:32:26 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 1.8326
2022-02-22 13:32:59 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.2261
2022-02-22 13:33:32 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 1.8713
2022-02-22 13:34:05 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.1705
2022-02-22 13:34:38 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 1.9055
2022-02-22 13:35:11 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.0041
2022-02-22 13:35:44 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.2591
2022-02-22 13:36:17 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 1.8587
2022-02-22 13:36:50 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.1125
2022-02-22 13:37:24 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.0585
2022-02-22 13:37:57 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.0083
2022-02-22 13:38:30 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.0783
2022-02-22 13:39:04 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.0152
2022-02-22 13:39:37 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.1387
2022-02-22 13:40:09 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.2607
2022-02-22 13:40:43 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.2319
2022-02-22 13:41:16 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.1634
2022-02-22 13:41:49 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.0519
2022-02-22 13:42:22 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 1.9611
2022-02-22 13:42:55 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.0632
2022-02-22 13:43:28 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.1077
2022-02-22 13:44:01 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.9613
2022-02-22 13:44:34 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.0143
2022-02-22 13:45:08 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.0795
2022-02-22 13:45:41 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.0034
2022-02-22 13:46:14 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 1.9519
2022-02-22 13:46:47 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 1.9255
2022-02-22 13:47:20 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 1.9846
2022-02-22 13:47:53 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.1110
2022-02-22 13:48:27 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.0935
2022-02-22 13:49:00 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 1.9947
2022-02-22 13:49:34 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.0644
2022-02-22 13:50:07 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.1698
2022-02-22 13:50:41 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 1.9308
2022-02-22 13:51:15 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.1008
2022-02-22 13:51:50 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.0180
2022-02-22 13:52:24 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 1.7466
2022-02-22 13:52:59 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.1810
2022-02-22 13:53:33 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.1058
2022-02-22 13:53:35 - train: epoch 024, train_loss: 2.0531
2022-02-22 13:54:51 - eval: epoch: 024, acc1: 58.450%, acc5: 82.352%, test_loss: 1.7404, per_image_load_time: 0.593ms, per_image_inference_time: 0.567ms
2022-02-22 13:54:52 - until epoch: 024, best_acc1: 58.450%
2022-02-22 13:54:52 - epoch 025 lr: 0.1
2022-02-22 13:55:30 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 1.8360
2022-02-22 13:56:02 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 1.7876
2022-02-22 13:56:34 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.0497
2022-02-22 13:57:07 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.1067
2022-02-22 13:57:39 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 1.8097
2022-02-22 13:58:10 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.0165
2022-02-22 13:58:42 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 1.9109
2022-02-22 13:59:14 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 1.9844
2022-02-22 13:59:47 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 1.8401
2022-02-22 14:00:19 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.1163
2022-02-22 14:00:51 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.0660
2022-02-22 14:01:24 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.1313
2022-02-22 14:01:56 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.1630
2022-02-22 14:02:29 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.0034
2022-02-22 14:03:01 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.1340
2022-02-22 14:03:33 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 1.8332
2022-02-22 14:04:06 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.0855
2022-02-22 14:04:38 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 1.7143
2022-02-22 14:05:10 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 1.9350
2022-02-22 14:05:43 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 1.9533
2022-02-22 14:06:15 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 1.9486
2022-02-22 14:06:48 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 1.8309
2022-02-22 14:07:21 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 1.9587
2022-02-22 14:07:53 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 1.8251
2022-02-22 14:08:26 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.0142
2022-02-22 14:08:59 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.1328
2022-02-22 14:09:32 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 1.9682
2022-02-22 14:10:05 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.1251
2022-02-22 14:10:38 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.3503
2022-02-22 14:11:10 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 1.9425
2022-02-22 14:11:43 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.0976
2022-02-22 14:12:16 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 1.9290
2022-02-22 14:12:49 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.0170
2022-02-22 14:13:21 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 1.9461
2022-02-22 14:13:54 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 1.7836
2022-02-22 14:14:27 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.1701
2022-02-22 14:15:00 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 1.8758
2022-02-22 14:15:33 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.0089
2022-02-22 14:16:06 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.0994
2022-02-22 14:16:39 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 1.9868
2022-02-22 14:17:13 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.3257
2022-02-22 14:17:46 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.1016
2022-02-22 14:18:19 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 1.9198
2022-02-22 14:18:53 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 1.9467
2022-02-22 14:19:27 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.1233
2022-02-22 14:20:01 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 1.8711
2022-02-22 14:20:35 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.0451
2022-02-22 14:21:09 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.0123
2022-02-22 14:21:44 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.1171
2022-02-22 14:22:19 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.3468
2022-02-22 14:22:21 - train: epoch 025, train_loss: 2.0495
2022-02-22 14:23:38 - eval: epoch: 025, acc1: 57.412%, acc5: 81.852%, test_loss: 1.7888, per_image_load_time: 0.999ms, per_image_inference_time: 0.579ms
2022-02-22 14:23:39 - until epoch: 025, best_acc1: 58.450%
2022-02-22 14:23:39 - epoch 026 lr: 0.1
2022-02-22 14:24:18 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 1.9345
2022-02-22 14:24:50 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.9273
2022-02-22 14:25:22 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 1.8764
2022-02-22 14:25:54 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.0544
2022-02-22 14:26:26 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 1.9809
2022-02-22 14:26:58 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.3060
2022-02-22 14:27:31 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.2351
2022-02-22 14:28:03 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 1.8883
2022-02-22 14:28:36 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.3142
2022-02-22 14:29:09 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.0625
2022-02-22 14:29:42 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.3470
2022-02-22 14:30:14 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 1.8775
2022-02-22 14:30:47 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 1.9754
2022-02-22 14:31:20 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.0279
2022-02-22 14:31:52 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 1.8700
2022-02-22 14:32:25 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.0192
2022-02-22 14:32:58 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.0224
2022-02-22 14:33:31 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.1769
2022-02-22 14:34:03 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.0164
2022-02-22 14:34:36 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.0942
2022-02-22 14:35:09 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.1554
2022-02-22 14:35:42 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.1881
2022-02-22 14:36:15 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.0812
2022-02-22 14:36:47 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.1113
2022-02-22 14:37:20 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.1269
2022-02-22 14:37:53 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.0634
2022-02-22 14:38:26 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.0360
2022-02-22 14:38:58 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.0442
2022-02-22 14:39:31 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 1.8912
2022-02-22 14:40:04 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.1267
2022-02-22 14:40:37 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 1.9342
2022-02-22 14:41:10 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 1.9356
2022-02-22 14:41:43 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 1.9433
2022-02-22 14:42:16 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.0533
2022-02-22 14:42:49 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.1533
2022-02-22 14:43:22 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 1.9903
2022-02-22 14:43:55 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 1.8900
2022-02-22 14:44:28 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.3463
2022-02-22 14:45:01 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.1364
2022-02-22 14:45:35 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.2094
2022-02-22 14:46:08 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.1833
2022-02-22 14:46:42 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.0418
2022-02-22 14:47:15 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.1226
2022-02-22 14:47:49 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.2488
2022-02-22 14:48:23 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.2435
2022-02-22 14:48:57 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.1123
2022-02-22 14:49:31 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.0003
2022-02-22 14:50:06 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.1131
2022-02-22 14:50:41 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.1695
2022-02-22 14:51:16 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 1.9560
2022-02-22 14:51:18 - train: epoch 026, train_loss: 2.0452
2022-02-22 14:52:34 - eval: epoch: 026, acc1: 57.032%, acc5: 81.892%, test_loss: 1.7922, per_image_load_time: 2.368ms, per_image_inference_time: 0.606ms
2022-02-22 14:52:35 - until epoch: 026, best_acc1: 58.450%
2022-02-22 14:52:35 - epoch 027 lr: 0.1
2022-02-22 14:53:13 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 1.9491
2022-02-22 14:53:45 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.1088
2022-02-22 14:54:17 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.0259
2022-02-22 14:54:49 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.0861
2022-02-22 14:55:21 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.2160
2022-02-22 14:55:53 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.2038
2022-02-22 14:56:25 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.0931
2022-02-22 14:56:58 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 1.9590
2022-02-22 14:57:30 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 1.9412
2022-02-22 14:58:02 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.0297
2022-02-22 14:58:35 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 1.8940
2022-02-22 14:59:07 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.1703
2022-02-22 14:59:40 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.1427
2022-02-22 15:00:12 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.2005
2022-02-22 15:00:45 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 1.9568
2022-02-22 15:01:17 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.1972
2022-02-22 15:01:50 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 1.8701
2022-02-22 15:02:22 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.1474
2022-02-22 15:02:55 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.1629
2022-02-22 15:03:28 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 1.9853
2022-02-22 15:04:01 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.1991
2022-02-22 15:04:34 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.1514
2022-02-22 15:05:07 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.1717
2022-02-22 15:05:40 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.1405
2022-02-22 15:06:13 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 1.9854
2022-02-22 15:06:45 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.0646
2022-02-22 15:07:18 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.0109
2022-02-22 15:07:51 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.3834
2022-02-22 15:08:24 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.0470
2022-02-22 15:08:57 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.0746
2022-02-22 15:09:30 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 1.8642
2022-02-22 15:10:02 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 1.8366
2022-02-22 15:10:35 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.0912
2022-02-22 15:11:08 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.1609
2022-02-22 15:11:41 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.1825
2022-02-22 15:12:14 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.1007
2022-02-22 15:12:47 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.0102
2022-02-22 15:13:20 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 1.9596
2022-02-22 15:13:53 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 1.9973
2022-02-22 15:14:26 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.1609
2022-02-22 15:14:59 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.2304
2022-02-22 15:15:32 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 1.9355
2022-02-22 15:16:06 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 1.9715
2022-02-22 15:16:40 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.2604
2022-02-22 15:17:14 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 1.9059
2022-02-22 15:17:48 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 1.8041
2022-02-22 15:18:22 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.2704
2022-02-22 15:18:57 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.1490
2022-02-22 15:19:32 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 1.9716
2022-02-22 15:20:06 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.8250
2022-02-22 15:20:08 - train: epoch 027, train_loss: 2.0426
2022-02-22 15:21:25 - eval: epoch: 027, acc1: 59.312%, acc5: 82.962%, test_loss: 1.7056, per_image_load_time: 0.968ms, per_image_inference_time: 0.582ms
2022-02-22 15:21:27 - until epoch: 027, best_acc1: 59.312%
2022-02-22 15:21:27 - epoch 028 lr: 0.1
2022-02-22 15:22:05 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 1.9488
2022-02-22 15:22:36 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.0886
2022-02-22 15:23:08 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 1.9249
2022-02-22 15:23:41 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.0276
2022-02-22 15:24:13 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 1.9253
2022-02-22 15:24:45 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.3344
2022-02-22 15:25:18 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.3838
2022-02-22 15:25:50 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.9783
2022-02-22 15:26:22 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.0350
2022-02-22 15:26:55 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.0479
2022-02-22 15:27:28 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 1.9297
2022-02-22 15:28:00 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 1.8266
2022-02-22 15:28:33 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 1.9457
2022-02-22 15:29:06 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.2505
2022-02-22 15:29:38 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.0951
2022-02-22 15:30:11 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 1.9832
2022-02-22 15:30:44 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.0673
2022-02-22 15:31:17 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 1.8302
2022-02-22 15:31:49 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.0960
2022-02-22 15:32:22 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 1.9907
2022-02-22 15:32:55 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.1487
2022-02-22 15:33:28 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.1076
2022-02-22 15:34:01 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.2079
2022-02-22 15:34:34 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.1771
2022-02-22 15:35:07 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 1.9444
2022-02-22 15:35:40 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 1.9539
2022-02-22 15:36:13 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.1113
2022-02-22 15:36:46 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.0333
2022-02-22 15:37:19 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.0432
2022-02-22 15:37:52 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 1.9223
2022-02-22 15:38:26 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.2418
2022-02-22 15:38:59 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 1.8846
2022-02-22 15:39:32 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 1.9337
2022-02-22 15:40:05 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 1.9932
2022-02-22 15:40:38 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 1.9965
2022-02-22 15:41:11 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 1.8838
2022-02-22 15:41:43 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.0472
2022-02-22 15:42:16 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.8809
2022-02-22 15:42:49 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.2180
2022-02-22 15:43:22 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.1760
2022-02-22 15:43:55 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 1.9787
2022-02-22 15:44:28 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.1184
2022-02-22 15:45:02 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.0301
2022-02-22 15:45:35 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.0300
2022-02-22 15:46:09 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.1060
2022-02-22 15:46:43 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.1169
2022-02-22 15:47:17 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.1812
2022-02-22 15:47:52 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 1.9707
2022-02-22 15:48:27 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.1806
2022-02-22 15:49:02 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 1.7780
2022-02-22 15:49:04 - train: epoch 028, train_loss: 2.0370
2022-02-22 15:50:21 - eval: epoch: 028, acc1: 58.084%, acc5: 82.328%, test_loss: 1.7556, per_image_load_time: 2.307ms, per_image_inference_time: 0.584ms
2022-02-22 15:50:22 - until epoch: 028, best_acc1: 59.312%
2022-02-22 15:50:22 - epoch 029 lr: 0.1
2022-02-22 15:51:00 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 1.9036
2022-02-22 15:51:31 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.0210
2022-02-22 15:52:03 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.0409
2022-02-22 15:52:35 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.1314
2022-02-22 15:53:07 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.0850
2022-02-22 15:53:39 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.0168
2022-02-22 15:54:11 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.9859
2022-02-22 15:54:44 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 1.9786
2022-02-22 15:55:16 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.0472
2022-02-22 15:55:48 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 1.9049
2022-02-22 15:56:21 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 1.8838
2022-02-22 15:56:54 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.1538
2022-02-22 15:57:26 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.0065
2022-02-22 15:57:59 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.3946
2022-02-22 15:58:31 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.0551
2022-02-22 15:59:04 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.1468
2022-02-22 15:59:36 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.0723
2022-02-22 16:00:09 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.1594
2022-02-22 16:00:42 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 1.9520
2022-02-22 16:01:15 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.0570
2022-02-22 16:01:47 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 1.8979
2022-02-22 16:02:20 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.3129
2022-02-22 16:02:53 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 1.9911
2022-02-22 16:03:26 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.0448
2022-02-22 16:03:58 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.1542
2022-02-22 16:04:31 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.2438
2022-02-22 16:05:04 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.0150
2022-02-22 16:05:36 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 1.8642
2022-02-22 16:06:09 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 1.9067
2022-02-22 16:06:42 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 1.8894
2022-02-22 16:07:15 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.0056
2022-02-22 16:07:47 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.1171
2022-02-22 16:08:20 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 1.9211
2022-02-22 16:08:53 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 1.8581
2022-02-22 16:09:26 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.3402
2022-02-22 16:09:59 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.0804
2022-02-22 16:10:32 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.0011
2022-02-22 16:11:05 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.0072
2022-02-22 16:11:39 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 1.7686
2022-02-22 16:12:12 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.0088
2022-02-22 16:12:45 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 1.9209
2022-02-22 16:13:18 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 1.9292
2022-02-22 16:13:52 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.1112
2022-02-22 16:14:25 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.0486
2022-02-22 16:14:59 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.2894
2022-02-22 16:15:33 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.0969
2022-02-22 16:16:07 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 1.7924
2022-02-22 16:16:42 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.1205
2022-02-22 16:17:17 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.1277
2022-02-22 16:17:52 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.9188
2022-02-22 16:17:54 - train: epoch 029, train_loss: 2.0336
2022-02-22 16:19:09 - eval: epoch: 029, acc1: 57.798%, acc5: 82.282%, test_loss: 1.7591, per_image_load_time: 2.312ms, per_image_inference_time: 0.599ms
2022-02-22 16:19:10 - until epoch: 029, best_acc1: 59.312%
2022-02-22 16:19:10 - epoch 030 lr: 0.1
2022-02-22 16:19:49 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.0810
2022-02-22 16:20:20 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.0612
2022-02-22 16:20:52 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 1.8831
2022-02-22 16:21:24 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.0718
2022-02-22 16:21:56 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.0365
2022-02-22 16:22:28 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.0911
2022-02-22 16:23:01 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 1.9611
2022-02-22 16:23:33 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.1052
2022-02-22 16:24:05 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.3096
2022-02-22 16:24:38 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 1.9693
2022-02-22 16:25:10 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 1.8952
2022-02-22 16:25:42 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.1788
2022-02-22 16:26:15 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.0027
2022-02-22 16:26:48 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 1.8911
2022-02-22 16:27:20 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.0538
2022-02-22 16:27:52 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.0114
2022-02-22 16:28:25 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.1260
2022-02-22 16:28:58 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 1.7934
2022-02-22 16:29:31 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.0291
2022-02-22 16:30:03 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 1.8817
2022-02-22 16:30:36 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.1859
2022-02-22 16:31:09 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.1469
2022-02-22 16:31:42 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.2787
2022-02-22 16:32:15 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.0519
2022-02-22 16:32:48 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.2717
2022-02-22 16:33:21 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 1.9758
2022-02-22 16:33:54 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.0955
2022-02-22 16:34:27 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.0041
2022-02-22 16:35:01 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 1.9854
2022-02-22 16:35:34 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.1277
2022-02-22 16:36:07 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.0493
2022-02-22 16:36:40 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 1.9812
2022-02-22 16:37:14 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.2636
2022-02-22 16:37:47 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.0378
2022-02-22 16:38:20 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.1153
2022-02-22 16:38:53 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.0344
2022-02-22 16:39:27 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 1.8661
2022-02-22 16:40:00 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.0548
2022-02-22 16:40:33 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 1.9728
2022-02-22 16:41:06 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 1.7223
2022-02-22 16:41:40 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 1.9123
2022-02-22 16:42:13 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.1513
2022-02-22 16:42:47 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.1334
2022-02-22 16:43:20 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.1606
2022-02-22 16:43:54 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.0714
2022-02-22 16:44:28 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 1.9106
2022-02-22 16:45:02 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 1.8689
2022-02-22 16:45:37 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 1.9361
2022-02-22 16:46:12 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.2135
2022-02-22 16:46:46 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.3382
2022-02-22 16:46:48 - train: epoch 030, train_loss: 2.0314
2022-02-22 16:48:03 - eval: epoch: 030, acc1: 58.704%, acc5: 82.586%, test_loss: 1.7376, per_image_load_time: 1.784ms, per_image_inference_time: 0.599ms
2022-02-22 16:48:04 - until epoch: 030, best_acc1: 59.312%
2022-02-22 16:48:04 - epoch 031 lr: 0.010000000000000002
2022-02-22 16:48:42 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 1.8454
2022-02-22 16:49:14 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.5504
2022-02-22 16:49:46 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.4648
2022-02-22 16:50:18 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.5109
2022-02-22 16:50:50 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.7373
2022-02-22 16:51:23 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.5632
2022-02-22 16:51:55 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.5747
2022-02-22 16:52:27 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.6340
2022-02-22 16:53:00 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.6685
2022-02-22 16:53:32 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 1.9499
2022-02-22 16:54:05 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 1.4478
2022-02-22 16:54:38 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.6413
2022-02-22 16:55:11 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.3495
2022-02-22 16:55:44 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.3441
2022-02-22 16:56:16 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.5617
2022-02-22 16:56:49 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.2916
2022-02-22 16:57:22 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.3355
2022-02-22 16:57:55 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.3300
2022-02-22 16:58:28 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.4204
2022-02-22 16:59:00 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.4431
2022-02-22 16:59:33 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.4095
2022-02-22 17:00:05 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.1913
2022-02-22 17:00:38 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.4737
2022-02-22 17:01:11 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.4752
2022-02-22 17:01:44 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.4256
2022-02-22 17:02:17 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.4803
2022-02-22 17:02:50 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.6345
2022-02-22 17:03:23 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.7495
2022-02-22 17:03:56 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.4930
2022-02-22 17:04:29 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.7236
2022-02-22 17:05:01 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.6646
2022-02-22 17:05:34 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.6980
2022-02-22 17:06:07 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.4812
2022-02-22 17:06:41 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.7147
2022-02-22 17:07:14 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.5228
2022-02-22 17:07:47 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.4896
2022-02-22 17:08:20 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.3737
2022-02-22 17:08:53 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.5281
2022-02-22 17:09:27 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.4993
2022-02-22 17:10:00 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.3553
2022-02-22 17:10:34 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.3501
2022-02-22 17:11:07 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.3954
2022-02-22 17:11:40 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.4222
2022-02-22 17:12:14 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.5081
2022-02-22 17:12:48 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.4419
2022-02-22 17:13:22 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.4700
2022-02-22 17:13:57 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.3528
2022-02-22 17:14:31 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.5656
2022-02-22 17:15:06 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.4521
2022-02-22 17:15:41 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.6411
2022-02-22 17:15:43 - train: epoch 031, train_loss: 1.5130
2022-02-22 17:17:00 - eval: epoch: 031, acc1: 70.790%, acc5: 90.094%, test_loss: 1.1680, per_image_load_time: 2.415ms, per_image_inference_time: 0.581ms
2022-02-22 17:17:01 - until epoch: 031, best_acc1: 70.790%
2022-02-22 17:17:01 - epoch 032 lr: 0.010000000000000002
2022-02-22 17:17:39 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.3092
2022-02-22 17:18:11 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.5304
2022-02-22 17:18:43 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.3200
2022-02-22 17:19:15 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.4591
2022-02-22 17:19:47 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.2770
2022-02-22 17:20:20 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.4458
2022-02-22 17:20:52 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.3620
2022-02-22 17:21:24 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.4658
2022-02-22 17:21:57 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.4105
2022-02-22 17:22:30 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.5124
2022-02-22 17:23:02 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.4499
2022-02-22 17:23:36 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.3267
2022-02-22 17:24:09 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.3768
2022-02-22 17:24:41 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.4958
2022-02-22 17:25:14 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.4058
2022-02-22 17:25:47 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.3975
2022-02-22 17:26:20 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.3960
2022-02-22 17:26:53 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.4783
2022-02-22 17:27:26 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.1483
2022-02-22 17:27:58 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.3567
2022-02-22 17:28:32 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.3872
2022-02-22 17:29:05 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.1617
2022-02-22 17:29:38 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.3196
2022-02-22 17:30:11 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.2158
2022-02-22 17:30:43 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.5180
2022-02-22 17:31:16 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.2836
2022-02-22 17:31:49 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.2491
2022-02-22 17:32:22 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.3078
2022-02-22 17:32:55 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.2927
2022-02-22 17:33:28 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.2420
2022-02-22 17:34:01 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.3265
2022-02-22 17:34:33 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.4017
2022-02-22 17:35:06 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.3142
2022-02-22 17:35:39 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.2658
2022-02-22 17:36:12 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.3557
2022-02-22 17:36:45 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.4298
2022-02-22 17:37:19 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.4807
2022-02-22 17:37:52 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.4574
2022-02-22 17:38:25 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.3747
2022-02-22 17:38:58 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.4437
2022-02-22 17:39:31 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.3757
2022-02-22 17:40:05 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.2626
2022-02-22 17:40:38 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.5135
2022-02-22 17:41:12 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.2992
2022-02-22 17:41:45 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.3724
2022-02-22 17:42:20 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.3312
2022-02-22 17:42:54 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.4202
2022-02-22 17:43:28 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.1813
2022-02-22 17:44:03 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.2163
2022-02-22 17:44:38 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.4733
2022-02-22 17:44:40 - train: epoch 032, train_loss: 1.3991
2022-02-22 17:45:57 - eval: epoch: 032, acc1: 71.520%, acc5: 90.638%, test_loss: 1.1274, per_image_load_time: 2.345ms, per_image_inference_time: 0.588ms
2022-02-22 17:45:59 - until epoch: 032, best_acc1: 71.520%
2022-02-22 17:45:59 - epoch 033 lr: 0.010000000000000002
2022-02-22 17:46:37 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.2861
2022-02-22 17:47:09 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.4488
2022-02-22 17:47:42 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.3464
2022-02-22 17:48:14 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.4303
2022-02-22 17:48:47 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.5313
2022-02-22 17:49:19 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.3087
2022-02-22 17:49:52 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.5980
2022-02-22 17:50:24 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.3454
2022-02-22 17:50:57 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.3157
2022-02-22 17:51:29 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.5869
2022-02-22 17:52:02 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.2212
2022-02-22 17:52:35 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.4716
2022-02-22 17:53:08 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.3919
2022-02-22 17:53:41 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.3357
2022-02-22 17:54:13 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.4167
2022-02-22 17:54:46 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.5183
2022-02-22 17:55:19 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.1708
2022-02-22 17:55:51 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.4399
2022-02-22 17:56:24 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.1899
2022-02-22 17:56:57 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.3791
2022-02-22 17:57:30 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.3580
2022-02-22 17:58:02 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.5169
2022-02-22 17:58:35 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.3872
2022-02-22 17:59:08 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.4190
2022-02-22 17:59:40 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.4036
2022-02-22 18:00:13 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.1955
2022-02-22 18:00:46 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.2347
2022-02-22 18:01:19 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.3955
2022-02-22 18:01:51 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.2939
2022-02-22 18:02:24 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.3461
2022-02-22 18:02:57 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.2472
2022-02-22 18:03:30 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.3959
2022-02-22 18:04:02 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.3449
2022-02-22 18:04:35 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.2524
2022-02-22 18:05:07 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.2287
2022-02-22 18:05:40 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.4733
2022-02-22 18:06:13 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.2215
2022-02-22 18:06:46 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.1976
2022-02-22 18:07:19 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.4360
2022-02-22 18:07:52 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.4481
2022-02-22 18:08:26 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.3352
2022-02-22 18:08:59 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.2559
2022-02-22 18:09:32 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.3718
2022-02-22 18:10:05 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.3006
2022-02-22 18:10:39 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.5846
2022-02-22 18:11:12 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.4249
2022-02-22 18:11:47 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.2739
2022-02-22 18:12:21 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.5772
2022-02-22 18:12:56 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.1144
2022-02-22 18:13:31 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.2654
2022-02-22 18:13:33 - train: epoch 033, train_loss: 1.3558
2022-02-22 18:14:49 - eval: epoch: 033, acc1: 71.968%, acc5: 90.980%, test_loss: 1.1101, per_image_load_time: 1.540ms, per_image_inference_time: 0.608ms
2022-02-22 18:14:51 - until epoch: 033, best_acc1: 71.968%
2022-02-22 18:14:51 - epoch 034 lr: 0.010000000000000002
2022-02-22 18:15:28 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.2945
2022-02-22 18:16:00 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.2802
2022-02-22 18:16:32 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.1869
2022-02-22 18:17:04 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.1477
2022-02-22 18:17:36 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.2915
2022-02-22 18:18:08 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.4064
2022-02-22 18:18:40 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.1885
2022-02-22 18:19:12 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.4545
2022-02-22 18:19:45 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.3756
2022-02-22 18:20:17 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.3633
2022-02-22 18:20:50 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.3749
2022-02-22 18:21:23 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.3568
2022-02-22 18:21:55 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.4414
2022-02-22 18:22:28 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.2042
2022-02-22 18:23:00 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.3719
2022-02-22 18:23:33 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.4115
2022-02-22 18:24:06 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.2233
2022-02-22 18:24:39 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.4475
2022-02-22 18:25:12 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.5344
2022-02-22 18:25:44 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.2861
2022-02-22 18:26:17 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.6280
2022-02-22 18:26:50 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.1700
2022-02-22 18:27:22 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.5596
2022-02-22 18:27:55 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.3357
2022-02-22 18:28:27 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.3786
2022-02-22 18:29:00 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.3707
2022-02-22 18:29:32 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.4225
2022-02-22 18:30:04 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.1081
2022-02-22 18:30:37 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.2889
2022-02-22 18:31:09 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.0997
2022-02-22 18:31:42 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.1803
2022-02-22 18:32:14 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.3804
2022-02-22 18:32:46 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.3524
2022-02-22 18:33:19 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.2392
2022-02-22 18:33:51 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.1807
2022-02-22 18:34:24 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.0900
2022-02-22 18:34:56 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.1898
2022-02-22 18:35:28 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.3310
2022-02-22 18:36:01 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.5708
2022-02-22 18:36:33 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.2700
2022-02-22 18:37:06 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.4484
2022-02-22 18:37:39 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.4529
2022-02-22 18:38:11 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.2963
2022-02-22 18:38:44 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.1435
2022-02-22 18:39:17 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.5765
2022-02-22 18:39:51 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.5253
2022-02-22 18:40:24 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.4791
2022-02-22 18:40:58 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.4174
2022-02-22 18:41:32 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.3075
2022-02-22 18:42:05 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.1991
2022-02-22 18:42:07 - train: epoch 034, train_loss: 1.3263
2022-02-22 18:43:23 - eval: epoch: 034, acc1: 72.206%, acc5: 91.122%, test_loss: 1.0985, per_image_load_time: 2.398ms, per_image_inference_time: 0.554ms
2022-02-22 18:43:25 - until epoch: 034, best_acc1: 72.206%
2022-02-22 18:43:25 - epoch 035 lr: 0.010000000000000002
2022-02-22 18:44:03 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.1128
2022-02-22 18:44:34 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 0.9249
2022-02-22 18:45:06 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.5558
2022-02-22 18:45:38 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.1706
2022-02-22 18:46:10 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.2598
2022-02-22 18:46:42 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.2789
2022-02-22 18:47:14 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.2549
2022-02-22 18:47:46 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.3256
2022-02-22 18:48:18 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.4381
2022-02-22 18:48:51 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.1622
2022-02-22 18:49:23 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.4830
2022-02-22 18:49:55 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.0840
2022-02-22 18:50:28 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.2994
2022-02-22 18:51:01 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.2286
2022-02-22 18:51:34 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.4640
2022-02-22 18:52:07 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.3555
2022-02-22 18:52:40 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.1563
2022-02-22 18:53:14 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.2946
2022-02-22 18:53:47 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.3361
2022-02-22 18:54:20 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.4448
2022-02-22 18:54:53 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.4308
2022-02-22 18:55:27 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.3881
2022-02-22 18:56:00 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.3576
2022-02-22 18:56:33 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.4211
2022-02-22 18:57:07 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.2193
2022-02-22 18:57:40 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.2566
2022-02-22 18:58:13 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.2752
2022-02-22 18:58:46 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.4084
2022-02-22 18:59:19 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.3867
2022-02-22 18:59:52 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.2086
2022-02-22 19:00:25 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.4918
2022-02-22 19:00:58 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.3557
2022-02-22 19:01:31 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.4432
2022-02-22 19:02:04 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.3474
2022-02-22 19:02:37 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.1564
2022-02-22 19:03:11 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.3381
2022-02-22 19:03:44 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.2237
2022-02-22 19:04:17 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.2575
2022-02-22 19:04:50 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.2204
2022-02-22 19:05:24 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.0990
2022-02-22 19:05:57 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.3569
2022-02-22 19:06:30 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.2901
2022-02-22 19:07:04 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.4645
2022-02-22 19:07:37 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.3221
2022-02-22 19:08:11 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.4892
2022-02-22 19:08:45 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.3865
2022-02-22 19:09:19 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.4199
2022-02-22 19:09:54 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.4337
2022-02-22 19:10:28 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.3006
2022-02-22 19:11:03 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.4150
2022-02-22 19:11:05 - train: epoch 035, train_loss: 1.3082
2022-02-22 19:12:23 - eval: epoch: 035, acc1: 72.412%, acc5: 91.114%, test_loss: 1.0905, per_image_load_time: 0.675ms, per_image_inference_time: 0.577ms
2022-02-22 19:12:24 - until epoch: 035, best_acc1: 72.412%
2022-02-22 19:12:24 - epoch 036 lr: 0.010000000000000002
2022-02-22 19:13:02 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.2923
2022-02-22 19:13:35 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.1150
2022-02-22 19:14:07 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.2653
2022-02-22 19:14:40 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.1058
2022-02-22 19:15:13 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.2991
2022-02-22 19:15:45 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.2273
2022-02-22 19:16:18 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.2733
2022-02-22 19:16:51 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.2471
2022-02-22 19:17:23 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.2119
2022-02-22 19:17:56 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.2220
2022-02-22 19:18:28 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.1778
2022-02-22 19:19:01 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.1871
2022-02-22 19:19:34 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.4007
2022-02-22 19:20:07 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.3532
2022-02-22 19:20:40 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.3935
2022-02-22 19:21:13 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.2858
2022-02-22 19:21:45 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.3564
2022-02-22 19:22:18 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.1948
2022-02-22 19:22:51 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.4928
2022-02-22 19:23:24 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.3488
2022-02-22 19:23:57 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.2174
2022-02-22 19:24:30 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.4716
2022-02-22 19:25:03 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.2147
2022-02-22 19:25:36 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.2701
2022-02-22 19:26:09 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.1982
2022-02-22 19:26:43 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.2150
2022-02-22 19:27:15 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.2310
2022-02-22 19:27:49 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.3136
2022-02-22 19:28:22 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.1377
2022-02-22 19:28:54 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.3394
2022-02-22 19:29:27 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.4052
2022-02-22 19:30:00 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.3902
2022-02-22 19:30:33 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.3361
2022-02-22 19:31:06 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.2523
2022-02-22 19:31:39 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.4506
2022-02-22 19:32:12 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.3924
2022-02-22 19:32:45 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.2696
2022-02-22 19:33:19 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.2830
2022-02-22 19:33:52 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.1987
2022-02-22 19:34:25 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.3120
2022-02-22 19:34:58 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.3314
2022-02-22 19:35:32 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.2788
2022-02-22 19:36:06 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.2493
2022-02-22 19:36:39 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.4386
2022-02-22 19:37:13 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.1946
2022-02-22 19:37:47 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.3520
2022-02-22 19:38:21 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.1205
2022-02-22 19:38:56 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.2834
2022-02-22 19:39:31 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.2532
2022-02-22 19:40:06 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.2914
2022-02-22 19:40:08 - train: epoch 036, train_loss: 1.2953
2022-02-22 19:41:25 - eval: epoch: 036, acc1: 72.698%, acc5: 91.052%, test_loss: 1.0888, per_image_load_time: 2.388ms, per_image_inference_time: 0.600ms
2022-02-22 19:41:27 - until epoch: 036, best_acc1: 72.698%
2022-02-22 19:41:27 - epoch 037 lr: 0.010000000000000002
2022-02-22 19:42:04 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.1179
2022-02-22 19:42:36 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.1049
2022-02-22 19:43:09 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.1819
2022-02-22 19:43:41 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.1675
2022-02-22 19:44:13 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.2448
2022-02-22 19:44:46 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.5370
2022-02-22 19:45:18 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.1423
2022-02-22 19:45:51 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.3340
2022-02-22 19:46:24 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.4608
2022-02-22 19:46:57 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.1210
2022-02-22 19:47:30 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.4411
2022-02-22 19:48:03 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.3624
2022-02-22 19:48:36 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.1756
2022-02-22 19:49:09 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.5656
2022-02-22 19:49:42 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.2166
2022-02-22 19:50:15 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.1743
2022-02-22 19:50:48 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.2249
2022-02-22 19:51:21 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.3701
2022-02-22 19:51:54 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.1911
2022-02-22 19:52:27 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.2093
2022-02-22 19:53:00 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.3494
2022-02-22 19:53:33 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.2514
2022-02-22 19:54:06 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.3439
2022-02-22 19:54:39 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.3202
2022-02-22 19:55:12 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.2328
2022-02-22 19:55:45 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.5259
2022-02-22 19:56:18 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.5460
2022-02-22 19:56:51 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.2890
2022-02-22 19:57:25 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.2884
2022-02-22 19:57:57 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.6219
2022-02-22 19:58:31 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.2920
2022-02-22 19:59:04 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.3182
2022-02-22 19:59:36 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.3053
2022-02-22 20:00:09 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.0169
2022-02-22 20:00:42 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.2817
2022-02-22 20:01:16 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.4501
2022-02-22 20:01:49 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.4081
2022-02-22 20:02:22 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.3904
2022-02-22 20:02:55 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.3573
2022-02-22 20:03:28 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.1968
2022-02-22 20:04:01 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.4173
2022-02-22 20:04:34 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.2778
2022-02-22 20:05:08 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.3531
2022-02-22 20:05:41 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.1709
2022-02-22 20:06:15 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.2107
2022-02-22 20:06:49 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.4457
2022-02-22 20:07:23 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.1325
2022-02-22 20:07:58 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.3689
2022-02-22 20:08:33 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.4549
2022-02-22 20:09:07 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.2259
2022-02-22 20:09:10 - train: epoch 037, train_loss: 1.2867
2022-02-22 20:10:28 - eval: epoch: 037, acc1: 72.504%, acc5: 91.148%, test_loss: 1.0884, per_image_load_time: 2.408ms, per_image_inference_time: 0.585ms
2022-02-22 20:10:29 - until epoch: 037, best_acc1: 72.698%
2022-02-22 20:10:29 - epoch 038 lr: 0.010000000000000002
2022-02-22 20:11:07 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.0943
2022-02-22 20:11:39 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.2066
2022-02-22 20:12:10 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.0177
2022-02-22 20:12:43 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.0238
2022-02-22 20:13:15 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.2020
2022-02-22 20:13:47 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.3432
2022-02-22 20:14:20 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.1352
2022-02-22 20:14:52 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.2486
2022-02-22 20:15:25 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.4181
2022-02-22 20:15:57 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.3997
2022-02-22 20:16:30 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.3396
2022-02-22 20:17:03 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.4858
2022-02-22 20:17:36 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.1103
2022-02-22 20:18:08 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.4510
2022-02-22 20:18:41 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.2750
2022-02-22 20:19:14 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.3749
2022-02-22 20:19:47 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.3906
2022-02-22 20:20:19 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.4148
2022-02-22 20:20:52 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.2873
2022-02-22 20:21:25 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.2565
2022-02-22 20:21:57 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.2691
2022-02-22 20:22:30 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.2838
2022-02-22 20:23:03 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.2671
2022-02-22 20:23:36 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.4037
2022-02-22 20:24:08 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.3307
2022-02-22 20:24:40 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.3723
2022-02-22 20:25:13 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.4473
2022-02-22 20:25:46 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.3223
2022-02-22 20:26:18 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.4042
2022-02-22 20:26:51 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.2415
2022-02-22 20:27:24 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.5433
2022-02-22 20:27:56 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.1522
2022-02-22 20:28:29 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.3712
2022-02-22 20:29:02 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.2881
2022-02-22 20:29:35 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.3437
2022-02-22 20:30:08 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.2045
2022-02-22 20:30:41 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.2960
2022-02-22 20:31:14 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.4731
2022-02-22 20:31:46 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.2898
2022-02-22 20:32:19 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.3449
2022-02-22 20:32:53 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.3382
2022-02-22 20:33:26 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.1693
2022-02-22 20:33:59 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.2324
2022-02-22 20:34:32 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.2167
2022-02-22 20:35:05 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.3830
2022-02-22 20:35:39 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.3670
2022-02-22 20:36:13 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.2441
2022-02-22 20:36:47 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.1365
2022-02-22 20:37:21 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.2576
2022-02-22 20:37:55 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 0.9754
2022-02-22 20:37:57 - train: epoch 038, train_loss: 1.2808
2022-02-22 20:39:14 - eval: epoch: 038, acc1: 72.624%, acc5: 90.998%, test_loss: 1.0917, per_image_load_time: 2.397ms, per_image_inference_time: 0.569ms
2022-02-22 20:39:15 - until epoch: 038, best_acc1: 72.698%
2022-02-22 20:39:15 - epoch 039 lr: 0.010000000000000002
2022-02-22 20:39:54 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.2620
2022-02-22 20:40:27 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.4035
2022-02-22 20:40:59 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.2250
2022-02-22 20:41:32 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.3287
2022-02-22 20:42:04 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.2670
2022-02-22 20:42:37 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.2022
2022-02-22 20:43:10 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.3679
2022-02-22 20:43:42 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.3261
2022-02-22 20:44:14 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.3779
2022-02-22 20:44:46 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.2718
2022-02-22 20:45:19 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.2425
2022-02-22 20:45:51 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.2903
2022-02-22 20:46:23 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.2948
2022-02-22 20:46:55 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.3998
2022-02-22 20:47:28 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.1714
2022-02-22 20:48:00 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.2585
2022-02-22 20:48:32 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.2441
2022-02-22 20:49:05 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.3216
2022-02-22 20:49:37 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 0.9164
2022-02-22 20:50:09 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.2015
2022-02-22 20:50:42 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.2151
2022-02-22 20:51:14 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.3590
2022-02-22 20:51:46 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.4370
2022-02-22 20:52:19 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.3336
2022-02-22 20:52:51 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.1375
2022-02-22 20:53:24 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.1767
2022-02-22 20:53:56 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.2685
2022-02-22 20:54:29 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.2748
2022-02-22 20:55:02 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.0743
2022-02-22 20:55:34 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.1931
2022-02-22 20:56:07 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.1788
2022-02-22 20:56:40 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.2367
2022-02-22 20:57:13 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.4297
2022-02-22 20:57:46 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.3883
2022-02-22 20:58:19 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.3006
2022-02-22 20:58:51 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.3685
2022-02-22 20:59:24 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.2309
2022-02-22 20:59:57 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.1465
2022-02-22 21:00:30 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.1374
2022-02-22 21:01:03 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.3224
2022-02-22 21:01:37 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.3725
2022-02-22 21:02:10 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.3574
2022-02-22 21:02:43 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.3573
2022-02-22 21:03:17 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.0509
2022-02-22 21:03:51 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.2273
2022-02-22 21:04:24 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.4487
2022-02-22 21:04:59 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.2862
2022-02-22 21:05:33 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.2586
2022-02-22 21:06:08 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.3527
2022-02-22 21:06:42 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.3333
2022-02-22 21:06:45 - train: epoch 039, train_loss: 1.2805
2022-02-22 21:08:02 - eval: epoch: 039, acc1: 72.472%, acc5: 91.128%, test_loss: 1.0909, per_image_load_time: 2.319ms, per_image_inference_time: 0.608ms
2022-02-22 21:08:03 - until epoch: 039, best_acc1: 72.698%
2022-02-22 21:08:03 - epoch 040 lr: 0.010000000000000002
2022-02-22 21:08:41 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.3560
2022-02-22 21:09:14 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.4643
2022-02-22 21:09:46 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.3004
2022-02-22 21:10:18 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.2910
2022-02-22 21:10:51 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.0048
2022-02-22 21:11:23 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.2090
2022-02-22 21:11:56 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.1518
2022-02-22 21:12:28 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.3138
2022-02-22 21:13:01 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.2006
2022-02-22 21:13:33 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.2476
2022-02-22 21:14:06 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.2137
2022-02-22 21:14:38 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.2216
2022-02-22 21:15:11 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.1579
2022-02-22 21:15:44 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.3820
2022-02-22 21:16:16 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.3514
2022-02-22 21:16:49 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.3204
2022-02-22 21:17:22 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.3662
2022-02-22 21:17:54 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.3209
2022-02-22 21:18:28 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.2738
2022-02-22 21:19:00 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.4117
2022-02-22 21:19:33 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.2233
2022-02-22 21:20:06 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.3317
2022-02-22 21:20:38 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.0688
2022-02-22 21:21:11 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.4034
2022-02-22 21:21:44 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.4561
2022-02-22 21:22:17 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.3608
2022-02-22 21:22:49 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.2581
2022-02-22 21:23:22 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.2496
2022-02-22 21:23:55 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.5093
2022-02-22 21:24:27 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.3583
2022-02-22 21:25:00 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.3845
2022-02-22 21:25:33 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.3813
2022-02-22 21:26:06 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.4028
2022-02-22 21:26:38 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.2456
2022-02-22 21:27:11 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.4064
2022-02-22 21:27:44 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.1882
2022-02-22 21:28:17 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.1113
2022-02-22 21:28:50 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.1212
2022-02-22 21:29:23 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.3479
2022-02-22 21:29:57 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.4047
2022-02-22 21:30:30 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.3551
2022-02-22 21:31:03 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.2083
2022-02-22 21:31:37 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.3675
2022-02-22 21:32:10 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.2693
2022-02-22 21:32:43 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.1397
2022-02-22 21:33:17 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.4779
2022-02-22 21:33:51 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.2720
2022-02-22 21:34:25 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.2614
2022-02-22 21:34:59 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.1488
2022-02-22 21:35:33 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.4428
2022-02-22 21:35:36 - train: epoch 040, train_loss: 1.2753
2022-02-22 21:36:51 - eval: epoch: 040, acc1: 72.326%, acc5: 90.908%, test_loss: 1.1010, per_image_load_time: 1.268ms, per_image_inference_time: 0.583ms
2022-02-22 21:36:52 - until epoch: 040, best_acc1: 72.698%
2022-02-22 21:36:52 - epoch 041 lr: 0.010000000000000002
2022-02-22 21:37:30 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.4052
2022-02-22 21:38:02 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.4730
2022-02-22 21:38:34 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.0928
2022-02-22 21:39:07 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.2033
2022-02-22 21:39:39 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.2497
2022-02-22 21:40:11 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.1790
2022-02-22 21:40:44 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.0497
2022-02-22 21:41:16 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.3176
2022-02-22 21:41:48 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.1557
2022-02-22 21:42:21 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.2483
2022-02-22 21:42:53 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.2409
2022-02-22 21:43:25 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.2407
2022-02-22 21:43:58 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.1584
2022-02-22 21:44:31 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.3767
2022-02-22 21:45:04 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.6737
2022-02-22 21:45:36 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.3204
2022-02-22 21:46:09 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.2850
2022-02-22 21:46:42 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.0463
2022-02-22 21:47:15 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.3060
2022-02-22 21:47:48 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.1768
2022-02-22 21:48:21 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.1368
2022-02-22 21:48:54 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.2798
2022-02-22 21:49:27 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.0730
2022-02-22 21:50:00 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.3948
2022-02-22 21:50:32 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.2128
2022-02-22 21:51:05 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.5172
2022-02-22 21:51:38 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.3697
2022-02-22 21:52:11 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.1605
2022-02-22 21:52:44 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.3669
2022-02-22 21:53:17 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.3000
2022-02-22 21:53:51 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.3283
2022-02-22 21:54:24 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.2714
2022-02-22 21:54:57 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.2000
2022-02-22 21:55:30 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.1057
2022-02-22 21:56:03 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.1016
2022-02-22 21:56:36 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.2929
2022-02-22 21:57:09 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.3166
2022-02-22 21:57:42 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.4620
2022-02-22 21:58:15 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.2029
2022-02-22 21:58:49 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.1700
2022-02-22 21:59:22 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.1858
2022-02-22 21:59:55 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.1058
2022-02-22 22:00:28 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.1388
2022-02-22 22:01:01 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.1186
2022-02-22 22:01:35 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.2566
2022-02-22 22:02:09 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.3942
2022-02-22 22:02:42 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.2442
2022-02-22 22:03:16 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.3679
2022-02-22 22:03:51 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.1520
2022-02-22 22:04:25 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.3840
2022-02-22 22:04:27 - train: epoch 041, train_loss: 1.2738
2022-02-22 22:05:44 - eval: epoch: 041, acc1: 72.370%, acc5: 91.008%, test_loss: 1.0976, per_image_load_time: 2.387ms, per_image_inference_time: 0.573ms
2022-02-22 22:05:44 - until epoch: 041, best_acc1: 72.698%
2022-02-22 22:05:44 - epoch 042 lr: 0.010000000000000002
2022-02-22 22:06:23 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.0100
2022-02-22 22:06:56 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.1301
2022-02-22 22:07:28 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.3168
2022-02-22 22:08:01 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.0887
2022-02-22 22:08:33 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.4077
2022-02-22 22:09:06 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.1338
2022-02-22 22:09:39 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.4286
2022-02-22 22:10:12 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.3368
2022-02-22 22:10:44 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.1804
2022-02-22 22:11:17 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.4324
2022-02-22 22:11:49 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.2541
2022-02-22 22:12:22 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.3289
2022-02-22 22:12:54 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.2560
2022-02-22 22:13:27 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.2741
2022-02-22 22:14:00 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.1612
2022-02-22 22:14:33 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.5866
2022-02-22 22:15:06 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.1924
2022-02-22 22:15:38 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.1358
2022-02-22 22:16:11 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.3305
2022-02-22 22:16:44 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.2921
2022-02-22 22:17:16 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 0.9774
2022-02-22 22:17:49 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.1331
2022-02-22 22:18:22 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.2897
2022-02-22 22:18:55 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.4053
2022-02-22 22:19:28 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.4921
2022-02-22 22:20:01 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.3713
2022-02-22 22:20:33 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.1175
2022-02-22 22:21:06 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.2045
2022-02-22 22:21:39 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.3387
2022-02-22 22:22:12 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.3176
2022-02-22 22:22:45 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.3008
2022-02-22 22:23:18 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.3726
2022-02-22 22:23:51 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.5224
2022-02-22 22:24:24 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.0812
2022-02-22 22:24:57 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.2189
2022-02-22 22:25:31 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.3058
2022-02-22 22:26:04 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.1698
2022-02-22 22:26:38 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.0613
2022-02-22 22:27:11 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.2730
2022-02-22 22:27:44 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.3175
2022-02-22 22:28:18 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.2979
2022-02-22 22:28:51 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.1905
2022-02-22 22:29:25 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.0007
2022-02-22 22:29:58 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.3103
2022-02-22 22:30:32 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.2940
2022-02-22 22:31:06 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.3549
2022-02-22 22:31:40 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.2147
2022-02-22 22:32:15 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.2157
2022-02-22 22:32:50 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.1384
2022-02-22 22:33:24 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.3239
2022-02-22 22:33:26 - train: epoch 042, train_loss: 1.2716
2022-02-22 22:34:42 - eval: epoch: 042, acc1: 72.258%, acc5: 90.956%, test_loss: 1.1067, per_image_load_time: 1.236ms, per_image_inference_time: 0.568ms
2022-02-22 22:34:43 - until epoch: 042, best_acc1: 72.698%
2022-02-23 06:08:10 - epoch 043 lr: 0.010000000000000002
2022-02-23 06:08:49 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.2716
2022-02-23 06:09:21 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.3036
2022-02-23 06:09:53 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.0186
2022-02-23 06:10:25 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.1402
2022-02-23 06:10:57 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.1573
2022-02-23 06:11:29 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.0819
2022-02-23 06:12:02 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.3449
2022-02-23 06:12:34 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.3462
2022-02-23 06:13:07 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.2820
2022-02-23 06:13:39 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.3972
2022-02-23 06:14:12 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.4026
2022-02-23 06:14:44 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.3396
2022-02-23 06:15:17 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.2721
2022-02-23 06:15:50 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.3552
2022-02-23 06:16:23 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.0623
2022-02-23 06:16:55 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.2218
2022-02-23 06:17:28 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.3231
2022-02-23 06:18:01 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.5429
2022-02-23 06:18:33 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.1839
2022-02-23 06:19:06 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.2715
2022-02-23 06:19:38 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.1862
2022-02-23 06:20:11 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.2122
2022-02-23 06:20:43 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.4030
2022-02-23 06:21:16 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.1747
2022-02-23 06:21:49 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.3308
2022-02-23 06:22:21 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.0854
2022-02-23 06:22:54 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.3780
2022-02-23 06:23:27 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.3265
2022-02-23 06:23:59 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.1810
2022-02-23 06:24:32 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.5002
2022-02-23 06:25:05 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.4535
2022-02-23 06:25:38 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.2726
2022-02-23 06:26:11 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.3019
2022-02-23 06:26:44 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.4128
2022-02-23 06:27:17 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.3080
2022-02-23 06:27:50 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.3970
2022-02-23 06:28:23 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.3481
2022-02-23 06:28:56 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.4319
2022-02-23 06:29:29 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.2679
2022-02-23 06:30:03 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.5094
2022-02-23 06:30:36 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.4684
2022-02-23 06:31:09 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.2124
2022-02-23 06:31:43 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.2122
2022-02-23 06:32:17 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.2532
2022-02-23 06:32:51 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.1438
2022-02-23 06:33:25 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.4369
2022-02-23 06:34:00 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.4245
2022-02-23 06:34:34 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.2293
2022-02-23 06:35:10 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.3212
2022-02-23 06:35:44 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.4553
2022-02-23 06:35:45 - train: epoch 043, train_loss: 1.2745
2022-02-23 06:37:02 - eval: epoch: 043, acc1: 72.040%, acc5: 91.014%, test_loss: 1.1087, per_image_load_time: 2.032ms, per_image_inference_time: 0.579ms
2022-02-23 06:37:03 - until epoch: 043, best_acc1: 72.698%
2022-02-23 06:37:03 - epoch 044 lr: 0.010000000000000002
2022-02-23 06:37:41 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.2589
2022-02-23 06:38:14 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.3280
2022-02-23 06:38:46 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.2427
2022-02-23 06:39:19 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.0724
2022-02-23 06:39:51 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.3138
2022-02-23 06:40:23 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.0594
2022-02-23 06:40:56 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.1107
2022-02-23 06:41:28 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.3188
2022-02-23 06:42:01 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.3550
2022-02-23 06:42:33 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.4885
2022-02-23 06:43:06 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.1463
2022-02-23 06:43:38 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.3863
2022-02-23 06:44:11 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.4437
2022-02-23 06:44:43 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.3304
2022-02-23 06:45:16 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.2181
2022-02-23 06:45:48 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.3214
2022-02-23 06:46:21 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.2342
2022-02-23 06:46:53 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.1816
2022-02-23 06:47:25 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.3661
2022-02-23 06:47:58 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.2183
2022-02-23 06:48:31 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.3539
2022-02-23 06:49:03 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.1843
2022-02-23 06:49:36 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.5758
2022-02-23 06:50:09 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.2675
2022-02-23 06:50:42 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.3608
2022-02-23 06:51:14 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.2748
2022-02-23 06:51:47 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.1802
2022-02-23 06:52:20 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.1532
2022-02-23 06:52:53 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 0.9627
2022-02-23 06:53:26 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.1645
2022-02-23 06:53:59 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.4261
2022-02-23 06:54:32 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.1889
2022-02-23 06:55:05 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.2480
2022-02-23 06:55:38 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.3434
2022-02-23 06:56:12 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.2915
2022-02-23 06:56:45 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.3465
2022-02-23 06:57:18 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.2374
2022-02-23 06:57:51 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.2019
2022-02-23 06:58:24 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.2955
2022-02-23 06:58:58 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.3462
2022-02-23 06:59:31 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.2193
2022-02-23 07:00:05 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.1275
2022-02-23 07:00:38 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.3161
2022-02-23 07:01:12 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.2072
2022-02-23 07:01:46 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.4724
2022-02-23 07:02:21 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.2179
2022-02-23 07:02:55 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.3335
2022-02-23 07:03:30 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.3117
2022-02-23 07:04:05 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.2195
2022-02-23 07:04:39 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.3182
2022-02-23 07:04:41 - train: epoch 044, train_loss: 1.2714
2022-02-23 07:05:57 - eval: epoch: 044, acc1: 72.338%, acc5: 91.090%, test_loss: 1.1002, per_image_load_time: 2.191ms, per_image_inference_time: 0.611ms
2022-02-23 07:05:58 - until epoch: 044, best_acc1: 72.698%
2022-02-23 07:05:58 - epoch 045 lr: 0.010000000000000002
2022-02-23 07:06:37 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.2875
2022-02-23 07:07:09 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.1163
2022-02-23 07:07:42 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.1640
2022-02-23 07:08:14 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.3298
2022-02-23 07:08:47 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.4266
2022-02-23 07:09:20 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.4124
2022-02-23 07:09:53 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.1289
2022-02-23 07:10:25 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.0592
2022-02-23 07:10:58 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.0325
2022-02-23 07:11:31 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.3165
2022-02-23 07:12:03 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.5298
2022-02-23 07:12:36 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.2295
2022-02-23 07:13:09 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.2091
2022-02-23 07:13:42 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.3972
2022-02-23 07:14:15 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.2353
2022-02-23 07:14:48 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.2252
2022-02-23 07:15:21 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.1782
2022-02-23 07:15:54 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.2546
2022-02-23 07:16:27 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.2228
2022-02-23 07:17:00 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.4617
2022-02-23 07:17:33 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.3361
2022-02-23 07:18:06 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.3787
2022-02-23 07:18:39 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.1426
2022-02-23 07:19:12 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.2417
2022-02-23 07:19:45 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.2984
2022-02-23 07:20:19 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.3258
2022-02-23 07:20:52 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.1269
2022-02-23 07:21:25 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.1355
2022-02-23 07:21:58 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.3635
2022-02-23 07:22:31 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.3764
2022-02-23 07:23:04 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.2911
2022-02-23 07:23:37 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.2500
2022-02-23 07:24:10 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.2774
2022-02-23 07:24:43 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.1020
2022-02-23 07:25:17 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.2565
2022-02-23 07:25:50 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.2899
2022-02-23 07:26:23 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.3081
2022-02-23 07:26:57 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.2915
2022-02-23 07:27:30 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.3401
2022-02-23 07:28:04 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.2320
2022-02-23 07:28:37 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.1612
2022-02-23 07:29:11 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.3994
2022-02-23 07:29:45 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.2778
2022-02-23 07:30:19 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.2622
2022-02-23 07:30:53 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.1636
2022-02-23 07:31:28 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.4666
2022-02-23 07:32:02 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.2641
2022-02-23 07:32:37 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.1836
2022-02-23 07:33:13 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.3214
2022-02-23 07:33:47 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.2774
2022-02-23 07:33:49 - train: epoch 045, train_loss: 1.2688
2022-02-23 07:35:05 - eval: epoch: 045, acc1: 72.252%, acc5: 91.122%, test_loss: 1.1016, per_image_load_time: 2.380ms, per_image_inference_time: 0.580ms
2022-02-23 07:35:05 - until epoch: 045, best_acc1: 72.698%
2022-02-23 07:35:05 - epoch 046 lr: 0.010000000000000002
2022-02-23 07:35:44 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.0799
2022-02-23 07:36:17 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.3833
2022-02-23 07:36:49 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.2116
2022-02-23 07:37:21 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.1568
2022-02-23 07:37:54 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.1322
2022-02-23 07:38:27 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.1720
2022-02-23 07:38:59 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.0706
2022-02-23 07:39:31 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.3030
2022-02-23 07:40:04 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.2155
2022-02-23 07:40:37 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.1121
2022-02-23 07:41:09 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.1214
2022-02-23 07:41:42 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.2943
2022-02-23 07:42:15 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.3143
2022-02-23 07:42:48 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.3557
2022-02-23 07:43:21 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.1143
2022-02-23 07:43:54 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.3388
2022-02-23 07:44:27 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.2054
2022-02-23 07:45:00 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.3536
2022-02-23 07:45:33 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.1912
2022-02-23 07:46:05 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.1258
2022-02-23 07:46:39 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.6092
2022-02-23 07:47:12 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.1676
2022-02-23 07:47:45 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.4940
2022-02-23 07:48:18 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.2619
2022-02-23 07:48:51 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.3034
2022-02-23 07:49:24 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.5911
2022-02-23 07:49:58 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.1060
2022-02-23 07:50:30 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.0916
2022-02-23 07:51:03 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.3624
2022-02-23 07:51:37 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.0607
2022-02-23 07:52:09 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.1096
2022-02-23 07:52:43 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.2714
2022-02-23 07:53:16 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.1994
2022-02-23 07:53:49 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.3964
2022-02-23 07:54:22 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.3309
2022-02-23 07:54:56 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.2067
2022-02-23 07:55:29 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.1764
2022-02-23 07:56:02 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.2817
2022-02-23 07:56:35 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.2303
2022-02-23 07:57:08 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.2689
2022-02-23 07:57:42 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.3891
2022-02-23 07:58:15 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 0.9653
2022-02-23 07:58:49 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.4454
2022-02-23 07:59:23 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.4673
2022-02-23 07:59:57 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.3397
2022-02-23 08:00:31 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.2657
2022-02-23 08:01:05 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.4120
2022-02-23 08:01:40 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.0565
2022-02-23 08:02:16 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.3431
2022-02-23 08:02:51 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.2620
2022-02-23 08:02:52 - train: epoch 046, train_loss: 1.2690
2022-02-23 08:04:09 - eval: epoch: 046, acc1: 72.250%, acc5: 91.042%, test_loss: 1.1013, per_image_load_time: 2.461ms, per_image_inference_time: 0.550ms
2022-02-23 08:04:10 - until epoch: 046, best_acc1: 72.698%
2022-02-23 08:04:10 - epoch 047 lr: 0.010000000000000002
2022-02-23 08:04:49 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.1655
2022-02-23 08:05:21 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.5616
2022-02-23 08:05:54 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.0055
2022-02-23 08:06:26 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.0718
2022-02-23 08:06:58 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.2441
2022-02-23 08:07:30 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.1811
2022-02-23 08:08:03 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.2885
2022-02-23 08:08:35 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.2987
2022-02-23 08:09:08 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.2855
2022-02-23 08:09:41 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.3000
2022-02-23 08:10:13 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.3241
2022-02-23 08:10:46 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.1158
2022-02-23 08:11:19 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.2428
2022-02-23 08:11:52 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.3142
2022-02-23 08:12:24 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.1865
2022-02-23 08:12:57 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.1342
2022-02-23 08:13:30 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.1481
2022-02-23 08:14:02 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.2467
2022-02-23 08:14:35 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.1847
2022-02-23 08:15:08 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.3421
2022-02-23 08:15:40 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.3126
2022-02-23 08:16:13 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.3185
2022-02-23 08:16:46 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.2387
2022-02-23 08:17:19 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.2418
2022-02-23 08:17:51 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.2952
2022-02-23 08:18:24 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.6364
2022-02-23 08:18:58 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.1083
2022-02-23 08:19:30 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.2949
2022-02-23 08:20:03 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.2528
2022-02-23 08:20:37 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.1910
2022-02-23 08:21:10 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.2973
2022-02-23 08:21:42 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.4542
2022-02-23 08:22:15 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.3168
2022-02-23 08:22:49 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.1283
2022-02-23 08:23:21 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.5135
2022-02-23 08:23:54 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.3038
2022-02-23 08:24:27 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.1886
2022-02-23 08:25:01 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.3234
2022-02-23 08:25:34 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.5271
2022-02-23 08:26:08 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.2737
2022-02-23 08:26:41 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.2862
2022-02-23 08:27:14 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.2195
2022-02-23 08:27:48 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.1702
2022-02-23 08:28:22 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.4055
2022-02-23 08:28:56 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.3206
2022-02-23 08:29:30 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.1515
2022-02-23 08:30:04 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.5919
2022-02-23 08:30:39 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.0157
2022-02-23 08:31:14 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.3591
2022-02-23 08:31:49 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.2234
2022-02-23 08:31:51 - train: epoch 047, train_loss: 1.2682
2022-02-23 08:33:07 - eval: epoch: 047, acc1: 72.204%, acc5: 91.070%, test_loss: 1.1000, per_image_load_time: 2.376ms, per_image_inference_time: 0.592ms
2022-02-23 08:33:08 - until epoch: 047, best_acc1: 72.698%
2022-02-23 08:33:08 - epoch 048 lr: 0.010000000000000002
2022-02-23 08:33:46 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.3239
2022-02-23 08:34:19 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.5151
2022-02-23 08:34:52 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.2935
2022-02-23 08:35:25 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.0870
2022-02-23 08:35:57 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.1186
2022-02-23 08:36:30 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.3079
2022-02-23 08:37:03 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.1079
2022-02-23 08:37:35 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.1829
2022-02-23 08:38:08 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.3918
2022-02-23 08:38:41 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.3543
2022-02-23 08:39:13 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.3279
2022-02-23 08:39:46 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.3445
2022-02-23 08:40:18 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.0314
2022-02-23 08:40:51 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.2960
2022-02-23 08:41:23 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.2766
2022-02-23 08:41:56 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.2330
2022-02-23 08:42:29 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.1677
2022-02-23 08:43:02 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.1848
2022-02-23 08:43:34 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.3917
2022-02-23 08:44:07 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.3549
2022-02-23 08:44:40 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.1293
2022-02-23 08:45:13 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.4394
2022-02-23 08:45:46 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.1451
2022-02-23 08:46:19 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.4178
2022-02-23 08:46:52 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.4020
2022-02-23 08:47:25 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.2801
2022-02-23 08:47:58 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.4765
2022-02-23 08:48:31 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.2933
2022-02-23 08:49:03 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.3741
2022-02-23 08:49:36 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.2573
2022-02-23 08:50:09 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.4114
2022-02-23 08:50:42 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.1852
2022-02-23 08:51:15 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.2930
2022-02-23 08:51:48 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.1054
2022-02-23 08:52:21 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.2952
2022-02-23 08:52:54 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.3068
2022-02-23 08:53:27 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.3855
2022-02-23 08:54:01 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.1949
2022-02-23 08:54:34 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.3058
2022-02-23 08:55:07 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.0868
2022-02-23 08:55:41 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.4848
2022-02-23 08:56:14 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.1944
2022-02-23 08:56:47 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.2318
2022-02-23 08:57:21 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.0974
2022-02-23 08:57:55 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.2709
2022-02-23 08:58:29 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.1620
2022-02-23 08:59:04 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.3265
2022-02-23 08:59:38 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.3751
2022-02-23 09:00:13 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.3203
2022-02-23 09:00:48 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.2438
2022-02-23 09:00:49 - train: epoch 048, train_loss: 1.2661
2022-02-23 09:02:05 - eval: epoch: 048, acc1: 72.086%, acc5: 91.174%, test_loss: 1.0980, per_image_load_time: 2.336ms, per_image_inference_time: 0.582ms
2022-02-23 09:02:06 - until epoch: 048, best_acc1: 72.698%
2022-02-23 09:02:06 - epoch 049 lr: 0.010000000000000002
2022-02-23 09:02:45 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.3235
2022-02-23 09:03:18 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.0087
2022-02-23 09:03:50 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.4730
2022-02-23 09:04:23 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.1337
2022-02-23 09:04:55 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.1465
2022-02-23 09:05:28 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.2273
2022-02-23 09:06:01 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.1615
2022-02-23 09:06:34 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.3103
2022-02-23 09:07:07 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.0225
2022-02-23 09:07:39 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.2444
2022-02-23 09:08:12 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.1973
2022-02-23 09:08:45 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.1760
2022-02-23 09:09:17 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.4315
2022-02-23 09:09:50 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.5290
2022-02-23 09:10:23 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.1925
2022-02-23 09:10:56 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.2046
2022-02-23 09:11:29 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.2625
2022-02-23 09:12:02 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.3077
2022-02-23 09:12:35 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.1515
2022-02-23 09:13:08 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.3647
2022-02-23 09:13:42 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.1828
2022-02-23 09:14:15 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.1038
2022-02-23 09:14:48 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.1226
2022-02-23 09:15:21 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.2215
2022-02-23 09:15:54 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.2632
2022-02-23 09:16:28 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.3358
2022-02-23 09:17:01 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.3350
2022-02-23 09:17:34 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.1626
2022-02-23 09:18:07 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.2800
2022-02-23 09:18:41 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.2017
2022-02-23 09:19:14 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.3748
2022-02-23 09:19:47 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.4568
2022-02-23 09:20:20 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.2102
2022-02-23 09:20:53 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.2510
2022-02-23 09:21:26 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.4275
2022-02-23 09:22:00 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.3621
2022-02-23 09:22:33 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.1818
2022-02-23 09:23:06 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.3504
2022-02-23 09:23:40 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.3781
2022-02-23 09:24:13 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.1567
2022-02-23 09:24:46 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.2087
2022-02-23 09:25:20 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.4352
2022-02-23 09:25:53 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.3512
2022-02-23 09:26:27 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.2796
2022-02-23 09:27:01 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.2426
2022-02-23 09:27:35 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.3729
2022-02-23 09:28:10 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.4806
2022-02-23 09:28:45 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.3089
2022-02-23 09:29:20 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.1381
2022-02-23 09:29:54 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.2030
2022-02-23 09:29:55 - train: epoch 049, train_loss: 1.2638
2022-02-23 09:31:13 - eval: epoch: 049, acc1: 72.012%, acc5: 90.902%, test_loss: 1.1122, per_image_load_time: 2.443ms, per_image_inference_time: 0.562ms
2022-02-23 09:31:14 - until epoch: 049, best_acc1: 72.698%
2022-02-23 09:31:14 - epoch 050 lr: 0.010000000000000002
2022-02-23 09:31:52 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.2104
2022-02-23 09:32:25 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.1017
2022-02-23 09:32:57 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.1740
2022-02-23 09:33:30 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.1446
2022-02-23 09:34:03 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.3099
2022-02-23 09:34:35 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.4429
2022-02-23 09:35:08 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.0903
2022-02-23 09:35:41 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.2481
2022-02-23 09:36:14 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.1184
2022-02-23 09:36:47 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.3132
2022-02-23 09:37:19 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.1661
2022-02-23 09:37:52 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.1135
2022-02-23 09:38:25 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.3559
2022-02-23 09:38:58 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.2882
2022-02-23 09:39:30 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.0678
2022-02-23 09:40:03 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.1757
2022-02-23 09:40:36 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.1450
2022-02-23 09:41:09 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.4116
2022-02-23 09:41:42 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.1552
2022-02-23 09:42:15 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.3329
2022-02-23 09:42:48 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.4598
2022-02-23 09:43:21 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.4388
2022-02-23 09:43:54 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.0921
2022-02-23 09:44:26 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.3674
2022-02-23 09:45:00 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.2055
2022-02-23 09:45:32 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.3090
2022-02-23 09:46:05 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.2630
2022-02-23 09:46:38 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.1775
2022-02-23 09:47:11 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.4154
2022-02-23 09:47:45 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.4046
2022-02-23 09:48:18 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.3226
2022-02-23 09:48:51 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.2035
2022-02-23 09:49:25 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.1887
2022-02-23 09:49:58 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.3452
2022-02-23 09:50:31 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.4388
2022-02-23 09:51:04 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.1729
2022-02-23 09:51:37 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.4687
2022-02-23 09:52:10 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.1284
2022-02-23 09:52:44 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.0208
2022-02-23 09:53:17 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.5047
2022-02-23 09:53:50 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.1631
2022-02-23 09:54:24 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.2879
2022-02-23 09:54:58 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.4059
2022-02-23 09:55:31 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.2762
2022-02-23 09:56:05 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.3335
2022-02-23 09:56:40 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.3279
2022-02-23 09:57:14 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.4881
2022-02-23 09:57:49 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.2864
2022-02-23 09:58:24 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.2229
2022-02-23 09:58:58 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.1281
2022-02-23 09:59:00 - train: epoch 050, train_loss: 1.2609
2022-02-23 10:00:16 - eval: epoch: 050, acc1: 71.934%, acc5: 90.850%, test_loss: 1.1151, per_image_load_time: 2.369ms, per_image_inference_time: 0.593ms
2022-02-23 10:00:17 - until epoch: 050, best_acc1: 72.698%
2022-02-23 10:00:17 - epoch 051 lr: 0.010000000000000002
2022-02-23 10:00:55 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.4827
2022-02-23 10:01:28 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.3787
2022-02-23 10:02:00 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.1510
2022-02-23 10:02:33 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.2150
2022-02-23 10:03:05 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.0978
2022-02-23 10:03:37 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.2173
2022-02-23 10:04:10 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.1558
2022-02-23 10:04:42 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.3471
2022-02-23 10:05:15 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.0799
2022-02-23 10:05:48 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.4942
2022-02-23 10:06:20 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.3565
2022-02-23 10:06:53 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.1741
2022-02-23 10:07:25 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 0.9365
2022-02-23 10:07:58 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.2773
2022-02-23 10:08:31 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.3053
2022-02-23 10:09:04 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.3022
2022-02-23 10:09:36 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.2729
2022-02-23 10:10:09 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.2486
2022-02-23 10:10:42 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.1328
2022-02-23 10:11:15 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.1192
2022-02-23 10:11:48 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.2642
2022-02-23 10:12:21 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.2430
2022-02-23 10:12:53 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.0345
2022-02-23 10:13:26 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.2992
2022-02-23 10:13:59 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.1689
2022-02-23 10:14:33 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.2981
2022-02-23 10:15:06 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.2354
2022-02-23 10:15:39 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 0.9394
2022-02-23 10:16:12 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.2057
2022-02-23 10:16:45 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.1338
2022-02-23 10:17:18 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.1109
2022-02-23 10:17:51 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.1161
2022-02-23 10:18:24 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.4051
2022-02-23 10:18:57 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.3730
2022-02-23 10:19:30 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.1983
2022-02-23 10:20:03 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.3206
2022-02-23 10:20:36 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.3912
2022-02-23 10:21:10 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.2193
2022-02-23 10:21:43 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.2516
2022-02-23 10:22:16 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.1737
2022-02-23 10:22:49 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.2217
2022-02-23 10:23:22 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.2560
2022-02-23 10:23:56 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.2457
2022-02-23 10:24:29 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.3879
2022-02-23 10:25:03 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.1096
2022-02-23 10:25:37 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.3346
2022-02-23 10:26:12 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.2780
2022-02-23 10:26:47 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.1655
2022-02-23 10:27:22 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.2109
2022-02-23 10:27:56 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.1368
2022-02-23 10:27:58 - train: epoch 051, train_loss: 1.2576
2022-02-23 10:29:15 - eval: epoch: 051, acc1: 71.906%, acc5: 90.910%, test_loss: 1.1149, per_image_load_time: 2.388ms, per_image_inference_time: 0.582ms
2022-02-23 10:29:16 - until epoch: 051, best_acc1: 72.698%
2022-02-23 10:29:16 - epoch 052 lr: 0.010000000000000002
2022-02-23 10:29:55 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.2458
2022-02-23 10:30:27 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.2342
2022-02-23 10:31:00 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.2096
2022-02-23 10:31:32 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.0889
2022-02-23 10:32:05 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.4007
2022-02-23 10:32:38 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.2332
2022-02-23 10:33:11 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.3249
2022-02-23 10:33:44 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.1669
2022-02-23 10:34:17 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.2271
2022-02-23 10:34:50 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.3738
2022-02-23 10:35:22 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.2509
2022-02-23 10:35:55 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.1340
2022-02-23 10:36:29 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.1452
2022-02-23 10:37:02 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.4782
2022-02-23 10:37:34 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.1775
2022-02-23 10:38:07 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.0444
2022-02-23 10:38:40 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.2170
2022-02-23 10:39:13 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.1324
2022-02-23 10:39:46 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.1004
2022-02-23 10:40:19 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.3431
2022-02-23 10:40:52 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.1748
2022-02-23 10:41:25 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.5540
2022-02-23 10:41:58 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.1165
2022-02-23 10:42:31 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 0.9923
2022-02-23 10:43:04 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.2148
2022-02-23 10:43:37 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.0178
2022-02-23 10:44:10 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.2933
2022-02-23 10:44:43 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.4062
2022-02-23 10:45:16 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.2267
2022-02-23 10:45:49 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.2483
2022-02-23 10:46:22 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.2535
2022-02-23 10:46:55 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.1997
2022-02-23 10:47:28 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.1114
2022-02-23 10:48:01 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.4912
2022-02-23 10:48:34 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.3247
2022-02-23 10:49:07 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.4026
2022-02-23 10:49:41 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.3323
2022-02-23 10:50:14 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.1970
2022-02-23 10:50:47 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.1553
2022-02-23 10:51:20 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.4978
2022-02-23 10:51:53 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 0.9810
2022-02-23 10:52:27 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.1989
2022-02-23 10:53:00 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.2507
2022-02-23 10:53:34 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.3191
2022-02-23 10:54:08 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.1565
2022-02-23 10:54:42 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.1132
2022-02-23 10:55:16 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.1851
2022-02-23 10:55:51 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.1388
2022-02-23 10:56:26 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.1546
2022-02-23 10:57:01 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.2995
2022-02-23 10:57:02 - train: epoch 052, train_loss: 1.2553
2022-02-23 10:58:20 - eval: epoch: 052, acc1: 72.150%, acc5: 90.968%, test_loss: 1.1126, per_image_load_time: 2.364ms, per_image_inference_time: 0.572ms
2022-02-23 10:58:21 - until epoch: 052, best_acc1: 72.698%
2022-02-23 10:58:21 - epoch 053 lr: 0.010000000000000002
2022-02-23 10:58:59 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.2802
2022-02-23 10:59:31 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.2329
2022-02-23 11:00:04 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.2335
2022-02-23 11:00:37 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.3182
2022-02-23 11:01:10 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.2085
2022-02-23 11:01:42 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.3095
2022-02-23 11:02:15 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.1642
2022-02-23 11:02:48 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.2716
2022-02-23 11:03:21 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.2863
2022-02-23 11:03:53 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.1582
2022-02-23 11:04:25 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.3062
2022-02-23 11:04:58 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.2868
2022-02-23 11:05:31 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.3598
2022-02-23 11:06:03 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.3476
2022-02-23 11:06:36 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.2529
2022-02-23 11:07:09 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.3624
2022-02-23 11:07:42 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.3472
2022-02-23 11:08:15 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.2857
2022-02-23 11:08:48 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.1401
2022-02-23 11:09:21 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.3228
2022-02-23 11:09:53 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.4364
2022-02-23 11:10:26 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.2773
2022-02-23 11:10:59 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.2794
2022-02-23 11:11:32 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.1996
2022-02-23 11:12:05 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.2588
2022-02-23 11:12:37 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.4471
2022-02-23 11:13:10 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.2319
2022-02-23 11:13:43 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.2511
2022-02-23 11:14:16 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.1970
2022-02-23 11:14:49 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.2110
2022-02-23 11:15:22 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.4723
2022-02-23 11:15:55 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.4118
2022-02-23 11:16:28 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.3044
2022-02-23 11:17:01 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.1681
2022-02-23 11:17:34 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.0443
2022-02-23 11:18:07 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.2978
2022-02-23 11:18:41 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.4533
2022-02-23 11:19:14 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.2587
2022-02-23 11:19:48 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.4555
2022-02-23 11:20:21 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.3319
2022-02-23 11:20:54 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.2100
2022-02-23 11:21:28 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.4346
2022-02-23 11:22:01 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.4017
2022-02-23 11:22:35 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.3413
2022-02-23 11:23:09 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.4087
2022-02-23 11:23:43 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.2423
2022-02-23 11:24:17 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.1072
2022-02-23 11:24:52 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.5168
2022-02-23 11:25:27 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.2360
2022-02-23 11:26:01 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.1833
2022-02-23 11:26:03 - train: epoch 053, train_loss: 1.2535
2022-02-23 11:27:21 - eval: epoch: 053, acc1: 72.100%, acc5: 91.050%, test_loss: 1.1045, per_image_load_time: 2.418ms, per_image_inference_time: 0.573ms
2022-02-23 11:27:21 - until epoch: 053, best_acc1: 72.698%
2022-02-23 11:27:21 - epoch 054 lr: 0.010000000000000002
2022-02-23 11:28:00 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.2230
2022-02-23 11:28:32 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.2824
2022-02-23 11:29:05 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.1982
2022-02-23 11:29:37 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.2856
2022-02-23 11:30:10 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.1355
2022-02-23 11:30:42 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.1747
2022-02-23 11:31:15 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.2939
2022-02-23 11:31:47 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.3705
2022-02-23 11:32:20 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.0427
2022-02-23 11:32:53 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.0311
2022-02-23 11:33:26 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 0.9704
2022-02-23 11:33:58 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.2876
2022-02-23 11:34:31 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.2705
2022-02-23 11:35:04 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.5382
2022-02-23 11:35:37 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.2067
2022-02-23 11:36:10 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.0059
2022-02-23 11:36:42 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.0521
2022-02-23 11:37:15 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.2046
2022-02-23 11:37:48 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.3796
2022-02-23 11:38:20 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.1280
2022-02-23 11:38:53 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.1516
2022-02-23 11:39:26 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.2011
2022-02-23 11:39:59 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.1617
2022-02-23 11:40:31 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.3050
2022-02-23 11:41:04 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.2995
2022-02-23 11:41:37 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.2785
2022-02-23 11:42:10 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.1828
2022-02-23 11:42:43 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.3271
2022-02-23 11:43:15 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.2281
2022-02-23 11:43:48 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.3282
2022-02-23 11:44:21 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.3825
2022-02-23 11:44:54 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.2401
2022-02-23 11:45:27 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.3385
2022-02-23 11:46:00 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.2650
2022-02-23 11:46:33 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.3653
2022-02-23 11:47:06 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.2757
2022-02-23 11:47:39 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.2898
2022-02-23 11:48:12 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.1261
2022-02-23 11:48:46 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.2532
2022-02-23 11:49:19 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.1457
2022-02-23 11:49:52 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.2643
2022-02-23 11:50:25 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.1814
2022-02-23 11:50:59 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.2969
2022-02-23 11:51:32 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.2444
2022-02-23 11:52:06 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.2485
2022-02-23 11:52:40 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.4097
2022-02-23 11:53:14 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.2762
2022-02-23 11:53:49 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.3034
2022-02-23 11:54:24 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.2769
2022-02-23 11:54:59 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.1813
2022-02-23 11:55:00 - train: epoch 054, train_loss: 1.2501
2022-02-23 11:56:17 - eval: epoch: 054, acc1: 72.024%, acc5: 91.008%, test_loss: 1.1117, per_image_load_time: 2.207ms, per_image_inference_time: 0.595ms
2022-02-23 11:56:18 - until epoch: 054, best_acc1: 72.698%
2022-02-23 11:56:18 - epoch 055 lr: 0.010000000000000002
2022-02-23 11:56:56 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.1171
2022-02-23 11:57:28 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.0504
2022-02-23 11:58:01 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.0660
2022-02-23 11:58:34 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.1635
2022-02-23 11:59:06 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.1636
2022-02-23 11:59:39 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.2971
2022-02-23 12:00:12 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.0536
2022-02-23 12:00:44 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.0525
2022-02-23 12:01:17 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.3281
2022-02-23 12:01:50 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.3017
2022-02-23 12:02:23 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.2874
2022-02-23 12:02:55 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.1791
2022-02-23 12:03:28 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.3269
2022-02-23 12:04:01 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.1643
2022-02-23 12:04:34 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.3147
2022-02-23 12:05:07 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.3294
2022-02-23 12:05:41 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.1819
2022-02-23 12:06:14 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.2756
2022-02-23 12:06:47 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.2543
2022-02-23 12:07:20 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.2749
2022-02-23 12:07:53 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.1788
2022-02-23 12:08:26 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.4554
2022-02-23 12:08:59 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.1571
2022-02-23 12:09:32 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.2677
2022-02-23 12:10:05 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.2389
2022-02-23 12:10:38 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.2581
2022-02-23 12:11:11 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.1929
2022-02-23 12:11:44 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.2350
2022-02-23 12:12:17 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.2737
2022-02-23 12:12:50 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.3701
2022-02-23 12:13:23 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.3919
2022-02-23 12:13:56 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.2119
2022-02-23 12:14:29 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.1238
2022-02-23 12:15:02 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.1083
2022-02-23 12:15:35 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.3134
2022-02-23 12:16:08 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.4075
2022-02-23 12:16:41 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.2185
2022-02-23 12:17:14 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.2633
2022-02-23 12:17:48 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.5372
2022-02-23 12:18:21 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.3376
2022-02-23 12:18:54 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.1547
2022-02-23 12:19:28 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.3477
2022-02-23 12:20:02 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.3724
2022-02-23 12:20:35 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.3732
2022-02-23 12:21:09 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.2502
2022-02-23 12:21:43 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.2760
2022-02-23 12:22:18 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.1580
2022-02-23 12:22:52 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.2410
2022-02-23 12:23:28 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.1392
2022-02-23 12:24:01 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.3504
2022-02-23 12:24:02 - train: epoch 055, train_loss: 1.2481
2022-02-23 12:25:20 - eval: epoch: 055, acc1: 72.110%, acc5: 91.008%, test_loss: 1.1080, per_image_load_time: 2.451ms, per_image_inference_time: 0.570ms
2022-02-23 12:25:21 - until epoch: 055, best_acc1: 72.698%
2022-02-23 12:25:21 - epoch 056 lr: 0.010000000000000002
2022-02-23 12:25:59 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.0970
2022-02-23 12:26:32 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.1479
2022-02-23 12:27:04 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.0001
2022-02-23 12:27:36 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.2829
2022-02-23 12:28:09 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.1645
2022-02-23 12:28:41 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.2617
2022-02-23 12:29:14 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.4613
2022-02-23 12:29:46 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.2228
2022-02-23 12:30:18 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.3769
2022-02-23 12:30:50 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.2269
2022-02-23 12:31:23 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.1749
2022-02-23 12:31:56 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.2953
2022-02-23 12:32:28 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.2541
2022-02-23 12:33:01 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.1503
2022-02-23 12:33:33 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.5130
2022-02-23 12:34:06 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.2687
2022-02-23 12:34:39 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.1671
2022-02-23 12:35:12 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.4706
2022-02-23 12:35:45 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.3809
2022-02-23 12:36:17 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.3706
2022-02-23 12:36:50 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.4725
2022-02-23 12:37:23 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.2615
2022-02-23 12:37:56 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.2375
2022-02-23 12:38:28 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.1844
2022-02-23 12:39:02 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.3547
2022-02-23 12:39:35 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.1907
2022-02-23 12:40:07 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.1952
2022-02-23 12:40:41 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.2426
2022-02-23 12:41:14 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.2778
2022-02-23 12:41:47 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.3326
2022-02-23 12:42:20 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.1364
2022-02-23 12:42:53 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.2098
2022-02-23 12:43:26 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.2833
2022-02-23 12:43:59 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.0713
2022-02-23 12:44:32 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.2093
2022-02-23 12:45:05 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.0579
2022-02-23 12:45:39 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.1566
2022-02-23 12:46:12 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.2502
2022-02-23 12:46:45 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.3755
2022-02-23 12:47:19 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.3831
2022-02-23 12:47:52 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.3789
2022-02-23 12:48:26 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.5298
2022-02-23 12:48:59 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.3101
2022-02-23 12:49:33 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.2633
2022-02-23 12:50:07 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.3184
2022-02-23 12:50:40 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.3339
2022-02-23 12:51:15 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.1345
2022-02-23 12:51:49 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.3177
2022-02-23 12:52:25 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.2373
2022-02-23 12:52:59 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.3365
2022-02-23 12:53:01 - train: epoch 056, train_loss: 1.2466
2022-02-23 12:54:18 - eval: epoch: 056, acc1: 72.468%, acc5: 91.122%, test_loss: 1.0948, per_image_load_time: 2.416ms, per_image_inference_time: 0.591ms
2022-02-23 12:54:19 - until epoch: 056, best_acc1: 72.698%
2022-02-23 12:54:19 - epoch 057 lr: 0.010000000000000002
2022-02-23 12:54:59 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.1779
2022-02-23 12:55:32 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.3336
2022-02-23 12:56:04 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.3414
2022-02-23 12:56:37 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.0502
2022-02-23 12:57:10 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.1173
2022-02-23 12:57:42 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.5131
2022-02-23 12:58:15 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 0.9958
2022-02-23 12:58:47 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.2148
2022-02-23 12:59:19 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.2470
2022-02-23 12:59:52 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 0.9939
2022-02-23 13:00:25 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.0664
2022-02-23 13:00:58 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.2131
2022-02-23 13:01:31 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.2687
2022-02-23 13:02:04 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.1927
2022-02-23 13:02:37 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.1271
2022-02-23 13:03:10 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.2844
2022-02-23 13:03:43 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.3456
2022-02-23 13:04:16 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.2600
2022-02-23 13:04:49 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.2765
2022-02-23 13:05:22 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.3649
2022-02-23 13:05:55 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.3464
2022-02-23 13:06:28 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.1694
2022-02-23 13:07:01 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.2082
2022-02-23 13:07:34 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.2994
2022-02-23 13:08:07 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.2997
2022-02-23 13:08:40 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 0.9908
2022-02-23 13:09:13 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.0876
2022-02-23 13:09:46 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 0.9915
2022-02-23 13:10:19 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.1917
2022-02-23 13:10:53 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.4075
2022-02-23 13:11:26 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.2367
2022-02-23 13:11:59 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.1703
2022-02-23 13:12:32 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.3143
2022-02-23 13:13:05 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.2748
2022-02-23 13:13:38 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.2411
2022-02-23 13:14:11 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.3114
2022-02-23 13:14:44 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.2588
2022-02-23 13:15:17 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.0992
2022-02-23 13:15:51 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.2670
2022-02-23 13:16:23 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.1906
2022-02-23 13:16:57 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.5522
2022-02-23 13:17:30 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.3145
2022-02-23 13:18:04 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.1613
2022-02-23 13:18:37 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.2492
2022-02-23 13:19:11 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.4515
2022-02-23 13:19:45 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.1318
2022-02-23 13:20:19 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.4212
2022-02-23 13:20:54 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.5373
2022-02-23 13:21:29 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.4254
2022-02-23 13:22:03 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.3601
2022-02-23 13:22:05 - train: epoch 057, train_loss: 1.2417
2022-02-23 13:23:22 - eval: epoch: 057, acc1: 72.150%, acc5: 91.002%, test_loss: 1.1068, per_image_load_time: 2.361ms, per_image_inference_time: 0.565ms
2022-02-23 13:23:23 - until epoch: 057, best_acc1: 72.698%
2022-02-23 13:23:23 - epoch 058 lr: 0.010000000000000002
2022-02-23 13:24:02 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.2736
2022-02-23 13:24:35 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.0914
2022-02-23 13:25:08 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.2257
2022-02-23 13:25:41 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.2996
2022-02-23 13:26:14 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.1922
2022-02-23 13:26:47 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.3799
2022-02-23 13:27:19 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.2567
2022-02-23 13:27:52 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.1697
2022-02-23 13:28:25 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.0423
2022-02-23 13:28:57 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.1084
2022-02-23 13:29:30 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.1781
2022-02-23 13:30:03 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.2772
2022-02-23 13:30:36 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.3540
2022-02-23 13:31:09 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.2445
2022-02-23 13:31:42 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.3020
2022-02-23 13:32:15 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.1521
2022-02-23 13:32:48 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.3116
2022-02-23 13:33:21 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.3751
2022-02-23 13:33:53 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.3499
2022-02-23 13:34:27 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.2496
2022-02-23 13:35:00 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.1058
2022-02-23 13:35:33 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.1811
2022-02-23 13:36:06 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.1651
2022-02-23 13:36:39 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.2714
2022-02-23 13:37:11 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.3206
2022-02-23 13:37:44 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.2244
2022-02-23 13:38:17 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.4811
2022-02-23 13:38:49 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.1524
2022-02-23 13:39:23 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.2690
2022-02-23 13:39:56 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.4411
2022-02-23 13:40:28 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.1413
2022-02-23 13:41:01 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.1725
2022-02-23 13:41:34 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.2448
2022-02-23 13:42:07 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.1825
2022-02-23 13:42:40 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.2346
2022-02-23 13:43:13 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.1423
2022-02-23 13:43:47 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.1931
2022-02-23 13:44:20 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.3352
2022-02-23 13:44:53 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.3929
2022-02-23 13:45:27 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.1995
2022-02-23 13:46:00 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.3126
2022-02-23 13:46:33 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.1154
2022-02-23 13:47:07 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.3117
2022-02-23 13:47:41 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.1655
2022-02-23 13:48:15 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.1906
2022-02-23 13:48:49 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.2295
2022-02-23 13:49:23 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.2594
2022-02-23 13:49:58 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.2316
2022-02-23 13:50:33 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.1563
2022-02-23 13:51:08 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.0582
2022-02-23 13:51:10 - train: epoch 058, train_loss: 1.2397
2022-02-23 13:52:27 - eval: epoch: 058, acc1: 72.206%, acc5: 91.148%, test_loss: 1.1025, per_image_load_time: 2.401ms, per_image_inference_time: 0.559ms
2022-02-23 13:52:28 - until epoch: 058, best_acc1: 72.698%
2022-02-23 13:52:28 - epoch 059 lr: 0.010000000000000002
2022-02-23 13:53:07 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.2521
2022-02-23 13:53:39 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.0750
2022-02-23 13:54:12 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.1721
2022-02-23 13:54:45 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.2316
2022-02-23 13:55:19 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.3379
2022-02-23 13:55:52 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.0871
2022-02-23 13:56:25 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.0537
2022-02-23 13:56:58 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.2530
2022-02-23 13:57:31 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.2301
2022-02-23 13:58:04 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.3538
2022-02-23 13:58:37 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 1.3215
2022-02-23 13:59:10 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.1884
2022-02-23 13:59:43 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.5148
2022-02-23 14:00:16 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.5016
2022-02-23 14:00:49 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.0934
2022-02-23 14:01:22 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.1677
2022-02-23 14:01:55 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.1990
2022-02-23 14:02:28 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.1056
2022-02-23 14:03:02 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.1690
2022-02-23 14:03:35 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.2097
2022-02-23 14:04:08 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.3528
2022-02-23 14:04:41 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.3781
2022-02-23 14:05:14 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.3074
2022-02-23 14:05:48 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.4966
2022-02-23 14:06:21 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.3966
2022-02-23 14:06:54 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.1435
2022-02-23 14:07:27 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.2017
2022-02-23 14:08:00 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.5209
2022-02-23 14:08:33 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.1588
2022-02-23 14:09:06 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 1.4512
2022-02-23 14:09:40 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.0373
2022-02-23 14:10:13 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.4817
2022-02-23 14:10:46 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.0710
2022-02-23 14:11:19 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 1.5239
2022-02-23 14:11:53 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.2146
2022-02-23 14:12:26 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.2893
2022-02-23 14:12:59 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.3213
2022-02-23 14:13:33 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.2629
2022-02-23 14:14:06 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 0.9673
2022-02-23 14:14:40 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 1.5041
2022-02-23 14:15:13 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.0637
2022-02-23 14:15:47 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.1992
2022-02-23 14:16:21 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.3262
2022-02-23 14:16:55 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.2840
2022-02-23 14:17:29 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.2296
2022-02-23 14:18:03 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.4192
2022-02-23 14:18:38 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.1577
2022-02-23 14:19:12 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.2607
2022-02-23 14:19:47 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.2463
2022-02-23 14:20:22 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.3259
2022-02-23 14:20:24 - train: epoch 059, train_loss: 1.2374
2022-02-23 14:21:41 - eval: epoch: 059, acc1: 72.092%, acc5: 91.122%, test_loss: 1.1121, per_image_load_time: 2.271ms, per_image_inference_time: 0.596ms
2022-02-23 14:21:42 - until epoch: 059, best_acc1: 72.698%
2022-02-23 14:21:42 - epoch 060 lr: 0.010000000000000002
2022-02-23 14:22:21 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.0784
2022-02-23 14:22:54 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.1029
2022-02-23 14:23:27 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.0389
2022-02-23 14:24:00 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.3314
2022-02-23 14:24:33 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.3737
2022-02-23 14:25:05 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.4725
2022-02-23 14:25:38 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.2499
2022-02-23 14:26:12 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.4268
2022-02-23 14:26:45 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.3634
2022-02-23 14:27:18 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.1393
2022-02-23 14:27:51 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.2371
2022-02-23 14:28:24 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.2236
2022-02-23 14:28:57 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.3673
2022-02-23 14:29:30 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.5114
2022-02-23 14:30:04 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.2140
2022-02-23 14:30:37 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.2375
2022-02-23 14:31:10 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.2417
2022-02-23 14:31:44 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.2548
2022-02-23 14:32:17 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.3120
2022-02-23 14:32:50 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.3008
2022-02-23 14:33:24 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.2787
2022-02-23 14:33:57 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.3584
2022-02-23 14:34:31 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.1718
2022-02-23 14:35:04 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.1554
2022-02-23 14:35:37 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.1718
2022-02-23 14:36:11 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.3594
2022-02-23 14:36:44 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 0.9986
2022-02-23 14:37:18 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.2695
2022-02-23 14:37:51 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.1052
2022-02-23 14:38:25 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.2848
2022-02-23 14:38:58 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.4780
2022-02-23 14:39:32 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.0176
2022-02-23 14:40:05 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.0765
2022-02-23 14:40:39 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.3102
2022-02-23 14:41:12 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.2524
2022-02-23 14:41:46 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.2630
2022-02-23 14:42:19 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.2732
2022-02-23 14:42:52 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.3109
2022-02-23 14:43:26 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.3684
2022-02-23 14:43:59 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.3721
2022-02-23 14:44:32 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.4592
2022-02-23 14:45:06 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.2911
2022-02-23 14:45:39 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.0869
2022-02-23 14:46:13 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.3259
2022-02-23 14:46:46 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.1511
2022-02-23 14:47:20 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.1395
2022-02-23 14:47:55 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.0852
2022-02-23 14:48:29 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.2300
2022-02-23 14:49:04 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.1381
2022-02-23 14:49:38 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.3321
2022-02-23 14:49:40 - train: epoch 060, train_loss: 1.2333
2022-02-23 14:50:58 - eval: epoch: 060, acc1: 72.144%, acc5: 91.082%, test_loss: 1.1081, per_image_load_time: 1.991ms, per_image_inference_time: 0.579ms
2022-02-23 14:50:59 - until epoch: 060, best_acc1: 72.698%
2022-02-23 14:50:59 - epoch 061 lr: 0.0010000000000000002
2022-02-23 14:51:38 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 0.8420
2022-02-23 14:52:11 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.0673
2022-02-23 14:52:43 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 0.8710
2022-02-23 14:53:17 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.1731
2022-02-23 14:53:50 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.2241
2022-02-23 14:54:23 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 0.9623
2022-02-23 14:54:56 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.0559
2022-02-23 14:55:29 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.0278
2022-02-23 14:56:02 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.0953
2022-02-23 14:56:35 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.1121
2022-02-23 14:57:08 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 0.8279
2022-02-23 14:57:41 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 0.9938
2022-02-23 14:58:13 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 0.9758
2022-02-23 14:58:46 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 0.7791
2022-02-23 14:59:18 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.1388
2022-02-23 14:59:51 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 0.9482
2022-02-23 15:00:24 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.1871
2022-02-23 15:00:57 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.0359
2022-02-23 15:01:29 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 0.9996
2022-02-23 15:02:02 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 0.8662
2022-02-23 15:02:35 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.3024
2022-02-23 15:03:08 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 0.9853
2022-02-23 15:03:40 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.0464
2022-02-23 15:04:13 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 0.8634
2022-02-23 15:04:46 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 0.8983
2022-02-23 15:05:19 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.1227
2022-02-23 15:05:52 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.2318
2022-02-23 15:06:25 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 0.9909
2022-02-23 15:06:58 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.0040
2022-02-23 15:07:32 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 0.9283
2022-02-23 15:08:05 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.0114
2022-02-23 15:08:38 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.0344
2022-02-23 15:09:11 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 0.9141
2022-02-23 15:09:45 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 0.9502
2022-02-23 15:10:18 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.0265
2022-02-23 15:10:51 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 0.9479
2022-02-23 15:11:25 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 0.9402
2022-02-23 15:11:58 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 0.9475
2022-02-23 15:12:32 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.1576
2022-02-23 15:13:05 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.0353
2022-02-23 15:13:39 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.1882
2022-02-23 15:14:13 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 0.8686
2022-02-23 15:14:46 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.0466
2022-02-23 15:15:20 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 0.8990
2022-02-23 15:15:54 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 0.9544
2022-02-23 15:16:28 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 0.9071
2022-02-23 15:17:03 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 0.9636
2022-02-23 15:17:37 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 0.8782
2022-02-23 15:18:12 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 0.9522
2022-02-23 15:18:47 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.0172
2022-02-23 15:18:49 - train: epoch 061, train_loss: 1.0330
2022-02-23 15:20:06 - eval: epoch: 061, acc1: 75.744%, acc5: 92.920%, test_loss: 0.9506, per_image_load_time: 2.338ms, per_image_inference_time: 0.604ms
2022-02-23 15:20:07 - until epoch: 061, best_acc1: 75.744%
2022-02-23 15:20:07 - epoch 062 lr: 0.0010000000000000002
2022-02-23 15:20:45 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.1159
2022-02-23 15:21:18 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.3670
2022-02-23 15:21:51 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 0.9621
2022-02-23 15:22:23 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 0.9365
2022-02-23 15:22:56 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 0.8530
2022-02-23 15:23:28 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 0.8856
2022-02-23 15:24:01 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 0.9588
2022-02-23 15:24:34 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.0498
2022-02-23 15:25:07 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.0259
2022-02-23 15:25:40 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.0953
2022-02-23 15:26:12 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.0038
2022-02-23 15:26:46 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.0345
2022-02-23 15:27:19 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 0.9239
2022-02-23 15:27:52 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 0.9542
2022-02-23 15:28:25 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 0.8600
2022-02-23 15:28:58 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.1597
2022-02-23 15:29:31 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 0.9657
2022-02-23 15:30:04 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.0228
2022-02-23 15:30:37 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.0021
2022-02-23 15:31:10 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.0133
2022-02-23 15:31:43 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 0.9860
2022-02-23 15:32:17 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 0.8362
2022-02-23 15:32:50 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 0.9307
2022-02-23 15:33:23 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 0.7820
2022-02-23 15:33:56 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.0241
2022-02-23 15:34:29 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 0.8276
2022-02-23 15:35:02 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 0.8239
2022-02-23 15:35:35 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.2517
2022-02-23 15:36:09 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.0048
2022-02-23 15:36:42 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 0.9872
2022-02-23 15:37:15 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.0135
2022-02-23 15:37:48 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 0.9778
2022-02-23 15:38:22 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 0.9304
2022-02-23 15:38:55 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.0989
2022-02-23 15:39:28 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.0181
2022-02-23 15:40:01 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 0.9653
2022-02-23 15:40:35 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 0.9756
2022-02-23 15:41:08 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 0.8575
2022-02-23 15:41:41 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.0003
2022-02-23 15:42:15 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 0.8705
2022-02-23 15:42:48 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.1294
2022-02-23 15:43:22 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 0.8987
2022-02-23 15:43:55 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 0.8955
2022-02-23 15:44:29 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.0127
2022-02-23 15:45:03 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 0.9125
2022-02-23 15:45:37 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 0.8971
2022-02-23 15:46:12 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 0.9142
2022-02-23 15:46:46 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 0.9967
2022-02-23 15:47:21 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 0.9162
2022-02-23 15:47:55 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.0935
2022-02-23 15:47:58 - train: epoch 062, train_loss: 0.9782
2022-02-23 15:49:14 - eval: epoch: 062, acc1: 76.044%, acc5: 92.982%, test_loss: 0.9385, per_image_load_time: 2.349ms, per_image_inference_time: 0.606ms
2022-02-23 15:49:16 - until epoch: 062, best_acc1: 76.044%
2022-02-23 15:49:16 - epoch 063 lr: 0.0010000000000000002
2022-02-23 15:49:54 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 0.9486
2022-02-23 15:50:27 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 0.8809
2022-02-23 15:50:59 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 0.9548
2022-02-23 15:51:31 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.0438
2022-02-23 15:52:03 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 0.9757
2022-02-23 15:52:36 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 0.8010
2022-02-23 15:53:08 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.0106
2022-02-23 15:53:41 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 0.9249
2022-02-23 15:54:14 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.0162
2022-02-23 15:54:47 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 0.9916
2022-02-23 15:55:19 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 0.8536
2022-02-23 15:55:52 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 0.8985
2022-02-23 15:56:25 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 0.8345
2022-02-23 15:56:58 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.2835
2022-02-23 15:57:31 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 0.9762
2022-02-23 15:58:03 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 0.9881
2022-02-23 15:58:36 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 0.9278
2022-02-23 15:59:09 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.0764
2022-02-23 15:59:41 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.0625
2022-02-23 16:00:14 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 0.8733
2022-02-23 16:00:47 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 0.8698
2022-02-23 16:01:19 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.1615
2022-02-23 16:01:52 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.0247
2022-02-23 16:02:25 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 0.9115
2022-02-23 16:02:58 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.0371
2022-02-23 16:03:31 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 0.9375
2022-02-23 16:04:04 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.1650
2022-02-23 16:04:37 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.0653
2022-02-23 16:05:10 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 0.8779
2022-02-23 16:05:43 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.1133
2022-02-23 16:06:16 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.2271
2022-02-23 16:06:49 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 0.9617
2022-02-23 16:07:22 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 0.7390
2022-02-23 16:07:56 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 0.7526
2022-02-23 16:08:29 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 0.8824
2022-02-23 16:09:02 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 0.8680
2022-02-23 16:09:35 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.1925
2022-02-23 16:10:08 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.0161
2022-02-23 16:10:41 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 0.7228
2022-02-23 16:11:15 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 0.9158
2022-02-23 16:11:48 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.0054
2022-02-23 16:12:21 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.0656
2022-02-23 16:12:54 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 0.9813
2022-02-23 16:13:28 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 0.9853
2022-02-23 16:14:02 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.0593
2022-02-23 16:14:36 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 0.8256
2022-02-23 16:15:10 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 0.9656
2022-02-23 16:15:44 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 0.9233
2022-02-23 16:16:18 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.0411
2022-02-23 16:16:52 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 0.7848
2022-02-23 16:16:54 - train: epoch 063, train_loss: 0.9546
2022-02-23 16:18:12 - eval: epoch: 063, acc1: 76.324%, acc5: 93.146%, test_loss: 0.9341, per_image_load_time: 2.417ms, per_image_inference_time: 0.577ms
2022-02-23 16:18:13 - until epoch: 063, best_acc1: 76.324%
2022-02-23 16:18:13 - epoch 064 lr: 0.0010000000000000002
2022-02-23 16:18:52 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 0.9960
2022-02-23 16:19:25 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 0.8049
2022-02-23 16:19:58 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 0.7421
2022-02-23 16:20:31 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 0.9688
2022-02-23 16:21:04 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 0.8944
2022-02-23 16:21:37 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.0320
2022-02-23 16:22:10 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.0023
2022-02-23 16:22:43 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 0.9223
2022-02-23 16:23:16 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 0.8474
2022-02-23 16:23:49 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 0.8709
2022-02-23 16:24:22 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.1468
2022-02-23 16:24:55 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.0090
2022-02-23 16:25:29 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.0411
2022-02-23 16:26:01 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.0574
2022-02-23 16:26:34 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.0387
2022-02-23 16:27:07 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 0.8857
2022-02-23 16:27:41 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 0.8564
2022-02-23 16:28:14 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 0.8601
2022-02-23 16:28:47 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 0.9711
2022-02-23 16:29:19 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 0.7590
2022-02-23 16:29:53 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 0.9903
2022-02-23 16:30:26 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 0.9970
2022-02-23 16:30:59 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.1514
2022-02-23 16:31:32 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 0.8156
2022-02-23 16:32:05 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 0.9132
2022-02-23 16:32:39 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 0.9400
2022-02-23 16:33:12 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 0.9153
2022-02-23 16:33:45 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 0.9574
2022-02-23 16:34:18 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 0.9827
2022-02-23 16:34:52 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 0.7659
2022-02-23 16:35:25 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 0.9298
2022-02-23 16:35:59 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 0.9370
2022-02-23 16:36:32 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 0.9721
2022-02-23 16:37:05 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 0.9545
2022-02-23 16:37:39 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 0.8630
2022-02-23 16:38:12 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 0.8302
2022-02-23 16:38:45 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 0.7875
2022-02-23 16:39:19 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 0.9545
2022-02-23 16:39:52 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 0.8825
2022-02-23 16:40:26 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 0.7640
2022-02-23 16:40:59 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.0453
2022-02-23 16:41:32 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 0.9382
2022-02-23 16:42:06 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.0452
2022-02-23 16:42:40 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 0.8574
2022-02-23 16:43:13 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 0.9118
2022-02-23 16:43:47 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.0187
2022-02-23 16:44:21 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.1916
2022-02-23 16:44:56 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 0.9658
2022-02-23 16:45:30 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.0478
2022-02-23 16:46:05 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 0.7654
2022-02-23 16:46:07 - train: epoch 064, train_loss: 0.9405
2022-02-23 16:47:24 - eval: epoch: 064, acc1: 76.336%, acc5: 93.116%, test_loss: 0.9287, per_image_load_time: 1.836ms, per_image_inference_time: 0.581ms
2022-02-23 16:47:25 - until epoch: 064, best_acc1: 76.336%
2022-02-23 16:47:25 - epoch 065 lr: 0.0010000000000000002
2022-02-23 16:48:03 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 0.9192
2022-02-23 16:48:36 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 0.9250
2022-02-23 16:49:09 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 0.9518
2022-02-23 16:49:41 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 0.9480
2022-02-23 16:50:14 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 0.9006
2022-02-23 16:50:46 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.0723
2022-02-23 16:51:18 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 0.8653
2022-02-23 16:51:50 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.0541
2022-02-23 16:52:23 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.0071
2022-02-23 16:52:56 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 0.9826
2022-02-23 16:53:29 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 0.9343
2022-02-23 16:54:01 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.0624
2022-02-23 16:54:34 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.0623
2022-02-23 16:55:07 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 0.7006
2022-02-23 16:55:39 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 0.9310
2022-02-23 16:56:12 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 0.8922
2022-02-23 16:56:45 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 0.9739
2022-02-23 16:57:18 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 0.8905
2022-02-23 16:57:51 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 0.9523
2022-02-23 16:58:24 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 0.8714
2022-02-23 16:58:57 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 0.9291
2022-02-23 16:59:30 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 0.8503
2022-02-23 17:00:03 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.0191
2022-02-23 17:00:36 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 0.8363
2022-02-23 17:01:10 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 0.9549
2022-02-23 17:01:43 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.1006
2022-02-23 17:02:16 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 0.8350
2022-02-23 17:02:49 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.0033
2022-02-23 17:03:22 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 0.8801
2022-02-23 17:03:56 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.0407
2022-02-23 17:04:29 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 0.8894
2022-02-23 17:05:02 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 0.8397
2022-02-23 17:05:36 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 0.9788
2022-02-23 17:06:09 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 0.8721
2022-02-23 17:06:42 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.1671
2022-02-23 17:07:16 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.1109
2022-02-23 17:07:49 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 0.8053
2022-02-23 17:08:22 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 0.9358
2022-02-23 17:08:56 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.0167
2022-02-23 17:09:29 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.0482
2022-02-23 17:10:03 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 0.8931
2022-02-23 17:10:36 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 0.8203
2022-02-23 17:11:10 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 0.9918
2022-02-23 17:11:44 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.0055
2022-02-23 17:12:17 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.0190
2022-02-23 17:12:51 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 0.8264
2022-02-23 17:13:26 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 0.9016
2022-02-23 17:14:00 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 0.6975
2022-02-23 17:14:35 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 0.9366
2022-02-23 17:15:09 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 0.9240
2022-02-23 17:15:11 - train: epoch 065, train_loss: 0.9280
2022-02-23 17:16:29 - eval: epoch: 065, acc1: 76.420%, acc5: 93.180%, test_loss: 0.9271, per_image_load_time: 1.434ms, per_image_inference_time: 0.575ms
2022-02-23 17:16:30 - until epoch: 065, best_acc1: 76.420%
2022-02-23 17:16:30 - epoch 066 lr: 0.0010000000000000002
2022-02-23 17:17:08 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 0.6846
2022-02-23 17:17:42 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 0.9092
2022-02-23 17:18:16 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 0.9600
2022-02-23 17:18:49 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 0.8796
2022-02-23 17:19:23 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 0.9143
2022-02-23 17:19:56 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 0.8262
2022-02-23 17:20:30 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 0.9035
2022-02-23 17:21:03 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 0.9684
2022-02-23 17:21:37 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 0.8175
2022-02-23 17:22:10 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 0.8637
2022-02-23 17:22:43 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 0.9318
2022-02-23 17:23:16 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.0435
2022-02-23 17:23:49 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.0714
2022-02-23 17:24:22 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 0.9083
2022-02-23 17:24:55 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 0.9016
2022-02-23 17:25:28 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.0392
2022-02-23 17:26:02 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 0.8335
2022-02-23 17:26:35 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 0.8551
2022-02-23 17:27:09 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.0467
2022-02-23 17:27:42 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 0.8803
2022-02-23 17:28:15 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.0002
2022-02-23 17:28:48 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 0.8972
2022-02-23 17:29:21 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 0.8942
2022-02-23 17:29:54 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 0.8641
2022-02-23 17:30:27 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 0.9167
2022-02-23 17:31:01 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 0.8762
2022-02-23 17:31:34 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.0274
2022-02-23 17:32:07 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 0.7554
2022-02-23 17:32:40 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 0.8004
2022-02-23 17:33:14 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.1056
2022-02-23 17:33:47 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.1634
2022-02-23 17:34:20 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.0257
2022-02-23 17:34:54 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 0.8953
2022-02-23 17:35:27 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.0275
2022-02-23 17:36:01 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 0.9154
2022-02-23 17:36:34 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 0.9112
2022-02-23 17:37:07 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 0.9537
2022-02-23 17:37:41 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 0.8918
2022-02-23 17:38:14 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 0.8928
2022-02-23 17:38:48 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 0.9522
2022-02-23 17:39:21 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 0.8144
2022-02-23 17:39:54 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 0.8761
2022-02-23 17:40:28 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 0.7951
2022-02-23 17:41:02 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 0.9449
2022-02-23 17:41:35 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 0.8960
2022-02-23 17:42:09 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.0922
2022-02-23 17:42:43 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 0.7892
2022-02-23 17:43:18 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 0.9533
2022-02-23 17:43:53 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 0.9580
2022-02-23 17:44:27 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 0.8861
2022-02-23 17:44:29 - train: epoch 066, train_loss: 0.9196
2022-02-23 17:45:46 - eval: epoch: 066, acc1: 76.466%, acc5: 93.234%, test_loss: 0.9259, per_image_load_time: 2.408ms, per_image_inference_time: 0.579ms
2022-02-23 17:45:48 - until epoch: 066, best_acc1: 76.466%
2022-02-23 17:45:48 - epoch 067 lr: 0.0010000000000000002
2022-02-23 17:46:26 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 0.7258
2022-02-23 17:46:59 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 0.9129
2022-02-23 17:47:32 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 0.9880
2022-02-23 17:48:05 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.0202
2022-02-23 17:48:38 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 0.9515
2022-02-23 17:49:11 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 0.8353
2022-02-23 17:49:43 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 0.9634
2022-02-23 17:50:16 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 0.9141
2022-02-23 17:50:49 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.1773
2022-02-23 17:51:21 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 0.7849
2022-02-23 17:51:53 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 0.8770
2022-02-23 17:52:26 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 0.9226
2022-02-23 17:52:58 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 0.9226
2022-02-23 17:53:31 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 0.9047
2022-02-23 17:54:04 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 0.8850
2022-02-23 17:54:37 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 0.9246
2022-02-23 17:55:10 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 0.8829
2022-02-23 17:55:42 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.1013
2022-02-23 17:56:14 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 0.8919
2022-02-23 17:56:47 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 0.9138
2022-02-23 17:57:19 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 0.8369
2022-02-23 17:57:52 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 0.8902
2022-02-23 17:58:25 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 0.8888
2022-02-23 17:58:58 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 0.7807
2022-02-23 17:59:31 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 0.8559
2022-02-23 18:00:04 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.0109
2022-02-23 18:00:37 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 0.9883
2022-02-23 18:01:11 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.0569
2022-02-23 18:01:44 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 0.9194
2022-02-23 18:02:17 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 0.8825
2022-02-23 18:02:50 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 0.6690
2022-02-23 18:03:23 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.0745
2022-02-23 18:03:57 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 0.7499
2022-02-23 18:04:30 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 0.9826
2022-02-23 18:05:03 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 0.8351
2022-02-23 18:05:36 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 0.9934
2022-02-23 18:06:09 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.0566
2022-02-23 18:06:42 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.0891
2022-02-23 18:07:15 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 0.9202
2022-02-23 18:07:49 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 0.9487
2022-02-23 18:08:22 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 0.9525
2022-02-23 18:08:55 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 0.9686
2022-02-23 18:09:29 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 0.7587
2022-02-23 18:10:02 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 0.9445
2022-02-23 18:10:36 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 0.9527
2022-02-23 18:11:10 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 0.8308
2022-02-23 18:11:44 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 0.8563
2022-02-23 18:12:18 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 0.7804
2022-02-23 18:12:53 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 0.8317
2022-02-23 18:13:27 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.0454
2022-02-23 18:13:29 - train: epoch 067, train_loss: 0.9114
2022-02-23 18:14:47 - eval: epoch: 067, acc1: 76.366%, acc5: 93.212%, test_loss: 0.9255, per_image_load_time: 1.123ms, per_image_inference_time: 0.575ms
2022-02-23 18:14:48 - until epoch: 067, best_acc1: 76.466%
2022-02-23 18:14:48 - epoch 068 lr: 0.0010000000000000002
2022-02-23 18:15:27 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.0917
2022-02-23 18:15:59 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 0.9605
2022-02-23 18:16:32 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 0.7595
2022-02-23 18:17:05 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 0.7433
2022-02-23 18:17:37 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 0.7981
2022-02-23 18:18:10 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 0.9543
2022-02-23 18:18:43 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 0.9402
2022-02-23 18:19:16 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 0.8298
2022-02-23 18:19:49 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 0.9656
2022-02-23 18:20:22 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 0.9696
2022-02-23 18:20:55 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.1002
2022-02-23 18:21:28 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 0.7659
2022-02-23 18:22:01 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 0.8095
2022-02-23 18:22:34 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 0.9209
2022-02-23 18:23:07 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.0429
2022-02-23 18:23:40 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.0578
2022-02-23 18:24:13 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 0.7325
2022-02-23 18:24:46 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 0.8855
2022-02-23 18:25:18 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 0.9945
2022-02-23 18:25:51 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 0.8518
2022-02-23 18:26:24 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 0.8377
2022-02-23 18:26:57 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 0.9380
2022-02-23 18:27:29 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 0.8683
2022-02-23 18:28:02 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 0.7655
2022-02-23 18:28:35 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 0.9507
2022-02-23 18:29:08 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 0.7041
2022-02-23 18:29:40 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 0.8661
2022-02-23 18:30:13 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 0.9710
2022-02-23 18:30:46 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.0690
2022-02-23 18:31:19 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.0035
2022-02-23 18:31:52 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.0160
2022-02-23 18:32:25 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 0.8678
2022-02-23 18:32:58 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 0.8671
2022-02-23 18:33:31 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 0.7827
2022-02-23 18:34:04 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 0.9754
2022-02-23 18:34:36 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 0.8145
2022-02-23 18:35:10 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 0.9178
2022-02-23 18:35:43 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.0419
2022-02-23 18:36:16 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 0.9742
2022-02-23 18:36:49 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 0.9503
2022-02-23 18:37:22 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 0.9919
2022-02-23 18:37:55 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 0.8662
2022-02-23 18:38:29 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 0.9081
2022-02-23 18:39:02 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 0.7303
2022-02-23 18:39:36 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 0.9314
2022-02-23 18:40:09 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 0.8444
2022-02-23 18:40:43 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.0333
2022-02-23 18:41:17 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 0.9114
2022-02-23 18:41:51 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 0.7605
2022-02-23 18:42:25 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 0.8583
2022-02-23 18:42:28 - train: epoch 068, train_loss: 0.9014
2022-02-23 18:43:44 - eval: epoch: 068, acc1: 76.486%, acc5: 93.314%, test_loss: 0.9246, per_image_load_time: 1.163ms, per_image_inference_time: 0.579ms
2022-02-23 18:43:45 - until epoch: 068, best_acc1: 76.486%
2022-02-23 18:43:45 - epoch 069 lr: 0.0010000000000000002
2022-02-23 18:44:24 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 0.9907
2022-02-23 18:44:57 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 0.8117
2022-02-23 18:45:30 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 0.9648
2022-02-23 18:46:03 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 0.7348
2022-02-23 18:46:36 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 0.8884
2022-02-23 18:47:08 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 0.8890
2022-02-23 18:47:41 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 0.9819
2022-02-23 18:48:13 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 0.9905
2022-02-23 18:48:46 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 0.8130
2022-02-23 18:49:19 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 0.9929
2022-02-23 18:49:52 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 0.7829
2022-02-23 18:50:24 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 0.8572
2022-02-23 18:50:57 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 0.9442
2022-02-23 18:51:30 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 0.7705
2022-02-23 18:52:02 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 0.9451
2022-02-23 18:52:35 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.0131
2022-02-23 18:53:08 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 0.8996
2022-02-23 18:53:40 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 0.9278
2022-02-23 18:54:13 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 0.8562
2022-02-23 18:54:45 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 0.8335
2022-02-23 18:55:18 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 0.8263
2022-02-23 18:55:51 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 0.9943
2022-02-23 18:56:24 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 0.8492
2022-02-23 18:56:57 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 0.9716
2022-02-23 18:57:29 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 0.8331
2022-02-23 18:58:02 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 0.9313
2022-02-23 18:58:35 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.0722
2022-02-23 18:59:08 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 0.9398
2022-02-23 18:59:41 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 0.9409
2022-02-23 19:00:14 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 0.8886
2022-02-23 19:00:47 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 0.8996
2022-02-23 19:01:20 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 0.8936
2022-02-23 19:01:53 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 0.8946
2022-02-23 19:02:26 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 0.8357
2022-02-23 19:02:59 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 0.6969
2022-02-23 19:03:33 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 0.6651
2022-02-23 19:04:06 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 0.9665
2022-02-23 19:04:39 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 0.9224
2022-02-23 19:05:13 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 0.8214
2022-02-23 19:05:45 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 0.9226
2022-02-23 19:06:19 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 0.9458
2022-02-23 19:06:52 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 0.9183
2022-02-23 19:07:25 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 0.8326
2022-02-23 19:07:59 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 0.8460
2022-02-23 19:08:32 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.0003
2022-02-23 19:09:06 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 0.9623
2022-02-23 19:09:40 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 0.9137
2022-02-23 19:10:14 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.0003
2022-02-23 19:10:48 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 0.8225
2022-02-23 19:11:22 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 0.9623
2022-02-23 19:11:25 - train: epoch 069, train_loss: 0.8951
2022-02-23 19:12:43 - eval: epoch: 069, acc1: 76.436%, acc5: 93.298%, test_loss: 0.9227, per_image_load_time: 2.420ms, per_image_inference_time: 0.562ms
2022-02-23 19:12:44 - until epoch: 069, best_acc1: 76.486%
2022-02-23 19:12:44 - epoch 070 lr: 0.0010000000000000002
2022-02-23 19:13:23 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 0.8453
2022-02-23 19:13:56 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 0.8097
2022-02-23 19:14:29 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 0.9041
2022-02-23 19:15:02 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 0.9183
2022-02-23 19:15:34 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 0.8764
2022-02-23 19:16:06 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 0.8735
2022-02-23 19:16:39 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 0.9035
2022-02-23 19:17:12 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 0.8796
2022-02-23 19:17:44 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 0.7594
2022-02-23 19:18:17 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 0.8682
2022-02-23 19:18:50 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.1225
2022-02-23 19:19:22 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 0.7919
2022-02-23 19:19:55 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 0.8161
2022-02-23 19:20:27 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 0.8004
2022-02-23 19:20:59 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 0.9532
2022-02-23 19:21:32 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 0.9410
2022-02-23 19:22:04 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 0.8830
2022-02-23 19:22:36 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 0.7782
2022-02-23 19:23:08 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 0.9207
2022-02-23 19:23:41 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 0.9283
2022-02-23 19:24:14 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 0.8936
2022-02-23 19:24:46 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 0.9862
2022-02-23 19:25:19 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.0067
2022-02-23 19:25:52 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 0.9319
2022-02-23 19:26:25 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 0.8596
2022-02-23 19:26:58 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 0.8366
2022-02-23 19:27:31 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 0.8716
2022-02-23 19:28:03 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 0.9523
2022-02-23 19:28:36 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 0.8039
2022-02-23 19:29:09 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 0.7986
2022-02-23 19:29:42 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 0.7888
2022-02-23 19:30:15 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 0.9290
2022-02-23 19:30:47 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 0.7532
2022-02-23 19:31:20 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 0.9349
2022-02-23 19:31:53 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.0347
2022-02-23 19:32:27 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 0.8489
2022-02-23 19:33:00 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.0042
2022-02-23 19:33:33 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 0.7660
2022-02-23 19:34:06 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 0.7602
2022-02-23 19:34:39 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 0.9197
2022-02-23 19:35:13 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 0.7619
2022-02-23 19:35:46 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 0.8874
2022-02-23 19:36:19 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 0.9059
2022-02-23 19:36:53 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 0.8062
2022-02-23 19:37:26 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 0.8718
2022-02-23 19:38:00 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.0091
2022-02-23 19:38:33 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 0.8842
2022-02-23 19:39:08 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.0282
2022-02-23 19:39:42 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 0.8449
2022-02-23 19:40:16 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 0.8527
2022-02-23 19:40:18 - train: epoch 070, train_loss: 0.8900
2022-02-23 19:41:37 - eval: epoch: 070, acc1: 76.598%, acc5: 93.256%, test_loss: 0.9212, per_image_load_time: 1.488ms, per_image_inference_time: 0.542ms
2022-02-23 19:41:38 - until epoch: 070, best_acc1: 76.598%
2022-02-23 19:41:38 - epoch 071 lr: 0.0010000000000000002
2022-02-23 19:42:17 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 0.8223
2022-02-23 19:42:50 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 0.7371
2022-02-23 19:43:23 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 0.9992
2022-02-23 19:43:55 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 0.9130
2022-02-23 19:44:28 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 0.9916
2022-02-23 19:45:00 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 0.9278
2022-02-23 19:45:32 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 0.8129
2022-02-23 19:46:04 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.0991
2022-02-23 19:46:37 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 0.8593
2022-02-23 19:47:10 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 0.7747
2022-02-23 19:47:42 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.0115
2022-02-23 19:48:14 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 0.8071
2022-02-23 19:48:47 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 0.8387
2022-02-23 19:49:19 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 0.7592
2022-02-23 19:49:52 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 0.8299
2022-02-23 19:50:25 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 0.8157
2022-02-23 19:50:58 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 0.9206
2022-02-23 19:51:30 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.0192
2022-02-23 19:52:03 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 0.8228
2022-02-23 19:52:36 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 0.9916
2022-02-23 19:53:09 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 0.7307
2022-02-23 19:53:42 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 0.8423
2022-02-23 19:54:15 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 0.8617
2022-02-23 19:54:48 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 0.8198
2022-02-23 19:55:21 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 0.9194
2022-02-23 19:55:54 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 0.8928
2022-02-23 19:56:27 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 0.9053
2022-02-23 19:57:00 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 0.9875
2022-02-23 19:57:33 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 0.8340
2022-02-23 19:58:06 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.0972
2022-02-23 19:58:39 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 0.8850
2022-02-23 19:59:12 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 0.8603
2022-02-23 19:59:45 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 0.9012
2022-02-23 20:00:18 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 0.7337
2022-02-23 20:00:51 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 0.9416
2022-02-23 20:01:24 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 0.9383
2022-02-23 20:01:57 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 0.9914
2022-02-23 20:02:30 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 0.8893
2022-02-23 20:03:03 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 0.9165
2022-02-23 20:03:37 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 0.9239
2022-02-23 20:04:10 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 0.8978
2022-02-23 20:04:43 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.0624
2022-02-23 20:05:16 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 0.7607
2022-02-23 20:05:50 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 0.9137
2022-02-23 20:06:24 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 0.7428
2022-02-23 20:06:57 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 0.9232
2022-02-23 20:07:31 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 0.6892
2022-02-23 20:08:05 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 0.7865
2022-02-23 20:08:40 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 0.8463
2022-02-23 20:09:14 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 0.8765
2022-02-23 20:09:16 - train: epoch 071, train_loss: 0.8841
2022-02-23 20:10:34 - eval: epoch: 071, acc1: 76.468%, acc5: 93.278%, test_loss: 0.9216, per_image_load_time: 1.515ms, per_image_inference_time: 0.560ms
2022-02-23 20:10:35 - until epoch: 071, best_acc1: 76.598%
2022-02-23 20:10:35 - epoch 072 lr: 0.0010000000000000002
2022-02-23 20:11:14 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 0.9583
2022-02-23 20:11:47 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 0.8652
2022-02-23 20:12:19 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 0.9607
2022-02-23 20:12:52 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 0.8237
2022-02-23 20:13:24 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 0.9340
2022-02-23 20:13:57 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 0.9031
2022-02-23 20:14:29 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 0.9348
2022-02-23 20:15:01 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.0604
2022-02-23 20:15:34 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 0.7591
2022-02-23 20:16:06 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 0.7899
2022-02-23 20:16:39 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 0.8621
2022-02-23 20:17:11 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 0.8179
2022-02-23 20:17:44 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 0.8937
2022-02-23 20:18:16 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 0.7577
2022-02-23 20:18:49 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 0.7986
2022-02-23 20:19:21 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 0.8801
2022-02-23 20:19:53 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 0.8144
2022-02-23 20:20:26 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 0.8054
2022-02-23 20:20:58 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 0.9128
2022-02-23 20:21:31 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 0.8379
2022-02-23 20:22:04 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.0897
2022-02-23 20:22:36 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 0.9465
2022-02-23 20:23:09 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 0.9705
2022-02-23 20:23:42 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 0.9681
2022-02-23 20:24:14 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 0.8788
2022-02-23 20:24:47 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 0.9643
2022-02-23 20:25:20 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 0.9860
2022-02-23 20:25:53 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 0.9776
2022-02-23 20:26:26 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 0.8649
2022-02-23 20:27:00 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 0.7964
2022-02-23 20:27:32 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.1080
2022-02-23 20:28:05 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 0.9378
2022-02-23 20:28:38 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 0.8539
2022-02-23 20:29:10 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 0.9006
2022-02-23 20:29:43 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 0.8852
2022-02-23 20:30:16 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 0.8809
2022-02-23 20:30:49 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 0.9592
2022-02-23 20:31:22 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 0.8640
2022-02-23 20:31:55 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 0.9210
2022-02-23 20:32:28 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 0.8100
2022-02-23 20:33:01 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 0.8665
2022-02-23 20:33:35 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 0.7017
2022-02-23 20:34:08 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 0.7936
2022-02-23 20:34:41 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 0.8041
2022-02-23 20:35:14 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 0.9155
2022-02-23 20:35:48 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 0.7059
2022-02-23 20:36:22 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.0527
2022-02-23 20:36:56 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 0.8652
2022-02-23 20:37:30 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 0.9222
2022-02-23 20:38:04 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.0176
2022-02-23 20:38:06 - train: epoch 072, train_loss: 0.8793
2022-02-23 20:39:24 - eval: epoch: 072, acc1: 76.436%, acc5: 93.230%, test_loss: 0.9225, per_image_load_time: 2.470ms, per_image_inference_time: 0.551ms
2022-02-23 20:39:25 - until epoch: 072, best_acc1: 76.598%
2022-02-23 20:39:25 - epoch 073 lr: 0.0010000000000000002
2022-02-23 20:40:04 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.0704
2022-02-23 20:40:37 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 0.8733
2022-02-23 20:41:09 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 0.8689
2022-02-23 20:41:42 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 0.7171
2022-02-23 20:42:15 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 0.7555
2022-02-23 20:42:48 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 0.6965
2022-02-23 20:43:21 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 0.8661
2022-02-23 20:43:54 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 0.8118
2022-02-23 20:44:27 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 0.9001
2022-02-23 20:45:00 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 0.7607
2022-02-23 20:45:33 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.1321
2022-02-23 20:46:06 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 0.9084
2022-02-23 20:46:39 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 0.8521
2022-02-23 20:47:12 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 0.6931
2022-02-23 20:47:46 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 0.8485
2022-02-23 20:48:19 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 0.9148
2022-02-23 20:48:52 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 0.7072
2022-02-23 20:49:25 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 0.8410
2022-02-23 20:49:59 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 0.9029
2022-02-23 20:50:32 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 0.9155
2022-02-23 20:51:05 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 0.8769
2022-02-23 20:51:38 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 0.8672
2022-02-23 20:52:11 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 0.8630
2022-02-23 20:52:44 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 0.8353
2022-02-23 20:53:17 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 0.9241
2022-02-23 20:53:50 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 0.7973
2022-02-23 20:54:23 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 0.8253
2022-02-23 20:54:56 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 0.9844
2022-02-23 20:55:29 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 0.9765
2022-02-23 20:56:02 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 0.7659
2022-02-23 20:56:36 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 0.7726
2022-02-23 20:57:09 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 0.9174
2022-02-23 20:57:42 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 0.8389
2022-02-23 20:58:16 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 0.8755
2022-02-23 20:58:49 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 0.8016
2022-02-23 20:59:22 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 0.7967
2022-02-23 20:59:55 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 0.8086
2022-02-23 21:00:29 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 0.8857
2022-02-23 21:01:02 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 0.8766
2022-02-23 21:01:35 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 0.9257
2022-02-23 21:02:09 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 0.8215
2022-02-23 21:02:42 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 0.7962
2022-02-23 21:03:16 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 0.8003
2022-02-23 21:03:50 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 0.9650
2022-02-23 21:04:24 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 0.8789
2022-02-23 21:04:58 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.0910
2022-02-23 21:05:33 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.0172
2022-02-23 21:06:07 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 0.8714
2022-02-23 21:06:42 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 0.8148
2022-02-23 21:07:17 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.0708
2022-02-23 21:07:19 - train: epoch 073, train_loss: 0.8733
2022-02-23 21:08:38 - eval: epoch: 073, acc1: 76.566%, acc5: 93.256%, test_loss: 0.9228, per_image_load_time: 2.392ms, per_image_inference_time: 0.597ms
2022-02-23 21:08:39 - until epoch: 073, best_acc1: 76.598%
2022-02-23 21:08:39 - epoch 074 lr: 0.0010000000000000002
2022-02-23 21:09:18 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 0.9053
2022-02-23 21:09:51 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 0.8431
2022-02-23 21:10:24 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 0.7865
2022-02-23 21:10:57 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.0669
2022-02-23 21:11:30 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 0.9091
2022-02-23 21:12:03 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 0.8745
2022-02-23 21:12:36 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 0.8120
2022-02-23 21:13:09 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 0.9211
2022-02-23 21:13:42 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 0.8374
2022-02-23 21:14:15 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 0.9715
2022-02-23 21:14:48 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 0.8383
2022-02-23 21:15:22 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 0.7594
2022-02-23 21:15:55 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 0.7839
2022-02-23 21:16:28 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 0.8533
2022-02-23 21:17:01 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 0.9465
2022-02-23 21:17:34 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 0.8691
2022-02-23 21:18:07 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 0.9793
2022-02-23 21:18:40 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.0918
2022-02-23 21:19:14 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.0658
2022-02-23 21:19:47 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 0.9635
2022-02-23 21:20:21 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 0.8721
2022-02-23 21:20:54 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.0507
2022-02-23 21:21:27 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 0.9248
2022-02-23 21:22:01 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 0.7735
2022-02-23 21:22:34 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 0.7850
2022-02-23 21:23:07 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 0.7187
2022-02-23 21:23:40 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 0.8428
2022-02-23 21:24:14 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 0.9906
2022-02-23 21:24:47 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 0.8353
2022-02-23 21:25:21 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 0.9014
2022-02-23 21:25:54 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 0.9881
2022-02-23 21:26:27 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 0.7045
2022-02-23 21:27:01 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 0.8974
2022-02-23 21:27:34 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 0.9463
2022-02-23 21:28:08 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 0.8098
2022-02-23 21:28:42 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 0.9265
2022-02-23 21:29:15 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 0.9113
2022-02-23 21:29:49 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 0.7812
2022-02-23 21:30:22 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 0.8312
2022-02-23 21:30:56 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 0.9973
2022-02-23 21:31:30 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 0.9367
2022-02-23 21:32:04 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.0060
2022-02-23 21:32:37 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 0.8764
2022-02-23 21:33:11 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 0.7540
2022-02-23 21:33:45 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 0.7806
2022-02-23 21:34:20 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 0.7830
2022-02-23 21:34:54 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 0.8727
2022-02-23 21:35:28 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 0.9048
2022-02-23 21:36:03 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.0949
2022-02-23 21:36:37 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 0.7948
2022-02-23 21:36:40 - train: epoch 074, train_loss: 0.8690
2022-02-23 21:37:57 - eval: epoch: 074, acc1: 76.524%, acc5: 93.286%, test_loss: 0.9225, per_image_load_time: 2.257ms, per_image_inference_time: 0.618ms
2022-02-23 21:37:58 - until epoch: 074, best_acc1: 76.598%
2022-02-23 21:37:58 - epoch 075 lr: 0.0010000000000000002
2022-02-23 21:38:36 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 0.8958
2022-02-23 21:39:08 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 0.8463
2022-02-23 21:39:41 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 0.8579
2022-02-23 21:40:13 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 0.8457
2022-02-23 21:40:46 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 0.9083
2022-02-23 21:41:18 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 0.7813
2022-02-23 21:41:51 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 0.8468
2022-02-23 21:42:24 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 0.8191
2022-02-23 21:42:57 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.0599
2022-02-23 21:43:29 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 0.5692
2022-02-23 21:44:02 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 0.9093
2022-02-23 21:44:35 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 0.8673
2022-02-23 21:45:07 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 0.8509
2022-02-23 21:45:40 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 0.8756
2022-02-23 21:46:13 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 0.9798
2022-02-23 21:46:45 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 0.7810
2022-02-23 21:47:18 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 0.9766
2022-02-23 21:47:50 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 0.8257
2022-02-23 21:48:23 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 0.7797
2022-02-23 21:48:56 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 0.9949
2022-02-23 21:49:29 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 0.9952
2022-02-23 21:50:02 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 0.8088
2022-02-23 21:50:35 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 0.7898
2022-02-23 21:51:08 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 0.8709
2022-02-23 21:51:41 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 0.9606
2022-02-23 21:52:14 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 0.7976
2022-02-23 21:52:47 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 0.8115
2022-02-23 21:53:20 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 0.7593
2022-02-23 21:53:53 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.0282
2022-02-23 21:54:26 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 0.9757
2022-02-23 21:54:59 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 0.7824
2022-02-23 21:55:32 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 0.9829
2022-02-23 21:56:05 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 0.9444
2022-02-23 21:56:38 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 0.7231
2022-02-23 21:57:11 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 0.9037
2022-02-23 21:57:44 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 0.7931
2022-02-23 21:58:17 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 0.9156
2022-02-23 21:58:50 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 0.9206
2022-02-23 21:59:24 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 0.7686
2022-02-23 21:59:57 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 0.8956
2022-02-23 22:00:30 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 0.7123
2022-02-23 22:01:03 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 0.7144
2022-02-23 22:01:37 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 0.9763
2022-02-23 22:02:10 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 0.6209
2022-02-23 22:02:44 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 0.8877
2022-02-23 22:03:18 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 0.8101
2022-02-23 22:03:52 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 0.9923
2022-02-23 22:04:27 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 0.7265
2022-02-23 22:05:02 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 0.7387
2022-02-23 22:05:36 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 0.8697
2022-02-23 22:05:38 - train: epoch 075, train_loss: 0.8648
2022-02-23 22:06:56 - eval: epoch: 075, acc1: 76.698%, acc5: 93.240%, test_loss: 0.9230, per_image_load_time: 2.389ms, per_image_inference_time: 0.570ms
2022-02-23 22:06:57 - until epoch: 075, best_acc1: 76.698%
2022-02-23 22:06:57 - epoch 076 lr: 0.0010000000000000002
2022-02-23 22:07:36 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 0.8524
2022-02-23 22:08:08 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 0.8337
2022-02-23 22:08:39 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 0.8369
2022-02-23 22:09:12 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 0.8681
2022-02-23 22:09:44 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 0.8370
2022-02-23 22:10:16 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 0.9399
2022-02-23 22:10:49 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 0.8642
2022-02-23 22:11:21 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 0.9214
2022-02-23 22:11:54 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 0.7859
2022-02-23 22:12:26 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.0243
2022-02-23 22:12:59 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 0.8825
2022-02-23 22:13:32 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 0.8258
2022-02-23 22:14:04 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 0.8200
2022-02-23 22:14:37 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 0.8204
2022-02-23 22:15:10 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 0.8836
2022-02-23 22:15:43 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 0.7349
2022-02-23 22:16:15 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 0.7762
2022-02-23 22:16:48 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 0.7866
2022-02-23 22:17:20 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.0640
2022-02-23 22:17:53 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 0.8900
2022-02-23 22:18:26 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 0.9442
2022-02-23 22:18:59 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 0.8261
2022-02-23 22:19:31 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 0.8289
2022-02-23 22:20:04 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 0.8820
2022-02-23 22:20:37 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 0.8425
2022-02-23 22:21:10 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 0.8737
2022-02-23 22:21:43 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 0.8425
2022-02-23 22:22:16 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 0.7844
2022-02-23 22:22:49 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 0.6975
2022-02-23 22:23:22 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 0.7903
2022-02-23 22:23:55 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 0.7973
2022-02-23 22:24:27 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 0.9028
2022-02-23 22:25:00 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.0080
2022-02-23 22:25:33 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 0.9817
2022-02-23 22:26:06 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 0.8308
2022-02-23 22:26:39 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 0.9916
2022-02-23 22:27:13 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 0.8030
2022-02-23 22:27:46 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 0.7873
2022-02-23 22:28:19 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 0.8167
2022-02-23 22:28:52 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 0.9120
2022-02-23 22:29:25 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 0.9419
2022-02-23 22:29:58 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 0.7853
2022-02-23 22:30:32 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 0.7213
2022-02-23 22:31:05 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 0.8094
2022-02-23 22:31:38 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 0.9721
2022-02-23 22:32:12 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 0.7342
2022-02-23 22:32:46 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 0.9000
2022-02-23 22:33:20 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 0.7941
2022-02-23 22:33:54 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 0.8358
2022-02-23 22:34:28 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.0648
2022-02-23 22:34:31 - train: epoch 076, train_loss: 0.8614
2022-02-23 22:35:49 - eval: epoch: 076, acc1: 76.474%, acc5: 93.296%, test_loss: 0.9214, per_image_load_time: 2.417ms, per_image_inference_time: 0.549ms
2022-02-23 22:35:50 - until epoch: 076, best_acc1: 76.698%
2022-02-23 22:35:50 - epoch 077 lr: 0.0010000000000000002
2022-02-23 22:36:28 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 0.9578
2022-02-23 22:37:00 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 0.8716
2022-02-23 22:37:33 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 0.8982
2022-02-23 22:38:06 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 0.8240
2022-02-23 22:38:38 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 0.7363
2022-02-23 22:39:11 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 0.7272
2022-02-23 22:39:43 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 0.8503
2022-02-23 22:40:16 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 0.9818
2022-02-23 22:40:48 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 0.8831
2022-02-23 22:41:21 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 0.8348
