2022-03-07 23:06:26 - network: yoloxnbackbone
2022-03-07 23:06:26 - num_classes: 1000
2022-03-07 23:06:26 - input_image_size: 256
2022-03-07 23:06:26 - scale: 1.1428571428571428
2022-03-07 23:06:26 - trained_model_path: 
2022-03-07 23:06:26 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-07 23:06:26 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f8893364f40>
2022-03-07 23:06:26 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f8877376250>
2022-03-07 23:06:26 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f8877376280>
2022-03-07 23:06:26 - seed: 0
2022-03-07 23:06:26 - batch_size: 256
2022-03-07 23:06:26 - num_workers: 16
2022-03-07 23:06:26 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-07 23:06:26 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-03-07 23:06:26 - epochs: 100
2022-03-07 23:06:26 - print_interval: 100
2022-03-07 23:06:26 - distributed: True
2022-03-07 23:06:26 - sync_bn: False
2022-03-07 23:06:26 - apex: True
2022-03-07 23:06:26 - gpus_type: NVIDIA RTX A5000
2022-03-07 23:06:26 - gpus_num: 2
2022-03-07 23:06:26 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f887631dbb0>
2022-03-07 23:06:26 - --------------------parameters--------------------
2022-03-07 23:06:26 - name: conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer1.0.depthwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer1.0.depthwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer1.0.depthwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer1.0.pointwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer1.0.pointwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer1.0.pointwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.1.depthwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.1.pointwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.0.depthwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.0.depthwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.0.depthwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.0.pointwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.0.pointwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.0.pointwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.1.depthwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.1.pointwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.1.depthwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.1.depthwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.1.depthwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.1.pointwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.1.pointwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.1.pointwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.1.depthwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.1.depthwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.1.depthwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.1.pointwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.1.pointwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.1.pointwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.0.depthwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.0.depthwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.0.depthwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.0.pointwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.0.pointwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.0.pointwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.1.depthwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.1.pointwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.1.depthwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.1.depthwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.1.depthwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.1.pointwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.1.pointwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.1.pointwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.1.depthwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.1.depthwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.1.depthwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.1.pointwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.1.pointwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.1.pointwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer4.0.depthwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer4.0.depthwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer4.0.depthwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer4.0.pointwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer4.0.pointwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer4.0.pointwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.1.depthwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.1.depthwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.1.depthwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.1.pointwise_conv.layer.0.weight, grad: True
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.1.pointwise_conv.layer.1.weight, grad: True
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.1.pointwise_conv.layer.1.bias, grad: True
2022-03-07 23:06:26 - name: fc.weight, grad: True
2022-03-07 23:06:26 - name: fc.bias, grad: True
2022-03-07 23:06:26 - --------------------buffers--------------------
2022-03-07 23:06:26 - name: conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer1.0.depthwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer1.0.depthwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer1.0.depthwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer1.0.pointwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer1.0.pointwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer1.0.pointwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer1.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.0.depthwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.0.depthwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.0.depthwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.0.pointwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.0.pointwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.0.pointwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.1.depthwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.1.depthwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.1.depthwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.1.pointwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.1.pointwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.1.conv.1.pointwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.1.depthwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.1.depthwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.1.depthwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.1.pointwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.1.pointwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer2.1.bottlenecks.2.conv.1.pointwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.0.depthwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.0.depthwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.0.depthwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.0.pointwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.0.pointwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.0.pointwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.1.depthwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.0.conv.1.pointwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.1.depthwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.1.depthwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.1.depthwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.1.pointwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.1.pointwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.1.conv.1.pointwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.1.depthwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.1.depthwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.1.depthwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.1.pointwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.1.pointwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer3.1.bottlenecks.2.conv.1.pointwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer4.0.depthwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer4.0.depthwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer4.0.depthwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer4.0.pointwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer4.0.pointwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer4.0.pointwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.1.depthwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.1.depthwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.1.depthwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.1.pointwise_conv.layer.1.running_mean, grad: False
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.1.pointwise_conv.layer.1.running_var, grad: False
2022-03-07 23:06:26 - name: layer4.2.bottlenecks.0.conv.1.pointwise_conv.layer.1.num_batches_tracked, grad: False
2022-03-07 23:06:26 - epoch 001 lr: 0.1
2022-03-07 23:07:06 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9025
2022-03-07 23:07:39 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.7303
2022-03-07 23:08:12 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.6619
2022-03-07 23:08:46 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.4627
2022-03-07 23:09:19 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.3353
2022-03-07 23:09:52 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.0728
2022-03-07 23:10:26 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.1352
2022-03-07 23:10:59 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 5.9559
2022-03-07 23:11:32 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.8743
2022-03-07 23:12:06 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.8011
2022-03-07 23:12:39 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.7970
2022-03-07 23:13:12 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.6004
2022-03-07 23:13:46 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.4082
2022-03-07 23:14:19 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.4824
2022-03-07 23:14:52 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.3988
2022-03-07 23:15:25 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.4953
2022-03-07 23:15:58 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.2297
2022-03-07 23:16:32 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.3611
2022-03-07 23:17:05 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.2413
2022-03-07 23:17:38 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.1319
2022-03-07 23:18:11 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.0654
2022-03-07 23:18:44 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.1354
2022-03-07 23:19:17 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.9410
2022-03-07 23:19:50 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.9453
2022-03-07 23:20:23 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.1052
2022-03-07 23:20:57 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.0915
2022-03-07 23:21:30 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.1446
2022-03-07 23:22:02 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.9154
2022-03-07 23:22:36 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.8132
2022-03-07 23:23:09 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.9570
2022-03-07 23:23:43 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.9905
2022-03-07 23:24:16 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.8971
2022-03-07 23:24:49 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.7147
2022-03-07 23:25:23 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.6610
2022-03-07 23:25:56 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.8759
2022-03-07 23:26:29 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.9176
2022-03-07 23:27:02 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.8508
2022-03-07 23:27:36 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.6602
2022-03-07 23:28:09 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.6317
2022-03-07 23:28:42 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.6048
2022-03-07 23:29:14 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.6562
2022-03-07 23:29:47 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.5435
2022-03-07 23:30:20 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.5742
2022-03-07 23:30:54 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.4133
2022-03-07 23:31:27 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.7069
2022-03-07 23:32:01 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.7062
2022-03-07 23:32:34 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.5084
2022-03-07 23:33:07 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.8480
2022-03-07 23:33:40 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.4470
2022-03-07 23:34:13 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.3189
2022-03-07 23:34:14 - train: epoch 001, train_loss: 5.2025
2022-03-07 23:35:28 - eval: epoch: 001, acc1: 14.718%, acc5: 34.704%, test_loss: 4.3733, per_image_load_time: 2.137ms, per_image_inference_time: 0.276ms
2022-03-07 23:35:28 - until epoch: 001, best_acc1: 14.718%
2022-03-07 23:35:28 - epoch 002 lr: 0.1
2022-03-07 23:36:07 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.5808
2022-03-07 23:36:40 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.3433
2022-03-07 23:37:13 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.6115
2022-03-07 23:37:46 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.4770
2022-03-07 23:38:19 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.2820
2022-03-07 23:38:52 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.2245
2022-03-07 23:39:25 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.5051
2022-03-07 23:39:58 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 4.1441
2022-03-07 23:40:31 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 4.0868
2022-03-07 23:41:04 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.5748
2022-03-07 23:41:38 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.3325
2022-03-07 23:42:11 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.3844
2022-03-07 23:42:45 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.3056
2022-03-07 23:43:18 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.4360
2022-03-07 23:43:52 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.4276
2022-03-07 23:44:25 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 4.2435
2022-03-07 23:44:59 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.2562
2022-03-07 23:45:32 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.2423
2022-03-07 23:46:06 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 4.0936
2022-03-07 23:46:39 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 4.0657
2022-03-07 23:47:13 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 4.2919
2022-03-07 23:47:47 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.9394
2022-03-07 23:48:20 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 4.2317
2022-03-07 23:48:54 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 4.0487
2022-03-07 23:49:27 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 4.1103
2022-03-07 23:50:01 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 4.1159
2022-03-07 23:50:34 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 4.3486
2022-03-07 23:51:08 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 4.2620
2022-03-07 23:51:41 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.9435
2022-03-07 23:52:15 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.9983
2022-03-07 23:52:49 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.9852
2022-03-07 23:53:23 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 4.1781
2022-03-07 23:53:56 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 4.0067
2022-03-07 23:54:30 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 4.2290
2022-03-07 23:55:03 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.9814
2022-03-07 23:55:36 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 4.0726
2022-03-07 23:56:09 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 4.0997
2022-03-07 23:56:43 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.8782
2022-03-07 23:57:18 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 4.0927
2022-03-07 23:57:52 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.9782
2022-03-07 23:58:26 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 4.0719
2022-03-07 23:59:00 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.9761
2022-03-07 23:59:34 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.9304
2022-03-08 00:00:08 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.9514
2022-03-08 00:00:40 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.8056
2022-03-08 00:01:15 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.9198
2022-03-08 00:01:49 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.9784
2022-03-08 00:02:24 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.8921
2022-03-08 00:02:59 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.6800
2022-03-08 00:03:32 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.9624
2022-03-08 00:03:33 - train: epoch 002, train_loss: 4.1584
2022-03-08 00:04:48 - eval: epoch: 002, acc1: 22.446%, acc5: 46.226%, test_loss: 3.7704, per_image_load_time: 2.620ms, per_image_inference_time: 0.248ms
2022-03-08 00:04:48 - until epoch: 002, best_acc1: 22.446%
2022-03-08 00:04:48 - epoch 003 lr: 0.1
2022-03-08 00:05:27 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.9863
2022-03-08 00:06:00 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.8435
2022-03-08 00:06:33 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.8847
2022-03-08 00:07:06 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.7703
2022-03-08 00:07:39 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.9215
2022-03-08 00:08:12 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.7634
2022-03-08 00:08:44 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 4.0186
2022-03-08 00:09:18 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 4.0189
2022-03-08 00:09:51 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.7379
2022-03-08 00:10:25 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.9093
2022-03-08 00:10:59 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.6863
2022-03-08 00:11:33 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.7775
2022-03-08 00:12:06 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.8918
2022-03-08 00:12:40 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.7754
2022-03-08 00:13:14 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.9995
2022-03-08 00:13:48 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.7111
2022-03-08 00:14:21 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.6952
2022-03-08 00:14:55 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.7179
2022-03-08 00:15:28 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.8751
2022-03-08 00:16:02 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 4.0866
2022-03-08 00:16:35 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.9775
2022-03-08 00:17:09 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 4.0823
2022-03-08 00:17:43 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.8273
2022-03-08 00:18:16 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.8055
2022-03-08 00:18:49 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.9388
2022-03-08 00:19:23 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.8025
2022-03-08 00:19:55 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.9810
2022-03-08 00:20:28 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.6006
2022-03-08 00:21:02 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.8014
2022-03-08 00:21:35 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.8980
2022-03-08 00:22:08 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.9231
2022-03-08 00:22:42 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.7805
2022-03-08 00:23:15 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.7351
2022-03-08 00:23:48 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.9352
2022-03-08 00:24:22 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.5804
2022-03-08 00:24:56 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.8045
2022-03-08 00:25:30 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.7641
2022-03-08 00:26:03 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.8131
2022-03-08 00:26:36 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 4.1029
2022-03-08 00:27:10 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.6796
2022-03-08 00:27:43 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.7363
2022-03-08 00:28:16 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.7496
2022-03-08 00:28:49 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 3.4565
2022-03-08 00:29:22 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.4922
2022-03-08 00:29:55 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.7942
2022-03-08 00:30:29 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.5352
2022-03-08 00:31:02 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.8046
2022-03-08 00:31:36 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.6772
2022-03-08 00:32:09 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.8614
2022-03-08 00:32:43 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.7128
2022-03-08 00:32:44 - train: epoch 003, train_loss: 3.8052
2022-03-08 00:33:59 - eval: epoch: 003, acc1: 25.394%, acc5: 49.524%, test_loss: 3.6053, per_image_load_time: 1.586ms, per_image_inference_time: 0.288ms
2022-03-08 00:33:59 - until epoch: 003, best_acc1: 25.394%
2022-03-08 00:33:59 - epoch 004 lr: 0.1
2022-03-08 00:34:39 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.8110
2022-03-08 00:35:11 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.6434
2022-03-08 00:35:44 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.6141
2022-03-08 00:36:17 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 3.5695
2022-03-08 00:36:49 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.6279
2022-03-08 00:37:22 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.9237
2022-03-08 00:37:55 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.7013
2022-03-08 00:38:28 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 3.5575
2022-03-08 00:39:01 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 3.4652
2022-03-08 00:39:34 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.5907
2022-03-08 00:40:08 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.5868
2022-03-08 00:40:41 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 3.4360
2022-03-08 00:41:14 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 3.4691
2022-03-08 00:41:47 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.7096
2022-03-08 00:42:21 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.7501
2022-03-08 00:42:55 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 3.5783
2022-03-08 00:43:28 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.6658
2022-03-08 00:44:01 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.8655
2022-03-08 00:44:35 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.7138
2022-03-08 00:45:09 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.6934
2022-03-08 00:45:42 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.7221
2022-03-08 00:46:16 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.5252
2022-03-08 00:46:50 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 3.5694
2022-03-08 00:47:23 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 3.4728
2022-03-08 00:47:57 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 3.4622
2022-03-08 00:48:31 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.7860
2022-03-08 00:49:05 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 3.4286
2022-03-08 00:49:38 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.6548
2022-03-08 00:50:11 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 3.6223
2022-03-08 00:50:45 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 3.6159
2022-03-08 00:51:18 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.6264
2022-03-08 00:51:52 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 3.6121
2022-03-08 00:52:25 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.6195
2022-03-08 00:52:58 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 3.6488
2022-03-08 00:53:31 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 3.5292
2022-03-08 00:54:05 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 3.3353
2022-03-08 00:54:39 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.6365
2022-03-08 00:55:12 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 3.5486
2022-03-08 00:55:46 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 3.4606
2022-03-08 00:56:19 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 3.2565
2022-03-08 00:56:53 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 3.5338
2022-03-08 00:57:27 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 3.5572
2022-03-08 00:58:00 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 3.4455
2022-03-08 00:58:34 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 3.3417
2022-03-08 00:59:08 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 3.0950
2022-03-08 00:59:42 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 3.5093
2022-03-08 01:00:21 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 3.4612
2022-03-08 01:00:56 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 3.4318
2022-03-08 01:01:30 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 3.5797
2022-03-08 01:02:03 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 3.6285
2022-03-08 01:02:05 - train: epoch 004, train_loss: 3.5984
2022-03-08 01:03:21 - eval: epoch: 004, acc1: 28.168%, acc5: 53.510%, test_loss: 3.3993, per_image_load_time: 2.710ms, per_image_inference_time: 0.233ms
2022-03-08 01:03:21 - until epoch: 004, best_acc1: 28.168%
2022-03-08 01:03:21 - epoch 005 lr: 0.1
2022-03-08 01:04:00 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.5889
2022-03-08 01:04:33 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.5946
2022-03-08 01:05:06 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.6690
2022-03-08 01:05:39 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.4770
2022-03-08 01:06:12 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 3.4261
2022-03-08 01:06:45 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 3.4470
2022-03-08 01:07:18 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 3.4443
2022-03-08 01:07:51 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.7379
2022-03-08 01:08:24 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 3.4586
2022-03-08 01:08:58 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.5251
2022-03-08 01:09:31 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.5025
2022-03-08 01:10:05 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 3.5291
2022-03-08 01:10:38 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 3.4143
2022-03-08 01:11:12 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.4859
2022-03-08 01:11:45 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 3.4007
2022-03-08 01:12:19 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 3.1833
2022-03-08 01:12:53 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 3.3936
2022-03-08 01:13:26 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 3.6932
2022-03-08 01:14:00 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 3.1739
2022-03-08 01:14:33 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 3.4561
2022-03-08 01:15:07 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 3.2384
2022-03-08 01:15:40 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 3.4197
2022-03-08 01:16:14 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 3.3597
2022-03-08 01:16:47 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 3.3933
2022-03-08 01:17:21 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 3.3988
2022-03-08 01:17:54 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.7102
2022-03-08 01:18:28 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.7596
2022-03-08 01:19:02 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 3.5368
2022-03-08 01:19:35 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 3.2634
2022-03-08 01:20:08 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 3.4971
2022-03-08 01:20:42 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.6112
2022-03-08 01:21:15 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 3.4271
2022-03-08 01:21:50 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 3.3732
2022-03-08 01:22:24 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 3.3292
2022-03-08 01:22:57 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 3.3315
2022-03-08 01:23:31 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 3.4251
2022-03-08 01:24:04 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 3.3612
2022-03-08 01:24:37 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 3.4390
2022-03-08 01:25:11 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.5434
2022-03-08 01:25:45 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 3.4739
2022-03-08 01:26:19 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 3.4489
2022-03-08 01:26:53 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 3.4141
2022-03-08 01:27:27 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 3.2235
2022-03-08 01:28:01 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 3.5046
2022-03-08 01:28:35 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 3.5088
2022-03-08 01:29:09 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 3.4105
2022-03-08 01:29:43 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 3.2542
2022-03-08 01:30:17 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 3.3129
2022-03-08 01:30:51 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 3.6044
2022-03-08 01:31:24 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 3.3053
2022-03-08 01:31:26 - train: epoch 005, train_loss: 3.4767
2022-03-08 01:32:41 - eval: epoch: 005, acc1: 27.870%, acc5: 53.310%, test_loss: 3.4252, per_image_load_time: 2.247ms, per_image_inference_time: 0.238ms
2022-03-08 01:32:41 - until epoch: 005, best_acc1: 28.168%
2022-03-08 01:32:41 - epoch 006 lr: 0.1
2022-03-08 01:33:21 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 3.3498
2022-03-08 01:33:54 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 3.3789
2022-03-08 01:34:27 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 3.2097
2022-03-08 01:35:01 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.5064
2022-03-08 01:35:34 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 3.4174
2022-03-08 01:36:08 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 3.2950
2022-03-08 01:36:41 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.4256
2022-03-08 01:37:14 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 3.5082
2022-03-08 01:37:48 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 3.3412
2022-03-08 01:38:21 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 3.2862
2022-03-08 01:38:54 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 3.2012
2022-03-08 01:39:27 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 3.5381
2022-03-08 01:40:00 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.6056
2022-03-08 01:40:34 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 3.4472
2022-03-08 01:41:07 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 3.5850
2022-03-08 01:41:40 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 3.2049
2022-03-08 01:42:13 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 3.4292
2022-03-08 01:42:46 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 3.4318
2022-03-08 01:43:21 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 3.3310
2022-03-08 01:43:54 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 3.6126
2022-03-08 01:44:27 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 3.2885
2022-03-08 01:45:01 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 3.2800
2022-03-08 01:45:34 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 3.2028
2022-03-08 01:46:07 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 3.4040
2022-03-08 01:46:41 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 3.5494
2022-03-08 01:47:14 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 3.4826
2022-03-08 01:47:48 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 3.5181
2022-03-08 01:48:22 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 3.2451
2022-03-08 01:48:55 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 3.3779
2022-03-08 01:49:29 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 3.3256
2022-03-08 01:50:03 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 3.1726
2022-03-08 01:50:35 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 3.2514
2022-03-08 01:51:09 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 3.3054
2022-03-08 01:51:42 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 3.5355
2022-03-08 01:52:16 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 3.4024
2022-03-08 01:52:50 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 3.5438
2022-03-08 01:53:23 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 3.2636
2022-03-08 01:53:57 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 3.2252
2022-03-08 01:54:31 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 3.2133
2022-03-08 01:55:04 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.5678
2022-03-08 01:55:38 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 3.3298
2022-03-08 01:56:11 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 3.2490
2022-03-08 01:56:45 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 3.2141
2022-03-08 01:57:18 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 3.4301
2022-03-08 01:57:52 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 3.4126
2022-03-08 01:58:26 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 3.3714
2022-03-08 01:59:00 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 3.4383
2022-03-08 01:59:33 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 3.2930
2022-03-08 02:00:07 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 3.4765
2022-03-08 02:00:40 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 3.3289
2022-03-08 02:00:42 - train: epoch 006, train_loss: 3.3992
2022-03-08 02:01:56 - eval: epoch: 006, acc1: 29.162%, acc5: 54.842%, test_loss: 3.3462, per_image_load_time: 2.679ms, per_image_inference_time: 0.229ms
2022-03-08 02:01:56 - until epoch: 006, best_acc1: 29.162%
2022-03-08 02:01:56 - epoch 007 lr: 0.1
2022-03-08 02:02:36 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 3.2403
2022-03-08 02:03:09 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 3.6295
2022-03-08 02:03:43 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 3.5030
2022-03-08 02:04:16 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 3.3098
2022-03-08 02:04:48 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 3.2665
2022-03-08 02:05:21 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 3.4498
2022-03-08 02:05:55 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 3.3718
2022-03-08 02:06:28 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 3.3796
2022-03-08 02:07:01 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 3.4773
2022-03-08 02:07:34 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 3.3332
2022-03-08 02:08:08 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 3.3205
2022-03-08 02:08:41 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 3.3036
2022-03-08 02:09:14 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 3.1975
2022-03-08 02:09:48 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 3.4377
2022-03-08 02:10:21 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 3.4073
2022-03-08 02:10:55 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 3.3114
2022-03-08 02:11:28 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 3.3929
2022-03-08 02:12:01 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 3.2036
2022-03-08 02:12:35 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 3.4660
2022-03-08 02:13:08 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 3.0645
2022-03-08 02:13:41 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 3.4485
2022-03-08 02:14:15 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 3.2103
2022-03-08 02:14:49 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 3.4514
2022-03-08 02:15:22 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 3.5120
2022-03-08 02:15:55 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 3.2416
2022-03-08 02:16:29 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 3.2307
2022-03-08 02:17:02 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 3.2010
2022-03-08 02:17:36 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 3.3112
2022-03-08 02:18:09 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 3.2645
2022-03-08 02:18:42 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 3.4806
2022-03-08 02:19:16 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 3.2065
2022-03-08 02:19:49 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 3.2483
2022-03-08 02:20:23 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 3.6299
2022-03-08 02:20:56 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 3.1701
2022-03-08 02:21:30 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 3.2685
2022-03-08 02:22:03 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 3.1932
2022-03-08 02:22:36 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 3.5949
2022-03-08 02:23:10 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 3.5059
2022-03-08 02:23:43 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 3.3570
2022-03-08 02:24:16 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 3.3829
2022-03-08 02:24:50 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 3.2100
2022-03-08 02:25:23 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 3.1487
2022-03-08 02:25:57 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 3.3139
2022-03-08 02:26:32 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 3.0966
2022-03-08 02:27:06 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 3.5456
2022-03-08 02:27:39 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 3.3197
2022-03-08 02:28:13 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 3.5274
2022-03-08 02:28:47 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 3.5215
2022-03-08 02:29:20 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 3.2887
2022-03-08 02:29:54 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 3.3509
2022-03-08 02:29:55 - train: epoch 007, train_loss: 3.3407
2022-03-08 02:31:10 - eval: epoch: 007, acc1: 31.750%, acc5: 58.196%, test_loss: 3.1621, per_image_load_time: 2.600ms, per_image_inference_time: 0.226ms
2022-03-08 02:31:10 - until epoch: 007, best_acc1: 31.750%
2022-03-08 02:31:10 - epoch 008 lr: 0.1
2022-03-08 02:31:49 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 3.2127
2022-03-08 02:32:22 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 3.3502
2022-03-08 02:32:55 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 3.0656
2022-03-08 02:33:28 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 3.2662
2022-03-08 02:34:01 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 3.0956
2022-03-08 02:34:34 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 3.2478
2022-03-08 02:35:08 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 3.4918
2022-03-08 02:35:41 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 3.2351
2022-03-08 02:36:14 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 3.2815
2022-03-08 02:36:48 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 3.3510
2022-03-08 02:37:21 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 3.2019
2022-03-08 02:37:54 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 3.1988
2022-03-08 02:38:28 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 3.3687
2022-03-08 02:39:01 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 3.2127
2022-03-08 02:39:35 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 3.3738
2022-03-08 02:40:09 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 3.3424
2022-03-08 02:40:42 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 3.2128
2022-03-08 02:41:16 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 3.4220
2022-03-08 02:41:49 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 3.1425
2022-03-08 02:42:23 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 3.5012
2022-03-08 02:42:57 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 3.3097
2022-03-08 02:43:30 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 3.1079
2022-03-08 02:44:04 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 3.2322
2022-03-08 02:44:37 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 3.1905
2022-03-08 02:45:11 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 3.2442
2022-03-08 02:45:44 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 3.3844
2022-03-08 02:46:18 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 3.4408
2022-03-08 02:46:51 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 3.3210
2022-03-08 02:47:25 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 3.2347
2022-03-08 02:47:59 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 3.4159
2022-03-08 02:48:32 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 3.2866
2022-03-08 02:49:05 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 3.3931
2022-03-08 02:49:39 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 3.5649
2022-03-08 02:50:12 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 3.4203
2022-03-08 02:50:46 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 3.2373
2022-03-08 02:51:20 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 3.3383
2022-03-08 02:51:54 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 3.2800
2022-03-08 02:52:27 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 3.3083
2022-03-08 02:53:00 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 3.3771
2022-03-08 02:53:34 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 3.7615
2022-03-08 02:54:07 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 3.1678
2022-03-08 02:54:41 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 3.2693
2022-03-08 02:55:14 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 3.0670
2022-03-08 02:55:48 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 3.3151
2022-03-08 02:56:21 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 3.3766
2022-03-08 02:56:55 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 3.3678
2022-03-08 02:57:29 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 3.1910
2022-03-08 02:58:02 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 3.2259
2022-03-08 02:58:36 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 3.3518
2022-03-08 02:59:09 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 3.2312
2022-03-08 02:59:11 - train: epoch 008, train_loss: 3.2969
2022-03-08 03:00:26 - eval: epoch: 008, acc1: 33.242%, acc5: 59.770%, test_loss: 3.0676, per_image_load_time: 2.667ms, per_image_inference_time: 0.227ms
2022-03-08 03:00:26 - until epoch: 008, best_acc1: 33.242%
2022-03-08 03:00:26 - epoch 009 lr: 0.1
2022-03-08 03:01:05 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.9990
2022-03-08 03:01:39 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 3.1565
2022-03-08 03:02:11 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.9412
2022-03-08 03:02:44 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 3.5281
2022-03-08 03:03:18 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 3.1973
2022-03-08 03:03:51 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 3.2054
2022-03-08 03:04:25 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 3.1523
2022-03-08 03:04:58 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 3.0156
2022-03-08 03:05:31 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 3.1792
2022-03-08 03:06:05 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 3.1397
2022-03-08 03:06:38 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 3.4858
2022-03-08 03:07:12 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 3.3910
2022-03-08 03:07:45 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 3.2718
2022-03-08 03:08:19 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 3.0506
2022-03-08 03:08:51 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 3.1282
2022-03-08 03:09:25 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 3.3254
2022-03-08 03:09:58 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 3.3958
2022-03-08 03:10:31 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 3.1072
2022-03-08 03:11:05 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 3.0648
2022-03-08 03:11:38 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 3.1910
2022-03-08 03:12:12 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 3.3924
2022-03-08 03:12:45 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 3.3945
2022-03-08 03:13:18 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 3.0652
2022-03-08 03:13:51 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 3.1563
2022-03-08 03:14:25 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 3.0457
2022-03-08 03:14:58 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 3.3783
2022-03-08 03:15:31 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 3.0680
2022-03-08 03:16:04 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 3.3161
2022-03-08 03:16:38 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.8898
2022-03-08 03:17:11 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 3.1984
2022-03-08 03:17:45 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 3.3814
2022-03-08 03:18:18 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 3.2589
2022-03-08 03:18:51 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 3.2471
2022-03-08 03:19:25 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 3.4800
2022-03-08 03:19:59 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 3.3602
2022-03-08 03:20:32 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 3.1314
2022-03-08 03:21:05 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 3.3954
2022-03-08 03:21:38 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 3.3486
2022-03-08 03:22:12 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.9950
2022-03-08 03:22:46 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 3.4671
2022-03-08 03:23:19 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 3.1751
2022-03-08 03:23:53 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 3.1593
2022-03-08 03:24:27 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 3.4185
2022-03-08 03:25:00 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 3.3300
2022-03-08 03:25:34 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 3.2839
2022-03-08 03:26:08 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 3.3315
2022-03-08 03:26:42 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 3.3114
2022-03-08 03:27:16 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 3.3858
2022-03-08 03:27:50 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 3.2541
2022-03-08 03:28:23 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 3.2103
2022-03-08 03:28:24 - train: epoch 009, train_loss: 3.2642
2022-03-08 03:29:40 - eval: epoch: 009, acc1: 31.106%, acc5: 57.294%, test_loss: 3.2124, per_image_load_time: 0.774ms, per_image_inference_time: 0.238ms
2022-03-08 03:29:40 - until epoch: 009, best_acc1: 33.242%
2022-03-08 03:29:40 - epoch 010 lr: 0.1
2022-03-08 03:30:19 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 3.3463
2022-03-08 03:30:53 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 3.3603
2022-03-08 03:31:26 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 3.2878
2022-03-08 03:31:58 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 3.3693
2022-03-08 03:32:31 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 3.1393
2022-03-08 03:33:04 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 3.3185
2022-03-08 03:33:37 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 3.4287
2022-03-08 03:34:11 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 3.2083
2022-03-08 03:34:43 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 3.1058
2022-03-08 03:35:16 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 3.1389
2022-03-08 03:35:49 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 3.3574
2022-03-08 03:36:22 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.9865
2022-03-08 03:36:56 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 3.1966
2022-03-08 03:37:29 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 3.2747
2022-03-08 03:38:02 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.8757
2022-03-08 03:38:35 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 3.1571
2022-03-08 03:39:09 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 3.4013
2022-03-08 03:39:42 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 3.1967
2022-03-08 03:40:16 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 3.2372
2022-03-08 03:40:49 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 3.2010
2022-03-08 03:41:22 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 3.1799
2022-03-08 03:41:55 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 3.3689
2022-03-08 03:42:28 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 3.3516
2022-03-08 03:43:01 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 3.3290
2022-03-08 03:43:35 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 3.4087
2022-03-08 03:44:08 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 3.3822
2022-03-08 03:44:41 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 3.1608
2022-03-08 03:45:14 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 3.1952
2022-03-08 03:45:47 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 3.2931
2022-03-08 03:46:20 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 3.1188
2022-03-08 03:46:53 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 3.3487
2022-03-08 03:47:27 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 3.3109
2022-03-08 03:48:00 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 3.3206
2022-03-08 03:48:33 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 3.5054
2022-03-08 03:49:07 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 3.4213
2022-03-08 03:49:40 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 3.5216
2022-03-08 03:50:14 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 3.2094
2022-03-08 03:50:47 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 3.3435
2022-03-08 03:51:20 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 3.0184
2022-03-08 03:51:53 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 3.1194
2022-03-08 03:52:27 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 3.1704
2022-03-08 03:53:00 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 3.4612
2022-03-08 03:53:34 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 3.2532
2022-03-08 03:54:08 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 3.1905
2022-03-08 03:54:41 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 3.2267
2022-03-08 03:55:15 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 3.2129
2022-03-08 03:55:49 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 3.2386
2022-03-08 03:56:23 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 3.1971
2022-03-08 03:56:57 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 3.0746
2022-03-08 03:57:30 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 3.0101
2022-03-08 03:57:32 - train: epoch 010, train_loss: 3.2356
2022-03-08 03:58:49 - eval: epoch: 010, acc1: 33.824%, acc5: 59.838%, test_loss: 3.0712, per_image_load_time: 2.807ms, per_image_inference_time: 0.202ms
2022-03-08 03:58:49 - until epoch: 010, best_acc1: 33.824%
2022-03-08 03:58:49 - epoch 011 lr: 0.1
2022-03-08 03:59:29 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 3.0614
2022-03-08 04:00:02 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 3.3756
2022-03-08 04:00:35 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 3.2530
2022-03-08 04:01:07 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 3.3005
2022-03-08 04:01:40 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 3.1906
2022-03-08 04:02:13 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 3.1520
2022-03-08 04:02:47 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 3.2445
2022-03-08 04:03:20 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 3.2439
2022-03-08 04:03:53 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 3.2052
2022-03-08 04:04:26 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 3.1382
2022-03-08 04:04:59 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 3.3363
2022-03-08 04:05:32 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 3.6035
2022-03-08 04:06:06 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 3.3937
2022-03-08 04:06:39 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 3.1363
2022-03-08 04:07:12 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 3.0010
2022-03-08 04:07:46 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 3.3098
2022-03-08 04:08:19 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 3.3242
2022-03-08 04:08:53 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.9862
2022-03-08 04:09:27 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 3.0472
2022-03-08 04:10:00 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 3.3585
2022-03-08 04:10:33 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 3.2477
2022-03-08 04:11:06 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 3.1773
2022-03-08 04:11:40 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 3.3470
2022-03-08 04:12:13 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 3.0827
2022-03-08 04:12:46 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 3.5046
2022-03-08 04:13:20 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 3.3283
2022-03-08 04:13:54 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 3.0774
2022-03-08 04:14:27 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.8948
2022-03-08 04:15:01 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 3.2604
2022-03-08 04:15:35 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 3.4021
2022-03-08 04:16:08 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 3.3461
2022-03-08 04:16:42 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.9739
2022-03-08 04:17:15 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 3.2501
2022-03-08 04:17:49 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 3.1407
2022-03-08 04:18:23 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 3.2659
2022-03-08 04:18:57 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 3.3054
2022-03-08 04:19:30 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 3.3334
2022-03-08 04:20:04 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.8318
2022-03-08 04:20:37 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 3.2670
2022-03-08 04:21:11 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 3.1341
2022-03-08 04:21:44 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 3.0678
2022-03-08 04:22:18 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 3.0570
2022-03-08 04:22:52 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 3.2446
2022-03-08 04:23:25 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 3.2324
2022-03-08 04:23:59 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 3.0293
2022-03-08 04:24:33 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 3.1472
2022-03-08 04:25:07 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.9607
2022-03-08 04:25:41 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.9543
2022-03-08 04:26:14 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 3.0247
2022-03-08 04:26:48 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 3.1443
2022-03-08 04:26:50 - train: epoch 011, train_loss: 3.2144
2022-03-08 04:28:04 - eval: epoch: 011, acc1: 34.612%, acc5: 60.696%, test_loss: 3.0191, per_image_load_time: 2.538ms, per_image_inference_time: 0.232ms
2022-03-08 04:28:04 - until epoch: 011, best_acc1: 34.612%
2022-03-08 04:28:04 - epoch 012 lr: 0.1
2022-03-08 04:28:44 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 3.1063
2022-03-08 04:29:17 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.9759
2022-03-08 04:29:50 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 3.1699
2022-03-08 04:30:22 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 3.3101
2022-03-08 04:30:55 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 3.4706
2022-03-08 04:31:28 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 3.0834
2022-03-08 04:32:01 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 3.1556
2022-03-08 04:32:34 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 3.0907
2022-03-08 04:33:07 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 3.2868
2022-03-08 04:33:41 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.9076
2022-03-08 04:34:14 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 3.5772
2022-03-08 04:34:47 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 3.2693
2022-03-08 04:35:20 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 3.0438
2022-03-08 04:35:54 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 3.2210
2022-03-08 04:36:27 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.9758
2022-03-08 04:37:01 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 3.1685
2022-03-08 04:37:34 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 3.1248
2022-03-08 04:38:07 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 3.1924
2022-03-08 04:38:41 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 3.2217
2022-03-08 04:39:14 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 3.4197
2022-03-08 04:39:47 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 3.2886
2022-03-08 04:40:21 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 3.3601
2022-03-08 04:40:54 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 3.2288
2022-03-08 04:41:28 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 3.3029
2022-03-08 04:42:02 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.9790
2022-03-08 04:42:35 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 3.1147
2022-03-08 04:43:08 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 3.0957
2022-03-08 04:43:42 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 3.1446
2022-03-08 04:44:15 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 3.0380
2022-03-08 04:44:48 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 3.0856
2022-03-08 04:45:22 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 3.2403
2022-03-08 04:45:56 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.8168
2022-03-08 04:46:29 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 3.2341
2022-03-08 04:47:03 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 3.1776
2022-03-08 04:47:36 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 3.2909
2022-03-08 04:48:09 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 3.2182
2022-03-08 04:48:43 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 3.2068
2022-03-08 04:49:16 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 3.1994
2022-03-08 04:49:50 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 3.1311
2022-03-08 04:50:23 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 3.0413
2022-03-08 04:50:57 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 3.1051
2022-03-08 04:51:31 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 3.0658
2022-03-08 04:52:04 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 3.3314
2022-03-08 04:52:38 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 3.0047
2022-03-08 04:53:12 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 3.1854
2022-03-08 04:53:45 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 3.5635
2022-03-08 04:54:19 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 3.0686
2022-03-08 04:54:53 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 3.3073
2022-03-08 04:55:28 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 3.1704
2022-03-08 04:56:01 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.9034
2022-03-08 04:56:02 - train: epoch 012, train_loss: 3.1941
2022-03-08 04:57:19 - eval: epoch: 012, acc1: 34.484%, acc5: 60.606%, test_loss: 3.0310, per_image_load_time: 2.418ms, per_image_inference_time: 0.237ms
2022-03-08 04:57:19 - until epoch: 012, best_acc1: 34.612%
2022-03-08 04:57:19 - epoch 013 lr: 0.1
2022-03-08 04:57:59 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.9747
2022-03-08 04:58:32 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 3.1963
2022-03-08 04:59:05 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 3.1149
2022-03-08 04:59:38 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 3.1057
2022-03-08 05:00:11 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 3.0122
2022-03-08 05:00:44 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 3.1673
2022-03-08 05:01:17 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 3.0384
2022-03-08 05:01:49 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 3.1796
2022-03-08 05:02:23 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 3.3843
2022-03-08 05:02:56 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 3.2525
2022-03-08 05:03:29 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 3.2588
2022-03-08 05:04:02 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 3.2742
2022-03-08 05:04:35 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 3.3243
2022-03-08 05:05:08 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 3.2182
2022-03-08 05:05:42 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 3.4229
2022-03-08 05:06:15 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.9078
2022-03-08 05:06:48 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 3.2441
2022-03-08 05:07:22 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 3.1177
2022-03-08 05:07:55 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 3.1645
2022-03-08 05:08:29 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 3.3579
2022-03-08 05:09:02 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 3.5027
2022-03-08 05:09:36 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 3.0967
2022-03-08 05:10:09 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 3.0458
2022-03-08 05:10:42 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 3.1737
2022-03-08 05:11:16 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 3.2530
2022-03-08 05:11:49 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 3.1429
2022-03-08 05:12:23 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 3.0093
2022-03-08 05:12:56 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 3.2547
2022-03-08 05:13:30 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 3.2754
2022-03-08 05:14:03 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 3.1324
2022-03-08 05:14:37 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 3.0191
2022-03-08 05:15:10 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 3.1294
2022-03-08 05:15:43 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 3.0038
2022-03-08 05:16:17 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 3.0891
2022-03-08 05:16:50 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 3.1344
2022-03-08 05:17:23 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 3.2999
2022-03-08 05:17:57 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.9176
2022-03-08 05:18:30 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 3.2371
2022-03-08 05:19:04 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 3.2471
2022-03-08 05:19:37 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 3.1576
2022-03-08 05:20:11 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 3.1164
2022-03-08 05:20:44 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 3.1366
2022-03-08 05:21:18 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 3.1182
2022-03-08 05:21:51 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 3.1185
2022-03-08 05:22:25 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 3.2989
2022-03-08 05:22:59 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 3.1052
2022-03-08 05:23:32 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 3.2339
2022-03-08 05:24:06 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 3.1428
2022-03-08 05:24:39 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 3.2895
2022-03-08 05:25:12 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 3.3041
2022-03-08 05:25:14 - train: epoch 013, train_loss: 3.1779
2022-03-08 05:26:30 - eval: epoch: 013, acc1: 35.134%, acc5: 61.506%, test_loss: 2.9850, per_image_load_time: 2.691ms, per_image_inference_time: 0.254ms
2022-03-08 05:26:30 - until epoch: 013, best_acc1: 35.134%
2022-03-08 05:26:30 - epoch 014 lr: 0.1
2022-03-08 05:27:10 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 3.1247
2022-03-08 05:27:43 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 3.2790
2022-03-08 05:28:17 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.8086
2022-03-08 05:28:49 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 3.0872
2022-03-08 05:29:22 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.9978
2022-03-08 05:29:55 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 3.1630
2022-03-08 05:30:28 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 3.1201
2022-03-08 05:31:01 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 3.0938
2022-03-08 05:31:34 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 3.2695
2022-03-08 05:32:07 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 3.3249
2022-03-08 05:32:40 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 3.0786
2022-03-08 05:33:13 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 3.1361
2022-03-08 05:33:46 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 3.0805
2022-03-08 05:34:19 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 3.3219
2022-03-08 05:34:52 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 3.1978
2022-03-08 05:35:25 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 3.0554
2022-03-08 05:35:59 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 3.3157
2022-03-08 05:36:32 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 3.2806
2022-03-08 05:37:06 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 3.0551
2022-03-08 05:37:39 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 3.1326
2022-03-08 05:38:12 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 3.3176
2022-03-08 05:38:45 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 3.1555
2022-03-08 05:39:19 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 3.1667
2022-03-08 05:39:52 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 3.3599
2022-03-08 05:40:25 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 3.1839
2022-03-08 05:40:58 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 3.2207
2022-03-08 05:41:32 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 3.0543
2022-03-08 05:42:04 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 3.3453
2022-03-08 05:42:38 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 3.0539
2022-03-08 05:43:11 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 3.3519
2022-03-08 05:43:45 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 3.1431
2022-03-08 05:44:18 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 3.1075
2022-03-08 05:44:52 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 3.0454
2022-03-08 05:45:25 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.9755
2022-03-08 05:45:59 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 3.1736
2022-03-08 05:46:32 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 3.1521
2022-03-08 05:47:06 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 3.0153
2022-03-08 05:47:39 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 3.3656
2022-03-08 05:48:12 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 3.1578
2022-03-08 05:48:46 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 3.3830
2022-03-08 05:49:20 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 3.1686
2022-03-08 05:49:54 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.8757
2022-03-08 05:50:28 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.9858
2022-03-08 05:51:02 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.9209
2022-03-08 05:51:35 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 3.1020
2022-03-08 05:52:09 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 3.1454
2022-03-08 05:52:43 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 3.1024
2022-03-08 05:53:17 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 3.1206
2022-03-08 05:53:52 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.9464
2022-03-08 05:54:25 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 3.1943
2022-03-08 05:54:26 - train: epoch 014, train_loss: 3.1647
2022-03-08 05:55:42 - eval: epoch: 014, acc1: 32.902%, acc5: 59.084%, test_loss: 3.1139, per_image_load_time: 2.658ms, per_image_inference_time: 0.243ms
2022-03-08 05:55:42 - until epoch: 014, best_acc1: 35.134%
2022-03-08 05:55:42 - epoch 015 lr: 0.1
2022-03-08 05:56:21 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 3.0626
2022-03-08 05:56:55 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 3.2414
2022-03-08 05:57:27 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 3.4115
2022-03-08 05:58:00 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 3.1940
2022-03-08 05:58:34 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 3.0035
2022-03-08 05:59:07 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 3.2121
2022-03-08 05:59:40 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 3.1683
2022-03-08 06:00:13 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.9823
2022-03-08 06:00:46 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 3.0600
2022-03-08 06:01:19 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 3.2225
2022-03-08 06:01:52 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 3.0931
2022-03-08 06:02:25 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 3.1681
2022-03-08 06:02:59 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 3.2806
2022-03-08 06:03:32 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 3.1478
2022-03-08 06:04:06 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.8621
2022-03-08 06:04:39 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 3.1302
2022-03-08 06:05:12 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 3.3510
2022-03-08 06:05:45 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.9227
2022-03-08 06:06:19 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 3.0598
2022-03-08 06:06:52 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 3.0499
2022-03-08 06:07:26 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.8957
2022-03-08 06:07:59 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 3.3256
2022-03-08 06:08:32 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.9872
2022-03-08 06:09:06 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 3.0791
2022-03-08 06:09:40 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 3.3335
2022-03-08 06:10:13 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.8912
2022-03-08 06:10:47 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 3.0989
2022-03-08 06:11:20 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 3.4049
2022-03-08 06:11:53 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.9676
2022-03-08 06:12:27 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 3.0106
2022-03-08 06:13:01 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 3.0109
2022-03-08 06:13:34 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 3.1162
2022-03-08 06:14:07 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 3.0616
2022-03-08 06:14:41 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 3.0888
2022-03-08 06:15:14 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 3.2637
2022-03-08 06:15:48 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 3.0764
2022-03-08 06:16:22 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 3.0962
2022-03-08 06:16:55 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 3.0473
2022-03-08 06:17:29 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 3.4498
2022-03-08 06:18:02 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 3.2031
2022-03-08 06:18:36 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 3.2605
2022-03-08 06:19:09 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.8879
2022-03-08 06:19:43 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 3.2087
2022-03-08 06:20:17 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 3.0138
2022-03-08 06:20:50 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 3.2239
2022-03-08 06:21:24 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 3.0494
2022-03-08 06:21:59 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 3.2883
2022-03-08 06:22:33 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 3.2764
2022-03-08 06:23:07 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 3.2389
2022-03-08 06:23:39 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 3.3707
2022-03-08 06:23:41 - train: epoch 015, train_loss: 3.1528
2022-03-08 06:24:57 - eval: epoch: 015, acc1: 32.824%, acc5: 59.170%, test_loss: 3.1071, per_image_load_time: 2.675ms, per_image_inference_time: 0.261ms
2022-03-08 06:24:57 - until epoch: 015, best_acc1: 35.134%
2022-03-08 06:24:57 - epoch 016 lr: 0.1
2022-03-08 06:25:36 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 3.1034
2022-03-08 06:26:09 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.9012
2022-03-08 06:26:42 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 3.0982
2022-03-08 06:27:14 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 3.4077
2022-03-08 06:27:48 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.8341
2022-03-08 06:28:21 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 3.2083
2022-03-08 06:28:55 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.8772
2022-03-08 06:29:28 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 3.0827
2022-03-08 06:30:02 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 3.2295
2022-03-08 06:30:35 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 3.0537
2022-03-08 06:31:08 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.8961
2022-03-08 06:31:41 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.9352
2022-03-08 06:32:14 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 3.2155
2022-03-08 06:32:47 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.9790
2022-03-08 06:33:20 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 3.2543
2022-03-08 06:33:53 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 3.2540
2022-03-08 06:34:27 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 3.0862
2022-03-08 06:35:00 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 3.0862
2022-03-08 06:35:34 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 3.1332
2022-03-08 06:36:07 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.6902
2022-03-08 06:36:41 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 3.3486
2022-03-08 06:37:15 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 3.1753
2022-03-08 06:37:48 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 3.2919
2022-03-08 06:38:22 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 3.3312
2022-03-08 06:38:55 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.9950
2022-03-08 06:39:29 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 3.1977
2022-03-08 06:40:01 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 3.1313
2022-03-08 06:40:35 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.9773
2022-03-08 06:41:09 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 3.1400
2022-03-08 06:41:42 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 3.3903
2022-03-08 06:42:16 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 3.1692
2022-03-08 06:42:49 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 3.2260
2022-03-08 06:43:23 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 3.2477
2022-03-08 06:43:57 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 3.1454
2022-03-08 06:44:30 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 3.0212
2022-03-08 06:45:04 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 3.0594
2022-03-08 06:45:37 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 3.3083
2022-03-08 06:46:10 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 3.4106
2022-03-08 06:46:43 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 3.2235
2022-03-08 06:47:17 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 3.1345
2022-03-08 06:47:50 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 3.1320
2022-03-08 06:48:23 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 3.1375
2022-03-08 06:48:57 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 3.0044
2022-03-08 06:49:31 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 3.0070
2022-03-08 06:50:04 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 3.2218
2022-03-08 06:50:38 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 3.0277
2022-03-08 06:51:12 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 3.3263
2022-03-08 06:51:46 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 3.0260
2022-03-08 06:52:20 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 3.1955
2022-03-08 06:52:52 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 3.3065
2022-03-08 06:52:54 - train: epoch 016, train_loss: 3.1413
2022-03-08 06:54:11 - eval: epoch: 016, acc1: 30.244%, acc5: 55.568%, test_loss: 3.3032, per_image_load_time: 2.706ms, per_image_inference_time: 0.240ms
2022-03-08 06:54:11 - until epoch: 016, best_acc1: 35.134%
2022-03-08 06:54:11 - epoch 017 lr: 0.1
2022-03-08 06:54:50 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 3.0151
2022-03-08 06:55:23 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 3.2377
2022-03-08 06:55:56 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 3.2708
2022-03-08 06:56:29 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.9216
2022-03-08 06:57:02 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 3.1395
2022-03-08 06:57:35 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 3.3889
2022-03-08 06:58:09 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 3.0606
2022-03-08 06:58:42 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 3.0936
2022-03-08 06:59:15 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 3.0716
2022-03-08 06:59:48 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.9539
2022-03-08 07:00:21 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 3.5347
2022-03-08 07:00:54 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 3.2947
2022-03-08 07:01:28 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 3.2699
2022-03-08 07:02:01 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 3.2753
2022-03-08 07:02:34 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.8622
2022-03-08 07:03:08 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.9352
2022-03-08 07:03:41 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 3.0159
2022-03-08 07:04:15 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 3.2233
2022-03-08 07:04:48 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.9398
2022-03-08 07:05:22 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 3.2227
2022-03-08 07:05:55 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 3.1220
2022-03-08 07:06:28 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 3.0696
2022-03-08 07:07:01 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 3.0748
2022-03-08 07:07:35 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 3.0177
2022-03-08 07:08:08 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 3.3190
2022-03-08 07:08:41 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 3.1047
2022-03-08 07:09:14 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 3.1176
2022-03-08 07:09:47 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 3.3140
2022-03-08 07:10:21 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 3.3397
2022-03-08 07:10:54 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.9272
2022-03-08 07:11:27 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 3.3362
2022-03-08 07:12:01 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.9937
2022-03-08 07:12:34 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 3.0411
2022-03-08 07:13:07 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.9989
2022-03-08 07:13:41 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 3.2185
2022-03-08 07:14:14 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 3.2421
2022-03-08 07:14:48 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 3.0183
2022-03-08 07:15:21 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 3.3888
2022-03-08 07:15:54 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 3.0337
2022-03-08 07:16:28 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.9417
2022-03-08 07:17:01 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 3.2004
2022-03-08 07:17:35 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 3.0243
2022-03-08 07:18:08 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 3.1146
2022-03-08 07:18:42 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 3.2090
2022-03-08 07:19:15 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 3.2168
2022-03-08 07:19:49 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 3.2791
2022-03-08 07:20:23 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 3.2158
2022-03-08 07:20:57 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 3.1935
2022-03-08 07:21:30 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 3.0481
2022-03-08 07:22:03 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.9236
2022-03-08 07:22:04 - train: epoch 017, train_loss: 3.1299
2022-03-08 07:23:20 - eval: epoch: 017, acc1: 34.758%, acc5: 61.630%, test_loss: 2.9825, per_image_load_time: 2.738ms, per_image_inference_time: 0.202ms
2022-03-08 07:23:20 - until epoch: 017, best_acc1: 35.134%
2022-03-08 07:23:20 - epoch 018 lr: 0.1
2022-03-08 07:24:00 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 3.0580
2022-03-08 07:24:33 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 3.1580
2022-03-08 07:25:06 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 3.3559
2022-03-08 07:25:39 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 3.2175
2022-03-08 07:26:12 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 3.1776
2022-03-08 07:26:46 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 3.2357
2022-03-08 07:27:18 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.9532
2022-03-08 07:27:52 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 3.1117
2022-03-08 07:28:25 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 3.2756
2022-03-08 07:28:58 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 3.1421
2022-03-08 07:29:31 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 3.4301
2022-03-08 07:30:04 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 3.2570
2022-03-08 07:30:38 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 3.4070
2022-03-08 07:31:11 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 3.2569
2022-03-08 07:31:45 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 3.2721
2022-03-08 07:32:18 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 3.0382
2022-03-08 07:32:51 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.8673
2022-03-08 07:33:25 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 3.0702
2022-03-08 07:33:58 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 3.1767
2022-03-08 07:34:32 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 3.3668
2022-03-08 07:35:05 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 3.2930
2022-03-08 07:35:38 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 3.1487
2022-03-08 07:36:11 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 3.2918
2022-03-08 07:36:45 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.9766
2022-03-08 07:37:18 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.8229
2022-03-08 07:37:52 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.9626
2022-03-08 07:38:25 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 3.2726
2022-03-08 07:38:58 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.8120
2022-03-08 07:39:32 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 3.1409
2022-03-08 07:40:05 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.8778
2022-03-08 07:40:38 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 3.4617
2022-03-08 07:41:12 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 3.0330
2022-03-08 07:41:45 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 3.0715
2022-03-08 07:42:18 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 3.1162
2022-03-08 07:42:52 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 3.2205
2022-03-08 07:43:25 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 3.2449
2022-03-08 07:43:59 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 3.5545
2022-03-08 07:44:32 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 3.1984
2022-03-08 07:45:06 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 3.1565
2022-03-08 07:45:39 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 3.0044
2022-03-08 07:46:13 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.9437
2022-03-08 07:46:46 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 3.1036
2022-03-08 07:47:20 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.8750
2022-03-08 07:47:54 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 3.2131
2022-03-08 07:48:27 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 3.1908
2022-03-08 07:49:01 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 3.0081
2022-03-08 07:49:34 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 3.2098
2022-03-08 07:50:08 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 3.2300
2022-03-08 07:50:42 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 3.2061
2022-03-08 07:51:15 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 3.2054
2022-03-08 07:51:17 - train: epoch 018, train_loss: 3.1196
2022-03-08 07:52:34 - eval: epoch: 018, acc1: 35.222%, acc5: 61.820%, test_loss: 2.9859, per_image_load_time: 2.754ms, per_image_inference_time: 0.220ms
2022-03-08 07:52:34 - until epoch: 018, best_acc1: 35.222%
2022-03-08 07:52:34 - epoch 019 lr: 0.1
2022-03-08 07:53:14 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 3.0878
2022-03-08 07:53:47 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 3.2564
2022-03-08 07:54:21 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 3.2600
2022-03-08 07:54:54 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 3.1508
2022-03-08 07:55:27 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 3.0041
2022-03-08 07:56:01 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 3.1095
2022-03-08 07:56:34 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.8664
2022-03-08 07:57:08 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 3.3639
2022-03-08 07:57:41 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 3.1630
2022-03-08 07:58:15 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 3.2046
2022-03-08 07:58:48 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.9180
2022-03-08 07:59:22 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 3.2228
2022-03-08 07:59:55 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 3.1664
2022-03-08 08:00:29 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.9451
2022-03-08 08:01:02 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 3.5304
2022-03-08 08:01:36 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 3.0099
2022-03-08 08:02:09 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 3.2899
2022-03-08 08:02:43 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 3.0346
2022-03-08 08:03:16 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 3.3285
2022-03-08 08:03:50 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 3.2038
2022-03-08 08:04:23 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 3.1447
2022-03-08 08:04:57 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 3.0935
2022-03-08 08:05:31 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 3.1935
2022-03-08 08:06:04 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 3.1898
2022-03-08 08:06:37 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 3.0835
2022-03-08 08:07:11 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 3.0552
2022-03-08 08:07:44 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 3.1630
2022-03-08 08:08:18 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 3.1910
2022-03-08 08:08:51 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 3.0965
2022-03-08 08:09:25 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 3.2674
2022-03-08 08:09:58 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 3.0865
2022-03-08 08:10:32 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 3.0063
2022-03-08 08:11:05 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 3.0219
2022-03-08 08:11:39 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 3.0539
2022-03-08 08:12:12 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 3.2422
2022-03-08 08:12:45 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.9570
2022-03-08 08:13:19 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 3.3122
2022-03-08 08:13:52 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 3.1708
2022-03-08 08:14:26 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 3.1350
2022-03-08 08:14:59 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 3.1046
2022-03-08 08:15:32 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 3.2135
2022-03-08 08:16:06 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 3.0689
2022-03-08 08:16:39 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 3.1487
2022-03-08 08:17:13 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 3.2633
2022-03-08 08:17:46 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 3.3394
2022-03-08 08:18:20 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.9884
2022-03-08 08:18:53 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 3.0364
2022-03-08 08:19:27 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 3.0849
2022-03-08 08:20:01 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.9037
2022-03-08 08:20:34 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 3.1229
2022-03-08 08:20:36 - train: epoch 019, train_loss: 3.1163
2022-03-08 08:21:52 - eval: epoch: 019, acc1: 36.122%, acc5: 62.878%, test_loss: 2.9146, per_image_load_time: 2.757ms, per_image_inference_time: 0.217ms
2022-03-08 08:21:52 - until epoch: 019, best_acc1: 36.122%
2022-03-08 08:21:52 - epoch 020 lr: 0.1
2022-03-08 08:22:32 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 3.2743
2022-03-08 08:23:05 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.8762
2022-03-08 08:23:39 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 3.2860
2022-03-08 08:24:12 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.8038
2022-03-08 08:24:46 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 3.0263
2022-03-08 08:25:20 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 3.3972
2022-03-08 08:25:53 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 3.1091
2022-03-08 08:26:26 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 3.0839
2022-03-08 08:26:59 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 3.2128
2022-03-08 08:27:32 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 3.0376
2022-03-08 08:28:05 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 3.0696
2022-03-08 08:28:38 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.9809
2022-03-08 08:29:12 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 3.0544
2022-03-08 08:29:45 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 3.2857
2022-03-08 08:30:19 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 3.0375
2022-03-08 08:30:52 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.9060
2022-03-08 08:31:25 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.8843
2022-03-08 08:31:59 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 3.0687
2022-03-08 08:32:32 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.9036
2022-03-08 08:33:05 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 3.1059
2022-03-08 08:33:39 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 3.2763
2022-03-08 08:34:12 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 3.0503
2022-03-08 08:34:46 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 3.0784
2022-03-08 08:35:19 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 3.1689
2022-03-08 08:35:52 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 3.0398
2022-03-08 08:36:26 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.9182
2022-03-08 08:36:59 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 3.1910
2022-03-08 08:37:32 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 3.2881
2022-03-08 08:38:06 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 3.2672
2022-03-08 08:38:39 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 3.1823
2022-03-08 08:39:13 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 3.1819
2022-03-08 08:39:46 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 3.3148
2022-03-08 08:40:19 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 3.1110
2022-03-08 08:40:53 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.9372
2022-03-08 08:41:26 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.9042
2022-03-08 08:41:59 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 3.0677
2022-03-08 08:42:33 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 3.0672
2022-03-08 08:43:06 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 3.1387
2022-03-08 08:43:40 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 3.2157
2022-03-08 08:44:13 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 3.1062
2022-03-08 08:44:46 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 3.0166
2022-03-08 08:45:20 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.9886
2022-03-08 08:45:54 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 3.2295
2022-03-08 08:46:27 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 3.0891
2022-03-08 08:47:01 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 3.0987
2022-03-08 08:47:35 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 3.2258
2022-03-08 08:48:09 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.8688
2022-03-08 08:48:43 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 3.2480
2022-03-08 08:49:17 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 3.2567
2022-03-08 08:49:50 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.9541
2022-03-08 08:49:51 - train: epoch 020, train_loss: 3.1023
2022-03-08 08:51:07 - eval: epoch: 020, acc1: 34.410%, acc5: 60.924%, test_loss: 3.0265, per_image_load_time: 2.666ms, per_image_inference_time: 0.250ms
2022-03-08 08:51:07 - until epoch: 020, best_acc1: 36.122%
2022-03-08 08:51:07 - epoch 021 lr: 0.1
2022-03-08 08:51:47 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.8739
2022-03-08 08:52:20 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 3.0767
2022-03-08 08:52:53 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.7594
2022-03-08 08:53:26 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 3.1610
2022-03-08 08:53:59 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.8561
2022-03-08 08:54:33 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.9646
2022-03-08 08:55:06 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 3.0593
2022-03-08 08:55:39 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 3.2455
2022-03-08 08:56:12 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 3.1728
2022-03-08 08:56:45 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.8410
2022-03-08 08:57:18 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 3.1179
2022-03-08 08:57:50 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 3.0798
2022-03-08 08:58:24 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 3.0880
2022-03-08 08:58:57 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 3.1559
2022-03-08 08:59:30 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.9761
2022-03-08 09:00:04 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 3.1518
2022-03-08 09:00:37 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 3.1938
2022-03-08 09:01:10 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.9403
2022-03-08 09:01:44 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 3.1865
2022-03-08 09:02:17 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 3.2125
2022-03-08 09:02:51 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.9038
2022-03-08 09:03:24 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 3.1464
2022-03-08 09:03:57 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 3.0651
2022-03-08 09:04:31 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.9709
2022-03-08 09:05:04 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 3.0280
2022-03-08 09:05:38 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 3.3194
2022-03-08 09:06:11 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.9183
2022-03-08 09:06:44 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 3.2635
2022-03-08 09:07:18 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.9941
2022-03-08 09:07:51 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 3.3501
2022-03-08 09:08:24 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 3.1734
2022-03-08 09:08:58 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 3.1058
2022-03-08 09:09:31 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 3.3602
2022-03-08 09:10:04 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 3.2946
2022-03-08 09:10:38 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 3.0375
2022-03-08 09:11:11 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.9860
2022-03-08 09:11:45 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 3.0766
2022-03-08 09:12:18 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.8805
2022-03-08 09:12:52 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 3.1056
2022-03-08 09:13:25 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 3.2779
2022-03-08 09:13:58 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 3.0302
2022-03-08 09:14:31 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 3.1380
2022-03-08 09:15:04 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 3.1448
2022-03-08 09:15:38 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 3.1009
2022-03-08 09:16:11 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 3.2929
2022-03-08 09:16:45 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 3.0325
2022-03-08 09:17:19 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 3.2033
2022-03-08 09:17:53 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 3.1226
2022-03-08 09:18:27 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 3.0490
2022-03-08 09:18:59 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.9566
2022-03-08 09:19:01 - train: epoch 021, train_loss: 3.0971
2022-03-08 09:20:16 - eval: epoch: 021, acc1: 34.288%, acc5: 61.060%, test_loss: 3.0252, per_image_load_time: 2.596ms, per_image_inference_time: 0.271ms
2022-03-08 09:20:16 - until epoch: 021, best_acc1: 36.122%
2022-03-08 09:20:16 - epoch 022 lr: 0.1
2022-03-08 09:20:55 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.8114
2022-03-08 09:21:27 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.9988
2022-03-08 09:21:59 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.7404
2022-03-08 09:22:31 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 3.0388
2022-03-08 09:23:03 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.9400
2022-03-08 09:23:35 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 3.0868
2022-03-08 09:24:07 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 3.2002
2022-03-08 09:24:39 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 3.2227
2022-03-08 09:25:12 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 3.2625
2022-03-08 09:25:44 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 3.1060
2022-03-08 09:26:16 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 3.1130
2022-03-08 09:26:48 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.6953
2022-03-08 09:27:20 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 3.1764
2022-03-08 09:27:52 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 3.2385
2022-03-08 09:28:25 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 3.1417
2022-03-08 09:28:58 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.9921
2022-03-08 09:29:31 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.8719
2022-03-08 09:30:04 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 3.3343
2022-03-08 09:30:37 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.9652
2022-03-08 09:31:11 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 3.0643
2022-03-08 09:31:44 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.9942
2022-03-08 09:32:17 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.9045
2022-03-08 09:32:50 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 3.1682
2022-03-08 09:33:23 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 3.0377
2022-03-08 09:33:56 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.9350
2022-03-08 09:34:30 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.9708
2022-03-08 09:35:03 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.9823
2022-03-08 09:35:37 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 3.5943
2022-03-08 09:36:10 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.9593
2022-03-08 09:36:44 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 3.1969
2022-03-08 09:37:17 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 3.2398
2022-03-08 09:37:51 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 3.0926
2022-03-08 09:38:25 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 3.1493
2022-03-08 09:38:58 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 3.0235
2022-03-08 09:39:32 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 3.2563
2022-03-08 09:40:06 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 3.1107
2022-03-08 09:40:39 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 3.1144
2022-03-08 09:41:13 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 3.2766
2022-03-08 09:41:47 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 3.0478
2022-03-08 09:42:20 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 3.0889
2022-03-08 09:42:53 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.9624
2022-03-08 09:43:27 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 3.0637
2022-03-08 09:44:01 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 3.1534
2022-03-08 09:44:35 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 3.1297
2022-03-08 09:45:08 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 3.0935
2022-03-08 09:45:42 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 3.3100
2022-03-08 09:46:16 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 3.2075
2022-03-08 09:46:49 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.8085
2022-03-08 09:47:23 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.9084
2022-03-08 09:47:55 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.8755
2022-03-08 09:47:57 - train: epoch 022, train_loss: 3.0883
2022-03-08 09:49:13 - eval: epoch: 022, acc1: 36.366%, acc5: 63.438%, test_loss: 2.8969, per_image_load_time: 2.048ms, per_image_inference_time: 0.223ms
2022-03-08 09:49:13 - until epoch: 022, best_acc1: 36.366%
2022-03-08 09:49:13 - epoch 023 lr: 0.1
2022-03-08 09:49:53 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.9322
2022-03-08 09:50:26 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.8730
2022-03-08 09:50:58 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.8823
2022-03-08 09:51:31 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 3.2564
2022-03-08 09:52:04 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 3.0805
2022-03-08 09:52:37 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 3.1646
2022-03-08 09:53:10 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.9427
2022-03-08 09:53:44 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 3.0004
2022-03-08 09:54:17 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 3.0167
2022-03-08 09:54:50 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.9290
2022-03-08 09:55:23 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 3.1487
2022-03-08 09:55:56 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.9203
2022-03-08 09:56:29 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 3.2111
2022-03-08 09:57:02 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 3.1351
2022-03-08 09:57:35 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.9006
2022-03-08 09:58:08 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.9714
2022-03-08 09:58:41 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 3.2110
2022-03-08 09:59:13 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 3.0240
2022-03-08 09:59:46 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 3.1482
2022-03-08 10:00:19 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.6636
2022-03-08 10:00:52 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 3.1734
2022-03-08 10:01:25 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.7056
2022-03-08 10:01:58 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 3.0423
2022-03-08 10:02:31 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 3.1715
2022-03-08 10:03:04 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 3.3250
2022-03-08 10:03:37 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 3.0956
2022-03-08 10:04:10 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.9912
2022-03-08 10:04:43 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 3.0825
2022-03-08 10:05:16 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 3.1596
2022-03-08 10:05:49 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 3.1621
2022-03-08 10:06:23 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 3.1373
2022-03-08 10:06:56 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 3.1034
2022-03-08 10:07:28 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 3.1934
2022-03-08 10:08:01 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 3.1257
2022-03-08 10:08:34 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.9652
2022-03-08 10:09:07 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.9565
2022-03-08 10:09:40 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 3.1339
2022-03-08 10:10:13 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 3.0353
2022-03-08 10:10:46 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 3.0237
2022-03-08 10:11:20 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.9982
2022-03-08 10:11:52 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.9407
2022-03-08 10:12:26 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.9046
2022-03-08 10:12:59 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.8766
2022-03-08 10:13:32 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.9145
2022-03-08 10:14:06 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 3.0798
2022-03-08 10:14:40 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 3.0069
2022-03-08 10:15:13 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.9427
2022-03-08 10:15:47 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.9754
2022-03-08 10:16:21 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 3.0213
2022-03-08 10:16:53 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 3.1621
2022-03-08 10:16:55 - train: epoch 023, train_loss: 3.0813
2022-03-08 10:18:11 - eval: epoch: 023, acc1: 35.978%, acc5: 62.476%, test_loss: 2.9244, per_image_load_time: 2.137ms, per_image_inference_time: 0.249ms
2022-03-08 10:18:11 - until epoch: 023, best_acc1: 36.366%
2022-03-08 10:18:11 - epoch 024 lr: 0.1
2022-03-08 10:18:51 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 3.0740
2022-03-08 10:19:23 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 3.0418
2022-03-08 10:19:56 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 3.1213
2022-03-08 10:20:29 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 3.1670
2022-03-08 10:21:01 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 3.1554
2022-03-08 10:21:34 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.8678
2022-03-08 10:22:06 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 3.0268
2022-03-08 10:22:39 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.9614
2022-03-08 10:23:12 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 3.0726
2022-03-08 10:23:45 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.9782
2022-03-08 10:24:17 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.8070
2022-03-08 10:24:50 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.9122
2022-03-08 10:25:23 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 3.4734
2022-03-08 10:25:57 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.8496
2022-03-08 10:26:30 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 3.4208
2022-03-08 10:27:03 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 3.1141
2022-03-08 10:27:36 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 3.1181
2022-03-08 10:28:09 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 3.1019
2022-03-08 10:28:42 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.8126
2022-03-08 10:29:16 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 3.1532
2022-03-08 10:29:49 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.9695
2022-03-08 10:30:22 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.8721
2022-03-08 10:30:55 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 3.0562
2022-03-08 10:31:28 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 3.0421
2022-03-08 10:32:01 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 3.0231
2022-03-08 10:32:34 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.9371
2022-03-08 10:33:07 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 3.1451
2022-03-08 10:33:40 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 3.2578
2022-03-08 10:34:14 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 3.1807
2022-03-08 10:34:47 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.9977
2022-03-08 10:35:20 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 3.0042
2022-03-08 10:35:54 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 3.1639
2022-03-08 10:36:27 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.8655
2022-03-08 10:37:01 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 3.1260
2022-03-08 10:37:34 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.8694
2022-03-08 10:38:08 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 3.1078
2022-03-08 10:38:41 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.9995
2022-03-08 10:39:15 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 3.2512
2022-03-08 10:39:48 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 3.0520
2022-03-08 10:40:21 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 3.0488
2022-03-08 10:40:55 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.9730
2022-03-08 10:41:28 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 3.0234
2022-03-08 10:42:01 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 3.0554
2022-03-08 10:42:34 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 3.0472
2022-03-08 10:43:09 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.8334
2022-03-08 10:43:42 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 3.1344
2022-03-08 10:44:16 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.9954
2022-03-08 10:44:50 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.9834
2022-03-08 10:45:24 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 3.0499
2022-03-08 10:45:56 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 3.1874
2022-03-08 10:45:58 - train: epoch 024, train_loss: 3.0757
2022-03-08 10:47:14 - eval: epoch: 024, acc1: 36.350%, acc5: 63.060%, test_loss: 2.9049, per_image_load_time: 2.622ms, per_image_inference_time: 0.242ms
2022-03-08 10:47:14 - until epoch: 024, best_acc1: 36.366%
2022-03-08 10:47:14 - epoch 025 lr: 0.1
2022-03-08 10:47:53 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 3.0130
2022-03-08 10:48:26 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.8177
2022-03-08 10:48:59 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.8328
2022-03-08 10:49:32 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 3.2815
2022-03-08 10:50:05 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.9878
2022-03-08 10:50:38 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 3.0896
2022-03-08 10:51:11 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 3.1280
2022-03-08 10:51:44 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 3.0372
2022-03-08 10:52:17 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.8922
2022-03-08 10:52:51 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.9496
2022-03-08 10:53:24 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 3.1700
2022-03-08 10:53:57 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 3.0615
2022-03-08 10:54:30 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 3.0501
2022-03-08 10:55:03 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.9923
2022-03-08 10:55:36 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.9559
2022-03-08 10:56:09 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.9620
2022-03-08 10:56:41 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 3.1907
2022-03-08 10:57:14 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.8596
2022-03-08 10:57:47 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.9580
2022-03-08 10:58:20 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 3.1602
2022-03-08 10:58:53 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.9773
2022-03-08 10:59:26 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 3.0328
2022-03-08 10:59:59 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 3.2050
2022-03-08 11:00:33 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.8010
2022-03-08 11:01:05 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 3.2385
2022-03-08 11:01:39 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 3.2056
2022-03-08 11:02:12 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 3.0672
2022-03-08 11:02:45 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 3.0063
2022-03-08 11:03:18 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 3.1854
2022-03-08 11:03:51 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 3.1225
2022-03-08 11:04:24 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 3.0140
2022-03-08 11:04:57 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 3.1160
2022-03-08 11:05:29 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 3.3003
2022-03-08 11:06:02 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 3.2549
2022-03-08 11:06:35 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.9964
2022-03-08 11:07:09 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 3.2044
2022-03-08 11:07:42 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 3.0217
2022-03-08 11:08:15 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 3.1917
2022-03-08 11:08:47 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 3.3157
2022-03-08 11:09:20 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 3.1776
2022-03-08 11:09:53 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 3.2083
2022-03-08 11:10:27 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 3.1124
2022-03-08 11:11:00 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.9581
2022-03-08 11:11:34 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.8808
2022-03-08 11:12:07 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 3.0267
2022-03-08 11:12:40 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.8576
2022-03-08 11:13:14 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 3.0321
2022-03-08 11:13:47 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.7622
2022-03-08 11:14:20 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 3.0684
2022-03-08 11:14:54 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 3.2512
2022-03-08 11:14:56 - train: epoch 025, train_loss: 3.0699
2022-03-08 11:16:12 - eval: epoch: 025, acc1: 36.054%, acc5: 62.526%, test_loss: 2.9339, per_image_load_time: 2.700ms, per_image_inference_time: 0.246ms
2022-03-08 11:16:12 - until epoch: 025, best_acc1: 36.366%
2022-03-08 11:16:12 - epoch 026 lr: 0.1
2022-03-08 11:16:52 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.8573
2022-03-08 11:17:25 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.9753
2022-03-08 11:17:58 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 3.0167
2022-03-08 11:18:31 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.9679
2022-03-08 11:19:04 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 3.0747
2022-03-08 11:19:38 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 3.1085
2022-03-08 11:20:11 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.9035
2022-03-08 11:20:44 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 3.0531
2022-03-08 11:21:17 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 3.2758
2022-03-08 11:21:51 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.9902
2022-03-08 11:22:24 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 3.0310
2022-03-08 11:22:57 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 3.1178
2022-03-08 11:23:31 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.9310
2022-03-08 11:24:04 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.9508
2022-03-08 11:24:37 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.9495
2022-03-08 11:25:10 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 3.2910
2022-03-08 11:25:43 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 3.0403
2022-03-08 11:26:16 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 3.3354
2022-03-08 11:26:49 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 3.3142
2022-03-08 11:27:23 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 3.2745
2022-03-08 11:27:56 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 3.0820
2022-03-08 11:28:29 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 3.1227
2022-03-08 11:29:03 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.9053
2022-03-08 11:29:36 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 3.3438
2022-03-08 11:30:10 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 3.2292
2022-03-08 11:30:43 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 3.1547
2022-03-08 11:31:16 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.9747
2022-03-08 11:31:49 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.9193
2022-03-08 11:32:23 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 3.2463
2022-03-08 11:32:56 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.9563
2022-03-08 11:33:29 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.9329
2022-03-08 11:34:02 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 3.0273
2022-03-08 11:34:36 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.9503
2022-03-08 11:35:09 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 3.1076
2022-03-08 11:35:42 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 3.0281
2022-03-08 11:36:16 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 3.1341
2022-03-08 11:36:49 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 3.2230
2022-03-08 11:37:23 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 3.2645
2022-03-08 11:37:56 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 3.2494
2022-03-08 11:38:30 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 3.2800
2022-03-08 11:39:03 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 3.2447
2022-03-08 11:39:36 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 3.1809
2022-03-08 11:40:10 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 3.0860
2022-03-08 11:40:43 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 3.2657
2022-03-08 11:41:17 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 3.3466
2022-03-08 11:41:50 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.9111
2022-03-08 11:42:24 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.9939
2022-03-08 11:42:58 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.9947
2022-03-08 11:43:31 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 3.1979
2022-03-08 11:44:04 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 3.3389
2022-03-08 11:44:06 - train: epoch 026, train_loss: 3.0636
2022-03-08 11:45:22 - eval: epoch: 026, acc1: 34.534%, acc5: 60.746%, test_loss: 3.0164, per_image_load_time: 2.703ms, per_image_inference_time: 0.230ms
2022-03-08 11:45:22 - until epoch: 026, best_acc1: 36.366%
2022-03-08 11:45:22 - epoch 027 lr: 0.1
2022-03-08 11:46:02 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 3.4067
2022-03-08 11:46:35 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.9057
2022-03-08 11:47:08 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 3.1122
2022-03-08 11:47:41 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 3.2361
2022-03-08 11:48:14 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 3.1546
2022-03-08 11:48:46 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 3.2990
2022-03-08 11:49:19 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 3.3037
2022-03-08 11:49:52 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 3.1320
2022-03-08 11:50:25 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 3.1278
2022-03-08 11:50:58 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 3.2108
2022-03-08 11:51:31 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.9787
2022-03-08 11:52:04 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 3.2244
2022-03-08 11:52:37 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 3.0126
2022-03-08 11:53:10 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 3.3607
2022-03-08 11:53:43 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.9068
2022-03-08 11:54:16 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 3.1835
2022-03-08 11:54:49 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 3.0767
2022-03-08 11:55:22 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 3.0676
2022-03-08 11:55:55 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 3.0776
2022-03-08 11:56:28 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 3.0249
2022-03-08 11:57:01 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 3.2859
2022-03-08 11:57:33 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 3.3465
2022-03-08 11:58:06 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 3.4135
2022-03-08 11:58:39 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.9990
2022-03-08 11:59:12 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 3.0845
2022-03-08 11:59:45 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 3.2343
2022-03-08 12:00:18 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 3.0685
2022-03-08 12:00:51 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 3.1107
2022-03-08 12:01:24 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 3.0416
2022-03-08 12:01:57 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.8809
2022-03-08 12:02:30 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.7555
2022-03-08 12:03:03 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.9456
2022-03-08 12:03:36 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 3.1274
2022-03-08 12:04:09 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.9511
2022-03-08 12:04:42 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 3.1973
2022-03-08 12:05:14 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.9506
2022-03-08 12:05:47 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 3.0365
2022-03-08 12:06:20 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.8201
2022-03-08 12:06:52 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.9322
2022-03-08 12:07:26 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 3.2041
2022-03-08 12:07:59 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.9797
2022-03-08 12:08:32 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 3.1215
2022-03-08 12:09:06 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.9640
2022-03-08 12:09:39 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 3.0935
2022-03-08 12:10:12 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 3.0688
2022-03-08 12:10:46 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 3.0587
2022-03-08 12:11:20 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 3.0903
2022-03-08 12:11:54 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 3.1895
2022-03-08 12:12:27 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 3.1137
2022-03-08 12:12:59 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.7519
2022-03-08 12:13:01 - train: epoch 027, train_loss: 3.0608
2022-03-08 12:14:17 - eval: epoch: 027, acc1: 37.496%, acc5: 63.890%, test_loss: 2.8480, per_image_load_time: 2.698ms, per_image_inference_time: 0.215ms
2022-03-08 12:14:17 - until epoch: 027, best_acc1: 37.496%
2022-03-08 12:14:17 - epoch 028 lr: 0.1
2022-03-08 12:14:56 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.9497
2022-03-08 12:15:29 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.9615
2022-03-08 12:16:02 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.8863
2022-03-08 12:16:36 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 3.0316
2022-03-08 12:17:09 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.9565
2022-03-08 12:17:42 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 3.0135
2022-03-08 12:18:15 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 3.2524
2022-03-08 12:18:48 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.9370
2022-03-08 12:19:21 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.8530
2022-03-08 12:19:54 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 3.0083
2022-03-08 12:20:28 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 3.0413
2022-03-08 12:21:01 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.9626
2022-03-08 12:21:34 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 3.0437
2022-03-08 12:22:07 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 3.3106
2022-03-08 12:22:41 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 3.0665
2022-03-08 12:23:14 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 3.0316
2022-03-08 12:23:47 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 3.0277
2022-03-08 12:24:20 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.9279
2022-03-08 12:24:53 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 3.0411
2022-03-08 12:25:26 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 3.1712
2022-03-08 12:26:00 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 3.1733
2022-03-08 12:26:33 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 3.1280
2022-03-08 12:27:06 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 3.2091
2022-03-08 12:27:39 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 3.2655
2022-03-08 12:28:12 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 3.0995
2022-03-08 12:28:46 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.8959
2022-03-08 12:29:19 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.9830
2022-03-08 12:29:52 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 3.0323
2022-03-08 12:30:25 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.9594
2022-03-08 12:30:59 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 3.1648
2022-03-08 12:31:32 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 3.2849
2022-03-08 12:32:05 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.9652
2022-03-08 12:32:38 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 3.0026
2022-03-08 12:33:12 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.9809
2022-03-08 12:33:45 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.8813
2022-03-08 12:34:18 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.9618
2022-03-08 12:34:51 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 3.1498
2022-03-08 12:35:24 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.7234
2022-03-08 12:35:57 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 3.1199
2022-03-08 12:36:30 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 3.1633
2022-03-08 12:37:03 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 3.0768
2022-03-08 12:37:37 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 3.1801
2022-03-08 12:38:10 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.9216
2022-03-08 12:38:43 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.9259
2022-03-08 12:39:16 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 3.2133
2022-03-08 12:39:50 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 3.0370
2022-03-08 12:40:23 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.9988
2022-03-08 12:40:58 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.9709
2022-03-08 12:41:31 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 3.0117
2022-03-08 12:42:04 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.9467
2022-03-08 12:42:06 - train: epoch 028, train_loss: 3.0529
2022-03-08 12:43:21 - eval: epoch: 028, acc1: 36.144%, acc5: 62.934%, test_loss: 2.9041, per_image_load_time: 2.660ms, per_image_inference_time: 0.238ms
2022-03-08 12:43:21 - until epoch: 028, best_acc1: 37.496%
2022-03-08 12:43:21 - epoch 029 lr: 0.1
2022-03-08 12:44:00 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 3.1108
2022-03-08 12:44:34 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 3.2607
2022-03-08 12:45:07 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 3.1062
2022-03-08 12:45:40 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 3.0608
2022-03-08 12:46:12 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 3.0749
2022-03-08 12:46:46 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 3.3653
2022-03-08 12:47:19 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.6888
2022-03-08 12:47:52 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 3.2139
2022-03-08 12:48:26 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.9239
2022-03-08 12:48:59 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.9452
2022-03-08 12:49:33 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.9868
2022-03-08 12:50:06 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 3.2105
2022-03-08 12:50:39 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 3.0245
2022-03-08 12:51:13 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 3.1948
2022-03-08 12:51:47 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 3.0531
2022-03-08 12:52:20 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 3.0483
2022-03-08 12:52:53 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.9689
2022-03-08 12:53:26 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 3.2578
2022-03-08 12:53:59 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.7765
2022-03-08 12:54:32 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 3.1666
2022-03-08 12:55:06 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 3.0647
2022-03-08 12:55:39 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 3.1657
2022-03-08 12:56:12 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 3.0566
2022-03-08 12:56:46 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.9740
2022-03-08 12:57:19 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.9876
2022-03-08 12:57:52 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.9866
2022-03-08 12:58:25 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.9861
2022-03-08 12:58:58 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.9819
2022-03-08 12:59:31 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.9665
2022-03-08 13:00:04 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 3.0372
2022-03-08 13:00:37 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 3.0921
2022-03-08 13:01:10 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 3.0486
2022-03-08 13:01:43 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.7956
2022-03-08 13:02:16 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 3.0410
2022-03-08 13:02:49 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 3.1799
2022-03-08 13:03:22 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.8905
2022-03-08 13:03:56 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 3.0394
2022-03-08 13:04:29 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 3.0178
2022-03-08 13:05:02 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.8336
2022-03-08 13:05:35 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.9876
2022-03-08 13:06:08 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 3.0936
2022-03-08 13:06:41 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.9885
2022-03-08 13:07:13 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 3.0612
2022-03-08 13:07:47 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 3.1051
2022-03-08 13:08:20 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 3.1019
2022-03-08 13:08:54 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 3.1322
2022-03-08 13:09:27 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.9884
2022-03-08 13:09:59 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 3.0385
2022-03-08 13:10:34 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 3.5004
2022-03-08 13:11:06 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.6546
2022-03-08 13:11:08 - train: epoch 029, train_loss: 3.0499
2022-03-08 13:12:23 - eval: epoch: 029, acc1: 36.628%, acc5: 62.980%, test_loss: 2.9041, per_image_load_time: 2.664ms, per_image_inference_time: 0.236ms
2022-03-08 13:12:23 - until epoch: 029, best_acc1: 37.496%
2022-03-08 13:12:23 - epoch 030 lr: 0.1
2022-03-08 13:13:03 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 3.1297
2022-03-08 13:13:36 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 3.1145
2022-03-08 13:14:09 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 3.0236
2022-03-08 13:14:42 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.8020
2022-03-08 13:15:14 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 3.3741
2022-03-08 13:15:48 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.8416
2022-03-08 13:16:21 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 3.1605
2022-03-08 13:16:54 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 3.1988
2022-03-08 13:17:26 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.8709
2022-03-08 13:18:00 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.8239
2022-03-08 13:18:33 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.8665
2022-03-08 13:19:06 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 3.0529
2022-03-08 13:19:40 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.7933
2022-03-08 13:20:13 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 3.0006
2022-03-08 13:20:46 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.9031
2022-03-08 13:21:19 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 3.0801
2022-03-08 13:21:52 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 3.1520
2022-03-08 13:22:25 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 3.0410
2022-03-08 13:22:58 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 3.1281
2022-03-08 13:23:31 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 3.0441
2022-03-08 13:24:04 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 3.2041
2022-03-08 13:24:37 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.9947
2022-03-08 13:25:10 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 3.0564
2022-03-08 13:25:43 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 3.3021
2022-03-08 13:26:16 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 3.0861
2022-03-08 13:26:49 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 3.1107
2022-03-08 13:27:22 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 3.0259
2022-03-08 13:27:56 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 3.0915
2022-03-08 13:28:29 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.9482
2022-03-08 13:29:02 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 3.2835
2022-03-08 13:29:35 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 3.1318
2022-03-08 13:30:08 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.8349
2022-03-08 13:30:42 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 3.3473
2022-03-08 13:31:15 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 3.0066
2022-03-08 13:31:48 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 3.1262
2022-03-08 13:32:22 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.8901
2022-03-08 13:32:56 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.9254
2022-03-08 13:33:30 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.9902
2022-03-08 13:34:04 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 3.0807
2022-03-08 13:34:37 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.9627
2022-03-08 13:35:11 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.9111
2022-03-08 13:35:45 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 3.3552
2022-03-08 13:36:19 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 3.0274
2022-03-08 13:36:53 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 3.0066
2022-03-08 13:37:27 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 3.3100
2022-03-08 13:38:00 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.7605
2022-03-08 13:38:34 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 3.0385
2022-03-08 13:39:08 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 3.1415
2022-03-08 13:39:42 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 3.1473
2022-03-08 13:40:15 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 3.2070
2022-03-08 13:40:17 - train: epoch 030, train_loss: 3.0478
2022-03-08 13:41:34 - eval: epoch: 030, acc1: 33.610%, acc5: 59.952%, test_loss: 3.0641, per_image_load_time: 2.747ms, per_image_inference_time: 0.217ms
2022-03-08 13:41:34 - until epoch: 030, best_acc1: 37.496%
2022-03-08 13:41:34 - epoch 031 lr: 0.010000000000000002
2022-03-08 13:42:14 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.8561
2022-03-08 13:42:48 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.7322
2022-03-08 13:43:21 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 2.5664
2022-03-08 13:43:55 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.6558
2022-03-08 13:44:28 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 2.5048
2022-03-08 13:45:02 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 2.5909
2022-03-08 13:45:36 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 2.6088
2022-03-08 13:46:09 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 2.5019
2022-03-08 13:46:43 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 2.6174
2022-03-08 13:47:17 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.9472
2022-03-08 13:47:50 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.8236
2022-03-08 13:48:24 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 2.7428
2022-03-08 13:48:57 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 2.4251
2022-03-08 13:49:31 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 2.5867
2022-03-08 13:50:04 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.7572
2022-03-08 13:50:38 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 2.5414
2022-03-08 13:51:11 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 2.3999
2022-03-08 13:51:44 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 2.6002
2022-03-08 13:52:18 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 2.6141
2022-03-08 13:52:51 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 2.6386
2022-03-08 13:53:24 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 2.3464
2022-03-08 13:53:58 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 2.4665
2022-03-08 13:54:31 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 2.4506
2022-03-08 13:55:05 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 2.6670
2022-03-08 13:55:38 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 2.4954
2022-03-08 13:56:11 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 2.5948
2022-03-08 13:56:45 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 2.5769
2022-03-08 13:57:18 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.8329
2022-03-08 13:57:52 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 2.5449
2022-03-08 13:58:25 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.7782
2022-03-08 13:58:58 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 2.3484
2022-03-08 13:59:30 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 2.6266
2022-03-08 14:00:03 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 2.5209
2022-03-08 14:00:36 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.6268
2022-03-08 14:01:10 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 2.5606
2022-03-08 14:01:43 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 2.6097
2022-03-08 14:02:17 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 2.5400
2022-03-08 14:02:50 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 2.3868
2022-03-08 14:03:24 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 2.5788
2022-03-08 14:03:57 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 2.5246
2022-03-08 14:04:30 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 2.5202
2022-03-08 14:05:04 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 2.6280
2022-03-08 14:05:37 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 2.5405
2022-03-08 14:06:11 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 2.7372
2022-03-08 14:06:44 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 2.5921
2022-03-08 14:07:18 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 2.5790
2022-03-08 14:07:52 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 2.5690
2022-03-08 14:08:25 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 2.3748
2022-03-08 14:08:59 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 2.2003
2022-03-08 14:09:31 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 2.4501
2022-03-08 14:09:33 - train: epoch 031, train_loss: 2.5766
2022-03-08 14:10:50 - eval: epoch: 031, acc1: 50.268%, acc5: 75.336%, test_loss: 2.1602, per_image_load_time: 2.701ms, per_image_inference_time: 0.245ms
2022-03-08 14:10:50 - until epoch: 031, best_acc1: 50.268%
2022-03-08 14:10:50 - epoch 032 lr: 0.010000000000000002
2022-03-08 14:11:29 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 2.3873
2022-03-08 14:12:02 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 2.5667
2022-03-08 14:12:36 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 2.4491
2022-03-08 14:13:09 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 2.5642
2022-03-08 14:13:42 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 2.6218
2022-03-08 14:14:15 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 2.7190
2022-03-08 14:14:48 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 2.5396
2022-03-08 14:15:20 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 2.5495
2022-03-08 14:15:53 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 2.4395
2022-03-08 14:16:26 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 2.4631
2022-03-08 14:16:59 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 2.4489
2022-03-08 14:17:32 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 2.5668
2022-03-08 14:18:05 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 2.5483
2022-03-08 14:18:38 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 2.7033
2022-03-08 14:19:10 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 2.6558
2022-03-08 14:19:43 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 2.6684
2022-03-08 14:20:17 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 2.6883
2022-03-08 14:20:50 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.8098
2022-03-08 14:21:22 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 2.3236
2022-03-08 14:21:55 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 2.4324
2022-03-08 14:22:29 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 2.3868
2022-03-08 14:23:02 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 2.4299
2022-03-08 14:23:35 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 2.5514
2022-03-08 14:24:08 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 2.5833
2022-03-08 14:24:42 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 2.6573
2022-03-08 14:25:15 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 2.2418
2022-03-08 14:25:48 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 2.6366
2022-03-08 14:26:21 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 2.5922
2022-03-08 14:26:55 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 2.4102
2022-03-08 14:27:28 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 2.2855
2022-03-08 14:28:01 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 2.6571
2022-03-08 14:28:34 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 2.5549
2022-03-08 14:29:08 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 2.2819
2022-03-08 14:29:41 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 2.2297
2022-03-08 14:30:15 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 2.5295
2022-03-08 14:30:49 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 2.5566
2022-03-08 14:31:23 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 2.5747
2022-03-08 14:31:56 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 2.2265
2022-03-08 14:32:30 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 2.5561
2022-03-08 14:33:04 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 2.3870
2022-03-08 14:33:37 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 2.3710
2022-03-08 14:34:11 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 2.3916
2022-03-08 14:34:45 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 2.6975
2022-03-08 14:35:18 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 2.4639
2022-03-08 14:35:52 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 2.6361
2022-03-08 14:36:26 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 2.4906
2022-03-08 14:37:00 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 2.6849
2022-03-08 14:37:34 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 2.3491
2022-03-08 14:38:08 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 2.5986
2022-03-08 14:38:41 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 2.4716
2022-03-08 14:38:43 - train: epoch 032, train_loss: 2.4848
2022-03-08 14:40:00 - eval: epoch: 032, acc1: 50.758%, acc5: 75.676%, test_loss: 2.1296, per_image_load_time: 2.680ms, per_image_inference_time: 0.246ms
2022-03-08 14:40:00 - until epoch: 032, best_acc1: 50.758%
2022-03-08 14:40:00 - epoch 033 lr: 0.010000000000000002
2022-03-08 14:40:41 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 2.2443
2022-03-08 14:41:14 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 2.4435
2022-03-08 14:41:46 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 2.3414
2022-03-08 14:42:19 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 2.2929
2022-03-08 14:42:52 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 2.5651
2022-03-08 14:43:25 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 2.3728
2022-03-08 14:43:58 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 2.6253
2022-03-08 14:44:31 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 2.6390
2022-03-08 14:45:04 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 2.4052
2022-03-08 14:45:37 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 2.5232
2022-03-08 14:46:10 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 2.2156
2022-03-08 14:46:44 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 2.5334
2022-03-08 14:47:17 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 2.4064
2022-03-08 14:47:51 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 2.5507
2022-03-08 14:48:24 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 2.7204
2022-03-08 14:48:57 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 2.7008
2022-03-08 14:49:31 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 2.3024
2022-03-08 14:50:04 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 2.6429
2022-03-08 14:50:38 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 2.5028
2022-03-08 14:51:11 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 2.2594
2022-03-08 14:51:44 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 2.4445
2022-03-08 14:52:18 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 2.5467
2022-03-08 14:52:51 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 2.3931
2022-03-08 14:53:24 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 2.5666
2022-03-08 14:53:58 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 2.4957
2022-03-08 14:54:31 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 2.2434
2022-03-08 14:55:04 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 2.6264
2022-03-08 14:55:37 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 2.4696
2022-03-08 14:56:10 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 2.6282
2022-03-08 14:56:44 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 2.4865
2022-03-08 14:57:17 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 2.4602
2022-03-08 14:57:51 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 2.5361
2022-03-08 14:58:24 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 2.4584
2022-03-08 14:58:58 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 2.3439
2022-03-08 14:59:31 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 2.5477
2022-03-08 15:00:04 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 2.6155
2022-03-08 15:00:37 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 2.3498
2022-03-08 15:01:10 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 2.4057
2022-03-08 15:01:43 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 2.5679
2022-03-08 15:02:17 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 2.5701
2022-03-08 15:02:50 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 2.4691
2022-03-08 15:03:24 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 2.4451
2022-03-08 15:03:57 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 2.4514
2022-03-08 15:04:30 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 2.5662
2022-03-08 15:05:03 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 2.7593
2022-03-08 15:05:36 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 2.3874
2022-03-08 15:06:10 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 2.4358
2022-03-08 15:06:43 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.9377
2022-03-08 15:07:15 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 2.3227
2022-03-08 15:07:49 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 2.4567
2022-03-08 15:07:52 - train: epoch 033, train_loss: 2.4525
2022-03-08 15:09:09 - eval: epoch: 033, acc1: 51.474%, acc5: 75.994%, test_loss: 2.1069, per_image_load_time: 2.711ms, per_image_inference_time: 0.250ms
2022-03-08 15:09:09 - until epoch: 033, best_acc1: 51.474%
2022-03-08 15:09:09 - epoch 034 lr: 0.010000000000000002
2022-03-08 15:09:48 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 2.4501
2022-03-08 15:10:22 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 2.3273
2022-03-08 15:10:55 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 2.3054
2022-03-08 15:11:28 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 2.2253
2022-03-08 15:12:02 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 2.4573
2022-03-08 15:12:36 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 2.5940
2022-03-08 15:13:09 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 2.1956
2022-03-08 15:13:42 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 2.4399
2022-03-08 15:14:16 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 2.3494
2022-03-08 15:14:49 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 2.3013
2022-03-08 15:15:22 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 2.5679
2022-03-08 15:15:55 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 2.4383
2022-03-08 15:16:27 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 2.3469
2022-03-08 15:17:00 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 2.4114
2022-03-08 15:17:33 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 2.3741
2022-03-08 15:18:07 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 2.3301
2022-03-08 15:18:40 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 2.4238
2022-03-08 15:19:14 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 2.6652
2022-03-08 15:19:47 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 2.6806
2022-03-08 15:20:20 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 2.4152
2022-03-08 15:20:54 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 2.6117
2022-03-08 15:21:27 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 2.2842
2022-03-08 15:22:00 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 2.4548
2022-03-08 15:22:33 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 2.3673
2022-03-08 15:23:07 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 2.3904
2022-03-08 15:23:40 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 2.5484
2022-03-08 15:24:13 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 2.4335
2022-03-08 15:24:46 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 2.4166
2022-03-08 15:25:20 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 2.2117
2022-03-08 15:25:53 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 2.2662
2022-03-08 15:26:27 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 2.3977
2022-03-08 15:27:00 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 2.4314
2022-03-08 15:27:34 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 2.3870
2022-03-08 15:28:08 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 2.5561
2022-03-08 15:28:41 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 2.4005
2022-03-08 15:29:14 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 2.2251
2022-03-08 15:29:48 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 2.2592
2022-03-08 15:30:21 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 2.3698
2022-03-08 15:30:54 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 2.5680
2022-03-08 15:31:28 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 2.2459
2022-03-08 15:32:01 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 2.4133
2022-03-08 15:32:35 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 2.3993
2022-03-08 15:33:09 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 2.3334
2022-03-08 15:33:43 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 2.4969
2022-03-08 15:34:16 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 2.5324
2022-03-08 15:34:49 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 2.5520
2022-03-08 15:35:22 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 2.5754
2022-03-08 15:35:55 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 2.4572
2022-03-08 15:36:28 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 2.4954
2022-03-08 15:37:01 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 2.3232
2022-03-08 15:37:04 - train: epoch 034, train_loss: 2.4378
2022-03-08 15:38:19 - eval: epoch: 034, acc1: 51.466%, acc5: 76.262%, test_loss: 2.1006, per_image_load_time: 2.661ms, per_image_inference_time: 0.242ms
2022-03-08 15:38:19 - until epoch: 034, best_acc1: 51.474%
2022-03-08 15:38:19 - epoch 035 lr: 0.010000000000000002
2022-03-08 15:38:58 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 2.2957
2022-03-08 15:39:32 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 2.1451
2022-03-08 15:40:05 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 2.4388
2022-03-08 15:40:38 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 2.3485
2022-03-08 15:41:12 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 2.4675
2022-03-08 15:41:45 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 2.4306
2022-03-08 15:42:18 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 2.4239
2022-03-08 15:42:51 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 2.5469
2022-03-08 15:43:24 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 2.4824
2022-03-08 15:43:57 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 2.5989
2022-03-08 15:44:31 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 2.5309
2022-03-08 15:45:04 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 2.4626
2022-03-08 15:45:37 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 2.6374
2022-03-08 15:46:10 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 2.4607
2022-03-08 15:46:43 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 2.4285
2022-03-08 15:47:16 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 2.4386
2022-03-08 15:47:49 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 2.1692
2022-03-08 15:48:22 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 2.4710
2022-03-08 15:48:56 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 2.4802
2022-03-08 15:49:29 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 2.5160
2022-03-08 15:50:02 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 2.1848
2022-03-08 15:50:35 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 2.5048
2022-03-08 15:51:08 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 2.4329
2022-03-08 15:51:42 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 2.5527
2022-03-08 15:52:15 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 2.3437
2022-03-08 15:52:49 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 2.4704
2022-03-08 15:53:22 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 2.5589
2022-03-08 15:53:56 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 2.4845
2022-03-08 15:54:29 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 2.3528
2022-03-08 15:55:02 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 2.4889
2022-03-08 15:55:36 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 2.5153
2022-03-08 15:56:09 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 2.1990
2022-03-08 15:56:42 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 2.3839
2022-03-08 15:57:16 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 2.5834
2022-03-08 15:57:49 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 2.0585
2022-03-08 15:58:22 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 2.4630
2022-03-08 15:58:55 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 2.3148
2022-03-08 15:59:28 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 2.3120
2022-03-08 16:00:01 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 2.5401
2022-03-08 16:00:34 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 2.1502
2022-03-08 16:01:08 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 2.7417
2022-03-08 16:01:41 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 2.2509
2022-03-08 16:02:14 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 2.4291
2022-03-08 16:02:48 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 2.4398
2022-03-08 16:03:21 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 2.5845
2022-03-08 16:03:55 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 2.2509
2022-03-08 16:04:28 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 2.8094
2022-03-08 16:05:01 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 2.6732
2022-03-08 16:05:34 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 2.5276
2022-03-08 16:06:07 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 2.3681
2022-03-08 16:06:09 - train: epoch 035, train_loss: 2.4285
2022-03-08 16:07:25 - eval: epoch: 035, acc1: 51.398%, acc5: 76.332%, test_loss: 2.0983, per_image_load_time: 2.592ms, per_image_inference_time: 0.230ms
2022-03-08 16:07:25 - until epoch: 035, best_acc1: 51.474%
2022-03-08 16:07:25 - epoch 036 lr: 0.010000000000000002
2022-03-08 16:08:05 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 2.5426
2022-03-08 16:08:38 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 2.2278
2022-03-08 16:09:11 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 2.4009
2022-03-08 16:09:44 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 2.3147
2022-03-08 16:10:17 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 2.4773
2022-03-08 16:10:50 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 2.3627
2022-03-08 16:11:24 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 2.1635
2022-03-08 16:11:58 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 2.2455
2022-03-08 16:12:31 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 2.3831
2022-03-08 16:13:04 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 2.4925
2022-03-08 16:13:37 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 2.6331
2022-03-08 16:14:11 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 2.4028
2022-03-08 16:14:44 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 2.4483
2022-03-08 16:15:18 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 2.4903
2022-03-08 16:15:51 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 2.5351
2022-03-08 16:16:25 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 2.6620
2022-03-08 16:16:58 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 2.4514
2022-03-08 16:17:31 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 2.2944
2022-03-08 16:18:04 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 2.4710
2022-03-08 16:18:37 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 2.2450
2022-03-08 16:19:11 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 2.4271
2022-03-08 16:19:44 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 2.3330
2022-03-08 16:20:17 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 2.5157
2022-03-08 16:20:50 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 2.5593
2022-03-08 16:21:23 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 2.2021
2022-03-08 16:21:57 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 2.4773
2022-03-08 16:22:30 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 2.2139
2022-03-08 16:23:03 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 2.3732
2022-03-08 16:23:36 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 2.2173
2022-03-08 16:24:09 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 2.6567
2022-03-08 16:24:42 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 2.5392
2022-03-08 16:25:15 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 2.5234
2022-03-08 16:25:48 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 2.3388
2022-03-08 16:26:21 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 2.3518
2022-03-08 16:26:54 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 2.6022
2022-03-08 16:27:28 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 2.5828
2022-03-08 16:28:01 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 2.3269
2022-03-08 16:28:35 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 2.5372
2022-03-08 16:29:08 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 2.3643
2022-03-08 16:29:41 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 2.2943
2022-03-08 16:30:15 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 2.3383
2022-03-08 16:30:48 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 2.3717
2022-03-08 16:31:21 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 2.4597
2022-03-08 16:31:55 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 2.4631
2022-03-08 16:32:28 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 2.1840
2022-03-08 16:33:02 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 2.2681
2022-03-08 16:33:35 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 2.5235
2022-03-08 16:34:09 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 2.4279
2022-03-08 16:34:42 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 2.3969
2022-03-08 16:35:14 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 2.5068
2022-03-08 16:35:16 - train: epoch 036, train_loss: 2.4227
2022-03-08 16:36:32 - eval: epoch: 036, acc1: 51.776%, acc5: 76.302%, test_loss: 2.0922, per_image_load_time: 2.687ms, per_image_inference_time: 0.226ms
2022-03-08 16:36:32 - until epoch: 036, best_acc1: 51.776%
2022-03-08 16:36:32 - epoch 037 lr: 0.010000000000000002
2022-03-08 16:37:12 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 2.2586
2022-03-08 16:37:45 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 2.0933
2022-03-08 16:38:18 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 2.3764
2022-03-08 16:38:52 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 2.4643
2022-03-08 16:39:24 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 2.3107
2022-03-08 16:39:56 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 2.5095
2022-03-08 16:40:29 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 2.5912
2022-03-08 16:41:02 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 2.4950
2022-03-08 16:41:35 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.8291
2022-03-08 16:42:08 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 2.1488
2022-03-08 16:42:41 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 2.5168
2022-03-08 16:43:14 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 2.4472
2022-03-08 16:43:47 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 2.5437
2022-03-08 16:44:20 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 2.6128
2022-03-08 16:44:53 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 2.3235
2022-03-08 16:45:26 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 2.1490
2022-03-08 16:46:00 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 2.5717
2022-03-08 16:46:33 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 2.5539
2022-03-08 16:47:06 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 2.3621
2022-03-08 16:47:39 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 2.5906
2022-03-08 16:48:12 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 2.3045
2022-03-08 16:48:46 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 2.3991
2022-03-08 16:49:19 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 2.3274
2022-03-08 16:49:51 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 2.4426
2022-03-08 16:50:25 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 2.4109
2022-03-08 16:50:58 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 2.5870
2022-03-08 16:51:31 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 2.5747
2022-03-08 16:52:05 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 2.3716
2022-03-08 16:52:38 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 2.3820
2022-03-08 16:53:11 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 2.5218
2022-03-08 16:53:45 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 2.3401
2022-03-08 16:54:18 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 2.4859
2022-03-08 16:54:51 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 2.3575
2022-03-08 16:55:24 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 2.2109
2022-03-08 16:55:58 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 2.4730
2022-03-08 16:56:31 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 2.5966
2022-03-08 16:57:04 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 2.3560
2022-03-08 16:57:37 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 2.3535
2022-03-08 16:58:10 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 2.5027
2022-03-08 16:58:43 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 2.4013
2022-03-08 16:59:16 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 2.4232
2022-03-08 16:59:49 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 2.6082
2022-03-08 17:00:23 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 2.4268
2022-03-08 17:00:56 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 2.4457
2022-03-08 17:01:29 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 2.2956
2022-03-08 17:02:03 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 2.3947
2022-03-08 17:02:36 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 2.3884
2022-03-08 17:03:10 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 2.3460
2022-03-08 17:03:43 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 2.3844
2022-03-08 17:04:15 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 2.3926
2022-03-08 17:04:18 - train: epoch 037, train_loss: 2.4224
2022-03-08 17:05:34 - eval: epoch: 037, acc1: 51.588%, acc5: 76.348%, test_loss: 2.1005, per_image_load_time: 2.622ms, per_image_inference_time: 0.254ms
2022-03-08 17:05:34 - until epoch: 037, best_acc1: 51.776%
2022-03-08 17:05:34 - epoch 038 lr: 0.010000000000000002
2022-03-08 17:06:13 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 2.3926
2022-03-08 17:06:47 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 2.2643
2022-03-08 17:07:20 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 2.1514
2022-03-08 17:07:53 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 2.3468
2022-03-08 17:08:26 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 2.2939
2022-03-08 17:08:59 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 2.7175
2022-03-08 17:09:32 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 2.2584
2022-03-08 17:10:05 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 2.2817
2022-03-08 17:10:39 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 2.4264
2022-03-08 17:11:12 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 2.6120
2022-03-08 17:11:45 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 2.3686
2022-03-08 17:12:18 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 2.5410
2022-03-08 17:12:51 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 2.6159
2022-03-08 17:13:25 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 2.4708
2022-03-08 17:13:58 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 2.4151
2022-03-08 17:14:31 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 2.4840
2022-03-08 17:15:04 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 2.5291
2022-03-08 17:15:37 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 2.4555
2022-03-08 17:16:11 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 2.4506
2022-03-08 17:16:44 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 2.3118
2022-03-08 17:17:17 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 2.3807
2022-03-08 17:17:50 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 2.3045
2022-03-08 17:18:24 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 2.6112
2022-03-08 17:18:57 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 2.7172
2022-03-08 17:19:31 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 2.4220
2022-03-08 17:20:04 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 2.4518
2022-03-08 17:20:37 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 2.4893
2022-03-08 17:21:11 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 2.6137
2022-03-08 17:21:44 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 2.4110
2022-03-08 17:22:17 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 2.3121
2022-03-08 17:22:51 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 2.4045
2022-03-08 17:23:24 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 2.1572
2022-03-08 17:23:58 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 2.2319
2022-03-08 17:24:31 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 2.3564
2022-03-08 17:25:04 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 2.3439
2022-03-08 17:25:38 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 2.5486
2022-03-08 17:26:11 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 2.2484
2022-03-08 17:26:44 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 2.4321
2022-03-08 17:27:18 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 2.3743
2022-03-08 17:27:51 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 2.5754
2022-03-08 17:28:24 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 2.2778
2022-03-08 17:28:58 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 2.1240
2022-03-08 17:29:31 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 2.5068
2022-03-08 17:30:04 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 2.5289
2022-03-08 17:30:37 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 2.4854
2022-03-08 17:31:11 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 2.4344
2022-03-08 17:31:44 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 2.2703
2022-03-08 17:32:17 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 2.5307
2022-03-08 17:32:50 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 2.3390
2022-03-08 17:33:22 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 2.2725
2022-03-08 17:33:24 - train: epoch 038, train_loss: 2.4217
2022-03-08 17:34:41 - eval: epoch: 038, acc1: 51.316%, acc5: 76.214%, test_loss: 2.1076, per_image_load_time: 2.691ms, per_image_inference_time: 0.214ms
2022-03-08 17:34:41 - until epoch: 038, best_acc1: 51.776%
2022-03-08 17:34:41 - epoch 039 lr: 0.010000000000000002
2022-03-08 17:35:20 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 2.4948
2022-03-08 17:35:54 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 2.6005
2022-03-08 17:36:28 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 2.1649
2022-03-08 17:37:01 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 2.5179
2022-03-08 17:37:34 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 2.4477
2022-03-08 17:38:08 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 2.4154
2022-03-08 17:38:41 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 2.6019
2022-03-08 17:39:14 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 2.4986
2022-03-08 17:39:47 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 2.4543
2022-03-08 17:40:20 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 2.2185
2022-03-08 17:40:54 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 2.5226
2022-03-08 17:41:27 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 2.4316
2022-03-08 17:42:01 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 2.6945
2022-03-08 17:42:34 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 2.4539
2022-03-08 17:43:07 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 2.3682
2022-03-08 17:43:40 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 2.3900
2022-03-08 17:44:14 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 2.1386
2022-03-08 17:44:47 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 2.3754
2022-03-08 17:45:20 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 2.3115
2022-03-08 17:45:53 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 2.4155
2022-03-08 17:46:27 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 2.5750
2022-03-08 17:46:59 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 2.3729
2022-03-08 17:47:33 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 2.8235
2022-03-08 17:48:06 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 2.5383
2022-03-08 17:48:39 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 2.3605
2022-03-08 17:49:12 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 2.4719
2022-03-08 17:49:45 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 2.5353
2022-03-08 17:50:18 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 2.3882
2022-03-08 17:50:51 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 2.1029
2022-03-08 17:51:25 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 2.4661
2022-03-08 17:51:58 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 2.4449
2022-03-08 17:52:31 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 2.4553
2022-03-08 17:53:04 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 2.4444
2022-03-08 17:53:37 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 2.5307
2022-03-08 17:54:10 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 2.6957
2022-03-08 17:54:44 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 2.5330
2022-03-08 17:55:17 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 2.3392
2022-03-08 17:55:50 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 2.3471
2022-03-08 17:56:23 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 2.5850
2022-03-08 17:56:57 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 2.4366
2022-03-08 17:57:30 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 2.5514
2022-03-08 17:58:04 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 2.4732
2022-03-08 17:58:37 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 2.4701
2022-03-08 17:59:10 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 2.1100
2022-03-08 17:59:44 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 2.1719
2022-03-08 18:00:17 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 2.6896
2022-03-08 18:00:49 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 2.4902
2022-03-08 18:01:23 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 2.4185
2022-03-08 18:01:56 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 2.3647
2022-03-08 18:02:29 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 2.2551
2022-03-08 18:02:31 - train: epoch 039, train_loss: 2.4214
2022-03-08 18:03:47 - eval: epoch: 039, acc1: 50.862%, acc5: 76.052%, test_loss: 2.1214, per_image_load_time: 2.513ms, per_image_inference_time: 0.263ms
2022-03-08 18:03:47 - until epoch: 039, best_acc1: 51.776%
2022-03-08 18:03:47 - epoch 040 lr: 0.010000000000000002
2022-03-08 18:04:27 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 2.6829
2022-03-08 18:05:00 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 2.6727
2022-03-08 18:05:33 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 2.5700
2022-03-08 18:06:06 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 2.4132
2022-03-08 18:06:39 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 2.2277
2022-03-08 18:07:12 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 2.4974
2022-03-08 18:07:46 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 2.5067
2022-03-08 18:08:19 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 2.5791
2022-03-08 18:08:52 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 2.2687
2022-03-08 18:09:25 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 2.2717
2022-03-08 18:09:58 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 2.4048
2022-03-08 18:10:31 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 2.4166
2022-03-08 18:11:04 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 2.2828
2022-03-08 18:11:37 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 2.3136
2022-03-08 18:12:10 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 2.4821
2022-03-08 18:12:43 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 2.4969
2022-03-08 18:13:17 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 2.5115
2022-03-08 18:13:50 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 2.2467
2022-03-08 18:14:24 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 2.3551
2022-03-08 18:14:57 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 2.5087
2022-03-08 18:15:30 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 2.4258
2022-03-08 18:16:03 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 2.1889
2022-03-08 18:16:37 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 2.3885
2022-03-08 18:17:10 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 2.4651
2022-03-08 18:17:44 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 2.4761
2022-03-08 18:18:17 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 2.2990
2022-03-08 18:18:51 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 2.5185
2022-03-08 18:19:24 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 2.3441
2022-03-08 18:19:57 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 2.7634
2022-03-08 18:20:30 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 2.4897
2022-03-08 18:21:03 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 2.4489
2022-03-08 18:21:36 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 2.5579
2022-03-08 18:22:09 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 2.4729
2022-03-08 18:22:42 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 2.3748
2022-03-08 18:23:15 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 2.4477
2022-03-08 18:23:48 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 2.4229
2022-03-08 18:24:22 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 2.4731
2022-03-08 18:24:55 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 2.2091
2022-03-08 18:25:28 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 2.4858
2022-03-08 18:26:02 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 2.5790
2022-03-08 18:26:35 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 2.3854
2022-03-08 18:27:08 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 2.2924
2022-03-08 18:27:41 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 2.4356
2022-03-08 18:28:14 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 2.5151
2022-03-08 18:28:48 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 2.1091
2022-03-08 18:29:21 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 2.3637
2022-03-08 18:29:55 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 2.3788
2022-03-08 18:30:28 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 2.4276
2022-03-08 18:31:02 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 2.5250
2022-03-08 18:31:34 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 2.3796
2022-03-08 18:31:37 - train: epoch 040, train_loss: 2.4215
2022-03-08 18:32:53 - eval: epoch: 040, acc1: 51.326%, acc5: 76.056%, test_loss: 2.1176, per_image_load_time: 2.063ms, per_image_inference_time: 0.259ms
2022-03-08 18:32:53 - until epoch: 040, best_acc1: 51.776%
2022-03-08 18:32:53 - epoch 041 lr: 0.010000000000000002
2022-03-08 18:33:33 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 2.6683
2022-03-08 18:34:07 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 2.6058
2022-03-08 18:34:40 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 2.4608
2022-03-08 18:35:14 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 2.4686
2022-03-08 18:35:47 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 2.1647
2022-03-08 18:36:20 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 2.4734
2022-03-08 18:36:53 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 2.2466
2022-03-08 18:37:26 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 2.1701
2022-03-08 18:37:59 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 2.0899
2022-03-08 18:38:32 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 2.5963
2022-03-08 18:39:05 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 2.3903
2022-03-08 18:39:38 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 2.2028
2022-03-08 18:40:11 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 2.5677
2022-03-08 18:40:44 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 2.5693
2022-03-08 18:41:17 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 2.7588
2022-03-08 18:41:50 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 2.1393
2022-03-08 18:42:23 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 2.3833
2022-03-08 18:42:56 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 2.3810
2022-03-08 18:43:29 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 2.4839
2022-03-08 18:44:02 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 2.3985
2022-03-08 18:44:36 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 2.3117
2022-03-08 18:45:09 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 2.2470
2022-03-08 18:45:42 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 2.1776
2022-03-08 18:46:15 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 2.5286
2022-03-08 18:46:48 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 2.2377
2022-03-08 18:47:22 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 2.5762
2022-03-08 18:47:55 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 2.6221
2022-03-08 18:48:28 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 2.3953
2022-03-08 18:49:01 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 2.5805
2022-03-08 18:49:35 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 2.4634
2022-03-08 18:50:08 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 2.5422
2022-03-08 18:50:41 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 2.3763
2022-03-08 18:51:14 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 2.2098
2022-03-08 18:51:46 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 2.2457
2022-03-08 18:52:19 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 2.4684
2022-03-08 18:52:52 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 2.5561
2022-03-08 18:53:25 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 2.4375
2022-03-08 18:53:58 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 2.2212
2022-03-08 18:54:31 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 2.5076
2022-03-08 18:55:05 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 2.2118
2022-03-08 18:55:38 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 2.3901
2022-03-08 18:56:12 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 2.4391
2022-03-08 18:56:45 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 2.4548
2022-03-08 18:57:18 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 2.3688
2022-03-08 18:57:52 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 2.3901
2022-03-08 18:58:25 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 2.5886
2022-03-08 18:58:58 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 2.4538
2022-03-08 18:59:32 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 2.4974
2022-03-08 19:00:06 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 2.4530
2022-03-08 19:00:38 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 2.5199
2022-03-08 19:00:40 - train: epoch 041, train_loss: 2.4245
2022-03-08 19:01:57 - eval: epoch: 041, acc1: 50.928%, acc5: 75.884%, test_loss: 2.1219, per_image_load_time: 2.661ms, per_image_inference_time: 0.250ms
2022-03-08 19:01:57 - until epoch: 041, best_acc1: 51.776%
2022-03-08 19:01:57 - epoch 042 lr: 0.010000000000000002
2022-03-08 19:02:37 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 2.1993
2022-03-08 19:03:10 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 2.3684
2022-03-08 19:03:43 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 2.4509
2022-03-08 19:04:16 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 2.0262
2022-03-08 19:04:50 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 2.5932
2022-03-08 19:05:23 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 2.2384
2022-03-08 19:05:56 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 2.4960
2022-03-08 19:06:28 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 2.3027
2022-03-08 19:07:01 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 2.6514
2022-03-08 19:07:35 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 2.6522
2022-03-08 19:08:08 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 2.2265
2022-03-08 19:08:41 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 2.3142
2022-03-08 19:09:14 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 2.5033
2022-03-08 19:09:48 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 2.7564
2022-03-08 19:10:21 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 2.3655
2022-03-08 19:10:55 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 2.4699
2022-03-08 19:11:28 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 2.3159
2022-03-08 19:12:01 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 2.4954
2022-03-08 19:12:34 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 2.4521
2022-03-08 19:13:07 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 2.2503
2022-03-08 19:13:40 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 2.2423
2022-03-08 19:14:13 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 2.2428
2022-03-08 19:14:46 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 2.3354
2022-03-08 19:15:20 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 2.6528
2022-03-08 19:15:53 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 2.5955
2022-03-08 19:16:26 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 2.5821
2022-03-08 19:17:00 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 2.3659
2022-03-08 19:17:33 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 2.3312
2022-03-08 19:18:06 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 2.5264
2022-03-08 19:18:40 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 2.4745
2022-03-08 19:19:13 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 2.5137
2022-03-08 19:19:47 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 2.5847
2022-03-08 19:20:20 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 2.5495
2022-03-08 19:20:54 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 2.1281
2022-03-08 19:21:27 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 2.4376
2022-03-08 19:22:00 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 2.6064
2022-03-08 19:22:34 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 2.4059
2022-03-08 19:23:08 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 2.2674
2022-03-08 19:23:41 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 2.4882
2022-03-08 19:24:15 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 2.3397
2022-03-08 19:24:49 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 2.7239
2022-03-08 19:25:23 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 2.4032
2022-03-08 19:25:56 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 2.2590
2022-03-08 19:26:30 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 2.2204
2022-03-08 19:27:04 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 2.3807
2022-03-08 19:27:38 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 2.6246
2022-03-08 19:28:11 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 2.3265
2022-03-08 19:28:45 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 2.4571
2022-03-08 19:29:19 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 2.4040
2022-03-08 19:29:52 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 2.3124
2022-03-08 19:29:55 - train: epoch 042, train_loss: 2.4247
2022-03-08 19:31:12 - eval: epoch: 042, acc1: 51.072%, acc5: 76.000%, test_loss: 2.1197, per_image_load_time: 2.611ms, per_image_inference_time: 0.268ms
2022-03-08 19:31:12 - until epoch: 042, best_acc1: 51.776%
2022-03-08 19:31:12 - epoch 043 lr: 0.010000000000000002
2022-03-08 19:31:51 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 2.4752
2022-03-08 19:32:24 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 2.4702
2022-03-08 19:32:57 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 2.2345
2022-03-08 19:33:30 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 2.5178
2022-03-08 19:34:03 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 2.4267
2022-03-08 19:34:36 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 2.4612
2022-03-08 19:35:10 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 2.5059
2022-03-08 19:35:43 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 2.5887
2022-03-08 19:36:16 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 2.4280
2022-03-08 19:36:50 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 2.5907
2022-03-08 19:37:23 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 2.4350
2022-03-08 19:37:57 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 2.3316
2022-03-08 19:38:30 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 2.4928
2022-03-08 19:39:04 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 2.2358
2022-03-08 19:39:37 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 2.3073
2022-03-08 19:40:11 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 2.4536
2022-03-08 19:40:44 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 2.4870
2022-03-08 19:41:18 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 2.6199
2022-03-08 19:41:51 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 2.4211
2022-03-08 19:42:24 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 2.0961
2022-03-08 19:42:58 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 2.3385
2022-03-08 19:43:31 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 2.4729
2022-03-08 19:44:04 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 2.5484
2022-03-08 19:44:38 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 2.3178
2022-03-08 19:45:11 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 2.6188
2022-03-08 19:45:44 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 2.1866
2022-03-08 19:46:17 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 2.5784
2022-03-08 19:46:51 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 2.0735
2022-03-08 19:47:24 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 2.4609
2022-03-08 19:47:58 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 2.8145
2022-03-08 19:48:31 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 2.6507
2022-03-08 19:49:04 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 2.4604
2022-03-08 19:49:38 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 2.5480
2022-03-08 19:50:11 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 2.5349
2022-03-08 19:50:44 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 2.3968
2022-03-08 19:51:17 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 2.5082
2022-03-08 19:51:50 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 2.3961
2022-03-08 19:52:24 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 2.5176
2022-03-08 19:52:58 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 2.5196
2022-03-08 19:53:31 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 2.4450
2022-03-08 19:54:04 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 2.7503
2022-03-08 19:54:37 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 2.4673
2022-03-08 19:55:11 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 2.4446
2022-03-08 19:55:44 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 2.5027
2022-03-08 19:56:17 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 2.3251
2022-03-08 19:56:51 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 2.3134
2022-03-08 19:57:24 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 2.4923
2022-03-08 19:57:58 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 2.4423
2022-03-08 19:58:32 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 2.3130
2022-03-08 19:59:04 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 2.3724
2022-03-08 19:59:07 - train: epoch 043, train_loss: 2.4240
2022-03-08 20:00:23 - eval: epoch: 043, acc1: 51.408%, acc5: 76.572%, test_loss: 2.0961, per_image_load_time: 1.743ms, per_image_inference_time: 0.241ms
2022-03-08 20:00:23 - until epoch: 043, best_acc1: 51.776%
2022-03-08 20:00:23 - epoch 044 lr: 0.010000000000000002
2022-03-08 20:01:03 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 2.5470
2022-03-08 20:01:36 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 2.6002
2022-03-08 20:02:09 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 2.3239
2022-03-08 20:02:43 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 2.1270
2022-03-08 20:03:16 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 2.3466
2022-03-08 20:03:48 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 2.1074
2022-03-08 20:04:21 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 2.1703
2022-03-08 20:04:54 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 2.5291
2022-03-08 20:05:27 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 2.4852
2022-03-08 20:06:01 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 2.5261
2022-03-08 20:06:34 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 2.2156
2022-03-08 20:07:07 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 2.3079
2022-03-08 20:07:41 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 2.3339
2022-03-08 20:08:14 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 2.1747
2022-03-08 20:08:48 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 2.2588
2022-03-08 20:09:21 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 2.2400
2022-03-08 20:09:54 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 2.4522
2022-03-08 20:10:28 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 2.3873
2022-03-08 20:11:01 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 2.5599
2022-03-08 20:11:34 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 2.3940
2022-03-08 20:12:08 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 2.5953
2022-03-08 20:12:41 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 2.4336
2022-03-08 20:13:14 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 2.5177
2022-03-08 20:13:48 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 2.5190
2022-03-08 20:14:21 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 2.5153
2022-03-08 20:14:55 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 2.4321
2022-03-08 20:15:28 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 2.5531
2022-03-08 20:16:02 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 2.1790
2022-03-08 20:16:36 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 2.3651
2022-03-08 20:17:09 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 2.3911
2022-03-08 20:17:43 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 2.3933
2022-03-08 20:18:16 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 2.2448
2022-03-08 20:18:50 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 2.4780
2022-03-08 20:19:23 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 2.5522
2022-03-08 20:19:58 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 2.3815
2022-03-08 20:20:31 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 2.4801
2022-03-08 20:21:05 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 2.4215
2022-03-08 20:21:38 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 2.0369
2022-03-08 20:22:12 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 2.5108
2022-03-08 20:22:45 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 2.4835
2022-03-08 20:23:19 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 2.2370
2022-03-08 20:23:53 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 2.5348
2022-03-08 20:24:26 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 2.5607
2022-03-08 20:25:00 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 2.2344
2022-03-08 20:25:33 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 2.7089
2022-03-08 20:26:07 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 2.3188
2022-03-08 20:26:41 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 2.4473
2022-03-08 20:27:14 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 2.6821
2022-03-08 20:27:48 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 2.3374
2022-03-08 20:28:20 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 2.4895
2022-03-08 20:28:23 - train: epoch 044, train_loss: 2.4253
2022-03-08 20:29:39 - eval: epoch: 044, acc1: 51.200%, acc5: 76.086%, test_loss: 2.1156, per_image_load_time: 2.675ms, per_image_inference_time: 0.249ms
2022-03-08 20:29:39 - until epoch: 044, best_acc1: 51.776%
2022-03-08 20:29:39 - epoch 045 lr: 0.010000000000000002
2022-03-08 20:30:18 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 2.3128
2022-03-08 20:30:52 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 2.4244
2022-03-08 20:31:25 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 2.4925
2022-03-08 20:31:58 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 2.2927
2022-03-08 20:32:31 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 2.5832
2022-03-08 20:33:04 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 2.5299
2022-03-08 20:33:38 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.9574
2022-03-08 20:34:11 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 2.4306
2022-03-08 20:34:44 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 2.4740
2022-03-08 20:35:17 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 2.2433
2022-03-08 20:35:51 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 2.4055
2022-03-08 20:36:24 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 2.4392
2022-03-08 20:36:58 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 2.3776
2022-03-08 20:37:31 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 2.5651
2022-03-08 20:38:04 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 2.4884
2022-03-08 20:38:38 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 2.3521
2022-03-08 20:39:11 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 2.2865
2022-03-08 20:39:44 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 2.2762
2022-03-08 20:40:17 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 2.6898
2022-03-08 20:40:51 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 2.6653
2022-03-08 20:41:24 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 2.5490
2022-03-08 20:41:57 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 2.5672
2022-03-08 20:42:31 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 2.1801
2022-03-08 20:43:04 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 2.5526
2022-03-08 20:43:37 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 2.4457
2022-03-08 20:44:10 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 2.3316
2022-03-08 20:44:43 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 2.2891
2022-03-08 20:45:16 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 2.3148
2022-03-08 20:45:49 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 2.5439
2022-03-08 20:46:22 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 2.6660
2022-03-08 20:46:55 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 2.3330
2022-03-08 20:47:29 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 2.4854
2022-03-08 20:48:02 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 2.4088
2022-03-08 20:48:34 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 2.2698
2022-03-08 20:49:08 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 2.6071
2022-03-08 20:49:41 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 2.5247
2022-03-08 20:50:14 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 2.3632
2022-03-08 20:50:48 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 2.3472
2022-03-08 20:51:21 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 2.7103
2022-03-08 20:51:55 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 2.4801
2022-03-08 20:52:28 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 2.3659
2022-03-08 20:53:01 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 2.6043
2022-03-08 20:53:34 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 2.5626
2022-03-08 20:54:07 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 2.6219
2022-03-08 20:54:40 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 2.2019
2022-03-08 20:55:14 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 2.6571
2022-03-08 20:55:48 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 2.3105
2022-03-08 20:56:22 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 2.3353
2022-03-08 20:56:55 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 2.3161
2022-03-08 20:57:28 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 2.3459
2022-03-08 20:57:30 - train: epoch 045, train_loss: 2.4238
2022-03-08 20:58:46 - eval: epoch: 045, acc1: 50.580%, acc5: 75.768%, test_loss: 2.1342, per_image_load_time: 2.627ms, per_image_inference_time: 0.256ms
2022-03-08 20:58:46 - until epoch: 045, best_acc1: 51.776%
2022-03-08 20:58:46 - epoch 046 lr: 0.010000000000000002
2022-03-08 20:59:25 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 2.1277
2022-03-08 20:59:58 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 2.1718
2022-03-08 21:00:31 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 2.3890
2022-03-08 21:01:04 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 2.4859
2022-03-08 21:01:37 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 2.2644
2022-03-08 21:02:10 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 2.5433
2022-03-08 21:02:43 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 2.3107
2022-03-08 21:03:16 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 2.5193
2022-03-08 21:03:49 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 2.4416
2022-03-08 21:04:22 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 2.2320
2022-03-08 21:04:55 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 2.4202
2022-03-08 21:05:29 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 2.3838
2022-03-08 21:06:02 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 2.3922
2022-03-08 21:06:35 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 2.6598
2022-03-08 21:07:08 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 2.4048
2022-03-08 21:07:42 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 2.4819
2022-03-08 21:08:15 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 2.2994
2022-03-08 21:08:48 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 2.4611
2022-03-08 21:09:22 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 2.2302
2022-03-08 21:09:55 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 2.3961
2022-03-08 21:10:28 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 2.5452
2022-03-08 21:11:01 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 2.1883
2022-03-08 21:11:33 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 2.6967
2022-03-08 21:12:06 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 2.4382
2022-03-08 21:12:39 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 2.5446
2022-03-08 21:13:13 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 2.6896
2022-03-08 21:13:46 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 2.3881
2022-03-08 21:14:19 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 2.3078
2022-03-08 21:14:52 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 2.4946
2022-03-08 21:15:25 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 2.1927
2022-03-08 21:15:58 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 2.1636
2022-03-08 21:16:31 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 2.5882
2022-03-08 21:17:04 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 2.5155
2022-03-08 21:17:37 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 2.4179
2022-03-08 21:18:10 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 2.5364
2022-03-08 21:18:43 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 2.3271
2022-03-08 21:19:16 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 2.3565
2022-03-08 21:19:50 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 2.5127
2022-03-08 21:20:23 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 2.2389
2022-03-08 21:20:57 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 2.2504
2022-03-08 21:21:30 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 2.5383
2022-03-08 21:22:03 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 2.3247
2022-03-08 21:22:36 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 2.5114
2022-03-08 21:23:10 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 2.5482
2022-03-08 21:23:44 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 2.3272
2022-03-08 21:24:17 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 2.5236
2022-03-08 21:24:51 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 2.4702
2022-03-08 21:25:24 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 2.3382
2022-03-08 21:25:57 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 2.5554
2022-03-08 21:26:29 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 2.4575
2022-03-08 21:26:31 - train: epoch 046, train_loss: 2.4235
2022-03-08 21:27:47 - eval: epoch: 046, acc1: 50.402%, acc5: 75.550%, test_loss: 2.1534, per_image_load_time: 2.630ms, per_image_inference_time: 0.243ms
2022-03-08 21:27:47 - until epoch: 046, best_acc1: 51.776%
2022-03-08 21:27:47 - epoch 047 lr: 0.010000000000000002
2022-03-08 21:28:26 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 2.3809
2022-03-08 21:28:59 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 2.5157
2022-03-08 21:29:32 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 2.2199
2022-03-08 21:30:05 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 2.1508
2022-03-08 21:30:39 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 2.4154
2022-03-08 21:31:12 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 2.4968
2022-03-08 21:31:45 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 2.5163
2022-03-08 21:32:18 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 2.2681
2022-03-08 21:32:50 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 2.5517
2022-03-08 21:33:23 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 2.5724
2022-03-08 21:33:56 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 2.5225
2022-03-08 21:34:29 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 2.4119
2022-03-08 21:35:02 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 2.5119
2022-03-08 21:35:34 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 2.3407
2022-03-08 21:36:08 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 2.3230
2022-03-08 21:36:41 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 2.4052
2022-03-08 21:37:14 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 2.3183
2022-03-08 21:37:47 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 2.4325
2022-03-08 21:38:20 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 2.3530
2022-03-08 21:38:54 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 2.3490
2022-03-08 21:39:27 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 2.5854
2022-03-08 21:40:00 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 2.5635
2022-03-08 21:40:33 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 2.4702
2022-03-08 21:41:07 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 2.2849
2022-03-08 21:41:40 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 2.2802
2022-03-08 21:42:14 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 2.7155
2022-03-08 21:42:47 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 2.3506
2022-03-08 21:43:21 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 2.6195
2022-03-08 21:43:54 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 2.3030
2022-03-08 21:44:28 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 2.4139
2022-03-08 21:45:01 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 2.3337
2022-03-08 21:45:34 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 2.7895
2022-03-08 21:46:07 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 2.2404
2022-03-08 21:46:40 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 2.4140
2022-03-08 21:47:13 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 2.6343
2022-03-08 21:47:47 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 2.4454
2022-03-08 21:48:20 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 2.5909
2022-03-08 21:48:53 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 2.3866
2022-03-08 21:49:27 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 2.2439
2022-03-08 21:50:00 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 2.4570
2022-03-08 21:50:34 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 2.4287
2022-03-08 21:51:07 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 2.4258
2022-03-08 21:51:40 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 2.3584
2022-03-08 21:52:13 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 2.3281
2022-03-08 21:52:47 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 2.5291
2022-03-08 21:53:20 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 2.4748
2022-03-08 21:53:54 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 2.3822
2022-03-08 21:54:27 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 2.3158
2022-03-08 21:55:01 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 2.6542
2022-03-08 21:55:34 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 2.3211
2022-03-08 21:55:36 - train: epoch 047, train_loss: 2.4241
2022-03-08 21:56:52 - eval: epoch: 047, acc1: 51.076%, acc5: 76.166%, test_loss: 2.1218, per_image_load_time: 2.301ms, per_image_inference_time: 0.264ms
2022-03-08 21:56:52 - until epoch: 047, best_acc1: 51.776%
2022-03-08 21:56:52 - epoch 048 lr: 0.010000000000000002
2022-03-08 21:57:32 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 2.5357
2022-03-08 21:58:06 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.7723
2022-03-08 21:58:39 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 2.3318
2022-03-08 21:59:13 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 2.3909
2022-03-08 21:59:46 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 2.2357
2022-03-08 22:00:19 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 2.3630
2022-03-08 22:00:52 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 2.3231
2022-03-08 22:01:25 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 2.5881
2022-03-08 22:01:59 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 2.5626
2022-03-08 22:02:32 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 2.5591
2022-03-08 22:03:05 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 2.5451
2022-03-08 22:03:38 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 2.4938
2022-03-08 22:04:11 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 2.1665
2022-03-08 22:04:45 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 2.3284
2022-03-08 22:05:18 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 2.4792
2022-03-08 22:05:51 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 2.4093
2022-03-08 22:06:24 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 2.6446
2022-03-08 22:06:57 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 2.4795
2022-03-08 22:07:30 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 2.6226
2022-03-08 22:08:03 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 2.5729
2022-03-08 22:08:36 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 2.2953
2022-03-08 22:09:10 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 2.6232
2022-03-08 22:09:43 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 2.1648
2022-03-08 22:10:16 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 2.5036
2022-03-08 22:10:49 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 2.4432
2022-03-08 22:11:22 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 2.7028
2022-03-08 22:11:55 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 2.6602
2022-03-08 22:12:28 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 2.4651
2022-03-08 22:13:01 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 2.5700
2022-03-08 22:13:35 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 2.4032
2022-03-08 22:14:08 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 2.4862
2022-03-08 22:14:41 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 2.1944
2022-03-08 22:15:14 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 2.3710
2022-03-08 22:15:47 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 2.4358
2022-03-08 22:16:21 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 2.7084
2022-03-08 22:16:54 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 2.2800
2022-03-08 22:17:28 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 2.5065
2022-03-08 22:18:02 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 2.4368
2022-03-08 22:18:35 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 2.4033
2022-03-08 22:19:09 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 2.3099
2022-03-08 22:19:42 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 2.6487
2022-03-08 22:20:16 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 2.2218
2022-03-08 22:20:50 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 2.4128
2022-03-08 22:21:23 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 2.2155
2022-03-08 22:21:56 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 2.4585
2022-03-08 22:22:29 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 2.3402
2022-03-08 22:23:01 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 2.5211
2022-03-08 22:23:35 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 2.7252
2022-03-08 22:24:09 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 2.4056
2022-03-08 22:24:43 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 2.5550
2022-03-08 22:24:46 - train: epoch 048, train_loss: 2.4262
2022-03-08 22:26:01 - eval: epoch: 048, acc1: 51.444%, acc5: 76.352%, test_loss: 2.1018, per_image_load_time: 1.713ms, per_image_inference_time: 0.235ms
2022-03-08 22:26:01 - until epoch: 048, best_acc1: 51.776%
2022-03-08 22:26:01 - epoch 049 lr: 0.010000000000000002
2022-03-08 22:26:40 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 2.5483
2022-03-08 22:27:13 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 2.3072
2022-03-08 22:27:45 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 2.5920
2022-03-08 22:28:18 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 2.4919
2022-03-08 22:28:51 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 2.3703
2022-03-08 22:29:24 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 2.4293
2022-03-08 22:29:57 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 2.3789
2022-03-08 22:30:30 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 2.7924
2022-03-08 22:31:02 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 2.2648
2022-03-08 22:31:35 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 2.4280
2022-03-08 22:32:08 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 2.3362
2022-03-08 22:32:41 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 2.1888
2022-03-08 22:33:14 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 2.4891
2022-03-08 22:33:47 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 2.5773
2022-03-08 22:34:20 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 2.2729
2022-03-08 22:34:52 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 2.5612
2022-03-08 22:35:26 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 2.6019
2022-03-08 22:35:59 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 2.3112
2022-03-08 22:36:31 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 2.2874
2022-03-08 22:37:05 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 2.3624
2022-03-08 22:37:38 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 2.4350
2022-03-08 22:38:11 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 2.3243
2022-03-08 22:38:44 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 2.2562
2022-03-08 22:39:17 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 2.5585
2022-03-08 22:39:50 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 2.3946
2022-03-08 22:40:23 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 2.3929
2022-03-08 22:40:56 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 2.2113
2022-03-08 22:41:29 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 2.3571
2022-03-08 22:42:02 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 2.3978
2022-03-08 22:42:35 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 2.5453
2022-03-08 22:43:08 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 2.4155
2022-03-08 22:43:41 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 2.5782
2022-03-08 22:44:14 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 2.3402
2022-03-08 22:44:48 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 2.5110
2022-03-08 22:45:21 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 2.5177
2022-03-08 22:45:54 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 2.5526
2022-03-08 22:46:27 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 2.4804
2022-03-08 22:47:00 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 2.6026
2022-03-08 22:47:33 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 2.6300
2022-03-08 22:48:06 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 2.3241
2022-03-08 22:48:39 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 2.3137
2022-03-08 22:49:12 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 2.6717
2022-03-08 22:49:45 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 2.6545
2022-03-08 22:50:18 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 2.5254
2022-03-08 22:50:50 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 2.3107
2022-03-08 22:51:24 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 2.5214
2022-03-08 22:51:57 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 2.4727
2022-03-08 22:52:31 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 2.2495
2022-03-08 22:53:04 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 2.3078
2022-03-08 22:53:37 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 2.2748
2022-03-08 22:53:40 - train: epoch 049, train_loss: 2.4226
2022-03-08 22:54:55 - eval: epoch: 049, acc1: 50.688%, acc5: 75.848%, test_loss: 2.1345, per_image_load_time: 1.358ms, per_image_inference_time: 0.238ms
2022-03-08 22:54:56 - until epoch: 049, best_acc1: 51.776%
2022-03-08 22:54:56 - epoch 050 lr: 0.010000000000000002
2022-03-08 22:55:35 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 2.8553
2022-03-08 22:56:08 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 2.4129
2022-03-08 22:56:40 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 2.6031
2022-03-08 22:57:13 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 2.4392
2022-03-08 22:57:46 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 2.4678
2022-03-08 22:58:19 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 2.6072
2022-03-08 22:58:53 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 2.2310
2022-03-08 22:59:26 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 2.0664
2022-03-08 22:59:59 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 2.2754
2022-03-08 23:00:32 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 2.6351
2022-03-08 23:01:05 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 2.6186
2022-03-08 23:01:38 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 2.4825
2022-03-08 23:02:12 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 2.2960
2022-03-08 23:02:45 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 2.4730
2022-03-08 23:03:18 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 2.2394
2022-03-08 23:03:50 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 2.4070
2022-03-08 23:04:23 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 2.5073
2022-03-08 23:04:56 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 2.5328
2022-03-08 23:05:30 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 2.2816
2022-03-08 23:06:03 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 2.4259
