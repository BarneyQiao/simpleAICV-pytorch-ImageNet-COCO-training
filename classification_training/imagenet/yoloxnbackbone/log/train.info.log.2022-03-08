2022-03-08 23:06:36 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 2.1633
2022-03-08 23:07:09 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 2.3589
2022-03-08 23:07:42 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 2.3259
2022-03-08 23:08:15 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 2.4148
2022-03-08 23:08:48 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 2.6077
2022-03-08 23:09:21 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 2.3933
2022-03-08 23:09:54 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 2.4276
2022-03-08 23:10:28 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 2.6135
2022-03-08 23:11:01 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 2.6545
2022-03-08 23:11:34 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 2.6737
2022-03-08 23:12:07 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 2.2842
2022-03-08 23:12:41 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 2.4537
2022-03-08 23:13:14 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 2.2576
2022-03-08 23:13:47 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 2.2709
2022-03-08 23:14:20 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 2.3001
2022-03-08 23:14:53 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 2.4838
2022-03-08 23:15:26 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 2.5730
2022-03-08 23:15:59 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 2.1488
2022-03-08 23:16:32 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 2.1292
2022-03-08 23:17:05 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 2.5759
2022-03-08 23:17:38 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 2.3787
2022-03-08 23:18:11 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 2.5197
2022-03-08 23:18:45 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 2.3263
2022-03-08 23:19:18 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 2.2683
2022-03-08 23:19:52 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 2.3283
2022-03-08 23:20:25 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 2.4467
2022-03-08 23:20:59 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 2.5382
2022-03-08 23:21:32 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 2.3590
2022-03-08 23:22:05 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 2.2312
2022-03-08 23:22:38 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 2.2725
2022-03-08 23:22:40 - train: epoch 050, train_loss: 2.4219
2022-03-08 23:23:56 - eval: epoch: 050, acc1: 51.232%, acc5: 76.106%, test_loss: 2.1165, per_image_load_time: 2.669ms, per_image_inference_time: 0.257ms
2022-03-08 23:23:56 - until epoch: 050, best_acc1: 51.776%
2022-03-08 23:23:56 - epoch 051 lr: 0.010000000000000002
2022-03-08 23:24:35 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 2.6199
2022-03-08 23:25:08 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 2.7550
2022-03-08 23:25:41 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 2.3525
2022-03-08 23:26:14 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 2.0253
2022-03-08 23:26:48 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 2.4739
2022-03-08 23:27:20 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 2.1974
2022-03-08 23:27:53 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 2.4512
2022-03-08 23:28:26 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 2.7586
2022-03-08 23:29:00 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 2.2093
2022-03-08 23:29:33 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 2.6416
2022-03-08 23:30:06 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 2.5658
2022-03-08 23:30:39 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 2.2917
2022-03-08 23:31:12 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 2.2225
2022-03-08 23:31:45 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 2.3036
2022-03-08 23:32:18 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 2.4048
2022-03-08 23:32:51 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 2.1752
2022-03-08 23:33:24 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 2.4214
2022-03-08 23:33:56 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 2.3264
2022-03-08 23:34:29 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 2.2031
2022-03-08 23:35:02 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 2.4113
2022-03-08 23:35:35 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 2.3025
2022-03-08 23:36:09 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 2.3007
2022-03-08 23:36:42 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 2.6602
2022-03-08 23:37:15 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 2.5131
2022-03-08 23:37:48 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 2.3503
2022-03-08 23:38:22 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 2.2080
2022-03-08 23:38:55 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 2.3930
2022-03-08 23:39:29 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 2.3930
2022-03-08 23:40:02 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 2.4381
2022-03-08 23:40:35 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 2.2484
2022-03-08 23:41:08 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 2.3411
2022-03-08 23:41:41 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 2.4123
2022-03-08 23:42:15 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 2.5510
2022-03-08 23:42:48 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 2.3365
2022-03-08 23:43:21 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 2.2542
2022-03-08 23:43:54 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 2.2837
2022-03-08 23:44:27 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 2.5516
2022-03-08 23:45:00 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 2.3789
2022-03-08 23:45:33 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 2.5487
2022-03-08 23:46:07 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 2.3467
2022-03-08 23:46:39 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 2.5078
2022-03-08 23:47:12 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 2.7092
2022-03-08 23:47:46 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 2.3234
2022-03-08 23:48:20 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 2.4463
2022-03-08 23:48:53 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 2.0287
2022-03-08 23:49:27 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 2.6160
2022-03-08 23:50:01 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 2.6279
2022-03-08 23:50:35 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 2.3737
2022-03-08 23:51:09 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 2.5621
2022-03-08 23:51:41 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 2.3759
2022-03-08 23:51:44 - train: epoch 051, train_loss: 2.4206
2022-03-08 23:53:01 - eval: epoch: 051, acc1: 51.392%, acc5: 76.112%, test_loss: 2.1066, per_image_load_time: 2.519ms, per_image_inference_time: 0.233ms
2022-03-08 23:53:01 - until epoch: 051, best_acc1: 51.776%
2022-03-08 23:53:01 - epoch 052 lr: 0.010000000000000002
2022-03-08 23:53:41 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 2.3290
2022-03-08 23:54:14 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 2.4151
2022-03-08 23:54:47 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 2.5133
2022-03-08 23:55:20 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 2.4423
2022-03-08 23:55:53 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 2.4170
2022-03-08 23:56:26 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 2.3694
2022-03-08 23:57:00 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 2.5689
2022-03-08 23:57:33 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 2.3110
2022-03-08 23:58:06 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 2.5615
2022-03-08 23:58:39 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 2.6561
2022-03-08 23:59:13 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 2.3564
2022-03-08 23:59:47 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 2.3806
2022-03-09 00:00:20 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 2.3203
2022-03-09 00:00:54 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 2.6138
2022-03-09 00:01:27 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 2.3619
2022-03-09 00:02:01 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 2.2216
2022-03-09 00:02:34 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 2.3250
2022-03-09 00:03:07 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 2.2248
2022-03-09 00:03:41 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 2.3591
2022-03-09 00:04:14 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 2.6648
2022-03-09 00:04:48 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 2.3251
2022-03-09 00:05:21 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 2.3843
2022-03-09 00:05:55 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 2.1903
2022-03-09 00:06:28 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 2.3404
2022-03-09 00:07:02 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 2.3045
2022-03-09 00:07:35 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 2.1689
2022-03-09 00:08:08 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 2.2668
2022-03-09 00:08:42 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 2.4391
2022-03-09 00:09:15 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 2.2448
2022-03-09 00:09:48 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 2.3504
2022-03-09 00:10:22 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 2.4818
2022-03-09 00:10:56 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 2.4567
2022-03-09 00:11:29 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 2.3061
2022-03-09 00:12:03 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 2.5740
2022-03-09 00:12:36 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 2.4817
2022-03-09 00:13:09 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 2.4639
2022-03-09 00:13:43 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 2.5153
2022-03-09 00:14:16 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 2.2170
2022-03-09 00:14:49 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 2.3271
2022-03-09 00:15:22 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 2.4917
2022-03-09 00:15:56 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 2.2609
2022-03-09 00:16:29 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 2.4658
2022-03-09 00:17:03 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 2.3211
2022-03-09 00:17:36 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 2.5784
2022-03-09 00:18:09 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 2.2902
2022-03-09 00:18:42 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 2.4531
2022-03-09 00:19:15 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 2.5128
2022-03-09 00:19:48 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 2.2579
2022-03-09 00:20:22 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 2.3202
2022-03-09 00:20:55 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 2.3977
2022-03-09 00:20:58 - train: epoch 052, train_loss: 2.4205
2022-03-09 00:22:14 - eval: epoch: 052, acc1: 51.632%, acc5: 76.330%, test_loss: 2.1043, per_image_load_time: 2.184ms, per_image_inference_time: 0.213ms
2022-03-09 00:22:14 - until epoch: 052, best_acc1: 51.776%
2022-03-09 00:22:14 - epoch 053 lr: 0.010000000000000002
2022-03-09 00:22:53 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 2.4276
2022-03-09 00:23:27 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 2.6073
2022-03-09 00:24:00 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 2.5232
2022-03-09 00:24:32 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 2.6594
2022-03-09 00:25:05 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 2.5825
2022-03-09 00:25:38 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 2.4589
2022-03-09 00:26:11 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 2.3171
2022-03-09 00:26:44 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 2.4875
2022-03-09 00:27:16 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 2.3237
2022-03-09 00:27:50 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 2.3705
2022-03-09 00:28:22 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 2.4393
2022-03-09 00:28:56 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 2.3070
2022-03-09 00:29:29 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 2.5143
2022-03-09 00:30:02 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 2.7438
2022-03-09 00:30:34 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 2.3516
2022-03-09 00:31:08 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 2.7031
2022-03-09 00:31:41 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 2.6497
2022-03-09 00:32:14 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 2.4057
2022-03-09 00:32:47 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 2.2943
2022-03-09 00:33:20 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 2.5271
2022-03-09 00:33:53 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 2.4079
2022-03-09 00:34:27 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 2.4399
2022-03-09 00:35:00 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 2.4592
2022-03-09 00:35:33 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 2.3849
2022-03-09 00:36:06 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 2.6653
2022-03-09 00:36:39 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 2.4548
2022-03-09 00:37:12 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 2.6552
2022-03-09 00:37:45 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 2.5138
2022-03-09 00:38:18 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 2.1978
2022-03-09 00:38:50 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 2.2401
2022-03-09 00:39:24 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 2.6082
2022-03-09 00:39:57 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 2.7728
2022-03-09 00:40:30 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 2.3473
2022-03-09 00:41:03 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 2.5387
2022-03-09 00:41:36 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 2.3322
2022-03-09 00:42:09 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 2.3591
2022-03-09 00:42:42 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 2.6157
2022-03-09 00:43:15 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 2.4504
2022-03-09 00:43:48 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 2.6384
2022-03-09 00:44:22 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 2.3361
2022-03-09 00:44:55 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 2.5335
2022-03-09 00:45:28 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 2.4662
2022-03-09 00:46:01 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 2.5443
2022-03-09 00:46:34 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 2.3616
2022-03-09 00:47:07 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 2.5862
2022-03-09 00:47:41 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 2.3655
2022-03-09 00:48:14 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 2.6441
2022-03-09 00:48:48 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 2.6032
2022-03-09 00:49:20 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 2.3547
2022-03-09 00:49:53 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 2.4341
2022-03-09 00:49:56 - train: epoch 053, train_loss: 2.4172
2022-03-09 00:51:11 - eval: epoch: 053, acc1: 50.900%, acc5: 75.970%, test_loss: 2.1295, per_image_load_time: 1.349ms, per_image_inference_time: 0.230ms
2022-03-09 00:51:11 - until epoch: 053, best_acc1: 51.776%
2022-03-09 00:51:11 - epoch 054 lr: 0.010000000000000002
2022-03-09 00:51:50 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 2.3181
2022-03-09 00:52:23 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 2.6771
2022-03-09 00:52:56 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 2.3970
2022-03-09 00:53:29 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 2.3535
2022-03-09 00:54:02 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 2.5222
2022-03-09 00:54:35 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 2.2395
2022-03-09 00:55:08 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 2.4506
2022-03-09 00:55:41 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 2.5012
2022-03-09 00:56:14 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 2.0760
2022-03-09 00:56:47 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 2.2355
2022-03-09 00:57:20 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 2.4071
2022-03-09 00:57:54 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 2.7565
2022-03-09 00:58:27 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 2.2696
2022-03-09 00:59:00 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 2.4563
2022-03-09 00:59:33 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 2.4879
2022-03-09 01:00:06 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 2.1295
2022-03-09 01:00:39 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 2.4976
2022-03-09 01:01:13 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 2.3729
2022-03-09 01:01:47 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 2.6848
2022-03-09 01:02:20 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 2.5995
2022-03-09 01:02:54 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 2.0851
2022-03-09 01:03:27 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 2.4210
2022-03-09 01:04:01 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 2.2614
2022-03-09 01:04:34 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 2.4475
2022-03-09 01:05:07 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 2.4855
2022-03-09 01:05:40 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 2.2344
2022-03-09 01:06:13 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 2.5016
2022-03-09 01:06:47 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 2.8604
2022-03-09 01:07:20 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 2.1258
2022-03-09 01:07:53 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 2.6155
2022-03-09 01:08:27 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 2.4965
2022-03-09 01:09:00 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 2.6310
2022-03-09 01:09:34 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 2.3688
2022-03-09 01:10:08 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 2.4700
2022-03-09 01:10:41 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 2.5569
2022-03-09 01:11:15 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 2.3774
2022-03-09 01:11:49 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 2.3583
2022-03-09 01:12:22 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 2.4029
2022-03-09 01:12:56 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 2.4108
2022-03-09 01:13:30 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 2.2508
2022-03-09 01:14:03 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 2.4420
2022-03-09 01:14:37 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 2.4472
2022-03-09 01:15:10 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 2.4659
2022-03-09 01:15:43 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 2.1597
2022-03-09 01:16:17 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 2.3685
2022-03-09 01:16:50 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 2.6078
2022-03-09 01:17:23 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 2.7868
2022-03-09 01:17:56 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 2.6344
2022-03-09 01:18:30 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 2.3353
2022-03-09 01:19:03 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 2.4752
2022-03-09 01:19:06 - train: epoch 054, train_loss: 2.4185
2022-03-09 01:20:24 - eval: epoch: 054, acc1: 51.394%, acc5: 76.582%, test_loss: 2.0907, per_image_load_time: 2.749ms, per_image_inference_time: 0.209ms
2022-03-09 01:20:24 - until epoch: 054, best_acc1: 51.776%
2022-03-09 01:20:24 - epoch 055 lr: 0.010000000000000002
2022-03-09 01:21:03 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 2.4403
2022-03-09 01:21:37 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 2.4597
2022-03-09 01:22:10 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 2.2238
2022-03-09 01:22:43 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 2.5484
2022-03-09 01:23:16 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 2.2623
2022-03-09 01:23:49 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 2.2930
2022-03-09 01:24:22 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 2.5151
2022-03-09 01:24:56 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 2.3060
2022-03-09 01:25:29 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 2.4231
2022-03-09 01:26:02 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 2.3736
2022-03-09 01:26:35 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 2.4897
2022-03-09 01:27:08 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 2.4312
2022-03-09 01:27:42 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 2.5337
2022-03-09 01:28:15 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 2.4562
2022-03-09 01:28:48 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 2.4963
2022-03-09 01:29:22 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 2.5192
2022-03-09 01:29:55 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 2.5703
2022-03-09 01:30:28 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 2.6192
2022-03-09 01:31:02 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 2.5972
2022-03-09 01:31:35 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 2.4191
2022-03-09 01:32:08 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 2.0467
2022-03-09 01:32:42 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 2.5491
2022-03-09 01:33:15 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 2.3131
2022-03-09 01:33:48 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 2.2490
2022-03-09 01:34:22 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 2.4182
2022-03-09 01:34:55 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 2.3287
2022-03-09 01:35:28 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 2.3476
2022-03-09 01:36:01 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 2.4274
2022-03-09 01:36:34 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 2.5731
2022-03-09 01:37:07 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 2.4578
2022-03-09 01:37:41 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 2.5261
2022-03-09 01:38:15 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 2.2826
2022-03-09 01:38:48 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 2.1558
2022-03-09 01:39:21 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 2.3118
2022-03-09 01:39:55 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 2.1924
2022-03-09 01:40:29 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 2.5768
2022-03-09 01:41:02 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 2.4008
2022-03-09 01:41:35 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 2.4314
2022-03-09 01:42:09 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 2.7740
2022-03-09 01:42:42 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 2.3929
2022-03-09 01:43:15 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 2.4992
2022-03-09 01:43:48 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 2.3150
2022-03-09 01:44:22 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 2.5345
2022-03-09 01:44:55 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 2.4569
2022-03-09 01:45:28 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 2.5041
2022-03-09 01:46:02 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 2.4557
2022-03-09 01:46:35 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 2.3157
2022-03-09 01:47:09 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 2.3963
2022-03-09 01:47:43 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 2.4091
2022-03-09 01:48:15 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 2.4663
2022-03-09 01:48:18 - train: epoch 055, train_loss: 2.4154
2022-03-09 01:49:34 - eval: epoch: 055, acc1: 51.632%, acc5: 76.380%, test_loss: 2.0931, per_image_load_time: 2.654ms, per_image_inference_time: 0.234ms
2022-03-09 01:49:34 - until epoch: 055, best_acc1: 51.776%
2022-03-09 01:49:34 - epoch 056 lr: 0.010000000000000002
2022-03-09 01:50:14 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 2.4356
2022-03-09 01:50:47 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 2.5777
2022-03-09 01:51:20 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 2.3319
2022-03-09 01:51:53 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 2.2913
2022-03-09 01:52:26 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 2.3341
2022-03-09 01:52:59 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 2.3936
2022-03-09 01:53:32 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 2.4602
2022-03-09 01:54:05 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 2.7248
2022-03-09 01:54:38 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 2.4861
2022-03-09 01:55:11 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 2.2833
2022-03-09 01:55:44 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 2.2658
2022-03-09 01:56:17 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 2.4275
2022-03-09 01:56:50 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 2.4554
2022-03-09 01:57:22 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 2.3650
2022-03-09 01:57:55 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 2.8174
2022-03-09 01:58:28 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 2.1530
2022-03-09 01:59:02 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 2.5385
2022-03-09 01:59:35 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 2.7455
2022-03-09 02:00:08 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 2.4814
2022-03-09 02:00:41 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 2.4416
2022-03-09 02:01:14 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 2.4954
2022-03-09 02:01:47 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 2.6066
2022-03-09 02:02:20 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 2.4812
2022-03-09 02:02:53 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 2.4624
2022-03-09 02:03:27 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 2.5071
2022-03-09 02:04:00 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 2.2809
2022-03-09 02:04:33 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 2.4113
2022-03-09 02:05:06 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 2.3533
2022-03-09 02:05:40 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 2.5185
2022-03-09 02:06:14 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 2.6165
2022-03-09 02:06:47 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 2.3688
2022-03-09 02:07:20 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 2.2713
2022-03-09 02:07:54 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 2.7357
2022-03-09 02:08:27 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 2.3480
2022-03-09 02:09:00 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 2.2806
2022-03-09 02:09:33 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 2.0679
2022-03-09 02:10:06 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 2.3743
2022-03-09 02:10:39 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 2.2375
2022-03-09 02:11:12 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 2.6513
2022-03-09 02:11:45 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 2.5593
2022-03-09 02:12:18 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 2.6121
2022-03-09 02:12:52 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 2.3529
2022-03-09 02:13:25 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 2.3463
2022-03-09 02:13:58 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 2.5226
2022-03-09 02:14:31 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 2.3548
2022-03-09 02:15:04 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 2.5131
2022-03-09 02:15:37 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 2.4995
2022-03-09 02:16:11 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 2.6513
2022-03-09 02:16:45 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 2.4991
2022-03-09 02:17:18 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 2.5094
2022-03-09 02:17:21 - train: epoch 056, train_loss: 2.4121
2022-03-09 02:18:37 - eval: epoch: 056, acc1: 51.208%, acc5: 76.178%, test_loss: 2.1157, per_image_load_time: 2.639ms, per_image_inference_time: 0.252ms
2022-03-09 02:18:37 - until epoch: 056, best_acc1: 51.776%
2022-03-09 02:18:37 - epoch 057 lr: 0.010000000000000002
2022-03-09 02:19:17 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 2.4658
2022-03-09 02:19:51 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 2.3945
2022-03-09 02:20:24 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 2.2470
2022-03-09 02:20:58 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 2.4208
2022-03-09 02:21:31 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 2.1067
2022-03-09 02:22:05 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 2.6329
2022-03-09 02:22:38 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 2.2962
2022-03-09 02:23:10 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 2.3896
2022-03-09 02:23:43 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 2.4255
2022-03-09 02:24:16 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 2.2091
2022-03-09 02:24:49 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 2.3271
2022-03-09 02:25:22 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 2.4400
2022-03-09 02:25:55 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 2.3922
2022-03-09 02:26:28 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 2.5666
2022-03-09 02:27:01 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 2.5445
2022-03-09 02:27:33 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 2.5103
2022-03-09 02:28:06 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 2.4896
2022-03-09 02:28:39 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 2.5489
2022-03-09 02:29:12 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 2.4437
2022-03-09 02:29:45 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 2.3447
2022-03-09 02:30:18 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 2.5224
2022-03-09 02:30:51 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 2.5518
2022-03-09 02:31:25 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 2.4598
2022-03-09 02:31:58 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 2.4449
2022-03-09 02:32:31 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 2.5044
2022-03-09 02:33:05 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 2.2337
2022-03-09 02:33:38 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 2.0947
2022-03-09 02:34:11 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 2.0700
2022-03-09 02:34:44 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 2.5057
2022-03-09 02:35:18 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 2.3618
2022-03-09 02:35:51 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 2.7352
2022-03-09 02:36:24 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 2.5211
2022-03-09 02:36:58 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 2.4549
2022-03-09 02:37:31 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 2.4523
2022-03-09 02:38:05 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 2.4297
2022-03-09 02:38:38 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 2.3704
2022-03-09 02:39:11 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 2.3372
2022-03-09 02:39:45 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 2.3297
2022-03-09 02:40:18 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 2.4014
2022-03-09 02:40:51 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 2.3542
2022-03-09 02:41:24 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 2.4939
2022-03-09 02:41:58 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 2.6009
2022-03-09 02:42:31 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 2.1695
2022-03-09 02:43:04 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 2.4384
2022-03-09 02:43:37 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 2.6202
2022-03-09 02:44:10 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 2.5266
2022-03-09 02:44:44 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 2.3573
2022-03-09 02:45:19 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 2.7184
2022-03-09 02:45:52 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 2.6142
2022-03-09 02:46:25 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 2.6053
2022-03-09 02:46:28 - train: epoch 057, train_loss: 2.4121
2022-03-09 02:47:44 - eval: epoch: 057, acc1: 51.260%, acc5: 76.414%, test_loss: 2.1098, per_image_load_time: 2.664ms, per_image_inference_time: 0.257ms
2022-03-09 02:47:44 - until epoch: 057, best_acc1: 51.776%
2022-03-09 02:47:44 - epoch 058 lr: 0.010000000000000002
2022-03-09 02:48:24 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 2.3419
2022-03-09 02:48:57 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 2.1777
2022-03-09 02:49:30 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 2.3626
2022-03-09 02:50:04 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 2.3421
2022-03-09 02:50:37 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 2.1396
2022-03-09 02:51:09 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 2.5069
2022-03-09 02:51:42 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 2.3867
2022-03-09 02:52:16 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 2.3413
2022-03-09 02:52:49 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 2.2023
2022-03-09 02:53:22 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 2.4758
2022-03-09 02:53:55 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 2.1076
2022-03-09 02:54:28 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 2.2392
2022-03-09 02:55:01 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 2.5550
2022-03-09 02:55:34 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 2.5024
2022-03-09 02:56:08 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 2.3424
2022-03-09 02:56:41 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 2.3863
2022-03-09 02:57:15 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 2.5368
2022-03-09 02:57:48 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 2.3777
2022-03-09 02:58:21 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 2.5623
2022-03-09 02:58:55 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 2.6979
2022-03-09 02:59:28 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 2.2561
2022-03-09 03:00:01 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 2.1795
2022-03-09 03:00:35 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 2.2170
2022-03-09 03:01:08 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 2.5950
2022-03-09 03:01:41 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 2.4820
2022-03-09 03:02:15 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 2.2262
2022-03-09 03:02:48 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 2.5925
2022-03-09 03:03:21 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 2.2240
2022-03-09 03:03:54 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 2.2360
2022-03-09 03:04:28 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 2.5859
2022-03-09 03:05:01 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 2.3733
2022-03-09 03:05:34 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 2.1683
2022-03-09 03:06:07 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 2.3741
2022-03-09 03:06:41 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 2.2318
2022-03-09 03:07:14 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 2.2741
2022-03-09 03:07:47 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 2.2473
2022-03-09 03:08:21 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 2.3285
2022-03-09 03:08:54 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 2.5130
2022-03-09 03:09:28 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 2.4687
2022-03-09 03:10:01 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 2.6047
2022-03-09 03:10:34 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 2.5406
2022-03-09 03:11:08 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 2.1367
2022-03-09 03:11:41 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 2.5743
2022-03-09 03:12:14 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 2.2836
2022-03-09 03:12:48 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 2.4977
2022-03-09 03:13:21 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 2.2801
2022-03-09 03:13:55 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 2.3681
2022-03-09 03:14:29 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 2.5411
2022-03-09 03:15:04 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 2.2571
2022-03-09 03:15:37 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 2.3228
2022-03-09 03:15:40 - train: epoch 058, train_loss: 2.4093
2022-03-09 03:16:56 - eval: epoch: 058, acc1: 51.898%, acc5: 76.530%, test_loss: 2.0858, per_image_load_time: 2.638ms, per_image_inference_time: 0.252ms
2022-03-09 03:16:56 - until epoch: 058, best_acc1: 51.898%
2022-03-09 03:16:56 - epoch 059 lr: 0.010000000000000002
2022-03-09 03:17:35 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 2.4763
2022-03-09 03:18:08 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 2.2368
2022-03-09 03:18:41 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 2.4939
2022-03-09 03:19:14 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 2.5244
2022-03-09 03:19:48 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 2.6092
2022-03-09 03:20:21 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 2.4397
2022-03-09 03:20:54 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 2.3444
2022-03-09 03:21:27 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 2.3552
2022-03-09 03:22:01 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 2.3401
2022-03-09 03:22:34 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 2.5271
2022-03-09 03:23:07 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 2.7275
2022-03-09 03:23:40 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 2.0692
2022-03-09 03:24:13 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 2.6251
2022-03-09 03:24:46 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 2.6258
2022-03-09 03:25:20 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 2.5898
2022-03-09 03:25:52 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 2.2447
2022-03-09 03:26:25 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 2.6340
2022-03-09 03:26:58 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 2.3438
2022-03-09 03:27:31 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 2.4543
2022-03-09 03:28:04 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 2.3275
2022-03-09 03:28:37 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 2.5768
2022-03-09 03:29:09 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 2.5659
2022-03-09 03:29:42 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 2.2513
2022-03-09 03:30:16 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 2.4884
2022-03-09 03:30:48 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 2.3597
2022-03-09 03:31:21 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 2.3304
2022-03-09 03:31:54 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 2.3070
2022-03-09 03:32:27 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 2.6444
2022-03-09 03:33:00 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 2.3470
2022-03-09 03:33:33 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 2.8175
2022-03-09 03:34:06 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 2.3988
2022-03-09 03:34:39 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 2.4870
2022-03-09 03:35:13 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 2.4325
2022-03-09 03:35:46 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 2.7487
2022-03-09 03:36:19 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 2.1074
2022-03-09 03:36:52 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 2.3693
2022-03-09 03:37:25 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 2.2014
2022-03-09 03:37:58 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 2.4585
2022-03-09 03:38:32 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 2.3142
2022-03-09 03:39:05 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 2.7069
2022-03-09 03:39:38 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 2.3947
2022-03-09 03:40:11 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 2.3421
2022-03-09 03:40:44 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 2.5952
2022-03-09 03:41:17 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 2.5318
2022-03-09 03:41:51 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 2.5141
2022-03-09 03:42:24 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 2.4950
2022-03-09 03:42:57 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 2.2742
2022-03-09 03:43:31 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 2.2840
2022-03-09 03:44:05 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 2.6246
2022-03-09 03:44:37 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 2.6515
2022-03-09 03:44:40 - train: epoch 059, train_loss: 2.4094
2022-03-09 03:45:56 - eval: epoch: 059, acc1: 50.744%, acc5: 75.698%, test_loss: 2.1386, per_image_load_time: 2.621ms, per_image_inference_time: 0.243ms
2022-03-09 03:45:56 - until epoch: 059, best_acc1: 51.898%
2022-03-09 03:45:56 - epoch 060 lr: 0.010000000000000002
2022-03-09 03:46:35 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 2.4211
2022-03-09 03:47:08 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 2.4830
2022-03-09 03:47:42 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 2.3467
2022-03-09 03:48:14 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 2.3978
2022-03-09 03:48:47 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 2.4984
2022-03-09 03:49:20 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 2.4276
2022-03-09 03:49:54 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 2.4905
2022-03-09 03:50:26 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 2.6213
2022-03-09 03:51:00 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 2.1399
2022-03-09 03:51:33 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.9769
2022-03-09 03:52:06 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 2.0652
2022-03-09 03:52:40 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 2.2945
2022-03-09 03:53:13 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 2.3325
2022-03-09 03:53:46 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 2.4100
2022-03-09 03:54:19 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 2.3937
2022-03-09 03:54:53 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 2.4391
2022-03-09 03:55:26 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 2.3946
2022-03-09 03:55:59 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 2.3685
2022-03-09 03:56:33 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 2.5972
2022-03-09 03:57:06 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 2.2493
2022-03-09 03:57:39 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 2.5349
2022-03-09 03:58:12 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 2.5392
2022-03-09 03:58:45 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 2.2600
2022-03-09 03:59:19 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 2.1999
2022-03-09 03:59:52 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 2.3211
2022-03-09 04:00:25 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 2.5034
2022-03-09 04:00:59 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 2.2795
2022-03-09 04:01:32 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 2.3890
2022-03-09 04:02:05 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 2.4243
2022-03-09 04:02:38 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 2.6657
2022-03-09 04:03:11 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 2.4886
2022-03-09 04:03:45 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 2.2943
2022-03-09 04:04:18 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 2.3120
2022-03-09 04:04:51 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 2.3984
2022-03-09 04:05:25 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 2.3743
2022-03-09 04:05:58 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 2.4683
2022-03-09 04:06:31 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 2.2366
2022-03-09 04:07:05 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 2.5342
2022-03-09 04:07:38 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 2.6947
2022-03-09 04:08:11 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 2.3907
2022-03-09 04:08:44 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 2.5170
2022-03-09 04:09:17 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 2.7006
2022-03-09 04:09:50 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 2.4082
2022-03-09 04:10:23 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 2.6690
2022-03-09 04:10:56 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 2.2402
2022-03-09 04:11:30 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 2.2288
2022-03-09 04:12:03 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 2.5435
2022-03-09 04:12:37 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 2.2840
2022-03-09 04:13:11 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 2.4684
2022-03-09 04:13:43 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 2.3753
2022-03-09 04:13:46 - train: epoch 060, train_loss: 2.4061
2022-03-09 04:15:02 - eval: epoch: 060, acc1: 48.758%, acc5: 74.176%, test_loss: 2.2365, per_image_load_time: 1.596ms, per_image_inference_time: 0.245ms
2022-03-09 04:15:02 - until epoch: 060, best_acc1: 51.898%
2022-03-09 04:15:02 - epoch 061 lr: 0.0010000000000000002
2022-03-09 04:15:42 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 2.1240
2022-03-09 04:16:15 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 2.3432
2022-03-09 04:16:48 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 2.0044
2022-03-09 04:17:22 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 2.4583
2022-03-09 04:17:55 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 2.2357
2022-03-09 04:18:28 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 2.3085
2022-03-09 04:19:02 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 2.0596
2022-03-09 04:19:35 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 2.3645
2022-03-09 04:20:08 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 2.3544
2022-03-09 04:20:42 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 2.0689
2022-03-09 04:21:14 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 2.1120
2022-03-09 04:21:47 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 2.0338
2022-03-09 04:22:20 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 2.0509
2022-03-09 04:22:53 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 2.1129
2022-03-09 04:23:26 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 2.2634
2022-03-09 04:23:59 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 2.0450
2022-03-09 04:24:32 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 2.3273
2022-03-09 04:25:05 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 2.2064
2022-03-09 04:25:38 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 2.4491
2022-03-09 04:26:11 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 2.2549
2022-03-09 04:26:44 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 2.5740
2022-03-09 04:27:17 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 2.1908
2022-03-09 04:27:51 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 2.0876
2022-03-09 04:28:24 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.9672
2022-03-09 04:28:57 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 2.2219
2022-03-09 04:29:30 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 2.3338
2022-03-09 04:30:03 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 2.3002
2022-03-09 04:30:37 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 2.2145
2022-03-09 04:31:10 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 2.3621
2022-03-09 04:31:43 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 2.2200
2022-03-09 04:32:17 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 2.3392
2022-03-09 04:32:50 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 2.3489
2022-03-09 04:33:23 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 2.2446
2022-03-09 04:33:56 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 2.0667
2022-03-09 04:34:30 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 2.2257
2022-03-09 04:35:03 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 2.2138
2022-03-09 04:35:36 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 2.3003
2022-03-09 04:36:09 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 2.2262
2022-03-09 04:36:42 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 2.1106
2022-03-09 04:37:15 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 2.1754
2022-03-09 04:37:49 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 2.4906
2022-03-09 04:38:22 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 2.1226
2022-03-09 04:38:55 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 2.2346
2022-03-09 04:39:28 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 2.2989
2022-03-09 04:40:02 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 2.1769
2022-03-09 04:40:35 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 2.2331
2022-03-09 04:41:09 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 2.2509
2022-03-09 04:41:42 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 2.1578
2022-03-09 04:42:16 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 2.2013
2022-03-09 04:42:48 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 2.4547
2022-03-09 04:42:51 - train: epoch 061, train_loss: 2.2332
2022-03-09 04:44:06 - eval: epoch: 061, acc1: 55.622%, acc5: 79.346%, test_loss: 1.9076, per_image_load_time: 2.643ms, per_image_inference_time: 0.230ms
2022-03-09 04:44:06 - until epoch: 061, best_acc1: 55.622%
2022-03-09 04:44:06 - epoch 062 lr: 0.0010000000000000002
2022-03-09 04:44:46 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 2.2463
2022-03-09 04:45:19 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 2.3213
2022-03-09 04:45:52 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 2.2732
2022-03-09 04:46:25 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 2.0716
2022-03-09 04:46:58 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 2.1451
2022-03-09 04:47:31 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 2.0960
2022-03-09 04:48:04 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 2.3689
2022-03-09 04:48:37 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 2.2238
2022-03-09 04:49:10 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 2.1654
2022-03-09 04:49:43 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 2.3664
2022-03-09 04:50:17 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 2.1088
2022-03-09 04:50:50 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 2.4067
2022-03-09 04:51:23 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 2.3498
2022-03-09 04:51:56 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 2.1666
2022-03-09 04:52:29 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 2.2271
2022-03-09 04:53:02 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 2.3962
2022-03-09 04:53:35 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 2.1462
2022-03-09 04:54:09 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 2.0627
2022-03-09 04:54:42 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 2.1605
2022-03-09 04:55:15 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 2.1644
2022-03-09 04:55:49 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 2.3171
2022-03-09 04:56:22 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 2.0218
2022-03-09 04:56:55 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 2.1539
2022-03-09 04:57:28 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 2.2491
2022-03-09 04:58:02 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 2.3007
2022-03-09 04:58:35 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 2.1856
2022-03-09 04:59:09 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 2.1687
2022-03-09 04:59:42 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 2.2870
2022-03-09 05:00:16 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 2.1263
2022-03-09 05:00:49 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 2.4448
2022-03-09 05:01:23 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 2.1990
2022-03-09 05:01:56 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.9430
2022-03-09 05:02:29 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 2.3716
2022-03-09 05:03:02 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 2.1376
2022-03-09 05:03:35 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 2.2370
2022-03-09 05:04:08 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 2.3449
2022-03-09 05:04:42 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 2.2505
2022-03-09 05:05:15 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 2.1706
2022-03-09 05:05:48 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 2.2211
2022-03-09 05:06:22 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.8995
2022-03-09 05:06:55 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 2.3968
2022-03-09 05:07:28 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 2.0608
2022-03-09 05:08:01 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 2.3561
2022-03-09 05:08:34 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 2.2736
2022-03-09 05:09:07 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 2.1396
2022-03-09 05:09:41 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 2.0294
2022-03-09 05:10:15 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 2.1281
2022-03-09 05:10:49 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 2.0807
2022-03-09 05:11:22 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 2.1650
2022-03-09 05:11:55 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 2.2591
2022-03-09 05:11:58 - train: epoch 062, train_loss: 2.2000
2022-03-09 05:13:15 - eval: epoch: 062, acc1: 55.886%, acc5: 79.538%, test_loss: 1.8934, per_image_load_time: 2.682ms, per_image_inference_time: 0.246ms
2022-03-09 05:13:15 - until epoch: 062, best_acc1: 55.886%
2022-03-09 05:13:15 - epoch 063 lr: 0.0010000000000000002
2022-03-09 05:13:54 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 2.2029
2022-03-09 05:14:27 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 2.1435
2022-03-09 05:15:00 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 2.3150
2022-03-09 05:15:33 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 2.0672
2022-03-09 05:16:06 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.9732
2022-03-09 05:16:39 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 2.3823
2022-03-09 05:17:12 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 2.1098
2022-03-09 05:17:45 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 2.2599
2022-03-09 05:18:18 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 2.3747
2022-03-09 05:18:51 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 2.2339
2022-03-09 05:19:24 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 2.1331
2022-03-09 05:19:58 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 2.1713
2022-03-09 05:20:31 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 2.0347
2022-03-09 05:21:04 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 2.5364
2022-03-09 05:21:37 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 2.1622
2022-03-09 05:22:10 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 2.0428
2022-03-09 05:22:43 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.9809
2022-03-09 05:23:16 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 2.3723
2022-03-09 05:23:49 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 2.4148
2022-03-09 05:24:23 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 2.0553
2022-03-09 05:24:55 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 2.0956
2022-03-09 05:25:29 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 2.4792
2022-03-09 05:26:02 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 2.4511
2022-03-09 05:26:35 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 2.4284
2022-03-09 05:27:08 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 2.2512
2022-03-09 05:27:42 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 2.0446
2022-03-09 05:28:15 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 2.4363
2022-03-09 05:28:49 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 2.2610
2022-03-09 05:29:22 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 2.1814
2022-03-09 05:29:55 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 2.3385
2022-03-09 05:30:28 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 2.5285
2022-03-09 05:31:01 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 2.2032
2022-03-09 05:31:35 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 2.0314
2022-03-09 05:32:08 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.9308
2022-03-09 05:32:41 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 2.2569
2022-03-09 05:33:14 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 2.1371
2022-03-09 05:33:47 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 2.3187
2022-03-09 05:34:20 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 2.4181
2022-03-09 05:34:53 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 1.9682
2022-03-09 05:35:27 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 1.9572
2022-03-09 05:36:00 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 2.2215
2022-03-09 05:36:33 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 2.1810
2022-03-09 05:37:07 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 2.2485
2022-03-09 05:37:40 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 2.1753
2022-03-09 05:38:12 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 2.0636
2022-03-09 05:38:46 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 2.1312
2022-03-09 05:39:21 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 2.1037
2022-03-09 05:39:54 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 2.2931
2022-03-09 05:40:27 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 2.0804
2022-03-09 05:41:00 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 2.2259
2022-03-09 05:41:03 - train: epoch 063, train_loss: 2.1905
2022-03-09 05:42:19 - eval: epoch: 063, acc1: 55.928%, acc5: 79.612%, test_loss: 1.8864, per_image_load_time: 2.671ms, per_image_inference_time: 0.240ms
2022-03-09 05:42:20 - until epoch: 063, best_acc1: 55.928%
2022-03-09 05:42:20 - epoch 064 lr: 0.0010000000000000002
2022-03-09 05:42:59 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 2.0685
2022-03-09 05:43:32 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 2.1630
2022-03-09 05:44:06 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.9844
2022-03-09 05:44:39 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 2.2352
2022-03-09 05:45:12 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 2.0710
2022-03-09 05:45:45 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 2.3156
2022-03-09 05:46:18 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 2.2820
2022-03-09 05:46:51 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 2.0101
2022-03-09 05:47:25 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 2.1762
2022-03-09 05:47:58 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 2.1624
2022-03-09 05:48:32 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 2.2609
2022-03-09 05:49:05 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 2.0491
2022-03-09 05:49:38 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 2.2234
2022-03-09 05:50:11 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 2.3456
2022-03-09 05:50:44 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 2.4007
2022-03-09 05:51:17 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 2.0256
2022-03-09 05:51:51 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 2.0103
2022-03-09 05:52:24 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 2.0735
2022-03-09 05:52:57 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 2.1310
2022-03-09 05:53:30 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 2.0360
2022-03-09 05:54:03 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 2.2358
2022-03-09 05:54:37 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 2.2587
2022-03-09 05:55:10 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 2.1917
2022-03-09 05:55:42 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 2.2173
2022-03-09 05:56:15 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 2.0417
2022-03-09 05:56:48 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 2.1313
2022-03-09 05:57:21 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 2.1773
2022-03-09 05:57:55 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.9000
2022-03-09 05:58:28 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 2.5026
2022-03-09 05:59:01 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 2.1744
2022-03-09 05:59:34 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 2.2495
2022-03-09 06:00:07 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 2.1258
2022-03-09 06:00:41 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 2.0508
2022-03-09 06:01:14 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 2.2930
2022-03-09 06:01:46 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 2.0943
2022-03-09 06:02:20 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 2.1376
2022-03-09 06:02:53 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 1.8824
2022-03-09 06:03:26 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 2.4785
2022-03-09 06:04:00 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 2.1658
2022-03-09 06:04:33 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 2.0528
2022-03-09 06:05:06 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 2.2381
2022-03-09 06:05:40 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 2.1093
2022-03-09 06:06:13 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 2.4273
2022-03-09 06:06:47 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 2.0738
2022-03-09 06:07:20 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 2.0977
2022-03-09 06:07:55 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 2.3185
2022-03-09 06:08:28 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 2.4270
2022-03-09 06:09:02 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 2.3684
2022-03-09 06:09:36 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 2.4397
2022-03-09 06:10:09 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 2.0810
2022-03-09 06:10:11 - train: epoch 064, train_loss: 2.1866
2022-03-09 06:11:28 - eval: epoch: 064, acc1: 55.942%, acc5: 79.724%, test_loss: 1.8781, per_image_load_time: 2.598ms, per_image_inference_time: 0.270ms
2022-03-09 06:11:28 - until epoch: 064, best_acc1: 55.942%
2022-03-09 06:11:28 - epoch 065 lr: 0.0010000000000000002
2022-03-09 06:12:07 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 2.4144
2022-03-09 06:12:41 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 2.1289
2022-03-09 06:13:14 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 2.2558
2022-03-09 06:13:47 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 2.3390
2022-03-09 06:14:21 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 2.1896
2022-03-09 06:14:54 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 2.2706
2022-03-09 06:15:27 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 2.3240
2022-03-09 06:16:00 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 2.0711
2022-03-09 06:16:33 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.9903
2022-03-09 06:17:06 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 2.0793
2022-03-09 06:17:39 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 2.1861
2022-03-09 06:18:12 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 2.3741
2022-03-09 06:18:45 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 2.1761
2022-03-09 06:19:19 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 2.0939
2022-03-09 06:19:52 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 2.2565
2022-03-09 06:20:25 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 2.3083
2022-03-09 06:20:58 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 2.0699
2022-03-09 06:21:31 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 2.0596
2022-03-09 06:22:04 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 2.0185
2022-03-09 06:22:37 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 2.1785
2022-03-09 06:23:11 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 2.2402
2022-03-09 06:23:44 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 2.2200
2022-03-09 06:24:17 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.9062
2022-03-09 06:24:51 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 2.1576
2022-03-09 06:25:24 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 2.1691
2022-03-09 06:25:57 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 2.4547
2022-03-09 06:26:30 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 2.2961
2022-03-09 06:27:03 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 2.0714
2022-03-09 06:27:36 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 2.2453
2022-03-09 06:28:10 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 2.0051
2022-03-09 06:28:43 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 2.0814
2022-03-09 06:29:16 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 2.3728
2022-03-09 06:29:49 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 2.1150
2022-03-09 06:30:22 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 2.0316
2022-03-09 06:30:56 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 2.5498
2022-03-09 06:31:29 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 2.2466
2022-03-09 06:32:02 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 2.0506
2022-03-09 06:32:35 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.9993
2022-03-09 06:33:09 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 2.3121
2022-03-09 06:33:42 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 2.1466
2022-03-09 06:34:16 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 2.0061
2022-03-09 06:34:50 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 2.1909
2022-03-09 06:35:23 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 2.0402
2022-03-09 06:35:57 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 2.2438
2022-03-09 06:36:31 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 2.0555
2022-03-09 06:37:04 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 2.1907
2022-03-09 06:37:37 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 2.0970
2022-03-09 06:38:10 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 1.9746
2022-03-09 06:38:43 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 2.1882
2022-03-09 06:39:16 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 2.2554
2022-03-09 06:39:19 - train: epoch 065, train_loss: 2.1792
2022-03-09 06:40:35 - eval: epoch: 065, acc1: 56.164%, acc5: 79.780%, test_loss: 1.8725, per_image_load_time: 1.690ms, per_image_inference_time: 0.219ms
2022-03-09 06:40:35 - until epoch: 065, best_acc1: 56.164%
2022-03-09 06:40:35 - epoch 066 lr: 0.0010000000000000002
2022-03-09 06:41:14 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 2.1206
2022-03-09 06:41:47 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 2.3949
2022-03-09 06:42:20 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 2.0729
2022-03-09 06:42:53 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 1.8631
2022-03-09 06:43:26 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 2.1481
2022-03-09 06:43:59 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 2.2327
2022-03-09 06:44:32 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 2.1019
2022-03-09 06:45:05 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 2.5730
2022-03-09 06:45:38 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 2.3881
2022-03-09 06:46:11 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 2.1491
2022-03-09 06:46:44 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 2.2545
2022-03-09 06:47:17 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 2.3575
2022-03-09 06:47:50 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 2.3200
2022-03-09 06:48:23 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 2.0946
2022-03-09 06:48:56 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 2.3320
2022-03-09 06:49:29 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 2.0913
2022-03-09 06:50:03 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 2.2064
2022-03-09 06:50:36 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.8789
2022-03-09 06:51:09 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 2.3011
2022-03-09 06:51:42 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 2.2404
2022-03-09 06:52:16 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 2.2610
2022-03-09 06:52:49 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 2.2100
2022-03-09 06:53:22 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 2.4044
2022-03-09 06:53:55 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 2.0908
2022-03-09 06:54:29 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 2.1744
2022-03-09 06:55:02 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 2.1522
2022-03-09 06:55:35 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 2.2822
2022-03-09 06:56:08 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 2.1198
2022-03-09 06:56:42 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 2.3374
2022-03-09 06:57:15 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.9981
2022-03-09 06:57:48 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 2.2619
2022-03-09 06:58:22 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 2.1123
2022-03-09 06:58:55 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 2.0900
2022-03-09 06:59:28 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 2.3626
2022-03-09 07:00:02 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 2.2107
2022-03-09 07:00:35 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 2.2114
2022-03-09 07:01:08 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 2.1246
2022-03-09 07:01:42 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 2.1079
2022-03-09 07:02:15 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 2.0499
2022-03-09 07:02:48 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 2.3002
2022-03-09 07:03:21 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 2.0964
2022-03-09 07:03:54 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 2.0626
2022-03-09 07:04:27 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.9593
2022-03-09 07:05:01 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 2.0248
2022-03-09 07:05:34 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 2.3949
2022-03-09 07:06:08 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 2.2398
2022-03-09 07:06:40 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 2.0873
2022-03-09 07:07:13 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 2.2044
2022-03-09 07:07:47 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 2.1056
2022-03-09 07:08:23 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 2.1448
2022-03-09 07:08:25 - train: epoch 066, train_loss: 2.1754
2022-03-09 07:09:41 - eval: epoch: 066, acc1: 56.234%, acc5: 79.872%, test_loss: 1.8729, per_image_load_time: 2.617ms, per_image_inference_time: 0.222ms
2022-03-09 07:09:41 - until epoch: 066, best_acc1: 56.234%
2022-03-09 07:09:41 - epoch 067 lr: 0.0010000000000000002
2022-03-09 07:10:20 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 2.1034
2022-03-09 07:10:53 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 2.2105
2022-03-09 07:11:26 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 2.3725
2022-03-09 07:11:59 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 2.1103
2022-03-09 07:12:32 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 2.0274
2022-03-09 07:13:05 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 2.0026
2022-03-09 07:13:38 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 2.2253
2022-03-09 07:14:11 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 2.2347
2022-03-09 07:14:44 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 2.5405
2022-03-09 07:15:17 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 2.1036
2022-03-09 07:15:50 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 2.1767
2022-03-09 07:16:23 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 2.1097
2022-03-09 07:16:56 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 2.3351
2022-03-09 07:17:29 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 2.0625
2022-03-09 07:18:02 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 2.2464
2022-03-09 07:18:35 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 2.2782
2022-03-09 07:19:09 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 2.0506
2022-03-09 07:19:42 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 2.4178
2022-03-09 07:20:15 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 2.1540
2022-03-09 07:20:48 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 2.2808
2022-03-09 07:21:21 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 2.0387
2022-03-09 07:21:54 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 2.2240
2022-03-09 07:22:27 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 2.1437
2022-03-09 07:22:59 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 2.1013
2022-03-09 07:23:33 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 2.1131
2022-03-09 07:24:06 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 2.1210
2022-03-09 07:24:39 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.9747
2022-03-09 07:25:12 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 2.2675
2022-03-09 07:25:45 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 2.1458
2022-03-09 07:26:18 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 2.0567
2022-03-09 07:26:51 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 1.9892
2022-03-09 07:27:24 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 2.3421
2022-03-09 07:27:57 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.9308
2022-03-09 07:28:30 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 2.1927
2022-03-09 07:29:04 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 2.2171
2022-03-09 07:29:37 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 2.0442
2022-03-09 07:30:10 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 2.3321
2022-03-09 07:30:43 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 2.2409
2022-03-09 07:31:16 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 2.3607
2022-03-09 07:31:49 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 2.3361
2022-03-09 07:32:22 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 2.3963
2022-03-09 07:32:55 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 2.3294
2022-03-09 07:33:28 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.9621
2022-03-09 07:34:01 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 2.2322
2022-03-09 07:34:34 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 2.0773
2022-03-09 07:35:08 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.9167
2022-03-09 07:35:42 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 2.0813
2022-03-09 07:36:15 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.9361
2022-03-09 07:36:49 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 2.1503
2022-03-09 07:37:22 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 2.2328
2022-03-09 07:37:24 - train: epoch 067, train_loss: 2.1725
2022-03-09 07:38:41 - eval: epoch: 067, acc1: 56.138%, acc5: 79.844%, test_loss: 1.8726, per_image_load_time: 2.527ms, per_image_inference_time: 0.221ms
2022-03-09 07:38:41 - until epoch: 067, best_acc1: 56.234%
2022-03-09 07:38:41 - epoch 068 lr: 0.0010000000000000002
2022-03-09 07:39:21 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 2.1772
2022-03-09 07:39:54 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 2.1948
2022-03-09 07:40:27 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 2.3680
2022-03-09 07:41:00 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 2.0877
2022-03-09 07:41:33 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 2.0864
2022-03-09 07:42:06 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 2.1911
2022-03-09 07:42:39 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 2.4411
2022-03-09 07:43:12 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 2.1206
2022-03-09 07:43:44 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 2.1259
2022-03-09 07:44:17 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 2.1726
2022-03-09 07:44:50 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 2.3941
2022-03-09 07:45:23 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 2.0600
2022-03-09 07:45:56 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 2.0862
2022-03-09 07:46:29 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 2.2114
2022-03-09 07:47:03 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 2.3684
2022-03-09 07:47:36 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 2.1950
2022-03-09 07:48:09 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 2.3289
2022-03-09 07:48:42 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 2.1968
2022-03-09 07:49:16 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 2.2641
2022-03-09 07:49:49 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 2.3168
2022-03-09 07:50:22 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 2.2005
2022-03-09 07:50:55 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 2.0995
2022-03-09 07:51:29 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 2.0594
2022-03-09 07:52:02 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 2.3172
2022-03-09 07:52:35 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 2.2656
2022-03-09 07:53:08 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 2.3149
2022-03-09 07:53:41 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 2.1836
2022-03-09 07:54:15 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 2.3220
2022-03-09 07:54:48 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 2.2660
2022-03-09 07:55:22 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 2.3840
2022-03-09 07:55:55 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.9206
2022-03-09 07:56:29 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 2.4544
2022-03-09 07:57:02 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 2.2175
2022-03-09 07:57:35 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 2.0506
2022-03-09 07:58:08 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 2.2917
2022-03-09 07:58:41 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 2.0583
2022-03-09 07:59:15 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 2.1530
2022-03-09 07:59:48 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 2.3142
2022-03-09 08:00:21 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 2.2438
2022-03-09 08:00:54 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 2.2686
2022-03-09 08:01:27 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.9554
2022-03-09 08:02:01 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 2.2827
2022-03-09 08:02:34 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 2.2584
2022-03-09 08:03:07 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 2.2011
2022-03-09 08:03:41 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 2.2743
2022-03-09 08:04:14 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 2.2821
2022-03-09 08:04:48 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 2.3095
2022-03-09 08:05:21 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 2.2550
2022-03-09 08:05:54 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 2.3790
2022-03-09 08:06:28 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 2.2856
2022-03-09 08:06:30 - train: epoch 068, train_loss: 2.1708
2022-03-09 08:07:46 - eval: epoch: 068, acc1: 56.278%, acc5: 79.936%, test_loss: 1.8670, per_image_load_time: 2.418ms, per_image_inference_time: 0.220ms
2022-03-09 08:07:46 - until epoch: 068, best_acc1: 56.278%
2022-03-09 08:07:46 - epoch 069 lr: 0.0010000000000000002
2022-03-09 08:08:26 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 2.4433
2022-03-09 08:08:59 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 2.4150
2022-03-09 08:09:32 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 2.0686
2022-03-09 08:10:05 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.9220
2022-03-09 08:10:38 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 2.0479
2022-03-09 08:11:11 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 2.0192
2022-03-09 08:11:44 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 2.1627
2022-03-09 08:12:17 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 2.1788
2022-03-09 08:12:50 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 2.0564
2022-03-09 08:13:22 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 2.0380
2022-03-09 08:13:56 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 2.1375
2022-03-09 08:14:29 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 2.0281
2022-03-09 08:15:02 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 2.4459
2022-03-09 08:15:35 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 2.2270
2022-03-09 08:16:09 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 2.0695
2022-03-09 08:16:42 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 2.2356
2022-03-09 08:17:15 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 2.0917
2022-03-09 08:17:48 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.8967
2022-03-09 08:18:21 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 2.2239
2022-03-09 08:18:55 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 1.9457
2022-03-09 08:19:28 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 2.1712
2022-03-09 08:20:01 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 2.0121
2022-03-09 08:20:34 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 2.0479
2022-03-09 08:21:07 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 2.2389
2022-03-09 08:21:40 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 2.0220
2022-03-09 08:22:14 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 2.1103
2022-03-09 08:22:47 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 2.5145
2022-03-09 08:23:20 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 2.2060
2022-03-09 08:23:53 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 2.0231
2022-03-09 08:24:27 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.9874
2022-03-09 08:25:00 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 2.0772
2022-03-09 08:25:34 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 2.1745
2022-03-09 08:26:07 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 2.1035
2022-03-09 08:26:40 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 2.1575
2022-03-09 08:27:13 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 2.0113
2022-03-09 08:27:47 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.9188
2022-03-09 08:28:20 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 2.1789
2022-03-09 08:28:53 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 2.2658
2022-03-09 08:29:26 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 2.1747
2022-03-09 08:29:59 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 2.2170
2022-03-09 08:30:33 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 2.1858
2022-03-09 08:31:06 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 2.1387
2022-03-09 08:31:39 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 2.2533
2022-03-09 08:32:13 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 2.1872
2022-03-09 08:32:46 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 2.2513
2022-03-09 08:33:19 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 2.3085
2022-03-09 08:33:53 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 2.1770
2022-03-09 08:34:26 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 2.0696
2022-03-09 08:35:00 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 2.0819
2022-03-09 08:35:33 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 2.3986
2022-03-09 08:35:35 - train: epoch 069, train_loss: 2.1676
2022-03-09 08:36:51 - eval: epoch: 069, acc1: 56.334%, acc5: 79.992%, test_loss: 1.8656, per_image_load_time: 1.806ms, per_image_inference_time: 0.231ms
2022-03-09 08:36:51 - until epoch: 069, best_acc1: 56.334%
2022-03-09 08:36:51 - epoch 070 lr: 0.0010000000000000002
2022-03-09 08:37:31 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 2.2673
2022-03-09 08:38:04 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 2.2862
2022-03-09 08:38:36 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 2.1945
2022-03-09 08:39:09 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 2.2059
2022-03-09 08:39:42 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 2.2467
2022-03-09 08:40:15 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 2.1526
2022-03-09 08:40:47 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 2.1284
2022-03-09 08:41:21 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 2.0246
2022-03-09 08:41:54 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 2.2219
2022-03-09 08:42:27 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 2.1320
2022-03-09 08:43:00 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 2.5123
2022-03-09 08:43:33 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 2.0398
2022-03-09 08:44:05 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 2.2605
2022-03-09 08:44:39 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 2.1427
2022-03-09 08:45:12 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 2.1161
2022-03-09 08:45:45 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 2.2824
2022-03-09 08:46:18 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 2.2026
2022-03-09 08:46:51 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.9710
2022-03-09 08:47:24 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 2.1580
2022-03-09 08:47:57 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 2.2315
2022-03-09 08:48:30 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 2.2495
2022-03-09 08:49:04 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 2.2721
2022-03-09 08:49:37 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 2.2169
2022-03-09 08:50:10 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 2.2357
2022-03-09 08:50:44 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 2.2570
2022-03-09 08:51:17 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 2.2616
2022-03-09 08:51:50 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 2.1740
2022-03-09 08:52:24 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 2.2508
2022-03-09 08:52:57 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 2.2482
2022-03-09 08:53:29 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 2.1170
2022-03-09 08:54:02 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 2.2304
2022-03-09 08:54:35 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 2.3722
2022-03-09 08:55:08 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 2.0546
2022-03-09 08:55:41 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 2.1511
2022-03-09 08:56:14 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 2.2727
2022-03-09 08:56:47 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 2.2556
2022-03-09 08:57:20 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 2.0836
2022-03-09 08:57:54 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 2.1377
2022-03-09 08:58:27 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 1.9999
2022-03-09 08:59:00 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 2.0485
2022-03-09 08:59:33 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 2.1074
2022-03-09 09:00:07 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 2.0947
2022-03-09 09:00:40 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 2.3275
2022-03-09 09:01:13 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 2.2100
2022-03-09 09:01:47 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 2.1272
2022-03-09 09:02:20 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 2.3168
2022-03-09 09:02:54 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 2.2097
2022-03-09 09:03:27 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 2.2749
2022-03-09 09:04:00 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 2.3022
2022-03-09 09:04:33 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 2.2122
2022-03-09 09:04:36 - train: epoch 070, train_loss: 2.1661
2022-03-09 09:05:52 - eval: epoch: 070, acc1: 56.304%, acc5: 79.962%, test_loss: 1.8660, per_image_load_time: 2.635ms, per_image_inference_time: 0.213ms
2022-03-09 09:05:52 - until epoch: 070, best_acc1: 56.334%
2022-03-09 09:05:52 - epoch 071 lr: 0.0010000000000000002
2022-03-09 09:06:31 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.9935
2022-03-09 09:07:04 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 2.0294
2022-03-09 09:07:37 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 2.2516
2022-03-09 09:08:10 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 2.3445
2022-03-09 09:08:43 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 2.4457
2022-03-09 09:09:16 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 2.2541
2022-03-09 09:09:49 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 2.3587
2022-03-09 09:10:22 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 2.2607
2022-03-09 09:10:56 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 2.3332
2022-03-09 09:11:29 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 2.2734
2022-03-09 09:12:02 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 2.2761
2022-03-09 09:12:35 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 2.1265
2022-03-09 09:13:08 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 2.1492
2022-03-09 09:13:41 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 2.1177
2022-03-09 09:14:14 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 1.9442
2022-03-09 09:14:48 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 2.0200
2022-03-09 09:15:21 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 2.1910
2022-03-09 09:15:54 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 2.0624
2022-03-09 09:16:27 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 2.0841
2022-03-09 09:17:00 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 2.2491
2022-03-09 09:17:34 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.9920
2022-03-09 09:18:07 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 1.9968
2022-03-09 09:18:40 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 2.1458
2022-03-09 09:19:13 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 2.1243
2022-03-09 09:19:47 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 2.2820
2022-03-09 09:20:20 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.9202
2022-03-09 09:20:53 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 2.2412
2022-03-09 09:21:26 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 2.2717
2022-03-09 09:21:59 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 2.0826
2022-03-09 09:22:33 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 2.2788
2022-03-09 09:23:06 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 2.2024
2022-03-09 09:23:39 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.9697
2022-03-09 09:24:12 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 2.0859
2022-03-09 09:24:45 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 2.0145
2022-03-09 09:25:18 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 2.2719
2022-03-09 09:25:51 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 2.3232
2022-03-09 09:26:24 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 2.1566
2022-03-09 09:26:58 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 2.0284
2022-03-09 09:27:31 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 2.1154
2022-03-09 09:28:04 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 2.3014
2022-03-09 09:28:38 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 2.0412
2022-03-09 09:29:11 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 2.3113
2022-03-09 09:29:44 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 2.1340
2022-03-09 09:30:18 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 2.1364
2022-03-09 09:30:51 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 2.1516
2022-03-09 09:31:25 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 2.1764
2022-03-09 09:31:59 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 2.1067
2022-03-09 09:32:34 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 2.1080
2022-03-09 09:33:07 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 2.0465
2022-03-09 09:33:40 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 2.0576
2022-03-09 09:33:43 - train: epoch 071, train_loss: 2.1634
2022-03-09 09:34:58 - eval: epoch: 071, acc1: 56.302%, acc5: 79.990%, test_loss: 1.8653, per_image_load_time: 1.626ms, per_image_inference_time: 0.227ms
2022-03-09 09:34:59 - until epoch: 071, best_acc1: 56.334%
2022-03-09 09:34:59 - epoch 072 lr: 0.0010000000000000002
2022-03-09 09:35:38 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 2.3235
2022-03-09 09:36:11 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.9990
2022-03-09 09:36:44 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 2.1283
2022-03-09 09:37:18 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 2.1303
2022-03-09 09:37:51 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 2.0266
2022-03-09 09:38:25 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.9758
2022-03-09 09:38:58 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 2.1348
2022-03-09 09:39:32 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 2.4262
2022-03-09 09:40:05 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.9522
2022-03-09 09:40:39 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 2.1609
2022-03-09 09:41:12 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 2.4022
2022-03-09 09:41:46 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 2.0321
2022-03-09 09:42:19 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 2.1309
2022-03-09 09:42:53 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 2.2436
2022-03-09 09:43:26 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 2.0861
2022-03-09 09:44:00 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 2.3024
2022-03-09 09:44:33 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 2.0728
2022-03-09 09:45:07 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.9828
2022-03-09 09:45:40 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 2.1912
2022-03-09 09:46:13 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 2.2552
2022-03-09 09:46:47 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 2.2606
2022-03-09 09:47:20 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 2.2340
2022-03-09 09:47:54 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 2.3066
2022-03-09 09:48:27 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 2.1017
2022-03-09 09:49:01 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.9805
2022-03-09 09:49:34 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 2.0725
2022-03-09 09:50:07 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 2.1099
2022-03-09 09:50:41 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 2.2994
2022-03-09 09:51:14 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 2.1081
2022-03-09 09:51:48 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 2.0499
2022-03-09 09:52:21 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 2.1724
2022-03-09 09:52:54 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 2.0736
2022-03-09 09:53:28 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 2.1541
2022-03-09 09:54:01 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 2.3603
2022-03-09 09:54:34 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 2.1598
2022-03-09 09:55:07 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 2.1771
2022-03-09 09:55:41 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 2.1456
2022-03-09 09:56:14 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 2.3043
2022-03-09 09:56:48 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 2.2659
2022-03-09 09:57:21 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.9451
2022-03-09 09:57:54 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 2.3182
2022-03-09 09:58:28 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 2.2428
2022-03-09 09:59:01 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 2.1242
2022-03-09 09:59:35 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 2.0144
2022-03-09 10:00:08 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 2.3268
2022-03-09 10:00:42 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.9872
2022-03-09 10:01:16 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 2.2835
2022-03-09 10:01:50 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 2.2727
2022-03-09 10:02:23 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 2.2812
2022-03-09 10:02:56 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 2.0986
2022-03-09 10:02:58 - train: epoch 072, train_loss: 2.1634
2022-03-09 10:04:14 - eval: epoch: 072, acc1: 56.408%, acc5: 79.898%, test_loss: 1.8599, per_image_load_time: 1.284ms, per_image_inference_time: 0.215ms
2022-03-09 10:04:14 - until epoch: 072, best_acc1: 56.408%
2022-03-09 10:04:14 - epoch 073 lr: 0.0010000000000000002
2022-03-09 10:04:54 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 2.5725
2022-03-09 10:05:27 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 2.1394
2022-03-09 10:06:00 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 2.1873
2022-03-09 10:06:33 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 1.8559
2022-03-09 10:07:07 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 2.0367
2022-03-09 10:07:40 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 2.0368
2022-03-09 10:08:13 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 2.1705
2022-03-09 10:08:46 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 2.1119
2022-03-09 10:09:20 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.9725
2022-03-09 10:09:53 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 2.0864
2022-03-09 10:10:26 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 2.2792
2022-03-09 10:10:59 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 2.1335
2022-03-09 10:11:33 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 2.1430
2022-03-09 10:12:06 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 2.2058
2022-03-09 10:12:39 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 2.1687
2022-03-09 10:13:12 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 2.2428
2022-03-09 10:13:45 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 2.4552
2022-03-09 10:14:19 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 1.9297
2022-03-09 10:14:52 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 2.2783
2022-03-09 10:15:25 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.8851
2022-03-09 10:15:58 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 2.2152
2022-03-09 10:16:31 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 2.2663
2022-03-09 10:17:04 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 2.1571
2022-03-09 10:17:37 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 2.1777
2022-03-09 10:18:10 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 2.3559
2022-03-09 10:18:43 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 2.1511
2022-03-09 10:19:17 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 2.2653
2022-03-09 10:19:50 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 2.1633
2022-03-09 10:20:23 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 2.2625
2022-03-09 10:20:56 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 1.8806
2022-03-09 10:21:29 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 2.2046
2022-03-09 10:22:02 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 2.0117
2022-03-09 10:22:35 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 2.4583
2022-03-09 10:23:09 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 2.1399
2022-03-09 10:23:42 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 2.2797
2022-03-09 10:24:15 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 2.0392
2022-03-09 10:24:49 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 2.2045
2022-03-09 10:25:22 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 2.4177
2022-03-09 10:25:55 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 2.1176
2022-03-09 10:26:28 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 2.4336
2022-03-09 10:27:01 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 2.1299
2022-03-09 10:27:35 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 2.4094
2022-03-09 10:28:08 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 2.0639
2022-03-09 10:28:41 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 2.2231
2022-03-09 10:29:14 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 2.0856
2022-03-09 10:29:47 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 2.3355
2022-03-09 10:30:21 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.9059
2022-03-09 10:31:01 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 2.2943
2022-03-09 10:31:36 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 2.0380
2022-03-09 10:32:09 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 2.3198
2022-03-09 10:32:11 - train: epoch 073, train_loss: 2.1622
2022-03-09 10:33:28 - eval: epoch: 073, acc1: 56.328%, acc5: 79.988%, test_loss: 1.8574, per_image_load_time: 2.101ms, per_image_inference_time: 0.251ms
2022-03-09 10:33:28 - until epoch: 073, best_acc1: 56.408%
2022-03-09 10:33:28 - epoch 074 lr: 0.0010000000000000002
2022-03-09 10:34:08 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 2.0541
2022-03-09 10:34:41 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 2.3959
2022-03-09 10:35:15 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 2.2070
2022-03-09 10:35:48 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 2.5000
2022-03-09 10:36:21 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 2.0605
2022-03-09 10:36:54 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 2.2517
2022-03-09 10:37:27 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 2.2174
2022-03-09 10:38:00 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 2.1813
2022-03-09 10:38:33 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.9552
2022-03-09 10:39:06 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 2.3079
2022-03-09 10:39:38 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 2.4084
2022-03-09 10:40:11 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 2.0738
2022-03-09 10:40:44 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 2.1536
2022-03-09 10:41:17 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 2.0623
2022-03-09 10:41:50 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 2.2138
2022-03-09 10:42:23 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 1.9797
2022-03-09 10:42:56 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 2.2136
2022-03-09 10:43:29 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 2.4509
2022-03-09 10:44:01 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 2.2271
2022-03-09 10:44:34 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 1.9170
2022-03-09 10:45:07 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 2.1186
2022-03-09 10:45:40 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 2.4249
2022-03-09 10:46:12 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 2.2241
2022-03-09 10:46:45 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 2.2331
2022-03-09 10:47:18 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.9406
2022-03-09 10:47:51 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.9281
2022-03-09 10:48:24 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 2.1728
2022-03-09 10:48:56 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 2.5262
2022-03-09 10:49:30 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 2.1583
2022-03-09 10:50:02 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 2.2169
2022-03-09 10:50:35 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 2.2496
2022-03-09 10:51:08 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 2.0409
2022-03-09 10:51:41 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 2.2256
2022-03-09 10:52:14 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 2.1149
2022-03-09 10:52:47 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.9827
2022-03-09 10:53:20 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 2.2136
2022-03-09 10:53:53 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 2.1976
2022-03-09 10:54:26 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 2.0942
2022-03-09 10:54:59 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 1.9389
2022-03-09 10:55:32 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 2.0446
2022-03-09 10:56:05 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 2.2704
2022-03-09 10:56:37 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 2.2087
2022-03-09 10:57:10 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 2.1396
2022-03-09 10:57:43 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 2.1086
2022-03-09 10:58:16 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 1.9276
2022-03-09 10:58:49 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 2.3151
2022-03-09 10:59:22 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 2.2104
2022-03-09 10:59:54 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 2.3516
2022-03-09 11:00:27 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 2.2812
2022-03-09 11:01:00 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 2.1390
2022-03-09 11:01:03 - train: epoch 074, train_loss: 2.1588
2022-03-09 11:02:19 - eval: epoch: 074, acc1: 56.302%, acc5: 80.066%, test_loss: 1.8572, per_image_load_time: 2.658ms, per_image_inference_time: 0.229ms
2022-03-09 11:02:19 - until epoch: 074, best_acc1: 56.408%
2022-03-09 11:02:19 - epoch 075 lr: 0.0010000000000000002
2022-03-09 11:02:58 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 2.2988
2022-03-09 11:03:32 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 2.1286
2022-03-09 11:04:05 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 2.3023
2022-03-09 11:04:37 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 2.2367
2022-03-09 11:05:10 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.9488
2022-03-09 11:05:43 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 2.1788
2022-03-09 11:06:16 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 2.3222
2022-03-09 11:06:48 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 2.3601
2022-03-09 11:07:21 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 2.1537
2022-03-09 11:07:54 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 1.8768
2022-03-09 11:08:27 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 2.3312
2022-03-09 11:09:01 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 2.0290
2022-03-09 11:09:34 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.9781
2022-03-09 11:10:07 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 2.1075
2022-03-09 11:10:40 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 2.4273
2022-03-09 11:11:13 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 1.8557
2022-03-09 11:11:46 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 2.0413
2022-03-09 11:12:19 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 2.2391
2022-03-09 11:12:52 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 2.0199
2022-03-09 11:13:25 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 2.0998
2022-03-09 11:13:59 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 2.0215
2022-03-09 11:14:32 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 2.2116
2022-03-09 11:15:05 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 2.0021
2022-03-09 11:15:38 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 2.2526
2022-03-09 11:16:11 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 2.2591
2022-03-09 11:16:44 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 2.2001
2022-03-09 11:17:17 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 1.9126
2022-03-09 11:17:49 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 2.2196
2022-03-09 11:18:21 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 2.2974
2022-03-09 11:18:54 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 2.3698
2022-03-09 11:19:27 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 2.3530
2022-03-09 11:20:00 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 2.0067
2022-03-09 11:20:34 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 2.2508
2022-03-09 11:21:06 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 2.0420
2022-03-09 11:21:39 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 2.0026
2022-03-09 11:22:12 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 2.1774
2022-03-09 11:22:45 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 2.1204
2022-03-09 11:23:18 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 2.0340
2022-03-09 11:23:51 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 2.4712
2022-03-09 11:24:24 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 2.0556
2022-03-09 11:24:58 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 1.9705
2022-03-09 11:25:30 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 2.2864
2022-03-09 11:26:04 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 2.3451
2022-03-09 11:26:36 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 2.0481
2022-03-09 11:27:09 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 2.1344
2022-03-09 11:27:42 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 1.9165
2022-03-09 11:28:15 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 2.3464
2022-03-09 11:28:48 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 2.1466
2022-03-09 11:29:22 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.8654
2022-03-09 11:29:53 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 2.1696
2022-03-09 11:29:56 - train: epoch 075, train_loss: 2.1585
2022-03-09 11:31:12 - eval: epoch: 075, acc1: 56.338%, acc5: 79.936%, test_loss: 1.8587, per_image_load_time: 2.672ms, per_image_inference_time: 0.227ms
2022-03-09 11:31:12 - until epoch: 075, best_acc1: 56.408%
2022-03-09 11:31:12 - epoch 076 lr: 0.0010000000000000002
2022-03-09 11:31:51 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 2.0022
2022-03-09 11:32:24 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 2.1640
2022-03-09 11:32:57 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 2.1439
2022-03-09 11:33:30 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 2.0670
2022-03-09 11:34:03 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 2.2046
2022-03-09 11:34:36 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 2.1035
2022-03-09 11:35:09 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 2.0585
2022-03-09 11:35:41 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 2.1264
2022-03-09 11:36:15 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 2.0565
2022-03-09 11:36:47 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 2.1323
2022-03-09 11:37:20 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.9642
2022-03-09 11:37:53 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 2.0459
2022-03-09 11:38:27 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 2.2698
2022-03-09 11:39:00 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 2.0207
2022-03-09 11:39:33 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 2.1638
2022-03-09 11:40:06 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 2.0857
2022-03-09 11:40:39 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 2.0243
2022-03-09 11:41:12 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 1.7809
2022-03-09 11:41:45 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 2.2175
2022-03-09 11:42:18 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 2.1178
2022-03-09 11:42:51 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 2.3394
2022-03-09 11:43:24 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 2.2824
2022-03-09 11:43:57 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 2.1185
2022-03-09 11:44:30 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 2.3825
2022-03-09 11:45:03 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 2.1156
2022-03-09 11:45:37 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 2.4898
2022-03-09 11:46:10 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 2.2883
2022-03-09 11:46:43 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 2.0586
2022-03-09 11:47:16 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 2.2123
2022-03-09 11:47:49 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 2.1144
2022-03-09 11:48:22 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 2.1687
2022-03-09 11:48:55 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 2.1902
2022-03-09 11:49:28 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.9320
2022-03-09 11:50:02 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 2.1057
2022-03-09 11:50:35 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 1.9024
2022-03-09 11:51:08 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 1.9317
2022-03-09 11:51:41 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 2.1658
2022-03-09 11:52:14 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 1.9609
2022-03-09 11:52:47 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 2.0728
2022-03-09 11:53:19 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 2.2251
2022-03-09 11:53:53 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 2.1295
2022-03-09 11:54:26 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 2.3575
2022-03-09 11:54:59 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 2.2144
2022-03-09 11:55:33 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 2.0191
2022-03-09 11:56:05 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 2.2070
2022-03-09 11:56:38 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 2.0497
2022-03-09 11:57:11 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 2.3021
2022-03-09 11:57:45 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 1.9539
2022-03-09 11:58:19 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 2.1257
2022-03-09 11:58:52 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 2.1013
2022-03-09 11:58:55 - train: epoch 076, train_loss: 2.1558
2022-03-09 12:00:10 - eval: epoch: 076, acc1: 56.480%, acc5: 80.084%, test_loss: 1.8544, per_image_load_time: 1.607ms, per_image_inference_time: 0.253ms
2022-03-09 12:00:10 - until epoch: 076, best_acc1: 56.480%
2022-03-09 12:00:10 - epoch 077 lr: 0.0010000000000000002
2022-03-09 12:00:50 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 2.2710
2022-03-09 12:01:22 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 2.3527
2022-03-09 12:01:55 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 1.8659
2022-03-09 12:02:28 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 2.2453
2022-03-09 12:03:01 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 2.0125
2022-03-09 12:03:34 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 1.8919
2022-03-09 12:04:07 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 2.1858
2022-03-09 12:04:40 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 2.2903
2022-03-09 12:05:14 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 2.4034
2022-03-09 12:05:47 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 1.8249
2022-03-09 12:06:20 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 2.3290
2022-03-09 12:06:54 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 2.2873
2022-03-09 12:07:27 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 1.9152
2022-03-09 12:08:00 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 2.0393
2022-03-09 12:08:33 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 2.0074
2022-03-09 12:09:06 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 2.1908
2022-03-09 12:09:39 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 2.4604
2022-03-09 12:10:13 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 1.9833
2022-03-09 12:10:46 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 1.9177
2022-03-09 12:11:19 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 2.1446
2022-03-09 12:11:52 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 2.4138
2022-03-09 12:12:26 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 2.2335
2022-03-09 12:12:59 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 2.2994
2022-03-09 12:13:32 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 2.0091
2022-03-09 12:14:05 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 2.4202
2022-03-09 12:14:39 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 1.8691
2022-03-09 12:15:12 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 2.0861
2022-03-09 12:15:45 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 2.2076
2022-03-09 12:16:18 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 2.5024
2022-03-09 12:16:52 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 2.0502
2022-03-09 12:17:25 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 2.1248
2022-03-09 12:17:58 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 2.2069
2022-03-09 12:18:31 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 2.0011
2022-03-09 12:19:04 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 2.3992
2022-03-09 12:19:37 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.8632
2022-03-09 12:20:10 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 2.1535
2022-03-09 12:20:44 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 2.2165
2022-03-09 12:21:17 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 2.1142
2022-03-09 12:21:50 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 2.1752
2022-03-09 12:22:24 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 2.2043
2022-03-09 12:22:57 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 2.2449
2022-03-09 12:23:30 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 2.3322
2022-03-09 12:24:03 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 2.0382
2022-03-09 12:24:37 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 2.1642
2022-03-09 12:25:10 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 2.3264
2022-03-09 12:25:43 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 2.0970
2022-03-09 12:26:16 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 1.9057
2022-03-09 12:26:49 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 2.1625
2022-03-09 12:27:23 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 2.2345
2022-03-09 12:27:55 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 2.3521
2022-03-09 12:27:58 - train: epoch 077, train_loss: 2.1538
2022-03-09 12:29:14 - eval: epoch: 077, acc1: 56.494%, acc5: 80.046%, test_loss: 1.8534, per_image_load_time: 2.187ms, per_image_inference_time: 0.232ms
2022-03-09 12:29:14 - until epoch: 077, best_acc1: 56.494%
2022-03-09 12:29:14 - epoch 078 lr: 0.0010000000000000002
2022-03-09 12:29:53 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 2.2173
2022-03-09 12:30:26 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 2.2098
2022-03-09 12:30:59 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 2.2243
2022-03-09 12:31:32 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 2.1804
2022-03-09 12:32:05 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 2.1632
2022-03-09 12:32:38 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 2.0006
2022-03-09 12:33:11 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 2.2935
2022-03-09 12:33:44 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 2.3489
2022-03-09 12:34:17 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 2.2007
2022-03-09 12:34:50 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 2.1787
2022-03-09 12:35:23 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 1.9479
2022-03-09 12:35:56 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 1.9139
2022-03-09 12:36:29 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 2.2470
2022-03-09 12:37:02 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 1.9026
2022-03-09 12:37:35 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 2.2101
2022-03-09 12:38:09 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 2.2552
2022-03-09 12:38:42 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 2.3075
2022-03-09 12:39:15 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 2.1567
2022-03-09 12:39:49 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 2.0579
2022-03-09 12:40:22 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 2.0592
2022-03-09 12:40:55 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 2.3664
2022-03-09 12:41:28 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 2.1355
2022-03-09 12:42:02 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 2.3551
2022-03-09 12:42:35 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 2.0502
2022-03-09 12:43:08 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.9243
2022-03-09 12:43:41 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 2.0730
2022-03-09 12:44:14 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 2.0341
2022-03-09 12:44:47 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 2.1579
2022-03-09 12:45:21 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 2.0824
2022-03-09 12:45:54 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 2.2612
2022-03-09 12:46:26 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 2.3303
2022-03-09 12:46:59 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 2.1600
2022-03-09 12:47:32 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 2.4052
2022-03-09 12:48:05 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 2.0856
2022-03-09 12:48:38 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 2.0488
2022-03-09 12:49:11 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 2.2437
2022-03-09 12:49:45 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 2.0320
2022-03-09 12:50:18 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 2.0401
2022-03-09 12:50:51 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 2.1311
2022-03-09 12:51:24 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 2.2176
2022-03-09 12:51:57 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 2.1697
2022-03-09 12:52:30 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 2.3475
2022-03-09 12:53:03 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 2.1368
2022-03-09 12:53:37 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 2.0470
2022-03-09 12:54:10 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 2.1629
2022-03-09 12:54:43 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 1.9431
2022-03-09 12:55:16 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 2.0790
2022-03-09 12:55:50 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 2.5373
2022-03-09 12:56:23 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 2.2087
2022-03-09 12:56:55 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 2.1339
2022-03-09 12:56:58 - train: epoch 078, train_loss: 2.1513
2022-03-09 12:58:14 - eval: epoch: 078, acc1: 56.568%, acc5: 80.118%, test_loss: 1.8520, per_image_load_time: 2.681ms, per_image_inference_time: 0.232ms
2022-03-09 12:58:14 - until epoch: 078, best_acc1: 56.568%
2022-03-09 12:58:14 - epoch 079 lr: 0.0010000000000000002
2022-03-09 12:58:53 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 2.0802
2022-03-09 12:59:26 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 2.0954
2022-03-09 12:59:59 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 2.3084
2022-03-09 13:00:32 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 2.3136
2022-03-09 13:01:05 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 2.1420
2022-03-09 13:01:38 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 2.0839
2022-03-09 13:02:11 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 2.0995
2022-03-09 13:02:44 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 2.1238
2022-03-09 13:03:17 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 2.1711
2022-03-09 13:03:50 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 2.1542
2022-03-09 13:04:23 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 2.3251
2022-03-09 13:04:56 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 2.4171
2022-03-09 13:05:29 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 1.9808
2022-03-09 13:06:02 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 2.0825
2022-03-09 13:06:35 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 2.1345
2022-03-09 13:07:08 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 1.9167
2022-03-09 13:07:41 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 2.0271
2022-03-09 13:08:14 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 2.1469
2022-03-09 13:08:47 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 2.0781
2022-03-09 13:09:20 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 2.3386
2022-03-09 13:09:53 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 2.0109
2022-03-09 13:10:26 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 2.2238
2022-03-09 13:10:59 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 2.1914
2022-03-09 13:11:33 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 2.0727
2022-03-09 13:12:06 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 2.1094
2022-03-09 13:12:39 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 2.0241
2022-03-09 13:13:12 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 1.8409
2022-03-09 13:13:45 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 1.9433
2022-03-09 13:14:18 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 2.0386
2022-03-09 13:14:51 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 2.1044
2022-03-09 13:15:24 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 2.0610
2022-03-09 13:15:57 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 2.4490
2022-03-09 13:16:30 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 2.1218
2022-03-09 13:17:03 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 2.0221
2022-03-09 13:17:36 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 2.3417
2022-03-09 13:18:09 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 2.0818
2022-03-09 13:18:42 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 1.8938
2022-03-09 13:19:15 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 2.1895
2022-03-09 13:19:49 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 2.0077
2022-03-09 13:20:22 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 1.9847
2022-03-09 13:20:55 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 2.1860
2022-03-09 13:21:28 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.9280
2022-03-09 13:22:01 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 1.8801
2022-03-09 13:22:34 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 2.4586
2022-03-09 13:23:07 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 2.4858
2022-03-09 13:23:41 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 2.3397
2022-03-09 13:24:14 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 2.2170
2022-03-09 13:24:47 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 2.2365
2022-03-09 13:25:20 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 2.3824
2022-03-09 13:25:52 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 1.9275
2022-03-09 13:25:55 - train: epoch 079, train_loss: 2.1529
2022-03-09 13:27:10 - eval: epoch: 079, acc1: 56.550%, acc5: 80.154%, test_loss: 1.8544, per_image_load_time: 2.596ms, per_image_inference_time: 0.268ms
2022-03-09 13:27:10 - until epoch: 079, best_acc1: 56.568%
2022-03-09 13:27:10 - epoch 080 lr: 0.0010000000000000002
2022-03-09 13:27:49 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 2.0487
2022-03-09 13:28:22 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 2.0834
2022-03-09 13:28:55 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 2.1309
2022-03-09 13:29:28 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 1.9566
2022-03-09 13:30:01 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 2.1805
2022-03-09 13:30:34 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 2.0806
2022-03-09 13:31:07 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 2.1034
2022-03-09 13:31:40 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 2.0087
2022-03-09 13:32:13 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 2.1285
2022-03-09 13:32:46 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 2.0233
2022-03-09 13:33:19 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 2.1479
2022-03-09 13:33:52 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 2.0714
2022-03-09 13:34:26 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 2.0610
2022-03-09 13:34:59 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 2.1382
2022-03-09 13:35:32 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 1.9928
2022-03-09 13:36:05 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 2.0715
2022-03-09 13:36:37 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 2.1589
2022-03-09 13:37:10 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 2.1884
2022-03-09 13:37:42 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 2.1934
2022-03-09 13:38:15 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 2.2140
2022-03-09 13:38:48 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 2.2211
2022-03-09 13:39:21 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 2.1580
2022-03-09 13:39:54 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 2.1004
2022-03-09 13:40:27 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 2.2039
2022-03-09 13:41:00 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 2.0542
2022-03-09 13:41:33 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 2.1868
2022-03-09 13:42:06 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 2.3165
2022-03-09 13:42:39 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 2.1454
2022-03-09 13:43:12 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 2.1089
2022-03-09 13:43:45 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 2.1138
2022-03-09 13:44:18 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 2.3304
2022-03-09 13:44:51 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 1.9215
2022-03-09 13:45:23 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 1.9971
2022-03-09 13:45:57 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 2.0405
2022-03-09 13:46:30 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 1.9415
2022-03-09 13:47:03 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 2.1497
2022-03-09 13:47:36 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 2.1660
2022-03-09 13:48:08 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 2.1790
2022-03-09 13:48:42 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 1.9718
2022-03-09 13:49:15 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 2.1683
2022-03-09 13:49:48 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 2.1408
2022-03-09 13:50:21 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 2.1110
2022-03-09 13:50:55 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 2.4413
2022-03-09 13:51:28 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 2.0299
2022-03-09 13:52:02 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 2.0003
2022-03-09 13:52:36 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 2.2101
2022-03-09 13:53:09 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 2.3474
2022-03-09 13:53:43 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 2.0847
2022-03-09 13:54:16 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 2.3945
2022-03-09 13:54:49 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 2.0617
2022-03-09 13:54:52 - train: epoch 080, train_loss: 2.1507
2022-03-09 13:56:07 - eval: epoch: 080, acc1: 56.484%, acc5: 80.042%, test_loss: 1.8540, per_image_load_time: 1.062ms, per_image_inference_time: 0.253ms
2022-03-09 13:56:08 - until epoch: 080, best_acc1: 56.568%
2022-03-09 13:56:08 - epoch 081 lr: 0.0010000000000000002
2022-03-09 13:56:47 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 2.1404
2022-03-09 13:57:20 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 2.1311
2022-03-09 13:57:53 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 2.1582
2022-03-09 13:58:25 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 2.2513
2022-03-09 13:58:58 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 2.3438
2022-03-09 13:59:31 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 2.1527
2022-03-09 14:00:04 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 2.1489
2022-03-09 14:00:37 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 2.3853
2022-03-09 14:01:10 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 2.2242
2022-03-09 14:01:43 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 2.1956
2022-03-09 14:02:16 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 2.0817
2022-03-09 14:02:49 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 2.2185
2022-03-09 14:03:22 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 2.0310
2022-03-09 14:03:55 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 1.9335
2022-03-09 14:04:28 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 2.3560
2022-03-09 14:05:01 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 1.9970
2022-03-09 14:05:34 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 2.1750
2022-03-09 14:06:07 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 2.1007
2022-03-09 14:06:40 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 2.1112
2022-03-09 14:07:13 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 2.1879
2022-03-09 14:07:46 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 2.2203
2022-03-09 14:08:19 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 2.1875
2022-03-09 14:08:52 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 2.0242
2022-03-09 14:09:25 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 2.1753
2022-03-09 14:09:59 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 2.3888
2022-03-09 14:10:32 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 2.3956
2022-03-09 14:11:05 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 2.1635
2022-03-09 14:11:38 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 2.0529
2022-03-09 14:12:11 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 2.0826
2022-03-09 14:12:45 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 2.1577
2022-03-09 14:13:18 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 2.0290
2022-03-09 14:13:51 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 2.1945
2022-03-09 14:14:24 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 1.9983
2022-03-09 14:14:57 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 2.4046
2022-03-09 14:15:31 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 2.0093
2022-03-09 14:16:04 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 2.1734
2022-03-09 14:16:37 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 2.2184
2022-03-09 14:17:10 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 2.0292
2022-03-09 14:17:43 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 2.3287
2022-03-09 14:18:16 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 1.9839
2022-03-09 14:18:49 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 2.0824
2022-03-09 14:19:22 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 2.1337
2022-03-09 14:19:55 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 2.2349
2022-03-09 14:20:29 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 2.3226
2022-03-09 14:21:02 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 2.3115
2022-03-09 14:21:35 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 2.1465
2022-03-09 14:22:10 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 2.2540
2022-03-09 14:22:58 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 2.2170
2022-03-09 14:23:48 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 2.0960
2022-03-09 14:24:42 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 1.9103
2022-03-09 14:24:44 - train: epoch 081, train_loss: 2.1499
2022-03-09 14:26:21 - eval: epoch: 081, acc1: 56.520%, acc5: 80.174%, test_loss: 1.8501, per_image_load_time: 1.525ms, per_image_inference_time: 0.156ms
2022-03-09 14:26:21 - until epoch: 081, best_acc1: 56.568%
2022-03-09 14:26:21 - epoch 082 lr: 0.0010000000000000002
2022-03-09 14:27:00 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 1.9104
2022-03-09 14:27:34 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 2.0498
2022-03-09 14:28:07 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 2.2436
2022-03-09 14:28:40 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 2.2656
2022-03-09 14:29:13 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 2.1535
2022-03-09 14:29:46 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 2.0994
2022-03-09 14:30:19 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 2.3090
2022-03-09 14:30:51 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 2.2262
2022-03-09 14:31:24 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 2.2882
2022-03-09 14:31:57 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 2.4299
2022-03-09 14:32:30 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 2.3552
2022-03-09 14:33:03 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 2.2506
2022-03-09 14:33:36 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 2.3165
2022-03-09 14:34:09 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 1.9114
2022-03-09 14:34:42 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 2.1370
2022-03-09 14:35:15 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 1.9926
2022-03-09 14:35:48 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 2.1259
2022-03-09 14:36:21 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 2.0392
2022-03-09 14:36:54 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 2.0437
2022-03-09 14:37:26 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 2.0294
2022-03-09 14:37:59 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 2.1828
2022-03-09 14:38:33 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 2.1140
2022-03-09 14:39:05 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 2.0973
2022-03-09 14:39:38 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 2.2927
2022-03-09 14:40:11 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 2.2272
2022-03-09 14:40:44 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 2.2465
2022-03-09 14:41:17 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 1.9659
2022-03-09 14:41:50 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 1.9069
2022-03-09 14:42:23 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 1.9211
2022-03-09 14:42:56 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 2.0788
2022-03-09 14:43:29 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 2.1485
2022-03-09 14:44:02 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 2.3691
2022-03-09 14:44:35 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 2.1398
2022-03-09 14:45:08 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 2.0367
2022-03-09 14:45:42 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 2.0229
2022-03-09 14:46:14 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 2.2514
2022-03-09 14:46:48 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 2.0172
2022-03-09 14:47:21 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 2.3152
2022-03-09 14:47:54 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 2.2540
2022-03-09 14:48:27 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 2.0823
2022-03-09 14:49:00 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 2.2294
2022-03-09 14:49:33 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 2.4106
2022-03-09 14:50:07 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 2.0607
2022-03-09 14:50:40 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 1.9276
2022-03-09 14:51:13 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 2.1348
2022-03-09 14:51:45 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 2.2356
2022-03-09 14:52:18 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 2.0130
2022-03-09 14:52:52 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 2.0845
2022-03-09 14:53:25 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 2.1903
2022-03-09 14:53:58 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 2.0636
2022-03-09 14:54:01 - train: epoch 082, train_loss: 2.1469
2022-03-09 14:55:16 - eval: epoch: 082, acc1: 56.758%, acc5: 80.106%, test_loss: 1.8498, per_image_load_time: 2.654ms, per_image_inference_time: 0.220ms
2022-03-09 14:55:16 - until epoch: 082, best_acc1: 56.758%
2022-03-09 14:55:16 - epoch 083 lr: 0.0010000000000000002
2022-03-09 14:55:56 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 1.9939
2022-03-09 14:56:29 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 2.0668
2022-03-09 14:57:01 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 2.1516
2022-03-09 14:57:35 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 2.3055
2022-03-09 14:58:08 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 2.2768
2022-03-09 14:58:41 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 2.1267
2022-03-09 14:59:14 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 2.1839
2022-03-09 14:59:47 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 1.8921
2022-03-09 15:00:21 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 2.1617
2022-03-09 15:00:54 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 2.2466
2022-03-09 15:01:27 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 2.2321
2022-03-09 15:02:00 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 2.1931
2022-03-09 15:02:33 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 2.1473
2022-03-09 15:03:06 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 2.1961
2022-03-09 15:03:39 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 1.9943
2022-03-09 15:04:13 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 2.1357
2022-03-09 15:04:46 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 2.2442
2022-03-09 15:05:18 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 2.3890
2022-03-09 15:05:51 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 2.0584
2022-03-09 15:06:25 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 1.9019
2022-03-09 15:06:58 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 2.0357
2022-03-09 15:07:30 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 2.0711
2022-03-09 15:08:03 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 2.3648
2022-03-09 15:08:36 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 2.0673
2022-03-09 15:09:10 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 2.1518
2022-03-09 15:09:43 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 2.0694
2022-03-09 15:10:15 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 2.2253
2022-03-09 15:10:48 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 2.1099
2022-03-09 15:11:21 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 2.2361
2022-03-09 15:11:54 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 2.2512
2022-03-09 15:12:27 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 1.9834
2022-03-09 15:13:00 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 2.1005
2022-03-09 15:13:33 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 2.3174
2022-03-09 15:14:06 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 2.2241
2022-03-09 15:14:38 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 2.2030
2022-03-09 15:15:12 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 2.2185
2022-03-09 15:15:45 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 2.0604
2022-03-09 15:16:17 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 1.9489
2022-03-09 15:16:51 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 2.4131
2022-03-09 15:17:24 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 2.2941
2022-03-09 15:17:57 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 2.1135
2022-03-09 15:18:30 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 2.4816
2022-03-09 15:19:03 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 2.2955
2022-03-09 15:19:36 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 2.0263
2022-03-09 15:20:09 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 2.1070
2022-03-09 15:20:43 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 2.2589
2022-03-09 15:21:16 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 2.3998
2022-03-09 15:21:49 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 2.2849
2022-03-09 15:22:23 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 2.3046
2022-03-09 15:22:54 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 2.1562
2022-03-09 15:22:57 - train: epoch 083, train_loss: 2.1496
2022-03-09 15:24:12 - eval: epoch: 083, acc1: 56.512%, acc5: 80.110%, test_loss: 1.8534, per_image_load_time: 2.219ms, per_image_inference_time: 0.257ms
2022-03-09 15:24:12 - until epoch: 083, best_acc1: 56.758%
2022-03-09 15:24:12 - epoch 084 lr: 0.0010000000000000002
2022-03-09 15:24:52 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 2.1665
2022-03-09 15:25:25 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 2.2393
2022-03-09 15:25:58 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 1.9576
2022-03-09 15:26:32 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 2.2687
2022-03-09 15:27:05 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 2.2612
2022-03-09 15:27:38 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 2.3940
2022-03-09 15:28:11 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 2.1483
2022-03-09 15:28:45 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 2.3212
2022-03-09 15:29:18 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 2.1388
2022-03-09 15:29:51 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 2.2514
2022-03-09 15:30:24 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 2.1158
2022-03-09 15:30:57 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 2.4087
2022-03-09 15:31:30 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 2.1724
2022-03-09 15:32:04 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 2.0553
2022-03-09 15:32:37 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 2.2938
2022-03-09 15:33:10 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 2.1419
2022-03-09 15:33:43 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 2.1731
2022-03-09 15:34:16 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 2.1980
2022-03-09 15:34:49 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 2.0380
2022-03-09 15:35:22 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 2.1284
2022-03-09 15:35:56 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 2.0654
2022-03-09 15:36:29 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 1.8908
2022-03-09 15:37:02 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 2.1362
2022-03-09 15:37:35 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 2.0777
2022-03-09 15:38:09 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 2.2112
2022-03-09 15:38:42 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 2.3021
2022-03-09 15:39:15 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 2.0845
2022-03-09 15:39:48 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 2.2302
2022-03-09 15:40:21 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 2.1088
2022-03-09 15:40:54 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 2.2646
2022-03-09 15:41:27 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 2.0827
2022-03-09 15:42:00 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 2.1547
2022-03-09 15:42:33 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 2.2621
2022-03-09 15:43:06 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 2.0512
2022-03-09 15:43:39 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 2.1169
2022-03-09 15:44:11 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 2.2772
2022-03-09 15:44:45 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 2.3424
2022-03-09 15:45:18 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 2.6226
2022-03-09 15:45:51 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 2.1886
2022-03-09 15:46:24 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 2.1849
2022-03-09 15:46:57 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 2.1595
2022-03-09 15:47:30 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 2.2435
2022-03-09 15:48:03 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 2.3286
2022-03-09 15:48:37 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 2.3695
2022-03-09 15:49:10 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 1.9818
2022-03-09 15:49:43 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 2.0447
2022-03-09 15:50:17 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 2.3841
2022-03-09 15:50:50 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 2.1197
2022-03-09 15:51:22 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 1.8992
2022-03-09 15:51:55 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 2.1115
2022-03-09 15:51:58 - train: epoch 084, train_loss: 2.1448
2022-03-09 15:53:13 - eval: epoch: 084, acc1: 56.676%, acc5: 80.070%, test_loss: 1.8493, per_image_load_time: 2.601ms, per_image_inference_time: 0.247ms
2022-03-09 15:53:14 - until epoch: 084, best_acc1: 56.758%
2022-03-09 15:53:14 - epoch 085 lr: 0.0010000000000000002
2022-03-09 15:53:52 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 2.0636
2022-03-09 15:54:25 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 2.1515
2022-03-09 15:54:58 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 2.2787
2022-03-09 15:55:31 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 2.1330
2022-03-09 15:56:04 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 2.3055
2022-03-09 15:56:37 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 2.0627
2022-03-09 15:57:09 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 2.0090
2022-03-09 15:57:42 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 2.0092
2022-03-09 15:58:14 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 2.0377
2022-03-09 15:58:47 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 2.2366
2022-03-09 15:59:20 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 2.2723
2022-03-09 15:59:53 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 2.0637
2022-03-09 16:00:26 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 2.1925
2022-03-09 16:00:59 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 2.0828
2022-03-09 16:01:32 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 1.9998
2022-03-09 16:02:05 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 2.1737
2022-03-09 16:02:37 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 2.3645
2022-03-09 16:03:09 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 1.9206
2022-03-09 16:03:42 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 1.9401
2022-03-09 16:04:14 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 2.2088
2022-03-09 16:04:47 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 1.7076
2022-03-09 16:05:20 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 2.2556
2022-03-09 16:05:52 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 1.9233
2022-03-09 16:06:24 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 2.2965
2022-03-09 16:06:57 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 2.4390
2022-03-09 16:07:30 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 2.2598
2022-03-09 16:08:03 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 2.0333
2022-03-09 16:08:36 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 2.2302
2022-03-09 16:09:09 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 2.3073
2022-03-09 16:09:42 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 2.3130
2022-03-09 16:10:14 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 2.1479
2022-03-09 16:10:47 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 2.1350
2022-03-09 16:11:19 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 2.1912
2022-03-09 16:11:52 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 2.1894
2022-03-09 16:12:25 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 2.1514
2022-03-09 16:12:57 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 2.2230
2022-03-09 16:13:30 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 2.0653
2022-03-09 16:14:02 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 2.1187
2022-03-09 16:14:34 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 2.1355
2022-03-09 16:15:07 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 2.3149
2022-03-09 16:15:40 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 1.9977
2022-03-09 16:16:12 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 2.2312
2022-03-09 16:16:44 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 1.8906
2022-03-09 16:17:17 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 2.1335
2022-03-09 16:17:50 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 2.3253
2022-03-09 16:18:23 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 2.2756
2022-03-09 16:18:56 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 2.4562
2022-03-09 16:19:28 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 1.9192
2022-03-09 16:20:00 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 2.1964
2022-03-09 16:20:33 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 1.9927
2022-03-09 16:20:35 - train: epoch 085, train_loss: 2.1476
2022-03-09 16:21:50 - eval: epoch: 085, acc1: 56.594%, acc5: 80.166%, test_loss: 1.8484, per_image_load_time: 2.646ms, per_image_inference_time: 0.243ms
2022-03-09 16:21:51 - until epoch: 085, best_acc1: 56.758%
2022-03-09 16:21:51 - epoch 086 lr: 0.0010000000000000002
2022-03-09 16:22:30 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 1.9131
2022-03-09 16:23:03 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 1.9852
2022-03-09 16:23:36 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 2.3677
2022-03-09 16:24:09 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 2.2059
2022-03-09 16:24:42 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 2.0370
2022-03-09 16:25:16 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 2.1040
2022-03-09 16:25:49 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 1.9532
2022-03-09 16:26:22 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 2.3761
2022-03-09 16:26:55 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 2.2420
2022-03-09 16:27:28 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 2.2352
2022-03-09 16:28:02 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 2.2356
2022-03-09 16:28:35 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 2.0714
2022-03-09 16:29:08 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 2.2610
2022-03-09 16:29:42 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 2.0063
2022-03-09 16:30:15 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 2.0400
2022-03-09 16:30:49 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 2.1537
2022-03-09 16:31:22 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 2.0120
2022-03-09 16:31:56 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 2.2791
2022-03-09 16:32:30 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 2.2937
2022-03-09 16:33:03 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 2.1124
2022-03-09 16:33:37 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 2.0623
2022-03-09 16:34:10 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 2.2022
2022-03-09 16:34:43 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 2.2551
2022-03-09 16:35:16 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 1.9052
2022-03-09 16:35:50 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 2.1772
2022-03-09 16:36:24 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 2.1273
2022-03-09 16:36:57 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 2.0791
2022-03-09 16:37:31 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 2.1890
2022-03-09 16:38:05 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 1.9148
2022-03-09 16:38:38 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 2.2516
2022-03-09 16:39:11 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 2.0263
2022-03-09 16:39:45 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 2.2511
2022-03-09 16:40:18 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 2.3844
2022-03-09 16:40:52 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 2.3168
2022-03-09 16:41:25 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 2.2386
2022-03-09 16:41:59 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 2.2207
2022-03-09 16:42:32 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 2.2054
2022-03-09 16:43:05 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 2.1546
2022-03-09 16:43:39 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 2.2374
2022-03-09 16:44:12 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 2.1654
2022-03-09 16:44:46 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 2.0594
2022-03-09 16:45:19 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 2.3088
2022-03-09 16:45:52 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 2.2115
2022-03-09 16:46:26 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 2.0354
2022-03-09 16:46:59 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 1.8947
2022-03-09 16:47:32 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 2.0289
2022-03-09 16:48:05 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 2.1019
2022-03-09 16:48:38 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 2.0369
2022-03-09 16:49:12 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 2.0968
2022-03-09 16:49:45 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 2.1185
2022-03-09 16:49:48 - train: epoch 086, train_loss: 2.1451
2022-03-09 16:51:04 - eval: epoch: 086, acc1: 56.616%, acc5: 80.108%, test_loss: 1.8489, per_image_load_time: 1.906ms, per_image_inference_time: 0.260ms
2022-03-09 16:51:04 - until epoch: 086, best_acc1: 56.758%
2022-03-09 16:51:04 - epoch 087 lr: 0.0010000000000000002
2022-03-09 16:51:43 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 2.2240
2022-03-09 16:52:16 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 2.0727
2022-03-09 16:52:50 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 2.1170
2022-03-09 16:53:23 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 2.3731
2022-03-09 16:53:56 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 1.8760
2022-03-09 16:54:29 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 2.2621
2022-03-09 16:55:02 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 1.9650
2022-03-09 16:55:35 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 2.0969
2022-03-09 16:56:08 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 2.2422
2022-03-09 16:56:41 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 2.1773
2022-03-09 16:57:15 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 2.2246
2022-03-09 16:57:48 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 2.1923
2022-03-09 16:58:21 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 2.3385
2022-03-09 16:58:54 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 2.1201
2022-03-09 16:59:28 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 2.0484
2022-03-09 17:00:00 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 2.0097
2022-03-09 17:00:33 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 2.3188
2022-03-09 17:01:07 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 2.1683
2022-03-09 17:01:40 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 2.2226
2022-03-09 17:02:13 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 2.2198
2022-03-09 17:02:46 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 2.1298
2022-03-09 17:03:19 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 2.1482
2022-03-09 17:03:52 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 2.2183
2022-03-09 17:04:26 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 2.1475
2022-03-09 17:04:59 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 2.1913
2022-03-09 17:05:32 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 2.2165
2022-03-09 17:06:05 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 2.2686
2022-03-09 17:06:38 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 1.9599
2022-03-09 17:07:11 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 2.0564
2022-03-09 17:07:44 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 2.2187
2022-03-09 17:08:17 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 2.1195
2022-03-09 17:08:50 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 2.2092
2022-03-09 17:09:23 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 2.0956
2022-03-09 17:09:56 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 2.1559
2022-03-09 17:10:29 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 1.9808
2022-03-09 17:11:02 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 1.9788
2022-03-09 17:11:35 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 2.2678
2022-03-09 17:12:09 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 2.1815
2022-03-09 17:12:41 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 2.0241
2022-03-09 17:13:16 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 2.2028
2022-03-09 17:13:49 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 2.1570
2022-03-09 17:14:22 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 1.9955
2022-03-09 17:14:55 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 2.1141
2022-03-09 17:15:29 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 2.1618
2022-03-09 17:16:02 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 2.2113
2022-03-09 17:16:36 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 2.2095
2022-03-09 17:17:09 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 2.1891
2022-03-09 17:17:42 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 1.9376
2022-03-09 17:18:15 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 2.0689
2022-03-09 17:18:48 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 2.2309
2022-03-09 17:18:51 - train: epoch 087, train_loss: 2.1431
2022-03-09 17:20:08 - eval: epoch: 087, acc1: 56.604%, acc5: 80.118%, test_loss: 1.8489, per_image_load_time: 2.676ms, per_image_inference_time: 0.239ms
2022-03-09 17:20:08 - until epoch: 087, best_acc1: 56.758%
2022-03-09 17:20:08 - epoch 088 lr: 0.0010000000000000002
2022-03-09 17:20:47 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 2.0215
2022-03-09 17:21:21 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 2.2122
2022-03-09 17:21:54 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 2.1579
2022-03-09 17:22:27 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 2.2685
2022-03-09 17:23:01 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 2.2200
2022-03-09 17:23:34 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 2.1107
2022-03-09 17:24:06 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 2.1417
2022-03-09 17:24:39 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 2.1243
2022-03-09 17:25:12 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 2.1956
2022-03-09 17:25:45 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 2.0745
2022-03-09 17:26:18 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 2.2052
2022-03-09 17:26:51 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 2.0091
2022-03-09 17:27:24 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 2.2126
2022-03-09 17:27:57 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 1.9480
2022-03-09 17:28:29 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 2.1919
2022-03-09 17:29:02 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 2.1247
2022-03-09 17:29:35 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 2.2178
2022-03-09 17:30:08 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 2.1060
2022-03-09 17:30:41 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 2.2459
2022-03-09 17:31:14 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 2.1091
2022-03-09 17:31:46 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 2.1819
2022-03-09 17:32:19 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 2.1180
2022-03-09 17:32:52 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 1.8588
2022-03-09 17:33:25 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 2.2723
2022-03-09 17:33:58 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 2.2919
2022-03-09 17:34:31 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 2.1736
2022-03-09 17:35:04 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 2.1780
2022-03-09 17:35:37 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 2.1933
2022-03-09 17:36:11 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 2.0735
2022-03-09 17:36:44 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 2.3887
2022-03-09 17:37:17 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 2.0319
2022-03-09 17:37:50 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 2.0348
2022-03-09 17:38:23 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 1.9437
2022-03-09 17:38:56 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 1.9379
2022-03-09 17:39:30 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 2.0004
2022-03-09 17:40:03 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 2.0297
2022-03-09 17:40:36 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 2.2031
2022-03-09 17:41:10 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 2.2113
2022-03-09 17:41:43 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 2.2715
2022-03-09 17:42:16 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 1.9817
2022-03-09 17:42:50 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 2.2400
2022-03-09 17:43:23 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 2.1864
2022-03-09 17:43:56 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 2.2912
2022-03-09 17:44:30 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 2.0218
2022-03-09 17:45:03 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 2.1563
2022-03-09 17:45:36 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 2.3901
2022-03-09 17:46:10 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 2.0226
2022-03-09 17:46:43 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 2.0948
2022-03-09 17:47:17 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 1.8859
2022-03-09 17:47:48 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 2.1070
2022-03-09 17:47:51 - train: epoch 088, train_loss: 2.1440
2022-03-09 17:49:07 - eval: epoch: 088, acc1: 56.662%, acc5: 80.170%, test_loss: 1.8466, per_image_load_time: 2.657ms, per_image_inference_time: 0.229ms
2022-03-09 17:49:07 - until epoch: 088, best_acc1: 56.758%
2022-03-09 17:49:07 - epoch 089 lr: 0.0010000000000000002
2022-03-09 17:49:47 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 2.3153
2022-03-09 17:50:20 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 2.0154
2022-03-09 17:50:54 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 2.1161
2022-03-09 17:51:27 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 2.0209
2022-03-09 17:52:00 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 2.1051
2022-03-09 17:52:33 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 2.0376
2022-03-09 17:53:06 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 2.1802
2022-03-09 17:53:39 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 2.4804
2022-03-09 17:54:12 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 2.1478
2022-03-09 17:54:45 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 2.2601
2022-03-09 17:55:18 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 2.1239
2022-03-09 17:55:51 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 2.2846
2022-03-09 17:56:24 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 2.1997
2022-03-09 17:56:57 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 2.4142
2022-03-09 17:57:30 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 2.2585
2022-03-09 17:58:03 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 2.0150
2022-03-09 17:58:37 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 2.1300
2022-03-09 17:59:10 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 2.2221
2022-03-09 17:59:43 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 2.0335
2022-03-09 18:00:17 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 2.1850
2022-03-09 18:00:50 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 2.2321
2022-03-09 18:01:23 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 2.2785
2022-03-09 18:01:56 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 1.9830
2022-03-09 18:02:29 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 2.3462
2022-03-09 18:03:02 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 2.1454
2022-03-09 18:03:36 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 2.0969
2022-03-09 18:04:09 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 2.0123
2022-03-09 18:04:42 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 2.1483
2022-03-09 18:05:15 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 2.0397
2022-03-09 18:05:49 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 2.2298
2022-03-09 18:06:22 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 2.2935
2022-03-09 18:06:55 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 2.2816
2022-03-09 18:07:28 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 2.2987
2022-03-09 18:08:02 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 2.0889
2022-03-09 18:08:35 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 1.8599
2022-03-09 18:09:08 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 2.1252
2022-03-09 18:09:42 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 2.3614
2022-03-09 18:10:15 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 2.0335
2022-03-09 18:10:49 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 2.2504
2022-03-09 18:11:22 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 1.9890
2022-03-09 18:11:55 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 2.4537
2022-03-09 18:12:29 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 2.1363
2022-03-09 18:13:02 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 2.0711
2022-03-09 18:13:36 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 2.1595
2022-03-09 18:14:09 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 1.9417
2022-03-09 18:14:43 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 2.0924
2022-03-09 18:15:16 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 2.1675
2022-03-09 18:15:50 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 2.1504
2022-03-09 18:16:24 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 2.0593
2022-03-09 18:16:55 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 1.7501
2022-03-09 18:16:58 - train: epoch 089, train_loss: 2.1439
2022-03-09 18:18:14 - eval: epoch: 089, acc1: 56.632%, acc5: 80.080%, test_loss: 1.8509, per_image_load_time: 2.612ms, per_image_inference_time: 0.240ms
2022-03-09 18:18:14 - until epoch: 089, best_acc1: 56.758%
2022-03-09 18:18:14 - epoch 090 lr: 0.0010000000000000002
2022-03-09 18:18:54 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 2.1430
2022-03-09 18:19:28 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 2.1503
2022-03-09 18:20:00 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 1.9792
2022-03-09 18:20:33 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 2.0745
2022-03-09 18:21:06 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 2.1635
2022-03-09 18:21:39 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 2.3663
2022-03-09 18:22:12 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 2.1950
2022-03-09 18:22:45 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 2.0830
2022-03-09 18:23:19 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 2.0767
2022-03-09 18:23:52 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 1.9114
2022-03-09 18:24:25 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 2.2310
2022-03-09 18:24:59 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 2.1474
2022-03-09 18:25:32 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 2.2404
2022-03-09 18:26:05 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 2.0041
2022-03-09 18:26:38 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 2.3576
2022-03-09 18:27:11 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 2.1543
2022-03-09 18:27:44 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 1.9709
2022-03-09 18:28:17 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 1.8910
2022-03-09 18:28:50 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 2.0489
2022-03-09 18:29:24 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 2.0956
2022-03-09 18:29:56 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 2.3008
2022-03-09 18:30:29 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 2.2436
2022-03-09 18:31:03 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 2.3079
2022-03-09 18:31:36 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 2.1308
2022-03-09 18:32:10 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 2.2153
2022-03-09 18:32:43 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 2.0194
2022-03-09 18:33:16 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 1.9515
2022-03-09 18:33:50 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 2.1595
2022-03-09 18:34:23 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 2.0686
2022-03-09 18:34:57 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 2.0908
2022-03-09 18:35:30 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 2.0931
2022-03-09 18:36:04 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 2.1744
2022-03-09 18:36:37 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 2.2662
2022-03-09 18:37:10 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 1.8592
2022-03-09 18:37:44 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 2.0742
2022-03-09 18:38:17 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 2.0150
2022-03-09 18:38:51 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 2.1883
2022-03-09 18:39:24 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 2.1456
2022-03-09 18:39:57 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 1.7799
2022-03-09 18:40:31 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 2.1389
2022-03-09 18:41:04 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 2.4430
2022-03-09 18:41:38 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 2.4737
2022-03-09 18:42:11 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 2.0966
2022-03-09 18:42:45 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 2.2079
2022-03-09 18:43:19 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 2.0355
2022-03-09 18:43:52 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 2.2359
2022-03-09 18:44:26 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 1.9884
2022-03-09 18:45:00 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 2.3273
2022-03-09 18:45:33 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 2.0027
2022-03-09 18:46:06 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 2.1497
2022-03-09 18:46:09 - train: epoch 090, train_loss: 2.1417
2022-03-09 18:47:26 - eval: epoch: 090, acc1: 56.698%, acc5: 80.138%, test_loss: 1.8465, per_image_load_time: 2.672ms, per_image_inference_time: 0.234ms
2022-03-09 18:47:26 - until epoch: 090, best_acc1: 56.758%
2022-03-09 18:47:26 - epoch 091 lr: 0.00010000000000000003
2022-03-09 18:48:06 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 2.1662
2022-03-09 18:48:39 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 1.9961
2022-03-09 18:49:12 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 2.0209
2022-03-09 18:49:45 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 2.0545
2022-03-09 18:50:18 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 2.0728
2022-03-09 18:50:51 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 2.1936
2022-03-09 18:51:24 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 2.0900
2022-03-09 18:51:57 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 1.7541
2022-03-09 18:52:30 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 2.0958
2022-03-09 18:53:03 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 2.0451
2022-03-09 18:53:36 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 1.9641
2022-03-09 18:54:09 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 2.0856
2022-03-09 18:54:43 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 2.0603
2022-03-09 18:55:16 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 2.1975
2022-03-09 18:55:50 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 2.1671
2022-03-09 18:56:23 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 2.1021
2022-03-09 18:56:57 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 2.1730
2022-03-09 18:57:30 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 2.2363
2022-03-09 18:58:04 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 2.1283
2022-03-09 18:58:37 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 2.0951
2022-03-09 18:59:10 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 1.9053
2022-03-09 18:59:43 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 2.2038
2022-03-09 19:00:16 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 2.0961
2022-03-09 19:00:49 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 2.0442
2022-03-09 19:01:23 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 2.0279
2022-03-09 19:01:56 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 2.1395
2022-03-09 19:02:29 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 1.8319
2022-03-09 19:03:02 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 2.1564
2022-03-09 19:03:35 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 2.1377
2022-03-09 19:04:08 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 2.1414
2022-03-09 19:04:42 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 2.0799
2022-03-09 19:05:15 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 2.2157
2022-03-09 19:05:48 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 2.0777
2022-03-09 19:06:21 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 2.0930
2022-03-09 19:06:54 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 2.1257
2022-03-09 19:07:28 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 2.1672
2022-03-09 19:08:01 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 2.2497
2022-03-09 19:08:34 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 2.1016
2022-03-09 19:09:07 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 1.9700
2022-03-09 19:09:41 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 2.0232
2022-03-09 19:10:14 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 2.0246
2022-03-09 19:10:47 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 2.1487
2022-03-09 19:11:21 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 2.2330
2022-03-09 19:11:54 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 2.0879
2022-03-09 19:12:27 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 2.0038
2022-03-09 19:13:01 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 2.0910
2022-03-09 19:13:34 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 2.2165
2022-03-09 19:14:08 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 2.0240
2022-03-09 19:14:41 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 2.1646
2022-03-09 19:15:13 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 2.4330
2022-03-09 19:15:16 - train: epoch 091, train_loss: 2.1127
2022-03-09 19:16:33 - eval: epoch: 091, acc1: 57.134%, acc5: 80.494%, test_loss: 1.8209, per_image_load_time: 1.587ms, per_image_inference_time: 0.231ms
2022-03-09 19:16:33 - until epoch: 091, best_acc1: 57.134%
2022-03-09 19:16:33 - epoch 092 lr: 0.00010000000000000003
2022-03-09 19:17:13 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 2.2330
2022-03-09 19:17:47 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 2.0224
2022-03-09 19:18:20 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 2.1583
2022-03-09 19:18:53 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 2.0831
2022-03-09 19:19:26 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 2.2863
2022-03-09 19:19:59 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 2.2223
2022-03-09 19:20:32 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 2.1451
2022-03-09 19:21:05 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 2.1569
2022-03-09 19:21:38 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 2.1132
2022-03-09 19:22:12 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 2.1759
2022-03-09 19:22:45 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 2.0291
2022-03-09 19:23:18 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 2.0531
2022-03-09 19:23:51 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 2.0414
2022-03-09 19:24:25 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 1.9476
2022-03-09 19:24:58 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 2.0853
2022-03-09 19:25:32 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 2.0142
2022-03-09 19:26:05 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 2.0753
2022-03-09 19:26:39 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 2.0070
2022-03-09 19:27:12 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 1.9511
2022-03-09 19:27:45 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 2.1591
2022-03-09 19:28:19 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 1.9510
2022-03-09 19:28:52 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 2.3339
2022-03-09 19:29:25 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 2.1687
2022-03-09 19:29:59 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 1.9373
2022-03-09 19:30:32 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 1.9449
2022-03-09 19:31:06 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 2.1286
2022-03-09 19:31:39 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 2.2024
2022-03-09 19:32:12 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 1.8642
2022-03-09 19:32:46 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 2.2731
2022-03-09 19:33:19 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 1.9765
2022-03-09 19:33:52 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 2.1458
2022-03-09 19:34:25 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 1.9294
2022-03-09 19:34:59 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 2.0518
2022-03-09 19:35:32 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 2.1443
2022-03-09 19:36:06 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 2.1451
2022-03-09 19:36:39 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 2.0625
2022-03-09 19:37:12 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 2.0491
2022-03-09 19:37:44 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 2.3096
2022-03-09 19:38:19 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 2.2189
2022-03-09 19:38:52 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 2.1725
2022-03-09 19:39:26 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 1.9639
2022-03-09 19:39:59 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 2.2026
2022-03-09 19:40:33 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 2.1504
2022-03-09 19:41:07 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 1.9674
2022-03-09 19:41:40 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 2.1806
2022-03-09 19:42:14 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 2.1938
2022-03-09 19:42:47 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 1.8450
2022-03-09 19:43:21 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 1.9666
2022-03-09 19:43:56 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 1.8858
2022-03-09 19:44:29 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 2.2505
2022-03-09 19:44:32 - train: epoch 092, train_loss: 2.1066
2022-03-09 19:45:48 - eval: epoch: 092, acc1: 57.204%, acc5: 80.530%, test_loss: 1.8197, per_image_load_time: 2.668ms, per_image_inference_time: 0.267ms
2022-03-09 19:45:49 - until epoch: 092, best_acc1: 57.204%
2022-03-09 19:45:49 - epoch 093 lr: 0.00010000000000000003
2022-03-09 19:46:28 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 2.0911
2022-03-09 19:47:01 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 1.9505
2022-03-09 19:47:35 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 2.1985
2022-03-09 19:48:08 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 2.1720
2022-03-09 19:48:42 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 2.3230
2022-03-09 19:49:15 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 2.0944
2022-03-09 19:49:48 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 2.2672
2022-03-09 19:50:22 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 2.1459
2022-03-09 19:50:55 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 2.0032
2022-03-09 19:51:28 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 1.9511
2022-03-09 19:52:02 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 2.0399
2022-03-09 19:52:35 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 2.0605
2022-03-09 19:53:08 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 2.0605
2022-03-09 19:53:42 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 2.1153
2022-03-09 19:54:15 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 2.3311
2022-03-09 19:54:48 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 2.1167
2022-03-09 19:55:22 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 2.0879
2022-03-09 19:55:55 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 2.2344
2022-03-09 19:56:29 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 2.0147
2022-03-09 19:57:02 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 2.0422
2022-03-09 19:57:36 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 2.0855
2022-03-09 19:58:09 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 2.1820
2022-03-09 19:58:43 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 2.1176
2022-03-09 19:59:16 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 1.8929
2022-03-09 19:59:50 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 2.0925
2022-03-09 20:00:23 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 2.3070
2022-03-09 20:00:56 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 2.0017
2022-03-09 20:01:29 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 2.1223
2022-03-09 20:02:03 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 2.0785
2022-03-09 20:02:36 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 1.8734
2022-03-09 20:03:10 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 2.3473
2022-03-09 20:03:44 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 2.0717
2022-03-09 20:04:17 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 2.1587
2022-03-09 20:04:51 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 2.3435
2022-03-09 20:05:24 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 1.9613
2022-03-09 20:05:57 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 2.3439
2022-03-09 20:06:30 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 1.9260
2022-03-09 20:07:04 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 2.0467
2022-03-09 20:07:37 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 2.1076
2022-03-09 20:08:10 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 2.3258
2022-03-09 20:08:43 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 2.2414
2022-03-09 20:09:16 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 2.2938
2022-03-09 20:09:49 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 2.2146
2022-03-09 20:10:22 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 1.9249
2022-03-09 20:10:55 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 2.0082
2022-03-09 20:11:28 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 1.8584
2022-03-09 20:12:01 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 2.5121
2022-03-09 20:12:33 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 1.9593
2022-03-09 20:13:07 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 2.0752
2022-03-09 20:13:40 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 2.0538
2022-03-09 20:13:42 - train: epoch 093, train_loss: 2.1055
2022-03-09 20:14:57 - eval: epoch: 093, acc1: 57.190%, acc5: 80.580%, test_loss: 1.8193, per_image_load_time: 2.035ms, per_image_inference_time: 0.258ms
2022-03-09 20:14:57 - until epoch: 093, best_acc1: 57.204%
2022-03-09 20:14:57 - epoch 094 lr: 0.00010000000000000003
2022-03-09 20:15:37 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 2.1207
2022-03-09 20:16:10 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 2.0300
2022-03-09 20:16:43 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 2.1862
2022-03-09 20:17:16 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 2.2024
2022-03-09 20:17:49 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 1.9635
2022-03-09 20:18:21 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 1.9134
2022-03-09 20:18:55 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 2.3023
2022-03-09 20:19:28 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 2.0641
2022-03-09 20:20:01 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 2.1386
2022-03-09 20:20:34 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 2.1262
2022-03-09 20:21:07 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 2.2795
2022-03-09 20:21:40 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 2.0619
2022-03-09 20:22:14 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 1.9914
2022-03-09 20:22:47 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 2.0603
2022-03-09 20:23:20 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 2.0734
2022-03-09 20:23:53 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 2.4304
2022-03-09 20:24:26 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 1.9614
2022-03-09 20:24:59 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 2.0267
2022-03-09 20:25:32 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 2.0507
2022-03-09 20:26:05 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 1.9225
2022-03-09 20:26:38 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 2.1863
2022-03-09 20:27:12 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 1.9930
2022-03-09 20:27:45 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 1.9662
2022-03-09 20:28:18 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 2.0694
2022-03-09 20:28:52 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 2.1107
2022-03-09 20:29:25 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 1.9636
2022-03-09 20:29:58 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 2.0718
2022-03-09 20:30:32 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 2.2708
2022-03-09 20:31:05 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 2.1681
2022-03-09 20:31:38 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 2.0907
2022-03-09 20:32:12 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 2.2505
2022-03-09 20:32:45 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 2.1152
2022-03-09 20:33:19 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 1.9541
2022-03-09 20:33:52 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 2.1382
2022-03-09 20:34:26 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 2.3627
2022-03-09 20:34:59 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 2.1053
2022-03-09 20:35:32 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 2.2130
2022-03-09 20:36:06 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 1.9654
2022-03-09 20:36:39 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 2.0867
2022-03-09 20:37:12 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 2.0947
2022-03-09 20:37:46 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 2.2534
2022-03-09 20:38:19 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 1.9172
2022-03-09 20:38:52 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 2.1330
2022-03-09 20:39:25 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 2.1541
2022-03-09 20:39:58 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 2.0916
2022-03-09 20:40:32 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 2.1749
2022-03-09 20:41:05 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 2.0483
2022-03-09 20:41:38 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 2.1550
2022-03-09 20:42:12 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 2.0842
2022-03-09 20:42:44 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 1.9063
2022-03-09 20:42:47 - train: epoch 094, train_loss: 2.1039
2022-03-09 20:44:03 - eval: epoch: 094, acc1: 57.206%, acc5: 80.562%, test_loss: 1.8185, per_image_load_time: 2.629ms, per_image_inference_time: 0.233ms
2022-03-09 20:44:03 - until epoch: 094, best_acc1: 57.206%
2022-03-09 20:44:03 - epoch 095 lr: 0.00010000000000000003
2022-03-09 20:44:43 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 2.0221
2022-03-09 20:45:16 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 2.3275
2022-03-09 20:45:49 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 2.1092
2022-03-09 20:46:23 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 2.1313
2022-03-09 20:46:56 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 2.2117
2022-03-09 20:47:29 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 2.1235
2022-03-09 20:48:02 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 2.1710
2022-03-09 20:48:35 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 2.2320
2022-03-09 20:49:08 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 2.3303
2022-03-09 20:49:41 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 2.2252
2022-03-09 20:50:14 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 2.0081
2022-03-09 20:50:47 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 1.9776
2022-03-09 20:51:21 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 2.1534
2022-03-09 20:51:54 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 2.2263
2022-03-09 20:52:27 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 2.1671
2022-03-09 20:53:01 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 1.7649
2022-03-09 20:53:34 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 1.8662
2022-03-09 20:54:07 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 2.0897
2022-03-09 20:54:41 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 1.9307
2022-03-09 20:55:14 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 2.1931
2022-03-09 20:55:47 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 2.1070
2022-03-09 20:56:20 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 1.8326
2022-03-09 20:56:54 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 2.2583
2022-03-09 20:57:27 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 2.1593
2022-03-09 20:58:00 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 2.0529
2022-03-09 20:58:33 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 2.1218
2022-03-09 20:59:06 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 2.0533
2022-03-09 20:59:39 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 1.9880
2022-03-09 21:00:13 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 2.1718
2022-03-09 21:00:46 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 2.2903
2022-03-09 21:01:19 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 2.2028
2022-03-09 21:01:52 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 2.0794
2022-03-09 21:02:25 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 2.1358
2022-03-09 21:02:59 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 1.8571
2022-03-09 21:03:32 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 2.0030
2022-03-09 21:04:05 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 1.9438
2022-03-09 21:04:38 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 2.0336
2022-03-09 21:05:11 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 1.9831
2022-03-09 21:05:44 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 2.2100
2022-03-09 21:06:17 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 2.0160
2022-03-09 21:06:50 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 2.1033
2022-03-09 21:07:24 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 1.9360
2022-03-09 21:07:57 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 2.0320
2022-03-09 21:08:30 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 2.2462
2022-03-09 21:09:04 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 1.9887
2022-03-09 21:09:37 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 2.0550
2022-03-09 21:10:11 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 2.0120
2022-03-09 21:10:45 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 2.1077
2022-03-09 21:11:18 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 1.9627
2022-03-09 21:11:51 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 1.8841
2022-03-09 21:11:53 - train: epoch 095, train_loss: 2.1055
2022-03-09 21:13:10 - eval: epoch: 095, acc1: 57.178%, acc5: 80.534%, test_loss: 1.8181, per_image_load_time: 1.445ms, per_image_inference_time: 0.265ms
2022-03-09 21:13:10 - until epoch: 095, best_acc1: 57.206%
2022-03-09 21:13:10 - epoch 096 lr: 0.00010000000000000003
2022-03-09 21:13:49 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 2.0717
2022-03-09 21:14:22 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 1.9912
2022-03-09 21:14:55 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 2.0576
2022-03-09 21:15:28 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 1.8767
2022-03-09 21:16:01 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 2.0175
2022-03-09 21:16:34 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 2.2146
2022-03-09 21:17:07 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 2.0173
2022-03-09 21:17:41 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 2.0258
2022-03-09 21:18:14 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 1.9251
2022-03-09 21:18:47 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 2.0548
2022-03-09 21:19:20 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 2.1783
2022-03-09 21:19:53 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 2.0963
2022-03-09 21:20:27 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 2.2214
2022-03-09 21:21:00 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 1.9480
2022-03-09 21:21:33 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 2.0113
2022-03-09 21:22:06 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 1.8901
2022-03-09 21:22:40 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 2.0405
2022-03-09 21:23:13 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 2.3426
2022-03-09 21:23:46 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 2.1747
2022-03-09 21:24:18 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 2.0577
2022-03-09 21:24:51 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 2.0894
2022-03-09 21:25:25 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 1.8667
2022-03-09 21:25:57 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 1.9748
2022-03-09 21:26:31 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 1.9503
2022-03-09 21:27:03 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 1.9551
2022-03-09 21:27:36 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 2.0534
2022-03-09 21:28:09 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 2.2982
2022-03-09 21:28:42 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 2.2470
2022-03-09 21:29:15 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 2.1043
2022-03-09 21:29:49 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 2.1453
2022-03-09 21:30:22 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 2.0277
2022-03-09 21:30:55 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 2.0787
2022-03-09 21:31:28 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 2.2484
2022-03-09 21:32:01 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 1.9622
2022-03-09 21:32:34 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 2.0133
2022-03-09 21:33:07 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 1.9326
2022-03-09 21:33:40 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 2.1226
2022-03-09 21:34:13 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 1.9835
2022-03-09 21:34:46 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 1.9994
2022-03-09 21:35:18 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 1.8831
2022-03-09 21:35:51 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 2.0856
2022-03-09 21:36:24 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 1.9984
2022-03-09 21:36:57 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 1.7845
2022-03-09 21:37:30 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 2.0468
2022-03-09 21:38:02 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 2.0602
2022-03-09 21:38:36 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 2.0518
2022-03-09 21:39:09 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 2.1616
2022-03-09 21:39:42 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 2.1314
2022-03-09 21:40:16 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 2.3410
2022-03-09 21:40:48 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 2.0259
2022-03-09 21:40:50 - train: epoch 096, train_loss: 2.1026
2022-03-09 21:42:05 - eval: epoch: 096, acc1: 57.294%, acc5: 80.586%, test_loss: 1.8164, per_image_load_time: 2.602ms, per_image_inference_time: 0.230ms
2022-03-09 21:42:05 - until epoch: 096, best_acc1: 57.294%
2022-03-09 21:42:05 - epoch 097 lr: 0.00010000000000000003
2022-03-09 21:42:44 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 2.1773
2022-03-09 21:43:17 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 1.9354
2022-03-09 21:43:50 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 2.2518
2022-03-09 21:44:23 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 2.3706
2022-03-09 21:44:56 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 2.1871
2022-03-09 21:45:29 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 2.1883
2022-03-09 21:46:02 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 1.9134
2022-03-09 21:46:35 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 2.0083
2022-03-09 21:47:08 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 1.9803
2022-03-09 21:47:41 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 2.0093
2022-03-09 21:48:14 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 1.8663
2022-03-09 21:48:47 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 1.9646
2022-03-09 21:49:20 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 2.0531
2022-03-09 21:49:53 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 2.1200
2022-03-09 21:50:26 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 2.0414
2022-03-09 21:51:00 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 2.0648
2022-03-09 21:51:33 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 2.0361
2022-03-09 21:52:06 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 2.2035
2022-03-09 21:52:39 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 2.0468
2022-03-09 21:53:13 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 2.0053
2022-03-09 21:53:46 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 2.0136
2022-03-09 21:54:19 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 2.1773
2022-03-09 21:54:52 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 2.0238
2022-03-09 21:55:25 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 2.1821
2022-03-09 21:55:58 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 2.1027
2022-03-09 21:56:31 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 2.1881
2022-03-09 21:57:05 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 1.8902
2022-03-09 21:57:38 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 1.9446
2022-03-09 21:58:11 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 2.2946
2022-03-09 21:58:44 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 2.0344
2022-03-09 21:59:17 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 2.0606
2022-03-09 21:59:51 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 2.0591
2022-03-09 22:00:24 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 2.1354
2022-03-09 22:00:57 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 2.1410
2022-03-09 22:01:30 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 2.1923
2022-03-09 22:02:03 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 2.1658
2022-03-09 22:02:36 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 1.7833
2022-03-09 22:03:09 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 1.8609
2022-03-09 22:03:42 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 2.0967
2022-03-09 22:04:14 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 2.2829
2022-03-09 22:04:47 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 2.0915
2022-03-09 22:05:21 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 2.0054
2022-03-09 22:05:55 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 1.9332
2022-03-09 22:06:28 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 1.9796
2022-03-09 22:07:01 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 2.0624
2022-03-09 22:07:34 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 2.2126
2022-03-09 22:08:07 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 2.1167
2022-03-09 22:08:39 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 2.1043
2022-03-09 22:09:15 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 2.2672
2022-03-09 22:09:47 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 2.0094
2022-03-09 22:09:50 - train: epoch 097, train_loss: 2.1027
2022-03-09 22:11:05 - eval: epoch: 097, acc1: 57.332%, acc5: 80.648%, test_loss: 1.8127, per_image_load_time: 2.637ms, per_image_inference_time: 0.243ms
2022-03-09 22:11:05 - until epoch: 097, best_acc1: 57.332%
2022-03-09 22:11:05 - epoch 098 lr: 0.00010000000000000003
2022-03-09 22:11:45 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 2.3587
2022-03-09 22:12:18 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 2.2374
2022-03-09 22:12:51 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 2.3276
2022-03-09 22:13:24 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 2.0499
2022-03-09 22:13:57 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 2.2571
2022-03-09 22:14:30 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 2.1553
2022-03-09 22:15:03 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 2.0632
2022-03-09 22:15:36 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 2.1497
2022-03-09 22:16:09 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 2.0690
2022-03-09 22:16:42 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 2.0472
2022-03-09 22:17:15 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 1.7721
2022-03-09 22:17:48 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 1.9508
2022-03-09 22:18:21 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 2.2116
2022-03-09 22:18:54 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 2.2006
2022-03-09 22:19:27 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 2.0155
2022-03-09 22:20:01 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 2.0442
2022-03-09 22:20:34 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 2.1360
2022-03-09 22:21:07 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 2.2171
2022-03-09 22:21:41 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 1.9742
2022-03-09 22:22:14 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 2.1655
2022-03-09 22:22:47 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 1.9946
2022-03-09 22:23:20 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 2.1269
2022-03-09 22:23:53 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 2.0003
2022-03-09 22:24:26 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 2.0761
2022-03-09 22:25:00 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 2.2714
2022-03-09 22:25:33 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 2.1482
2022-03-09 22:26:05 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 2.1110
2022-03-09 22:26:38 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 2.1400
2022-03-09 22:27:11 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 1.9694
2022-03-09 22:27:45 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 2.0447
2022-03-09 22:28:18 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 2.1750
2022-03-09 22:28:50 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 1.8476
2022-03-09 22:29:24 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 1.9262
2022-03-09 22:29:56 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 2.1811
2022-03-09 22:30:29 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 2.0079
2022-03-09 22:31:02 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 2.3527
2022-03-09 22:31:35 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 2.1838
2022-03-09 22:32:08 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 1.9616
2022-03-09 22:32:41 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 2.1565
2022-03-09 22:33:14 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 2.1238
2022-03-09 22:33:46 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 2.2171
2022-03-09 22:34:20 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 2.0337
2022-03-09 22:34:53 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 1.9763
2022-03-09 22:35:27 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 2.3624
2022-03-09 22:36:00 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 2.1679
2022-03-09 22:36:33 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 2.1404
2022-03-09 22:37:06 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 1.8767
2022-03-09 22:37:39 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 1.8962
2022-03-09 22:38:12 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 2.0449
2022-03-09 22:38:45 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 2.1748
2022-03-09 22:38:47 - train: epoch 098, train_loss: 2.1014
2022-03-09 22:40:02 - eval: epoch: 098, acc1: 57.290%, acc5: 80.636%, test_loss: 1.8135, per_image_load_time: 2.636ms, per_image_inference_time: 0.238ms
2022-03-09 22:40:02 - until epoch: 098, best_acc1: 57.332%
2022-03-09 22:40:02 - epoch 099 lr: 0.00010000000000000003
2022-03-09 22:40:41 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 1.9518
2022-03-09 22:41:14 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 2.0295
2022-03-09 22:41:46 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 1.9874
2022-03-09 22:42:19 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 2.0222
2022-03-09 22:42:52 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 2.1803
2022-03-09 22:43:25 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 2.1769
2022-03-09 22:43:58 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 2.1930
2022-03-09 22:44:31 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 2.0376
2022-03-09 22:45:04 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 1.9792
2022-03-09 22:45:36 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 2.2354
2022-03-09 22:46:10 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 2.1122
2022-03-09 22:46:43 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 2.0759
2022-03-09 22:47:16 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 2.3074
2022-03-09 22:47:49 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 2.1731
2022-03-09 22:48:22 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 1.9753
2022-03-09 22:48:55 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 2.1219
2022-03-09 22:49:28 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 2.0415
2022-03-09 22:50:01 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 2.2310
2022-03-09 22:50:34 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 2.1599
2022-03-09 22:51:07 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 1.8758
2022-03-09 22:51:40 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 1.9016
2022-03-09 22:52:14 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 2.1484
2022-03-09 22:52:47 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 2.1527
2022-03-09 22:53:21 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 2.3915
2022-03-09 22:53:54 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 2.0602
2022-03-09 22:54:28 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 2.0412
2022-03-09 22:55:01 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 2.0749
2022-03-09 22:55:34 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 2.1597
2022-03-09 22:56:07 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 1.9860
2022-03-09 22:56:40 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 2.2160
2022-03-09 22:57:13 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 1.9678
2022-03-09 22:57:47 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 2.2264
2022-03-09 22:58:20 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 2.1546
2022-03-09 22:58:53 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 2.1039
2022-03-09 22:59:26 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 2.2511
2022-03-09 22:59:59 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 1.9957
2022-03-09 23:00:32 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 1.9883
2022-03-09 23:01:04 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 2.1403
2022-03-09 23:01:37 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 2.0413
2022-03-09 23:02:10 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 2.2146
2022-03-09 23:02:43 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 1.9439
2022-03-09 23:03:17 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 2.2343
2022-03-09 23:03:50 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 2.0475
2022-03-09 23:04:23 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 2.1229
2022-03-09 23:04:56 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 2.1088
2022-03-09 23:05:30 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 2.3086
2022-03-09 23:06:03 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 2.0344
