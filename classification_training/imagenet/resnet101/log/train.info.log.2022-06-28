2022-06-28 01:32:33 - network: resnet101
2022-06-28 01:32:33 - num_classes: 1000
2022-06-28 01:32:33 - input_image_size: 224
2022-06-28 01:32:33 - scale: 1.1428571428571428
2022-06-28 01:32:33 - trained_model_path: 
2022-06-28 01:32:33 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-06-28 01:32:33 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-06-28 01:32:33 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fa8d013af40>
2022-06-28 01:32:33 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fa8ae82c250>
2022-06-28 01:32:33 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fa8ae82c280>
2022-06-28 01:32:33 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fa8ae82c2e0>
2022-06-28 01:32:33 - seed: 0
2022-06-28 01:32:33 - batch_size: 256
2022-06-28 01:32:33 - num_workers: 16
2022-06-28 01:32:33 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-06-28 01:32:33 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-06-28 01:32:33 - epochs: 100
2022-06-28 01:32:33 - print_interval: 100
2022-06-28 01:32:33 - sync_bn: False
2022-06-28 01:32:33 - apex: True
2022-06-28 01:32:33 - use_ema_model: False
2022-06-28 01:32:33 - ema_model_decay: 0.9999
2022-06-28 01:32:33 - gpus_type: NVIDIA RTX A5000
2022-06-28 01:32:33 - gpus_num: 2
2022-06-28 01:32:33 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fa8aeb37470>
2022-06-28 01:32:33 - --------------------parameters--------------------
2022-06-28 01:32:33 - name: conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.6.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.6.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.6.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.6.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.6.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.6.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.6.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.6.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.6.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.7.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.7.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.7.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.7.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.7.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.7.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.7.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.7.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.7.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.8.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.8.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.8.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.8.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.8.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.8.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.8.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.8.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.8.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.9.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.9.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.9.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.9.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.9.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.9.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.9.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.9.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.9.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.10.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.10.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.10.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.10.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.10.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.10.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.10.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.10.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.10.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.11.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.11.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.11.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.11.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.11.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.11.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.11.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.11.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.11.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.12.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.12.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.12.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.12.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.12.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.12.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.12.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.12.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.12.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.13.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.13.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.13.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.13.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.13.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.13.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.13.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.13.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.13.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.14.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.14.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.14.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.14.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.14.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.14.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.14.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.14.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.14.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.15.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.15.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.15.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.15.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.15.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.15.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.15.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.15.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.15.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.16.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.16.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.16.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.16.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.16.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.16.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.16.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.16.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.16.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.17.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.17.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.17.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.17.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.17.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.17.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.17.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.17.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.17.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.18.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.18.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.18.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.18.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.18.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.18.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.18.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.18.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.18.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.19.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.19.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.19.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.19.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.19.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.19.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.19.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.19.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.19.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.20.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.20.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.20.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.20.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.20.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.20.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.20.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.20.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.20.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.21.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.21.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.21.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.21.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.21.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.21.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.21.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.21.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.21.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.22.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.22.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.22.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.22.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.22.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.22.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer3.22.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer3.22.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer3.22.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-06-28 01:32:33 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-06-28 01:32:33 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-06-28 01:32:33 - name: fc.weight, grad: True
2022-06-28 01:32:33 - name: fc.bias, grad: True
2022-06-28 01:32:33 - --------------------buffers--------------------
2022-06-28 01:32:33 - name: conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.6.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.6.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.6.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.6.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.6.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.6.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.6.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.6.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.6.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.7.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.7.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.7.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.7.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.7.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.7.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.7.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.7.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.7.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.8.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.8.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.8.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.8.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.8.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.8.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.8.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.8.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.8.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.9.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.9.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.9.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.9.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.9.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.9.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.9.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.9.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.9.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.10.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.10.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.10.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.10.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.10.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.10.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.10.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.10.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.10.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.11.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.11.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.11.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.11.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.11.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.11.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.11.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.11.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.11.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.12.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.12.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.12.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.12.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.12.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.12.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.12.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.12.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.12.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.13.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.13.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.13.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.13.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.13.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.13.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.13.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.13.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.13.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.14.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.14.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.14.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.14.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.14.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.14.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.14.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.14.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.14.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.15.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.15.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.15.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.15.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.15.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.15.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.15.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.15.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.15.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.16.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.16.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.16.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.16.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.16.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.16.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.16.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.16.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.16.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.17.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.17.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.17.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.17.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.17.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.17.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.17.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.17.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.17.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.18.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.18.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.18.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.18.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.18.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.18.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.18.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.18.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.18.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.19.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.19.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.19.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.19.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.19.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.19.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.19.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.19.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.19.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.20.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.20.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.20.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.20.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.20.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.20.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.20.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.20.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.20.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.21.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.21.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.21.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.21.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.21.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.21.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.21.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.21.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.21.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.22.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.22.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.22.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.22.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.22.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.22.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer3.22.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer3.22.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer3.22.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-06-28 01:32:33 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-06-28 01:32:33 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-06-28 01:32:33 - -----------no weight decay layers--------------
2022-06-28 01:32:33 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.7.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.7.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.7.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.7.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.7.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.7.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.8.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.8.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.8.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.8.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.8.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.8.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.9.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.9.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.9.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.9.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.9.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.9.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.10.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.10.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.10.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.10.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.10.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.10.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.11.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.11.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.11.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.11.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.11.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.11.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.12.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.12.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.12.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.12.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.12.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.12.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.13.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.13.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.13.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.13.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.13.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.13.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.14.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.14.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.14.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.14.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.14.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.14.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.15.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.15.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.15.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.15.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.15.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.15.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.16.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.16.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.16.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.16.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.16.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.16.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.17.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.17.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.17.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.17.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.17.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.17.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.18.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.18.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.18.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.18.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.18.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.18.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.19.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.19.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.19.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.19.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.19.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.19.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.20.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.20.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.20.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.20.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.20.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.20.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.21.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.21.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.21.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.21.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.21.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.21.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.22.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.22.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.22.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.22.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.22.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.22.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-28 01:32:33 - -------------weight decay layers---------------
2022-06-28 01:32:33 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer1.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer2.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.6.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.6.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.6.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.7.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.7.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.7.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.8.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.8.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.8.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.9.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.9.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.9.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.10.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.10.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.10.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.11.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.11.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.11.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.12.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.12.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.12.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.13.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.13.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.13.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.14.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.14.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.14.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.15.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.15.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.15.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.16.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.16.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.16.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.17.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.17.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.17.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.18.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.18.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.18.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.19.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.19.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.19.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.20.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.20.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.20.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.21.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.21.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.21.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.22.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.22.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer3.22.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-28 01:32:33 - epoch 001 lr: 0.100000
2022-06-28 01:33:13 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9066
2022-06-28 01:33:47 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.8973
2022-06-28 01:34:20 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.8832
2022-06-28 01:34:54 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.8617
2022-06-28 01:35:27 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.8032
2022-06-28 01:36:00 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.7173
2022-06-28 01:36:33 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.7358
2022-06-28 01:37:06 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.6187
2022-06-28 01:37:39 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.5801
2022-06-28 01:38:11 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.6371
2022-06-28 01:38:44 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.4805
2022-06-28 01:39:17 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 6.4608
2022-06-28 01:39:49 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 6.3848
2022-06-28 01:40:22 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 6.3350
2022-06-28 01:40:55 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 6.1606
2022-06-28 01:41:28 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 6.3402
2022-06-28 01:42:00 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 6.0212
2022-06-28 01:42:33 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.9804
2022-06-28 01:43:05 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.9786
2022-06-28 01:43:37 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.8565
2022-06-28 01:44:09 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.8714
2022-06-28 01:44:42 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.6463
2022-06-28 01:45:14 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 5.6210
2022-06-28 01:45:46 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 5.6176
2022-06-28 01:46:18 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.4473
2022-06-28 01:46:50 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.6151
2022-06-28 01:47:23 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.6138
2022-06-28 01:47:55 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 5.3644
2022-06-28 01:48:27 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 5.2570
2022-06-28 01:48:59 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.3905
2022-06-28 01:49:32 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.4358
2022-06-28 01:50:04 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 5.3353
2022-06-28 01:50:36 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 5.0570
2022-06-28 01:51:08 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 5.1062
2022-06-28 01:51:41 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 5.0805
2022-06-28 01:52:13 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 5.0576
2022-06-28 01:52:45 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 5.2233
2022-06-28 01:53:18 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.8549
2022-06-28 01:53:50 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.9876
2022-06-28 01:54:22 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.9307
2022-06-28 01:54:54 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.9244
2022-06-28 01:55:26 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.8202
2022-06-28 01:55:59 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.7843
2022-06-28 01:56:31 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.5844
2022-06-28 01:57:04 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.8656
2022-06-28 01:57:36 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.8936
2022-06-28 01:58:08 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.5141
2022-06-28 01:58:40 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.7960
2022-06-28 01:59:13 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.6347
2022-06-28 01:59:45 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.4679
2022-06-28 01:59:47 - train: epoch 001, train_loss: 5.6755
2022-06-28 02:01:01 - eval: epoch: 001, acc1: 14.802%, acc5: 34.488%, test_loss: 4.4433, per_image_load_time: 1.916ms, per_image_inference_time: 0.683ms
2022-06-28 02:01:02 - until epoch: 001, best_acc1: 14.802%
2022-06-28 02:01:02 - epoch 002 lr: 0.100000
2022-06-28 02:01:40 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.6243
2022-06-28 02:02:12 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.3310
2022-06-28 02:02:44 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.6572
2022-06-28 02:03:17 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.5590
2022-06-28 02:03:49 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.2239
2022-06-28 02:04:22 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.2788
2022-06-28 02:04:55 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.4856
2022-06-28 02:05:28 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 4.0729
2022-06-28 02:06:01 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.8761
2022-06-28 02:06:34 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.4191
2022-06-28 02:07:07 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.3728
2022-06-28 02:07:40 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.1261
2022-06-28 02:08:14 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.1031
2022-06-28 02:08:47 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.2085
2022-06-28 02:09:20 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.1227
2022-06-28 02:09:53 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 4.0161
2022-06-28 02:10:26 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.0680
2022-06-28 02:10:59 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.2096
2022-06-28 02:11:32 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.9161
2022-06-28 02:12:06 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.7444
2022-06-28 02:12:39 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.9591
2022-06-28 02:13:12 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.7618
2022-06-28 02:13:46 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.9123
2022-06-28 02:14:19 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.7160
2022-06-28 02:14:52 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.8020
2022-06-28 02:15:25 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.7213
2022-06-28 02:15:58 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.9246
2022-06-28 02:16:31 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.9029
2022-06-28 02:17:04 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.7578
2022-06-28 02:17:36 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.6365
2022-06-28 02:18:09 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.7005
2022-06-28 02:18:42 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.7577
2022-06-28 02:19:15 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.7481
2022-06-28 02:19:48 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.7605
2022-06-28 02:20:20 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.6113
2022-06-28 02:20:53 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.6574
2022-06-28 02:21:26 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.7991
2022-06-28 02:21:58 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.4177
2022-06-28 02:22:31 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.6525
2022-06-28 02:23:03 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.6080
2022-06-28 02:23:36 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.6720
2022-06-28 02:24:08 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.5284
2022-06-28 02:24:40 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.5318
2022-06-28 02:25:13 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.3225
2022-06-28 02:25:46 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.3942
2022-06-28 02:26:18 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.3505
2022-06-28 02:26:51 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.5896
2022-06-28 02:27:23 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.4901
2022-06-28 02:27:55 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.4343
2022-06-28 02:28:27 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.4126
2022-06-28 02:28:29 - train: epoch 002, train_loss: 3.8911
2022-06-28 02:29:44 - eval: epoch: 002, acc1: 27.634%, acc5: 52.844%, test_loss: 3.4814, per_image_load_time: 2.113ms, per_image_inference_time: 0.699ms
2022-06-28 02:29:45 - until epoch: 002, best_acc1: 27.634%
2022-06-28 02:29:45 - epoch 003 lr: 0.100000
2022-06-28 02:30:23 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.4057
2022-06-28 02:30:55 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.4521
2022-06-28 02:31:26 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.4252
2022-06-28 02:31:59 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.3433
2022-06-28 02:32:30 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.4999
2022-06-28 02:33:03 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.3019
2022-06-28 02:33:35 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.6269
2022-06-28 02:34:08 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.4931
2022-06-28 02:34:40 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.2176
2022-06-28 02:35:13 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.2851
2022-06-28 02:35:46 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.1684
2022-06-28 02:36:18 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.0938
2022-06-28 02:36:51 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.2191
2022-06-28 02:37:23 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.2052
2022-06-28 02:37:56 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.5031
2022-06-28 02:38:28 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.2225
2022-06-28 02:39:02 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.2016
2022-06-28 02:39:35 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.1556
2022-06-28 02:40:07 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.2945
2022-06-28 02:40:40 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.5110
2022-06-28 02:41:13 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.4031
2022-06-28 02:41:45 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.7005
2022-06-28 02:42:17 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.1709
2022-06-28 02:42:50 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.1331
2022-06-28 02:43:22 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.2521
2022-06-28 02:43:54 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.2648
2022-06-28 02:44:27 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.4092
2022-06-28 02:45:00 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.0506
2022-06-28 02:45:32 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.1820
2022-06-28 02:46:05 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.1980
2022-06-28 02:46:37 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.4083
2022-06-28 02:47:10 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.2280
2022-06-28 02:47:43 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.0860
2022-06-28 02:48:15 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.3706
2022-06-28 02:48:48 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.8774
2022-06-28 02:49:21 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.0839
2022-06-28 02:49:53 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.1303
2022-06-28 02:50:26 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.1341
2022-06-28 02:50:59 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.2514
2022-06-28 02:51:32 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.1178
2022-06-28 02:52:05 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.1533
2022-06-28 02:52:37 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.9430
2022-06-28 02:53:10 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.6608
2022-06-28 02:53:43 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.9006
2022-06-28 02:54:15 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 2.9402
2022-06-28 02:54:47 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.0708
2022-06-28 02:55:20 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.7748
2022-06-28 02:55:53 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.1083
2022-06-28 02:56:25 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.0486
2022-06-28 02:56:58 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 2.9993
2022-06-28 02:56:59 - train: epoch 003, train_loss: 3.1840
2022-06-28 02:58:14 - eval: epoch: 003, acc1: 36.126%, acc5: 61.990%, test_loss: 3.0527, per_image_load_time: 2.223ms, per_image_inference_time: 0.692ms
2022-06-28 02:58:15 - until epoch: 003, best_acc1: 36.126%
2022-06-28 02:58:15 - epoch 004 lr: 0.100000
2022-06-28 02:58:53 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 2.9260
2022-06-28 02:59:25 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.8491
2022-06-28 02:59:57 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.9809
2022-06-28 03:00:29 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.9581
2022-06-28 03:01:01 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.7715
2022-06-28 03:01:33 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.0474
2022-06-28 03:02:06 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 2.9877
2022-06-28 03:02:38 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.7644
2022-06-28 03:03:10 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.6167
2022-06-28 03:03:43 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.8321
2022-06-28 03:04:16 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.0047
2022-06-28 03:04:48 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.6060
2022-06-28 03:05:21 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.6754
2022-06-28 03:05:53 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 2.9387
2022-06-28 03:06:26 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 2.9349
2022-06-28 03:06:58 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.7652
2022-06-28 03:07:31 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.9115
2022-06-28 03:08:04 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 2.9633
2022-06-28 03:08:36 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 2.9095
2022-06-28 03:09:09 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.8212
2022-06-28 03:09:41 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.9230
2022-06-28 03:10:14 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.8699
2022-06-28 03:10:46 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.6644
2022-06-28 03:11:19 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.6840
2022-06-28 03:11:52 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.7172
2022-06-28 03:12:24 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.7872
2022-06-28 03:12:57 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.5750
2022-06-28 03:13:30 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.7926
2022-06-28 03:14:02 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.6481
2022-06-28 03:14:35 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.7419
2022-06-28 03:15:08 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.8077
2022-06-28 03:15:40 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.7171
2022-06-28 03:16:13 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.9284
2022-06-28 03:16:45 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.7488
2022-06-28 03:17:17 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.7008
2022-06-28 03:17:50 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.6045
2022-06-28 03:18:22 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.7202
2022-06-28 03:18:55 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.7083
2022-06-28 03:19:28 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.7084
2022-06-28 03:20:00 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.3866
2022-06-28 03:20:33 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.7284
2022-06-28 03:21:05 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.5994
2022-06-28 03:21:37 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.5252
2022-06-28 03:22:10 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.6579
2022-06-28 03:22:42 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.2171
2022-06-28 03:23:15 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.6460
2022-06-28 03:23:47 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.5878
2022-06-28 03:24:20 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.6227
2022-06-28 03:24:52 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.7033
2022-06-28 03:25:25 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.8034
2022-06-28 03:25:26 - train: epoch 004, train_loss: 2.8115
2022-06-28 03:26:41 - eval: epoch: 004, acc1: 44.074%, acc5: 70.624%, test_loss: 2.5092, per_image_load_time: 2.195ms, per_image_inference_time: 0.682ms
2022-06-28 03:26:42 - until epoch: 004, best_acc1: 44.074%
2022-06-28 03:26:42 - epoch 005 lr: 0.100000
2022-06-28 03:27:20 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.7368
2022-06-28 03:27:52 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.8822
2022-06-28 03:28:25 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.7179
2022-06-28 03:28:57 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.6671
2022-06-28 03:29:30 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.4153
2022-06-28 03:30:02 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.6673
2022-06-28 03:30:34 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.6964
2022-06-28 03:31:07 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 2.8463
2022-06-28 03:31:40 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.5635
2022-06-28 03:32:12 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.6875
2022-06-28 03:32:45 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.6636
2022-06-28 03:33:17 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.7121
2022-06-28 03:33:50 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.5818
2022-06-28 03:34:22 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.6819
2022-06-28 03:34:55 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.4563
2022-06-28 03:35:27 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.3724
2022-06-28 03:35:59 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.4687
2022-06-28 03:36:32 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.6291
2022-06-28 03:37:05 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.4190
2022-06-28 03:37:37 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.5594
2022-06-28 03:38:10 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.3515
2022-06-28 03:38:42 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.4834
2022-06-28 03:39:15 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.3721
2022-06-28 03:39:47 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.6225
2022-06-28 03:40:20 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.7894
2022-06-28 03:40:52 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.7737
2022-06-28 03:41:25 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.6897
2022-06-28 03:41:58 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.5501
2022-06-28 03:42:31 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.4950
2022-06-28 03:43:03 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.4996
2022-06-28 03:43:36 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.5664
2022-06-28 03:44:08 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.6451
2022-06-28 03:44:41 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.4074
2022-06-28 03:45:14 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.4154
2022-06-28 03:45:46 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.5371
2022-06-28 03:46:19 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.6267
2022-06-28 03:46:52 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.3447
2022-06-28 03:47:24 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.3429
2022-06-28 03:47:57 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.9849
2022-06-28 03:48:30 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.5025
2022-06-28 03:49:02 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.5026
2022-06-28 03:49:35 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.5590
2022-06-28 03:50:08 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.3902
2022-06-28 03:50:41 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.5167
2022-06-28 03:51:13 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.5341
2022-06-28 03:51:46 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.4940
2022-06-28 03:52:18 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.3114
2022-06-28 03:52:51 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.3303
2022-06-28 03:53:23 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.5665
2022-06-28 03:53:56 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.3932
2022-06-28 03:53:57 - train: epoch 005, train_loss: 2.5739
2022-06-28 03:55:12 - eval: epoch: 005, acc1: 43.784%, acc5: 70.030%, test_loss: 2.4988, per_image_load_time: 2.161ms, per_image_inference_time: 0.690ms
2022-06-28 03:55:13 - until epoch: 005, best_acc1: 44.074%
2022-06-28 03:55:13 - epoch 006 lr: 0.100000
2022-06-28 03:55:52 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.4547
2022-06-28 03:56:24 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.5337
2022-06-28 03:56:56 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.2748
2022-06-28 03:57:29 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.4930
2022-06-28 03:58:01 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.5224
2022-06-28 03:58:34 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.4605
2022-06-28 03:59:06 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.4315
2022-06-28 03:59:39 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.3680
2022-06-28 04:00:12 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.3375
2022-06-28 04:00:44 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.3182
2022-06-28 04:01:17 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.4125
2022-06-28 04:01:49 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.4665
2022-06-28 04:02:22 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.5715
2022-06-28 04:02:55 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.5302
2022-06-28 04:03:28 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.6211
2022-06-28 04:04:00 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.2186
2022-06-28 04:04:33 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.6033
2022-06-28 04:05:06 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.5701
2022-06-28 04:05:39 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.3403
2022-06-28 04:06:12 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.5482
2022-06-28 04:06:44 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.5307
2022-06-28 04:07:17 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.2365
2022-06-28 04:07:50 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.3317
2022-06-28 04:08:23 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.4231
2022-06-28 04:08:55 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.6139
2022-06-28 04:09:28 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.2742
2022-06-28 04:10:01 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.5502
2022-06-28 04:10:34 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.3525
2022-06-28 04:11:07 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.4609
2022-06-28 04:11:39 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.3693
2022-06-28 04:12:12 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.1749
2022-06-28 04:12:44 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.3277
2022-06-28 04:13:17 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.1606
2022-06-28 04:13:50 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.6072
2022-06-28 04:14:22 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.5633
2022-06-28 04:14:55 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.3343
2022-06-28 04:15:27 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.4732
2022-06-28 04:16:00 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.2347
2022-06-28 04:16:32 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.3425
2022-06-28 04:17:05 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.6146
2022-06-28 04:17:38 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.3769
2022-06-28 04:18:10 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.2456
2022-06-28 04:18:43 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.5106
2022-06-28 04:19:16 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.4005
2022-06-28 04:19:48 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.4366
2022-06-28 04:20:21 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.3939
2022-06-28 04:20:54 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.3772
2022-06-28 04:21:26 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.4041
2022-06-28 04:21:59 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.4568
2022-06-28 04:22:31 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.1672
2022-06-28 04:22:33 - train: epoch 006, train_loss: 2.4256
2022-06-28 04:23:47 - eval: epoch: 006, acc1: 46.496%, acc5: 72.274%, test_loss: 2.3839, per_image_load_time: 2.136ms, per_image_inference_time: 0.678ms
2022-06-28 04:23:48 - until epoch: 006, best_acc1: 46.496%
2022-06-28 04:23:48 - epoch 007 lr: 0.100000
2022-06-28 04:24:26 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.2144
2022-06-28 04:24:58 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.4785
2022-06-28 04:25:30 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.6287
2022-06-28 04:26:02 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.4424
2022-06-28 04:26:34 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.3279
2022-06-28 04:27:06 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.4619
2022-06-28 04:27:39 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.3626
2022-06-28 04:28:11 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.3499
2022-06-28 04:28:43 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.3500
2022-06-28 04:29:16 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.3798
2022-06-28 04:29:48 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.1219
2022-06-28 04:30:20 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.2520
2022-06-28 04:30:53 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.2495
2022-06-28 04:31:26 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.3160
2022-06-28 04:31:58 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.4170
2022-06-28 04:32:31 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.3205
2022-06-28 04:33:04 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.3758
2022-06-28 04:33:37 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.2756
2022-06-28 04:34:10 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.3311
2022-06-28 04:34:42 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.1228
2022-06-28 04:35:15 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.4485
2022-06-28 04:35:48 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.1704
2022-06-28 04:36:21 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.2748
2022-06-28 04:36:54 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.3931
2022-06-28 04:37:27 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.2069
2022-06-28 04:38:00 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.3152
2022-06-28 04:38:32 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.2086
2022-06-28 04:39:05 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.3513
2022-06-28 04:39:38 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.2521
2022-06-28 04:40:11 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.3190
2022-06-28 04:40:43 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.1853
2022-06-28 04:41:16 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.2429
2022-06-28 04:41:49 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.5838
2022-06-28 04:42:22 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.1159
2022-06-28 04:42:54 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.3657
2022-06-28 04:43:27 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.1455
2022-06-28 04:44:00 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.3097
2022-06-28 04:44:33 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.5138
2022-06-28 04:45:06 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.1903
2022-06-28 04:45:38 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.3086
2022-06-28 04:46:11 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.2525
2022-06-28 04:46:44 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.2016
2022-06-28 04:47:16 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.4036
2022-06-28 04:47:49 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.0851
2022-06-28 04:48:21 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.4930
2022-06-28 04:48:54 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.3146
2022-06-28 04:49:27 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.4017
2022-06-28 04:50:00 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.6795
2022-06-28 04:50:32 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.2692
2022-06-28 04:51:05 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.3218
2022-06-28 04:51:06 - train: epoch 007, train_loss: 2.3209
2022-06-28 04:52:20 - eval: epoch: 007, acc1: 47.904%, acc5: 73.510%, test_loss: 2.2925, per_image_load_time: 1.750ms, per_image_inference_time: 0.691ms
2022-06-28 04:52:20 - until epoch: 007, best_acc1: 47.904%
2022-06-28 04:52:20 - epoch 008 lr: 0.100000
2022-06-28 04:52:59 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.2405
2022-06-28 04:53:32 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.3114
2022-06-28 04:54:05 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.0446
2022-06-28 04:54:37 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.0471
2022-06-28 04:55:09 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.1057
2022-06-28 04:55:42 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.1765
2022-06-28 04:56:15 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.5191
2022-06-28 04:56:47 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.0667
2022-06-28 04:57:20 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.2306
2022-06-28 04:57:53 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.2316
2022-06-28 04:58:25 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.0628
2022-06-28 04:58:58 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.1279
2022-06-28 04:59:30 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.1925
2022-06-28 05:00:03 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.2540
2022-06-28 05:00:35 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.3205
2022-06-28 05:01:08 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.2201
2022-06-28 05:01:40 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.2110
2022-06-28 05:02:12 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.3410
2022-06-28 05:02:44 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.0833
2022-06-28 05:03:16 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.2525
2022-06-28 05:03:48 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.1925
2022-06-28 05:04:21 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.2225
2022-06-28 05:04:53 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.2965
2022-06-28 05:05:25 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.1754
2022-06-28 05:05:57 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.2208
2022-06-28 05:06:29 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.4134
2022-06-28 05:07:02 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.4109
2022-06-28 05:07:34 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.3407
2022-06-28 05:08:06 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.1179
2022-06-28 05:08:38 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.3196
2022-06-28 05:09:10 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.2028
2022-06-28 05:09:42 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.3926
2022-06-28 05:10:14 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.5218
2022-06-28 05:10:47 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.3014
2022-06-28 05:11:19 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.2053
2022-06-28 05:11:51 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.4469
2022-06-28 05:12:23 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.1826
2022-06-28 05:12:56 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.1629
2022-06-28 05:13:28 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.2647
2022-06-28 05:14:00 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.5395
2022-06-28 05:14:32 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.2713
2022-06-28 05:15:04 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.1583
2022-06-28 05:15:36 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 1.9104
2022-06-28 05:16:09 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.0671
2022-06-28 05:16:41 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.3994
2022-06-28 05:17:13 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.3045
2022-06-28 05:17:45 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.1738
2022-06-28 05:18:17 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.2399
2022-06-28 05:18:50 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.3730
2022-06-28 05:19:22 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.2814
2022-06-28 05:19:24 - train: epoch 008, train_loss: 2.2484
2022-06-28 05:20:39 - eval: epoch: 008, acc1: 52.452%, acc5: 77.874%, test_loss: 2.0470, per_image_load_time: 1.588ms, per_image_inference_time: 0.694ms
2022-06-28 05:20:39 - until epoch: 008, best_acc1: 52.452%
2022-06-28 05:20:39 - epoch 009 lr: 0.100000
2022-06-28 05:21:19 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 1.9725
2022-06-28 05:21:51 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.0966
2022-06-28 05:22:24 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 1.8603
2022-06-28 05:22:56 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.3598
2022-06-28 05:23:29 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.1550
2022-06-28 05:24:01 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.1629
2022-06-28 05:24:34 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.2384
2022-06-28 05:25:07 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.0510
2022-06-28 05:25:39 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.0713
2022-06-28 05:26:12 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.0859
2022-06-28 05:26:45 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.4636
2022-06-28 05:27:18 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.3346
2022-06-28 05:27:51 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.4085
2022-06-28 05:28:24 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 1.9007
2022-06-28 05:28:57 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 1.9821
2022-06-28 05:29:29 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.1716
2022-06-28 05:30:02 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.2916
2022-06-28 05:30:34 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.1245
2022-06-28 05:31:07 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 1.8949
2022-06-28 05:31:40 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 1.8876
2022-06-28 05:32:12 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.2061
2022-06-28 05:32:45 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.2805
2022-06-28 05:33:17 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 1.9631
2022-06-28 05:33:50 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.0880
2022-06-28 05:34:22 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.0090
2022-06-28 05:34:55 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.2808
2022-06-28 05:35:28 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.2189
2022-06-28 05:36:00 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.1983
2022-06-28 05:36:32 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 1.9145
2022-06-28 05:37:05 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.0433
2022-06-28 05:37:37 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.1744
2022-06-28 05:38:10 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.1809
2022-06-28 05:38:42 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.1563
2022-06-28 05:39:14 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.3536
2022-06-28 05:39:47 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.2468
2022-06-28 05:40:20 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.2421
2022-06-28 05:40:52 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.4089
2022-06-28 05:41:24 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.4455
2022-06-28 05:41:57 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 1.8264
2022-06-28 05:42:29 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.3396
2022-06-28 05:43:02 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.2288
2022-06-28 05:43:34 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.3921
2022-06-28 05:44:07 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.4031
2022-06-28 05:44:39 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.2800
2022-06-28 05:45:12 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.1377
2022-06-28 05:45:44 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.2529
2022-06-28 05:46:17 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.4227
2022-06-28 05:46:49 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.4338
2022-06-28 05:47:22 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.2368
2022-06-28 05:47:54 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.1259
2022-06-28 05:47:56 - train: epoch 009, train_loss: 2.2013
2022-06-28 05:49:11 - eval: epoch: 009, acc1: 50.300%, acc5: 75.530%, test_loss: 2.1754, per_image_load_time: 2.070ms, per_image_inference_time: 0.690ms
2022-06-28 05:49:11 - until epoch: 009, best_acc1: 52.452%
2022-06-28 05:49:11 - epoch 010 lr: 0.100000
2022-06-28 05:49:50 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.1821
2022-06-28 05:50:23 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.2039
2022-06-28 05:50:56 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.2116
2022-06-28 05:51:28 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.1898
2022-06-28 05:52:01 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.0561
2022-06-28 05:52:33 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.2671
2022-06-28 05:53:06 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.2191
2022-06-28 05:53:38 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.0290
2022-06-28 05:54:11 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 1.9838
2022-06-28 05:54:44 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 1.9104
2022-06-28 05:55:16 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.1099
2022-06-28 05:55:49 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 1.9688
2022-06-28 05:56:22 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.0468
2022-06-28 05:56:55 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.1960
2022-06-28 05:57:28 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 1.9464
2022-06-28 05:58:00 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.2877
2022-06-28 05:58:33 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.1986
2022-06-28 05:59:06 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.0869
2022-06-28 05:59:38 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.1720
2022-06-28 06:00:10 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.1906
2022-06-28 06:00:43 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.0580
2022-06-28 06:01:16 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.3412
2022-06-28 06:01:48 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.3317
2022-06-28 06:02:21 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.3371
2022-06-28 06:02:54 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.1563
2022-06-28 06:03:27 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.1291
2022-06-28 06:03:59 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 1.9341
2022-06-28 06:04:32 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.2365
2022-06-28 06:05:05 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.2767
2022-06-28 06:05:37 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.1852
2022-06-28 06:06:10 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.4190
2022-06-28 06:06:43 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.1893
2022-06-28 06:07:16 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.3009
2022-06-28 06:07:48 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.4257
2022-06-28 06:08:21 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.3220
2022-06-28 06:08:54 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.3448
2022-06-28 06:09:27 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 1.9673
2022-06-28 06:09:59 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.2636
2022-06-28 06:10:32 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 1.8773
2022-06-28 06:11:05 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.1329
2022-06-28 06:11:37 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.0000
2022-06-28 06:12:10 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.2288
2022-06-28 06:12:43 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.1765
2022-06-28 06:13:16 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.1930
2022-06-28 06:13:49 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 1.9456
2022-06-28 06:14:22 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.2450
2022-06-28 06:14:55 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.2347
2022-06-28 06:15:27 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.1269
2022-06-28 06:16:00 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.0645
2022-06-28 06:16:32 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.0014
2022-06-28 06:16:34 - train: epoch 010, train_loss: 2.1580
2022-06-28 06:17:49 - eval: epoch: 010, acc1: 52.930%, acc5: 78.044%, test_loss: 2.0226, per_image_load_time: 1.366ms, per_image_inference_time: 0.717ms
2022-06-28 06:17:50 - until epoch: 010, best_acc1: 52.930%
2022-06-28 06:17:50 - epoch 011 lr: 0.100000
2022-06-28 06:18:29 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 1.8250
2022-06-28 06:19:02 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.1569
2022-06-28 06:19:35 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 1.8775
2022-06-28 06:20:07 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.1894
2022-06-28 06:20:40 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.1561
2022-06-28 06:21:13 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.1860
2022-06-28 06:21:46 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.1033
2022-06-28 06:22:19 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.1310
2022-06-28 06:22:52 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.3422
2022-06-28 06:23:25 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.0384
2022-06-28 06:23:58 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.1949
2022-06-28 06:24:31 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.4734
2022-06-28 06:25:05 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.2987
2022-06-28 06:25:38 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.2083
2022-06-28 06:26:11 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.0166
2022-06-28 06:26:44 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.1926
2022-06-28 06:27:17 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.1529
2022-06-28 06:27:50 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.0364
2022-06-28 06:28:23 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 1.9600
2022-06-28 06:28:56 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.2730
2022-06-28 06:29:29 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.0942
2022-06-28 06:30:02 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.0137
2022-06-28 06:30:35 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.3256
2022-06-28 06:31:08 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 1.9889
2022-06-28 06:31:41 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.3657
2022-06-28 06:32:14 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.1372
2022-06-28 06:32:47 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.1690
2022-06-28 06:33:20 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 1.8156
2022-06-28 06:33:54 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.1295
2022-06-28 06:34:27 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.3689
2022-06-28 06:35:00 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.1574
2022-06-28 06:35:33 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 1.9313
2022-06-28 06:36:06 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.1444
2022-06-28 06:36:39 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.0389
2022-06-28 06:37:12 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.0543
2022-06-28 06:37:45 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.1357
2022-06-28 06:38:18 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.2976
2022-06-28 06:38:52 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 1.9294
2022-06-28 06:39:25 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.1464
2022-06-28 06:39:58 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.0595
2022-06-28 06:40:31 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 1.8377
2022-06-28 06:41:05 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.0106
2022-06-28 06:41:38 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.1850
2022-06-28 06:42:11 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.0128
2022-06-28 06:42:44 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 1.9680
2022-06-28 06:43:18 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.0284
2022-06-28 06:43:51 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 1.8755
2022-06-28 06:44:24 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 1.9315
2022-06-28 06:44:57 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 1.8681
2022-06-28 06:45:30 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.1049
2022-06-28 06:45:32 - train: epoch 011, train_loss: 2.1104
2022-06-28 06:46:47 - eval: epoch: 011, acc1: 52.298%, acc5: 76.654%, test_loss: 2.3076, per_image_load_time: 0.998ms, per_image_inference_time: 0.693ms
2022-06-28 06:46:47 - until epoch: 011, best_acc1: 52.930%
2022-06-28 06:46:47 - epoch 012 lr: 0.100000
2022-06-28 06:47:27 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 1.9278
2022-06-28 06:47:59 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 1.9653
2022-06-28 06:48:32 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.0933
2022-06-28 06:49:05 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.0931
2022-06-28 06:49:38 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.2334
2022-06-28 06:50:11 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 1.9150
2022-06-28 06:50:44 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 1.9036
2022-06-28 06:51:17 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.1303
2022-06-28 06:51:50 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.2084
2022-06-28 06:52:23 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 1.9384
2022-06-28 06:52:56 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.4468
2022-06-28 06:53:29 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 1.8586
2022-06-28 06:54:02 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.0110
2022-06-28 06:54:35 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.2512
2022-06-28 06:55:08 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 1.8861
2022-06-28 06:55:41 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 1.9628
2022-06-28 06:56:14 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 1.9045
2022-06-28 06:56:47 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.0962
2022-06-28 06:57:20 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.2076
2022-06-28 06:57:53 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.2657
2022-06-28 06:58:26 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 1.9362
2022-06-28 06:58:59 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.3018
2022-06-28 06:59:32 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.1114
2022-06-28 07:00:05 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.1104
2022-06-28 07:00:38 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 1.8012
2022-06-28 07:01:12 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 1.8740
2022-06-28 07:01:45 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 1.9199
2022-06-28 07:02:18 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.1005
2022-06-28 07:02:52 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 1.8700
2022-06-28 07:03:25 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.0490
2022-06-28 07:03:58 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.1899
2022-06-28 07:04:31 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 1.9205
2022-06-28 07:05:04 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.0120
2022-06-28 07:05:37 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 1.9846
2022-06-28 07:06:10 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.2349
2022-06-28 07:06:43 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.0190
2022-06-28 07:07:16 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.0488
2022-06-28 07:07:49 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.0441
2022-06-28 07:08:22 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.0353
2022-06-28 07:08:55 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.0474
2022-06-28 07:09:28 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 1.9932
2022-06-28 07:10:02 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 1.8531
2022-06-28 07:10:35 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.1058
2022-06-28 07:11:08 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 1.8566
2022-06-28 07:11:41 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 1.8950
2022-06-28 07:12:15 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.3962
2022-06-28 07:12:48 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.0318
2022-06-28 07:13:21 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.1925
2022-06-28 07:13:54 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.1008
2022-06-28 07:14:27 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 1.9386
2022-06-28 07:14:28 - train: epoch 012, train_loss: 2.0810
2022-06-28 07:15:43 - eval: epoch: 012, acc1: 55.488%, acc5: 80.100%, test_loss: 1.8871, per_image_load_time: 1.662ms, per_image_inference_time: 0.729ms
2022-06-28 07:15:44 - until epoch: 012, best_acc1: 55.488%
2022-06-28 07:15:44 - epoch 013 lr: 0.100000
2022-06-28 07:16:23 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 1.7613
2022-06-28 07:16:56 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.0136
2022-06-28 07:17:29 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 1.9606
2022-06-28 07:18:02 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.0076
2022-06-28 07:18:35 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 1.9794
2022-06-28 07:19:08 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.1180
2022-06-28 07:19:41 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.0388
2022-06-28 07:20:14 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.0815
2022-06-28 07:20:47 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 1.8846
2022-06-28 07:21:20 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.0441
2022-06-28 07:21:53 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.0328
2022-06-28 07:22:27 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.1923
2022-06-28 07:23:00 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.0561
2022-06-28 07:23:33 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 1.8597
2022-06-28 07:24:06 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.0356
2022-06-28 07:24:39 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 1.8067
2022-06-28 07:25:12 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.0821
2022-06-28 07:25:45 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.1006
2022-06-28 07:26:18 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.1411
2022-06-28 07:26:51 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.2452
2022-06-28 07:27:24 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.3082
2022-06-28 07:27:58 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 1.9363
2022-06-28 07:28:31 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.1310
2022-06-28 07:29:04 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.0020
2022-06-28 07:29:37 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.0353
2022-06-28 07:30:11 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.0334
2022-06-28 07:30:44 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 1.8750
2022-06-28 07:31:17 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.1004
2022-06-28 07:31:51 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.1309
2022-06-28 07:32:23 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 1.8797
2022-06-28 07:32:57 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 1.8504
2022-06-28 07:33:30 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.0519
2022-06-28 07:34:03 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 1.9559
2022-06-28 07:34:36 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 1.9732
2022-06-28 07:35:09 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 1.8509
2022-06-28 07:35:42 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.3332
2022-06-28 07:36:15 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 1.7181
2022-06-28 07:36:48 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.1553
2022-06-28 07:37:21 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.2597
2022-06-28 07:37:55 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.0832
2022-06-28 07:38:28 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 1.9525
2022-06-28 07:39:01 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.0425
2022-06-28 07:39:34 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 1.9490
2022-06-28 07:40:07 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.0158
2022-06-28 07:40:40 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 1.9581
2022-06-28 07:41:13 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.0361
2022-06-28 07:41:47 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.1309
2022-06-28 07:42:20 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.1294
2022-06-28 07:42:53 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.2028
2022-06-28 07:43:26 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.2253
2022-06-28 07:43:27 - train: epoch 013, train_loss: 2.0545
2022-06-28 07:44:42 - eval: epoch: 013, acc1: 51.224%, acc5: 75.152%, test_loss: 2.9218, per_image_load_time: 1.335ms, per_image_inference_time: 0.730ms
2022-06-28 07:44:43 - until epoch: 013, best_acc1: 55.488%
2022-06-28 07:44:43 - epoch 014 lr: 0.100000
2022-06-28 07:45:22 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.0879
2022-06-28 07:45:54 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.1429
2022-06-28 07:46:27 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 1.8492
2022-06-28 07:47:00 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 1.8866
2022-06-28 07:47:33 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.5052
2022-06-28 07:48:06 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.1654
2022-06-28 07:48:39 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.1689
2022-06-28 07:49:12 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.1227
2022-06-28 07:49:44 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.1327
2022-06-28 07:50:17 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.2007
2022-06-28 07:50:50 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 1.9526
2022-06-28 07:51:23 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.0830
2022-06-28 07:51:57 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.1194
2022-06-28 07:52:30 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.2124
2022-06-28 07:53:03 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.1420
2022-06-28 07:53:36 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.0129
2022-06-28 07:54:09 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.2524
2022-06-28 07:54:42 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.2662
2022-06-28 07:55:15 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 1.9347
2022-06-28 07:55:48 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 1.9685
2022-06-28 07:56:21 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.1953
2022-06-28 07:56:54 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 1.9970
2022-06-28 07:57:27 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.0197
2022-06-28 07:58:00 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.1834
2022-06-28 07:58:33 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 1.9720
2022-06-28 07:59:07 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.0696
2022-06-28 07:59:40 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 1.9268
2022-06-28 08:00:13 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.4295
2022-06-28 08:00:46 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 1.9522
2022-06-28 08:01:19 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.1399
2022-06-28 08:01:52 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 1.8270
2022-06-28 08:02:25 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 1.9327
2022-06-28 08:02:58 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 1.9251
2022-06-28 08:03:32 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 1.9837
2022-06-28 08:04:05 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 1.8863
2022-06-28 08:04:38 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 1.9612
2022-06-28 08:05:10 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 1.9780
2022-06-28 08:05:43 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.1920
2022-06-28 08:06:17 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 1.9569
2022-06-28 08:06:50 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.1162
2022-06-28 08:07:23 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 1.9446
2022-06-28 08:07:56 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 1.9070
2022-06-28 08:08:30 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 1.8886
2022-06-28 08:09:03 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 1.8234
2022-06-28 08:09:36 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 1.9898
2022-06-28 08:10:09 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.0284
2022-06-28 08:10:42 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 1.9731
2022-06-28 08:11:16 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 1.9021
2022-06-28 08:11:49 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 1.7990
2022-06-28 08:12:21 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 1.9962
2022-06-28 08:12:23 - train: epoch 014, train_loss: 2.0540
2022-06-28 08:13:37 - eval: epoch: 014, acc1: 54.538%, acc5: 79.538%, test_loss: 1.9479, per_image_load_time: 1.363ms, per_image_inference_time: 0.734ms
2022-06-28 08:13:38 - until epoch: 014, best_acc1: 55.488%
2022-06-28 08:13:38 - epoch 015 lr: 0.100000
2022-06-28 08:14:17 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 1.7294
2022-06-28 08:14:50 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.1582
2022-06-28 08:15:23 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.1535
2022-06-28 08:15:55 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.0575
2022-06-28 08:16:28 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.0033
2022-06-28 08:17:01 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.1819
2022-06-28 08:17:35 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 1.8755
2022-06-28 08:18:08 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 1.8208
2022-06-28 08:18:41 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 1.9874
2022-06-28 08:19:14 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.1754
2022-06-28 08:19:47 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 1.7790
2022-06-28 08:20:20 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.0310
2022-06-28 08:20:53 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.4826
2022-06-28 08:21:26 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 1.8771
2022-06-28 08:21:59 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 1.7257
2022-06-28 08:22:32 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 1.9771
2022-06-28 08:23:05 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.1772
2022-06-28 08:23:38 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 1.9220
2022-06-28 08:24:11 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 1.8977
2022-06-28 08:24:44 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.0350
2022-06-28 08:25:17 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 1.9313
2022-06-28 08:25:50 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.2070
2022-06-28 08:26:23 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 1.8213
2022-06-28 08:26:56 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.0745
2022-06-28 08:27:29 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.0957
2022-06-28 08:28:02 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 1.9377
2022-06-28 08:28:35 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.0628
2022-06-28 08:29:08 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.1390
2022-06-28 08:29:41 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.0110
2022-06-28 08:30:14 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 1.8437
2022-06-28 08:30:47 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 1.9576
2022-06-28 08:31:20 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 1.9579
2022-06-28 08:31:53 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 1.8060
2022-06-28 08:32:26 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.1829
2022-06-28 08:32:59 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.1860
2022-06-28 08:33:32 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.1179
2022-06-28 08:34:05 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 1.8073
2022-06-28 08:34:38 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 1.9096
2022-06-28 08:35:11 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.1563
2022-06-28 08:35:44 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.0652
2022-06-28 08:36:17 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.1631
2022-06-28 08:36:50 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 1.6996
2022-06-28 08:37:23 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.0889
2022-06-28 08:37:56 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.0955
2022-06-28 08:38:29 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 1.9979
2022-06-28 08:39:02 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 1.9385
2022-06-28 08:39:35 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.1360
2022-06-28 08:40:09 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.0310
2022-06-28 08:40:42 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 1.9788
2022-06-28 08:41:15 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.0375
2022-06-28 08:41:16 - train: epoch 015, train_loss: 2.0212
2022-06-28 08:42:31 - eval: epoch: 015, acc1: 54.972%, acc5: 79.498%, test_loss: 1.9516, per_image_load_time: 2.098ms, per_image_inference_time: 0.716ms
2022-06-28 08:42:32 - until epoch: 015, best_acc1: 55.488%
2022-06-28 08:42:32 - epoch 016 lr: 0.100000
2022-06-28 08:43:11 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.0447
2022-06-28 08:43:44 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 1.8344
2022-06-28 08:44:16 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.0375
2022-06-28 08:44:49 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.1901
2022-06-28 08:45:22 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 1.7616
2022-06-28 08:45:56 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.2056
2022-06-28 08:46:29 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 1.7673
2022-06-28 08:47:02 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 1.9806
2022-06-28 08:47:35 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.0781
2022-06-28 08:48:08 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 1.8776
2022-06-28 08:48:41 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 1.9179
2022-06-28 08:49:14 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 1.9753
2022-06-28 08:49:47 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.1401
2022-06-28 08:50:20 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 1.9502
2022-06-28 08:50:53 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 1.9102
2022-06-28 08:51:26 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.1604
2022-06-28 08:51:59 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 1.9594
2022-06-28 08:52:32 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 1.8509
2022-06-28 08:53:05 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.0158
2022-06-28 08:53:39 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 1.8119
2022-06-28 08:54:12 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.0469
2022-06-28 08:54:45 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.0312
2022-06-28 08:55:18 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.2472
2022-06-28 08:55:51 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.1717
2022-06-28 08:56:24 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 1.9081
2022-06-28 08:56:57 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.2097
2022-06-28 08:57:30 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 1.9130
2022-06-28 08:58:03 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 1.9039
2022-06-28 08:58:37 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.0725
2022-06-28 08:59:10 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.3005
2022-06-28 08:59:43 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.1821
2022-06-28 09:00:16 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.2211
2022-06-28 09:00:50 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.0822
2022-06-28 09:01:23 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 1.8934
2022-06-28 09:01:56 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 1.9775
2022-06-28 09:02:30 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 1.8237
2022-06-28 09:03:03 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.0546
2022-06-28 09:03:36 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.3296
2022-06-28 09:04:09 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.0860
2022-06-28 09:04:42 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.1524
2022-06-28 09:05:15 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 1.9632
2022-06-28 09:05:49 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 1.9768
2022-06-28 09:06:22 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 1.8713
2022-06-28 09:06:55 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 1.9363
2022-06-28 09:07:29 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.1050
2022-06-28 09:08:02 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 1.9049
2022-06-28 09:08:35 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.3253
2022-06-28 09:09:08 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 1.9399
2022-06-28 09:09:41 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 1.9771
2022-06-28 09:10:14 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.0623
2022-06-28 09:10:16 - train: epoch 016, train_loss: 2.0058
2022-06-28 09:11:31 - eval: epoch: 016, acc1: 53.592%, acc5: 78.260%, test_loss: 2.0703, per_image_load_time: 0.683ms, per_image_inference_time: 0.687ms
2022-06-28 09:11:32 - until epoch: 016, best_acc1: 55.488%
2022-06-28 09:11:32 - epoch 017 lr: 0.100000
2022-06-28 09:12:11 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 1.9220
2022-06-28 09:12:44 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.0464
2022-06-28 09:13:17 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.1886
2022-06-28 09:13:50 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 1.7686
2022-06-28 09:14:23 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 1.9678
2022-06-28 09:14:56 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.2538
2022-06-28 09:15:30 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.0267
2022-06-28 09:16:03 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 1.8420
2022-06-28 09:16:36 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 1.8639
2022-06-28 09:17:09 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 1.8745
2022-06-28 09:17:42 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.3009
2022-06-28 09:18:16 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.1517
2022-06-28 09:18:49 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 1.9500
2022-06-28 09:19:22 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.0228
2022-06-28 09:19:54 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 1.6998
2022-06-28 09:20:27 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 1.8852
2022-06-28 09:21:00 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 1.9655
2022-06-28 09:21:32 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 1.9023
2022-06-28 09:22:05 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 1.8448
2022-06-28 09:22:38 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.2021
2022-06-28 09:23:12 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.0777
2022-06-28 09:23:44 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 1.8738
2022-06-28 09:24:18 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 1.8549
2022-06-28 09:24:51 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 1.9876
2022-06-28 09:25:24 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.1133
2022-06-28 09:25:57 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 1.8598
2022-06-28 09:26:30 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 1.9364
2022-06-28 09:27:03 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.1656
2022-06-28 09:27:36 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.1983
2022-06-28 09:28:09 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 1.8286
2022-06-28 09:28:43 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.0903
2022-06-28 09:29:16 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 1.8346
2022-06-28 09:29:49 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 1.9898
2022-06-28 09:30:22 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 1.8358
2022-06-28 09:30:55 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 1.9756
2022-06-28 09:31:28 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.2445
2022-06-28 09:32:01 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 1.9724
2022-06-28 09:32:34 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.2063
2022-06-28 09:33:07 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 1.8183
2022-06-28 09:33:40 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 1.8572
2022-06-28 09:34:13 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.0694
2022-06-28 09:34:46 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 1.9592
2022-06-28 09:35:19 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.0938
2022-06-28 09:35:52 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 1.9853
2022-06-28 09:36:26 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.0719
2022-06-28 09:36:59 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 1.9614
2022-06-28 09:37:32 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.1414
2022-06-28 09:38:05 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.1522
2022-06-28 09:38:38 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 1.8475
2022-06-28 09:39:11 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 1.8892
2022-06-28 09:39:12 - train: epoch 017, train_loss: 1.9878
2022-06-28 09:40:27 - eval: epoch: 017, acc1: 55.632%, acc5: 80.356%, test_loss: 1.8834, per_image_load_time: 1.393ms, per_image_inference_time: 0.723ms
2022-06-28 09:40:28 - until epoch: 017, best_acc1: 55.632%
2022-06-28 09:40:28 - epoch 018 lr: 0.100000
2022-06-28 09:41:07 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.0437
2022-06-28 09:41:39 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 1.8635
2022-06-28 09:42:12 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.1401
2022-06-28 09:42:45 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.0809
2022-06-28 09:43:18 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 1.8543
2022-06-28 09:43:50 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.0867
2022-06-28 09:44:23 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 1.7788
2022-06-28 09:44:56 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 1.8778
2022-06-28 09:45:28 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.0984
2022-06-28 09:46:01 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 1.8874
2022-06-28 09:46:34 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.2361
2022-06-28 09:47:07 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 1.9563
2022-06-28 09:47:40 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.3107
2022-06-28 09:48:12 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 1.9264
2022-06-28 09:48:45 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.2126
2022-06-28 09:49:18 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.0218
2022-06-28 09:49:51 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 1.9368
2022-06-28 09:50:23 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 1.6299
2022-06-28 09:50:56 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.0158
2022-06-28 09:51:28 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.1687
2022-06-28 09:52:01 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.0881
2022-06-28 09:52:34 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 1.9916
2022-06-28 09:53:06 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.0107
2022-06-28 09:53:39 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 1.8524
2022-06-28 09:54:12 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 1.8035
2022-06-28 09:54:44 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 1.8610
2022-06-28 09:55:17 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.1127
2022-06-28 09:55:50 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 1.7799
2022-06-28 09:56:23 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 1.9669
2022-06-28 09:56:56 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 1.9160
2022-06-28 09:57:28 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.2896
2022-06-28 09:58:01 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 1.8488
2022-06-28 09:58:35 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 1.9094
2022-06-28 09:59:08 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 1.9869
2022-06-28 09:59:41 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.3021
2022-06-28 10:00:14 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 1.9780
2022-06-28 10:00:47 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.3432
2022-06-28 10:01:20 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.1352
2022-06-28 10:01:53 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.0467
2022-06-28 10:02:26 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 1.7243
2022-06-28 10:02:59 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 1.9815
2022-06-28 10:03:32 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 1.9858
2022-06-28 10:04:05 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 1.7041
2022-06-28 10:04:38 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.2380
2022-06-28 10:05:11 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 1.8525
2022-06-28 10:05:44 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 1.8388
2022-06-28 10:06:17 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.2116
2022-06-28 10:06:50 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 1.9879
2022-06-28 10:07:23 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 1.9285
2022-06-28 10:07:56 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.0819
2022-06-28 10:07:57 - train: epoch 018, train_loss: 1.9735
2022-06-28 10:09:12 - eval: epoch: 018, acc1: 51.224%, acc5: 76.242%, test_loss: 2.1949, per_image_load_time: 2.050ms, per_image_inference_time: 0.692ms
2022-06-28 10:09:13 - until epoch: 018, best_acc1: 55.632%
2022-06-28 10:09:13 - epoch 019 lr: 0.100000
2022-06-28 10:09:52 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 1.7884
2022-06-28 10:10:24 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.2472
2022-06-28 10:10:56 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.2303
2022-06-28 10:11:28 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 1.8432
2022-06-28 10:12:00 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.0506
2022-06-28 10:12:32 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 1.8063
2022-06-28 10:13:05 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 1.8127
2022-06-28 10:13:37 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.0648
2022-06-28 10:14:09 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 1.9163
2022-06-28 10:14:42 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 1.9648
2022-06-28 10:15:14 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 1.9204
2022-06-28 10:15:47 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.0326
2022-06-28 10:16:18 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.1782
2022-06-28 10:16:51 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 1.8675
2022-06-28 10:17:23 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.3425
2022-06-28 10:17:56 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 1.9898
2022-06-28 10:18:28 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.0512
2022-06-28 10:19:01 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 1.8708
2022-06-28 10:19:34 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.1543
2022-06-28 10:20:06 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 1.9286
2022-06-28 10:20:39 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 1.8295
2022-06-28 10:21:12 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 1.9414
2022-06-28 10:21:44 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 1.9644
2022-06-28 10:22:17 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.0334
2022-06-28 10:22:50 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 1.8490
2022-06-28 10:23:23 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.0370
2022-06-28 10:23:56 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.0825
2022-06-28 10:24:28 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 1.9049
2022-06-28 10:25:01 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.0133
2022-06-28 10:25:34 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.2690
2022-06-28 10:26:07 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.0816
2022-06-28 10:26:39 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.7640
2022-06-28 10:27:12 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 1.9461
2022-06-28 10:27:45 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 1.9688
2022-06-28 10:28:18 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.0474
2022-06-28 10:28:51 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 1.8637
2022-06-28 10:29:24 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 1.9695
2022-06-28 10:29:57 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.1173
2022-06-28 10:30:29 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 1.9069
2022-06-28 10:31:02 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 1.8785
2022-06-28 10:31:35 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.0150
2022-06-28 10:32:08 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 1.9440
2022-06-28 10:32:40 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 1.9089
2022-06-28 10:33:13 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.1207
2022-06-28 10:33:46 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.2340
2022-06-28 10:34:19 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 1.9837
2022-06-28 10:34:51 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 1.7954
2022-06-28 10:35:24 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.0611
2022-06-28 10:35:57 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.0176
2022-06-28 10:36:30 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.0218
2022-06-28 10:36:32 - train: epoch 019, train_loss: 1.9663
2022-06-28 10:37:46 - eval: epoch: 019, acc1: 55.666%, acc5: 80.238%, test_loss: 1.8825, per_image_load_time: 2.149ms, per_image_inference_time: 0.685ms
2022-06-28 10:37:47 - until epoch: 019, best_acc1: 55.666%
2022-06-28 10:37:47 - epoch 020 lr: 0.100000
2022-06-28 10:38:25 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.1413
2022-06-28 10:38:57 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 1.7759
2022-06-28 10:39:29 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 1.9630
2022-06-28 10:40:01 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 1.6392
2022-06-28 10:40:33 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 1.8085
2022-06-28 10:41:05 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.1331
2022-06-28 10:41:37 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 1.8019
2022-06-28 10:42:09 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.0268
2022-06-28 10:42:41 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.2462
2022-06-28 10:43:14 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.0381
2022-06-28 10:43:46 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 1.8764
2022-06-28 10:44:18 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 1.6247
2022-06-28 10:44:51 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 1.8321
2022-06-28 10:45:23 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.2605
2022-06-28 10:45:56 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.0156
2022-06-28 10:46:28 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 1.8218
2022-06-28 10:47:01 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 1.6823
2022-06-28 10:47:34 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 1.9191
2022-06-28 10:48:06 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 1.7143
2022-06-28 10:48:39 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 1.9222
2022-06-28 10:49:11 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.1527
2022-06-28 10:49:44 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 1.8029
2022-06-28 10:50:16 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 1.8282
2022-06-28 10:50:49 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.1358
2022-06-28 10:51:22 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 1.9097
2022-06-28 10:51:54 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 1.7550
2022-06-28 10:52:27 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.0119
2022-06-28 10:52:59 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 1.9204
2022-06-28 10:53:32 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.0946
2022-06-28 10:54:05 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.0166
2022-06-28 10:54:37 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.0845
2022-06-28 10:55:10 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.0125
2022-06-28 10:55:42 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 1.7794
2022-06-28 10:56:16 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.0082
2022-06-28 10:56:48 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 1.7770
2022-06-28 10:57:21 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 1.8745
2022-06-28 10:57:54 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 1.8945
2022-06-28 10:58:27 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 1.9677
2022-06-28 10:58:59 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.0581
2022-06-28 10:59:32 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 1.8085
2022-06-28 11:00:05 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 1.9646
2022-06-28 11:00:37 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 1.8920
2022-06-28 11:01:10 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 1.9487
2022-06-28 11:01:43 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.0952
2022-06-28 11:02:16 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.0182
2022-06-28 11:02:49 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.1279
2022-06-28 11:03:22 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 1.8878
2022-06-28 11:03:55 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 1.9266
2022-06-28 11:04:27 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.0358
2022-06-28 11:05:00 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 1.8682
2022-06-28 11:05:02 - train: epoch 020, train_loss: 1.9474
2022-06-28 11:06:17 - eval: epoch: 020, acc1: 58.594%, acc5: 82.214%, test_loss: 1.7503, per_image_load_time: 2.219ms, per_image_inference_time: 0.705ms
2022-06-28 11:06:18 - until epoch: 020, best_acc1: 58.594%
2022-06-28 11:06:18 - epoch 021 lr: 0.100000
2022-06-28 11:06:57 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 1.8367
2022-06-28 11:07:29 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 1.9598
2022-06-28 11:08:02 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.6590
2022-06-28 11:08:34 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.1243
2022-06-28 11:09:06 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 1.9451
2022-06-28 11:09:39 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 1.8425
2022-06-28 11:10:11 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 1.8430
2022-06-28 11:10:44 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.1233
2022-06-28 11:11:16 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 1.9562
2022-06-28 11:11:49 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 1.8091
2022-06-28 11:12:22 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 1.8298
2022-06-28 11:12:54 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 1.8080
2022-06-28 11:13:27 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 1.8545
2022-06-28 11:13:59 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 1.8474
2022-06-28 11:14:32 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 1.6970
2022-06-28 11:15:05 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.0139
2022-06-28 11:15:38 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 1.9774
2022-06-28 11:16:11 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 1.7401
2022-06-28 11:16:44 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.0337
2022-06-28 11:17:16 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.1761
2022-06-28 11:17:49 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 1.7114
2022-06-28 11:18:22 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 1.9407
2022-06-28 11:18:55 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 1.7943
2022-06-28 11:19:28 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 1.8587
2022-06-28 11:20:01 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 1.9483
2022-06-28 11:20:34 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.2024
2022-06-28 11:21:07 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 1.8325
2022-06-28 11:21:40 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 1.9251
2022-06-28 11:22:13 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 1.7351
2022-06-28 11:22:46 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.0815
2022-06-28 11:23:19 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 1.9837
2022-06-28 11:23:52 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 1.9365
2022-06-28 11:24:25 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.1810
2022-06-28 11:24:58 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.1696
2022-06-28 11:25:31 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 1.8784
2022-06-28 11:26:04 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 1.8966
2022-06-28 11:26:37 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 1.9616
2022-06-28 11:27:10 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 1.9178
2022-06-28 11:27:43 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 1.8604
2022-06-28 11:28:16 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.2165
2022-06-28 11:28:49 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 1.8877
2022-06-28 11:29:22 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 1.9065
2022-06-28 11:29:55 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 1.8198
2022-06-28 11:30:27 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.0641
2022-06-28 11:31:00 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.0031
2022-06-28 11:31:33 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 1.8047
2022-06-28 11:32:06 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.0622
2022-06-28 11:32:39 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.0777
2022-06-28 11:33:12 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 1.7917
2022-06-28 11:33:45 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 1.9809
2022-06-28 11:33:46 - train: epoch 021, train_loss: 1.9614
2022-06-28 11:35:01 - eval: epoch: 021, acc1: 57.482%, acc5: 81.930%, test_loss: 1.7830, per_image_load_time: 1.957ms, per_image_inference_time: 0.722ms
2022-06-28 11:35:02 - until epoch: 021, best_acc1: 58.594%
2022-06-28 11:35:02 - epoch 022 lr: 0.100000
2022-06-28 11:35:41 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 1.7846
2022-06-28 11:36:13 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 1.8522
2022-06-28 11:36:45 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 1.6462
2022-06-28 11:37:18 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 1.8214
2022-06-28 11:37:50 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 1.9029
2022-06-28 11:38:23 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.0290
2022-06-28 11:38:55 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 1.9386
2022-06-28 11:39:28 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.0219
2022-06-28 11:40:00 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.0392
2022-06-28 11:40:32 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 1.9050
2022-06-28 11:41:05 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 1.9197
2022-06-28 11:41:38 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.5759
2022-06-28 11:42:10 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 1.9446
2022-06-28 11:42:43 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 1.9470
2022-06-28 11:43:15 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 1.9832
2022-06-28 11:43:48 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 1.6591
2022-06-28 11:44:21 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 1.8182
2022-06-28 11:44:54 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.0662
2022-06-28 11:45:27 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 1.7759
2022-06-28 11:46:00 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.0143
2022-06-28 11:46:32 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.0231
2022-06-28 11:47:05 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 1.7310
2022-06-28 11:47:38 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 1.9568
2022-06-28 11:48:11 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.0672
2022-06-28 11:48:44 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 1.9181
2022-06-28 11:49:16 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 1.7072
2022-06-28 11:49:49 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 1.6988
2022-06-28 11:50:22 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.2663
2022-06-28 11:50:55 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 1.8450
2022-06-28 11:51:27 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.0664
2022-06-28 11:52:00 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.0701
2022-06-28 11:52:33 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.1259
2022-06-28 11:53:06 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 1.8902
2022-06-28 11:53:39 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 1.7294
2022-06-28 11:54:11 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 1.9689
2022-06-28 11:54:45 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 1.9087
2022-06-28 11:55:17 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.0145
2022-06-28 11:55:50 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 1.9902
2022-06-28 11:56:23 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 1.8476
2022-06-28 11:56:57 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 1.9745
2022-06-28 11:57:29 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 1.8593
2022-06-28 11:58:03 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 1.9324
2022-06-28 11:58:35 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.1160
2022-06-28 11:59:08 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 1.9503
2022-06-28 11:59:41 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 1.9765
2022-06-28 12:00:14 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.1196
2022-06-28 12:00:47 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.0825
2022-06-28 12:01:19 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 1.7886
2022-06-28 12:01:52 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 1.8224
2022-06-28 12:02:25 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 1.8777
2022-06-28 12:02:26 - train: epoch 022, train_loss: 1.9356
2022-06-28 12:03:42 - eval: epoch: 022, acc1: 57.738%, acc5: 81.612%, test_loss: 1.7896, per_image_load_time: 2.208ms, per_image_inference_time: 0.700ms
2022-06-28 12:03:42 - until epoch: 022, best_acc1: 58.594%
2022-06-28 12:03:42 - epoch 023 lr: 0.100000
2022-06-28 12:04:21 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 1.7896
2022-06-28 12:04:53 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 1.5684
2022-06-28 12:05:26 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 1.8462
2022-06-28 12:05:58 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 1.9312
2022-06-28 12:06:31 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 1.9563
2022-06-28 12:07:04 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 1.8140
2022-06-28 12:07:36 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 1.6879
2022-06-28 12:08:09 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 1.8925
2022-06-28 12:08:41 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 1.8593
2022-06-28 12:09:14 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 1.8584
2022-06-28 12:09:47 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.0919
2022-06-28 12:10:20 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 1.7796
2022-06-28 12:10:53 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 1.8519
2022-06-28 12:11:26 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 1.9658
2022-06-28 12:11:59 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 1.6698
2022-06-28 12:12:31 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 1.9784
2022-06-28 12:13:04 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.1057
2022-06-28 12:13:37 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 1.7996
2022-06-28 12:14:10 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.0098
2022-06-28 12:14:43 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.6986
2022-06-28 12:15:16 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.0257
2022-06-28 12:15:49 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.7477
2022-06-28 12:16:22 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 1.8855
2022-06-28 12:16:55 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 1.9104
2022-06-28 12:17:28 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 1.8755
2022-06-28 12:18:01 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.0372
2022-06-28 12:18:34 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 1.8844
2022-06-28 12:19:07 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 1.9920
2022-06-28 12:19:40 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.0501
2022-06-28 12:20:13 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.2152
2022-06-28 12:20:46 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.0422
2022-06-28 12:21:19 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.0900
2022-06-28 12:21:52 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 1.9218
2022-06-28 12:22:25 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.0188
2022-06-28 12:22:58 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 1.7240
2022-06-28 12:23:31 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 1.7292
2022-06-28 12:24:04 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 1.9431
2022-06-28 12:24:37 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.0297
2022-06-28 12:25:10 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 1.9903
2022-06-28 12:25:43 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 1.8088
2022-06-28 12:26:16 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 1.8295
2022-06-28 12:26:49 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 1.8746
2022-06-28 12:27:22 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 1.7852
2022-06-28 12:27:55 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 1.7504
2022-06-28 12:28:28 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 1.7502
2022-06-28 12:29:02 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.0439
2022-06-28 12:29:35 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 1.6931
2022-06-28 12:30:08 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 1.8267
2022-06-28 12:30:41 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 1.7685
2022-06-28 12:31:14 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 1.9351
2022-06-28 12:31:16 - train: epoch 023, train_loss: 1.9242
2022-06-28 12:32:30 - eval: epoch: 023, acc1: 57.602%, acc5: 81.710%, test_loss: 1.7860, per_image_load_time: 1.082ms, per_image_inference_time: 0.710ms
2022-06-28 12:32:31 - until epoch: 023, best_acc1: 58.594%
2022-06-28 12:32:31 - epoch 024 lr: 0.100000
2022-06-28 12:33:10 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 1.8678
2022-06-28 12:33:42 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 1.9813
2022-06-28 12:34:15 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 1.9601
2022-06-28 12:34:47 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 1.9767
2022-06-28 12:35:20 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 1.8477
2022-06-28 12:35:53 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 1.7253
2022-06-28 12:36:26 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 1.8319
2022-06-28 12:36:58 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 1.8755
2022-06-28 12:37:31 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 1.8639
2022-06-28 12:38:04 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 1.8592
2022-06-28 12:38:36 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.6043
2022-06-28 12:39:09 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 1.7986
2022-06-28 12:39:42 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.1445
2022-06-28 12:40:14 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 1.8649
2022-06-28 12:40:47 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.1307
2022-06-28 12:41:20 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.0032
2022-06-28 12:41:53 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 1.9431
2022-06-28 12:42:26 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.3564
2022-06-28 12:42:59 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 1.6805
2022-06-28 12:43:32 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 1.9220
2022-06-28 12:44:05 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 1.8471
2022-06-28 12:44:38 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 1.7071
2022-06-28 12:45:11 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 1.8696
2022-06-28 12:45:44 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 1.8412
2022-06-28 12:46:17 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 1.8561
2022-06-28 12:46:49 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 1.8458
2022-06-28 12:47:22 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.0171
2022-06-28 12:47:55 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.0207
2022-06-28 12:48:28 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 1.9373
2022-06-28 12:49:01 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 1.7826
2022-06-28 12:49:34 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 1.8179
2022-06-28 12:50:06 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.1576
2022-06-28 12:50:39 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.6021
2022-06-28 12:51:12 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.0465
2022-06-28 12:51:45 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 1.8268
2022-06-28 12:52:18 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.0200
2022-06-28 12:52:51 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 1.8178
2022-06-28 12:53:24 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.1553
2022-06-28 12:53:57 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 1.9107
2022-06-28 12:54:29 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.0153
2022-06-28 12:55:02 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 1.8881
2022-06-28 12:55:35 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.0082
2022-06-28 12:56:08 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 1.8850
2022-06-28 12:56:41 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.0159
2022-06-28 12:57:14 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 1.8860
2022-06-28 12:57:47 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.0738
2022-06-28 12:58:20 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 1.9031
2022-06-28 12:58:53 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 1.8376
2022-06-28 12:59:25 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 1.9432
2022-06-28 12:59:58 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 1.9539
2022-06-28 12:59:59 - train: epoch 024, train_loss: 1.9195
2022-06-28 13:01:15 - eval: epoch: 024, acc1: 57.406%, acc5: 81.732%, test_loss: 1.7944, per_image_load_time: 1.571ms, per_image_inference_time: 0.711ms
2022-06-28 13:01:15 - until epoch: 024, best_acc1: 58.594%
2022-06-28 13:01:15 - epoch 025 lr: 0.100000
2022-06-28 13:01:54 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 1.7819
2022-06-28 13:02:27 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 1.7592
2022-06-28 13:02:59 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 1.8087
2022-06-28 13:03:32 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 1.8143
2022-06-28 13:04:05 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 1.7905
2022-06-28 13:04:38 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 1.8192
2022-06-28 13:05:10 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.0132
2022-06-28 13:05:43 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 1.8844
2022-06-28 13:06:16 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 1.8031
2022-06-28 13:06:49 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 1.8954
2022-06-28 13:07:22 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 1.9051
2022-06-28 13:07:55 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 1.9641
2022-06-28 13:08:28 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 1.9161
2022-06-28 13:09:01 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.0450
2022-06-28 13:09:34 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 1.8120
2022-06-28 13:10:07 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 1.6206
2022-06-28 13:10:40 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 1.8625
2022-06-28 13:11:13 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 1.7922
2022-06-28 13:11:46 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 1.8034
2022-06-28 13:12:19 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 1.9451
2022-06-28 13:12:52 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 1.7476
2022-06-28 13:13:25 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 1.8446
2022-06-28 13:13:58 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 1.9142
2022-06-28 13:14:31 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 1.7833
2022-06-28 13:15:04 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 1.9455
2022-06-28 13:15:37 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.0348
2022-06-28 13:16:10 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 1.9841
2022-06-28 13:16:43 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 1.9511
2022-06-28 13:17:16 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.1427
2022-06-28 13:17:50 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.1476
2022-06-28 13:18:23 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 1.8509
2022-06-28 13:18:56 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.1889
2022-06-28 13:19:29 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 1.8401
2022-06-28 13:20:02 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.1728
2022-06-28 13:20:35 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 1.7300
2022-06-28 13:21:08 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.0452
2022-06-28 13:21:42 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.0008
2022-06-28 13:22:15 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.0050
2022-06-28 13:22:48 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.1386
2022-06-28 13:23:21 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.0252
2022-06-28 13:23:54 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.1821
2022-06-28 13:24:27 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 1.9396
2022-06-28 13:25:01 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 1.6952
2022-06-28 13:25:34 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 1.8077
2022-06-28 13:26:07 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 1.8134
2022-06-28 13:26:40 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 1.9094
2022-06-28 13:27:13 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 1.9219
2022-06-28 13:27:46 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 1.7515
2022-06-28 13:28:19 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 1.9431
2022-06-28 13:28:51 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.0199
2022-06-28 13:28:53 - train: epoch 025, train_loss: 1.9099
2022-06-28 13:30:08 - eval: epoch: 025, acc1: 55.938%, acc5: 80.570%, test_loss: 1.8638, per_image_load_time: 1.717ms, per_image_inference_time: 0.714ms
2022-06-28 13:30:08 - until epoch: 025, best_acc1: 58.594%
2022-06-28 13:30:08 - epoch 026 lr: 0.100000
2022-06-28 13:30:47 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 1.7033
2022-06-28 13:31:20 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.7738
2022-06-28 13:31:52 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 1.7914
2022-06-28 13:32:24 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 1.9009
2022-06-28 13:32:57 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 1.8488
2022-06-28 13:33:29 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.0438
2022-06-28 13:34:02 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 1.8049
2022-06-28 13:34:35 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 1.6908
2022-06-28 13:35:08 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.1257
2022-06-28 13:35:41 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 1.7818
2022-06-28 13:36:14 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 1.8700
2022-06-28 13:36:46 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 1.9626
2022-06-28 13:37:19 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 1.8276
2022-06-28 13:37:52 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 1.9248
2022-06-28 13:38:25 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 1.9029
2022-06-28 13:38:58 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.0871
2022-06-28 13:39:31 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 1.8152
2022-06-28 13:40:03 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 1.9898
2022-06-28 13:40:36 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.2428
2022-06-28 13:41:09 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 1.9940
2022-06-28 13:41:42 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 1.9609
2022-06-28 13:42:15 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 1.8262
2022-06-28 13:42:48 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 1.8835
2022-06-28 13:43:21 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.0949
2022-06-28 13:43:54 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 1.9412
2022-06-28 13:44:26 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 1.9378
2022-06-28 13:44:59 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 1.8271
2022-06-28 13:45:32 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 1.7806
2022-06-28 13:46:05 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 1.9877
2022-06-28 13:46:38 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 1.8457
2022-06-28 13:47:11 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 1.9520
2022-06-28 13:47:44 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 1.9083
2022-06-28 13:48:17 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 1.8199
2022-06-28 13:48:50 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.1276
2022-06-28 13:49:23 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.1048
2022-06-28 13:49:56 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 1.8162
2022-06-28 13:50:29 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.0313
2022-06-28 13:51:02 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.1372
2022-06-28 13:51:35 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 1.9659
2022-06-28 13:52:08 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.1354
2022-06-28 13:52:40 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.0169
2022-06-28 13:53:13 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 1.9879
2022-06-28 13:53:46 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.0387
2022-06-28 13:54:19 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.1292
2022-06-28 13:54:52 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.0268
2022-06-28 13:55:25 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.0260
2022-06-28 13:55:58 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 1.7899
2022-06-28 13:56:31 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 1.8645
2022-06-28 13:57:04 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 1.8756
2022-06-28 13:57:36 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.0638
2022-06-28 13:57:38 - train: epoch 026, train_loss: 1.9057
2022-06-28 13:58:53 - eval: epoch: 026, acc1: 59.224%, acc5: 83.064%, test_loss: 1.7813, per_image_load_time: 1.280ms, per_image_inference_time: 0.715ms
2022-06-28 13:58:54 - until epoch: 026, best_acc1: 59.224%
2022-06-28 13:58:54 - epoch 027 lr: 0.100000
2022-06-28 13:59:32 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.0318
2022-06-28 14:00:04 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 1.8397
2022-06-28 14:00:36 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 1.9693
2022-06-28 14:01:08 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.0605
2022-06-28 14:01:40 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.0063
2022-06-28 14:02:12 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.1783
2022-06-28 14:02:44 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.1129
2022-06-28 14:03:16 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 1.9298
2022-06-28 14:03:49 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 1.7544
2022-06-28 14:04:21 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 1.9465
2022-06-28 14:04:54 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 1.7836
2022-06-28 14:05:26 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.1477
2022-06-28 14:05:59 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 1.8957
2022-06-28 14:06:31 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.0209
2022-06-28 14:07:04 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.0030
2022-06-28 14:07:37 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 1.9441
2022-06-28 14:08:09 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.0194
2022-06-28 14:08:42 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 1.8307
2022-06-28 14:09:14 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.0421
2022-06-28 14:09:47 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 1.7774
2022-06-28 14:10:20 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.1816
2022-06-28 14:10:52 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.0563
2022-06-28 14:11:25 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.1228
2022-06-28 14:11:58 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 1.8374
2022-06-28 14:12:30 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 1.7339
2022-06-28 14:13:03 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.0985
2022-06-28 14:13:36 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 1.8211
2022-06-28 14:14:08 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.1335
2022-06-28 14:14:40 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 1.9427
2022-06-28 14:15:13 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 1.7593
2022-06-28 14:15:46 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 1.8006
2022-06-28 14:16:19 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 1.7163
2022-06-28 14:16:51 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.0072
2022-06-28 14:17:23 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 1.8730
2022-06-28 14:17:56 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.1536
2022-06-28 14:18:28 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 1.7413
2022-06-28 14:19:01 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 1.8092
2022-06-28 14:19:33 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 1.5558
2022-06-28 14:20:06 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 1.7072
2022-06-28 14:20:39 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 1.9159
2022-06-28 14:21:11 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 1.8984
2022-06-28 14:21:44 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 1.9885
2022-06-28 14:22:16 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 1.8806
2022-06-28 14:22:49 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 1.8781
2022-06-28 14:23:21 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 1.8244
2022-06-28 14:23:54 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 1.9007
2022-06-28 14:24:26 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.0681
2022-06-28 14:24:58 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.0773
2022-06-28 14:25:31 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 1.9432
2022-06-28 14:26:03 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.5803
2022-06-28 14:26:04 - train: epoch 027, train_loss: 1.8985
2022-06-28 14:27:19 - eval: epoch: 027, acc1: 58.460%, acc5: 82.142%, test_loss: 1.7736, per_image_load_time: 2.198ms, per_image_inference_time: 0.701ms
2022-06-28 14:27:20 - until epoch: 027, best_acc1: 59.224%
2022-06-28 14:27:20 - epoch 028 lr: 0.100000
2022-06-28 14:27:59 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 1.7135
2022-06-28 14:28:31 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 1.7184
2022-06-28 14:29:03 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 1.8849
2022-06-28 14:29:36 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 1.6334
2022-06-28 14:30:09 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 1.7860
2022-06-28 14:30:40 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 1.9173
2022-06-28 14:31:12 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 1.9937
2022-06-28 14:31:43 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.6112
2022-06-28 14:32:15 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 1.7366
2022-06-28 14:32:47 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 1.9443
2022-06-28 14:33:19 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 1.9841
2022-06-28 14:33:51 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 1.8310
2022-06-28 14:34:22 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 1.8575
2022-06-28 14:34:54 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.1781
2022-06-28 14:35:27 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 1.8271
2022-06-28 14:35:59 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 1.9834
2022-06-28 14:36:30 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 1.8428
2022-06-28 14:37:02 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 1.6944
2022-06-28 14:37:33 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 1.8677
2022-06-28 14:38:05 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 1.9498
2022-06-28 14:38:36 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 1.9246
2022-06-28 14:39:08 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 1.9864
2022-06-28 14:39:40 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.1122
2022-06-28 14:40:12 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.0567
2022-06-28 14:40:43 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 1.8555
2022-06-28 14:41:15 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 1.6856
2022-06-28 14:41:47 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 1.9021
2022-06-28 14:42:19 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 1.9112
2022-06-28 14:42:51 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.0278
2022-06-28 14:43:23 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 1.9546
2022-06-28 14:43:54 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.0839
2022-06-28 14:44:26 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 1.8417
2022-06-28 14:44:58 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 1.9096
2022-06-28 14:45:30 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 1.8798
2022-06-28 14:46:02 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 1.8567
2022-06-28 14:46:33 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 1.7644
2022-06-28 14:47:05 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 1.7947
2022-06-28 14:47:37 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.5252
2022-06-28 14:48:08 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 1.8727
2022-06-28 14:48:40 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 1.9550
2022-06-28 14:49:12 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 1.9310
2022-06-28 14:49:43 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 1.9947
2022-06-28 14:50:15 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 1.7462
2022-06-28 14:50:47 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 1.9102
2022-06-28 14:51:19 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 1.8635
2022-06-28 14:51:51 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 1.9398
2022-06-28 14:52:22 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 1.9707
2022-06-28 14:52:54 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 1.8810
2022-06-28 14:53:26 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 1.6793
2022-06-28 14:53:57 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 1.8509
2022-06-28 14:53:59 - train: epoch 028, train_loss: 1.8904
2022-06-28 14:55:12 - eval: epoch: 028, acc1: 57.214%, acc5: 81.306%, test_loss: 1.8087, per_image_load_time: 2.064ms, per_image_inference_time: 0.697ms
2022-06-28 14:55:13 - until epoch: 028, best_acc1: 59.224%
2022-06-28 14:55:13 - epoch 029 lr: 0.100000
2022-06-28 14:55:51 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 1.9812
2022-06-28 14:56:22 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 1.9432
2022-06-28 14:56:54 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.0740
2022-06-28 14:57:25 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 1.7613
2022-06-28 14:57:57 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 1.8652
2022-06-28 14:58:29 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.0881
2022-06-28 14:59:00 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.4794
2022-06-28 14:59:32 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.0184
2022-06-28 15:00:04 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 1.7752
2022-06-28 15:00:36 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 1.7200
2022-06-28 15:01:07 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 1.8502
2022-06-28 15:01:39 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 1.9825
2022-06-28 15:02:11 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 1.7611
2022-06-28 15:02:43 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.0727
2022-06-28 15:03:15 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 1.9355
2022-06-28 15:03:46 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 1.8869
2022-06-28 15:04:17 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 1.8881
2022-06-28 15:04:49 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.0301
2022-06-28 15:05:21 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 1.6759
2022-06-28 15:05:52 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.0854
2022-06-28 15:06:24 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 1.8585
2022-06-28 15:06:55 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 1.9100
2022-06-28 15:07:27 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 1.9281
2022-06-28 15:07:59 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 1.8291
2022-06-28 15:08:31 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 1.7437
2022-06-28 15:09:02 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.0226
2022-06-28 15:09:34 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 1.8539
2022-06-28 15:10:06 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 1.7464
2022-06-28 15:10:37 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 1.8455
2022-06-28 15:11:09 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 1.9094
2022-06-28 15:11:41 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 1.8100
2022-06-28 15:12:12 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.0157
2022-06-28 15:12:44 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 1.8812
2022-06-28 15:13:16 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 1.6512
2022-06-28 15:13:48 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 1.9568
2022-06-28 15:14:19 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 1.8073
2022-06-28 15:14:51 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 1.7927
2022-06-28 15:15:22 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 1.9318
2022-06-28 15:15:54 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 1.6415
2022-06-28 15:16:26 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 1.7948
2022-06-28 15:16:58 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 1.7293
2022-06-28 15:17:29 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 1.7233
2022-06-28 15:18:01 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 1.9988
2022-06-28 15:18:33 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 1.8949
2022-06-28 15:19:04 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 1.9835
2022-06-28 15:19:36 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.1063
2022-06-28 15:20:08 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 1.6543
2022-06-28 15:20:39 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 1.9088
2022-06-28 15:21:11 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.2256
2022-06-28 15:21:43 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.6591
2022-06-28 15:21:45 - train: epoch 029, train_loss: 1.8906
2022-06-28 15:22:58 - eval: epoch: 029, acc1: 52.474%, acc5: 77.542%, test_loss: 2.0695, per_image_load_time: 1.021ms, per_image_inference_time: 0.673ms
2022-06-28 15:22:58 - until epoch: 029, best_acc1: 59.224%
2022-06-28 15:22:58 - epoch 030 lr: 0.100000
2022-06-28 15:23:37 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.0271
2022-06-28 15:24:08 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 1.8848
2022-06-28 15:24:40 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 1.8304
2022-06-28 15:25:11 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 1.5692
2022-06-28 15:25:42 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.1168
2022-06-28 15:26:14 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 1.6659
2022-06-28 15:26:45 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 1.7775
2022-06-28 15:27:17 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 1.9925
2022-06-28 15:27:48 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 1.8286
2022-06-28 15:28:20 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 1.6150
2022-06-28 15:28:51 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 1.7524
2022-06-28 15:29:23 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 1.8427
2022-06-28 15:29:54 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 1.7739
2022-06-28 15:30:25 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 1.8772
2022-06-28 15:30:57 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 1.8883
2022-06-28 15:31:29 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 1.8577
2022-06-28 15:32:00 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.1191
2022-06-28 15:32:31 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 1.9068
2022-06-28 15:33:03 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.1076
2022-06-28 15:33:34 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 1.8808
2022-06-28 15:34:06 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.0670
2022-06-28 15:34:37 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 1.7754
2022-06-28 15:35:09 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 1.8872
2022-06-28 15:35:40 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.0462
2022-06-28 15:36:12 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 1.9185
2022-06-28 15:36:44 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 1.9235
2022-06-28 15:37:15 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 1.7667
2022-06-28 15:37:47 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 1.8736
2022-06-28 15:38:19 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 1.9731
2022-06-28 15:38:50 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.0325
2022-06-28 15:39:22 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 1.8883
2022-06-28 15:39:53 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 1.9277
2022-06-28 15:40:25 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.1420
2022-06-28 15:40:57 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 1.8952
2022-06-28 15:41:28 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.0394
2022-06-28 15:42:00 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 1.8068
2022-06-28 15:42:32 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 1.8691
2022-06-28 15:43:03 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 1.9437
2022-06-28 15:43:35 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 1.9340
2022-06-28 15:44:07 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 1.7470
2022-06-28 15:44:38 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 1.7373
2022-06-28 15:45:10 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.0270
2022-06-28 15:45:42 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 1.8219
2022-06-28 15:46:13 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.0263
2022-06-28 15:46:45 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.0776
2022-06-28 15:47:17 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 1.6340
2022-06-28 15:47:48 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 1.9480
2022-06-28 15:48:20 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 1.9077
2022-06-28 15:48:52 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.1240
2022-06-28 15:49:23 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 1.9295
2022-06-28 15:49:25 - train: epoch 030, train_loss: 1.8840
2022-06-28 15:50:38 - eval: epoch: 030, acc1: 59.330%, acc5: 83.054%, test_loss: 1.7123, per_image_load_time: 1.586ms, per_image_inference_time: 0.703ms
2022-06-28 15:50:39 - until epoch: 030, best_acc1: 59.330%
2022-06-28 15:50:39 - epoch 031 lr: 0.010000
2022-06-28 15:51:17 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 1.8147
2022-06-28 15:51:48 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.5224
2022-06-28 15:52:20 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.4554
2022-06-28 15:52:51 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.6822
2022-06-28 15:53:22 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.4721
2022-06-28 15:53:54 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.4011
2022-06-28 15:54:26 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.4137
2022-06-28 15:54:57 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.4434
2022-06-28 15:55:29 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.4805
2022-06-28 15:56:01 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 1.6093
2022-06-28 15:56:33 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 1.6464
2022-06-28 15:57:05 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.3978
2022-06-28 15:57:36 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.2010
2022-06-28 15:58:08 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.3695
2022-06-28 15:58:40 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.4397
2022-06-28 15:59:11 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.3170
2022-06-28 15:59:43 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.3239
2022-06-28 16:00:15 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.3369
2022-06-28 16:00:46 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.5242
2022-06-28 16:01:18 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.3862
2022-06-28 16:01:49 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.2451
2022-06-28 16:02:21 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.2327
2022-06-28 16:02:53 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.2506
2022-06-28 16:03:24 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.4171
2022-06-28 16:03:56 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.2798
2022-06-28 16:04:28 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.3980
2022-06-28 16:04:59 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.3693
2022-06-28 16:05:31 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.6444
2022-06-28 16:06:03 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.4090
2022-06-28 16:06:34 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.5027
2022-06-28 16:07:06 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.3382
2022-06-28 16:07:38 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.4920
2022-06-28 16:08:09 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.5149
2022-06-28 16:08:41 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.4433
2022-06-28 16:09:13 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.4890
2022-06-28 16:09:44 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.3670
2022-06-28 16:10:16 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.3487
2022-06-28 16:10:47 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.2754
2022-06-28 16:11:19 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.2819
2022-06-28 16:11:50 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.2571
2022-06-28 16:12:22 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.2994
2022-06-28 16:12:53 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.3954
2022-06-28 16:13:25 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.2014
2022-06-28 16:13:57 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.4748
2022-06-28 16:14:29 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.4175
2022-06-28 16:15:01 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.3352
2022-06-28 16:15:33 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.3728
2022-06-28 16:16:04 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.2523
2022-06-28 16:16:36 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.2554
2022-06-28 16:17:07 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.3086
2022-06-28 16:17:09 - train: epoch 031, train_loss: 1.4011
2022-06-28 16:18:22 - eval: epoch: 031, acc1: 71.690%, acc5: 90.522%, test_loss: 1.1314, per_image_load_time: 1.770ms, per_image_inference_time: 0.681ms
2022-06-28 16:18:23 - until epoch: 031, best_acc1: 71.690%
2022-06-28 16:18:23 - epoch 032 lr: 0.010000
2022-06-28 16:19:01 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.2420
2022-06-28 16:19:33 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.3416
2022-06-28 16:20:05 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.4056
2022-06-28 16:20:36 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.5229
2022-06-28 16:21:08 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.3694
2022-06-28 16:21:40 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.3387
2022-06-28 16:22:11 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.3059
2022-06-28 16:22:43 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.2731
2022-06-28 16:23:15 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.2764
2022-06-28 16:23:46 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.4387
2022-06-28 16:24:18 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.4151
2022-06-28 16:24:50 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.2975
2022-06-28 16:25:21 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.2164
2022-06-28 16:25:53 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.4551
2022-06-28 16:26:25 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.2389
2022-06-28 16:26:57 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.3656
2022-06-28 16:27:28 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.3824
2022-06-28 16:28:00 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.5451
2022-06-28 16:28:32 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.1579
2022-06-28 16:29:03 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.2484
2022-06-28 16:29:35 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.2059
2022-06-28 16:30:07 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.1682
2022-06-28 16:30:38 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.2404
2022-06-28 16:31:10 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.1702
2022-06-28 16:31:41 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.4065
2022-06-28 16:32:13 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.0722
2022-06-28 16:32:45 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.2819
2022-06-28 16:33:16 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.3930
2022-06-28 16:33:48 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.0554
2022-06-28 16:34:20 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.0463
2022-06-28 16:34:51 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.3353
2022-06-28 16:35:22 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.2164
2022-06-28 16:35:54 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.2369
2022-06-28 16:36:25 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.0757
2022-06-28 16:36:57 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.2103
2022-06-28 16:37:29 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.3363
2022-06-28 16:38:00 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.2630
2022-06-28 16:38:32 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.1309
2022-06-28 16:39:04 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.3676
2022-06-28 16:39:35 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.1868
2022-06-28 16:40:07 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.2364
2022-06-28 16:40:39 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.1017
2022-06-28 16:41:10 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.5589
2022-06-28 16:41:42 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.2374
2022-06-28 16:42:14 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.4039
2022-06-28 16:42:45 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.2530
2022-06-28 16:43:17 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.3496
2022-06-28 16:43:49 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.1562
2022-06-28 16:44:21 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.4025
2022-06-28 16:44:52 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.3015
2022-06-28 16:44:54 - train: epoch 032, train_loss: 1.2777
2022-06-28 16:46:07 - eval: epoch: 032, acc1: 72.628%, acc5: 91.088%, test_loss: 1.0845, per_image_load_time: 1.685ms, per_image_inference_time: 0.724ms
2022-06-28 16:46:08 - until epoch: 032, best_acc1: 72.628%
2022-06-28 16:46:08 - epoch 033 lr: 0.010000
2022-06-28 16:46:46 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.0676
2022-06-28 16:47:18 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.3672
2022-06-28 16:47:49 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.1464
2022-06-28 16:48:21 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.0885
2022-06-28 16:48:53 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.2721
2022-06-28 16:49:24 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.0262
2022-06-28 16:49:56 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.3002
2022-06-28 16:50:28 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.2304
2022-06-28 16:50:59 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.2078
2022-06-28 16:51:31 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.2732
2022-06-28 16:52:02 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.1860
2022-06-28 16:52:34 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.2249
2022-06-28 16:53:06 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.1115
2022-06-28 16:53:37 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.3813
2022-06-28 16:54:09 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.4092
2022-06-28 16:54:40 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.3164
2022-06-28 16:55:12 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.0941
2022-06-28 16:55:44 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.4756
2022-06-28 16:56:15 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.2668
2022-06-28 16:56:47 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.1189
2022-06-28 16:57:18 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.2423
2022-06-28 16:57:50 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.4112
2022-06-28 16:58:22 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.1178
2022-06-28 16:58:54 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.4612
2022-06-28 16:59:26 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.2228
2022-06-28 16:59:58 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.0447
2022-06-28 17:00:29 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.3593
2022-06-28 17:01:01 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.1846
2022-06-28 17:01:33 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.3344
2022-06-28 17:02:05 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.2390
2022-06-28 17:02:37 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.2443
2022-06-28 17:03:09 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.2508
2022-06-28 17:03:41 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.3343
2022-06-28 17:04:12 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.1390
2022-06-28 17:04:44 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.1936
2022-06-28 17:05:16 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.3445
2022-06-28 17:05:47 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.1653
2022-06-28 17:06:19 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.1256
2022-06-28 17:06:51 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.2690
2022-06-28 17:07:23 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.2184
2022-06-28 17:07:55 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.2147
2022-06-28 17:08:27 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.1130
2022-06-28 17:09:00 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.4192
2022-06-28 17:09:32 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.2665
2022-06-28 17:10:03 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.4468
2022-06-28 17:10:35 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.1845
2022-06-28 17:11:07 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.0638
2022-06-28 17:11:38 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.5794
2022-06-28 17:12:10 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.1086
2022-06-28 17:12:42 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.2300
2022-06-28 17:12:44 - train: epoch 033, train_loss: 1.2243
2022-06-28 17:13:57 - eval: epoch: 033, acc1: 73.158%, acc5: 91.420%, test_loss: 1.0686, per_image_load_time: 2.013ms, per_image_inference_time: 0.711ms
2022-06-28 17:13:58 - until epoch: 033, best_acc1: 73.158%
2022-06-28 17:13:58 - epoch 034 lr: 0.010000
2022-06-28 17:14:37 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.1343
2022-06-28 17:15:08 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.2054
2022-06-28 17:15:40 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.2296
2022-06-28 17:16:11 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.0159
2022-06-28 17:16:43 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.2481
2022-06-28 17:17:14 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.4200
2022-06-28 17:17:46 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.2175
2022-06-28 17:18:17 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.1436
2022-06-28 17:18:49 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.1626
2022-06-28 17:19:21 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.1280
2022-06-28 17:19:52 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.0807
2022-06-28 17:20:24 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.1786
2022-06-28 17:20:56 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.1711
2022-06-28 17:21:28 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.1931
2022-06-28 17:22:00 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.0733
2022-06-28 17:22:32 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.1272
2022-06-28 17:23:04 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.0741
2022-06-28 17:23:36 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.2964
2022-06-28 17:24:08 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.2390
2022-06-28 17:24:40 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.2480
2022-06-28 17:25:12 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.3443
2022-06-28 17:25:44 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.0812
2022-06-28 17:26:16 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.2912
2022-06-28 17:26:47 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.0430
2022-06-28 17:27:19 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.1890
2022-06-28 17:27:52 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.2333
2022-06-28 17:28:23 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.1385
2022-06-28 17:28:55 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 0.9516
2022-06-28 17:29:27 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.0063
2022-06-28 17:29:59 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.0194
2022-06-28 17:30:31 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.0408
2022-06-28 17:31:03 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.1520
2022-06-28 17:31:35 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.0958
2022-06-28 17:32:06 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.3206
2022-06-28 17:32:38 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.0643
2022-06-28 17:33:10 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.0912
2022-06-28 17:33:43 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.1189
2022-06-28 17:34:15 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.1530
2022-06-28 17:34:47 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.3325
2022-06-28 17:35:19 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.0002
2022-06-28 17:35:51 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.2004
2022-06-28 17:36:23 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.1999
2022-06-28 17:36:55 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.1681
2022-06-28 17:37:27 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.2642
2022-06-28 17:37:59 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.4283
2022-06-28 17:38:31 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.3250
2022-06-28 17:39:03 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.1531
2022-06-28 17:39:35 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.0947
2022-06-28 17:40:07 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.1533
2022-06-28 17:40:39 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.0737
2022-06-28 17:40:40 - train: epoch 034, train_loss: 1.1907
2022-06-28 17:41:53 - eval: epoch: 034, acc1: 73.488%, acc5: 91.628%, test_loss: 1.0565, per_image_load_time: 2.123ms, per_image_inference_time: 0.686ms
2022-06-28 17:41:54 - until epoch: 034, best_acc1: 73.488%
2022-06-28 17:41:54 - epoch 035 lr: 0.010000
2022-06-28 17:42:32 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.0195
2022-06-28 17:43:04 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 0.9029
2022-06-28 17:43:36 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.3588
2022-06-28 17:44:08 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.1154
2022-06-28 17:44:39 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.1825
2022-06-28 17:45:11 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.1148
2022-06-28 17:45:42 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.2616
2022-06-28 17:46:14 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.1528
2022-06-28 17:46:46 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.2991
2022-06-28 17:47:18 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.3228
2022-06-28 17:47:50 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.2960
2022-06-28 17:48:21 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.1106
2022-06-28 17:48:53 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.1409
2022-06-28 17:49:25 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.2383
2022-06-28 17:49:58 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.2707
2022-06-28 17:50:30 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.1488
2022-06-28 17:51:02 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.0802
2022-06-28 17:51:34 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.1459
2022-06-28 17:52:06 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.2022
2022-06-28 17:52:38 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.1641
2022-06-28 17:53:10 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.1514
2022-06-28 17:53:42 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.2923
2022-06-28 17:54:14 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.2762
2022-06-28 17:54:47 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.1727
2022-06-28 17:55:19 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.0606
2022-06-28 17:55:51 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.2977
2022-06-28 17:56:23 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.3045
2022-06-28 17:56:56 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.1665
2022-06-28 17:57:27 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.1576
2022-06-28 17:58:00 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.3211
2022-06-28 17:58:32 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.1896
2022-06-28 17:59:04 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.0318
2022-06-28 17:59:36 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.0953
2022-06-28 18:00:08 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.1495
2022-06-28 18:00:40 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 0.9922
2022-06-28 18:01:12 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.1187
2022-06-28 18:01:44 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.0003
2022-06-28 18:02:16 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.1830
2022-06-28 18:02:49 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.2867
2022-06-28 18:03:21 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.0068
2022-06-28 18:03:52 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.3156
2022-06-28 18:04:24 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.1986
2022-06-28 18:04:56 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.2695
2022-06-28 18:05:28 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.1875
2022-06-28 18:06:00 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.2876
2022-06-28 18:06:32 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.0397
2022-06-28 18:07:04 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.3932
2022-06-28 18:07:36 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.3871
2022-06-28 18:08:08 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.2086
2022-06-28 18:08:40 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.0754
2022-06-28 18:08:42 - train: epoch 035, train_loss: 1.1676
2022-06-28 18:09:55 - eval: epoch: 035, acc1: 73.558%, acc5: 91.702%, test_loss: 1.0481, per_image_load_time: 1.630ms, per_image_inference_time: 0.692ms
2022-06-28 18:09:55 - until epoch: 035, best_acc1: 73.558%
2022-06-28 18:09:55 - epoch 036 lr: 0.010000
2022-06-28 18:10:34 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.2747
2022-06-28 18:11:05 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 0.9490
2022-06-28 18:11:37 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.1257
2022-06-28 18:12:08 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.0903
2022-06-28 18:12:40 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.1290
2022-06-28 18:13:12 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.0639
2022-06-28 18:13:44 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 0.9993
2022-06-28 18:14:16 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 0.9361
2022-06-28 18:14:47 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.2193
2022-06-28 18:15:18 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.1186
2022-06-28 18:15:50 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.2139
2022-06-28 18:16:21 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.0887
2022-06-28 18:16:53 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.1664
2022-06-28 18:17:25 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.3273
2022-06-28 18:17:58 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.2575
2022-06-28 18:18:30 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.0462
2022-06-28 18:19:01 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.1041
2022-06-28 18:19:33 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.0398
2022-06-28 18:20:06 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.1857
2022-06-28 18:20:38 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.1456
2022-06-28 18:21:10 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.1884
2022-06-28 18:21:42 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.1521
2022-06-28 18:22:14 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.3534
2022-06-28 18:22:46 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.2827
2022-06-28 18:23:18 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.0212
2022-06-28 18:23:50 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.2034
2022-06-28 18:24:22 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.0501
2022-06-28 18:24:54 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.0270
2022-06-28 18:25:26 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.0166
2022-06-28 18:25:58 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.2376
2022-06-28 18:26:30 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.1042
2022-06-28 18:27:02 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.2196
2022-06-28 18:27:33 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 0.9975
2022-06-28 18:28:06 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.1052
2022-06-28 18:28:38 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.0865
2022-06-28 18:29:09 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.1889
2022-06-28 18:29:42 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.1639
2022-06-28 18:30:14 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.0138
2022-06-28 18:30:46 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.2280
2022-06-28 18:31:18 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.1893
2022-06-28 18:31:50 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.1216
2022-06-28 18:32:22 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.0756
2022-06-28 18:32:54 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.0925
2022-06-28 18:33:26 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.2504
2022-06-28 18:33:58 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 0.9462
2022-06-28 18:34:30 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 0.9761
2022-06-28 18:35:03 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.1542
2022-06-28 18:35:35 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.1500
2022-06-28 18:36:07 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.1561
2022-06-28 18:36:39 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.1107
2022-06-28 18:36:40 - train: epoch 036, train_loss: 1.1462
2022-06-28 18:37:53 - eval: epoch: 036, acc1: 73.416%, acc5: 91.590%, test_loss: 1.0544, per_image_load_time: 1.882ms, per_image_inference_time: 0.686ms
2022-06-28 18:37:54 - until epoch: 036, best_acc1: 73.558%
2022-06-28 18:37:54 - epoch 037 lr: 0.010000
2022-06-28 18:38:32 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 0.9356
2022-06-28 18:39:03 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 0.8943
2022-06-28 18:39:35 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.0722
2022-06-28 18:40:07 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.2372
2022-06-28 18:40:39 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.0150
2022-06-28 18:41:11 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.1171
2022-06-28 18:41:43 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.2069
2022-06-28 18:42:15 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.0973
2022-06-28 18:42:47 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.4448
2022-06-28 18:43:19 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.1616
2022-06-28 18:43:51 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.1909
2022-06-28 18:44:23 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.0992
2022-06-28 18:44:55 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.1418
2022-06-28 18:45:27 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.1751
2022-06-28 18:45:59 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.0889
2022-06-28 18:46:31 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 0.9649
2022-06-28 18:47:03 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.3537
2022-06-28 18:47:35 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.1965
2022-06-28 18:48:07 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.2254
2022-06-28 18:48:39 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.1314
2022-06-28 18:49:11 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.1149
2022-06-28 18:49:43 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.1375
2022-06-28 18:50:15 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 0.9921
2022-06-28 18:50:47 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.2195
2022-06-28 18:51:19 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.0257
2022-06-28 18:51:51 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.3319
2022-06-28 18:52:23 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.1108
2022-06-28 18:52:54 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.1509
2022-06-28 18:53:26 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.1611
2022-06-28 18:53:58 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.1743
2022-06-28 18:54:30 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.0606
2022-06-28 18:55:02 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.1753
2022-06-28 18:55:34 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.0552
2022-06-28 18:56:07 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 0.9655
2022-06-28 18:56:39 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.1089
2022-06-28 18:57:11 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.3046
2022-06-28 18:57:43 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.2356
2022-06-28 18:58:16 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.0382
2022-06-28 18:58:48 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.3053
2022-06-28 18:59:20 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 0.9896
2022-06-28 18:59:52 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.2019
2022-06-28 19:00:24 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.2315
2022-06-28 19:00:56 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.1790
2022-06-28 19:01:28 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.1147
2022-06-28 19:02:00 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.1510
2022-06-28 19:02:32 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.1661
2022-06-28 19:03:03 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.1554
2022-06-28 19:03:36 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.1004
2022-06-28 19:04:07 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.1942
2022-06-28 19:04:39 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.0894
2022-06-28 19:04:41 - train: epoch 037, train_loss: 1.1358
2022-06-28 19:05:54 - eval: epoch: 037, acc1: 73.678%, acc5: 91.632%, test_loss: 1.0532, per_image_load_time: 1.423ms, per_image_inference_time: 0.693ms
2022-06-28 19:05:55 - until epoch: 037, best_acc1: 73.678%
2022-06-28 19:05:55 - epoch 038 lr: 0.010000
2022-06-28 19:06:33 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.0621
2022-06-28 19:07:05 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 0.8588
2022-06-28 19:07:36 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 0.7508
2022-06-28 19:08:08 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.0147
2022-06-28 19:08:40 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 0.9681
2022-06-28 19:09:12 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.1426
2022-06-28 19:09:44 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.0686
2022-06-28 19:10:16 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.0063
2022-06-28 19:10:48 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.0548
2022-06-28 19:11:20 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.2213
2022-06-28 19:11:52 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.0781
2022-06-28 19:12:23 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.1113
2022-06-28 19:12:55 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.2631
2022-06-28 19:13:27 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.1611
2022-06-28 19:13:59 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.1941
2022-06-28 19:14:31 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.1623
2022-06-28 19:15:03 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.2652
2022-06-28 19:15:35 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.1851
2022-06-28 19:16:07 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.2132
2022-06-28 19:16:39 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.1258
2022-06-28 19:17:11 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.2199
2022-06-28 19:17:43 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.0320
2022-06-28 19:18:16 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.2058
2022-06-28 19:18:48 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.2688
2022-06-28 19:19:20 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.0601
2022-06-28 19:19:52 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.1644
2022-06-28 19:20:24 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.2423
2022-06-28 19:20:56 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.1632
2022-06-28 19:21:28 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.2666
2022-06-28 19:22:00 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.2149
2022-06-28 19:22:32 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.0766
2022-06-28 19:23:04 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 0.8720
2022-06-28 19:23:36 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 0.9091
2022-06-28 19:24:09 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.0792
2022-06-28 19:24:41 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.2004
2022-06-28 19:25:13 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.1650
2022-06-28 19:25:45 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 0.9599
2022-06-28 19:26:17 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.1849
2022-06-28 19:26:50 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.1322
2022-06-28 19:27:22 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.0519
2022-06-28 19:27:54 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.1284
2022-06-28 19:28:26 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.0133
2022-06-28 19:28:58 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.2802
2022-06-28 19:29:30 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.2851
2022-06-28 19:30:02 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.2503
2022-06-28 19:30:34 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.3329
2022-06-28 19:31:06 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.0374
2022-06-28 19:31:38 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.1515
2022-06-28 19:32:10 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.1859
2022-06-28 19:32:41 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 0.9661
2022-06-28 19:32:43 - train: epoch 038, train_loss: 1.1237
2022-06-28 19:33:58 - eval: epoch: 038, acc1: 72.396%, acc5: 90.936%, test_loss: 1.0993, per_image_load_time: 2.199ms, per_image_inference_time: 0.683ms
2022-06-28 19:33:58 - until epoch: 038, best_acc1: 73.678%
2022-06-28 19:33:58 - epoch 039 lr: 0.010000
2022-06-28 19:34:37 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.1575
2022-06-28 19:35:09 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.3198
2022-06-28 19:35:40 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 0.9798
2022-06-28 19:36:12 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.1127
2022-06-28 19:36:44 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.0051
2022-06-28 19:37:15 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.0458
2022-06-28 19:37:47 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.2287
2022-06-28 19:38:19 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.0438
2022-06-28 19:38:50 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.1972
2022-06-28 19:39:22 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.0557
2022-06-28 19:39:54 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.1699
2022-06-28 19:40:25 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.1762
2022-06-28 19:40:57 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.2819
2022-06-28 19:41:29 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.1673
2022-06-28 19:42:01 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.0593
2022-06-28 19:42:33 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.1989
2022-06-28 19:43:05 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 0.9509
2022-06-28 19:43:37 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.0436
2022-06-28 19:44:09 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 0.9257
2022-06-28 19:44:41 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 0.9471
2022-06-28 19:45:13 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.1123
2022-06-28 19:45:45 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.1085
2022-06-28 19:46:18 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.2884
2022-06-28 19:46:50 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.2305
2022-06-28 19:47:22 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.0813
2022-06-28 19:47:54 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.1211
2022-06-28 19:48:27 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.3000
2022-06-28 19:48:59 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 0.9711
2022-06-28 19:49:31 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.0120
2022-06-28 19:50:03 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.1963
2022-06-28 19:50:36 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.0172
2022-06-28 19:51:08 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.1003
2022-06-28 19:51:40 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.2021
2022-06-28 19:52:12 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.2090
2022-06-28 19:52:44 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.2847
2022-06-28 19:53:16 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.2846
2022-06-28 19:53:48 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.0920
2022-06-28 19:54:20 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 0.9970
2022-06-28 19:54:52 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.1644
2022-06-28 19:55:25 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.1791
2022-06-28 19:55:57 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.1872
2022-06-28 19:56:28 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.0982
2022-06-28 19:57:01 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.0983
2022-06-28 19:57:33 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 0.9378
2022-06-28 19:58:05 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.0636
2022-06-28 19:58:37 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.3495
2022-06-28 19:59:09 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.1021
2022-06-28 19:59:41 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.1762
2022-06-28 20:00:13 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 0.9696
2022-06-28 20:00:45 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 0.9817
2022-06-28 20:00:46 - train: epoch 039, train_loss: 1.1155
2022-06-28 20:02:00 - eval: epoch: 039, acc1: 73.508%, acc5: 91.786%, test_loss: 1.0481, per_image_load_time: 1.566ms, per_image_inference_time: 0.673ms
2022-06-28 20:02:01 - until epoch: 039, best_acc1: 73.678%
2022-06-28 20:02:01 - epoch 040 lr: 0.010000
2022-06-28 20:02:39 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.2124
2022-06-28 20:03:10 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.3798
2022-06-28 20:03:42 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.1386
2022-06-28 20:04:13 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.0746
2022-06-28 20:04:44 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.0747
2022-06-28 20:05:15 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.0999
2022-06-28 20:05:47 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.1122
2022-06-28 20:06:18 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.1827
2022-06-28 20:06:50 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.0086
2022-06-28 20:07:21 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 0.8351
2022-06-28 20:07:53 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.0740
2022-06-28 20:08:25 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.0495
2022-06-28 20:08:56 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.0118
2022-06-28 20:09:28 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.0087
2022-06-28 20:09:59 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.2467
2022-06-28 20:10:31 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.2187
2022-06-28 20:11:03 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.1674
2022-06-28 20:11:34 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 0.9796
2022-06-28 20:12:06 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.1258
2022-06-28 20:12:38 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.1483
2022-06-28 20:13:10 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.0016
2022-06-28 20:13:42 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 0.9688
2022-06-28 20:14:14 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 0.9869
2022-06-28 20:14:46 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.1724
2022-06-28 20:15:18 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.1702
2022-06-28 20:15:50 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.0743
2022-06-28 20:16:22 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.1325
2022-06-28 20:16:54 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.0805
2022-06-28 20:17:25 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.3437
2022-06-28 20:17:57 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.2794
2022-06-28 20:18:29 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 0.9981
2022-06-28 20:19:01 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.3378
2022-06-28 20:19:33 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.1494
2022-06-28 20:20:05 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.1948
2022-06-28 20:20:36 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.2490
2022-06-28 20:21:08 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.1379
2022-06-28 20:21:40 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.1450
2022-06-28 20:22:12 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 0.9657
2022-06-28 20:22:44 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.1231
2022-06-28 20:23:16 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.2353
2022-06-28 20:23:48 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.2099
2022-06-28 20:24:20 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 0.9973
2022-06-28 20:24:52 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.2370
2022-06-28 20:25:24 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.1853
2022-06-28 20:25:56 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 0.9111
2022-06-28 20:26:28 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.1330
2022-06-28 20:27:00 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.0963
2022-06-28 20:27:32 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.0828
2022-06-28 20:28:04 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.2413
2022-06-28 20:28:36 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.2411
2022-06-28 20:28:38 - train: epoch 040, train_loss: 1.1100
2022-06-28 20:29:51 - eval: epoch: 040, acc1: 72.974%, acc5: 91.564%, test_loss: 1.0653, per_image_load_time: 1.711ms, per_image_inference_time: 0.695ms
2022-06-28 20:29:52 - until epoch: 040, best_acc1: 73.678%
2022-06-28 20:29:52 - epoch 041 lr: 0.010000
2022-06-28 20:30:30 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.1865
2022-06-28 20:31:01 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.1342
2022-06-28 20:31:33 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.0286
2022-06-28 20:32:05 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.0308
2022-06-28 20:32:36 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 0.9795
2022-06-28 20:33:08 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.0764
2022-06-28 20:33:39 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.0343
2022-06-28 20:34:11 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 0.8317
2022-06-28 20:34:43 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 0.9614
2022-06-28 20:35:15 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.2496
2022-06-28 20:35:47 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 0.9329
2022-06-28 20:36:18 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 0.9290
2022-06-28 20:36:50 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.1045
2022-06-28 20:37:22 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.2486
2022-06-28 20:37:54 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.3338
2022-06-28 20:38:26 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 0.9614
2022-06-28 20:38:58 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.1375
2022-06-28 20:39:30 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.0995
2022-06-28 20:40:02 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.2315
2022-06-28 20:40:34 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.0247
2022-06-28 20:41:06 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.0433
2022-06-28 20:41:38 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 0.9787
2022-06-28 20:42:10 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 0.9036
2022-06-28 20:42:43 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.2058
2022-06-28 20:43:15 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 0.9723
2022-06-28 20:43:47 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.1769
2022-06-28 20:44:19 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.2595
2022-06-28 20:44:51 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.0687
2022-06-28 20:45:23 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.2225
2022-06-28 20:45:56 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.2007
2022-06-28 20:46:28 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.2318
2022-06-28 20:47:00 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 0.9245
2022-06-28 20:47:32 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 0.9743
2022-06-28 20:48:04 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.0537
2022-06-28 20:48:36 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.1495
2022-06-28 20:49:08 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.1919
2022-06-28 20:49:40 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.1045
2022-06-28 20:50:12 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.0093
2022-06-28 20:50:44 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.1724
2022-06-28 20:51:16 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 0.9441
2022-06-28 20:51:48 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.1161
2022-06-28 20:52:20 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.0569
2022-06-28 20:52:53 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.1267
2022-06-28 20:53:25 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.0775
2022-06-28 20:53:57 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.1522
2022-06-28 20:54:29 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.1555
2022-06-28 20:55:01 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.3071
2022-06-28 20:55:33 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.1217
2022-06-28 20:56:05 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.1093
2022-06-28 20:56:37 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.2603
2022-06-28 20:56:38 - train: epoch 041, train_loss: 1.1076
2022-06-28 20:57:53 - eval: epoch: 041, acc1: 73.116%, acc5: 91.502%, test_loss: 1.0613, per_image_load_time: 1.112ms, per_image_inference_time: 0.676ms
2022-06-28 20:57:53 - until epoch: 041, best_acc1: 73.678%
2022-06-28 20:57:53 - epoch 042 lr: 0.010000
2022-06-28 20:58:32 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 0.7918
2022-06-28 20:59:03 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.1408
2022-06-28 20:59:35 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.1730
2022-06-28 21:00:07 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 0.8870
2022-06-28 21:00:38 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.1563
2022-06-28 21:01:10 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 0.9309
2022-06-28 21:01:42 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.0490
2022-06-28 21:02:14 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 0.9853
2022-06-28 21:02:46 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.2511
2022-06-28 21:03:18 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.1485
2022-06-28 21:03:50 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 0.9299
2022-06-28 21:04:22 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.1277
2022-06-28 21:04:54 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.2378
2022-06-28 21:05:26 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.2901
2022-06-28 21:05:58 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.0365
2022-06-28 21:06:30 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.1928
2022-06-28 21:07:02 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.0623
2022-06-28 21:07:34 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.0446
2022-06-28 21:08:06 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.2251
2022-06-28 21:08:38 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 0.9969
2022-06-28 21:09:10 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 0.9219
2022-06-28 21:09:42 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.0166
2022-06-28 21:10:14 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.0644
2022-06-28 21:10:45 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.3229
2022-06-28 21:11:17 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.1955
2022-06-28 21:11:50 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.1462
2022-06-28 21:12:21 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 0.9668
2022-06-28 21:12:53 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.0650
2022-06-28 21:13:25 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.0773
2022-06-28 21:13:57 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.1649
2022-06-28 21:14:29 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.0851
2022-06-28 21:15:01 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.1438
2022-06-28 21:15:33 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.2269
2022-06-28 21:16:05 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.0892
2022-06-28 21:16:37 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.0891
2022-06-28 21:17:08 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.2490
2022-06-28 21:17:40 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.0772
2022-06-28 21:18:12 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 0.9554
2022-06-28 21:18:44 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.0179
2022-06-28 21:19:16 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.1514
2022-06-28 21:19:48 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.2157
2022-06-28 21:20:20 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.2106
2022-06-28 21:20:52 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 0.9983
2022-06-28 21:21:24 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.0516
2022-06-28 21:21:56 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.1060
2022-06-28 21:22:28 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.3200
2022-06-28 21:23:00 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.0508
2022-06-28 21:23:32 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.1741
2022-06-28 21:24:04 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.1489
2022-06-28 21:24:36 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.0983
2022-06-28 21:24:37 - train: epoch 042, train_loss: 1.1025
2022-06-28 21:25:50 - eval: epoch: 042, acc1: 72.814%, acc5: 91.354%, test_loss: 1.0836, per_image_load_time: 1.931ms, per_image_inference_time: 0.702ms
2022-06-28 21:25:51 - until epoch: 042, best_acc1: 73.678%
2022-06-28 21:25:51 - epoch 043 lr: 0.010000
2022-06-28 21:26:29 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.1239
2022-06-28 21:27:01 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.1075
2022-06-28 21:27:32 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 0.8627
2022-06-28 21:28:04 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 0.9572
2022-06-28 21:28:36 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.1832
2022-06-28 21:29:08 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.0627
2022-06-28 21:29:40 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.1550
2022-06-28 21:30:12 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.2473
2022-06-28 21:30:43 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.0608
2022-06-28 21:31:15 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.2171
2022-06-28 21:31:47 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.1831
2022-06-28 21:32:19 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.1990
2022-06-28 21:32:51 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.1938
2022-06-28 21:33:23 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.0884
2022-06-28 21:33:55 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 0.9930
2022-06-28 21:34:28 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.1200
2022-06-28 21:35:00 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.1838
2022-06-28 21:35:32 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.0899
2022-06-28 21:36:04 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.1143
2022-06-28 21:36:36 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 0.9069
2022-06-28 21:37:08 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.1052
2022-06-28 21:37:39 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.1023
2022-06-28 21:38:11 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.3219
2022-06-28 21:38:43 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 0.9296
2022-06-28 21:39:15 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.3976
2022-06-28 21:39:47 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 0.8434
2022-06-28 21:40:19 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.1126
2022-06-28 21:40:51 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.0501
2022-06-28 21:41:23 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.1713
2022-06-28 21:41:55 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.3131
2022-06-28 21:42:27 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.2342
2022-06-28 21:42:59 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.1528
2022-06-28 21:43:31 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.0880
2022-06-28 21:44:03 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.1731
2022-06-28 21:44:35 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.0788
2022-06-28 21:45:07 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.1209
2022-06-28 21:45:39 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.1309
2022-06-28 21:46:11 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.1629
2022-06-28 21:46:43 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.1505
2022-06-28 21:47:15 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.1494
2022-06-28 21:47:47 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.3538
2022-06-28 21:48:19 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.1815
2022-06-28 21:48:51 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.0714
2022-06-28 21:49:23 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.0835
2022-06-28 21:49:55 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.0604
2022-06-28 21:50:27 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.1567
2022-06-28 21:50:59 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.0548
2022-06-28 21:51:31 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.0137
2022-06-28 21:52:03 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.0465
2022-06-28 21:52:35 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.0226
2022-06-28 21:52:37 - train: epoch 043, train_loss: 1.0990
2022-06-28 21:53:51 - eval: epoch: 043, acc1: 72.912%, acc5: 91.450%, test_loss: 1.0722, per_image_load_time: 1.452ms, per_image_inference_time: 0.698ms
2022-06-28 21:53:51 - until epoch: 043, best_acc1: 73.678%
2022-06-28 21:53:51 - epoch 044 lr: 0.010000
2022-06-28 21:54:30 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.0635
2022-06-28 21:55:02 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.1509
2022-06-28 21:55:34 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.1007
2022-06-28 21:56:06 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 0.8705
2022-06-28 21:56:38 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.0433
2022-06-28 21:57:10 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 0.9415
2022-06-28 21:57:42 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 0.8348
2022-06-28 21:58:14 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.0431
2022-06-28 21:58:46 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.1407
2022-06-28 21:59:18 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.1056
2022-06-28 21:59:50 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.0996
2022-06-28 22:00:22 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.0184
2022-06-28 22:00:54 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.1491
2022-06-28 22:01:26 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.0276
2022-06-28 22:01:58 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 0.9461
2022-06-28 22:02:30 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 0.9274
2022-06-28 22:03:03 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.0676
2022-06-28 22:03:35 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.1397
2022-06-28 22:04:06 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.2166
2022-06-28 22:04:38 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 0.9991
2022-06-28 22:05:10 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.1603
2022-06-28 22:05:42 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.0918
2022-06-28 22:06:14 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.1736
2022-06-28 22:06:46 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.0758
2022-06-28 22:07:18 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.1800
2022-06-28 22:07:50 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.0506
2022-06-28 22:08:22 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.1756
2022-06-28 22:08:54 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 0.9305
2022-06-28 22:09:26 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 0.9329
2022-06-28 22:09:58 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 0.9463
2022-06-28 22:10:29 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.2127
2022-06-28 22:11:01 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 0.8962
2022-06-28 22:11:33 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.0161
2022-06-28 22:12:05 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.2643
2022-06-28 22:12:37 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.0686
2022-06-28 22:13:09 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.3275
2022-06-28 22:13:41 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.2616
2022-06-28 22:14:13 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 0.9205
2022-06-28 22:14:45 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.1950
2022-06-28 22:15:17 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.1199
2022-06-28 22:15:49 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.0292
2022-06-28 22:16:21 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.1084
2022-06-28 22:16:53 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.2911
2022-06-28 22:17:25 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.0468
2022-06-28 22:17:57 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.1690
2022-06-28 22:18:29 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.2106
2022-06-28 22:19:00 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.1682
2022-06-28 22:19:32 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.3098
2022-06-28 22:20:04 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 0.9252
2022-06-28 22:20:36 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.2241
2022-06-28 22:20:37 - train: epoch 044, train_loss: 1.0992
2022-06-28 22:21:51 - eval: epoch: 044, acc1: 72.812%, acc5: 91.234%, test_loss: 1.0836, per_image_load_time: 1.425ms, per_image_inference_time: 0.682ms
2022-06-28 22:21:52 - until epoch: 044, best_acc1: 73.678%
2022-06-28 22:21:52 - epoch 045 lr: 0.010000
2022-06-28 22:22:31 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 0.9879
2022-06-28 22:23:02 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.0592
2022-06-28 22:23:34 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.1227
2022-06-28 22:24:05 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.0012
2022-06-28 22:24:38 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.1855
2022-06-28 22:25:09 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.0697
2022-06-28 22:25:41 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 0.8987
2022-06-28 22:26:13 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.0110
2022-06-28 22:26:45 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.1245
2022-06-28 22:27:17 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.0286
2022-06-28 22:27:49 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.1998
2022-06-28 22:28:21 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.1341
2022-06-28 22:28:53 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.1745
2022-06-28 22:29:25 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.0452
2022-06-28 22:29:57 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.2520
2022-06-28 22:30:29 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.0845
2022-06-28 22:31:01 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 0.9828
2022-06-28 22:31:33 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 0.9527
2022-06-28 22:32:05 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.0386
2022-06-28 22:32:37 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.0757
2022-06-28 22:33:09 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.2353
2022-06-28 22:33:42 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.1419
2022-06-28 22:34:14 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.1390
2022-06-28 22:34:46 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.2680
2022-06-28 22:35:18 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.2243
2022-06-28 22:35:50 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.0945
2022-06-28 22:36:22 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 0.9691
2022-06-28 22:36:54 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.1733
2022-06-28 22:37:26 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.2259
2022-06-28 22:37:58 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.2649
2022-06-28 22:38:30 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.1290
2022-06-28 22:39:03 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.1724
2022-06-28 22:39:35 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.1273
2022-06-28 22:40:06 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 0.9012
2022-06-28 22:40:38 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.2454
2022-06-28 22:41:10 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.1526
2022-06-28 22:41:42 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.0624
2022-06-28 22:42:14 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 0.9436
2022-06-28 22:42:46 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.2258
2022-06-28 22:43:18 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.1615
2022-06-28 22:43:50 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 0.9814
2022-06-28 22:44:22 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.1576
2022-06-28 22:44:54 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.2584
2022-06-28 22:45:26 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.1939
2022-06-28 22:45:58 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.0001
2022-06-28 22:46:30 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.2179
2022-06-28 22:47:02 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.1585
2022-06-28 22:47:35 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.0927
2022-06-28 22:48:07 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.1212
2022-06-28 22:48:39 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.1269
2022-06-28 22:48:40 - train: epoch 045, train_loss: 1.0929
2022-06-28 22:49:54 - eval: epoch: 045, acc1: 73.150%, acc5: 91.620%, test_loss: 1.0621, per_image_load_time: 1.283ms, per_image_inference_time: 0.708ms
2022-06-28 22:49:55 - until epoch: 045, best_acc1: 73.678%
2022-06-28 22:49:55 - epoch 046 lr: 0.010000
2022-06-28 22:50:33 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 0.8744
2022-06-28 22:51:05 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 0.9851
2022-06-28 22:51:37 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.1065
2022-06-28 22:52:08 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.2506
2022-06-28 22:52:40 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 0.9317
2022-06-28 22:53:11 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.1897
2022-06-28 22:53:43 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.0372
2022-06-28 22:54:15 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.1027
2022-06-28 22:54:46 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.1638
2022-06-28 22:55:18 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 0.8884
2022-06-28 22:55:50 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.0920
2022-06-28 22:56:22 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.1522
2022-06-28 22:56:54 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.1238
2022-06-28 22:57:25 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.2019
2022-06-28 22:57:57 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.0805
2022-06-28 22:58:29 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.1191
2022-06-28 22:59:01 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 0.9847
2022-06-28 22:59:33 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.1225
2022-06-28 23:00:05 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 0.8828
2022-06-28 23:00:37 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.1188
2022-06-28 23:01:09 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.2250
2022-06-28 23:01:40 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 0.9408
2022-06-28 23:02:12 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.2569
2022-06-28 23:02:44 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.0985
2022-06-28 23:03:16 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.1372
2022-06-28 23:03:48 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.1906
2022-06-28 23:04:20 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 0.9889
2022-06-28 23:04:52 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 0.8799
2022-06-28 23:05:23 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.1390
2022-06-28 23:05:55 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 0.9720
2022-06-28 23:06:27 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 0.9695
2022-06-28 23:06:59 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.3650
2022-06-28 23:07:31 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.1088
2022-06-28 23:08:02 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.1485
2022-06-28 23:08:34 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.0235
2022-06-28 23:09:07 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.0959
2022-06-28 23:09:38 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 0.9242
2022-06-28 23:10:10 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.2018
2022-06-28 23:10:42 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.2153
2022-06-28 23:11:14 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.0042
2022-06-28 23:11:46 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.2325
2022-06-28 23:12:18 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.1398
2022-06-28 23:12:49 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.2262
2022-06-28 23:13:21 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.0312
2022-06-28 23:13:53 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.0672
2022-06-28 23:14:25 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.1798
2022-06-28 23:14:57 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 0.9893
2022-06-28 23:15:30 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.1255
2022-06-28 23:16:02 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.1637
2022-06-28 23:16:33 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.0612
2022-06-28 23:16:35 - train: epoch 046, train_loss: 1.0900
2022-06-28 23:17:48 - eval: epoch: 046, acc1: 72.820%, acc5: 91.352%, test_loss: 1.0843, per_image_load_time: 1.172ms, per_image_inference_time: 0.707ms
2022-06-28 23:17:49 - until epoch: 046, best_acc1: 73.678%
2022-06-28 23:17:49 - epoch 047 lr: 0.010000
2022-06-28 23:18:27 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.0087
2022-06-28 23:18:59 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.1867
2022-06-28 23:19:31 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 0.9670
2022-06-28 23:20:03 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.0007
2022-06-28 23:20:34 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.1668
2022-06-28 23:21:06 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.0375
2022-06-28 23:21:38 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.2782
2022-06-28 23:22:10 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 0.9508
2022-06-28 23:22:42 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.1601
2022-06-28 23:23:14 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.0777
2022-06-28 23:23:46 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.1143
2022-06-28 23:24:18 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 0.9830
2022-06-28 23:24:50 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.0624
2022-06-28 23:25:22 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.0734
2022-06-28 23:25:54 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 0.9940
2022-06-28 23:26:26 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 0.9661
2022-06-28 23:26:58 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 0.8969
2022-06-28 23:27:30 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.0582
2022-06-28 23:28:02 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.0326
2022-06-28 23:28:34 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.0287
2022-06-28 23:29:06 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.2856
2022-06-28 23:29:38 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.2035
2022-06-28 23:30:10 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.0851
2022-06-28 23:30:42 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 0.8944
2022-06-28 23:31:15 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.0912
2022-06-28 23:31:46 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.3856
2022-06-28 23:32:19 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.1088
2022-06-28 23:32:51 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.1359
2022-06-28 23:33:23 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.0497
2022-06-28 23:33:55 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.2084
2022-06-28 23:34:26 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.0041
2022-06-28 23:34:58 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.3497
2022-06-28 23:35:30 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 0.9574
2022-06-28 23:36:02 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.1558
2022-06-28 23:36:34 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.2240
2022-06-28 23:37:06 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.2146
2022-06-28 23:37:38 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.2652
2022-06-28 23:38:09 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.2359
2022-06-28 23:38:41 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.0595
2022-06-28 23:39:13 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.1465
2022-06-28 23:39:46 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.1535
2022-06-28 23:40:17 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.0868
2022-06-28 23:40:49 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.0143
2022-06-28 23:41:22 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.0446
2022-06-28 23:41:54 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.2763
2022-06-28 23:42:26 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.1219
2022-06-28 23:42:58 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.2007
2022-06-28 23:43:30 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.0191
2022-06-28 23:44:02 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.1414
2022-06-28 23:44:34 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.0424
2022-06-28 23:44:35 - train: epoch 047, train_loss: 1.0903
2022-06-28 23:45:49 - eval: epoch: 047, acc1: 72.964%, acc5: 91.364%, test_loss: 1.0812, per_image_load_time: 1.796ms, per_image_inference_time: 0.687ms
2022-06-28 23:45:49 - until epoch: 047, best_acc1: 73.678%
2022-06-28 23:45:49 - epoch 048 lr: 0.010000
2022-06-28 23:46:28 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.2332
2022-06-28 23:47:00 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.3372
2022-06-28 23:47:31 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.0041
2022-06-28 23:48:03 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.0020
2022-06-28 23:48:35 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.0278
2022-06-28 23:49:07 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.0920
2022-06-28 23:49:38 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.0579
2022-06-28 23:50:10 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.0547
2022-06-28 23:50:42 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.1830
2022-06-28 23:51:14 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.1030
2022-06-28 23:51:46 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.0635
2022-06-28 23:52:18 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.1781
2022-06-28 23:52:50 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 0.9696
2022-06-28 23:53:22 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 0.9902
2022-06-28 23:53:54 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.1155
2022-06-28 23:54:26 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.0736
2022-06-28 23:54:58 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.1723
2022-06-28 23:55:30 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.0662
2022-06-28 23:56:02 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.0907
2022-06-28 23:56:34 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.3434
2022-06-28 23:57:06 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.0917
2022-06-28 23:57:38 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.2113
2022-06-28 23:58:10 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.0315
2022-06-28 23:58:42 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.1798
2022-06-28 23:59:14 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.0455
2022-06-28 23:59:46 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.2414
2022-06-29 00:00:17 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.3690
2022-06-29 00:00:49 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.0392
2022-06-29 00:01:21 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.2164
2022-06-29 00:01:52 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.0278
2022-06-29 00:02:24 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 0.9598
2022-06-29 00:02:56 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 0.9782
2022-06-29 00:03:27 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.1383
2022-06-29 00:03:59 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 0.9908
2022-06-29 00:04:31 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.3783
2022-06-29 00:05:03 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.2140
2022-06-29 00:05:36 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.1146
2022-06-29 00:06:07 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.0165
2022-06-29 00:06:39 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.2769
2022-06-29 00:07:12 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 0.9220
2022-06-29 00:07:43 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.3032
2022-06-29 00:08:15 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.1529
2022-06-29 00:08:47 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.0882
2022-06-29 00:09:19 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 0.9621
2022-06-29 00:09:50 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.1360
2022-06-29 00:10:22 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 0.9538
2022-06-29 00:10:54 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.0913
2022-06-29 00:11:26 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.3201
2022-06-29 00:11:58 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.1386
2022-06-29 00:12:30 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.1199
2022-06-29 00:12:31 - train: epoch 048, train_loss: 1.0882
2022-06-29 00:13:45 - eval: epoch: 048, acc1: 71.808%, acc5: 90.848%, test_loss: 1.1310, per_image_load_time: 1.784ms, per_image_inference_time: 0.703ms
2022-06-29 00:13:45 - until epoch: 048, best_acc1: 73.678%
2022-06-29 00:13:45 - epoch 049 lr: 0.010000
2022-06-29 00:14:24 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.2329
2022-06-29 00:14:56 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.0796
2022-06-29 00:15:28 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.0708
2022-06-29 00:16:00 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.1253
2022-06-29 00:16:32 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.0094
2022-06-29 00:17:03 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.0139
2022-06-29 00:17:35 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.1905
2022-06-29 00:18:07 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.2775
2022-06-29 00:18:39 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 0.9962
2022-06-29 00:19:11 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.1295
2022-06-29 00:19:43 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 0.9748
2022-06-29 00:20:16 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 0.8146
2022-06-29 00:20:47 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.1969
2022-06-29 00:21:19 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.1623
2022-06-29 00:21:51 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.0181
2022-06-29 00:22:23 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.2588
2022-06-29 00:22:55 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.1364
2022-06-29 00:23:27 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 0.9653
2022-06-29 00:23:59 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.0538
2022-06-29 00:24:31 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 0.9028
2022-06-29 00:25:03 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.0308
2022-06-29 00:25:35 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.0615
2022-06-29 00:26:07 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.0496
2022-06-29 00:26:40 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.1711
2022-06-29 00:27:12 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.1024
2022-06-29 00:27:44 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.0634
2022-06-29 00:28:16 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.0492
2022-06-29 00:28:48 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.0667
2022-06-29 00:29:20 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.1960
2022-06-29 00:29:52 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.2800
2022-06-29 00:30:24 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.1944
2022-06-29 00:30:56 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.0811
2022-06-29 00:31:28 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.1656
2022-06-29 00:31:59 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.2409
2022-06-29 00:32:31 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.1346
2022-06-29 00:33:03 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.2801
2022-06-29 00:33:35 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.1425
2022-06-29 00:34:07 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.2914
2022-06-29 00:34:39 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.2964
2022-06-29 00:35:11 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.1421
2022-06-29 00:35:42 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 0.9852
2022-06-29 00:36:14 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.1579
2022-06-29 00:36:46 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.3877
2022-06-29 00:37:18 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.0877
2022-06-29 00:37:50 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.0508
2022-06-29 00:38:22 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.2177
2022-06-29 00:38:54 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.2371
2022-06-29 00:39:27 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 0.9272
2022-06-29 00:39:59 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 0.8959
2022-06-29 00:40:30 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.0631
2022-06-29 00:40:32 - train: epoch 049, train_loss: 1.0855
2022-06-29 00:41:46 - eval: epoch: 049, acc1: 72.898%, acc5: 91.284%, test_loss: 1.0862, per_image_load_time: 1.253ms, per_image_inference_time: 0.686ms
2022-06-29 00:41:46 - until epoch: 049, best_acc1: 73.678%
2022-06-29 00:41:46 - epoch 050 lr: 0.010000
2022-06-29 00:42:24 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.1598
2022-06-29 00:42:56 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.0173
2022-06-29 00:43:28 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.0653
2022-06-29 00:43:59 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 0.9327
2022-06-29 00:44:31 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.0816
2022-06-29 00:45:03 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.1349
2022-06-29 00:45:35 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 0.9382
2022-06-29 00:46:06 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 0.9047
2022-06-29 00:46:38 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 0.9800
2022-06-29 00:47:10 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.1770
2022-06-29 00:47:42 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.1421
2022-06-29 00:48:14 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 0.9985
2022-06-29 00:48:45 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.0051
2022-06-29 00:49:17 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.0817
2022-06-29 00:49:49 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.1085
2022-06-29 00:50:22 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 0.9730
2022-06-29 00:50:54 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.0865
2022-06-29 00:51:26 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 0.9673
2022-06-29 00:51:58 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.0289
2022-06-29 00:52:30 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.0830
2022-06-29 00:53:02 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 0.8891
2022-06-29 00:53:34 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.1850
2022-06-29 00:54:06 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.1065
2022-06-29 00:54:38 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.0648
2022-06-29 00:55:10 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.1785
2022-06-29 00:55:42 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 0.9702
2022-06-29 00:56:14 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.1408
2022-06-29 00:56:47 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.2679
2022-06-29 00:57:19 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.2043
2022-06-29 00:57:51 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.1080
2022-06-29 00:58:23 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 0.9974
2022-06-29 00:58:56 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.0917
2022-06-29 00:59:28 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 0.9960
2022-06-29 01:00:00 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 0.9624
2022-06-29 01:00:32 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.1279
2022-06-29 01:01:04 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.1338
2022-06-29 01:01:36 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.1063
2022-06-29 01:02:08 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 0.9369
2022-06-29 01:02:40 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 0.9933
2022-06-29 01:03:12 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.2099
2022-06-29 01:03:45 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.0149
2022-06-29 01:04:17 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.0190
2022-06-29 01:04:49 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.0412
2022-06-29 01:05:21 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.0520
2022-06-29 01:05:53 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.0221
2022-06-29 01:06:25 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.0931
2022-06-29 01:06:57 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.0863
2022-06-29 01:07:29 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 0.9771
2022-06-29 01:08:01 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 0.9685
2022-06-29 01:08:33 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.0768
2022-06-29 01:08:34 - train: epoch 050, train_loss: 1.0804
2022-06-29 01:09:48 - eval: epoch: 050, acc1: 72.564%, acc5: 91.284%, test_loss: 1.0901, per_image_load_time: 1.245ms, per_image_inference_time: 0.709ms
2022-06-29 01:09:49 - until epoch: 050, best_acc1: 73.678%
2022-06-29 01:09:49 - epoch 051 lr: 0.010000
2022-06-29 01:10:27 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.1940
2022-06-29 01:10:58 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.3254
2022-06-29 01:11:30 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.1153
2022-06-29 01:12:02 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.0101
2022-06-29 01:12:34 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.0852
2022-06-29 01:13:06 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.0950
2022-06-29 01:13:38 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 0.9606
2022-06-29 01:14:10 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.2382
2022-06-29 01:14:42 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 0.9106
2022-06-29 01:15:13 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.2765
2022-06-29 01:15:45 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.1519
2022-06-29 01:16:17 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 0.9532
2022-06-29 01:16:49 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 0.8031
2022-06-29 01:17:21 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 0.9648
2022-06-29 01:17:53 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.0498
2022-06-29 01:18:25 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.0460
2022-06-29 01:18:57 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 0.9971
2022-06-29 01:19:29 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.1030
2022-06-29 01:20:01 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 0.8718
2022-06-29 01:20:33 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.0484
2022-06-29 01:21:05 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.1199
2022-06-29 01:21:37 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.1399
2022-06-29 01:22:09 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.0550
2022-06-29 01:22:41 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.1166
2022-06-29 01:23:13 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 0.9872
2022-06-29 01:23:45 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.1230
2022-06-29 01:24:17 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.1378
2022-06-29 01:24:49 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 0.9666
2022-06-29 01:25:22 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.0596
2022-06-29 01:25:53 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 0.9593
2022-06-29 01:26:25 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 0.9225
2022-06-29 01:26:57 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.1001
2022-06-29 01:27:29 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.1360
2022-06-29 01:28:01 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.1406
2022-06-29 01:28:33 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.0416
2022-06-29 01:29:05 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.0266
2022-06-29 01:29:38 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.2030
2022-06-29 01:30:10 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.0657
2022-06-29 01:30:42 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.0693
2022-06-29 01:31:14 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 0.9788
2022-06-29 01:31:47 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.1784
2022-06-29 01:32:19 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.3616
