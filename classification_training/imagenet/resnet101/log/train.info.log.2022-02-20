2022-02-20 11:29:11 - network: resnet101
2022-02-20 11:29:11 - num_classes: 1000
2022-02-20 11:29:11 - input_image_size: 224
2022-02-20 11:29:11 - scale: 1.1428571428571428
2022-02-20 11:29:11 - trained_model_path: 
2022-02-20 11:29:11 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-20 11:29:11 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f31b00c5940>
2022-02-20 11:29:11 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f31b00c5c10>
2022-02-20 11:29:11 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f31b00c5c40>
2022-02-20 11:29:11 - seed: 0
2022-02-20 11:29:11 - batch_size: 256
2022-02-20 11:29:11 - num_workers: 16
2022-02-20 11:29:11 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-20 11:29:11 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-20 11:29:11 - epochs: 100
2022-02-20 11:29:11 - print_interval: 100
2022-02-20 11:29:11 - distributed: True
2022-02-20 11:29:11 - sync_bn: False
2022-02-20 11:29:11 - apex: True
2022-02-20 11:29:11 - gpus_type: NVIDIA GeForce RTX 3090
2022-02-20 11:29:11 - gpus_num: 2
2022-02-20 11:29:11 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f31909730b0>
2022-02-20 11:29:15 - --------------------parameters--------------------
2022-02-20 11:29:15 - name: conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.6.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.6.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.6.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.6.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.6.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.6.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.6.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.6.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.6.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.7.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.7.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.7.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.7.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.7.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.7.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.7.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.7.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.7.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.8.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.8.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.8.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.8.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.8.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.8.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.8.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.8.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.8.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.9.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.9.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.9.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.9.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.9.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.9.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.9.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.9.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.9.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.10.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.10.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.10.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.10.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.10.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.10.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.10.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.10.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.10.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.11.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.11.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.11.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.11.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.11.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.11.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.11.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.11.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.11.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.12.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.12.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.12.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.12.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.12.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.12.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.12.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.12.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.12.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.13.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.13.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.13.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.13.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.13.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.13.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.13.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.13.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.13.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.14.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.14.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.14.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.14.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.14.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.14.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.14.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.14.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.14.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.15.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.15.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.15.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.15.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.15.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.15.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.15.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.15.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.15.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.16.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.16.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.16.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.16.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.16.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.16.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.16.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.16.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.16.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.17.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.17.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.17.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.17.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.17.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.17.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.17.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.17.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.17.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.18.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.18.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.18.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.18.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.18.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.18.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.18.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.18.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.18.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.19.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.19.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.19.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.19.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.19.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.19.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.19.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.19.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.19.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.20.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.20.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.20.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.20.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.20.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.20.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.20.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.20.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.20.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.21.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.21.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.21.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.21.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.21.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.21.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.21.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.21.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.21.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.22.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.22.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.22.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.22.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.22.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.22.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer3.22.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer3.22.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer3.22.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-02-20 11:29:15 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-02-20 11:29:15 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-02-20 11:29:15 - name: fc.weight, grad: True
2022-02-20 11:29:15 - name: fc.bias, grad: True
2022-02-20 11:29:15 - --------------------buffers--------------------
2022-02-20 11:29:15 - name: conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.6.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.6.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.6.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.6.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.6.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.6.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.6.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.6.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.6.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.7.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.7.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.7.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.7.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.7.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.7.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.7.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.7.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.7.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.8.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.8.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.8.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.8.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.8.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.8.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.8.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.8.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.8.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.9.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.9.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.9.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.9.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.9.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.9.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.9.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.9.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.9.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.10.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.10.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.10.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.10.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.10.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.10.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.10.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.10.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.10.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.11.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.11.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.11.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.11.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.11.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.11.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.11.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.11.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.11.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.12.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.12.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.12.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.12.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.12.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.12.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.12.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.12.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.12.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.13.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.13.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.13.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.13.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.13.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.13.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.13.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.13.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.13.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.14.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.14.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.14.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.14.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.14.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.14.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.14.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.14.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.14.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.15.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.15.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.15.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.15.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.15.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.15.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.15.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.15.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.15.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.16.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.16.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.16.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.16.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.16.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.16.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.16.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.16.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.16.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.17.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.17.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.17.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.17.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.17.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.17.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.17.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.17.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.17.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.18.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.18.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.18.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.18.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.18.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.18.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.18.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.18.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.18.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.19.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.19.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.19.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.19.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.19.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.19.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.19.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.19.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.19.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.20.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.20.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.20.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.20.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.20.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.20.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.20.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.20.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.20.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.21.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.21.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.21.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.21.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.21.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.21.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.21.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.21.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.21.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.22.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.22.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.22.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.22.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.22.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.22.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer3.22.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer3.22.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer3.22.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:15 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-02-20 11:29:15 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-02-20 11:29:15 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:29:16 - epoch 001 lr: 0.1
2022-02-20 11:29:54 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8959
2022-02-20 11:30:25 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.9094
2022-02-20 11:30:56 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.9125
2022-02-20 11:31:28 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.8575
2022-02-20 11:32:00 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.8373
2022-02-20 11:32:32 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.7114
2022-02-20 11:33:03 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.7102
2022-02-20 11:33:35 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.6216
2022-02-20 11:34:07 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.5362
2022-02-20 11:34:39 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.4832
2022-02-20 11:35:11 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.4375
2022-02-20 11:35:43 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 6.2894
2022-02-20 11:36:14 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 6.3513
2022-02-20 11:36:46 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 6.2631
2022-02-20 11:37:18 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 6.1149
2022-02-20 11:37:50 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 6.1844
2022-02-20 11:38:22 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.9921
2022-02-20 11:38:54 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.9159
2022-02-20 11:39:26 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.8199
2022-02-20 11:39:58 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.7924
2022-02-20 11:40:30 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.8042
2022-02-20 11:41:01 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.6899
2022-02-20 11:41:33 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 5.5950
2022-02-20 11:42:05 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 5.5117
2022-02-20 11:42:37 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.5009
2022-02-20 11:43:09 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.5623
2022-02-20 11:43:41 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.4653
2022-02-20 11:44:13 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 5.3058
2022-02-20 11:44:44 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 5.1202
2022-02-20 11:45:16 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.2715
2022-02-20 11:45:48 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.3755
2022-02-20 11:46:20 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 5.2558
2022-02-20 11:46:52 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 5.0694
2022-02-20 11:47:24 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.9470
2022-02-20 11:47:55 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.9845
2022-02-20 11:48:27 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 5.0091
2022-02-20 11:48:59 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 5.0994
2022-02-20 11:49:31 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.8549
2022-02-20 11:50:03 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.8961
2022-02-20 11:50:35 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.7977
2022-02-20 11:51:07 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.9354
2022-02-20 11:51:39 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.7599
2022-02-20 11:52:12 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.6545
2022-02-20 11:52:44 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.5018
2022-02-20 11:53:16 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.7144
2022-02-20 11:53:49 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.8269
2022-02-20 11:54:22 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.4193
2022-02-20 11:54:55 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.6608
2022-02-20 11:55:29 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.4655
2022-02-20 11:56:02 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.3506
2022-02-20 11:56:04 - train: epoch 001, train_loss: 5.6162
2022-02-20 11:57:19 - eval: epoch: 001, acc1: 14.476%, acc5: 33.396%, test_loss: 4.6395, per_image_load_time: 2.261ms, per_image_inference_time: 0.662ms
2022-02-20 11:57:20 - until epoch: 001, best_acc1: 14.476%
2022-02-20 11:57:20 - epoch 002 lr: 0.1
2022-02-20 11:57:58 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.4760
2022-02-20 11:58:30 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.2464
2022-02-20 11:59:01 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.5620
2022-02-20 11:59:33 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.3484
2022-02-20 12:00:04 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.1870
2022-02-20 12:00:36 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.1345
2022-02-20 12:01:08 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.3752
2022-02-20 12:01:41 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 4.1763
2022-02-20 12:02:13 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.8101
2022-02-20 12:02:45 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.3810
2022-02-20 12:03:17 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.2698
2022-02-20 12:03:49 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.1191
2022-02-20 12:04:22 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.0925
2022-02-20 12:04:54 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.1849
2022-02-20 12:05:26 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.0570
2022-02-20 12:05:58 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 4.0316
2022-02-20 12:06:30 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.1500
2022-02-20 12:07:02 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.0285
2022-02-20 12:07:34 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.9630
2022-02-20 12:08:07 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.5980
2022-02-20 12:08:39 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.9632
2022-02-20 12:09:11 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.6934
2022-02-20 12:09:44 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.9711
2022-02-20 12:10:16 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.6968
2022-02-20 12:10:48 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.7427
2022-02-20 12:11:20 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.6941
2022-02-20 12:11:52 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.8690
2022-02-20 12:12:24 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.7913
2022-02-20 12:12:56 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.6961
2022-02-20 12:13:29 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.5530
2022-02-20 12:14:01 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.6511
2022-02-20 12:14:33 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.7209
2022-02-20 12:15:05 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.6035
2022-02-20 12:15:37 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.6792
2022-02-20 12:16:09 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.6038
2022-02-20 12:16:41 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.7045
2022-02-20 12:17:14 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.7775
2022-02-20 12:17:46 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.3926
2022-02-20 12:18:19 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.5877
2022-02-20 12:18:51 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.5243
2022-02-20 12:19:24 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.5513
2022-02-20 12:19:56 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.4575
2022-02-20 12:20:29 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.5400
2022-02-20 12:21:01 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.3132
2022-02-20 12:21:34 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.3234
2022-02-20 12:22:07 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.2468
2022-02-20 12:22:41 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.4624
2022-02-20 12:23:14 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.5147
2022-02-20 12:23:48 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.4227
2022-02-20 12:24:22 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.3175
2022-02-20 12:24:24 - train: epoch 002, train_loss: 3.8259
2022-02-20 12:25:38 - eval: epoch: 002, acc1: 30.956%, acc5: 56.798%, test_loss: 3.8890, per_image_load_time: 2.199ms, per_image_inference_time: 0.661ms
2022-02-20 12:25:40 - until epoch: 002, best_acc1: 30.956%
2022-02-20 12:25:40 - epoch 003 lr: 0.1
2022-02-20 12:26:17 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.3923
2022-02-20 12:26:49 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.3291
2022-02-20 12:27:20 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.2293
2022-02-20 12:27:52 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.3739
2022-02-20 12:28:24 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.4701
2022-02-20 12:28:55 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.1549
2022-02-20 12:29:28 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.5903
2022-02-20 12:30:00 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.4190
2022-02-20 12:30:32 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.1861
2022-02-20 12:31:04 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.3254
2022-02-20 12:31:36 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.1557
2022-02-20 12:32:08 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.1729
2022-02-20 12:32:40 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.0956
2022-02-20 12:33:12 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.1346
2022-02-20 12:33:44 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.3969
2022-02-20 12:34:16 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.1781
2022-02-20 12:34:49 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.1161
2022-02-20 12:35:21 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.0718
2022-02-20 12:35:53 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.2684
2022-02-20 12:36:25 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.3924
2022-02-20 12:36:57 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.3174
2022-02-20 12:37:30 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.6077
2022-02-20 12:38:02 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.1568
2022-02-20 12:38:34 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.0991
2022-02-20 12:39:06 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.1010
2022-02-20 12:39:39 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.1202
2022-02-20 12:40:11 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.4089
2022-02-20 12:40:43 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 2.9872
2022-02-20 12:41:15 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.1454
2022-02-20 12:41:47 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.1851
2022-02-20 12:42:20 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.1611
2022-02-20 12:42:52 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.1606
2022-02-20 12:43:24 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.1542
2022-02-20 12:43:56 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.2329
2022-02-20 12:44:29 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.8243
2022-02-20 12:45:01 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 2.9549
2022-02-20 12:45:33 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.0119
2022-02-20 12:46:06 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.0770
2022-02-20 12:46:38 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.1882
2022-02-20 12:47:10 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.0616
2022-02-20 12:47:42 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 2.9951
2022-02-20 12:48:14 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.9100
2022-02-20 12:48:47 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.6846
2022-02-20 12:49:19 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.8525
2022-02-20 12:49:52 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 2.9507
2022-02-20 12:50:25 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.0100
2022-02-20 12:50:58 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.8000
2022-02-20 12:51:31 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.0013
2022-02-20 12:52:05 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.0054
2022-02-20 12:52:38 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 2.9984
2022-02-20 12:52:40 - train: epoch 003, train_loss: 3.1141
2022-02-20 12:53:55 - eval: epoch: 003, acc1: 31.420%, acc5: 56.624%, test_loss: 3.3406, per_image_load_time: 2.238ms, per_image_inference_time: 0.689ms
2022-02-20 12:53:57 - until epoch: 003, best_acc1: 31.420%
2022-02-20 12:53:57 - epoch 004 lr: 0.1
2022-02-20 12:54:34 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 2.8864
2022-02-20 12:55:05 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.7973
2022-02-20 12:55:36 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.9262
2022-02-20 12:56:08 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.8504
2022-02-20 12:56:40 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.7296
2022-02-20 12:57:12 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.0366
2022-02-20 12:57:43 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 2.9593
2022-02-20 12:58:15 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.7459
2022-02-20 12:58:47 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.6010
2022-02-20 12:59:19 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.8268
2022-02-20 12:59:51 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 2.8990
2022-02-20 13:00:24 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.6697
2022-02-20 13:00:56 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.6659
2022-02-20 13:01:27 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 2.8653
2022-02-20 13:02:00 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 2.9679
2022-02-20 13:02:32 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.7194
2022-02-20 13:03:04 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.8542
2022-02-20 13:03:36 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 2.9607
2022-02-20 13:04:09 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 2.8749
2022-02-20 13:04:41 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.8032
2022-02-20 13:05:13 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.9082
2022-02-20 13:05:46 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.6557
2022-02-20 13:06:18 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.6048
2022-02-20 13:06:50 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.6783
2022-02-20 13:07:22 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.6992
2022-02-20 13:07:54 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.7753
2022-02-20 13:08:26 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.6240
2022-02-20 13:08:58 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.7603
2022-02-20 13:09:30 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.5614
2022-02-20 13:10:02 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.6631
2022-02-20 13:10:34 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.7535
2022-02-20 13:11:06 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.7474
2022-02-20 13:11:38 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.8482
2022-02-20 13:12:10 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.8786
2022-02-20 13:12:42 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.6271
2022-02-20 13:13:15 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.6008
2022-02-20 13:13:47 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.6640
2022-02-20 13:14:20 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.7051
2022-02-20 13:14:52 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.6805
2022-02-20 13:15:24 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.4159
2022-02-20 13:15:56 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.6172
2022-02-20 13:16:29 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.5684
2022-02-20 13:17:01 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.4836
2022-02-20 13:17:34 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.5933
2022-02-20 13:18:06 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.2341
2022-02-20 13:18:39 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.6553
2022-02-20 13:19:12 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.5529
2022-02-20 13:19:45 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.5485
2022-02-20 13:20:19 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.6951
2022-02-20 13:20:53 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.7799
2022-02-20 13:20:54 - train: epoch 004, train_loss: 2.7832
2022-02-20 13:22:09 - eval: epoch: 004, acc1: 43.578%, acc5: 70.194%, test_loss: 2.5123, per_image_load_time: 2.246ms, per_image_inference_time: 0.693ms
2022-02-20 13:22:11 - until epoch: 004, best_acc1: 43.578%
2022-02-20 13:22:11 - epoch 005 lr: 0.1
2022-02-20 13:22:48 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.6877
2022-02-20 13:23:19 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.8232
2022-02-20 13:23:51 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.6705
2022-02-20 13:24:22 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.6940
2022-02-20 13:24:54 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.4375
2022-02-20 13:25:26 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.6060
2022-02-20 13:25:58 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.7413
2022-02-20 13:26:30 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 2.9558
2022-02-20 13:27:02 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.6056
2022-02-20 13:27:34 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.6982
2022-02-20 13:28:06 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.7268
2022-02-20 13:28:38 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.7263
2022-02-20 13:29:10 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.5851
2022-02-20 13:29:42 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.7630
2022-02-20 13:30:14 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.3787
2022-02-20 13:30:46 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.3848
2022-02-20 13:31:18 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.5640
2022-02-20 13:31:50 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.6108
2022-02-20 13:32:22 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.3788
2022-02-20 13:32:55 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.7122
2022-02-20 13:33:27 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.3309
2022-02-20 13:33:59 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.5031
2022-02-20 13:34:32 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.4793
2022-02-20 13:35:04 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.4983
2022-02-20 13:35:36 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.7150
2022-02-20 13:36:09 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.7598
2022-02-20 13:36:41 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.6385
2022-02-20 13:37:13 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.5533
2022-02-20 13:37:45 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.4556
2022-02-20 13:38:17 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.4961
2022-02-20 13:38:50 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.6107
2022-02-20 13:39:22 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.7172
2022-02-20 13:39:54 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.3686
2022-02-20 13:40:26 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.4146
2022-02-20 13:40:59 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.6155
2022-02-20 13:41:31 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.6189
2022-02-20 13:42:03 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.5469
2022-02-20 13:42:36 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.3811
2022-02-20 13:43:08 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.8810
2022-02-20 13:43:40 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.4725
2022-02-20 13:44:12 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.5701
2022-02-20 13:44:45 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.6078
2022-02-20 13:45:17 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.4667
2022-02-20 13:45:50 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.5539
2022-02-20 13:46:23 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.5391
2022-02-20 13:46:56 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.4861
2022-02-20 13:47:29 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.3919
2022-02-20 13:48:02 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.4298
2022-02-20 13:48:36 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.6485
2022-02-20 13:49:10 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.4693
2022-02-20 13:49:11 - train: epoch 005, train_loss: 2.5943
2022-02-20 13:50:26 - eval: epoch: 005, acc1: 46.046%, acc5: 72.684%, test_loss: 2.3533, per_image_load_time: 2.217ms, per_image_inference_time: 0.673ms
2022-02-20 13:50:28 - until epoch: 005, best_acc1: 46.046%
2022-02-20 13:50:28 - epoch 006 lr: 0.1
2022-02-20 13:51:05 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.4006
2022-02-20 13:51:36 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.5583
2022-02-20 13:52:07 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.3502
2022-02-20 13:52:39 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.5617
2022-02-20 13:53:11 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.4640
2022-02-20 13:53:43 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.6021
2022-02-20 13:54:14 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.4804
2022-02-20 13:54:46 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.4872
2022-02-20 13:55:18 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.4915
2022-02-20 13:55:50 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.4223
2022-02-20 13:56:22 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.4649
2022-02-20 13:56:54 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.5262
2022-02-20 13:57:26 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.6685
2022-02-20 13:57:58 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.5839
2022-02-20 13:58:30 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.6096
2022-02-20 13:59:02 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.2784
2022-02-20 13:59:35 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.6983
2022-02-20 14:00:07 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.5838
2022-02-20 14:00:39 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.5450
2022-02-20 14:01:11 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.6396
2022-02-20 14:01:43 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.5431
2022-02-20 14:02:16 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.4232
2022-02-20 14:02:48 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.3117
2022-02-20 14:03:20 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.5229
2022-02-20 14:03:52 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.7168
2022-02-20 14:04:24 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.3567
2022-02-20 14:04:56 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.6408
2022-02-20 14:05:29 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.3922
2022-02-20 14:06:01 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.5549
2022-02-20 14:06:33 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.4470
2022-02-20 14:07:06 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.2508
2022-02-20 14:07:38 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.3825
2022-02-20 14:08:10 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.1967
2022-02-20 14:08:42 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.6093
2022-02-20 14:09:14 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.5166
2022-02-20 14:09:46 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.4981
2022-02-20 14:10:18 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.5012
2022-02-20 14:10:51 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.3224
2022-02-20 14:11:23 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.3546
2022-02-20 14:11:55 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.6048
2022-02-20 14:12:28 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.4703
2022-02-20 14:13:00 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.2608
2022-02-20 14:13:32 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.4931
2022-02-20 14:14:05 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.3992
2022-02-20 14:14:37 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.4668
2022-02-20 14:15:10 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.3757
2022-02-20 14:15:43 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.4379
2022-02-20 14:16:16 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.4499
2022-02-20 14:16:50 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.4561
2022-02-20 14:17:23 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.3073
2022-02-20 14:17:25 - train: epoch 006, train_loss: 2.4744
2022-02-20 14:18:39 - eval: epoch: 006, acc1: 47.182%, acc5: 73.410%, test_loss: 2.3104, per_image_load_time: 2.186ms, per_image_inference_time: 0.695ms
2022-02-20 14:18:41 - until epoch: 006, best_acc1: 47.182%
2022-02-20 14:18:41 - epoch 007 lr: 0.1
2022-02-20 14:19:18 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.3312
2022-02-20 14:19:49 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.4793
2022-02-20 14:20:21 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.6628
2022-02-20 14:20:53 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.5608
2022-02-20 14:21:24 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.3958
2022-02-20 14:21:56 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.5892
2022-02-20 14:22:28 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.3826
2022-02-20 14:23:00 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.4059
2022-02-20 14:23:32 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.3800
2022-02-20 14:24:04 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.4426
2022-02-20 14:24:36 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.2589
2022-02-20 14:25:08 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.4125
2022-02-20 14:25:40 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.4016
2022-02-20 14:26:12 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.3717
2022-02-20 14:26:44 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.4728
2022-02-20 14:27:16 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.3964
2022-02-20 14:27:48 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.5023
2022-02-20 14:28:20 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.3130
2022-02-20 14:28:52 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.3321
2022-02-20 14:29:24 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.2422
2022-02-20 14:29:56 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.5412
2022-02-20 14:30:28 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.2386
2022-02-20 14:31:00 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.4501
2022-02-20 14:31:32 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.4846
2022-02-20 14:32:04 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.3838
2022-02-20 14:32:36 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.3564
2022-02-20 14:33:08 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.3112
2022-02-20 14:33:41 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.3797
2022-02-20 14:34:13 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.2689
2022-02-20 14:34:45 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.4483
2022-02-20 14:35:17 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.2585
2022-02-20 14:35:49 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.3552
2022-02-20 14:36:21 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.6971
2022-02-20 14:36:53 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.2186
2022-02-20 14:37:25 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.4605
2022-02-20 14:37:57 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.1311
2022-02-20 14:38:29 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.2709
2022-02-20 14:39:01 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.5571
2022-02-20 14:39:33 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.2158
2022-02-20 14:40:05 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.5439
2022-02-20 14:40:37 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.2937
2022-02-20 14:41:09 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.1834
2022-02-20 14:41:41 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.4922
2022-02-20 14:42:14 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.1796
2022-02-20 14:42:46 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.5344
2022-02-20 14:43:19 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.4086
2022-02-20 14:43:52 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.4892
2022-02-20 14:44:26 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.6295
2022-02-20 14:44:59 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.3033
2022-02-20 14:45:33 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.3131
2022-02-20 14:45:35 - train: epoch 007, train_loss: 2.3885
2022-02-20 14:46:49 - eval: epoch: 007, acc1: 48.794%, acc5: 74.952%, test_loss: 2.2136, per_image_load_time: 2.178ms, per_image_inference_time: 0.725ms
2022-02-20 14:46:51 - until epoch: 007, best_acc1: 48.794%
2022-02-20 14:46:51 - epoch 008 lr: 0.1
2022-02-20 14:47:28 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.3161
2022-02-20 14:47:59 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.4412
2022-02-20 14:48:30 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.1378
2022-02-20 14:49:02 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.2156
2022-02-20 14:49:33 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.2034
2022-02-20 14:50:05 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.3408
2022-02-20 14:50:37 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.6726
2022-02-20 14:51:09 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.1355
2022-02-20 14:51:41 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.2626
2022-02-20 14:52:13 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.3482
2022-02-20 14:52:45 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.1783
2022-02-20 14:53:17 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.1968
2022-02-20 14:53:49 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.2880
2022-02-20 14:54:21 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.2895
2022-02-20 14:54:53 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.2864
2022-02-20 14:55:25 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.4600
2022-02-20 14:55:57 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.1588
2022-02-20 14:56:29 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.4311
2022-02-20 14:57:01 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.1221
2022-02-20 14:57:33 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.3657
2022-02-20 14:58:05 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.3184
2022-02-20 14:58:37 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.3240
2022-02-20 14:59:09 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.4262
2022-02-20 14:59:41 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.3159
2022-02-20 15:00:13 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.3328
2022-02-20 15:00:45 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.5038
2022-02-20 15:01:17 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.3761
2022-02-20 15:01:49 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.4538
2022-02-20 15:02:21 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.3282
2022-02-20 15:02:53 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.5100
2022-02-20 15:03:25 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.2029
2022-02-20 15:03:58 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.5342
2022-02-20 15:04:30 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.5316
2022-02-20 15:05:02 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.3631
2022-02-20 15:05:34 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.3063
2022-02-20 15:06:07 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.4464
2022-02-20 15:06:39 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.2568
2022-02-20 15:07:11 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.2302
2022-02-20 15:07:43 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.4186
2022-02-20 15:08:16 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.5506
2022-02-20 15:08:48 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.3730
2022-02-20 15:09:21 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.2112
2022-02-20 15:09:53 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.0777
2022-02-20 15:10:25 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.2352
2022-02-20 15:10:58 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.3944
2022-02-20 15:11:31 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.4199
2022-02-20 15:12:04 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.2592
2022-02-20 15:12:37 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.3786
2022-02-20 15:13:11 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.4370
2022-02-20 15:13:44 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.2787
2022-02-20 15:13:46 - train: epoch 008, train_loss: 2.3282
2022-02-20 15:15:01 - eval: epoch: 008, acc1: 50.178%, acc5: 76.150%, test_loss: 2.1538, per_image_load_time: 2.180ms, per_image_inference_time: 0.720ms
2022-02-20 15:15:02 - until epoch: 008, best_acc1: 50.178%
2022-02-20 15:15:02 - epoch 009 lr: 0.1
2022-02-20 15:15:39 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.0465
2022-02-20 15:16:11 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.1532
2022-02-20 15:16:43 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.0486
2022-02-20 15:17:14 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.4954
2022-02-20 15:17:47 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.3060
2022-02-20 15:18:19 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.2716
2022-02-20 15:18:51 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.2354
2022-02-20 15:19:23 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.1674
2022-02-20 15:19:55 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.2024
2022-02-20 15:20:27 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.1261
2022-02-20 15:20:59 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.5631
2022-02-20 15:21:31 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.3907
2022-02-20 15:22:03 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.3790
2022-02-20 15:22:35 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.0178
2022-02-20 15:23:07 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.0741
2022-02-20 15:23:40 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.3254
2022-02-20 15:24:12 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.5249
2022-02-20 15:24:44 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.1925
2022-02-20 15:25:16 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.0030
2022-02-20 15:25:48 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 1.8891
2022-02-20 15:26:20 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.3430
2022-02-20 15:26:53 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.3760
2022-02-20 15:27:25 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.0362
2022-02-20 15:27:57 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.2050
2022-02-20 15:28:29 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.1550
2022-02-20 15:29:01 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.3204
2022-02-20 15:29:34 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.2660
2022-02-20 15:30:06 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.3839
2022-02-20 15:30:38 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 1.9537
2022-02-20 15:31:10 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.1235
2022-02-20 15:31:42 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.3733
2022-02-20 15:32:15 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.3317
2022-02-20 15:32:47 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.2957
2022-02-20 15:33:19 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.4715
2022-02-20 15:33:51 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.4103
2022-02-20 15:34:24 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.2029
2022-02-20 15:34:56 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.4202
2022-02-20 15:35:28 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.4057
2022-02-20 15:36:00 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 1.9119
2022-02-20 15:36:33 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.4569
2022-02-20 15:37:05 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.2584
2022-02-20 15:37:38 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.2170
2022-02-20 15:38:10 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.4183
2022-02-20 15:38:43 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.2586
2022-02-20 15:39:15 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.1245
2022-02-20 15:39:48 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.4035
2022-02-20 15:40:21 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.5038
2022-02-20 15:40:55 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.4493
2022-02-20 15:41:28 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.3324
2022-02-20 15:42:02 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.2986
2022-02-20 15:42:04 - train: epoch 009, train_loss: 2.2836
2022-02-20 15:43:18 - eval: epoch: 009, acc1: 50.726%, acc5: 76.850%, test_loss: 2.1047, per_image_load_time: 2.068ms, per_image_inference_time: 0.718ms
2022-02-20 15:43:19 - until epoch: 009, best_acc1: 50.726%
2022-02-20 15:43:19 - epoch 010 lr: 0.1
2022-02-20 15:43:56 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.1972
2022-02-20 15:44:28 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.3754
2022-02-20 15:44:59 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.1428
2022-02-20 15:45:31 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.2934
2022-02-20 15:46:02 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.1699
2022-02-20 15:46:34 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.3093
2022-02-20 15:47:06 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.2874
2022-02-20 15:47:38 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.1709
2022-02-20 15:48:10 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.1641
2022-02-20 15:48:42 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.0877
2022-02-20 15:49:14 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.2843
2022-02-20 15:49:46 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.0266
2022-02-20 15:50:18 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.1556
2022-02-20 15:50:50 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.2285
2022-02-20 15:51:22 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 1.8951
2022-02-20 15:51:54 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.3302
2022-02-20 15:52:26 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.2847
2022-02-20 15:52:58 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.2104
2022-02-20 15:53:31 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.3239
2022-02-20 15:54:03 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.2667
2022-02-20 15:54:35 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.1399
2022-02-20 15:55:07 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.3922
2022-02-20 15:55:39 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.4777
2022-02-20 15:56:11 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.3285
2022-02-20 15:56:43 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.1583
2022-02-20 15:57:16 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.3125
2022-02-20 15:57:48 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 1.9845
2022-02-20 15:58:20 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.2460
2022-02-20 15:58:52 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.3366
2022-02-20 15:59:24 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.2522
2022-02-20 15:59:56 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.4242
2022-02-20 16:00:29 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.2402
2022-02-20 16:01:01 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.3394
2022-02-20 16:01:33 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.4372
2022-02-20 16:02:05 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.4583
2022-02-20 16:02:37 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.4112
2022-02-20 16:03:09 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.1354
2022-02-20 16:03:41 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.3068
2022-02-20 16:04:13 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 1.9099
2022-02-20 16:04:46 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.1578
2022-02-20 16:05:18 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.0908
2022-02-20 16:05:50 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.3029
2022-02-20 16:06:22 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.2136
2022-02-20 16:06:55 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.3065
2022-02-20 16:07:28 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.0448
2022-02-20 16:08:00 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.3196
2022-02-20 16:08:33 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.3391
2022-02-20 16:09:07 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.1458
2022-02-20 16:09:40 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.1495
2022-02-20 16:10:14 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.0715
2022-02-20 16:10:15 - train: epoch 010, train_loss: 2.2453
2022-02-20 16:11:30 - eval: epoch: 010, acc1: 49.296%, acc5: 75.346%, test_loss: 2.2000, per_image_load_time: 2.158ms, per_image_inference_time: 0.707ms
2022-02-20 16:11:31 - until epoch: 010, best_acc1: 50.726%
2022-02-20 16:11:31 - epoch 011 lr: 0.1
2022-02-20 16:12:08 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 1.9940
2022-02-20 16:12:39 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.3987
2022-02-20 16:13:10 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.0128
2022-02-20 16:13:42 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.3318
2022-02-20 16:14:14 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.1108
2022-02-20 16:14:45 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.2618
2022-02-20 16:15:17 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.2041
2022-02-20 16:15:49 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.2997
2022-02-20 16:16:21 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.3598
2022-02-20 16:16:52 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.1917
2022-02-20 16:17:24 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.2343
2022-02-20 16:17:57 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.5541
2022-02-20 16:18:29 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.4550
2022-02-20 16:19:01 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.3658
2022-02-20 16:19:33 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.1152
2022-02-20 16:20:06 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.3120
2022-02-20 16:20:38 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.2608
2022-02-20 16:21:10 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.1350
2022-02-20 16:21:42 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.1190
2022-02-20 16:22:14 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.3498
2022-02-20 16:22:46 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.2752
2022-02-20 16:23:19 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.0631
2022-02-20 16:23:50 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.4463
2022-02-20 16:24:23 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 1.9786
2022-02-20 16:24:54 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.4821
2022-02-20 16:25:27 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.2624
2022-02-20 16:25:59 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.2668
2022-02-20 16:26:31 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 1.8831
2022-02-20 16:27:03 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.2708
2022-02-20 16:27:35 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.4626
2022-02-20 16:28:06 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.2161
2022-02-20 16:28:39 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.0072
2022-02-20 16:29:11 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.1979
2022-02-20 16:29:43 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.1442
2022-02-20 16:30:14 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.1111
2022-02-20 16:30:47 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.1661
2022-02-20 16:31:18 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.3493
2022-02-20 16:31:50 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 1.9203
2022-02-20 16:32:23 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.2830
2022-02-20 16:32:55 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.1518
2022-02-20 16:33:27 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 1.9810
2022-02-20 16:34:00 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.0412
2022-02-20 16:34:32 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.3089
2022-02-20 16:35:05 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.2016
2022-02-20 16:35:38 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.0192
2022-02-20 16:36:11 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.0777
2022-02-20 16:36:44 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 1.9711
2022-02-20 16:37:17 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.0032
2022-02-20 16:37:51 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.0712
2022-02-20 16:38:25 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.1488
2022-02-20 16:38:26 - train: epoch 011, train_loss: 2.2154
2022-02-20 16:39:42 - eval: epoch: 011, acc1: 52.704%, acc5: 78.352%, test_loss: 2.0094, per_image_load_time: 2.213ms, per_image_inference_time: 0.703ms
2022-02-20 16:39:43 - until epoch: 011, best_acc1: 52.704%
2022-02-20 16:39:43 - epoch 012 lr: 0.1
2022-02-20 16:40:20 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.1304
2022-02-20 16:40:51 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.0453
2022-02-20 16:41:22 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.1728
2022-02-20 16:41:54 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.2546
2022-02-20 16:42:27 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.2800
2022-02-20 16:42:59 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 1.9147
2022-02-20 16:43:31 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 1.9633
2022-02-20 16:44:03 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.2557
2022-02-20 16:44:36 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.3486
2022-02-20 16:45:08 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 1.9997
2022-02-20 16:45:40 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.5228
2022-02-20 16:46:12 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 1.9727
2022-02-20 16:46:44 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.1408
2022-02-20 16:47:17 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.3961
2022-02-20 16:47:49 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.0033
2022-02-20 16:48:21 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.0898
2022-02-20 16:48:53 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.0584
2022-02-20 16:49:25 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.1867
2022-02-20 16:49:57 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.2279
2022-02-20 16:50:29 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.3667
2022-02-20 16:51:01 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.1767
2022-02-20 16:51:33 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.3450
2022-02-20 16:52:05 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.2316
2022-02-20 16:52:37 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.1835
2022-02-20 16:53:08 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 1.9377
2022-02-20 16:53:41 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 1.9458
2022-02-20 16:54:13 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.0738
2022-02-20 16:54:45 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.1838
2022-02-20 16:55:17 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 1.9986
2022-02-20 16:55:49 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.1702
2022-02-20 16:56:21 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.2890
2022-02-20 16:56:52 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.0396
2022-02-20 16:57:25 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.2781
2022-02-20 16:57:57 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.1052
2022-02-20 16:58:29 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.3161
2022-02-20 16:59:01 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.1769
2022-02-20 16:59:34 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.1204
2022-02-20 17:00:06 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.1944
2022-02-20 17:00:38 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.1075
2022-02-20 17:01:10 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.1621
2022-02-20 17:01:43 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.1528
2022-02-20 17:02:15 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.1351
2022-02-20 17:02:47 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.1613
2022-02-20 17:03:20 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.0183
2022-02-20 17:03:53 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.1122
2022-02-20 17:04:25 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.4976
2022-02-20 17:04:59 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.1134
2022-02-20 17:05:32 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.2637
2022-02-20 17:06:05 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.1630
2022-02-20 17:06:39 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 1.9334
2022-02-20 17:06:41 - train: epoch 012, train_loss: 2.1917
2022-02-20 17:07:56 - eval: epoch: 012, acc1: 50.788%, acc5: 76.544%, test_loss: 2.1228, per_image_load_time: 2.178ms, per_image_inference_time: 0.701ms
2022-02-20 17:07:57 - until epoch: 012, best_acc1: 52.704%
2022-02-20 17:07:57 - epoch 013 lr: 0.1
2022-02-20 17:08:34 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 1.9349
2022-02-20 17:09:06 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.1189
2022-02-20 17:09:37 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.1309
2022-02-20 17:10:09 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.1106
2022-02-20 17:10:41 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.1459
2022-02-20 17:11:13 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.3357
2022-02-20 17:11:45 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.0781
2022-02-20 17:12:17 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.3042
2022-02-20 17:12:49 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.0558
2022-02-20 17:13:21 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.2180
2022-02-20 17:13:54 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.1728
2022-02-20 17:14:26 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.2767
2022-02-20 17:14:58 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.3093
2022-02-20 17:15:30 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 1.9820
2022-02-20 17:16:02 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.2626
2022-02-20 17:16:34 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 1.9203
2022-02-20 17:17:06 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.1738
2022-02-20 17:17:38 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.2148
2022-02-20 17:18:10 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.2057
2022-02-20 17:18:43 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.3741
2022-02-20 17:19:15 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.4969
2022-02-20 17:19:47 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 1.9968
2022-02-20 17:20:19 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.1882
2022-02-20 17:20:51 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.2612
2022-02-20 17:21:24 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.1709
2022-02-20 17:21:56 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.1356
2022-02-20 17:22:28 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.1133
2022-02-20 17:23:00 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.2085
2022-02-20 17:23:33 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.3355
2022-02-20 17:24:04 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.0396
2022-02-20 17:24:37 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.0765
2022-02-20 17:25:09 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.1840
2022-02-20 17:25:41 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.0433
2022-02-20 17:26:13 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.1676
2022-02-20 17:26:46 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.0964
2022-02-20 17:27:18 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.4538
2022-02-20 17:27:50 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 1.8912
2022-02-20 17:28:22 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.2941
2022-02-20 17:28:54 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.2313
2022-02-20 17:29:26 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.2398
2022-02-20 17:29:58 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.0835
2022-02-20 17:30:31 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.0632
2022-02-20 17:31:03 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.1405
2022-02-20 17:31:35 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.0449
2022-02-20 17:32:08 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.1587
2022-02-20 17:32:41 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.2080
2022-02-20 17:33:14 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.3143
2022-02-20 17:33:47 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.2410
2022-02-20 17:34:20 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.2484
2022-02-20 17:34:54 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.3247
2022-02-20 17:34:56 - train: epoch 013, train_loss: 2.1698
2022-02-20 17:36:10 - eval: epoch: 013, acc1: 53.564%, acc5: 79.186%, test_loss: 1.9661, per_image_load_time: 2.166ms, per_image_inference_time: 0.729ms
2022-02-20 17:36:12 - until epoch: 013, best_acc1: 53.564%
2022-02-20 17:36:12 - epoch 014 lr: 0.1
2022-02-20 17:36:48 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.2001
2022-02-20 17:37:20 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.2871
2022-02-20 17:37:51 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 1.9020
2022-02-20 17:38:23 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.0746
2022-02-20 17:38:55 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.0567
2022-02-20 17:39:27 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.2193
2022-02-20 17:39:59 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.1444
2022-02-20 17:40:31 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.1074
2022-02-20 17:41:03 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.1814
2022-02-20 17:41:34 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.2748
2022-02-20 17:42:06 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.1640
2022-02-20 17:42:38 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.1678
2022-02-20 17:43:10 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.1866
2022-02-20 17:43:42 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.1923
2022-02-20 17:44:14 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.3021
2022-02-20 17:44:46 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.1479
2022-02-20 17:45:19 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.4091
2022-02-20 17:45:51 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.3234
2022-02-20 17:46:23 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.0746
2022-02-20 17:46:55 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.1419
2022-02-20 17:47:27 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.3034
2022-02-20 17:47:59 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.1758
2022-02-20 17:48:31 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.1661
2022-02-20 17:49:04 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.3440
2022-02-20 17:49:36 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.0637
2022-02-20 17:50:08 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.1183
2022-02-20 17:50:40 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.0663
2022-02-20 17:51:12 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.4721
2022-02-20 17:51:43 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.2194
2022-02-20 17:52:16 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.2670
2022-02-20 17:52:48 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 1.9894
2022-02-20 17:53:20 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.0742
2022-02-20 17:53:52 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.0006
2022-02-20 17:54:24 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.0886
2022-02-20 17:54:56 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.0730
2022-02-20 17:55:29 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.1251
2022-02-20 17:56:01 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.0287
2022-02-20 17:56:33 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.3108
2022-02-20 17:57:05 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.0412
2022-02-20 17:57:37 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.2761
2022-02-20 17:58:09 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 1.9785
2022-02-20 17:58:42 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.0593
2022-02-20 17:59:14 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.0077
2022-02-20 17:59:47 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 1.8872
2022-02-20 18:00:19 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.0997
2022-02-20 18:00:52 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.0758
2022-02-20 18:01:25 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.0532
2022-02-20 18:01:59 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 1.9394
2022-02-20 18:02:32 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 1.9441
2022-02-20 18:03:06 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.1436
2022-02-20 18:03:08 - train: epoch 014, train_loss: 2.1522
2022-02-20 18:04:22 - eval: epoch: 014, acc1: 52.030%, acc5: 77.684%, test_loss: 2.0749, per_image_load_time: 2.056ms, per_image_inference_time: 0.709ms
2022-02-20 18:04:24 - until epoch: 014, best_acc1: 53.564%
2022-02-20 18:04:24 - epoch 015 lr: 0.1
2022-02-20 18:05:01 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 1.8457
2022-02-20 18:05:32 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.3158
2022-02-20 18:06:04 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.3328
2022-02-20 18:06:35 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.0484
2022-02-20 18:07:07 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.1505
2022-02-20 18:07:39 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.2188
2022-02-20 18:08:11 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.0834
2022-02-20 18:08:43 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 1.9165
2022-02-20 18:09:15 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.0424
2022-02-20 18:09:47 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.1984
2022-02-20 18:10:19 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 1.9582
2022-02-20 18:10:50 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.1511
2022-02-20 18:11:22 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.4938
2022-02-20 18:11:54 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 1.9380
2022-02-20 18:12:26 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 1.8593
2022-02-20 18:12:58 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.1440
2022-02-20 18:13:30 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.3419
2022-02-20 18:14:03 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.0445
2022-02-20 18:14:35 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 1.9814
2022-02-20 18:15:07 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.1136
2022-02-20 18:15:39 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 1.8883
2022-02-20 18:16:11 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.4196
2022-02-20 18:16:43 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 1.8876
2022-02-20 18:17:15 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.2023
2022-02-20 18:17:47 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.1411
2022-02-20 18:18:19 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.0105
2022-02-20 18:18:51 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.1996
2022-02-20 18:19:23 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.2847
2022-02-20 18:19:55 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.0521
2022-02-20 18:20:27 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 1.9069
2022-02-20 18:20:59 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.0076
2022-02-20 18:21:31 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.1232
2022-02-20 18:22:04 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 1.9834
2022-02-20 18:22:36 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.2753
2022-02-20 18:23:08 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.2834
2022-02-20 18:23:40 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.1709
2022-02-20 18:24:12 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.0456
2022-02-20 18:24:45 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.0686
2022-02-20 18:25:17 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.2308
2022-02-20 18:25:49 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.1638
2022-02-20 18:26:21 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.3484
2022-02-20 18:26:53 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 1.8952
2022-02-20 18:27:26 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.2075
2022-02-20 18:27:58 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.1129
2022-02-20 18:28:31 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.0987
2022-02-20 18:29:03 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.0998
2022-02-20 18:29:36 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.1785
2022-02-20 18:30:10 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.0863
2022-02-20 18:30:43 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.0924
2022-02-20 18:31:16 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.1401
2022-02-20 18:31:18 - train: epoch 015, train_loss: 2.1380
2022-02-20 18:32:32 - eval: epoch: 015, acc1: 51.350%, acc5: 76.774%, test_loss: 2.1098, per_image_load_time: 1.822ms, per_image_inference_time: 0.702ms
2022-02-20 18:32:34 - until epoch: 015, best_acc1: 53.564%
2022-02-20 18:32:34 - epoch 016 lr: 0.1
2022-02-20 18:33:11 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.1370
2022-02-20 18:33:42 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 1.9676
2022-02-20 18:34:14 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.1597
2022-02-20 18:34:46 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.3002
2022-02-20 18:35:18 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 1.9237
2022-02-20 18:35:50 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.4128
2022-02-20 18:36:22 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 1.9183
2022-02-20 18:36:54 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.0997
2022-02-20 18:37:27 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.2043
2022-02-20 18:37:59 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.0395
2022-02-20 18:38:31 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 1.9648
2022-02-20 18:39:03 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 1.9722
2022-02-20 18:39:35 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.1818
2022-02-20 18:40:07 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.0672
2022-02-20 18:40:39 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.1809
2022-02-20 18:41:12 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.2554
2022-02-20 18:41:44 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.0084
2022-02-20 18:42:16 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.0298
2022-02-20 18:42:48 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.0815
2022-02-20 18:43:21 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 1.8347
2022-02-20 18:43:53 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.1253
2022-02-20 18:44:25 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.1930
2022-02-20 18:44:57 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.4399
2022-02-20 18:45:30 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.2262
2022-02-20 18:46:02 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.0638
2022-02-20 18:46:34 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.2378
2022-02-20 18:47:06 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.1623
2022-02-20 18:47:38 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 1.9517
2022-02-20 18:48:10 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.1661
2022-02-20 18:48:42 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.3373
2022-02-20 18:49:15 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.2562
2022-02-20 18:49:47 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.2200
2022-02-20 18:50:19 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.2816
2022-02-20 18:50:52 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.0346
2022-02-20 18:51:24 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.0846
2022-02-20 18:51:56 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.0463
2022-02-20 18:52:29 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.2767
2022-02-20 18:53:01 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.4306
2022-02-20 18:53:33 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.1543
2022-02-20 18:54:05 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.2288
2022-02-20 18:54:37 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.1775
2022-02-20 18:55:09 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.0555
2022-02-20 18:55:42 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.0032
2022-02-20 18:56:14 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.0416
2022-02-20 18:56:47 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.1413
2022-02-20 18:57:20 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.1055
2022-02-20 18:57:53 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.3931
2022-02-20 18:58:26 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.0577
2022-02-20 18:59:00 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.1301
2022-02-20 18:59:33 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.2525
2022-02-20 18:59:35 - train: epoch 016, train_loss: 2.1257
2022-02-20 19:00:49 - eval: epoch: 016, acc1: 53.344%, acc5: 79.514%, test_loss: 1.9632, per_image_load_time: 2.147ms, per_image_inference_time: 0.719ms
2022-02-20 19:00:51 - until epoch: 016, best_acc1: 53.564%
2022-02-20 19:00:51 - epoch 017 lr: 0.1
2022-02-20 19:01:28 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.0139
2022-02-20 19:01:59 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.2589
2022-02-20 19:02:31 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.3198
2022-02-20 19:03:03 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 1.9033
2022-02-20 19:03:34 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.1327
2022-02-20 19:04:07 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.3711
2022-02-20 19:04:39 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.0769
2022-02-20 19:05:10 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.0378
2022-02-20 19:05:42 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.0716
2022-02-20 19:06:15 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.0717
2022-02-20 19:06:47 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.4017
2022-02-20 19:07:19 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.3831
2022-02-20 19:07:51 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.0915
2022-02-20 19:08:24 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.2036
2022-02-20 19:08:56 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 1.7894
2022-02-20 19:09:28 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 1.9600
2022-02-20 19:10:01 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.1799
2022-02-20 19:10:33 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.1510
2022-02-20 19:11:05 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.0157
2022-02-20 19:11:38 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.1942
2022-02-20 19:12:10 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.1989
2022-02-20 19:12:42 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 1.9897
2022-02-20 19:13:15 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.0547
2022-02-20 19:13:47 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.0855
2022-02-20 19:14:20 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.2204
2022-02-20 19:14:52 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.0012
2022-02-20 19:15:24 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.0010
2022-02-20 19:15:56 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.2858
2022-02-20 19:16:29 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.3102
2022-02-20 19:17:01 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 1.9671
2022-02-20 19:17:33 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.2677
2022-02-20 19:18:05 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.0233
2022-02-20 19:18:38 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.2300
2022-02-20 19:19:10 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.0593
2022-02-20 19:19:42 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.0838
2022-02-20 19:20:15 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.3267
2022-02-20 19:20:47 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.0551
2022-02-20 19:21:20 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.2648
2022-02-20 19:21:52 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 1.9526
2022-02-20 19:22:24 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 1.9637
2022-02-20 19:22:57 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.1561
2022-02-20 19:23:29 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.1284
2022-02-20 19:24:02 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.2689
2022-02-20 19:24:34 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.0906
2022-02-20 19:25:07 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.2001
2022-02-20 19:25:40 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.1095
2022-02-20 19:26:13 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.2477
2022-02-20 19:26:46 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.1787
2022-02-20 19:27:20 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 1.9190
2022-02-20 19:27:53 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 1.9316
2022-02-20 19:27:55 - train: epoch 017, train_loss: 2.1146
2022-02-20 19:29:10 - eval: epoch: 017, acc1: 53.916%, acc5: 79.444%, test_loss: 1.9535, per_image_load_time: 2.163ms, per_image_inference_time: 0.725ms
2022-02-20 19:29:11 - until epoch: 017, best_acc1: 53.916%
2022-02-20 19:29:11 - epoch 018 lr: 0.1
2022-02-20 19:29:49 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.0446
2022-02-20 19:30:21 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.0202
2022-02-20 19:30:52 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.1997
2022-02-20 19:31:24 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.2543
2022-02-20 19:31:56 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 1.9622
2022-02-20 19:32:28 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.2051
2022-02-20 19:33:00 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 1.7839
2022-02-20 19:33:32 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.0339
2022-02-20 19:34:04 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.1928
2022-02-20 19:34:36 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 1.9632
2022-02-20 19:35:08 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.3320
2022-02-20 19:35:40 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.1041
2022-02-20 19:36:12 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.5101
2022-02-20 19:36:45 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.0695
2022-02-20 19:37:17 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.2946
2022-02-20 19:37:49 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.0007
2022-02-20 19:38:21 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.0620
2022-02-20 19:38:53 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 1.8711
2022-02-20 19:39:25 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.1082
2022-02-20 19:39:57 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.3787
2022-02-20 19:40:29 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.2609
2022-02-20 19:41:01 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.0628
2022-02-20 19:41:33 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.1371
2022-02-20 19:42:05 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.0614
2022-02-20 19:42:38 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 1.8223
2022-02-20 19:43:10 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 1.9810
2022-02-20 19:43:42 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.2597
2022-02-20 19:44:14 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 1.8304
2022-02-20 19:44:46 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.2260
2022-02-20 19:45:18 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.1095
2022-02-20 19:45:50 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.3629
2022-02-20 19:46:22 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.0169
2022-02-20 19:46:54 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 1.9623
2022-02-20 19:47:26 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.0417
2022-02-20 19:47:58 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.3794
2022-02-20 19:48:30 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.0816
2022-02-20 19:49:02 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.4891
2022-02-20 19:49:35 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.2136
2022-02-20 19:50:07 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.1053
2022-02-20 19:50:39 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 1.9219
2022-02-20 19:51:11 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.1093
2022-02-20 19:51:44 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.0631
2022-02-20 19:52:16 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 1.8667
2022-02-20 19:52:49 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.3363
2022-02-20 19:53:21 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.0853
2022-02-20 19:53:55 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 1.9496
2022-02-20 19:54:28 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.3172
2022-02-20 19:55:01 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.2737
2022-02-20 19:55:35 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.0234
2022-02-20 19:56:08 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.2490
2022-02-20 19:56:10 - train: epoch 018, train_loss: 2.1014
2022-02-20 19:57:24 - eval: epoch: 018, acc1: 54.270%, acc5: 79.596%, test_loss: 1.9369, per_image_load_time: 2.034ms, per_image_inference_time: 0.735ms
2022-02-20 19:57:26 - until epoch: 018, best_acc1: 54.270%
2022-02-20 19:57:26 - epoch 019 lr: 0.1
2022-02-20 19:58:02 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 1.9308
2022-02-20 19:58:34 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.2585
2022-02-20 19:59:05 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.2929
2022-02-20 19:59:37 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.0264
2022-02-20 20:00:08 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.1176
2022-02-20 20:00:41 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.1411
2022-02-20 20:01:13 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 1.8674
2022-02-20 20:01:44 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.2386
2022-02-20 20:02:16 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.0718
2022-02-20 20:02:48 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.2254
2022-02-20 20:03:21 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.0346
2022-02-20 20:03:53 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.1058
2022-02-20 20:04:25 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.2620
2022-02-20 20:04:57 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 1.9871
2022-02-20 20:05:29 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.5104
2022-02-20 20:06:00 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.0994
2022-02-20 20:06:33 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.2073
2022-02-20 20:07:05 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.0018
2022-02-20 20:07:37 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.2775
2022-02-20 20:08:09 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.1002
2022-02-20 20:08:41 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 1.9821
2022-02-20 20:09:13 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.0954
2022-02-20 20:09:45 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.0814
2022-02-20 20:10:18 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.1850
2022-02-20 20:10:50 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.0855
2022-02-20 20:11:22 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.0774
2022-02-20 20:11:54 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.1692
2022-02-20 20:12:26 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.2084
2022-02-20 20:12:58 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.1352
2022-02-20 20:13:30 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.3940
2022-02-20 20:14:02 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.1735
2022-02-20 20:14:34 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.8695
2022-02-20 20:15:06 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.0300
2022-02-20 20:15:37 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.0261
2022-02-20 20:16:09 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.0658
2022-02-20 20:16:41 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 1.8423
2022-02-20 20:17:13 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.1607
2022-02-20 20:17:45 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.2272
2022-02-20 20:18:18 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.0880
2022-02-20 20:18:50 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.0291
2022-02-20 20:19:22 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.0311
2022-02-20 20:19:54 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.0560
2022-02-20 20:20:27 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 1.9766
2022-02-20 20:20:59 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.1841
2022-02-20 20:21:32 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.2997
2022-02-20 20:22:05 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.1314
2022-02-20 20:22:38 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 1.9342
2022-02-20 20:23:11 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.0469
2022-02-20 20:23:45 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.0890
2022-02-20 20:24:18 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.2004
2022-02-20 20:24:20 - train: epoch 019, train_loss: 2.0978
2022-02-20 20:25:35 - eval: epoch: 019, acc1: 53.880%, acc5: 79.068%, test_loss: 1.9587, per_image_load_time: 2.111ms, per_image_inference_time: 0.754ms
2022-02-20 20:25:36 - until epoch: 019, best_acc1: 54.270%
2022-02-20 20:25:36 - epoch 020 lr: 0.1
2022-02-20 20:26:13 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.1661
2022-02-20 20:26:44 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 1.8351
2022-02-20 20:27:16 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.1450
2022-02-20 20:27:48 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 1.9030
2022-02-20 20:28:20 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.0439
2022-02-20 20:28:52 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.2386
2022-02-20 20:29:24 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 1.8142
2022-02-20 20:29:56 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.1704
2022-02-20 20:30:28 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.3528
2022-02-20 20:31:00 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.1319
2022-02-20 20:31:33 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 1.9809
2022-02-20 20:32:05 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 1.7452
2022-02-20 20:32:37 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 1.9883
2022-02-20 20:33:09 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.3537
2022-02-20 20:33:42 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.2813
2022-02-20 20:34:14 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 1.9671
2022-02-20 20:34:46 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 1.8226
2022-02-20 20:35:19 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.0883
2022-02-20 20:35:51 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 1.9507
2022-02-20 20:36:23 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.0180
2022-02-20 20:36:55 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.2340
2022-02-20 20:37:27 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 1.8914
2022-02-20 20:38:00 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 1.8959
2022-02-20 20:38:32 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.3215
2022-02-20 20:39:04 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.0045
2022-02-20 20:39:36 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 1.9685
2022-02-20 20:40:09 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.1435
2022-02-20 20:40:41 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.1170
2022-02-20 20:41:13 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.2129
2022-02-20 20:41:45 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.0769
2022-02-20 20:42:17 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.1663
2022-02-20 20:42:49 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.2454
2022-02-20 20:43:21 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 1.8804
2022-02-20 20:43:54 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.1849
2022-02-20 20:44:26 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 1.9975
2022-02-20 20:44:58 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.0629
2022-02-20 20:45:30 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.0023
2022-02-20 20:46:02 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.1464
2022-02-20 20:46:35 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.0869
2022-02-20 20:47:07 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.0346
2022-02-20 20:47:39 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 1.9332
2022-02-20 20:48:11 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.0303
2022-02-20 20:48:44 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.0847
2022-02-20 20:49:16 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.1010
2022-02-20 20:49:49 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.0836
2022-02-20 20:50:22 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.2811
2022-02-20 20:50:55 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 1.9745
2022-02-20 20:51:28 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.1162
2022-02-20 20:52:02 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.2071
2022-02-20 20:52:35 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 1.9421
2022-02-20 20:52:37 - train: epoch 020, train_loss: 2.0854
2022-02-20 20:53:51 - eval: epoch: 020, acc1: 55.170%, acc5: 80.390%, test_loss: 1.8918, per_image_load_time: 2.124ms, per_image_inference_time: 0.729ms
2022-02-20 20:53:53 - until epoch: 020, best_acc1: 55.170%
2022-02-20 20:53:53 - epoch 021 lr: 0.1
2022-02-20 20:54:30 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 1.9872
2022-02-20 20:55:01 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.0625
2022-02-20 20:55:33 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.8309
2022-02-20 20:56:04 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.2118
2022-02-20 20:56:36 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.1060
2022-02-20 20:57:08 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 1.9790
2022-02-20 20:57:41 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 1.8546
2022-02-20 20:58:13 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.2051
2022-02-20 20:58:45 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.1359
2022-02-20 20:59:17 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.0395
2022-02-20 20:59:50 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 1.9892
2022-02-20 21:00:22 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 1.9009
2022-02-20 21:00:54 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 1.9760
2022-02-20 21:01:26 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.0011
2022-02-20 21:01:58 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 1.9077
2022-02-20 21:02:31 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.0887
2022-02-20 21:03:03 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.1810
2022-02-20 21:03:35 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 1.9053
2022-02-20 21:04:08 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.1924
2022-02-20 21:04:40 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.2682
2022-02-20 21:05:12 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 1.9371
2022-02-20 21:05:45 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.0462
2022-02-20 21:06:17 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.0162
2022-02-20 21:06:49 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 1.9004
2022-02-20 21:07:22 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.0276
2022-02-20 21:07:54 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.3370
2022-02-20 21:08:27 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 1.9872
2022-02-20 21:08:59 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.0293
2022-02-20 21:09:31 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 1.9661
2022-02-20 21:10:03 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.2567
2022-02-20 21:10:35 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.1519
2022-02-20 21:11:07 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.0207
2022-02-20 21:11:39 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.3148
2022-02-20 21:12:12 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.2844
2022-02-20 21:12:44 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 1.9724
2022-02-20 21:13:16 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 1.9417
2022-02-20 21:13:49 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.0633
2022-02-20 21:14:21 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.0851
2022-02-20 21:14:54 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 1.9657
2022-02-20 21:15:26 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.3629
2022-02-20 21:15:58 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 1.9552
2022-02-20 21:16:30 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.0698
2022-02-20 21:17:03 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 1.9966
2022-02-20 21:17:36 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.0810
2022-02-20 21:18:08 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.2642
2022-02-20 21:18:41 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.0517
2022-02-20 21:19:14 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.2780
2022-02-20 21:19:48 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.2725
2022-02-20 21:20:22 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 1.8870
2022-02-20 21:20:55 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 1.9444
2022-02-20 21:20:57 - train: epoch 021, train_loss: 2.0808
2022-02-20 21:22:12 - eval: epoch: 021, acc1: 54.604%, acc5: 80.036%, test_loss: 1.9134, per_image_load_time: 2.210ms, per_image_inference_time: 0.688ms
2022-02-20 21:22:13 - until epoch: 021, best_acc1: 55.170%
2022-02-20 21:22:13 - epoch 022 lr: 0.1
2022-02-20 21:22:50 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 1.8206
2022-02-20 21:23:22 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 1.9536
2022-02-20 21:23:54 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 1.8097
2022-02-20 21:24:26 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 1.9188
2022-02-20 21:24:57 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.0914
2022-02-20 21:25:30 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.2284
2022-02-20 21:26:02 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.1846
2022-02-20 21:26:34 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.1265
2022-02-20 21:27:06 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.3099
2022-02-20 21:27:38 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.1170
2022-02-20 21:28:10 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.0861
2022-02-20 21:28:43 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.7661
2022-02-20 21:29:15 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.1433
2022-02-20 21:29:47 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.0287
2022-02-20 21:30:19 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.0890
2022-02-20 21:30:51 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 1.8740
2022-02-20 21:31:24 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 1.8564
2022-02-20 21:31:56 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.1864
2022-02-20 21:32:28 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 1.9273
2022-02-20 21:33:00 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.1534
2022-02-20 21:33:32 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.1243
2022-02-20 21:34:04 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 1.8922
2022-02-20 21:34:36 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.1180
2022-02-20 21:35:08 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.1465
2022-02-20 21:35:40 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 1.9405
2022-02-20 21:36:12 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 1.8260
2022-02-20 21:36:44 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 1.9161
2022-02-20 21:37:17 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.4637
2022-02-20 21:37:49 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 1.9414
2022-02-20 21:38:21 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.0532
2022-02-20 21:38:53 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.3042
2022-02-20 21:39:25 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.2377
2022-02-20 21:39:57 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.0694
2022-02-20 21:40:30 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 1.9506
2022-02-20 21:41:02 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.1994
2022-02-20 21:41:34 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.0708
2022-02-20 21:42:07 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.1453
2022-02-20 21:42:39 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.1532
2022-02-20 21:43:11 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.0341
2022-02-20 21:43:44 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.1802
2022-02-20 21:44:16 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 1.9979
2022-02-20 21:44:49 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.1151
2022-02-20 21:45:22 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.2978
2022-02-20 21:45:54 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 1.9935
2022-02-20 21:46:27 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.0768
2022-02-20 21:46:59 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.3205
2022-02-20 21:47:32 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.2258
2022-02-20 21:48:06 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 1.9389
2022-02-20 21:48:39 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 1.9074
2022-02-20 21:49:13 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.0348
2022-02-20 21:49:15 - train: epoch 022, train_loss: 2.0747
2022-02-20 21:50:30 - eval: epoch: 022, acc1: 54.612%, acc5: 79.994%, test_loss: 1.9188, per_image_load_time: 2.184ms, per_image_inference_time: 0.725ms
2022-02-20 21:50:30 - until epoch: 022, best_acc1: 55.170%
2022-02-20 21:50:30 - epoch 023 lr: 0.1
2022-02-20 21:51:09 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 1.9149
2022-02-20 21:51:40 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 1.7687
2022-02-20 21:52:12 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 1.9071
2022-02-20 21:52:44 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.1454
2022-02-20 21:53:16 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.1004
2022-02-20 21:53:48 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 1.9843
2022-02-20 21:54:20 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 1.7478
2022-02-20 21:54:52 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 1.9580
2022-02-20 21:55:24 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.0845
2022-02-20 21:55:56 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 1.9659
2022-02-20 21:56:29 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.2123
2022-02-20 21:57:01 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 1.9287
2022-02-20 21:57:33 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.0480
2022-02-20 21:58:06 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 1.9756
2022-02-20 21:58:38 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 1.9831
2022-02-20 21:59:10 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.0758
2022-02-20 21:59:42 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.2188
2022-02-20 22:00:14 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 1.9678
2022-02-20 22:00:46 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.1161
2022-02-20 22:01:18 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.7055
2022-02-20 22:01:50 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.1735
2022-02-20 22:02:22 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.7783
2022-02-20 22:02:55 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 1.9012
2022-02-20 22:03:27 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.0542
2022-02-20 22:03:59 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.0641
2022-02-20 22:04:31 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.0979
2022-02-20 22:05:03 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.1181
2022-02-20 22:05:35 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.0414
2022-02-20 22:06:07 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.1014
2022-02-20 22:06:39 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.2253
2022-02-20 22:07:12 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.2067
2022-02-20 22:07:44 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.1546
2022-02-20 22:08:16 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.0696
2022-02-20 22:08:48 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.2429
2022-02-20 22:09:20 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 1.9384
2022-02-20 22:09:52 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 1.9126
2022-02-20 22:10:24 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.0403
2022-02-20 22:10:56 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.0669
2022-02-20 22:11:28 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.1434
2022-02-20 22:12:00 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.0341
2022-02-20 22:12:33 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 1.9624
2022-02-20 22:13:05 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 1.9373
2022-02-20 22:13:37 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 1.9755
2022-02-20 22:14:10 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 1.9079
2022-02-20 22:14:42 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 1.9514
2022-02-20 22:15:15 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.1190
2022-02-20 22:15:48 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 1.8777
2022-02-20 22:16:21 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 1.9840
2022-02-20 22:16:55 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 1.9683
2022-02-20 22:17:28 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.1077
2022-02-20 22:17:31 - train: epoch 023, train_loss: 2.0676
2022-02-20 22:18:45 - eval: epoch: 023, acc1: 55.960%, acc5: 80.876%, test_loss: 1.8593, per_image_load_time: 2.140ms, per_image_inference_time: 0.735ms
2022-02-20 22:18:47 - until epoch: 023, best_acc1: 55.960%
2022-02-20 22:18:47 - epoch 024 lr: 0.1
2022-02-20 22:19:24 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 1.9869
2022-02-20 22:19:55 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.0832
2022-02-20 22:20:27 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.1193
2022-02-20 22:20:59 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.0895
2022-02-20 22:21:31 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 1.9547
2022-02-20 22:22:03 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 1.9203
2022-02-20 22:22:35 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.0121
2022-02-20 22:23:07 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 1.9234
2022-02-20 22:23:39 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.0263
2022-02-20 22:24:11 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.0087
2022-02-20 22:24:43 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.8025
2022-02-20 22:25:15 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 1.8893
2022-02-20 22:25:47 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.3304
2022-02-20 22:26:19 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 1.9487
2022-02-20 22:26:51 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.3259
2022-02-20 22:27:23 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.0617
2022-02-20 22:27:56 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.1620
2022-02-20 22:28:27 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.3475
2022-02-20 22:28:59 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 1.9073
2022-02-20 22:29:31 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.0562
2022-02-20 22:30:04 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.1162
2022-02-20 22:30:36 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 1.9367
2022-02-20 22:31:08 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.1335
2022-02-20 22:31:40 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.0304
2022-02-20 22:32:12 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 1.9831
2022-02-20 22:32:44 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.0028
2022-02-20 22:33:16 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.2233
2022-02-20 22:33:48 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.2695
2022-02-20 22:34:20 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.1014
2022-02-20 22:34:52 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 1.8885
2022-02-20 22:35:25 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 1.9904
2022-02-20 22:35:57 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.1786
2022-02-20 22:36:29 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.7596
2022-02-20 22:37:01 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.0229
2022-02-20 22:37:33 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.0030
2022-02-20 22:38:05 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.1896
2022-02-20 22:38:37 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.0584
2022-02-20 22:39:10 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.3622
2022-02-20 22:39:42 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 1.9809
2022-02-20 22:40:14 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.0158
2022-02-20 22:40:47 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.0571
2022-02-20 22:41:19 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.1059
2022-02-20 22:41:51 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.0441
2022-02-20 22:42:24 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.1260
2022-02-20 22:42:56 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 1.9903
2022-02-20 22:43:29 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.1375
2022-02-20 22:44:02 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.0008
2022-02-20 22:44:36 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 1.9576
2022-02-20 22:45:09 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.0397
2022-02-20 22:45:43 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.1519
2022-02-20 22:45:45 - train: epoch 024, train_loss: 2.0642
2022-02-20 22:47:00 - eval: epoch: 024, acc1: 56.630%, acc5: 81.352%, test_loss: 1.8226, per_image_load_time: 2.140ms, per_image_inference_time: 0.739ms
2022-02-20 22:47:02 - until epoch: 024, best_acc1: 56.630%
2022-02-20 22:47:02 - epoch 025 lr: 0.1
2022-02-20 22:47:38 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 1.9834
2022-02-20 22:48:10 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 1.8654
2022-02-20 22:48:42 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 1.9373
2022-02-20 22:49:13 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.0399
2022-02-20 22:49:45 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 1.9292
2022-02-20 22:50:17 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.0755
2022-02-20 22:50:49 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.1601
2022-02-20 22:51:21 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.0543
2022-02-20 22:51:53 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 1.8745
2022-02-20 22:52:25 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.0305
2022-02-20 22:52:57 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.0975
2022-02-20 22:53:29 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.1548
2022-02-20 22:54:01 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.1457
2022-02-20 22:54:33 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.2179
2022-02-20 22:55:05 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 1.9946
2022-02-20 22:55:37 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 1.8465
2022-02-20 22:56:09 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 1.9808
2022-02-20 22:56:41 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 1.9412
2022-02-20 22:57:13 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 1.9554
2022-02-20 22:57:46 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.0553
2022-02-20 22:58:18 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 1.9822
2022-02-20 22:58:50 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 1.8751
2022-02-20 22:59:22 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.1094
2022-02-20 22:59:55 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 1.8758
2022-02-20 23:00:27 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.1231
2022-02-20 23:01:00 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.1738
2022-02-20 23:01:32 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.1233
2022-02-20 23:02:04 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.0549
2022-02-20 23:02:37 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.2651
2022-02-20 23:03:09 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.3094
2022-02-20 23:03:41 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 1.9678
2022-02-20 23:04:13 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.2977
2022-02-20 23:04:45 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.0143
2022-02-20 23:05:17 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.1390
2022-02-20 23:05:50 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 1.7703
2022-02-20 23:06:22 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.2179
2022-02-20 23:06:54 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.0014
2022-02-20 23:07:26 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.1332
2022-02-20 23:07:59 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.2556
2022-02-20 23:08:31 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.2140
2022-02-20 23:09:03 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.1100
2022-02-20 23:09:36 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.1540
2022-02-20 23:10:08 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.0313
2022-02-20 23:10:41 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 1.9183
2022-02-20 23:11:14 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.0341
2022-02-20 23:11:46 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.0549
2022-02-20 23:12:19 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 1.9829
2022-02-20 23:12:53 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 1.7745
2022-02-20 23:13:26 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.0023
2022-02-20 23:14:00 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.2517
2022-02-20 23:14:02 - train: epoch 025, train_loss: 2.0589
2022-02-20 23:15:16 - eval: epoch: 025, acc1: 55.002%, acc5: 80.108%, test_loss: 1.8998, per_image_load_time: 1.957ms, per_image_inference_time: 0.724ms
2022-02-20 23:15:18 - until epoch: 025, best_acc1: 56.630%
2022-02-20 23:15:18 - epoch 026 lr: 0.1
2022-02-20 23:15:55 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 1.8671
2022-02-20 23:16:27 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.8133
2022-02-20 23:16:58 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 1.8924
2022-02-20 23:17:30 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.0474
2022-02-20 23:18:02 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.0221
2022-02-20 23:18:33 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.1219
2022-02-20 23:19:06 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 1.8960
2022-02-20 23:19:38 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 1.8069
2022-02-20 23:20:10 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.2901
2022-02-20 23:20:42 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 1.9524
2022-02-20 23:21:14 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 1.9906
2022-02-20 23:21:46 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.1162
2022-02-20 23:22:18 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 1.9947
2022-02-20 23:22:50 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.1263
2022-02-20 23:23:22 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.0309
2022-02-20 23:23:55 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.1128
2022-02-20 23:24:27 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.0606
2022-02-20 23:24:59 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.1117
2022-02-20 23:25:31 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.3650
2022-02-20 23:26:03 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.1163
2022-02-20 23:26:35 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.1418
2022-02-20 23:27:07 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 1.9705
2022-02-20 23:27:40 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.0272
2022-02-20 23:28:12 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.3043
2022-02-20 23:28:44 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.1895
2022-02-20 23:29:16 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.0001
2022-02-20 23:29:49 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.0383
2022-02-20 23:30:21 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 1.9562
2022-02-20 23:30:53 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.0714
2022-02-20 23:31:25 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.0072
2022-02-20 23:31:58 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.0216
2022-02-20 23:32:30 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.0761
2022-02-20 23:33:02 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.0072
2022-02-20 23:33:35 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.2728
2022-02-20 23:34:07 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.1191
2022-02-20 23:34:39 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 1.8321
2022-02-20 23:35:11 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.1124
2022-02-20 23:35:43 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.2254
2022-02-20 23:36:16 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.2377
2022-02-20 23:36:48 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.3103
2022-02-20 23:37:20 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.0959
2022-02-20 23:37:52 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.2214
2022-02-20 23:38:25 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.1502
2022-02-20 23:38:57 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.2831
2022-02-20 23:39:30 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.2720
2022-02-20 23:40:02 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.0458
2022-02-20 23:40:35 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 1.9855
2022-02-20 23:41:09 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.0682
2022-02-20 23:41:42 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.0865
2022-02-20 23:42:16 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.2132
2022-02-20 23:42:18 - train: epoch 026, train_loss: 2.0542
2022-02-20 23:43:32 - eval: epoch: 026, acc1: 55.294%, acc5: 80.426%, test_loss: 1.8723, per_image_load_time: 2.067ms, per_image_inference_time: 0.729ms
2022-02-20 23:43:33 - until epoch: 026, best_acc1: 56.630%
2022-02-20 23:43:33 - epoch 027 lr: 0.1
2022-02-20 23:44:10 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.2009
2022-02-20 23:44:42 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 1.9865
2022-02-20 23:45:14 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.0870
2022-02-20 23:45:46 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.2374
2022-02-20 23:46:18 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.2209
2022-02-20 23:46:50 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.3393
2022-02-20 23:47:22 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.2021
2022-02-20 23:47:55 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.0741
2022-02-20 23:48:27 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 1.9731
2022-02-20 23:48:59 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.0729
2022-02-20 23:49:31 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 1.8957
2022-02-20 23:50:03 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.2603
2022-02-20 23:50:35 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.0853
2022-02-20 23:51:08 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.2889
2022-02-20 23:51:40 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.1319
2022-02-20 23:52:12 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.0322
2022-02-20 23:52:44 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.1227
2022-02-20 23:53:16 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 1.9845
2022-02-20 23:53:48 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.2580
2022-02-20 23:54:19 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.0871
2022-02-20 23:54:51 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.1956
2022-02-20 23:55:23 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.2445
2022-02-20 23:55:55 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.3938
2022-02-20 23:56:27 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.1712
2022-02-20 23:57:00 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 1.9039
2022-02-20 23:57:32 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.2979
2022-02-20 23:58:04 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.0687
2022-02-20 23:58:36 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.1952
2022-02-20 23:59:08 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.0283
2022-02-20 23:59:40 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 1.9554
2022-02-21 00:00:12 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 1.8806
2022-02-21 00:00:45 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 1.8959
2022-02-21 00:01:17 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.1311
2022-02-21 00:01:49 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 1.9904
2022-02-21 00:02:21 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.2965
2022-02-21 00:02:53 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 1.9289
2022-02-21 00:03:25 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 1.9825
2022-02-21 00:03:57 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 1.8253
2022-02-21 00:04:29 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 1.8624
2022-02-21 00:05:01 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.0800
2022-02-21 00:05:34 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.0174
2022-02-21 00:06:06 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.1377
2022-02-21 00:06:38 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 1.9385
2022-02-21 00:07:11 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.1367
2022-02-21 00:07:43 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 1.9439
2022-02-21 00:08:16 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.0262
2022-02-21 00:08:49 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.0509
2022-02-21 00:09:22 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.3009
2022-02-21 00:09:55 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.1727
2022-02-21 00:10:28 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.7794
2022-02-21 00:10:31 - train: epoch 027, train_loss: 2.0494
2022-02-21 00:11:46 - eval: epoch: 027, acc1: 55.094%, acc5: 80.066%, test_loss: 1.9080, per_image_load_time: 2.209ms, per_image_inference_time: 0.715ms
2022-02-21 00:11:47 - until epoch: 027, best_acc1: 56.630%
2022-02-21 00:11:47 - epoch 028 lr: 0.1
2022-02-21 00:12:24 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 1.7950
2022-02-21 00:12:56 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 1.9835
2022-02-21 00:13:27 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.0105
2022-02-21 00:13:59 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 1.9299
2022-02-21 00:14:31 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 1.8715
2022-02-21 00:15:03 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.1094
2022-02-21 00:15:35 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.2296
2022-02-21 00:16:07 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.7416
2022-02-21 00:16:39 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 1.9301
2022-02-21 00:17:11 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.2032
2022-02-21 00:17:43 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.0637
2022-02-21 00:18:15 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 1.8415
2022-02-21 00:18:47 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.0539
2022-02-21 00:19:20 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.2924
2022-02-21 00:19:52 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 1.9941
2022-02-21 00:20:24 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 1.9504
2022-02-21 00:20:56 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 1.9579
2022-02-21 00:21:29 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 1.8624
2022-02-21 00:22:01 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 1.9542
2022-02-21 00:22:32 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.2095
2022-02-21 00:23:04 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.0012
2022-02-21 00:23:36 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.1594
2022-02-21 00:24:08 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.1712
2022-02-21 00:24:41 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.2558
2022-02-21 00:25:13 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.1569
2022-02-21 00:25:45 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 1.7932
2022-02-21 00:26:17 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 1.9818
2022-02-21 00:26:49 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 1.9998
2022-02-21 00:27:21 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.1508
2022-02-21 00:27:53 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.1496
2022-02-21 00:28:25 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.2380
2022-02-21 00:28:58 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.0777
2022-02-21 00:29:30 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.1069
2022-02-21 00:30:02 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 1.9970
2022-02-21 00:30:34 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.0155
2022-02-21 00:31:06 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 1.9135
2022-02-21 00:31:38 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.0726
2022-02-21 00:32:11 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.7458
2022-02-21 00:32:43 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 1.9590
2022-02-21 00:33:15 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.1520
2022-02-21 00:33:47 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.1213
2022-02-21 00:34:19 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.2606
2022-02-21 00:34:51 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 1.9548
2022-02-21 00:35:24 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.0460
2022-02-21 00:35:56 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 1.9953
2022-02-21 00:36:29 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.1455
2022-02-21 00:37:02 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.0048
2022-02-21 00:37:35 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.0098
2022-02-21 00:38:08 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 1.8286
2022-02-21 00:38:42 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.0555
2022-02-21 00:38:44 - train: epoch 028, train_loss: 2.0439
2022-02-21 00:39:59 - eval: epoch: 028, acc1: 55.654%, acc5: 80.322%, test_loss: 1.8869, per_image_load_time: 2.212ms, per_image_inference_time: 0.693ms
2022-02-21 00:40:00 - until epoch: 028, best_acc1: 56.630%
2022-02-21 00:40:00 - epoch 029 lr: 0.1
2022-02-21 00:40:38 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.1142
2022-02-21 00:41:10 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.0298
2022-02-21 00:41:42 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.1844
2022-02-21 00:42:14 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.0564
2022-02-21 00:42:45 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.0320
2022-02-21 00:43:17 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.2778
2022-02-21 00:43:49 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.5544
2022-02-21 00:44:21 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.1278
2022-02-21 00:44:53 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.0077
2022-02-21 00:45:25 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 1.8616
2022-02-21 00:45:57 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 1.9460
2022-02-21 00:46:29 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.0690
2022-02-21 00:47:01 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.0304
2022-02-21 00:47:34 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.1761
2022-02-21 00:48:06 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.0940
2022-02-21 00:48:38 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 1.9567
2022-02-21 00:49:10 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.0598
2022-02-21 00:49:42 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.1588
2022-02-21 00:50:14 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 1.8358
2022-02-21 00:50:46 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.2565
2022-02-21 00:51:18 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 1.9910
2022-02-21 00:51:51 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.0741
2022-02-21 00:52:23 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.1269
2022-02-21 00:52:55 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.0269
2022-02-21 00:53:28 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 1.9839
2022-02-21 00:54:00 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.0710
2022-02-21 00:54:32 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 1.9430
2022-02-21 00:55:04 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 1.9397
2022-02-21 00:55:36 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 1.9038
2022-02-21 00:56:08 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 1.9404
2022-02-21 00:56:40 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.0040
2022-02-21 00:57:13 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.1723
2022-02-21 00:57:45 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.0524
2022-02-21 00:58:17 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 1.8447
2022-02-21 00:58:50 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.0557
2022-02-21 00:59:22 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.0128
2022-02-21 00:59:54 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.0135
2022-02-21 01:00:26 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 1.9822
2022-02-21 01:00:59 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 1.8966
2022-02-21 01:01:31 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 1.9228
2022-02-21 01:02:03 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 1.9242
2022-02-21 01:02:36 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 1.9052
2022-02-21 01:03:08 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.1165
2022-02-21 01:03:40 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 1.9642
2022-02-21 01:04:12 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.1357
2022-02-21 01:04:45 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.1090
2022-02-21 01:05:18 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 1.8164
2022-02-21 01:05:51 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.1554
2022-02-21 01:06:24 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.3797
2022-02-21 01:06:58 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.7340
2022-02-21 01:07:00 - train: epoch 029, train_loss: 2.0413
2022-02-21 01:08:14 - eval: epoch: 029, acc1: 55.980%, acc5: 81.190%, test_loss: 1.8381, per_image_load_time: 2.159ms, per_image_inference_time: 0.733ms
2022-02-21 01:08:16 - until epoch: 029, best_acc1: 56.630%
2022-02-21 01:08:16 - epoch 030 lr: 0.1
2022-02-21 01:08:53 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.1655
2022-02-21 01:09:24 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.0768
2022-02-21 01:09:56 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.0505
2022-02-21 01:10:28 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 1.7917
2022-02-21 01:11:00 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.3494
2022-02-21 01:11:31 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 1.8422
2022-02-21 01:12:04 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 1.9946
2022-02-21 01:12:36 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.3018
2022-02-21 01:13:08 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.0132
2022-02-21 01:13:40 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 1.7880
2022-02-21 01:14:12 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 1.8448
2022-02-21 01:14:44 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.0284
2022-02-21 01:15:17 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 1.8735
2022-02-21 01:15:49 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 1.9856
2022-02-21 01:16:21 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 1.9916
2022-02-21 01:16:53 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.0599
2022-02-21 01:17:25 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.3146
2022-02-21 01:17:58 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.0644
2022-02-21 01:18:30 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.2526
2022-02-21 01:19:02 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 1.9943
2022-02-21 01:19:34 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.1238
2022-02-21 01:20:06 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 1.9603
2022-02-21 01:20:38 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.0272
2022-02-21 01:21:10 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.1888
2022-02-21 01:21:42 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.0965
2022-02-21 01:22:15 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.0272
2022-02-21 01:22:47 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 1.9945
2022-02-21 01:23:19 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 1.9196
2022-02-21 01:23:52 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.0842
2022-02-21 01:24:24 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.2765
2022-02-21 01:24:56 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.0707
2022-02-21 01:25:28 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 1.9798
2022-02-21 01:26:01 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.1793
2022-02-21 01:26:33 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.0637
2022-02-21 01:27:05 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.1069
2022-02-21 01:27:37 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 1.9024
2022-02-21 01:28:09 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.0510
2022-02-21 01:28:41 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.0714
2022-02-21 01:29:14 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.1457
2022-02-21 01:29:46 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 1.9649
2022-02-21 01:30:19 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 1.9634
2022-02-21 01:30:51 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.2512
2022-02-21 01:31:23 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 1.9834
2022-02-21 01:31:56 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.0576
2022-02-21 01:32:28 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.2519
2022-02-21 01:33:01 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 1.8124
2022-02-21 01:33:34 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.0794
2022-02-21 01:34:07 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.0367
2022-02-21 01:34:40 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.1829
2022-02-21 01:35:14 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.1116
2022-02-21 01:35:16 - train: epoch 030, train_loss: 2.0428
2022-02-21 01:36:31 - eval: epoch: 030, acc1: 55.632%, acc5: 80.608%, test_loss: 1.8600, per_image_load_time: 2.106ms, per_image_inference_time: 0.730ms
2022-02-21 01:36:32 - until epoch: 030, best_acc1: 56.630%
2022-02-21 01:36:32 - epoch 031 lr: 0.010000000000000002
2022-02-21 01:37:09 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 1.8759
2022-02-21 01:37:41 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.4442
2022-02-21 01:38:12 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.5717
2022-02-21 01:38:44 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.7733
2022-02-21 01:39:15 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.5046
2022-02-21 01:39:47 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.5295
2022-02-21 01:40:19 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.4450
2022-02-21 01:40:51 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.4531
2022-02-21 01:41:23 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.4801
2022-02-21 01:41:56 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 1.7843
2022-02-21 01:42:28 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 1.7097
2022-02-21 01:43:00 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.4978
2022-02-21 01:43:32 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.3720
2022-02-21 01:44:04 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.5021
2022-02-21 01:44:37 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.6529
2022-02-21 01:45:09 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.3436
2022-02-21 01:45:41 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.3559
2022-02-21 01:46:13 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.4625
2022-02-21 01:46:45 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.5956
2022-02-21 01:47:17 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.5087
2022-02-21 01:47:49 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.3169
2022-02-21 01:48:21 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.3211
2022-02-21 01:48:54 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.3337
2022-02-21 01:49:26 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.4399
2022-02-21 01:49:58 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.3409
2022-02-21 01:50:31 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.5281
2022-02-21 01:51:03 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.5451
2022-02-21 01:51:35 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.7054
2022-02-21 01:52:07 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.4826
2022-02-21 01:52:39 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.5704
2022-02-21 01:53:11 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.3562
2022-02-21 01:53:43 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.5295
2022-02-21 01:54:15 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.5475
2022-02-21 01:54:47 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.6022
2022-02-21 01:55:19 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.6444
2022-02-21 01:55:52 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.4782
2022-02-21 01:56:24 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.4268
2022-02-21 01:56:56 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.2627
2022-02-21 01:57:28 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.3366
2022-02-21 01:58:00 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.3717
2022-02-21 01:58:32 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.3965
2022-02-21 01:59:04 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.4960
2022-02-21 01:59:37 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.3917
2022-02-21 02:00:09 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.4682
2022-02-21 02:00:42 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.5273
2022-02-21 02:01:14 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.4872
2022-02-21 02:01:47 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.4856
2022-02-21 02:02:20 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.3319
2022-02-21 02:02:53 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.3735
2022-02-21 02:03:27 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.3838
2022-02-21 02:03:29 - train: epoch 031, train_loss: 1.4875
2022-02-21 02:04:44 - eval: epoch: 031, acc1: 71.022%, acc5: 90.320%, test_loss: 1.1507, per_image_load_time: 2.137ms, per_image_inference_time: 0.738ms
2022-02-21 02:04:45 - until epoch: 031, best_acc1: 71.022%
2022-02-21 02:04:45 - epoch 032 lr: 0.010000000000000002
2022-02-21 02:05:22 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.3888
2022-02-21 02:05:54 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.4370
2022-02-21 02:06:26 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.4186
2022-02-21 02:06:57 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.5368
2022-02-21 02:07:29 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.3900
2022-02-21 02:08:01 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.4745
2022-02-21 02:08:33 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.4035
2022-02-21 02:09:05 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.4174
2022-02-21 02:09:37 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.3654
2022-02-21 02:10:09 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.5243
2022-02-21 02:10:41 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.4752
2022-02-21 02:11:14 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.3364
2022-02-21 02:11:46 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.2949
2022-02-21 02:12:18 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.5193
2022-02-21 02:12:50 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.3849
2022-02-21 02:13:22 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.4678
2022-02-21 02:13:54 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.4737
2022-02-21 02:14:26 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.5715
2022-02-21 02:14:58 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.2058
2022-02-21 02:15:30 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.4085
2022-02-21 02:16:02 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.3022
2022-02-21 02:16:35 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.2202
2022-02-21 02:17:07 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.3754
2022-02-21 02:17:39 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.3057
2022-02-21 02:18:11 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.4425
2022-02-21 02:18:43 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.1670
2022-02-21 02:19:16 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.4060
2022-02-21 02:19:48 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.4855
2022-02-21 02:20:20 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.2039
2022-02-21 02:20:53 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.1449
2022-02-21 02:21:25 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.3547
2022-02-21 02:21:57 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.3359
2022-02-21 02:22:29 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.3134
2022-02-21 02:23:01 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.1705
2022-02-21 02:23:34 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.3436
2022-02-21 02:24:06 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.4765
2022-02-21 02:24:38 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.3632
2022-02-21 02:25:10 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.1850
2022-02-21 02:25:43 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.3946
2022-02-21 02:26:15 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.2513
2022-02-21 02:26:47 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.3298
2022-02-21 02:27:19 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.2157
2022-02-21 02:27:52 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.6003
2022-02-21 02:28:24 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.3002
2022-02-21 02:28:57 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.5816
2022-02-21 02:29:30 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.3732
2022-02-21 02:30:03 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.4088
2022-02-21 02:30:36 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.2400
2022-02-21 02:31:09 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.5174
2022-02-21 02:31:43 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.3384
2022-02-21 02:31:45 - train: epoch 032, train_loss: 1.3701
2022-02-21 02:33:00 - eval: epoch: 032, acc1: 71.976%, acc5: 90.784%, test_loss: 1.1087, per_image_load_time: 2.114ms, per_image_inference_time: 0.733ms
2022-02-21 02:33:02 - until epoch: 032, best_acc1: 71.976%
2022-02-21 02:33:02 - epoch 033 lr: 0.010000000000000002
2022-02-21 02:33:39 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.1364
2022-02-21 02:34:11 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.4477
2022-02-21 02:34:42 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.1957
2022-02-21 02:35:14 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.1691
2022-02-21 02:35:46 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.3728
2022-02-21 02:36:18 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.1558
2022-02-21 02:36:50 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.3356
2022-02-21 02:37:22 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.3797
2022-02-21 02:37:54 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.3421
2022-02-21 02:38:26 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.3627
2022-02-21 02:38:59 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.3094
2022-02-21 02:39:31 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.3202
2022-02-21 02:40:03 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.1616
2022-02-21 02:40:35 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.4821
2022-02-21 02:41:08 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.4818
2022-02-21 02:41:40 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.4819
2022-02-21 02:42:12 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.2316
2022-02-21 02:42:44 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.4488
2022-02-21 02:43:16 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.3995
2022-02-21 02:43:49 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.2207
2022-02-21 02:44:21 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.4054
2022-02-21 02:44:53 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.4594
2022-02-21 02:45:25 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.2697
2022-02-21 02:45:58 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.5958
2022-02-21 02:46:30 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.2572
2022-02-21 02:47:02 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.1973
2022-02-21 02:47:34 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.4798
2022-02-21 02:48:06 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.2690
2022-02-21 02:48:38 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.4141
2022-02-21 02:49:10 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.4075
2022-02-21 02:49:43 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.3630
2022-02-21 02:50:15 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.3002
2022-02-21 02:50:47 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.4263
2022-02-21 02:51:19 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.2264
2022-02-21 02:51:51 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.3305
2022-02-21 02:52:23 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.5284
2022-02-21 02:52:55 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.2912
2022-02-21 02:53:27 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.1572
2022-02-21 02:53:59 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.3455
2022-02-21 02:54:32 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.4067
2022-02-21 02:55:04 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.2997
2022-02-21 02:55:36 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.2398
2022-02-21 02:56:08 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.4476
2022-02-21 02:56:41 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.3592
2022-02-21 02:57:13 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.5013
2022-02-21 02:57:46 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.2758
2022-02-21 02:58:19 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.1802
2022-02-21 02:58:52 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.7209
2022-02-21 02:59:26 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.1805
2022-02-21 03:00:00 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.2492
2022-02-21 03:00:02 - train: epoch 033, train_loss: 1.3202
2022-02-21 03:01:17 - eval: epoch: 033, acc1: 72.374%, acc5: 91.062%, test_loss: 1.0916, per_image_load_time: 2.133ms, per_image_inference_time: 0.740ms
2022-02-21 03:01:19 - until epoch: 033, best_acc1: 72.374%
2022-02-21 03:01:19 - epoch 034 lr: 0.010000000000000002
2022-02-21 03:01:56 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.2613
2022-02-21 03:02:27 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.3375
2022-02-21 03:02:59 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.2784
2022-02-21 03:03:30 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.1371
2022-02-21 03:04:02 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.3919
2022-02-21 03:04:34 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.5886
2022-02-21 03:05:06 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.2449
2022-02-21 03:05:38 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.1787
2022-02-21 03:06:10 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.2781
2022-02-21 03:06:42 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.2274
2022-02-21 03:07:15 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.2796
2022-02-21 03:07:47 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.3401
2022-02-21 03:08:19 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.2243
2022-02-21 03:08:51 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.2388
2022-02-21 03:09:23 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.2542
2022-02-21 03:09:56 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.2040
2022-02-21 03:10:28 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.2890
2022-02-21 03:11:00 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.4313
2022-02-21 03:11:32 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.4483
2022-02-21 03:12:05 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.3387
2022-02-21 03:12:37 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.4828
2022-02-21 03:13:09 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.1836
2022-02-21 03:13:41 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.3880
2022-02-21 03:14:13 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.2001
2022-02-21 03:14:46 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.2346
2022-02-21 03:15:18 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.3558
2022-02-21 03:15:50 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.2987
2022-02-21 03:16:22 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.0673
2022-02-21 03:16:54 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.0863
2022-02-21 03:17:27 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.0681
2022-02-21 03:17:59 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.2161
2022-02-21 03:18:31 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.2690
2022-02-21 03:19:03 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.2439
2022-02-21 03:19:35 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.3190
2022-02-21 03:20:08 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.2124
2022-02-21 03:20:40 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.0951
2022-02-21 03:21:13 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.2013
2022-02-21 03:21:45 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.2247
2022-02-21 03:22:17 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.4500
2022-02-21 03:22:50 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.1546
2022-02-21 03:23:22 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.3315
2022-02-21 03:23:55 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.2221
2022-02-21 03:24:27 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.2237
2022-02-21 03:25:00 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.3380
2022-02-21 03:25:33 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.4428
2022-02-21 03:26:05 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.3853
2022-02-21 03:26:38 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.2104
2022-02-21 03:27:11 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.2145
2022-02-21 03:27:45 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.2647
2022-02-21 03:28:18 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.1358
2022-02-21 03:28:21 - train: epoch 034, train_loss: 1.2925
2022-02-21 03:29:36 - eval: epoch: 034, acc1: 72.560%, acc5: 91.210%, test_loss: 1.0805, per_image_load_time: 0.922ms, per_image_inference_time: 0.722ms
2022-02-21 03:29:37 - until epoch: 034, best_acc1: 72.560%
2022-02-21 03:29:37 - epoch 035 lr: 0.010000000000000002
2022-02-21 03:30:14 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.1439
2022-02-21 03:30:46 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.1217
2022-02-21 03:31:18 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.4823
2022-02-21 03:31:49 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.1714
2022-02-21 03:32:21 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.2507
2022-02-21 03:32:53 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.2362
2022-02-21 03:33:25 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.2538
2022-02-21 03:33:57 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.2400
2022-02-21 03:34:29 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.3495
2022-02-21 03:35:01 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.4738
2022-02-21 03:35:33 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.4884
2022-02-21 03:36:05 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.3219
2022-02-21 03:36:37 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.2879
2022-02-21 03:37:09 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.2310
2022-02-21 03:37:41 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.3590
2022-02-21 03:38:14 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.2535
2022-02-21 03:38:46 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.1003
2022-02-21 03:39:18 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.3035
2022-02-21 03:39:51 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.3318
2022-02-21 03:40:23 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.2220
2022-02-21 03:40:55 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.2360
2022-02-21 03:41:27 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.3848
2022-02-21 03:41:59 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.2694
2022-02-21 03:42:31 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.3756
2022-02-21 03:43:04 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.2166
2022-02-21 03:43:36 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.4500
2022-02-21 03:44:08 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.3494
2022-02-21 03:44:40 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.2401
2022-02-21 03:45:12 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.2236
2022-02-21 03:45:44 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.4204
2022-02-21 03:46:17 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.2330
2022-02-21 03:46:49 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.1358
2022-02-21 03:47:21 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.2261
2022-02-21 03:47:53 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.2104
2022-02-21 03:48:26 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.1011
2022-02-21 03:48:58 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.2815
2022-02-21 03:49:30 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.0352
2022-02-21 03:50:02 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.3031
2022-02-21 03:50:35 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.3710
2022-02-21 03:51:07 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.1107
2022-02-21 03:51:39 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.5183
2022-02-21 03:52:12 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.2264
2022-02-21 03:52:44 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.3032
2022-02-21 03:53:16 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.3080
2022-02-21 03:53:49 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.3906
2022-02-21 03:54:22 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.2216
2022-02-21 03:54:55 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.5382
2022-02-21 03:55:28 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.4707
2022-02-21 03:56:01 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.2869
2022-02-21 03:56:35 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.1887
2022-02-21 03:56:37 - train: epoch 035, train_loss: 1.2717
2022-02-21 03:57:53 - eval: epoch: 035, acc1: 72.392%, acc5: 91.266%, test_loss: 1.0835, per_image_load_time: 2.177ms, per_image_inference_time: 0.693ms
2022-02-21 03:57:54 - until epoch: 035, best_acc1: 72.560%
2022-02-21 03:57:54 - epoch 036 lr: 0.010000000000000002
2022-02-21 03:58:31 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.3581
2022-02-21 03:59:03 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.0811
2022-02-21 03:59:35 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.1704
2022-02-21 04:00:06 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.2871
2022-02-21 04:00:38 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.2117
2022-02-21 04:01:10 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.1198
2022-02-21 04:01:42 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.0788
2022-02-21 04:02:15 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.0022
2022-02-21 04:02:47 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.2343
2022-02-21 04:03:19 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.2338
2022-02-21 04:03:51 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.3598
2022-02-21 04:04:23 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.2574
2022-02-21 04:04:55 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.2199
2022-02-21 04:05:27 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.4301
2022-02-21 04:06:00 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.3834
2022-02-21 04:06:32 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.2243
2022-02-21 04:07:04 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.3169
2022-02-21 04:07:36 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.1850
2022-02-21 04:08:08 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.2403
2022-02-21 04:08:40 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.2791
2022-02-21 04:09:12 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.2229
2022-02-21 04:09:45 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.2312
2022-02-21 04:10:17 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.3514
2022-02-21 04:10:49 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.3137
2022-02-21 04:11:22 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.1455
2022-02-21 04:11:54 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.2953
2022-02-21 04:12:26 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.1657
2022-02-21 04:12:58 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.1483
2022-02-21 04:13:31 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.1173
2022-02-21 04:14:03 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.3988
2022-02-21 04:14:35 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.2508
2022-02-21 04:15:07 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.3289
2022-02-21 04:15:39 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.1467
2022-02-21 04:16:12 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.2408
2022-02-21 04:16:44 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.2091
2022-02-21 04:17:16 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.3097
2022-02-21 04:17:49 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.2722
2022-02-21 04:18:21 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.1098
2022-02-21 04:18:53 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.2724
2022-02-21 04:19:26 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.2859
2022-02-21 04:19:58 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.2477
2022-02-21 04:20:30 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.1820
2022-02-21 04:21:03 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.1794
2022-02-21 04:21:35 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.3675
2022-02-21 04:22:08 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.0270
2022-02-21 04:22:40 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.1250
2022-02-21 04:23:13 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.2117
2022-02-21 04:23:46 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.2834
2022-02-21 04:24:20 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.2310
2022-02-21 04:24:53 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.2737
2022-02-21 04:24:56 - train: epoch 036, train_loss: 1.2560
2022-02-21 04:26:11 - eval: epoch: 036, acc1: 72.968%, acc5: 91.454%, test_loss: 1.0687, per_image_load_time: 2.128ms, per_image_inference_time: 0.709ms
2022-02-21 04:26:12 - until epoch: 036, best_acc1: 72.968%
2022-02-21 04:26:12 - epoch 037 lr: 0.010000000000000002
2022-02-21 04:26:49 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.0494
2022-02-21 04:27:20 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.0367
2022-02-21 04:27:52 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.1515
2022-02-21 04:28:24 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.3360
2022-02-21 04:28:56 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.1749
2022-02-21 04:29:28 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.2906
2022-02-21 04:30:00 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.3523
2022-02-21 04:30:32 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.1660
2022-02-21 04:31:04 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.4980
2022-02-21 04:31:37 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.2125
2022-02-21 04:32:09 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.2996
2022-02-21 04:32:41 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.1839
2022-02-21 04:33:13 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.2664
2022-02-21 04:33:45 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.2928
2022-02-21 04:34:18 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.0905
2022-02-21 04:34:50 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.1170
2022-02-21 04:35:22 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.3870
2022-02-21 04:35:54 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.3313
2022-02-21 04:36:26 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.3322
2022-02-21 04:36:58 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.2509
2022-02-21 04:37:31 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.3064
2022-02-21 04:38:03 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.2923
2022-02-21 04:38:35 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.1344
2022-02-21 04:39:07 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.2153
2022-02-21 04:39:39 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.1538
2022-02-21 04:40:12 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.4391
2022-02-21 04:40:44 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.2629
2022-02-21 04:41:16 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.2479
2022-02-21 04:41:48 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.2915
2022-02-21 04:42:21 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.3155
2022-02-21 04:42:53 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.2033
2022-02-21 04:43:25 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.2579
2022-02-21 04:43:57 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.1443
2022-02-21 04:44:29 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.0771
2022-02-21 04:45:02 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.2734
2022-02-21 04:45:34 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.3874
2022-02-21 04:46:07 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.3496
2022-02-21 04:46:39 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.2210
2022-02-21 04:47:12 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.4044
2022-02-21 04:47:44 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.1517
2022-02-21 04:48:16 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.2684
2022-02-21 04:48:49 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.4095
2022-02-21 04:49:21 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.3172
2022-02-21 04:49:54 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.2366
2022-02-21 04:50:27 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.2293
2022-02-21 04:50:59 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.3186
2022-02-21 04:51:32 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.3103
2022-02-21 04:52:06 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.2246
2022-02-21 04:52:39 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.3443
2022-02-21 04:53:13 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.2092
2022-02-21 04:53:15 - train: epoch 037, train_loss: 1.2495
2022-02-21 04:54:30 - eval: epoch: 037, acc1: 72.940%, acc5: 91.570%, test_loss: 1.0678, per_image_load_time: 2.113ms, per_image_inference_time: 0.736ms
2022-02-21 04:54:31 - until epoch: 037, best_acc1: 72.968%
2022-02-21 04:54:31 - epoch 038 lr: 0.010000000000000002
2022-02-21 04:55:08 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.1853
2022-02-21 04:55:40 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 0.9574
2022-02-21 04:56:11 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 0.9432
2022-02-21 04:56:43 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.1490
2022-02-21 04:57:15 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.1063
2022-02-21 04:57:47 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.2777
2022-02-21 04:58:19 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.1295
2022-02-21 04:58:51 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.1819
2022-02-21 04:59:23 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.1732
2022-02-21 04:59:56 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.3772
2022-02-21 05:00:28 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.1993
2022-02-21 05:01:00 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.2735
2022-02-21 05:01:32 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.3085
2022-02-21 05:02:04 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.3015
2022-02-21 05:02:36 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.3627
2022-02-21 05:03:09 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.3179
2022-02-21 05:03:40 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.3734
2022-02-21 05:04:13 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.2859
2022-02-21 05:04:45 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.3294
2022-02-21 05:05:17 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.2069
2022-02-21 05:05:49 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.2865
2022-02-21 05:06:22 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.1824
2022-02-21 05:06:54 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.2730
2022-02-21 05:07:26 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.5147
2022-02-21 05:07:58 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.1349
2022-02-21 05:08:30 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.2582
2022-02-21 05:09:03 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.3048
2022-02-21 05:09:35 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.3410
2022-02-21 05:10:07 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.3966
2022-02-21 05:10:39 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.2540
2022-02-21 05:11:11 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.2164
2022-02-21 05:11:43 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 0.9952
2022-02-21 05:12:16 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.0647
2022-02-21 05:12:48 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.2450
2022-02-21 05:13:20 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.3683
2022-02-21 05:13:52 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.3550
2022-02-21 05:14:24 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.0537
2022-02-21 05:14:56 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.3317
2022-02-21 05:15:29 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.2334
2022-02-21 05:16:01 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.2497
2022-02-21 05:16:33 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.1566
2022-02-21 05:17:05 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.0909
2022-02-21 05:17:38 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.3405
2022-02-21 05:18:10 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.2775
2022-02-21 05:18:43 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.2823
2022-02-21 05:19:15 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.3856
2022-02-21 05:19:48 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.1194
2022-02-21 05:20:21 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.2727
2022-02-21 05:20:54 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.2434
2022-02-21 05:21:27 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.0725
2022-02-21 05:21:30 - train: epoch 038, train_loss: 1.2428
2022-02-21 05:22:44 - eval: epoch: 038, acc1: 73.016%, acc5: 91.526%, test_loss: 1.0656, per_image_load_time: 1.783ms, per_image_inference_time: 0.732ms
2022-02-21 05:22:46 - until epoch: 038, best_acc1: 73.016%
2022-02-21 05:22:46 - epoch 039 lr: 0.010000000000000002
2022-02-21 05:23:23 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.2947
2022-02-21 05:23:55 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.4840
2022-02-21 05:24:27 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.0839
2022-02-21 05:24:58 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.2906
2022-02-21 05:25:30 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.2130
2022-02-21 05:26:02 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.1174
2022-02-21 05:26:34 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.3508
2022-02-21 05:27:06 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.1661
2022-02-21 05:27:38 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.3381
2022-02-21 05:28:10 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.1926
2022-02-21 05:28:42 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.2397
2022-02-21 05:29:14 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.2206
2022-02-21 05:29:46 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.4102
2022-02-21 05:30:18 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.2641
2022-02-21 05:30:50 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.2195
2022-02-21 05:31:22 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.4584
2022-02-21 05:31:54 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.0938
2022-02-21 05:32:26 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.2476
2022-02-21 05:32:58 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.0376
2022-02-21 05:33:30 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.0930
2022-02-21 05:34:02 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.2366
2022-02-21 05:34:33 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.2075
2022-02-21 05:35:06 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.4467
2022-02-21 05:35:37 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.3803
2022-02-21 05:36:09 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.1356
2022-02-21 05:36:41 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.1941
2022-02-21 05:37:13 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.3838
2022-02-21 05:37:45 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.1768
2022-02-21 05:38:17 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.0932
2022-02-21 05:38:49 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.2963
2022-02-21 05:39:22 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.1252
2022-02-21 05:39:54 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.2366
2022-02-21 05:40:26 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.3523
2022-02-21 05:40:58 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.3321
2022-02-21 05:41:30 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.3902
2022-02-21 05:42:02 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.4368
2022-02-21 05:42:34 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.2748
2022-02-21 05:43:06 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.1128
2022-02-21 05:43:38 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.3602
2022-02-21 05:44:10 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.2402
2022-02-21 05:44:42 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.3053
2022-02-21 05:45:15 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.1841
2022-02-21 05:45:47 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.2459
2022-02-21 05:46:19 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.0493
2022-02-21 05:46:52 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.1365
2022-02-21 05:47:24 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.4457
2022-02-21 05:47:57 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.2869
2022-02-21 05:48:30 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.2317
2022-02-21 05:49:03 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.1574
2022-02-21 05:49:37 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.1323
2022-02-21 05:49:39 - train: epoch 039, train_loss: 1.2399
2022-02-21 05:50:55 - eval: epoch: 039, acc1: 72.324%, acc5: 91.208%, test_loss: 1.0861, per_image_load_time: 2.136ms, per_image_inference_time: 0.716ms
2022-02-21 05:50:55 - until epoch: 039, best_acc1: 73.016%
2022-02-21 05:50:55 - epoch 040 lr: 0.010000000000000002
2022-02-21 05:51:34 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.3465
2022-02-21 05:52:06 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.4733
2022-02-21 05:52:38 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.3651
2022-02-21 05:53:10 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.2088
2022-02-21 05:53:42 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.1057
2022-02-21 05:54:14 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.3240
2022-02-21 05:54:46 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.2896
2022-02-21 05:55:18 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.3358
2022-02-21 05:55:50 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.1016
2022-02-21 05:56:22 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 0.9584
2022-02-21 05:56:55 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.1792
2022-02-21 05:57:27 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.1572
2022-02-21 05:57:59 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.0883
2022-02-21 05:58:31 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.1521
2022-02-21 05:59:03 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.4206
2022-02-21 05:59:35 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.2364
2022-02-21 06:00:08 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.2591
2022-02-21 06:00:40 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.0985
2022-02-21 06:01:12 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.1605
2022-02-21 06:01:44 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.3210
2022-02-21 06:02:16 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.0952
2022-02-21 06:02:49 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.0588
2022-02-21 06:03:21 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.1161
2022-02-21 06:03:53 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.2261
2022-02-21 06:04:25 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.3289
2022-02-21 06:04:57 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.1667
2022-02-21 06:05:30 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.2830
2022-02-21 06:06:02 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.2762
2022-02-21 06:06:34 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.4418
2022-02-21 06:07:06 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.3958
2022-02-21 06:07:38 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.1120
2022-02-21 06:08:11 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.3031
2022-02-21 06:08:43 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.2560
2022-02-21 06:09:15 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.2509
2022-02-21 06:09:47 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.4140
2022-02-21 06:10:19 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.2836
2022-02-21 06:10:51 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.2466
2022-02-21 06:11:23 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.0828
2022-02-21 06:11:55 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.2986
2022-02-21 06:12:28 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.3475
2022-02-21 06:13:00 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.2078
2022-02-21 06:13:32 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.2040
2022-02-21 06:14:05 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.3111
2022-02-21 06:14:37 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.2414
2022-02-21 06:15:09 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.0203
2022-02-21 06:15:42 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.1905
2022-02-21 06:16:15 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.1850
2022-02-21 06:16:48 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.1287
2022-02-21 06:17:21 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.3737
2022-02-21 06:17:54 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.3464
2022-02-21 06:17:57 - train: epoch 040, train_loss: 1.2384
2022-02-21 06:19:12 - eval: epoch: 040, acc1: 72.118%, acc5: 91.088%, test_loss: 1.1038, per_image_load_time: 2.115ms, per_image_inference_time: 0.719ms
2022-02-21 06:19:13 - until epoch: 040, best_acc1: 73.016%
2022-02-21 06:19:13 - epoch 041 lr: 0.010000000000000002
2022-02-21 06:19:50 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.3233
2022-02-21 06:20:22 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.3152
2022-02-21 06:20:53 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.1764
2022-02-21 06:21:25 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.1691
2022-02-21 06:21:57 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.0982
2022-02-21 06:22:29 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.2503
2022-02-21 06:23:00 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.1207
2022-02-21 06:23:32 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 0.9738
2022-02-21 06:24:05 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.0094
2022-02-21 06:24:37 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.3937
2022-02-21 06:25:09 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.1265
2022-02-21 06:25:41 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.0417
2022-02-21 06:26:13 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.2216
2022-02-21 06:26:45 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.4335
2022-02-21 06:27:17 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.4487
2022-02-21 06:27:49 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.0349
2022-02-21 06:28:21 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.2546
2022-02-21 06:28:54 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.2545
2022-02-21 06:29:26 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.2386
2022-02-21 06:29:58 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.1879
2022-02-21 06:30:31 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.1318
2022-02-21 06:31:03 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.1034
2022-02-21 06:31:35 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.0043
2022-02-21 06:32:07 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.4326
2022-02-21 06:32:39 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.1768
2022-02-21 06:33:11 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.4496
2022-02-21 06:33:43 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.5177
2022-02-21 06:34:16 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.2537
2022-02-21 06:34:48 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.3212
2022-02-21 06:35:20 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.3166
2022-02-21 06:35:52 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.3741
2022-02-21 06:36:25 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.0474
2022-02-21 06:36:57 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.1098
2022-02-21 06:37:29 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.0892
2022-02-21 06:38:02 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.2848
2022-02-21 06:38:34 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.3611
2022-02-21 06:39:06 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.2228
2022-02-21 06:39:38 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.1758
2022-02-21 06:40:10 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.2664
2022-02-21 06:40:43 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.1570
2022-02-21 06:41:15 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.2037
2022-02-21 06:41:47 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.1568
2022-02-21 06:42:20 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.2692
2022-02-21 06:42:52 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.1408
2022-02-21 06:43:25 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.2834
2022-02-21 06:43:57 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.2746
2022-02-21 06:44:30 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.4608
2022-02-21 06:45:03 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.2822
2022-02-21 06:45:37 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.1974
2022-02-21 06:46:10 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.3171
2022-02-21 06:46:13 - train: epoch 041, train_loss: 1.2392
2022-02-21 06:47:28 - eval: epoch: 041, acc1: 72.686%, acc5: 91.196%, test_loss: 1.0809, per_image_load_time: 2.175ms, per_image_inference_time: 0.714ms
2022-02-21 06:47:29 - until epoch: 041, best_acc1: 73.016%
2022-02-21 06:47:29 - epoch 042 lr: 0.010000000000000002
2022-02-21 06:48:07 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 0.9923
2022-02-21 06:48:38 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.2311
2022-02-21 06:49:10 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.2871
2022-02-21 06:49:41 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.0323
2022-02-21 06:50:13 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.3508
2022-02-21 06:50:45 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.0755
2022-02-21 06:51:17 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.1728
2022-02-21 06:51:49 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.1899
2022-02-21 06:52:21 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.4076
2022-02-21 06:52:53 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.2677
2022-02-21 06:53:25 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.0831
2022-02-21 06:53:57 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.2183
2022-02-21 06:54:30 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.2848
2022-02-21 06:55:02 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.4558
2022-02-21 06:55:34 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.1795
2022-02-21 06:56:06 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.3263
2022-02-21 06:56:39 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.2316
2022-02-21 06:57:11 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.2108
2022-02-21 06:57:43 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.3535
2022-02-21 06:58:15 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.0877
2022-02-21 06:58:47 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.0504
2022-02-21 06:59:20 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.1196
2022-02-21 06:59:52 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.2535
2022-02-21 07:00:24 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.4754
2022-02-21 07:00:56 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.3536
2022-02-21 07:01:29 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.3503
2022-02-21 07:02:01 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.1311
2022-02-21 07:02:34 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.1674
2022-02-21 07:03:06 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.1813
2022-02-21 07:03:39 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.2372
2022-02-21 07:04:11 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.1308
2022-02-21 07:04:43 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.2668
2022-02-21 07:05:15 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.3460
2022-02-21 07:05:47 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.1586
2022-02-21 07:06:20 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.1351
2022-02-21 07:06:52 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.3176
2022-02-21 07:07:24 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.2479
2022-02-21 07:07:56 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.0605
2022-02-21 07:08:29 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.2224
2022-02-21 07:09:01 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.3325
2022-02-21 07:09:33 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.3286
2022-02-21 07:10:05 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.3101
2022-02-21 07:10:38 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.0903
2022-02-21 07:11:10 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.2228
2022-02-21 07:11:42 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.2524
2022-02-21 07:12:15 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.4902
2022-02-21 07:12:48 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.1692
2022-02-21 07:13:21 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.3099
2022-02-21 07:13:55 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.3507
2022-02-21 07:14:28 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.2694
2022-02-21 07:14:31 - train: epoch 042, train_loss: 1.2388
2022-02-21 07:15:46 - eval: epoch: 042, acc1: 72.096%, acc5: 90.942%, test_loss: 1.1116, per_image_load_time: 2.149ms, per_image_inference_time: 0.712ms
2022-02-21 07:15:47 - until epoch: 042, best_acc1: 73.016%
2022-02-21 07:15:47 - epoch 043 lr: 0.010000000000000002
2022-02-21 07:16:24 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.2195
2022-02-21 07:16:56 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.3024
2022-02-21 07:17:27 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 0.9377
2022-02-21 07:17:59 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.1188
2022-02-21 07:18:31 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.2432
2022-02-21 07:19:03 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.1493
2022-02-21 07:19:35 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.4448
2022-02-21 07:20:07 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.3967
2022-02-21 07:20:39 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.2131
2022-02-21 07:21:11 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.3126
2022-02-21 07:21:43 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.2930
2022-02-21 07:22:15 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.4052
2022-02-21 07:22:48 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.2932
2022-02-21 07:23:20 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.1819
2022-02-21 07:23:52 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.0761
2022-02-21 07:24:25 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.2351
2022-02-21 07:24:57 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.2750
2022-02-21 07:25:29 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.2344
2022-02-21 07:26:01 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.2918
2022-02-21 07:26:33 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.0149
2022-02-21 07:27:05 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.2460
2022-02-21 07:27:36 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.3046
2022-02-21 07:28:08 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.4121
2022-02-21 07:28:40 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.0739
2022-02-21 07:29:12 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.4372
2022-02-21 07:29:44 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.0305
2022-02-21 07:30:15 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.2931
2022-02-21 07:30:47 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.1531
2022-02-21 07:31:19 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.2889
2022-02-21 07:31:51 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.4957
2022-02-21 07:32:23 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.3917
2022-02-21 07:32:55 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.3667
2022-02-21 07:33:27 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.2844
2022-02-21 07:33:59 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.2614
2022-02-21 07:34:31 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.2453
2022-02-21 07:35:03 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.2881
2022-02-21 07:35:35 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.2362
2022-02-21 07:36:07 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.2602
2022-02-21 07:36:39 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.3465
2022-02-21 07:37:11 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.3593
2022-02-21 07:37:43 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.5246
2022-02-21 07:38:15 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.2833
2022-02-21 07:38:47 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.2291
2022-02-21 07:39:19 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.1803
2022-02-21 07:39:51 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.2534
2022-02-21 07:40:24 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.2644
2022-02-21 07:40:56 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.2045
2022-02-21 07:41:29 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.2327
2022-02-21 07:42:02 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.2032
2022-02-21 07:42:35 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.2225
2022-02-21 07:42:38 - train: epoch 043, train_loss: 1.2365
2022-02-21 07:43:53 - eval: epoch: 043, acc1: 72.232%, acc5: 91.120%, test_loss: 1.0941, per_image_load_time: 1.828ms, per_image_inference_time: 0.717ms
2022-02-21 07:43:54 - until epoch: 043, best_acc1: 73.016%
2022-02-21 07:43:54 - epoch 044 lr: 0.010000000000000002
2022-02-21 07:44:32 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.2538
2022-02-21 07:45:03 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.2604
2022-02-21 07:45:35 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.3080
2022-02-21 07:46:06 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 0.9987
2022-02-21 07:46:38 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.1517
2022-02-21 07:47:10 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.0262
2022-02-21 07:47:42 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.0073
2022-02-21 07:48:14 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.1504
2022-02-21 07:48:46 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.2237
2022-02-21 07:49:18 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.2955
2022-02-21 07:49:50 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.2071
2022-02-21 07:50:22 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.1479
2022-02-21 07:50:54 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.2853
2022-02-21 07:51:26 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.0761
2022-02-21 07:51:58 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.1433
2022-02-21 07:52:30 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.1229
2022-02-21 07:53:02 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.2073
2022-02-21 07:53:33 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.1198
2022-02-21 07:54:05 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.3526
2022-02-21 07:54:37 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.2426
2022-02-21 07:55:09 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.4285
2022-02-21 07:55:40 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.1498
2022-02-21 07:56:12 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.3616
2022-02-21 07:56:44 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.3189
2022-02-21 07:57:17 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.3509
2022-02-21 07:57:49 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.2493
2022-02-21 07:58:21 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.3442
2022-02-21 07:58:53 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 0.9713
2022-02-21 07:59:25 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.0392
2022-02-21 07:59:57 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.1480
2022-02-21 08:00:29 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.2809
2022-02-21 08:01:01 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.0104
2022-02-21 08:01:33 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.1664
2022-02-21 08:02:05 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.4360
2022-02-21 08:02:37 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.1316
2022-02-21 08:03:09 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.4136
2022-02-21 08:03:41 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.4141
2022-02-21 08:04:13 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.0208
2022-02-21 08:04:46 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.3752
2022-02-21 08:05:18 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.4447
2022-02-21 08:05:50 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.1369
2022-02-21 08:06:23 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.1661
2022-02-21 08:06:55 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.5040
2022-02-21 08:07:27 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.1548
2022-02-21 08:08:00 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.2724
2022-02-21 08:08:32 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.2046
2022-02-21 08:09:05 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.3634
2022-02-21 08:09:38 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.4566
2022-02-21 08:10:12 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 0.9841
2022-02-21 08:10:45 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.2157
2022-02-21 08:10:48 - train: epoch 044, train_loss: 1.2382
2022-02-21 08:12:03 - eval: epoch: 044, acc1: 71.140%, acc5: 90.622%, test_loss: 1.1466, per_image_load_time: 2.138ms, per_image_inference_time: 0.736ms
2022-02-21 08:12:04 - until epoch: 044, best_acc1: 73.016%
2022-02-21 08:12:04 - epoch 045 lr: 0.010000000000000002
2022-02-21 08:12:41 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.0689
2022-02-21 08:13:13 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.2894
2022-02-21 08:13:44 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.3338
2022-02-21 08:14:16 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.1348
2022-02-21 08:14:48 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.3276
2022-02-21 08:15:20 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.2745
2022-02-21 08:15:52 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.0555
2022-02-21 08:16:25 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.1906
2022-02-21 08:16:57 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.1946
2022-02-21 08:17:29 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.2088
2022-02-21 08:18:01 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.3330
2022-02-21 08:18:33 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.2710
2022-02-21 08:19:05 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.4327
2022-02-21 08:19:37 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.1758
2022-02-21 08:20:09 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.3700
2022-02-21 08:20:41 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.1814
2022-02-21 08:21:13 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.0747
2022-02-21 08:21:45 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.0833
2022-02-21 08:22:18 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.2620
2022-02-21 08:22:50 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.2393
2022-02-21 08:23:22 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.3720
2022-02-21 08:23:54 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.3355
2022-02-21 08:24:26 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.1841
2022-02-21 08:24:58 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.3862
2022-02-21 08:25:30 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.3654
2022-02-21 08:26:03 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.2156
2022-02-21 08:26:35 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.1457
2022-02-21 08:27:07 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.1908
2022-02-21 08:27:39 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.2899
2022-02-21 08:28:11 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.4332
2022-02-21 08:28:44 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.2758
2022-02-21 08:29:16 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.2997
2022-02-21 08:29:48 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.2428
2022-02-21 08:30:20 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 0.9902
2022-02-21 08:30:53 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.4595
2022-02-21 08:31:25 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.3280
2022-02-21 08:31:57 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.1403
2022-02-21 08:32:29 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.1128
2022-02-21 08:33:01 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.3548
2022-02-21 08:33:33 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.4220
2022-02-21 08:34:06 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.2334
2022-02-21 08:34:38 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.4076
2022-02-21 08:35:10 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.4311
2022-02-21 08:35:42 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.3316
2022-02-21 08:36:15 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.1945
2022-02-21 08:36:47 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.3532
2022-02-21 08:37:20 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.3075
2022-02-21 08:37:53 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.2331
2022-02-21 08:38:26 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.1941
2022-02-21 08:39:00 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.2573
2022-02-21 08:39:03 - train: epoch 045, train_loss: 1.2362
2022-02-21 08:40:19 - eval: epoch: 045, acc1: 71.836%, acc5: 91.006%, test_loss: 1.1137, per_image_load_time: 2.208ms, per_image_inference_time: 0.704ms
2022-02-21 08:40:20 - until epoch: 045, best_acc1: 73.016%
2022-02-21 08:40:20 - epoch 046 lr: 0.010000000000000002
2022-02-21 08:40:57 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.0137
2022-02-21 08:41:29 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.1393
2022-02-21 08:42:01 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.2287
2022-02-21 08:42:32 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.3860
2022-02-21 08:43:05 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 0.9806
2022-02-21 08:43:37 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.3728
2022-02-21 08:44:09 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.1362
2022-02-21 08:44:42 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.2466
2022-02-21 08:45:14 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.2905
2022-02-21 08:45:46 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.0452
2022-02-21 08:46:18 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.1920
2022-02-21 08:46:51 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.3097
2022-02-21 08:47:23 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.2426
2022-02-21 08:47:55 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.3243
2022-02-21 08:48:28 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.2091
2022-02-21 08:49:00 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.2392
2022-02-21 08:49:32 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.0701
2022-02-21 08:50:04 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.2637
2022-02-21 08:50:37 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.0765
2022-02-21 08:51:09 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.1527
2022-02-21 08:51:41 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.4259
2022-02-21 08:52:13 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.1000
2022-02-21 08:52:45 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.3751
2022-02-21 08:53:18 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.2261
2022-02-21 08:53:50 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.3153
2022-02-21 08:54:22 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.3692
2022-02-21 08:54:54 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.1073
2022-02-21 08:55:26 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.0459
2022-02-21 08:55:58 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.2651
2022-02-21 08:56:31 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.0402
2022-02-21 08:57:03 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.0754
2022-02-21 08:57:35 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.5181
2022-02-21 08:58:07 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.2553
2022-02-21 08:58:40 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.2723
2022-02-21 08:59:12 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.1168
2022-02-21 08:59:44 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.1234
2022-02-21 09:00:16 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.0998
2022-02-21 09:00:48 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.3371
2022-02-21 09:01:20 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.3089
2022-02-21 09:01:53 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.1612
2022-02-21 09:02:25 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.3619
2022-02-21 09:02:57 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.2230
2022-02-21 09:03:30 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.4083
2022-02-21 09:04:02 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.2192
2022-02-21 09:04:34 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.1217
2022-02-21 09:05:07 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.3505
2022-02-21 09:05:40 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.2040
2022-02-21 09:06:14 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.2093
2022-02-21 09:06:47 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.3171
2022-02-21 09:07:20 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.2289
2022-02-21 09:07:23 - train: epoch 046, train_loss: 1.2353
2022-02-21 09:08:39 - eval: epoch: 046, acc1: 72.032%, acc5: 91.068%, test_loss: 1.1030, per_image_load_time: 2.036ms, per_image_inference_time: 0.712ms
2022-02-21 09:08:40 - until epoch: 046, best_acc1: 73.016%
2022-02-21 09:08:40 - epoch 047 lr: 0.010000000000000002
2022-02-21 09:09:17 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.1245
2022-02-21 09:09:49 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.3796
2022-02-21 09:10:20 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.1190
2022-02-21 09:10:52 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.1560
2022-02-21 09:11:24 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.3241
2022-02-21 09:11:56 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.2625
2022-02-21 09:12:27 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.3877
2022-02-21 09:12:59 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.0453
2022-02-21 09:13:31 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.3002
2022-02-21 09:14:03 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.2873
2022-02-21 09:14:35 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.3154
2022-02-21 09:15:07 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.1896
2022-02-21 09:15:39 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.2284
2022-02-21 09:16:11 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.2258
2022-02-21 09:16:43 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.1731
2022-02-21 09:17:15 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.1050
2022-02-21 09:17:47 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.0221
2022-02-21 09:18:19 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.2157
2022-02-21 09:18:51 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.1028
2022-02-21 09:19:23 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.2196
2022-02-21 09:19:55 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.3469
2022-02-21 09:20:27 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.4607
2022-02-21 09:20:59 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.2169
2022-02-21 09:21:31 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.0410
2022-02-21 09:22:03 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.3027
2022-02-21 09:22:35 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.4632
2022-02-21 09:23:07 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.2631
2022-02-21 09:23:39 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.3139
2022-02-21 09:24:11 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.2220
2022-02-21 09:24:44 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.2681
2022-02-21 09:25:16 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.1504
2022-02-21 09:25:48 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.4341
2022-02-21 09:26:20 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.1971
2022-02-21 09:26:52 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.2795
2022-02-21 09:27:24 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.4907
2022-02-21 09:27:56 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.3230
2022-02-21 09:28:29 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.3653
2022-02-21 09:29:01 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.2527
2022-02-21 09:29:33 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.1565
2022-02-21 09:30:06 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.2434
2022-02-21 09:30:38 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.2556
2022-02-21 09:31:11 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.1917
2022-02-21 09:31:43 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.0536
2022-02-21 09:32:16 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.2276
2022-02-21 09:32:48 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.4258
2022-02-21 09:33:21 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.2197
2022-02-21 09:33:54 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.3616
2022-02-21 09:34:27 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.1843
2022-02-21 09:35:01 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.3535
2022-02-21 09:35:34 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.1518
2022-02-21 09:35:37 - train: epoch 047, train_loss: 1.2346
2022-02-21 09:36:52 - eval: epoch: 047, acc1: 71.200%, acc5: 90.504%, test_loss: 1.1419, per_image_load_time: 1.913ms, per_image_inference_time: 0.759ms
2022-02-21 09:36:53 - until epoch: 047, best_acc1: 73.016%
2022-02-21 09:36:53 - epoch 048 lr: 0.010000000000000002
2022-02-21 09:37:32 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.3757
2022-02-21 09:38:04 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.5505
2022-02-21 09:38:36 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.1506
2022-02-21 09:39:08 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.2446
2022-02-21 09:39:40 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.1760
2022-02-21 09:40:12 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.2413
2022-02-21 09:40:44 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.1689
2022-02-21 09:41:16 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.2975
2022-02-21 09:41:48 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.3752
2022-02-21 09:42:20 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.2278
2022-02-21 09:42:52 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.1938
2022-02-21 09:43:24 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.2752
2022-02-21 09:43:56 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.1505
2022-02-21 09:44:28 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.1448
2022-02-21 09:45:00 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.2456
2022-02-21 09:45:32 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.1991
2022-02-21 09:46:05 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.2667
2022-02-21 09:46:37 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.2840
2022-02-21 09:47:08 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.2597
2022-02-21 09:47:41 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.4510
2022-02-21 09:48:13 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.2577
2022-02-21 09:48:45 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.4243
2022-02-21 09:49:17 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.1036
2022-02-21 09:49:49 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.3586
2022-02-21 09:50:21 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.2129
2022-02-21 09:50:53 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.4399
2022-02-21 09:51:25 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.4206
2022-02-21 09:51:57 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.1333
2022-02-21 09:52:29 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.3136
2022-02-21 09:53:01 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.2269
2022-02-21 09:53:33 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.1391
2022-02-21 09:54:05 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.0923
2022-02-21 09:54:37 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.4259
2022-02-21 09:55:09 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.2081
2022-02-21 09:55:41 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.4794
2022-02-21 09:56:13 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.2502
2022-02-21 09:56:45 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.3009
2022-02-21 09:57:17 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.2380
2022-02-21 09:57:49 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.4429
2022-02-21 09:58:22 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.0823
2022-02-21 09:58:54 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.4822
2022-02-21 09:59:26 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.2349
2022-02-21 09:59:58 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.2269
2022-02-21 10:00:30 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.1457
2022-02-21 10:01:03 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.3669
2022-02-21 10:01:35 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.1387
2022-02-21 10:02:08 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.2582
2022-02-21 10:02:41 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.5305
2022-02-21 10:03:14 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.3555
2022-02-21 10:03:47 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.3197
2022-02-21 10:03:50 - train: epoch 048, train_loss: 1.2359
2022-02-21 10:05:05 - eval: epoch: 048, acc1: 72.086%, acc5: 90.984%, test_loss: 1.1070, per_image_load_time: 2.058ms, per_image_inference_time: 0.707ms
2022-02-21 10:05:06 - until epoch: 048, best_acc1: 73.016%
2022-02-21 10:05:06 - epoch 049 lr: 0.010000000000000002
2022-02-21 10:05:44 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.3861
2022-02-21 10:06:16 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.2237
2022-02-21 10:06:48 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.2419
2022-02-21 10:07:20 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.2482
2022-02-21 10:07:52 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.1579
2022-02-21 10:08:24 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.1597
2022-02-21 10:08:56 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.3039
2022-02-21 10:09:28 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.4273
2022-02-21 10:10:00 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.0585
2022-02-21 10:10:32 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.1689
2022-02-21 10:11:04 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.1914
2022-02-21 10:11:36 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.0104
2022-02-21 10:12:07 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.2965
2022-02-21 10:12:39 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.4535
2022-02-21 10:13:12 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.2847
2022-02-21 10:13:44 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.2919
2022-02-21 10:14:16 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.2963
2022-02-21 10:14:48 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.1384
2022-02-21 10:15:19 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.1184
2022-02-21 10:15:51 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.0195
2022-02-21 10:16:24 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.1875
2022-02-21 10:16:56 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.2647
2022-02-21 10:17:28 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.1751
2022-02-21 10:18:00 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.2941
2022-02-21 10:18:32 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.2558
2022-02-21 10:19:05 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.2572
2022-02-21 10:19:37 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.1747
2022-02-21 10:20:09 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.1723
2022-02-21 10:20:41 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.3498
2022-02-21 10:21:12 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.2131
2022-02-21 10:21:45 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.2857
2022-02-21 10:22:16 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.3158
2022-02-21 10:22:48 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.2890
2022-02-21 10:23:21 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.3811
2022-02-21 10:23:52 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.2978
2022-02-21 10:24:25 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.3510
2022-02-21 10:24:57 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.2306
2022-02-21 10:25:29 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.3677
2022-02-21 10:26:01 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.3674
2022-02-21 10:26:33 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.2567
2022-02-21 10:27:05 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.1721
2022-02-21 10:27:38 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.3430
2022-02-21 10:28:10 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.5062
2022-02-21 10:28:42 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.2106
2022-02-21 10:29:15 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.1724
2022-02-21 10:29:47 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.2705
2022-02-21 10:30:20 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.3416
2022-02-21 10:30:53 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.0245
2022-02-21 10:31:26 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.1125
2022-02-21 10:31:59 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.2155
2022-02-21 10:32:02 - train: epoch 049, train_loss: 1.2308
2022-02-21 10:33:18 - eval: epoch: 049, acc1: 71.542%, acc5: 90.804%, test_loss: 1.1227, per_image_load_time: 1.379ms, per_image_inference_time: 0.742ms
2022-02-21 10:33:18 - until epoch: 049, best_acc1: 73.016%
2022-02-21 10:33:18 - epoch 050 lr: 0.010000000000000002
2022-02-21 10:33:57 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.3466
2022-02-21 10:34:29 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.1348
2022-02-21 10:35:01 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.2376
2022-02-21 10:35:33 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.0814
2022-02-21 10:36:05 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.2525
2022-02-21 10:36:37 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.3241
2022-02-21 10:37:09 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.0869
2022-02-21 10:37:40 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 0.9940
2022-02-21 10:38:13 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.0337
2022-02-21 10:38:45 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.3267
2022-02-21 10:39:18 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.2700
2022-02-21 10:39:50 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.2296
2022-02-21 10:40:22 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.0475
2022-02-21 10:40:54 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.2484
2022-02-21 10:41:26 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.2377
2022-02-21 10:42:06 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.1378
2022-02-21 10:42:43 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.2658
2022-02-21 10:43:16 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.1560
2022-02-21 10:43:49 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.2705
2022-02-21 10:44:20 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.2118
2022-02-21 10:44:51 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.0039
2022-02-21 10:45:23 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.3314
2022-02-21 10:45:55 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.1445
2022-02-21 10:46:27 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.1348
2022-02-21 10:46:59 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.2977
2022-02-21 10:47:32 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.1600
2022-02-21 10:48:04 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.2841
2022-02-21 10:48:37 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.3364
2022-02-21 10:49:09 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.4355
2022-02-21 10:49:41 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.3381
2022-02-21 10:50:13 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.1635
2022-02-21 10:50:44 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.2151
2022-02-21 10:51:17 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.1255
2022-02-21 10:52:52 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.1139
2022-02-21 10:56:19 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.2070
2022-02-21 10:57:55 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.2071
2022-02-21 10:58:58 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.2943
2022-02-21 11:00:19 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.0996
2022-02-21 11:01:04 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.1210
2022-02-21 11:01:50 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.3472
2022-02-21 11:02:52 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.1289
2022-02-21 11:03:25 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.2757
2022-02-21 11:03:57 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.2144
2022-02-21 11:04:30 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.1223
2022-02-21 11:05:02 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.1833
2022-02-21 11:05:34 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.2107
2022-02-21 11:06:07 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.2181
2022-02-21 11:06:39 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.1068
2022-02-21 11:07:12 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.1188
2022-02-21 11:07:44 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.1773
2022-02-21 11:07:47 - train: epoch 050, train_loss: 1.2285
2022-02-21 11:09:02 - eval: epoch: 050, acc1: 72.270%, acc5: 91.176%, test_loss: 1.0998, per_image_load_time: 1.024ms, per_image_inference_time: 0.741ms
2022-02-21 11:09:03 - until epoch: 050, best_acc1: 73.016%
2022-02-21 11:09:03 - epoch 051 lr: 0.010000000000000002
2022-02-21 11:09:40 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.3140
2022-02-21 11:10:12 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.4723
2022-02-21 11:10:45 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.2363
2022-02-21 11:11:17 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 0.9979
2022-02-21 11:11:48 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.3099
2022-02-21 11:12:19 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.2364
2022-02-21 11:12:51 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.1614
2022-02-21 11:13:22 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.4474
2022-02-21 11:13:54 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.0970
2022-02-21 11:14:26 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.4984
2022-02-21 11:14:58 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.2985
2022-02-21 11:15:30 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.1553
2022-02-21 11:16:02 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 0.9563
2022-02-21 11:16:34 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.1336
2022-02-21 11:17:06 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.2105
2022-02-21 11:17:37 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.1481
2022-02-21 11:18:09 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.3498
2022-02-21 11:18:42 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.2400
2022-02-21 11:19:14 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.0336
2022-02-21 11:19:46 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.2400
2022-02-21 11:20:18 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.2522
2022-02-21 11:20:50 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.2010
2022-02-21 11:21:22 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.2226
2022-02-21 11:21:54 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.2652
2022-02-21 11:22:26 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.2269
2022-02-21 11:22:57 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.2097
2022-02-21 11:23:30 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.3050
2022-02-21 11:24:02 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.1237
2022-02-21 11:24:34 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.2379
2022-02-21 11:25:06 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.1704
2022-02-21 11:25:38 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.1478
2022-02-21 11:26:10 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.1790
2022-02-21 11:26:42 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.3754
2022-02-21 11:27:14 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.2433
2022-02-21 11:27:45 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.2203
2022-02-21 11:28:18 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.2097
2022-02-21 11:28:50 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.3679
