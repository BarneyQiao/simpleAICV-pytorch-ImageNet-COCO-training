2022-02-24 08:02:15 - network: darknet19
2022-02-24 08:02:15 - num_classes: 1000
2022-02-24 08:02:15 - input_image_size: 256
2022-02-24 08:02:15 - scale: 1.1428571428571428
2022-02-24 08:02:15 - trained_model_path: 
2022-02-24 08:02:15 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-24 08:02:15 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f74f9be3d60>
2022-02-24 08:02:15 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f74d9bba070>
2022-02-24 08:02:15 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f74d9bba0a0>
2022-02-24 08:02:15 - seed: 0
2022-02-24 08:02:15 - batch_size: 256
2022-02-24 08:02:15 - num_workers: 16
2022-02-24 08:02:15 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-24 08:02:15 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-24 08:02:15 - epochs: 100
2022-02-24 08:02:15 - print_interval: 100
2022-02-24 08:02:15 - distributed: True
2022-02-24 08:02:15 - sync_bn: False
2022-02-24 08:02:15 - apex: True
2022-02-24 08:02:15 - gpus_type: NVIDIA GeForce RTX 3090
2022-02-24 08:02:15 - gpus_num: 2
2022-02-24 08:02:15 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f74d7f47130>
2022-02-24 08:02:19 - --------------------parameters--------------------
2022-02-24 08:02:19 - name: layer1.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer1.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer1.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer2.Darknet19Block.0.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer2.Darknet19Block.0.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer2.Darknet19Block.0.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer3.Darknet19Block.0.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer3.Darknet19Block.0.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer3.Darknet19Block.0.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer3.Darknet19Block.1.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer3.Darknet19Block.1.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer3.Darknet19Block.1.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer3.Darknet19Block.2.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer3.Darknet19Block.2.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer3.Darknet19Block.2.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer4.Darknet19Block.0.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer4.Darknet19Block.0.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer4.Darknet19Block.0.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer4.Darknet19Block.1.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer4.Darknet19Block.1.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer4.Darknet19Block.1.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer4.Darknet19Block.2.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer4.Darknet19Block.2.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer4.Darknet19Block.2.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.0.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.0.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.0.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.1.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.1.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.1.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.2.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.2.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.2.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.3.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.3.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.3.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.4.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.4.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer5.Darknet19Block.4.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.0.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.0.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.0.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.1.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.1.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.1.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.2.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.2.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.2.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.3.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.3.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.3.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.4.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.4.layer.1.weight, grad: True
2022-02-24 08:02:19 - name: layer6.Darknet19Block.4.layer.1.bias, grad: True
2022-02-24 08:02:19 - name: layer7.layer.0.weight, grad: True
2022-02-24 08:02:19 - name: layer7.layer.0.bias, grad: True
2022-02-24 08:02:19 - --------------------buffers--------------------
2022-02-24 08:02:19 - name: layer1.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer1.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer2.Darknet19Block.0.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer2.Darknet19Block.0.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer2.Darknet19Block.0.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer3.Darknet19Block.0.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer3.Darknet19Block.0.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer3.Darknet19Block.0.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer3.Darknet19Block.1.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer3.Darknet19Block.1.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer3.Darknet19Block.1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer3.Darknet19Block.2.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer3.Darknet19Block.2.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer3.Darknet19Block.2.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer4.Darknet19Block.0.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer4.Darknet19Block.0.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer4.Darknet19Block.0.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer4.Darknet19Block.1.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer4.Darknet19Block.1.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer4.Darknet19Block.1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer4.Darknet19Block.2.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer4.Darknet19Block.2.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer4.Darknet19Block.2.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.0.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.0.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.0.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.1.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.1.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.2.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.2.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.2.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.3.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.3.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.3.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.4.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.4.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer5.Darknet19Block.4.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.0.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.0.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.0.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.1.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.1.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.2.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.2.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.2.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.3.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.3.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.3.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.4.layer.1.running_mean, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.4.layer.1.running_var, grad: False
2022-02-24 08:02:19 - name: layer6.Darknet19Block.4.layer.1.num_batches_tracked, grad: False
2022-02-24 08:02:19 - epoch 001 lr: 0.1
2022-02-24 08:02:59 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8332
2022-02-24 08:03:32 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.4503
2022-02-24 08:04:05 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.2649
2022-02-24 08:04:40 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.1415
2022-02-24 08:05:14 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.0027
2022-02-24 08:05:46 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 5.7439
2022-02-24 08:06:20 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 5.7145
2022-02-24 08:06:54 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 5.6694
2022-02-24 08:07:27 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.5670
2022-02-24 08:08:01 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.5029
2022-02-24 08:08:35 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.4979
2022-02-24 08:09:09 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.2740
2022-02-24 08:09:43 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.1596
2022-02-24 08:10:16 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.2171
2022-02-24 08:10:50 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.0372
2022-02-24 08:11:23 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.1015
2022-02-24 08:11:57 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 4.9593
2022-02-24 08:12:30 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.0432
2022-02-24 08:13:04 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 4.8249
2022-02-24 08:13:38 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 4.7907
2022-02-24 08:14:12 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 4.7098
2022-02-24 08:14:46 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 4.7857
2022-02-24 08:15:19 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.5415
2022-02-24 08:15:53 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.6899
2022-02-24 08:16:26 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.5866
2022-02-24 08:17:00 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 4.7757
2022-02-24 08:17:33 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 4.7653
2022-02-24 08:18:07 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.3639
2022-02-24 08:18:41 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.3473
2022-02-24 08:19:15 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.4688
2022-02-24 08:19:49 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.6327
2022-02-24 08:20:23 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.3636
2022-02-24 08:20:56 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.1513
2022-02-24 08:21:30 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.1738
2022-02-24 08:22:04 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.2236
2022-02-24 08:22:38 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.3105
2022-02-24 08:23:10 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.3475
2022-02-24 08:23:44 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.0527
2022-02-24 08:24:18 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.2375
2022-02-24 08:24:52 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.0428
2022-02-24 08:25:25 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.1901
2022-02-24 08:26:00 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.0034
2022-02-24 08:26:33 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.0170
2022-02-24 08:27:08 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 3.8306
2022-02-24 08:27:40 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.0739
2022-02-24 08:28:16 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.2117
2022-02-24 08:28:49 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 3.9158
2022-02-24 08:29:23 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.2459
2022-02-24 08:29:57 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 3.8671
2022-02-24 08:30:31 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 3.8271
2022-02-24 08:30:31 - train: epoch 001, train_loss: 4.7840
2022-02-24 08:31:48 - eval: epoch: 001, acc1: 22.376%, acc5: 46.742%, test_loss: 3.7715, per_image_load_time: 0.946ms, per_image_inference_time: 0.287ms
2022-02-24 08:31:49 - until epoch: 001, best_acc1: 22.376%
2022-02-24 08:31:49 - epoch 002 lr: 0.1
2022-02-24 08:32:28 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 3.8957
2022-02-24 08:33:01 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 3.6973
2022-02-24 08:33:34 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 3.9137
2022-02-24 08:34:08 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 3.9160
2022-02-24 08:34:41 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.6458
2022-02-24 08:35:14 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 3.6720
2022-02-24 08:35:48 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 3.8775
2022-02-24 08:36:20 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.5581
2022-02-24 08:36:55 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.5350
2022-02-24 08:37:28 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 3.8996
2022-02-24 08:38:02 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 3.8610
2022-02-24 08:38:35 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 3.7209
2022-02-24 08:39:09 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.6805
2022-02-24 08:39:43 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 3.7263
2022-02-24 08:40:16 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 3.7922
2022-02-24 08:40:51 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.6165
2022-02-24 08:41:25 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.6915
2022-02-24 08:41:58 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.6285
2022-02-24 08:42:32 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.4828
2022-02-24 08:43:05 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.2373
2022-02-24 08:43:39 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.5259
2022-02-24 08:44:13 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.3228
2022-02-24 08:44:47 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.6025
2022-02-24 08:45:20 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.3830
2022-02-24 08:45:55 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.3312
2022-02-24 08:46:28 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.3497
2022-02-24 08:47:02 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.5884
2022-02-24 08:47:36 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.4669
2022-02-24 08:48:10 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.4580
2022-02-24 08:48:44 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.3048
2022-02-24 08:49:18 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.2882
2022-02-24 08:49:52 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.4763
2022-02-24 08:50:26 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.3459
2022-02-24 08:51:00 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.4967
2022-02-24 08:51:33 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.3643
2022-02-24 08:52:07 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.3691
2022-02-24 08:52:41 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.3975
2022-02-24 08:53:15 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.1441
2022-02-24 08:53:50 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.4207
2022-02-24 08:54:25 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.2542
2022-02-24 08:54:57 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.4569
2022-02-24 08:55:32 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.2199
2022-02-24 08:56:07 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.3191
2022-02-24 08:56:42 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.1852
2022-02-24 08:57:15 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.1384
2022-02-24 08:57:49 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.3080
2022-02-24 08:58:23 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.3251
2022-02-24 08:58:58 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.2829
2022-02-24 08:59:33 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.2169
2022-02-24 09:00:07 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.1232
2022-02-24 09:00:08 - train: epoch 002, train_loss: 3.5073
2022-02-24 09:01:26 - eval: epoch: 002, acc1: 33.294%, acc5: 60.250%, test_loss: 3.0622, per_image_load_time: 2.692ms, per_image_inference_time: 0.303ms
2022-02-24 09:01:27 - until epoch: 002, best_acc1: 33.294%
2022-02-24 09:01:27 - epoch 003 lr: 0.1
2022-02-24 09:02:05 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.3662
2022-02-24 09:02:38 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.2164
2022-02-24 09:03:11 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.1558
2022-02-24 09:03:44 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.3205
2022-02-24 09:04:18 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.2937
2022-02-24 09:04:52 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.1255
2022-02-24 09:05:24 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.3363
2022-02-24 09:05:57 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.3108
2022-02-24 09:06:31 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.1224
2022-02-24 09:07:04 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.1407
2022-02-24 09:07:39 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.1000
2022-02-24 09:08:12 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.0453
2022-02-24 09:08:46 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.1741
2022-02-24 09:09:19 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.0524
2022-02-24 09:09:52 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.3496
2022-02-24 09:10:27 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.0658
2022-02-24 09:11:00 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.0587
2022-02-24 09:11:34 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 2.9879
2022-02-24 09:12:08 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.1666
2022-02-24 09:12:40 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.4244
2022-02-24 09:13:15 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.3959
2022-02-24 09:13:48 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.5902
2022-02-24 09:14:21 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.0812
2022-02-24 09:14:55 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.0248
2022-02-24 09:15:30 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.1793
2022-02-24 09:16:03 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.1079
2022-02-24 09:16:37 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.4086
2022-02-24 09:17:11 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 2.9785
2022-02-24 09:17:45 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.0316
2022-02-24 09:18:18 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.3127
2022-02-24 09:18:53 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.2092
2022-02-24 09:19:27 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.0624
2022-02-24 09:20:01 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.1116
2022-02-24 09:20:34 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.1826
2022-02-24 09:21:09 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.9165
2022-02-24 09:21:43 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 2.9246
2022-02-24 09:22:17 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 2.9558
2022-02-24 09:22:51 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.1043
2022-02-24 09:23:25 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.3320
2022-02-24 09:23:59 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.0832
2022-02-24 09:24:33 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.1094
2022-02-24 09:25:07 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.9264
2022-02-24 09:25:42 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.6704
2022-02-24 09:26:15 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.9075
2022-02-24 09:26:50 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.0322
2022-02-24 09:27:24 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.0359
2022-02-24 09:27:59 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.8567
2022-02-24 09:28:34 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.0413
2022-02-24 09:29:08 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.1167
2022-02-24 09:29:43 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.0747
2022-02-24 09:29:44 - train: epoch 003, train_loss: 3.1016
2022-02-24 09:31:00 - eval: epoch: 003, acc1: 39.570%, acc5: 66.424%, test_loss: 2.7238, per_image_load_time: 0.858ms, per_image_inference_time: 0.303ms
2022-02-24 09:31:00 - until epoch: 003, best_acc1: 39.570%
2022-02-24 09:31:00 - epoch 004 lr: 0.1
2022-02-24 09:31:38 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 2.9697
2022-02-24 09:32:12 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.8971
2022-02-24 09:32:45 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.0540
2022-02-24 09:33:19 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.8625
2022-02-24 09:33:52 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.8001
2022-02-24 09:34:26 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.0964
2022-02-24 09:35:00 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 2.9958
2022-02-24 09:35:33 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.8109
2022-02-24 09:36:08 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.6226
2022-02-24 09:36:41 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.9076
2022-02-24 09:37:15 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.1088
2022-02-24 09:37:49 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.7130
2022-02-24 09:38:21 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.7613
2022-02-24 09:38:55 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.0149
2022-02-24 09:39:29 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 2.8590
2022-02-24 09:40:03 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.8190
2022-02-24 09:40:37 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.9302
2022-02-24 09:41:11 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.1098
2022-02-24 09:41:45 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 2.9639
2022-02-24 09:42:19 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.8918
2022-02-24 09:42:52 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.9697
2022-02-24 09:43:26 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.9403
2022-02-24 09:43:59 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.7744
2022-02-24 09:44:34 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.8507
2022-02-24 09:45:07 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.7959
2022-02-24 09:45:42 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.0906
2022-02-24 09:46:15 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.7854
2022-02-24 09:46:49 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.8934
2022-02-24 09:47:24 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.8449
2022-02-24 09:47:57 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.8871
2022-02-24 09:48:31 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.9210
2022-02-24 09:49:05 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.8658
2022-02-24 09:49:39 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.9279
2022-02-24 09:50:13 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.9524
2022-02-24 09:50:47 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.7952
2022-02-24 09:51:21 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.7121
2022-02-24 09:51:55 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.7773
2022-02-24 09:52:29 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.7644
2022-02-24 09:53:04 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.7603
2022-02-24 09:53:37 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.5605
2022-02-24 09:54:11 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.8806
2022-02-24 09:54:46 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.5334
2022-02-24 09:55:20 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.6942
2022-02-24 09:55:53 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.6732
2022-02-24 09:56:28 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.3538
2022-02-24 09:57:01 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.8505
2022-02-24 09:57:36 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.7775
2022-02-24 09:58:11 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.7470
2022-02-24 09:58:47 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.7998
2022-02-24 09:59:20 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.8632
2022-02-24 09:59:21 - train: epoch 004, train_loss: 2.8897
2022-02-24 10:00:38 - eval: epoch: 004, acc1: 42.286%, acc5: 69.266%, test_loss: 2.5767, per_image_load_time: 2.557ms, per_image_inference_time: 0.331ms
2022-02-24 10:00:38 - until epoch: 004, best_acc1: 42.286%
2022-02-24 10:00:38 - epoch 005 lr: 0.1
2022-02-24 10:01:17 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.8351
2022-02-24 10:01:51 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.7553
2022-02-24 10:02:24 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.0085
2022-02-24 10:02:56 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.7343
2022-02-24 10:03:30 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.5683
2022-02-24 10:04:02 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.7464
2022-02-24 10:04:36 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.7464
2022-02-24 10:05:09 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.0615
2022-02-24 10:05:43 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.7465
2022-02-24 10:06:16 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.9687
2022-02-24 10:06:50 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.8222
2022-02-24 10:07:23 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.7678
2022-02-24 10:07:57 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.7637
2022-02-24 10:08:30 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.7748
2022-02-24 10:09:04 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.6609
2022-02-24 10:09:38 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.6365
2022-02-24 10:10:12 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.6629
2022-02-24 10:10:45 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.7671
2022-02-24 10:11:18 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.6799
2022-02-24 10:11:52 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.8521
2022-02-24 10:12:26 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.5637
2022-02-24 10:13:00 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.6171
2022-02-24 10:13:33 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.5922
2022-02-24 10:14:06 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.7254
2022-02-24 10:14:39 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.9160
2022-02-24 10:15:14 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.9066
2022-02-24 10:15:47 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.8584
2022-02-24 10:16:21 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.8065
2022-02-24 10:16:55 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.6721
2022-02-24 10:17:30 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.6649
2022-02-24 10:18:04 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.7540
2022-02-24 10:18:38 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.8757
2022-02-24 10:19:12 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.6103
2022-02-24 10:19:46 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.6563
2022-02-24 10:20:19 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.8421
2022-02-24 10:20:54 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.8110
2022-02-24 10:21:27 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.6485
2022-02-24 10:22:01 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.5361
2022-02-24 10:22:35 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.9372
2022-02-24 10:23:09 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.8491
2022-02-24 10:23:42 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.6905
2022-02-24 10:24:17 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.8107
2022-02-24 10:24:50 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.6216
2022-02-24 10:25:25 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.8289
2022-02-24 10:25:58 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.7144
2022-02-24 10:26:33 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.6036
2022-02-24 10:27:08 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.4312
2022-02-24 10:27:43 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.5109
2022-02-24 10:28:17 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.8184
2022-02-24 10:28:51 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.6740
2022-02-24 10:28:52 - train: epoch 005, train_loss: 2.7584
2022-02-24 10:30:08 - eval: epoch: 005, acc1: 43.614%, acc5: 70.674%, test_loss: 2.4733, per_image_load_time: 2.650ms, per_image_inference_time: 0.296ms
2022-02-24 10:30:09 - until epoch: 005, best_acc1: 43.614%
2022-02-24 10:30:09 - epoch 006 lr: 0.1
2022-02-24 10:30:47 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.6240
2022-02-24 10:31:20 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.7102
2022-02-24 10:31:54 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.4683
2022-02-24 10:32:26 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.8036
2022-02-24 10:33:00 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.7172
2022-02-24 10:33:34 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.7691
2022-02-24 10:34:06 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.7085
2022-02-24 10:34:41 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.6189
2022-02-24 10:35:14 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.5420
2022-02-24 10:35:48 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.6433
2022-02-24 10:36:20 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.5929
2022-02-24 10:36:55 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.7830
2022-02-24 10:37:29 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.7845
2022-02-24 10:38:02 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.7827
2022-02-24 10:38:35 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.7401
2022-02-24 10:39:10 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.4998
2022-02-24 10:39:43 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.8013
2022-02-24 10:40:18 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.8047
2022-02-24 10:40:52 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.5796
2022-02-24 10:41:25 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.8484
2022-02-24 10:42:00 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.7121
2022-02-24 10:42:33 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.4870
2022-02-24 10:43:07 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.4750
2022-02-24 10:43:41 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.7065
2022-02-24 10:44:15 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.9241
2022-02-24 10:44:49 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.5984
2022-02-24 10:45:22 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.7939
2022-02-24 10:45:56 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.5164
2022-02-24 10:46:29 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.8443
2022-02-24 10:47:04 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.6137
2022-02-24 10:47:37 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.4854
2022-02-24 10:48:10 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.5720
2022-02-24 10:48:45 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.5670
2022-02-24 10:49:18 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.9378
2022-02-24 10:49:52 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.7829
2022-02-24 10:50:26 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.7314
2022-02-24 10:51:00 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.6464
2022-02-24 10:51:33 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.6248
2022-02-24 10:52:09 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.5800
2022-02-24 10:52:42 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.8427
2022-02-24 10:53:16 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.5026
2022-02-24 10:53:49 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.4395
2022-02-24 10:54:23 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.7029
2022-02-24 10:54:57 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.7270
2022-02-24 10:55:31 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.5893
2022-02-24 10:56:05 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.5714
2022-02-24 10:56:39 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.7138
2022-02-24 10:57:14 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.6860
2022-02-24 10:57:49 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.6469
2022-02-24 10:58:23 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.4575
2022-02-24 10:58:24 - train: epoch 006, train_loss: 2.6732
2022-02-24 10:59:39 - eval: epoch: 006, acc1: 45.140%, acc5: 71.604%, test_loss: 2.4216, per_image_load_time: 2.572ms, per_image_inference_time: 0.301ms
2022-02-24 10:59:39 - until epoch: 006, best_acc1: 45.140%
2022-02-24 10:59:39 - epoch 007 lr: 0.1
2022-02-24 11:00:17 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.5168
2022-02-24 11:00:52 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.7928
2022-02-24 11:01:24 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.9067
2022-02-24 11:01:58 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.7769
2022-02-24 11:02:31 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.5292
2022-02-24 11:03:04 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.8487
2022-02-24 11:03:38 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.5994
2022-02-24 11:04:11 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.6502
2022-02-24 11:04:45 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.5923
2022-02-24 11:05:18 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.6618
2022-02-24 11:05:53 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.4623
2022-02-24 11:06:26 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.5679
2022-02-24 11:07:01 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.5647
2022-02-24 11:07:34 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.6747
2022-02-24 11:08:08 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.7412
2022-02-24 11:08:41 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.5518
2022-02-24 11:09:15 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.6707
2022-02-24 11:09:49 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.4600
2022-02-24 11:10:22 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.7184
2022-02-24 11:10:56 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.3439
2022-02-24 11:11:30 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.7962
2022-02-24 11:12:05 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.5309
2022-02-24 11:12:39 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.6131
2022-02-24 11:13:12 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.5884
2022-02-24 11:13:45 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.5686
2022-02-24 11:14:19 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.7224
2022-02-24 11:14:53 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.4272
2022-02-24 11:15:26 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.6063
2022-02-24 11:16:00 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.5090
2022-02-24 11:16:34 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.7516
2022-02-24 11:17:09 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.5969
2022-02-24 11:17:42 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.5330
2022-02-24 11:18:15 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.9026
2022-02-24 11:18:50 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.4648
2022-02-24 11:19:23 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.6405
2022-02-24 11:19:57 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.3406
2022-02-24 11:20:30 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.6185
2022-02-24 11:21:04 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.8240
2022-02-24 11:21:38 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.4176
2022-02-24 11:22:13 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.6730
2022-02-24 11:22:46 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.4562
2022-02-24 11:23:21 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.4221
2022-02-24 11:23:54 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.7279
2022-02-24 11:24:28 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.5520
2022-02-24 11:25:01 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.8393
2022-02-24 11:25:37 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.6330
2022-02-24 11:26:11 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.7366
2022-02-24 11:26:46 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.6422
2022-02-24 11:27:20 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.6086
2022-02-24 11:27:55 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.5051
2022-02-24 11:27:56 - train: epoch 007, train_loss: 2.6121
2022-02-24 11:29:12 - eval: epoch: 007, acc1: 45.812%, acc5: 72.830%, test_loss: 2.3549, per_image_load_time: 2.544ms, per_image_inference_time: 0.297ms
2022-02-24 11:29:12 - until epoch: 007, best_acc1: 45.812%
2022-02-24 11:29:12 - epoch 008 lr: 0.1
2022-02-24 11:29:51 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.6109
2022-02-24 11:30:24 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.6620
2022-02-24 11:30:57 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.3282
2022-02-24 11:31:30 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.4864
2022-02-24 11:32:03 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.5074
2022-02-24 11:32:37 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.4430
2022-02-24 11:33:11 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.9196
2022-02-24 11:33:45 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.3529
2022-02-24 11:34:18 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.5556
2022-02-24 11:34:52 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.5911
2022-02-24 11:35:26 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.2881
2022-02-24 11:35:59 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.4259
2022-02-24 11:36:33 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.6000
2022-02-24 11:37:06 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.5263
2022-02-24 11:37:39 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.6437
2022-02-24 11:38:14 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.6304
2022-02-24 11:38:47 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.4975
2022-02-24 11:39:21 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.7251
2022-02-24 11:39:54 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.4410
2022-02-24 11:40:28 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.5741
2022-02-24 11:41:03 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.5575
2022-02-24 11:41:36 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.4751
2022-02-24 11:42:10 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.6490
2022-02-24 11:42:44 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.4777
2022-02-24 11:43:18 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.6253
2022-02-24 11:43:50 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.7168
2022-02-24 11:44:24 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.6754
2022-02-24 11:44:59 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.6067
2022-02-24 11:45:32 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.5531
2022-02-24 11:46:06 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.6079
2022-02-24 11:46:39 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.5188
2022-02-24 11:47:14 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.7603
2022-02-24 11:47:47 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.6822
2022-02-24 11:48:21 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.7571
2022-02-24 11:48:55 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.4911
2022-02-24 11:49:29 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.7087
2022-02-24 11:50:03 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.4885
2022-02-24 11:50:37 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.5044
2022-02-24 11:51:10 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.5991
2022-02-24 11:51:45 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.8614
2022-02-24 11:52:18 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.5421
2022-02-24 11:52:52 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.5247
2022-02-24 11:53:26 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.2683
2022-02-24 11:54:01 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.5777
2022-02-24 11:54:34 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.7792
2022-02-24 11:55:09 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.6923
2022-02-24 11:55:42 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.4002
2022-02-24 11:56:17 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.5844
2022-02-24 11:56:51 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.5807
2022-02-24 11:57:26 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.5067
2022-02-24 11:57:27 - train: epoch 008, train_loss: 2.5649
2022-02-24 11:58:43 - eval: epoch: 008, acc1: 47.522%, acc5: 73.834%, test_loss: 2.2895, per_image_load_time: 1.961ms, per_image_inference_time: 0.303ms
2022-02-24 11:58:44 - until epoch: 008, best_acc1: 47.522%
2022-02-24 11:58:44 - epoch 009 lr: 0.1
2022-02-24 11:59:23 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.2245
2022-02-24 11:59:55 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.4082
2022-02-24 12:00:29 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.3097
2022-02-24 12:01:03 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.6901
2022-02-24 12:01:35 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.5070
2022-02-24 12:02:09 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.5888
2022-02-24 12:02:43 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.3645
2022-02-24 12:03:17 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.4452
2022-02-24 12:03:51 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.4500
2022-02-24 12:04:24 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.4462
2022-02-24 12:04:58 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.9034
2022-02-24 12:05:32 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.7034
2022-02-24 12:06:05 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.6516
2022-02-24 12:06:39 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.3690
2022-02-24 12:07:13 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.4072
2022-02-24 12:07:47 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.5440
2022-02-24 12:08:21 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.6901
2022-02-24 12:08:54 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.3424
2022-02-24 12:09:28 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.3202
2022-02-24 12:10:01 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.3160
2022-02-24 12:10:35 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.4902
2022-02-24 12:11:09 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.6827
2022-02-24 12:11:42 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.2269
2022-02-24 12:12:17 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.4781
2022-02-24 12:12:50 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.4612
2022-02-24 12:13:25 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.5378
2022-02-24 12:13:58 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.4051
2022-02-24 12:14:33 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.5207
2022-02-24 12:15:05 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.2370
2022-02-24 12:15:39 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.3973
2022-02-24 12:16:13 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.6818
2022-02-24 12:16:46 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.5726
2022-02-24 12:17:21 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.5629
2022-02-24 12:17:55 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.7371
2022-02-24 12:18:29 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.6042
2022-02-24 12:19:02 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.5441
2022-02-24 12:19:36 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.7024
2022-02-24 12:20:11 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.5809
2022-02-24 12:20:43 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.3017
2022-02-24 12:21:18 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.5788
2022-02-24 12:21:53 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.5621
2022-02-24 12:22:26 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.4838
2022-02-24 12:22:59 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.6499
2022-02-24 12:23:34 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.4930
2022-02-24 12:24:08 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.4418
2022-02-24 12:24:41 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.5720
2022-02-24 12:25:16 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.6305
2022-02-24 12:25:50 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.6728
2022-02-24 12:26:25 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.5829
2022-02-24 12:26:59 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.4020
2022-02-24 12:27:00 - train: epoch 009, train_loss: 2.5323
2022-02-24 12:28:15 - eval: epoch: 009, acc1: 47.650%, acc5: 74.462%, test_loss: 2.2639, per_image_load_time: 1.339ms, per_image_inference_time: 0.305ms
2022-02-24 12:28:16 - until epoch: 009, best_acc1: 47.650%
2022-02-24 12:28:16 - epoch 010 lr: 0.1
2022-02-24 12:28:55 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.4572
2022-02-24 12:29:27 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.6847
2022-02-24 12:30:01 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.5047
2022-02-24 12:30:36 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.5664
2022-02-24 12:31:09 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.4407
2022-02-24 12:31:43 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.5824
2022-02-24 12:32:16 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.5756
2022-02-24 12:32:50 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.4074
2022-02-24 12:33:24 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.3422
2022-02-24 12:33:57 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.2678
2022-02-24 12:34:31 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.5159
2022-02-24 12:35:05 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.3499
2022-02-24 12:35:39 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.2684
2022-02-24 12:36:13 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.5644
2022-02-24 12:36:47 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.1915
2022-02-24 12:37:20 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.5656
2022-02-24 12:37:56 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.7114
2022-02-24 12:38:28 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.4328
2022-02-24 12:39:03 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.6597
2022-02-24 12:39:35 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.5409
2022-02-24 12:40:11 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.4216
2022-02-24 12:40:44 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.6475
2022-02-24 12:41:18 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.6023
2022-02-24 12:41:52 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.6972
2022-02-24 12:42:26 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.4950
2022-02-24 12:43:00 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.6157
2022-02-24 12:43:33 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.3375
2022-02-24 12:44:07 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.5836
2022-02-24 12:44:41 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.5403
2022-02-24 12:45:16 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.4262
2022-02-24 12:45:48 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.6117
2022-02-24 12:46:23 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.5225
2022-02-24 12:46:57 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.4871
2022-02-24 12:47:31 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.7700
2022-02-24 12:48:04 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.7838
2022-02-24 12:48:38 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.7666
2022-02-24 12:49:12 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.3595
2022-02-24 12:49:46 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.5373
2022-02-24 12:50:20 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.1043
2022-02-24 12:50:54 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.3978
2022-02-24 12:51:28 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.3463
2022-02-24 12:52:02 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.5815
2022-02-24 12:52:36 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.5458
2022-02-24 12:53:10 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.4613
2022-02-24 12:53:44 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.3501
2022-02-24 12:54:19 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.6241
2022-02-24 12:54:53 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.6378
2022-02-24 12:55:28 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.4091
2022-02-24 12:56:02 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.3395
2022-02-24 12:56:37 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.3016
2022-02-24 12:56:38 - train: epoch 010, train_loss: 2.5026
2022-02-24 12:57:53 - eval: epoch: 010, acc1: 48.524%, acc5: 74.850%, test_loss: 2.2340, per_image_load_time: 2.581ms, per_image_inference_time: 0.340ms
2022-02-24 12:57:53 - until epoch: 010, best_acc1: 48.524%
2022-02-24 12:57:53 - epoch 011 lr: 0.1
2022-02-24 12:58:32 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.2466
2022-02-24 12:59:05 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.5432
2022-02-24 12:59:38 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.3156
2022-02-24 13:00:11 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.5277
2022-02-24 13:00:46 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.4422
2022-02-24 13:01:19 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.4825
2022-02-24 13:01:53 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.4425
2022-02-24 13:02:27 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.5804
2022-02-24 13:03:00 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.6301
2022-02-24 13:03:34 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.5040
2022-02-24 13:04:08 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.5679
2022-02-24 13:04:41 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.7287
2022-02-24 13:05:15 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.6514
2022-02-24 13:05:50 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.5476
2022-02-24 13:06:23 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.3631
2022-02-24 13:06:58 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.5438
2022-02-24 13:07:31 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.5027
2022-02-24 13:08:05 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.2988
2022-02-24 13:08:39 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.3097
2022-02-24 13:09:13 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.6397
2022-02-24 13:09:47 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.4616
2022-02-24 13:10:22 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.3536
2022-02-24 13:10:55 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.5602
2022-02-24 13:11:29 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.2857
2022-02-24 13:12:03 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.7525
2022-02-24 13:12:37 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.3887
2022-02-24 13:13:12 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.5310
2022-02-24 13:13:46 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.0931
2022-02-24 13:14:20 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.4875
2022-02-24 13:14:54 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.7591
2022-02-24 13:15:28 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.6294
2022-02-24 13:16:01 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.3818
2022-02-24 13:16:35 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.6141
2022-02-24 13:17:09 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.4607
2022-02-24 13:17:45 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.4693
2022-02-24 13:18:18 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.4942
2022-02-24 13:18:53 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.5072
2022-02-24 13:19:25 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.2370
2022-02-24 13:19:59 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.5572
2022-02-24 13:20:34 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.3393
2022-02-24 13:21:08 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.1597
2022-02-24 13:21:41 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.4358
2022-02-24 13:22:16 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.5691
2022-02-24 13:22:49 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.4300
2022-02-24 13:23:23 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.3360
2022-02-24 13:23:58 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.3813
2022-02-24 13:24:31 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.2255
2022-02-24 13:25:06 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.2989
2022-02-24 13:25:41 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.2995
2022-02-24 13:26:15 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.5233
2022-02-24 13:26:16 - train: epoch 011, train_loss: 2.4797
2022-02-24 13:27:33 - eval: epoch: 011, acc1: 47.618%, acc5: 74.098%, test_loss: 2.2712, per_image_load_time: 2.678ms, per_image_inference_time: 0.285ms
2022-02-24 13:27:33 - until epoch: 011, best_acc1: 48.524%
2022-02-24 13:27:33 - epoch 012 lr: 0.1
2022-02-24 13:28:11 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.3068
2022-02-24 13:28:45 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.4523
2022-02-24 13:29:18 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.4106
2022-02-24 13:29:52 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.4946
2022-02-24 13:30:25 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.6925
2022-02-24 13:30:59 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.1961
2022-02-24 13:31:32 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.2758
2022-02-24 13:32:06 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.4935
2022-02-24 13:32:39 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.6063
2022-02-24 13:33:14 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.2721
2022-02-24 13:33:46 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.8768
2022-02-24 13:34:20 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.3763
2022-02-24 13:34:53 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.5015
2022-02-24 13:35:27 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.5845
2022-02-24 13:36:00 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.3111
2022-02-24 13:36:34 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.4396
2022-02-24 13:37:08 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.2742
2022-02-24 13:37:42 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.4694
2022-02-24 13:38:15 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.5471
2022-02-24 13:38:48 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.6681
2022-02-24 13:39:22 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.4951
2022-02-24 13:39:55 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.6034
2022-02-24 13:40:30 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.5732
2022-02-24 13:41:04 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.4784
2022-02-24 13:41:37 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.2743
2022-02-24 13:42:10 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.3351
2022-02-24 13:42:44 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.3974
2022-02-24 13:43:18 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.3732
2022-02-24 13:43:52 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.3413
2022-02-24 13:44:25 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.3955
2022-02-24 13:45:00 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.6031
2022-02-24 13:45:32 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.1742
2022-02-24 13:46:07 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.3857
2022-02-24 13:46:41 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.5203
2022-02-24 13:47:15 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.5801
2022-02-24 13:47:49 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.5115
2022-02-24 13:48:23 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.4730
2022-02-24 13:48:57 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.4915
2022-02-24 13:49:30 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.3286
2022-02-24 13:50:04 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.4303
2022-02-24 13:50:39 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.2968
2022-02-24 13:51:14 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.3420
2022-02-24 13:51:47 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.6330
2022-02-24 13:52:21 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.3142
2022-02-24 13:52:55 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.4608
2022-02-24 13:53:29 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.7448
2022-02-24 13:54:04 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.3572
2022-02-24 13:54:39 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.5605
2022-02-24 13:55:13 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.5035
2022-02-24 13:55:47 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.0955
2022-02-24 13:55:49 - train: epoch 012, train_loss: 2.4605
2022-02-24 13:57:05 - eval: epoch: 012, acc1: 50.468%, acc5: 76.286%, test_loss: 2.1401, per_image_load_time: 2.693ms, per_image_inference_time: 0.284ms
2022-02-24 13:57:06 - until epoch: 012, best_acc1: 50.468%
2022-02-24 13:57:06 - epoch 013 lr: 0.1
2022-02-24 13:57:43 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.1538
2022-02-24 13:58:17 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.3406
2022-02-24 13:58:51 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.3894
2022-02-24 13:59:24 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.2679
2022-02-24 13:59:57 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.4983
2022-02-24 14:00:31 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.5327
2022-02-24 14:01:03 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.3207
2022-02-24 14:01:38 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.6675
2022-02-24 14:02:12 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.4834
2022-02-24 14:02:45 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.5621
2022-02-24 14:03:19 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.5235
2022-02-24 14:03:54 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.7054
2022-02-24 14:04:26 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.5553
2022-02-24 14:05:00 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.3560
2022-02-24 14:05:34 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.6196
2022-02-24 14:06:08 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.0977
2022-02-24 14:06:41 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.5026
2022-02-24 14:07:15 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.4280
2022-02-24 14:07:49 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.3870
2022-02-24 14:08:22 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.7624
2022-02-24 14:08:56 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.8085
2022-02-24 14:09:29 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.3674
2022-02-24 14:10:03 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.4692
2022-02-24 14:10:36 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.5004
2022-02-24 14:11:10 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.3362
2022-02-24 14:11:44 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.3579
2022-02-24 14:12:18 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.2601
2022-02-24 14:12:52 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.4235
2022-02-24 14:13:26 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.3826
2022-02-24 14:14:00 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.4158
2022-02-24 14:14:34 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.2784
2022-02-24 14:15:08 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.4447
2022-02-24 14:15:42 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.2291
2022-02-24 14:16:16 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.4289
2022-02-24 14:16:50 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.3150
2022-02-24 14:17:23 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.6992
2022-02-24 14:17:58 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.2297
2022-02-24 14:18:31 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.5169
2022-02-24 14:19:05 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.4965
2022-02-24 14:19:39 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.4672
2022-02-24 14:20:13 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.4929
2022-02-24 14:20:48 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.4103
2022-02-24 14:21:21 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.5265
2022-02-24 14:21:55 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.3126
2022-02-24 14:22:29 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.4029
2022-02-24 14:23:04 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.4283
2022-02-24 14:23:38 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.6392
2022-02-24 14:24:13 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.4893
2022-02-24 14:24:47 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.6302
2022-02-24 14:25:21 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.5859
2022-02-24 14:25:22 - train: epoch 013, train_loss: 2.4446
2022-02-24 14:26:38 - eval: epoch: 013, acc1: 50.730%, acc5: 76.416%, test_loss: 2.1362, per_image_load_time: 2.597ms, per_image_inference_time: 0.344ms
2022-02-24 14:26:39 - until epoch: 013, best_acc1: 50.730%
2022-02-24 14:26:39 - epoch 014 lr: 0.1
2022-02-24 14:27:18 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.2703
2022-02-24 14:27:50 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.5426
2022-02-24 14:28:23 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.1144
2022-02-24 14:28:57 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.2647
2022-02-24 14:29:31 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.2922
2022-02-24 14:30:04 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.5015
2022-02-24 14:30:37 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.3880
2022-02-24 14:31:10 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.4887
2022-02-24 14:31:44 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.3990
2022-02-24 14:32:17 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.4784
2022-02-24 14:32:51 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.3576
2022-02-24 14:33:25 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.4372
2022-02-24 14:33:58 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.4667
2022-02-24 14:34:32 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.4852
2022-02-24 14:35:05 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.5672
2022-02-24 14:35:39 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.2938
2022-02-24 14:36:12 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.6797
2022-02-24 14:36:46 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.5997
2022-02-24 14:37:20 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.2982
2022-02-24 14:37:54 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.3045
2022-02-24 14:38:28 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.5875
2022-02-24 14:39:01 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.4222
2022-02-24 14:39:35 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.4748
2022-02-24 14:40:08 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.6015
2022-02-24 14:40:42 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.4021
2022-02-24 14:41:16 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.3991
2022-02-24 14:41:51 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.3817
2022-02-24 14:42:24 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.7463
2022-02-24 14:42:58 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.3038
2022-02-24 14:43:32 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.5292
2022-02-24 14:44:05 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.2186
2022-02-24 14:44:39 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.4414
2022-02-24 14:45:14 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.3679
2022-02-24 14:45:48 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.3495
2022-02-24 14:46:22 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.3490
2022-02-24 14:46:57 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.2856
2022-02-24 14:47:29 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.4037
2022-02-24 14:48:03 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.5203
2022-02-24 14:48:38 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.3051
2022-02-24 14:49:13 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.5577
2022-02-24 14:49:46 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.4344
2022-02-24 14:50:21 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.2429
2022-02-24 14:50:54 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.3073
2022-02-24 14:51:29 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.2447
2022-02-24 14:52:02 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.4523
2022-02-24 14:52:36 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.3921
2022-02-24 14:53:12 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.2979
2022-02-24 14:53:45 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.2620
2022-02-24 14:54:21 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.1704
2022-02-24 14:54:55 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.5127
2022-02-24 14:54:56 - train: epoch 014, train_loss: 2.4304
2022-02-24 14:56:12 - eval: epoch: 014, acc1: 48.192%, acc5: 74.532%, test_loss: 2.2633, per_image_load_time: 2.619ms, per_image_inference_time: 0.322ms
2022-02-24 14:56:12 - until epoch: 014, best_acc1: 50.730%
2022-02-24 14:56:12 - epoch 015 lr: 0.1
2022-02-24 14:56:51 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.2861
2022-02-24 14:57:24 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.6307
2022-02-24 14:57:58 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.6958
2022-02-24 14:58:31 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.4621
2022-02-24 14:59:05 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.4251
2022-02-24 14:59:39 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.5536
2022-02-24 15:00:13 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.4109
2022-02-24 15:00:46 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.1698
2022-02-24 15:01:20 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.3526
2022-02-24 15:01:54 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.5075
2022-02-24 15:02:27 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.2434
2022-02-24 15:03:00 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.4313
2022-02-24 15:03:34 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.7575
2022-02-24 15:04:07 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.2808
2022-02-24 15:04:42 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.1411
2022-02-24 15:05:15 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.4321
2022-02-24 15:05:49 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.5311
2022-02-24 15:06:22 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.2748
2022-02-24 15:06:57 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.3426
2022-02-24 15:07:30 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.3193
2022-02-24 15:08:03 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.2765
2022-02-24 15:08:36 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.6068
2022-02-24 15:09:11 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.3055
2022-02-24 15:09:44 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.3661
2022-02-24 15:10:18 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.5857
2022-02-24 15:10:52 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.2902
2022-02-24 15:11:25 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.5391
2022-02-24 15:11:59 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.5082
2022-02-24 15:12:32 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.2647
2022-02-24 15:13:06 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.3087
2022-02-24 15:13:39 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.3629
2022-02-24 15:14:13 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.3739
2022-02-24 15:14:48 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.1524
2022-02-24 15:15:21 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.3871
2022-02-24 15:15:55 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.5432
2022-02-24 15:16:28 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.4300
2022-02-24 15:17:03 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.4159
2022-02-24 15:17:36 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.4389
2022-02-24 15:18:10 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.5570
2022-02-24 15:18:44 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.4254
2022-02-24 15:19:18 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.6044
2022-02-24 15:19:52 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.2360
2022-02-24 15:20:25 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.5494
2022-02-24 15:20:59 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.4156
2022-02-24 15:21:33 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.4022
2022-02-24 15:22:08 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.2920
2022-02-24 15:22:42 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.6717
2022-02-24 15:23:16 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.4651
2022-02-24 15:23:52 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.3933
2022-02-24 15:24:25 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.4691
2022-02-24 15:24:27 - train: epoch 015, train_loss: 2.4194
2022-02-24 15:25:42 - eval: epoch: 015, acc1: 49.672%, acc5: 76.188%, test_loss: 2.1515, per_image_load_time: 2.475ms, per_image_inference_time: 0.355ms
2022-02-24 15:25:43 - until epoch: 015, best_acc1: 50.730%
2022-02-24 15:25:43 - epoch 016 lr: 0.1
2022-02-24 15:26:21 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.4210
2022-02-24 15:26:55 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.2265
2022-02-24 15:27:27 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.3555
2022-02-24 15:28:01 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.7577
2022-02-24 15:28:35 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.1590
2022-02-24 15:29:08 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.5014
2022-02-24 15:29:42 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.2038
2022-02-24 15:30:15 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.4386
2022-02-24 15:30:49 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.4479
2022-02-24 15:31:23 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.3042
2022-02-24 15:31:56 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.3166
2022-02-24 15:32:30 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.2347
2022-02-24 15:33:03 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.4431
2022-02-24 15:33:37 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.3430
2022-02-24 15:34:11 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.4708
2022-02-24 15:34:45 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.5090
2022-02-24 15:35:18 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.2317
2022-02-24 15:35:52 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.3148
2022-02-24 15:36:26 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.5175
2022-02-24 15:36:59 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.1423
2022-02-24 15:37:33 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.6148
2022-02-24 15:38:08 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.4055
2022-02-24 15:38:41 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.6111
2022-02-24 15:39:14 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.5713
2022-02-24 15:39:47 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.2344
2022-02-24 15:40:21 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.5578
2022-02-24 15:40:55 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.3466
2022-02-24 15:41:28 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.2516
2022-02-24 15:42:02 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.4865
2022-02-24 15:42:35 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.6893
2022-02-24 15:43:09 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.5603
2022-02-24 15:43:42 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.4916
2022-02-24 15:44:17 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.4909
2022-02-24 15:44:50 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.3629
2022-02-24 15:45:24 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.2163
2022-02-24 15:45:58 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.2494
2022-02-24 15:46:32 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.4913
2022-02-24 15:47:05 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.6581
2022-02-24 15:47:39 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.5462
2022-02-24 15:48:13 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.4668
2022-02-24 15:48:47 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.3765
2022-02-24 15:49:21 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.4217
2022-02-24 15:49:55 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.2832
2022-02-24 15:50:29 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.3505
2022-02-24 15:51:03 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.5280
2022-02-24 15:51:37 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.4155
2022-02-24 15:52:11 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.6086
2022-02-24 15:52:46 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.3324
2022-02-24 15:53:20 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.5207
2022-02-24 15:53:53 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.3991
2022-02-24 15:53:55 - train: epoch 016, train_loss: 2.4077
2022-02-24 15:55:12 - eval: epoch: 016, acc1: 49.094%, acc5: 75.012%, test_loss: 2.2161, per_image_load_time: 2.660ms, per_image_inference_time: 0.301ms
2022-02-24 15:55:12 - until epoch: 016, best_acc1: 50.730%
2022-02-24 15:55:12 - epoch 017 lr: 0.1
2022-02-24 15:55:51 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.2662
2022-02-24 15:56:23 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.5125
2022-02-24 15:56:57 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.7045
2022-02-24 15:57:30 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.1569
2022-02-24 15:58:04 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.4178
2022-02-24 15:58:38 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.6271
2022-02-24 15:59:12 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.4258
2022-02-24 15:59:44 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.3490
2022-02-24 16:00:18 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.3619
2022-02-24 16:00:52 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.1937
2022-02-24 16:01:24 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.7168
2022-02-24 16:01:58 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.6334
2022-02-24 16:02:32 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.4712
2022-02-24 16:03:07 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.4829
2022-02-24 16:03:40 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.1566
2022-02-24 16:04:14 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.1777
2022-02-24 16:04:48 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.3840
2022-02-24 16:05:20 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.3323
2022-02-24 16:05:55 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.2381
2022-02-24 16:06:29 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.4720
2022-02-24 16:07:03 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.3405
2022-02-24 16:07:36 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.4298
2022-02-24 16:08:10 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.3314
2022-02-24 16:08:44 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.2944
2022-02-24 16:09:18 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.5853
2022-02-24 16:09:52 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.3040
2022-02-24 16:10:26 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.3366
2022-02-24 16:10:59 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.6462
2022-02-24 16:11:33 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.7234
2022-02-24 16:12:07 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.3045
2022-02-24 16:12:40 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.5741
2022-02-24 16:13:15 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.2519
2022-02-24 16:13:48 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.5322
2022-02-24 16:14:22 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.2580
2022-02-24 16:14:56 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.3694
2022-02-24 16:15:29 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.6247
2022-02-24 16:16:03 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.3488
2022-02-24 16:16:37 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.5207
2022-02-24 16:17:11 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.2133
2022-02-24 16:17:44 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.2439
2022-02-24 16:18:18 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.3955
2022-02-24 16:18:52 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.3922
2022-02-24 16:19:26 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.3437
2022-02-24 16:20:00 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.4304
2022-02-24 16:20:34 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.4804
2022-02-24 16:21:08 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.3761
2022-02-24 16:21:43 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.5330
2022-02-24 16:22:16 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.4992
2022-02-24 16:22:51 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.2166
2022-02-24 16:23:24 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.2609
2022-02-24 16:23:26 - train: epoch 017, train_loss: 2.3970
2022-02-24 16:24:42 - eval: epoch: 017, acc1: 50.692%, acc5: 76.600%, test_loss: 2.1301, per_image_load_time: 2.641ms, per_image_inference_time: 0.308ms
2022-02-24 16:24:43 - until epoch: 017, best_acc1: 50.730%
2022-02-24 16:24:43 - epoch 018 lr: 0.1
2022-02-24 16:25:21 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.3715
2022-02-24 16:25:54 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.5156
2022-02-24 16:26:28 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.5965
2022-02-24 16:27:01 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.5223
2022-02-24 16:27:35 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.4197
2022-02-24 16:28:08 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.3911
2022-02-24 16:28:42 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.1924
2022-02-24 16:29:15 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.3687
2022-02-24 16:29:48 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.4651
2022-02-24 16:30:22 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.2194
2022-02-24 16:30:55 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.6860
2022-02-24 16:31:29 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.4418
2022-02-24 16:32:03 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.7643
2022-02-24 16:32:36 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.3958
2022-02-24 16:33:10 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.5910
2022-02-24 16:33:43 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.4706
2022-02-24 16:34:18 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.2591
2022-02-24 16:34:52 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.1694
2022-02-24 16:35:26 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.4456
2022-02-24 16:35:59 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.6127
2022-02-24 16:36:33 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.4464
2022-02-24 16:37:07 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.4260
2022-02-24 16:37:40 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.5297
2022-02-24 16:38:14 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.1830
2022-02-24 16:38:48 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.1593
2022-02-24 16:39:22 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.2999
2022-02-24 16:39:55 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.4958
2022-02-24 16:40:29 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.1665
2022-02-24 16:41:03 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.4422
2022-02-24 16:41:36 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.2461
2022-02-24 16:42:10 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.7039
2022-02-24 16:42:43 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.1824
2022-02-24 16:43:18 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.2406
2022-02-24 16:43:52 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.3362
2022-02-24 16:44:26 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.5819
2022-02-24 16:44:59 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.5127
2022-02-24 16:45:33 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.7775
2022-02-24 16:46:06 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.3777
2022-02-24 16:46:40 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.3641
2022-02-24 16:47:15 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.2523
2022-02-24 16:47:48 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.3207
2022-02-24 16:48:22 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.4009
2022-02-24 16:48:56 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.1714
2022-02-24 16:49:30 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.6113
2022-02-24 16:50:04 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.3862
2022-02-24 16:50:38 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.1149
2022-02-24 16:51:13 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.6170
2022-02-24 16:51:47 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.5466
2022-02-24 16:52:21 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.3417
2022-02-24 16:52:55 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.5753
2022-02-24 16:52:57 - train: epoch 018, train_loss: 2.3898
2022-02-24 16:54:12 - eval: epoch: 018, acc1: 50.504%, acc5: 76.460%, test_loss: 2.1283, per_image_load_time: 2.570ms, per_image_inference_time: 0.334ms
2022-02-24 16:54:13 - until epoch: 018, best_acc1: 50.730%
2022-02-24 16:54:13 - epoch 019 lr: 0.1
2022-02-24 16:54:52 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.2261
2022-02-24 16:55:25 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.5553
2022-02-24 16:55:59 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.6066
2022-02-24 16:56:32 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.2684
2022-02-24 16:57:04 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.3562
2022-02-24 16:57:37 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.4025
2022-02-24 16:58:11 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.1444
2022-02-24 16:58:45 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.6601
2022-02-24 16:59:18 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.3637
2022-02-24 16:59:52 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.5214
2022-02-24 17:00:24 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.2762
2022-02-24 17:00:58 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.4516
2022-02-24 17:01:32 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.5276
2022-02-24 17:02:06 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.2938
2022-02-24 17:02:39 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.6785
2022-02-24 17:03:12 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.3670
2022-02-24 17:03:47 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.5349
2022-02-24 17:04:21 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.4155
2022-02-24 17:04:55 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.6387
2022-02-24 17:05:27 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.3853
2022-02-24 17:06:02 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.2810
2022-02-24 17:06:35 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.3785
2022-02-24 17:07:09 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.3459
2022-02-24 17:07:43 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.5008
2022-02-24 17:08:16 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.2734
2022-02-24 17:08:49 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.3685
2022-02-24 17:09:23 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.4362
2022-02-24 17:09:56 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.4291
2022-02-24 17:10:31 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.3571
2022-02-24 17:11:04 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.6814
2022-02-24 17:11:38 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.4128
2022-02-24 17:12:12 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.1596
2022-02-24 17:12:46 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.3351
2022-02-24 17:13:20 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.4787
2022-02-24 17:13:53 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.5467
2022-02-24 17:14:27 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.3542
2022-02-24 17:15:01 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.4624
2022-02-24 17:15:35 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.5413
2022-02-24 17:16:09 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.3686
2022-02-24 17:16:43 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.2393
2022-02-24 17:17:16 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.3827
2022-02-24 17:17:51 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.3683
2022-02-24 17:18:24 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.3535
2022-02-24 17:18:58 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.5254
2022-02-24 17:19:31 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.8166
2022-02-24 17:20:06 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.4149
2022-02-24 17:20:40 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.2472
2022-02-24 17:21:15 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.3456
2022-02-24 17:21:49 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.3283
2022-02-24 17:22:23 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.3632
2022-02-24 17:22:24 - train: epoch 019, train_loss: 2.3856
2022-02-24 17:23:40 - eval: epoch: 019, acc1: 51.530%, acc5: 77.390%, test_loss: 2.0736, per_image_load_time: 2.622ms, per_image_inference_time: 0.309ms
2022-02-24 17:23:41 - until epoch: 019, best_acc1: 51.530%
2022-02-24 17:23:41 - epoch 020 lr: 0.1
2022-02-24 17:24:18 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.5278
2022-02-24 17:24:52 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.1736
2022-02-24 17:25:26 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.4714
2022-02-24 17:25:59 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.2186
2022-02-24 17:26:33 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.3248
2022-02-24 17:27:06 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.4932
2022-02-24 17:27:40 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.2146
2022-02-24 17:28:14 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.4765
2022-02-24 17:28:48 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.6011
2022-02-24 17:29:22 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.5078
2022-02-24 17:29:56 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.2126
2022-02-24 17:30:30 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.1465
2022-02-24 17:31:04 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.3348
2022-02-24 17:31:38 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.4911
2022-02-24 17:32:12 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.4813
2022-02-24 17:32:45 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.2739
2022-02-24 17:33:20 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.0943
2022-02-24 17:33:52 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.3570
2022-02-24 17:34:26 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.1953
2022-02-24 17:35:00 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.4219
2022-02-24 17:35:34 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.5678
2022-02-24 17:36:08 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.2021
2022-02-24 17:36:41 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.2364
2022-02-24 17:37:16 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.5692
2022-02-24 17:37:48 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.2692
2022-02-24 17:38:23 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.2141
2022-02-24 17:38:56 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.4823
2022-02-24 17:39:31 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.4691
2022-02-24 17:40:04 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.5013
2022-02-24 17:40:37 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.4317
2022-02-24 17:41:12 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.3838
2022-02-24 17:41:46 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.4797
2022-02-24 17:42:19 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.2666
2022-02-24 17:42:53 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.3636
2022-02-24 17:43:28 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.2460
2022-02-24 17:44:02 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.2954
2022-02-24 17:44:36 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.1998
2022-02-24 17:45:10 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.3786
2022-02-24 17:45:44 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.4620
2022-02-24 17:46:17 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.3601
2022-02-24 17:46:52 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.2252
2022-02-24 17:47:26 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.2787
2022-02-24 17:47:59 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.3433
2022-02-24 17:48:33 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.3841
2022-02-24 17:49:08 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.4090
2022-02-24 17:49:42 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.5237
2022-02-24 17:50:16 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.1161
2022-02-24 17:50:50 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.3331
2022-02-24 17:51:25 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.4547
2022-02-24 17:52:00 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.2768
2022-02-24 17:52:01 - train: epoch 020, train_loss: 2.3728
2022-02-24 17:53:17 - eval: epoch: 020, acc1: 50.718%, acc5: 76.418%, test_loss: 2.1435, per_image_load_time: 2.586ms, per_image_inference_time: 0.340ms
2022-02-24 17:53:18 - until epoch: 020, best_acc1: 51.530%
2022-02-24 17:53:18 - epoch 021 lr: 0.1
2022-02-24 17:53:56 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.2407
2022-02-24 17:54:30 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.4445
2022-02-24 17:55:03 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.1129
2022-02-24 17:55:37 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.3762
2022-02-24 17:56:09 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.2744
2022-02-24 17:56:43 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.2440
2022-02-24 17:57:17 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.1455
2022-02-24 17:57:51 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.5740
2022-02-24 17:58:24 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.4416
2022-02-24 17:58:58 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.2124
2022-02-24 17:59:32 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.3603
2022-02-24 18:00:05 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.2941
2022-02-24 18:00:39 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.2803
2022-02-24 18:01:12 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.2809
2022-02-24 18:01:46 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.1119
2022-02-24 18:02:19 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.3867
2022-02-24 18:02:53 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.4299
2022-02-24 18:03:27 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.2225
2022-02-24 18:04:01 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.5412
2022-02-24 18:04:34 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.4974
2022-02-24 18:05:07 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.2012
2022-02-24 18:05:41 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.3397
2022-02-24 18:06:14 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.1769
2022-02-24 18:06:49 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.1941
2022-02-24 18:07:22 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.2600
2022-02-24 18:07:57 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.5864
2022-02-24 18:08:30 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.2203
2022-02-24 18:09:04 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.3041
2022-02-24 18:09:36 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.3146
2022-02-24 18:10:11 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.5947
2022-02-24 18:10:44 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.3805
2022-02-24 18:11:19 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.3944
2022-02-24 18:11:51 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.6477
2022-02-24 18:12:25 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.5822
2022-02-24 18:12:59 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.2232
2022-02-24 18:13:33 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.3330
2022-02-24 18:14:07 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.4365
2022-02-24 18:14:41 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.2346
2022-02-24 18:15:15 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.2603
2022-02-24 18:15:49 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.5968
2022-02-24 18:16:22 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.1368
2022-02-24 18:16:57 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.3945
2022-02-24 18:17:30 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.3901
2022-02-24 18:18:04 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.4731
2022-02-24 18:18:38 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.6074
2022-02-24 18:19:13 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.2866
2022-02-24 18:19:47 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.5570
2022-02-24 18:20:22 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.5588
2022-02-24 18:20:56 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.1644
2022-02-24 18:21:30 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.2697
2022-02-24 18:21:31 - train: epoch 021, train_loss: 2.3681
2022-02-24 18:22:47 - eval: epoch: 021, acc1: 49.676%, acc5: 75.822%, test_loss: 2.1700, per_image_load_time: 2.565ms, per_image_inference_time: 0.335ms
2022-02-24 18:22:47 - until epoch: 021, best_acc1: 51.530%
2022-02-24 18:22:47 - epoch 022 lr: 0.1
2022-02-24 18:23:26 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.2253
2022-02-24 18:23:59 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.1899
2022-02-24 18:24:32 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.1149
2022-02-24 18:25:06 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.1914
2022-02-24 18:25:38 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.3356
2022-02-24 18:26:12 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.4959
2022-02-24 18:26:46 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.4941
2022-02-24 18:27:20 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.4537
2022-02-24 18:27:53 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.4516
2022-02-24 18:28:27 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.3771
2022-02-24 18:29:01 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.4668
2022-02-24 18:29:35 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.0779
2022-02-24 18:30:07 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.3809
2022-02-24 18:30:41 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.3504
2022-02-24 18:31:15 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.3401
2022-02-24 18:31:49 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.2055
2022-02-24 18:32:23 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.1267
2022-02-24 18:32:56 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.4831
2022-02-24 18:33:29 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.1511
2022-02-24 18:34:03 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.4037
2022-02-24 18:34:37 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.4226
2022-02-24 18:35:11 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.1581
2022-02-24 18:35:44 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.5083
2022-02-24 18:36:17 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.4577
2022-02-24 18:36:51 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.3099
2022-02-24 18:37:26 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.1851
2022-02-24 18:37:59 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.1739
2022-02-24 18:38:33 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.6861
2022-02-24 18:39:06 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.2276
2022-02-24 18:39:40 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.3273
2022-02-24 18:40:13 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.5441
2022-02-24 18:40:48 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.5038
2022-02-24 18:41:21 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.3546
2022-02-24 18:41:55 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.1838
2022-02-24 18:42:28 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.4547
2022-02-24 18:43:03 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.4146
2022-02-24 18:43:36 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.4611
2022-02-24 18:44:09 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.5698
2022-02-24 18:44:43 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.3634
2022-02-24 18:45:17 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.4300
2022-02-24 18:45:50 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.2377
2022-02-24 18:46:24 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.3177
2022-02-24 18:46:58 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.4955
2022-02-24 18:47:31 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.4355
2022-02-24 18:48:06 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.3385
2022-02-24 18:48:40 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.5042
2022-02-24 18:49:13 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.4592
2022-02-24 18:49:48 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.2069
2022-02-24 18:50:22 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.1810
2022-02-24 18:50:56 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.2865
2022-02-24 18:50:57 - train: epoch 022, train_loss: 2.3663
2022-02-24 18:52:13 - eval: epoch: 022, acc1: 50.216%, acc5: 76.344%, test_loss: 2.1543, per_image_load_time: 2.573ms, per_image_inference_time: 0.340ms
2022-02-24 18:52:14 - until epoch: 022, best_acc1: 51.530%
2022-02-24 18:52:14 - epoch 023 lr: 0.1
2022-02-24 18:52:52 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.3259
2022-02-24 18:53:25 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.0428
2022-02-24 18:53:58 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.2053
2022-02-24 18:54:32 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.3611
2022-02-24 18:55:06 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.3906
2022-02-24 18:55:40 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.2943
2022-02-24 18:56:13 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.2312
2022-02-24 18:56:47 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.2593
2022-02-24 18:57:20 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.3758
2022-02-24 18:57:55 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.2104
2022-02-24 18:58:27 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.4179
2022-02-24 18:59:01 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.2149
2022-02-24 18:59:35 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.3877
2022-02-24 19:00:09 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.3136
2022-02-24 19:00:43 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.1915
2022-02-24 19:01:16 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.3434
2022-02-24 19:01:50 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.5963
2022-02-24 19:02:24 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.2365
2022-02-24 19:02:57 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.4364
2022-02-24 19:03:31 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.9610
2022-02-24 19:04:05 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.4713
2022-02-24 19:04:39 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.0063
2022-02-24 19:05:13 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.2811
2022-02-24 19:05:47 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.3208
2022-02-24 19:06:20 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.4757
2022-02-24 19:06:54 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.3971
2022-02-24 19:07:27 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.2488
2022-02-24 19:08:02 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.4292
2022-02-24 19:08:35 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.3302
2022-02-24 19:09:09 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.5700
2022-02-24 19:09:43 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.4164
2022-02-24 19:10:18 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.4540
2022-02-24 19:10:51 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.3424
2022-02-24 19:11:25 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.4417
2022-02-24 19:11:58 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.2219
2022-02-24 19:12:32 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.1925
2022-02-24 19:13:05 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.4613
2022-02-24 19:13:39 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.2953
2022-02-24 19:14:13 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.3280
2022-02-24 19:14:47 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.4060
2022-02-24 19:15:21 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.2943
2022-02-24 19:15:55 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.2024
2022-02-24 19:16:28 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.2315
2022-02-24 19:17:02 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.1708
2022-02-24 19:17:36 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.3037
2022-02-24 19:18:10 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.4906
2022-02-24 19:18:44 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.1970
2022-02-24 19:19:19 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.2004
2022-02-24 19:19:54 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.3064
2022-02-24 19:20:27 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.4011
2022-02-24 19:20:29 - train: epoch 023, train_loss: 2.3576
2022-02-24 19:21:44 - eval: epoch: 023, acc1: 50.450%, acc5: 76.470%, test_loss: 2.1351, per_image_load_time: 2.535ms, per_image_inference_time: 0.356ms
2022-02-24 19:21:45 - until epoch: 023, best_acc1: 51.530%
2022-02-24 19:21:45 - epoch 024 lr: 0.1
2022-02-24 19:22:23 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.3513
2022-02-24 19:22:57 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.3763
2022-02-24 19:23:30 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.2791
2022-02-24 19:24:05 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.3766
2022-02-24 19:24:37 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.2647
2022-02-24 19:25:11 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.0487
2022-02-24 19:25:44 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.2922
2022-02-24 19:26:19 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.2963
2022-02-24 19:26:52 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.3155
2022-02-24 19:27:26 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.3637
2022-02-24 19:27:59 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.0875
2022-02-24 19:28:32 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.1598
2022-02-24 19:29:06 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.7126
2022-02-24 19:29:40 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.1893
2022-02-24 19:30:14 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.5515
2022-02-24 19:30:48 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.3882
2022-02-24 19:31:21 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.3613
2022-02-24 19:31:55 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.4811
2022-02-24 19:32:28 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.1931
2022-02-24 19:33:02 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.3357
2022-02-24 19:33:36 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.2885
2022-02-24 19:34:09 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.1307
2022-02-24 19:34:43 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.3722
2022-02-24 19:35:16 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.2917
2022-02-24 19:35:50 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.2797
2022-02-24 19:36:24 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.2439
2022-02-24 19:36:57 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.4165
2022-02-24 19:37:31 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.4083
2022-02-24 19:38:03 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.5378
2022-02-24 19:38:38 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.1960
2022-02-24 19:39:11 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.1857
2022-02-24 19:39:45 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.4976
2022-02-24 19:40:19 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.0286
2022-02-24 19:40:53 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.2765
2022-02-24 19:41:27 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.2678
2022-02-24 19:42:01 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.4408
2022-02-24 19:42:34 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.3310
2022-02-24 19:43:09 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.6445
2022-02-24 19:43:43 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.3999
2022-02-24 19:44:17 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.4578
2022-02-24 19:44:50 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.3127
2022-02-24 19:45:23 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.4255
2022-02-24 19:45:57 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.3412
2022-02-24 19:46:31 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.3187
2022-02-24 19:47:05 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.2693
2022-02-24 19:47:39 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.3835
2022-02-24 19:48:15 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.3229
2022-02-24 19:48:48 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.2606
2022-02-24 19:49:24 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.3196
2022-02-24 19:49:57 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.5055
2022-02-24 19:49:59 - train: epoch 024, train_loss: 2.3543
2022-02-24 19:51:14 - eval: epoch: 024, acc1: 51.224%, acc5: 76.766%, test_loss: 2.1130, per_image_load_time: 2.522ms, per_image_inference_time: 0.324ms
2022-02-24 19:51:14 - until epoch: 024, best_acc1: 51.530%
2022-02-24 19:51:14 - epoch 025 lr: 0.1
2022-02-24 19:51:53 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.2255
2022-02-24 19:52:26 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.0759
2022-02-24 19:52:59 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.1347
2022-02-24 19:53:33 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.2783
2022-02-24 19:54:07 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.2140
2022-02-24 19:54:40 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.3079
2022-02-24 19:55:14 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.4206
2022-02-24 19:55:47 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.3904
2022-02-24 19:56:21 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.2083
2022-02-24 19:56:54 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.1770
2022-02-24 19:57:27 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.3290
2022-02-24 19:58:02 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.3441
2022-02-24 19:58:34 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.3955
2022-02-24 19:59:08 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.4662
2022-02-24 19:59:41 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.3458
2022-02-24 20:00:15 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.0861
2022-02-24 20:00:48 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.3163
2022-02-24 20:01:22 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.2082
2022-02-24 20:01:56 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.2354
2022-02-24 20:02:31 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.2766
2022-02-24 20:03:04 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.0976
2022-02-24 20:03:38 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.1563
2022-02-24 20:04:11 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.3513
2022-02-24 20:04:46 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.0795
2022-02-24 20:05:19 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.4292
2022-02-24 20:05:53 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.4435
2022-02-24 20:06:27 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.3618
2022-02-24 20:07:00 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.3054
2022-02-24 20:07:34 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.5185
2022-02-24 20:08:07 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.4811
2022-02-24 20:08:41 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.1791
2022-02-24 20:09:14 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.4649
2022-02-24 20:09:49 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.3621
2022-02-24 20:10:22 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.4465
2022-02-24 20:10:56 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.0906
2022-02-24 20:11:30 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.5064
2022-02-24 20:12:04 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.2198
2022-02-24 20:12:37 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.3261
2022-02-24 20:13:10 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.4668
2022-02-24 20:13:43 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.4131
2022-02-24 20:14:18 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.5347
2022-02-24 20:14:51 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.3798
2022-02-24 20:15:25 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.2410
2022-02-24 20:15:59 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.2644
2022-02-24 20:16:34 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.3513
2022-02-24 20:17:08 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.3417
2022-02-24 20:17:42 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.2600
2022-02-24 20:18:16 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.0912
2022-02-24 20:18:51 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.2045
2022-02-24 20:19:24 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.5050
2022-02-24 20:19:26 - train: epoch 025, train_loss: 2.3482
2022-02-24 20:20:42 - eval: epoch: 025, acc1: 51.336%, acc5: 77.240%, test_loss: 2.0925, per_image_load_time: 2.551ms, per_image_inference_time: 0.355ms
2022-02-24 20:20:43 - until epoch: 025, best_acc1: 51.530%
2022-02-24 20:20:43 - epoch 026 lr: 0.1
2022-02-24 20:21:21 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.1483
2022-02-24 20:21:55 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.2013
2022-02-24 20:22:28 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.3045
2022-02-24 20:23:01 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.2555
2022-02-24 20:23:34 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.4337
2022-02-24 20:24:07 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.4323
2022-02-24 20:24:41 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.2050
2022-02-24 20:25:14 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.1689
2022-02-24 20:25:48 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.5909
2022-02-24 20:26:22 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.1993
2022-02-24 20:26:55 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.3371
2022-02-24 20:27:29 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.4688
2022-02-24 20:28:02 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.2351
2022-02-24 20:28:36 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.4520
2022-02-24 20:29:10 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.3294
2022-02-24 20:29:44 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.5042
2022-02-24 20:30:17 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.2135
2022-02-24 20:30:51 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.4612
2022-02-24 20:31:24 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.6479
2022-02-24 20:31:57 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.5476
2022-02-24 20:32:31 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.3687
2022-02-24 20:33:06 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.3512
2022-02-24 20:33:40 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.2950
2022-02-24 20:34:15 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.5649
2022-02-24 20:34:48 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.5097
2022-02-24 20:35:22 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.3122
2022-02-24 20:35:55 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.2418
2022-02-24 20:36:29 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.2242
2022-02-24 20:37:03 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.4093
2022-02-24 20:37:36 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.1908
2022-02-24 20:38:11 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.4299
2022-02-24 20:38:44 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.3479
2022-02-24 20:39:18 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.1345
2022-02-24 20:39:51 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.4552
2022-02-24 20:40:26 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.3714
2022-02-24 20:40:59 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.2561
2022-02-24 20:41:33 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.4380
2022-02-24 20:42:08 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.4890
2022-02-24 20:42:41 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.5270
2022-02-24 20:43:15 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.5742
2022-02-24 20:43:50 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.3414
2022-02-24 20:44:24 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.4439
2022-02-24 20:44:57 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.4872
2022-02-24 20:45:31 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.5157
2022-02-24 20:46:05 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.5793
2022-02-24 20:46:40 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.2527
2022-02-24 20:47:14 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.2598
2022-02-24 20:47:48 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.2978
2022-02-24 20:48:23 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.2931
2022-02-24 20:48:57 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.5254
2022-02-24 20:48:58 - train: epoch 026, train_loss: 2.3444
2022-02-24 20:50:14 - eval: epoch: 026, acc1: 51.918%, acc5: 77.710%, test_loss: 2.0565, per_image_load_time: 2.565ms, per_image_inference_time: 0.326ms
2022-02-24 20:50:15 - until epoch: 026, best_acc1: 51.918%
2022-02-24 20:50:15 - epoch 027 lr: 0.1
2022-02-24 20:50:53 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.4180
2022-02-24 20:51:27 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.2420
2022-02-24 20:52:00 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.4216
2022-02-24 20:52:33 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.3866
2022-02-24 20:53:06 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.3654
2022-02-24 20:53:40 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.6356
2022-02-24 20:54:14 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.6549
2022-02-24 20:54:46 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.3484
2022-02-24 20:55:20 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.2315
2022-02-24 20:55:54 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.3781
2022-02-24 20:56:26 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.2067
2022-02-24 20:57:01 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.6135
2022-02-24 20:57:33 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.4225
2022-02-24 20:58:08 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.5131
2022-02-24 20:58:41 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.4451
2022-02-24 20:59:15 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.3361
2022-02-24 20:59:48 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.2892
2022-02-24 21:00:22 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.3746
2022-02-24 21:00:55 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.4550
2022-02-24 21:01:29 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.2663
2022-02-24 21:02:02 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.5967
2022-02-24 21:02:37 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.4546
2022-02-24 21:03:10 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.6375
2022-02-24 21:03:45 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.3246
2022-02-24 21:04:18 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.1646
2022-02-24 21:04:51 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.6155
2022-02-24 21:05:26 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.3481
2022-02-24 21:05:59 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.5409
2022-02-24 21:06:33 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.4145
2022-02-24 21:07:07 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.2205
2022-02-24 21:07:41 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.2528
2022-02-24 21:08:14 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.1503
2022-02-24 21:08:48 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.4351
2022-02-24 21:09:22 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.3281
2022-02-24 21:09:56 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.6690
2022-02-24 21:10:30 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.1656
2022-02-24 21:11:03 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.3167
2022-02-24 21:11:36 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.1311
2022-02-24 21:12:11 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.1350
2022-02-24 21:12:45 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.3613
2022-02-24 21:13:19 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.2812
2022-02-24 21:13:52 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.4547
2022-02-24 21:14:26 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.2496
2022-02-24 21:15:01 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.3034
2022-02-24 21:15:35 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.1023
2022-02-24 21:16:10 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.3134
2022-02-24 21:16:43 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.3292
2022-02-24 21:17:17 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.5565
2022-02-24 21:17:52 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.3584
2022-02-24 21:18:26 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.1131
2022-02-24 21:18:27 - train: epoch 027, train_loss: 2.3413
2022-02-24 21:19:44 - eval: epoch: 027, acc1: 51.682%, acc5: 77.284%, test_loss: 2.0758, per_image_load_time: 2.599ms, per_image_inference_time: 0.326ms
2022-02-24 21:19:44 - until epoch: 027, best_acc1: 51.918%
2022-02-24 21:19:44 - epoch 028 lr: 0.1
2022-02-24 21:20:23 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.1688
2022-02-24 21:20:56 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.3794
2022-02-24 21:21:29 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.2984
2022-02-24 21:22:03 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.2186
2022-02-24 21:22:36 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.2249
2022-02-24 21:23:10 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.5521
2022-02-24 21:23:43 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.6341
2022-02-24 21:24:17 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.0406
2022-02-24 21:24:50 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.0563
2022-02-24 21:25:24 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.4070
2022-02-24 21:25:57 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.3439
2022-02-24 21:26:31 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.2465
2022-02-24 21:27:05 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.3328
2022-02-24 21:27:38 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.6813
2022-02-24 21:28:13 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.2913
2022-02-24 21:28:45 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.3220
2022-02-24 21:29:19 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.2990
2022-02-24 21:29:53 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.2069
2022-02-24 21:30:27 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.3460
2022-02-24 21:31:01 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.4363
2022-02-24 21:31:35 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.3363
2022-02-24 21:32:08 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.3339
2022-02-24 21:32:42 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.4760
2022-02-24 21:33:15 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.5794
2022-02-24 21:33:50 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.3112
2022-02-24 21:34:23 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.1717
2022-02-24 21:34:57 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.3628
2022-02-24 21:35:32 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.2837
2022-02-24 21:36:04 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.2788
2022-02-24 21:36:39 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.4725
2022-02-24 21:37:12 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.5915
2022-02-24 21:37:46 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.4075
2022-02-24 21:38:20 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.4140
2022-02-24 21:38:53 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.3426
2022-02-24 21:39:27 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.1893
2022-02-24 21:40:01 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.2788
2022-02-24 21:40:36 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.3019
2022-02-24 21:41:09 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.0370
2022-02-24 21:41:44 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.3668
2022-02-24 21:42:18 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.4529
2022-02-24 21:42:51 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.4004
2022-02-24 21:43:24 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.4754
2022-02-24 21:43:59 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.3226
2022-02-24 21:44:34 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.2992
2022-02-24 21:45:08 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.2967
2022-02-24 21:45:42 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.3844
2022-02-24 21:46:15 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.3487
2022-02-24 21:46:50 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.3425
2022-02-24 21:47:25 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.2492
2022-02-24 21:47:59 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.1974
2022-02-24 21:48:01 - train: epoch 028, train_loss: 2.3341
2022-02-24 21:49:16 - eval: epoch: 028, acc1: 51.154%, acc5: 76.930%, test_loss: 2.1121, per_image_load_time: 2.545ms, per_image_inference_time: 0.326ms
2022-02-24 21:49:16 - until epoch: 028, best_acc1: 51.918%
2022-02-24 23:56:08 - epoch 029 lr: 0.1
2022-02-24 23:56:47 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.1521
2022-02-24 23:57:20 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.2457
2022-02-24 23:57:53 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.5763
2022-02-24 23:58:26 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.5341
2022-02-24 23:59:00 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.3080
2022-02-24 23:59:34 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.4649
2022-02-25 00:00:07 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.0914
2022-02-25 00:00:41 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.1467
2022-02-25 00:01:14 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.1353
2022-02-25 00:01:48 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.1892
2022-02-25 00:02:22 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.3258
2022-02-25 00:02:55 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.3905
2022-02-25 00:03:29 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.2521
2022-02-25 00:04:03 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.3345
2022-02-25 00:04:36 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.3519
2022-02-25 00:05:10 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.4457
2022-02-25 00:05:43 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.3241
2022-02-25 00:06:17 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.5088
2022-02-25 00:06:51 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.1784
2022-02-25 00:07:25 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.1440
2022-02-25 00:07:58 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.3370
2022-02-25 00:08:31 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.2918
2022-02-25 00:09:05 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.5033
2022-02-25 00:09:39 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.2530
2022-02-25 00:10:13 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.5197
2022-02-25 00:10:47 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.5822
2022-02-25 00:11:19 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.1737
2022-02-25 00:11:55 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.3256
2022-02-25 00:12:28 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.5249
2022-02-25 00:13:01 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.2695
2022-02-25 00:13:35 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.3789
2022-02-25 00:14:08 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.2506
2022-02-25 00:14:42 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.1765
2022-02-25 00:15:15 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.2903
2022-02-25 00:15:49 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.2573
2022-02-25 00:16:23 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.0868
2022-02-25 00:16:56 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.3565
2022-02-25 00:17:30 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.2131
2022-02-25 00:18:04 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.1576
2022-02-25 00:18:38 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.5635
2022-02-25 00:19:11 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.4438
2022-02-25 00:19:45 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.1890
2022-02-25 00:20:19 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.2844
2022-02-25 00:20:53 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.2790
2022-02-25 00:21:27 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.2785
2022-02-25 00:22:01 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.4995
2022-02-25 00:22:35 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.1102
2022-02-25 00:23:10 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.4168
2022-02-25 00:23:44 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.3436
2022-02-25 00:24:16 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.1200
2022-02-25 00:24:17 - train: epoch 029, train_loss: 2.3318
2022-02-25 00:25:33 - eval: epoch: 029, acc1: 52.624%, acc5: 78.258%, test_loss: 2.0279, per_image_load_time: 1.393ms, per_image_inference_time: 0.288ms
2022-02-25 00:25:34 - until epoch: 029, best_acc1: 52.624%
2022-02-25 00:25:34 - epoch 030 lr: 0.1
2022-02-25 00:26:12 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.3396
2022-02-25 00:26:45 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.5223
2022-02-25 00:27:19 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.1180
2022-02-25 00:27:52 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.2830
2022-02-25 00:28:25 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.2893
2022-02-25 00:28:59 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.3270
2022-02-25 00:29:32 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.2738
2022-02-25 00:30:06 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.3818
2022-02-25 00:30:39 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.2816
2022-02-25 00:31:13 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.2754
2022-02-25 00:31:47 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.2319
2022-02-25 00:32:20 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.3576
2022-02-25 00:32:53 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.1745
2022-02-25 00:33:27 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.1623
2022-02-25 00:34:01 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.1290
2022-02-25 00:34:35 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.3014
2022-02-25 00:35:08 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.5228
2022-02-25 00:35:41 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.2916
2022-02-25 00:36:15 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.1648
2022-02-25 00:36:49 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.3582
2022-02-25 00:37:23 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.5079
2022-02-25 00:37:56 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.4725
2022-02-25 00:38:30 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.2578
2022-02-25 00:39:04 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.4360
2022-02-25 00:39:38 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.2544
2022-02-25 00:40:12 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.4143
2022-02-25 00:40:45 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.4784
2022-02-25 00:41:18 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.2407
2022-02-25 00:41:52 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.5031
2022-02-25 00:42:25 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.5366
2022-02-25 00:42:59 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.4015
2022-02-25 00:43:33 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.3001
2022-02-25 00:44:06 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.3750
2022-02-25 00:44:41 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.2302
2022-02-25 00:45:15 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.2679
2022-02-25 00:45:48 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.2853
2022-02-25 00:46:22 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.1147
2022-02-25 00:46:55 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.4924
2022-02-25 00:47:29 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.3435
2022-02-25 00:48:03 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.1847
2022-02-25 00:48:38 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.4700
2022-02-25 00:49:11 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.4662
2022-02-25 00:49:45 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.3190
2022-02-25 00:50:20 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.2771
2022-02-25 00:50:53 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.3378
2022-02-25 00:51:27 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.1851
2022-02-25 00:52:02 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.2976
2022-02-25 00:52:35 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.3183
2022-02-25 00:53:10 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.5416
2022-02-25 00:53:44 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.2748
2022-02-25 00:53:45 - train: epoch 030, train_loss: 2.3302
2022-02-25 00:55:00 - eval: epoch: 030, acc1: 52.562%, acc5: 78.024%, test_loss: 2.0344, per_image_load_time: 2.393ms, per_image_inference_time: 0.309ms
2022-02-25 00:55:01 - until epoch: 030, best_acc1: 52.624%
2022-02-25 00:55:01 - epoch 031 lr: 0.010000000000000002
2022-02-25 00:55:39 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.1502
2022-02-25 00:56:13 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.8589
2022-02-25 00:56:46 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.7738
2022-02-25 00:57:20 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.7731
2022-02-25 00:57:53 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.8658
2022-02-25 00:58:26 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.8053
2022-02-25 00:58:59 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.8626
2022-02-25 00:59:34 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.8943
2022-02-25 01:00:07 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.8090
2022-02-25 01:00:42 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 1.8366
2022-02-25 01:01:15 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 1.7617
2022-02-25 01:01:49 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.8172
2022-02-25 01:02:23 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.5129
2022-02-25 01:02:57 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.6011
2022-02-25 01:03:31 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.9091
2022-02-25 01:04:05 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.7525
2022-02-25 01:04:39 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.7578
2022-02-25 01:05:13 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.4821
2022-02-25 01:05:47 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.5623
2022-02-25 01:06:20 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.7600
2022-02-25 01:06:53 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.7380
2022-02-25 01:07:27 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.4741
2022-02-25 01:08:01 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.6652
2022-02-25 01:08:35 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.7534
2022-02-25 01:09:10 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.7220
2022-02-25 01:09:43 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.6473
2022-02-25 01:10:17 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.8655
2022-02-25 01:10:50 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.0565
2022-02-25 01:11:25 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.8278
2022-02-25 01:11:58 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.0666
2022-02-25 01:12:32 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.8731
2022-02-25 01:13:06 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.9033
2022-02-25 01:13:40 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.8888
2022-02-25 01:14:15 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.7798
2022-02-25 01:14:48 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.6830
2022-02-25 01:15:22 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.7289
2022-02-25 01:15:55 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.5687
2022-02-25 01:16:30 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.6028
2022-02-25 01:17:04 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.7220
2022-02-25 01:17:37 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.6145
2022-02-25 01:18:11 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.7639
2022-02-25 01:18:45 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.6416
2022-02-25 01:19:19 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.6687
2022-02-25 01:19:53 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.7626
2022-02-25 01:20:28 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.7227
2022-02-25 01:21:02 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.6228
2022-02-25 01:21:37 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.5124
2022-02-25 01:22:11 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.7589
2022-02-25 01:22:47 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.4934
2022-02-25 01:23:20 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.5285
2022-02-25 01:23:21 - train: epoch 031, train_loss: 1.7690
2022-02-25 01:24:36 - eval: epoch: 031, acc1: 66.196%, acc5: 87.274%, test_loss: 1.3809, per_image_load_time: 2.614ms, per_image_inference_time: 0.308ms
2022-02-25 01:24:37 - until epoch: 031, best_acc1: 66.196%
2022-02-25 01:24:37 - epoch 032 lr: 0.010000000000000002
2022-02-25 01:25:16 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.5487
2022-02-25 01:25:48 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.7907
2022-02-25 01:26:22 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.4894
2022-02-25 01:26:55 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.8138
2022-02-25 01:27:29 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.7394
2022-02-25 01:28:02 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.8314
2022-02-25 01:28:36 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.8067
2022-02-25 01:29:09 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.5682
2022-02-25 01:29:43 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.7283
2022-02-25 01:30:17 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.7479
2022-02-25 01:30:50 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.7153
2022-02-25 01:31:24 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.7111
2022-02-25 01:31:59 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.6266
2022-02-25 01:32:33 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.7605
2022-02-25 01:33:06 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.5853
2022-02-25 01:33:39 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.6623
2022-02-25 01:34:14 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.6865
2022-02-25 01:34:47 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.8367
2022-02-25 01:35:21 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.4554
2022-02-25 01:35:55 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.7170
2022-02-25 01:36:29 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.6365
2022-02-25 01:37:02 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.6914
2022-02-25 01:37:36 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.6320
2022-02-25 01:38:10 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.4893
2022-02-25 01:38:44 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.7188
2022-02-25 01:39:18 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.5909
2022-02-25 01:39:51 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.6406
2022-02-25 01:40:26 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.5688
2022-02-25 01:40:59 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.6599
2022-02-25 01:41:34 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.5564
2022-02-25 01:42:06 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.7474
2022-02-25 01:42:40 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.6808
2022-02-25 01:43:14 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.4553
2022-02-25 01:43:49 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.4357
2022-02-25 01:44:22 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.5904
2022-02-25 01:44:57 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.5947
2022-02-25 01:45:31 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.7232
2022-02-25 01:46:05 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.4639
2022-02-25 01:46:38 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.6715
2022-02-25 01:47:13 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.6430
2022-02-25 01:47:45 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.6663
2022-02-25 01:48:20 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.4748
2022-02-25 01:48:54 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.8690
2022-02-25 01:49:28 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.5267
2022-02-25 01:50:02 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.8336
2022-02-25 01:50:35 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.6252
2022-02-25 01:51:11 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.7046
2022-02-25 01:51:45 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.7210
2022-02-25 01:52:20 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.6787
2022-02-25 01:52:54 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.5626
2022-02-25 01:52:55 - train: epoch 032, train_loss: 1.6497
2022-02-25 01:54:12 - eval: epoch: 032, acc1: 66.980%, acc5: 87.866%, test_loss: 1.3406, per_image_load_time: 2.717ms, per_image_inference_time: 0.292ms
2022-02-25 01:54:13 - until epoch: 032, best_acc1: 66.980%
2022-02-25 01:54:13 - epoch 033 lr: 0.010000000000000002
2022-02-25 01:54:52 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.3396
2022-02-25 01:55:25 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.9035
2022-02-25 01:55:58 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.5537
2022-02-25 01:56:32 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.5551
2022-02-25 01:57:05 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.6867
2022-02-25 01:57:39 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.3543
2022-02-25 01:58:12 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.9030
2022-02-25 01:58:46 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.7891
2022-02-25 01:59:20 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.6671
2022-02-25 01:59:53 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.6984
2022-02-25 02:00:27 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.5913
2022-02-25 02:01:00 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.8825
2022-02-25 02:01:34 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.3638
2022-02-25 02:02:08 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.7069
2022-02-25 02:02:41 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.6487
2022-02-25 02:03:14 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.5969
2022-02-25 02:03:48 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.7075
2022-02-25 02:04:22 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.6816
2022-02-25 02:04:56 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.6333
2022-02-25 02:05:29 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.3878
2022-02-25 02:06:03 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.6613
2022-02-25 02:06:37 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.5480
2022-02-25 02:07:11 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.4080
2022-02-25 02:07:45 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.6632
2022-02-25 02:08:19 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.6227
2022-02-25 02:08:52 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.3637
2022-02-25 02:09:27 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.5089
2022-02-25 02:10:00 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.7091
2022-02-25 02:10:34 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.5828
2022-02-25 02:11:08 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.6429
2022-02-25 02:11:42 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.5914
2022-02-25 02:12:15 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.6789
2022-02-25 02:12:49 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.5960
2022-02-25 02:13:23 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.5049
2022-02-25 02:13:57 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.5786
2022-02-25 02:14:30 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.8052
2022-02-25 02:15:04 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.4671
2022-02-25 02:15:38 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.6879
2022-02-25 02:16:12 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.9626
2022-02-25 02:16:45 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.6248
2022-02-25 02:17:19 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.5760
2022-02-25 02:17:53 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.4946
2022-02-25 02:18:28 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.8002
2022-02-25 02:19:01 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.5399
2022-02-25 02:19:35 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.9543
2022-02-25 02:20:10 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.6894
2022-02-25 02:20:44 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.4022
2022-02-25 02:21:18 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.7305
2022-02-25 02:21:53 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.4377
2022-02-25 02:22:27 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.6444
2022-02-25 02:22:28 - train: epoch 033, train_loss: 1.6067
2022-02-25 02:23:43 - eval: epoch: 033, acc1: 67.254%, acc5: 88.246%, test_loss: 1.3206, per_image_load_time: 1.233ms, per_image_inference_time: 0.309ms
2022-02-25 02:23:44 - until epoch: 033, best_acc1: 67.254%
2022-02-25 02:23:44 - epoch 034 lr: 0.010000000000000002
2022-02-25 02:24:23 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.5351
2022-02-25 02:24:56 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.6032
2022-02-25 02:25:29 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.3801
2022-02-25 02:26:02 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.2875
2022-02-25 02:26:35 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.4670
2022-02-25 02:27:09 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.6608
2022-02-25 02:27:43 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.5151
2022-02-25 02:28:16 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.6117
2022-02-25 02:28:49 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.5484
2022-02-25 02:29:23 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.4936
2022-02-25 02:29:56 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.5612
2022-02-25 02:30:32 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.6367
2022-02-25 02:31:03 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.6893
2022-02-25 02:31:37 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.5886
2022-02-25 02:32:10 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.3771
2022-02-25 02:32:45 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.5879
2022-02-25 02:33:18 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.7626
2022-02-25 02:33:51 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.5217
2022-02-25 02:34:24 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.5997
2022-02-25 02:34:57 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.4855
2022-02-25 02:35:31 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.6634
2022-02-25 02:36:07 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.5590
2022-02-25 02:36:39 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.6393
2022-02-25 02:37:13 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.5268
2022-02-25 02:37:46 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.7079
2022-02-25 02:38:20 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.8328
2022-02-25 02:38:54 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.7017
2022-02-25 02:39:28 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.3779
2022-02-25 02:40:00 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.4279
2022-02-25 02:40:34 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.4317
2022-02-25 02:41:07 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.7020
2022-02-25 02:41:41 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.5674
2022-02-25 02:42:15 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.5029
2022-02-25 02:42:50 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.7026
2022-02-25 02:43:24 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.4368
2022-02-25 02:43:58 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.5385
2022-02-25 02:44:31 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.6526
2022-02-25 02:45:06 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.6000
2022-02-25 02:45:38 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.6838
2022-02-25 02:46:13 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.5615
2022-02-25 02:46:46 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.6668
2022-02-25 02:47:21 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.4319
2022-02-25 02:47:54 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.5937
2022-02-25 02:48:29 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.6173
2022-02-25 02:49:03 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.5066
2022-02-25 02:49:38 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.6278
2022-02-25 02:50:11 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.7443
2022-02-25 02:50:47 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.5720
2022-02-25 02:51:21 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.7044
2022-02-25 02:51:55 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.6067
2022-02-25 02:51:56 - train: epoch 034, train_loss: 1.5778
2022-02-25 02:53:11 - eval: epoch: 034, acc1: 67.716%, acc5: 88.324%, test_loss: 1.3058, per_image_load_time: 0.620ms, per_image_inference_time: 0.302ms
2022-02-25 02:53:12 - until epoch: 034, best_acc1: 67.716%
2022-02-25 02:53:12 - epoch 035 lr: 0.010000000000000002
2022-02-25 02:53:51 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.2897
2022-02-25 02:54:24 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.3963
2022-02-25 02:54:58 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.5519
2022-02-25 02:55:32 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.4471
2022-02-25 02:56:05 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.6127
2022-02-25 02:56:38 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.5868
2022-02-25 02:57:11 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.5042
2022-02-25 02:57:45 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.4899
2022-02-25 02:58:18 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.6236
2022-02-25 02:58:52 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.4338
2022-02-25 02:59:25 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.5947
2022-02-25 02:59:59 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.5437
2022-02-25 03:00:32 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.7140
2022-02-25 03:01:05 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.6309
2022-02-25 03:01:39 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.5843
2022-02-25 03:02:13 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.5093
2022-02-25 03:02:46 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.4514
2022-02-25 03:03:21 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.6521
2022-02-25 03:03:55 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.5540
2022-02-25 03:04:29 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.7996
2022-02-25 03:05:01 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.4889
2022-02-25 03:05:34 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.6864
2022-02-25 03:06:10 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.5234
2022-02-25 03:06:43 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.6006
2022-02-25 03:07:17 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.6139
2022-02-25 03:07:50 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.7789
2022-02-25 03:08:24 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.5085
2022-02-25 03:08:58 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.5027
2022-02-25 03:09:31 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.5274
2022-02-25 03:10:06 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.3013
2022-02-25 03:10:39 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.5774
2022-02-25 03:11:13 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.5407
2022-02-25 03:11:47 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.4277
2022-02-25 03:12:21 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.4209
2022-02-25 03:12:55 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.3654
2022-02-25 03:13:29 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.5304
2022-02-25 03:14:03 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.4273
2022-02-25 03:14:37 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.4894
2022-02-25 03:15:12 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.6744
2022-02-25 03:15:45 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.3869
2022-02-25 03:16:19 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.6529
2022-02-25 03:16:53 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.4073
2022-02-25 03:17:27 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.5026
2022-02-25 03:18:01 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.5569
2022-02-25 03:18:34 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.6839
2022-02-25 03:19:09 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.5927
2022-02-25 03:19:43 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.6544
2022-02-25 03:20:17 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.5662
2022-02-25 03:20:52 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.5793
2022-02-25 03:21:26 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.5711
2022-02-25 03:21:27 - train: epoch 035, train_loss: 1.5586
2022-02-25 03:22:43 - eval: epoch: 035, acc1: 67.626%, acc5: 88.338%, test_loss: 1.3022, per_image_load_time: 1.142ms, per_image_inference_time: 0.327ms
2022-02-25 03:22:43 - until epoch: 035, best_acc1: 67.716%
2022-02-25 03:22:43 - epoch 036 lr: 0.010000000000000002
2022-02-25 03:23:20 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.7019
2022-02-25 03:23:55 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.3202
2022-02-25 03:24:28 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.4022
2022-02-25 03:25:02 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.4337
2022-02-25 03:25:35 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.5003
2022-02-25 03:26:09 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.5815
2022-02-25 03:26:42 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.5326
2022-02-25 03:27:15 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.5341
2022-02-25 03:27:48 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.5939
2022-02-25 03:28:21 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.4676
2022-02-25 03:28:55 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.5076
2022-02-25 03:29:29 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.3484
2022-02-25 03:30:03 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.3877
2022-02-25 03:30:35 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.5255
2022-02-25 03:31:09 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.5456
2022-02-25 03:31:43 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.4733
2022-02-25 03:32:16 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.3733
2022-02-25 03:32:50 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.4770
2022-02-25 03:33:24 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.4436
2022-02-25 03:33:57 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.4714
2022-02-25 03:34:30 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.3227
2022-02-25 03:35:05 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.8031
2022-02-25 03:35:38 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.6218
2022-02-25 03:36:12 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.6179
2022-02-25 03:36:45 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.2843
2022-02-25 03:37:19 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.6164
2022-02-25 03:37:52 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.5234
2022-02-25 03:38:26 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.4850
2022-02-25 03:38:59 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.4265
2022-02-25 03:39:34 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.5055
2022-02-25 03:40:07 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.6437
2022-02-25 03:40:41 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.6449
2022-02-25 03:41:16 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.4413
2022-02-25 03:41:49 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.4541
2022-02-25 03:42:22 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.7008
2022-02-25 03:42:56 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.7960
2022-02-25 03:43:30 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.4987
2022-02-25 03:44:04 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.5442
2022-02-25 03:44:38 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.4258
2022-02-25 03:45:12 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.7145
2022-02-25 03:45:46 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.6764
2022-02-25 03:46:20 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.5574
2022-02-25 03:46:54 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.5007
2022-02-25 03:47:28 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.5389
2022-02-25 03:48:03 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.6741
2022-02-25 03:48:36 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.4690
2022-02-25 03:49:11 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.5811
2022-02-25 03:49:46 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.4840
2022-02-25 03:50:21 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.6039
2022-02-25 03:50:54 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.4437
2022-02-25 03:50:56 - train: epoch 036, train_loss: 1.5462
2022-02-25 03:52:12 - eval: epoch: 036, acc1: 67.860%, acc5: 88.404%, test_loss: 1.3010, per_image_load_time: 0.975ms, per_image_inference_time: 0.301ms
2022-02-25 03:52:12 - until epoch: 036, best_acc1: 67.860%
2022-02-25 03:52:12 - epoch 037 lr: 0.010000000000000002
2022-02-25 03:52:51 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.4573
2022-02-25 03:53:24 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.3249
2022-02-25 03:53:57 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.4508
2022-02-25 03:54:32 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.3658
2022-02-25 03:55:05 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.4547
2022-02-25 03:55:38 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.6530
2022-02-25 03:56:12 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.7376
2022-02-25 03:56:45 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.4803
2022-02-25 03:57:18 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.8278
2022-02-25 03:57:52 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.4907
2022-02-25 03:58:26 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.5236
2022-02-25 03:59:00 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.6393
2022-02-25 03:59:34 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.7431
2022-02-25 04:00:08 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.5457
2022-02-25 04:00:41 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.3455
2022-02-25 04:01:15 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.4015
2022-02-25 04:01:48 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.6040
2022-02-25 04:02:22 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.4392
2022-02-25 04:02:56 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.4109
2022-02-25 04:03:29 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.5039
2022-02-25 04:04:04 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.4937
2022-02-25 04:04:38 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.5376
2022-02-25 04:05:11 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.5891
2022-02-25 04:05:45 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.4639
2022-02-25 04:06:18 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.5684
2022-02-25 04:06:53 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.7951
2022-02-25 04:07:27 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.6740
2022-02-25 04:08:00 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.5771
2022-02-25 04:08:34 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.4843
2022-02-25 04:09:08 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.8422
2022-02-25 04:09:42 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.4537
2022-02-25 04:10:16 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.4640
2022-02-25 04:10:49 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.4123
2022-02-25 04:11:23 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.4538
2022-02-25 04:11:57 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.5247
2022-02-25 04:12:30 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.5588
2022-02-25 04:13:04 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.5098
2022-02-25 04:13:38 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.6317
2022-02-25 04:14:11 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.5577
2022-02-25 04:14:45 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.5228
2022-02-25 04:15:18 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.3600
2022-02-25 04:15:52 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.7717
2022-02-25 04:16:26 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.7193
2022-02-25 04:17:00 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.4886
2022-02-25 04:17:34 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.4344
2022-02-25 04:18:08 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.6478
2022-02-25 04:18:42 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.5007
2022-02-25 04:19:17 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.4944
2022-02-25 04:19:52 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.4794
2022-02-25 04:20:26 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.5393
2022-02-25 04:20:27 - train: epoch 037, train_loss: 1.5386
2022-02-25 04:21:43 - eval: epoch: 037, acc1: 68.144%, acc5: 88.536%, test_loss: 1.2917, per_image_load_time: 0.579ms, per_image_inference_time: 0.291ms
2022-02-25 04:21:43 - until epoch: 037, best_acc1: 68.144%
2022-02-25 04:21:43 - epoch 038 lr: 0.010000000000000002
2022-02-25 04:22:22 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.3245
2022-02-25 04:22:56 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.4718
2022-02-25 04:23:28 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.3048
2022-02-25 04:24:02 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.3614
2022-02-25 04:24:35 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.3241
2022-02-25 04:25:08 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.7988
2022-02-25 04:25:42 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.3221
2022-02-25 04:26:16 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.4167
2022-02-25 04:26:50 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.5427
2022-02-25 04:27:23 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.7685
2022-02-25 04:27:56 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.5170
2022-02-25 04:28:30 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.6523
2022-02-25 04:29:04 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.4429
2022-02-25 04:29:37 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.5559
2022-02-25 04:30:12 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.5155
2022-02-25 04:30:45 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.6477
2022-02-25 04:31:19 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.7576
2022-02-25 04:31:52 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.6587
2022-02-25 04:32:26 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.4565
2022-02-25 04:32:59 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.5405
2022-02-25 04:33:33 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.4552
2022-02-25 04:34:07 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.5115
2022-02-25 04:34:40 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.5221
2022-02-25 04:35:14 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.6076
2022-02-25 04:35:48 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.3165
2022-02-25 04:36:21 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.6930
2022-02-25 04:36:55 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.3656
2022-02-25 04:37:29 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.8560
2022-02-25 04:38:03 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.5019
2022-02-25 04:38:37 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.5598
2022-02-25 04:39:11 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.8130
2022-02-25 04:39:44 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.2069
2022-02-25 04:40:17 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.5002
2022-02-25 04:40:51 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.5897
2022-02-25 04:41:25 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.6002
2022-02-25 04:41:59 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.5033
2022-02-25 04:42:32 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.3510
2022-02-25 04:43:07 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.6714
2022-02-25 04:43:41 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.5449
2022-02-25 04:44:14 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.6195
2022-02-25 04:44:47 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.6207
2022-02-25 04:45:21 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.2505
2022-02-25 04:45:56 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.5069
2022-02-25 04:46:30 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.6809
2022-02-25 04:47:04 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.7174
2022-02-25 04:47:38 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.5791
2022-02-25 04:48:12 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.4364
2022-02-25 04:48:47 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.3987
2022-02-25 04:49:20 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.5622
2022-02-25 04:49:54 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.1672
2022-02-25 04:49:55 - train: epoch 038, train_loss: 1.5352
2022-02-25 04:51:12 - eval: epoch: 038, acc1: 68.144%, acc5: 88.622%, test_loss: 1.2835, per_image_load_time: 0.640ms, per_image_inference_time: 0.304ms
2022-02-25 04:51:13 - until epoch: 038, best_acc1: 68.144%
2022-02-25 04:51:13 - epoch 039 lr: 0.010000000000000002
2022-02-25 04:51:52 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.5347
2022-02-25 04:52:25 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.6889
2022-02-25 04:52:58 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.3050
2022-02-25 04:53:32 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.6276
2022-02-25 04:54:05 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.5175
2022-02-25 04:54:39 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.5203
2022-02-25 04:55:12 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.5997
2022-02-25 04:55:46 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.3914
2022-02-25 04:56:18 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.5805
2022-02-25 04:56:52 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.5655
2022-02-25 04:57:26 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.7376
2022-02-25 04:57:59 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.5013
2022-02-25 04:58:32 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.7019
2022-02-25 04:59:06 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.5509
2022-02-25 04:59:40 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.4479
2022-02-25 05:00:13 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.4977
2022-02-25 05:00:47 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.5063
2022-02-25 05:01:21 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.5463
2022-02-25 05:01:54 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.3978
2022-02-25 05:02:29 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.4710
2022-02-25 05:03:02 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.7323
2022-02-25 05:03:36 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.5065
2022-02-25 05:04:11 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.9700
2022-02-25 05:04:44 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.5844
2022-02-25 05:05:17 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.5262
2022-02-25 05:05:51 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.4604
2022-02-25 05:06:25 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.4081
2022-02-25 05:06:58 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.5195
2022-02-25 05:07:33 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.2738
2022-02-25 05:08:07 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.5948
2022-02-25 05:08:40 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.4680
2022-02-25 05:09:14 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.6498
2022-02-25 05:09:48 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.5196
2022-02-25 05:10:22 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.6025
2022-02-25 05:10:55 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.6453
2022-02-25 05:11:30 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.6414
2022-02-25 05:12:03 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.3741
2022-02-25 05:12:37 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.4233
2022-02-25 05:13:10 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.4840
2022-02-25 05:13:45 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.5724
2022-02-25 05:14:19 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.5192
2022-02-25 05:14:53 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.6140
2022-02-25 05:15:26 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.8852
2022-02-25 05:16:00 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.4945
2022-02-25 05:16:34 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.4097
2022-02-25 05:17:09 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.6374
2022-02-25 05:17:42 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.3590
2022-02-25 05:18:17 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.4762
2022-02-25 05:18:51 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.5511
2022-02-25 05:19:25 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.3192
2022-02-25 05:19:26 - train: epoch 039, train_loss: 1.5323
2022-02-25 05:20:42 - eval: epoch: 039, acc1: 67.516%, acc5: 88.464%, test_loss: 1.2977, per_image_load_time: 1.996ms, per_image_inference_time: 0.319ms
2022-02-25 05:20:43 - until epoch: 039, best_acc1: 68.144%
2022-02-25 05:20:43 - epoch 040 lr: 0.010000000000000002
2022-02-25 05:21:21 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.6985
2022-02-25 05:21:55 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.6004
2022-02-25 05:22:29 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.5227
2022-02-25 05:23:01 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.3558
2022-02-25 05:23:35 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.4288
2022-02-25 05:24:07 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.6440
2022-02-25 05:24:42 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.5895
2022-02-25 05:25:14 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.5446
2022-02-25 05:25:48 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.4510
2022-02-25 05:26:22 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.5331
2022-02-25 05:26:55 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.3517
2022-02-25 05:27:27 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.4229
2022-02-25 05:28:01 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.5588
2022-02-25 05:28:35 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.5620
2022-02-25 05:29:08 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.6333
2022-02-25 05:29:42 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.6408
2022-02-25 05:30:15 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.5085
2022-02-25 05:30:48 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.3716
2022-02-25 05:31:23 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.5191
2022-02-25 05:31:57 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.5338
2022-02-25 05:32:30 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.4848
2022-02-25 05:33:03 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.5352
2022-02-25 05:33:37 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.5341
2022-02-25 05:34:11 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.5523
2022-02-25 05:34:45 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.6450
2022-02-25 05:35:19 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.4528
2022-02-25 05:35:52 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.7579
2022-02-25 05:36:25 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.6564
2022-02-25 05:37:00 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.7355
2022-02-25 05:37:33 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.4353
2022-02-25 05:38:07 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.4498
2022-02-25 05:38:40 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.7062
2022-02-25 05:39:14 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.7364
2022-02-25 05:39:47 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.4951
2022-02-25 05:40:21 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.7228
2022-02-25 05:40:55 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.5695
2022-02-25 05:41:29 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.5161
2022-02-25 05:42:02 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.6089
2022-02-25 05:42:36 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.7190
2022-02-25 05:43:09 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.5610
2022-02-25 05:43:43 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.6188
2022-02-25 05:44:18 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.4965
2022-02-25 05:44:52 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.5049
2022-02-25 05:45:25 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.3783
2022-02-25 05:46:00 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.3883
2022-02-25 05:46:33 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.5110
2022-02-25 05:47:08 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.5107
2022-02-25 05:47:42 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.4934
2022-02-25 05:48:17 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.5354
2022-02-25 05:48:51 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.5010
2022-02-25 05:48:52 - train: epoch 040, train_loss: 1.5290
2022-02-25 05:50:08 - eval: epoch: 040, acc1: 67.734%, acc5: 88.502%, test_loss: 1.2979, per_image_load_time: 1.508ms, per_image_inference_time: 0.333ms
2022-02-25 05:50:08 - until epoch: 040, best_acc1: 68.144%
2022-02-25 05:50:08 - epoch 041 lr: 0.010000000000000002
2022-02-25 05:50:46 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.6799
2022-02-25 05:51:20 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.6944
2022-02-25 05:51:53 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.5746
2022-02-25 05:52:27 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.5046
2022-02-25 05:53:01 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.2020
2022-02-25 05:53:34 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.5638
2022-02-25 05:54:07 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.4560
2022-02-25 05:54:40 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.3553
2022-02-25 05:55:14 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.3029
2022-02-25 05:55:47 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.6257
2022-02-25 05:56:20 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.4630
2022-02-25 05:56:53 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.4214
2022-02-25 05:57:27 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.4225
2022-02-25 05:58:00 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.5819
2022-02-25 05:58:34 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.4796
2022-02-25 05:59:07 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.4053
2022-02-25 05:59:41 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.4426
2022-02-25 06:00:15 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.3183
2022-02-25 06:00:47 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.5313
2022-02-25 06:01:21 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.4059
2022-02-25 06:01:55 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.2553
2022-02-25 06:02:29 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.3495
2022-02-25 06:03:01 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.4311
2022-02-25 06:03:35 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.6958
2022-02-25 06:04:09 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.5157
2022-02-25 06:04:42 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.6412
2022-02-25 06:05:15 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.7958
2022-02-25 06:05:49 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.6185
2022-02-25 06:06:23 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.5321
2022-02-25 06:06:57 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.5561
2022-02-25 06:07:31 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.6213
2022-02-25 06:08:04 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.4330
2022-02-25 06:08:38 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.5414
2022-02-25 06:09:11 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.2804
2022-02-25 06:09:45 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.4522
2022-02-25 06:10:18 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.7209
2022-02-25 06:10:52 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.5404
2022-02-25 06:11:25 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.4852
2022-02-25 06:11:59 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.6162
2022-02-25 06:12:33 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.4983
2022-02-25 06:13:08 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.6352
2022-02-25 06:13:40 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.4021
2022-02-25 06:14:16 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.4533
2022-02-25 06:14:49 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.3532
2022-02-25 06:15:22 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.5478
2022-02-25 06:15:56 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.5184
2022-02-25 06:16:31 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.5757
2022-02-25 06:17:06 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.5026
2022-02-25 06:17:40 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.5699
2022-02-25 06:18:14 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.3981
2022-02-25 06:18:15 - train: epoch 041, train_loss: 1.5263
2022-02-25 06:19:31 - eval: epoch: 041, acc1: 67.524%, acc5: 88.208%, test_loss: 1.3168, per_image_load_time: 0.847ms, per_image_inference_time: 0.312ms
2022-02-25 06:19:31 - until epoch: 041, best_acc1: 68.144%
2022-02-25 06:19:31 - epoch 042 lr: 0.010000000000000002
2022-02-25 06:20:10 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.2221
2022-02-25 06:20:43 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.3563
2022-02-25 06:21:17 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.5526
2022-02-25 06:21:52 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.3854
2022-02-25 06:22:24 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.4252
2022-02-25 06:22:57 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.4000
2022-02-25 06:23:31 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.7022
2022-02-25 06:24:04 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.6325
2022-02-25 06:24:37 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.7128
2022-02-25 06:25:11 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.8005
2022-02-25 06:25:44 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.5119
2022-02-25 06:26:19 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.3738
2022-02-25 06:26:52 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.3759
2022-02-25 06:27:26 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.5278
2022-02-25 06:27:59 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.4573
2022-02-25 06:28:33 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.5733
2022-02-25 06:29:07 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.6534
2022-02-25 06:29:41 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.4962
2022-02-25 06:30:14 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.5092
2022-02-25 06:30:48 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.5970
2022-02-25 06:31:22 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.3239
2022-02-25 06:31:56 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.4959
2022-02-25 06:32:30 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.5537
2022-02-25 06:33:03 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.5864
2022-02-25 06:33:37 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.6182
2022-02-25 06:34:11 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.7663
2022-02-25 06:34:44 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.4810
2022-02-25 06:35:19 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.4754
2022-02-25 06:35:52 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.4701
2022-02-25 06:36:27 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.5030
2022-02-25 06:37:00 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.5237
2022-02-25 06:37:34 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.7108
2022-02-25 06:38:07 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.6172
2022-02-25 06:38:41 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.3840
2022-02-25 06:39:14 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.7312
2022-02-25 06:39:48 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.7559
2022-02-25 06:40:21 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.6124
2022-02-25 06:40:55 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.3115
2022-02-25 06:41:29 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.5940
2022-02-25 06:42:03 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.5853
2022-02-25 06:42:38 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.7167
2022-02-25 06:43:12 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.5947
2022-02-25 06:43:46 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.3539
2022-02-25 06:44:20 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.4249
2022-02-25 06:44:54 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.3954
2022-02-25 06:45:28 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.5586
2022-02-25 06:46:02 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.4616
2022-02-25 06:46:37 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.6346
2022-02-25 06:47:12 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.4392
2022-02-25 06:47:46 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.6518
2022-02-25 06:47:47 - train: epoch 042, train_loss: 1.5287
2022-02-25 06:49:05 - eval: epoch: 042, acc1: 67.778%, acc5: 88.536%, test_loss: 1.3007, per_image_load_time: 2.663ms, per_image_inference_time: 0.317ms
2022-02-25 06:49:05 - until epoch: 042, best_acc1: 68.144%
2022-02-25 06:49:05 - epoch 043 lr: 0.010000000000000002
2022-02-25 06:49:44 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.4937
2022-02-25 06:50:17 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.6914
2022-02-25 06:50:50 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.2485
2022-02-25 06:51:24 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.2775
2022-02-25 06:51:58 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.5103
2022-02-25 06:52:31 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.5428
2022-02-25 06:53:04 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.5062
2022-02-25 06:53:37 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.6576
2022-02-25 06:54:11 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.4288
2022-02-25 06:54:44 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.6330
2022-02-25 06:55:18 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.5916
2022-02-25 06:55:51 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.5657
2022-02-25 06:56:25 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.6918
2022-02-25 06:56:59 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.4199
2022-02-25 06:57:32 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.4000
2022-02-25 06:58:06 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.4177
2022-02-25 06:58:39 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.8765
2022-02-25 06:59:13 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.7113
2022-02-25 06:59:47 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.4636
2022-02-25 07:00:21 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.3393
2022-02-25 07:00:54 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.5448
2022-02-25 07:01:27 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.7350
2022-02-25 07:02:01 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.5482
2022-02-25 07:02:35 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.3099
2022-02-25 07:03:08 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.5619
2022-02-25 07:03:42 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.2794
2022-02-25 07:04:16 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.6258
2022-02-25 07:04:50 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.4233
2022-02-25 07:05:23 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.4899
2022-02-25 07:05:57 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.6686
2022-02-25 07:06:31 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.8002
2022-02-25 07:07:04 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.4800
2022-02-25 07:07:38 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.4296
2022-02-25 07:08:12 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.5182
2022-02-25 07:08:45 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.5529
2022-02-25 07:09:20 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.7929
2022-02-25 07:09:54 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.5244
2022-02-25 07:10:27 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.6299
2022-02-25 07:11:01 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.4623
2022-02-25 07:11:35 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.6053
2022-02-25 07:12:08 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.7055
2022-02-25 07:12:43 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.5045
2022-02-25 07:13:17 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.4897
2022-02-25 07:13:51 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.5590
2022-02-25 07:14:25 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.7065
2022-02-25 07:14:59 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.4474
2022-02-25 07:15:34 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.6767
2022-02-25 07:16:08 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.6325
2022-02-25 07:16:43 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.5486
2022-02-25 07:17:17 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.4653
2022-02-25 07:17:18 - train: epoch 043, train_loss: 1.5306
2022-02-25 07:18:34 - eval: epoch: 043, acc1: 67.426%, acc5: 88.186%, test_loss: 1.3099, per_image_load_time: 1.734ms, per_image_inference_time: 0.314ms
2022-02-25 07:18:34 - until epoch: 043, best_acc1: 68.144%
2022-02-25 07:18:34 - epoch 044 lr: 0.010000000000000002
2022-02-25 07:19:13 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.5655
2022-02-25 07:19:46 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.5374
2022-02-25 07:20:21 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.6574
2022-02-25 07:20:53 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.3172
2022-02-25 07:21:27 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.7622
2022-02-25 07:22:01 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.3563
2022-02-25 07:22:34 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.3956
2022-02-25 07:23:08 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.6462
2022-02-25 07:23:41 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.5257
2022-02-25 07:24:15 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.6332
2022-02-25 07:24:48 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.3565
2022-02-25 07:25:22 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.4887
2022-02-25 07:25:55 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.5928
2022-02-25 07:26:29 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.4844
2022-02-25 07:27:04 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.3832
2022-02-25 07:27:37 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.5453
2022-02-25 07:28:10 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.5827
2022-02-25 07:28:45 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.5060
2022-02-25 07:29:18 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.7522
2022-02-25 07:29:52 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.4291
2022-02-25 07:30:25 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.5858
2022-02-25 07:30:59 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.5005
2022-02-25 07:31:32 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.6151
2022-02-25 07:32:06 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.7598
2022-02-25 07:32:40 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.6686
2022-02-25 07:33:14 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.6734
2022-02-25 07:33:48 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.4127
2022-02-25 07:34:22 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.4348
2022-02-25 07:34:55 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.2975
2022-02-25 07:35:28 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.3399
2022-02-25 07:36:03 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.5720
2022-02-25 07:36:36 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.5130
2022-02-25 07:37:08 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.4079
2022-02-25 07:37:43 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.7816
2022-02-25 07:38:17 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.4067
2022-02-25 07:38:51 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.5963
2022-02-25 07:39:24 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.6028
2022-02-25 07:39:59 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.3920
2022-02-25 07:40:32 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.4712
2022-02-25 07:41:06 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.5737
2022-02-25 07:41:40 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.5069
2022-02-25 07:42:15 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.4161
2022-02-25 07:42:48 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.4542
2022-02-25 07:43:22 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.5840
2022-02-25 07:43:56 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.5260
2022-02-25 07:44:31 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.5444
2022-02-25 07:45:04 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.6919
2022-02-25 07:45:39 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.4704
2022-02-25 07:46:13 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.6176
2022-02-25 07:46:48 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.5853
2022-02-25 07:46:49 - train: epoch 044, train_loss: 1.5266
2022-02-25 07:48:04 - eval: epoch: 044, acc1: 67.942%, acc5: 88.406%, test_loss: 1.3067, per_image_load_time: 0.748ms, per_image_inference_time: 0.309ms
2022-02-25 07:48:05 - until epoch: 044, best_acc1: 68.144%
2022-02-25 08:31:34 - epoch 045 lr: 0.010000000000000002
2022-02-25 08:32:13 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.2752
2022-02-25 08:32:45 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.3749
2022-02-25 08:33:19 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.6656
2022-02-25 08:33:52 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.5333
2022-02-25 08:34:26 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.6734
2022-02-25 08:34:59 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.5573
2022-02-25 08:35:33 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.2953
2022-02-25 08:36:06 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.3790
2022-02-25 08:36:40 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.3398
2022-02-25 08:37:13 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.4863
2022-02-25 08:37:47 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.5874
2022-02-25 08:38:20 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.3465
2022-02-25 08:38:53 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.4875
2022-02-25 08:39:28 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.5568
2022-02-25 08:40:01 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.5303
2022-02-25 08:40:34 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.3152
2022-02-25 08:41:08 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.4030
2022-02-25 08:41:41 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.4684
2022-02-25 08:42:14 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.3024
2022-02-25 08:42:48 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.3958
2022-02-25 08:43:22 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.5958
2022-02-25 08:43:55 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.6858
2022-02-25 08:44:29 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.4657
2022-02-25 08:45:02 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.4446
2022-02-25 08:45:36 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.6151
2022-02-25 08:46:10 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.4197
2022-02-25 08:46:44 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.4759
2022-02-25 08:47:17 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.5446
2022-02-25 08:47:50 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.6413
2022-02-25 08:48:25 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.6193
2022-02-25 08:48:58 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.6023
2022-02-25 08:49:32 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.6227
2022-02-25 08:50:04 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.4608
2022-02-25 08:50:38 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.3831
2022-02-25 08:51:12 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.7240
2022-02-25 08:51:45 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.6431
2022-02-25 08:52:19 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.4691
2022-02-25 08:52:53 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.7478
2022-02-25 08:53:28 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.5286
2022-02-25 08:54:00 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.6390
2022-02-25 08:54:34 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.2902
2022-02-25 08:55:08 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.5724
2022-02-25 08:55:42 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.6930
2022-02-25 08:56:15 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.6327
2022-02-25 08:56:50 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.4030
2022-02-25 08:57:24 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.7467
2022-02-25 08:57:59 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.5907
2022-02-25 08:58:33 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.5133
2022-02-25 08:59:09 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.7103
2022-02-25 08:59:42 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.4940
2022-02-25 08:59:43 - train: epoch 045, train_loss: 1.5266
2022-02-25 09:01:00 - eval: epoch: 045, acc1: 67.760%, acc5: 88.340%, test_loss: 1.3097, per_image_load_time: 2.466ms, per_image_inference_time: 0.307ms
2022-02-25 09:01:00 - until epoch: 045, best_acc1: 68.144%
2022-02-25 09:01:00 - epoch 046 lr: 0.010000000000000002
2022-02-25 09:01:39 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.3818
2022-02-25 09:02:12 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.3947
2022-02-25 09:02:46 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.6129
2022-02-25 09:03:19 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.5379
2022-02-25 09:03:53 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.5205
2022-02-25 09:04:27 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.5170
2022-02-25 09:05:00 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.4061
2022-02-25 09:05:33 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.6844
2022-02-25 09:06:07 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.6146
2022-02-25 09:06:41 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.4244
2022-02-25 09:07:14 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.4231
2022-02-25 09:07:48 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.5318
2022-02-25 09:08:22 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.5038
2022-02-25 09:08:56 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.5289
2022-02-25 09:09:29 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.5549
2022-02-25 09:10:04 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.7479
2022-02-25 09:10:38 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.5117
2022-02-25 09:11:11 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.6775
2022-02-25 09:11:44 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.5881
2022-02-25 09:12:19 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.5426
2022-02-25 09:12:53 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.6206
2022-02-25 09:13:26 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.4010
2022-02-25 09:14:01 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.7324
2022-02-25 09:14:34 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.4173
2022-02-25 09:15:08 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.4811
2022-02-25 09:15:41 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.7161
2022-02-25 09:16:15 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.2280
2022-02-25 09:16:48 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.2529
2022-02-25 09:17:22 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.8012
2022-02-25 09:17:56 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.1952
2022-02-25 09:18:30 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.4055
2022-02-25 09:19:03 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.7329
2022-02-25 09:19:38 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.5327
2022-02-25 09:20:12 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.5780
2022-02-25 09:20:47 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.6282
2022-02-25 09:21:20 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.5880
2022-02-25 09:21:54 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.3648
2022-02-25 09:22:27 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.4930
2022-02-25 09:23:01 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.5110
2022-02-25 09:23:36 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.4502
2022-02-25 09:24:10 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.5694
2022-02-25 09:24:44 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.2914
2022-02-25 09:25:18 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.7285
2022-02-25 09:25:52 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.7565
2022-02-25 09:26:25 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.5451
2022-02-25 09:27:00 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.5714
2022-02-25 09:27:34 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.5297
2022-02-25 09:28:10 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.3611
2022-02-25 09:28:44 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.5298
2022-02-25 09:29:18 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.4027
2022-02-25 09:29:19 - train: epoch 046, train_loss: 1.5250
2022-02-25 09:30:36 - eval: epoch: 046, acc1: 67.074%, acc5: 88.276%, test_loss: 1.3273, per_image_load_time: 1.006ms, per_image_inference_time: 0.307ms
2022-02-25 09:30:36 - until epoch: 046, best_acc1: 68.144%
2022-02-25 09:30:36 - epoch 047 lr: 0.010000000000000002
2022-02-25 09:31:15 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.7148
2022-02-25 09:31:48 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.5280
2022-02-25 09:32:21 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.2583
2022-02-25 09:32:54 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.3600
2022-02-25 09:33:28 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.3960
2022-02-25 09:34:01 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.6444
2022-02-25 09:34:34 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.6038
2022-02-25 09:35:08 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.5005
2022-02-25 09:35:41 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.5807
2022-02-25 09:36:16 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.6334
2022-02-25 09:36:48 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.6217
2022-02-25 09:37:22 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.5096
2022-02-25 09:37:56 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.4029
2022-02-25 09:38:31 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.4748
2022-02-25 09:39:04 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.6724
2022-02-25 09:39:38 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.5396
2022-02-25 09:40:12 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.4330
2022-02-25 09:40:47 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.4842
2022-02-25 09:41:20 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.4674
2022-02-25 09:41:53 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.6899
2022-02-25 09:42:26 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.6841
2022-02-25 09:43:01 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.7007
2022-02-25 09:43:35 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.5338
2022-02-25 09:44:08 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.2421
2022-02-25 09:44:42 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.6359
2022-02-25 09:45:15 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.7464
2022-02-25 09:45:48 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.4241
2022-02-25 09:46:20 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.5671
2022-02-25 09:46:54 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.5388
2022-02-25 09:47:25 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.3854
2022-02-25 09:48:00 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.4753
2022-02-25 09:48:32 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.7309
2022-02-25 09:49:05 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.4491
2022-02-25 09:49:37 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.4389
2022-02-25 09:50:10 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.6007
2022-02-25 09:50:42 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.5119
2022-02-25 09:51:16 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.4561
2022-02-25 09:51:48 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.6590
2022-02-25 09:52:20 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.5265
2022-02-25 09:52:53 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.5882
2022-02-25 09:53:27 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.5079
2022-02-25 09:53:59 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.3822
2022-02-25 09:54:31 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.4115
2022-02-25 09:55:05 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.4041
2022-02-25 09:55:37 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.4415
2022-02-25 09:56:09 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.3851
2022-02-25 09:56:42 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.5470
2022-02-25 09:57:15 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.3985
2022-02-25 09:57:48 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.4839
2022-02-25 09:58:21 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.5878
2022-02-25 09:58:22 - train: epoch 047, train_loss: 1.5260
2022-02-25 09:59:34 - eval: epoch: 047, acc1: 67.422%, acc5: 88.354%, test_loss: 1.3124, per_image_load_time: 2.466ms, per_image_inference_time: 0.306ms
2022-02-25 09:59:34 - until epoch: 047, best_acc1: 68.144%
2022-02-25 09:59:34 - epoch 048 lr: 0.010000000000000002
2022-02-25 10:00:12 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.6151
2022-02-25 10:00:45 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.7886
2022-02-25 10:01:18 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.5074
2022-02-25 10:01:50 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.3506
2022-02-25 10:02:22 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.3750
2022-02-25 10:02:54 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.5320
2022-02-25 10:03:28 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.4736
2022-02-25 10:04:00 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.4714
2022-02-25 10:04:33 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.5701
2022-02-25 10:05:06 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.7570
2022-02-25 10:05:39 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.6735
2022-02-25 10:06:11 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.6032
2022-02-25 10:06:44 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.3379
2022-02-25 10:07:17 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.5705
2022-02-25 10:07:49 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.4931
2022-02-25 10:08:22 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.5527
2022-02-25 10:08:56 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.5497
2022-02-25 10:09:28 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.5055
2022-02-25 10:10:01 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.4899
2022-02-25 10:10:34 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.6183
2022-02-25 10:11:06 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.6329
2022-02-25 10:11:39 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.8619
2022-02-25 10:12:11 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.3521
2022-02-25 10:12:45 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.3700
2022-02-25 10:13:18 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.5733
2022-02-25 10:13:51 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.4509
2022-02-25 10:14:23 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.6506
2022-02-25 10:14:56 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.4628
2022-02-25 10:15:29 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.6137
2022-02-25 10:16:02 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.3062
2022-02-25 10:16:34 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.5896
2022-02-25 10:17:08 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.2321
2022-02-25 10:17:39 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.5807
2022-02-25 10:18:12 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.4875
2022-02-25 10:18:44 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.7330
2022-02-25 10:19:17 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.2999
2022-02-25 10:19:50 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.6195
2022-02-25 10:20:23 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.5111
2022-02-25 10:20:55 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.6258
2022-02-25 10:21:28 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.5084
2022-02-25 10:22:01 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.6913
2022-02-25 10:22:34 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.3458
2022-02-25 10:23:06 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.6108
2022-02-25 10:23:38 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.4933
2022-02-25 10:24:12 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.5411
2022-02-25 10:24:44 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.5337
2022-02-25 10:25:18 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.6113
2022-02-25 10:25:49 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.9466
2022-02-25 10:26:23 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.5782
2022-02-25 10:26:55 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.7059
2022-02-25 10:26:56 - train: epoch 048, train_loss: 1.5240
2022-02-25 10:28:09 - eval: epoch: 048, acc1: 67.850%, acc5: 88.430%, test_loss: 1.3028, per_image_load_time: 2.485ms, per_image_inference_time: 0.286ms
2022-02-25 10:28:09 - until epoch: 048, best_acc1: 68.144%
2022-02-25 10:28:09 - epoch 049 lr: 0.010000000000000002
2022-02-25 10:28:47 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.6344
2022-02-25 10:29:20 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.4657
2022-02-25 10:29:52 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.5146
2022-02-25 10:30:25 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.4097
2022-02-25 10:30:58 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.5918
2022-02-25 10:31:31 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.3919
2022-02-25 10:32:03 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.5488
2022-02-25 10:32:35 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.7317
2022-02-25 10:33:08 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.2002
2022-02-25 10:33:40 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.4729
2022-02-25 10:34:13 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.4969
2022-02-25 10:34:45 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.4612
2022-02-25 10:35:18 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.6454
2022-02-25 10:35:51 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.6530
2022-02-25 10:36:23 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.3831
2022-02-25 10:36:56 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.5077
2022-02-25 10:37:28 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.6398
2022-02-25 10:38:02 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.4232
2022-02-25 10:38:33 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.4444
2022-02-25 10:39:07 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.5358
2022-02-25 10:39:38 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.2645
2022-02-25 10:40:11 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.7193
2022-02-25 10:40:43 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.3242
2022-02-25 10:41:15 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.6142
2022-02-25 10:41:48 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.6359
2022-02-25 10:42:21 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.5178
2022-02-25 10:42:53 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.6270
2022-02-25 10:43:26 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.6343
2022-02-25 10:43:57 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.4630
2022-02-25 10:44:31 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.4917
2022-02-25 10:45:05 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.6685
2022-02-25 10:45:36 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.8317
2022-02-25 10:46:09 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.5842
2022-02-25 10:46:42 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.5826
2022-02-25 10:47:16 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.7699
2022-02-25 10:47:47 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.5040
2022-02-25 10:48:19 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.2944
2022-02-25 10:48:53 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.7677
2022-02-25 10:49:25 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.6487
2022-02-25 10:49:59 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.4174
2022-02-25 10:50:31 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.5177
2022-02-25 10:51:04 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.6065
2022-02-25 10:51:37 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.7624
2022-02-25 10:52:10 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.6050
2022-02-25 10:52:43 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.3920
2022-02-25 10:53:16 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.5125
2022-02-25 10:53:49 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.7345
2022-02-25 10:54:24 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.4141
2022-02-25 10:54:56 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.5566
2022-02-25 10:55:28 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.4089
2022-02-25 10:55:29 - train: epoch 049, train_loss: 1.5211
2022-02-25 10:56:42 - eval: epoch: 049, acc1: 67.758%, acc5: 88.232%, test_loss: 1.3063, per_image_load_time: 1.932ms, per_image_inference_time: 0.271ms
2022-02-25 10:56:43 - until epoch: 049, best_acc1: 68.144%
2022-02-25 10:56:43 - epoch 050 lr: 0.010000000000000002
2022-02-25 10:57:20 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.5731
2022-02-25 10:57:52 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.5123
2022-02-25 10:58:24 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.2969
2022-02-25 10:58:57 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.5014
2022-02-25 10:59:29 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.4401
2022-02-25 11:00:01 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.7429
2022-02-25 11:00:34 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.3500
2022-02-25 11:01:06 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.3452
2022-02-25 11:01:39 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.5414
2022-02-25 11:02:11 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.4405
2022-02-25 11:02:44 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.6805
2022-02-25 11:03:16 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.3475
2022-02-25 11:03:49 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.6594
2022-02-25 11:04:21 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.4213
2022-02-25 11:04:54 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.3200
2022-02-25 11:05:26 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.5613
2022-02-25 11:05:59 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.5330
2022-02-25 11:06:31 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.5798
2022-02-25 11:07:05 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.5868
2022-02-25 11:07:37 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.4733
2022-02-25 11:08:10 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.4286
2022-02-25 11:08:42 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.5489
2022-02-25 11:09:16 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.4268
2022-02-25 11:09:47 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.8383
2022-02-25 11:10:21 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.5242
2022-02-25 11:10:52 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.4639
2022-02-25 11:11:25 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.4273
2022-02-25 11:11:58 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.5463
2022-02-25 11:12:30 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.6010
2022-02-25 11:13:03 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.6735
2022-02-25 11:13:35 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.6580
2022-02-25 11:14:08 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.4959
2022-02-25 11:14:41 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.4480
2022-02-25 11:15:13 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.5274
2022-02-25 11:15:46 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.5324
2022-02-25 11:16:19 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.5993
2022-02-25 11:16:51 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.6829
2022-02-25 11:17:24 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.3447
2022-02-25 11:17:56 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.3667
2022-02-25 11:18:29 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.5353
2022-02-25 11:19:01 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.5059
2022-02-25 11:19:34 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.5652
2022-02-25 11:20:07 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.7339
2022-02-25 11:20:39 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.5032
2022-02-25 11:21:11 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.4747
2022-02-25 11:21:45 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.5091
2022-02-25 11:22:17 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.7113
2022-02-25 11:22:51 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.2929
2022-02-25 11:23:23 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.3815
2022-02-25 11:23:55 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.5610
2022-02-25 11:23:56 - train: epoch 050, train_loss: 1.5204
2022-02-25 11:25:09 - eval: epoch: 050, acc1: 67.412%, acc5: 88.296%, test_loss: 1.3196, per_image_load_time: 2.527ms, per_image_inference_time: 0.279ms
2022-02-25 11:25:10 - until epoch: 050, best_acc1: 68.144%
2022-02-25 11:25:10 - epoch 051 lr: 0.010000000000000002
2022-02-25 11:25:48 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.5753
2022-02-25 11:26:20 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.7294
2022-02-25 11:26:53 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.6066
2022-02-25 11:27:25 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.4102
2022-02-25 11:27:58 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.6362
2022-02-25 11:28:30 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.6255
2022-02-25 11:29:03 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.4931
2022-02-25 11:29:35 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.6940
2022-02-25 11:30:08 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.5713
2022-02-25 11:30:40 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.9422
2022-02-25 11:31:13 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.6289
2022-02-25 11:31:45 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.4843
2022-02-25 11:32:18 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.3402
2022-02-25 11:32:51 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.4404
2022-02-25 11:33:24 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.4678
2022-02-25 11:33:56 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.3568
2022-02-25 11:34:30 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.5874
2022-02-25 11:35:02 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.4135
2022-02-25 11:35:35 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.3445
2022-02-25 11:36:07 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.4077
2022-02-25 11:36:40 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.5148
2022-02-25 11:37:13 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.3466
2022-02-25 11:37:45 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.7217
2022-02-25 11:38:18 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.2773
2022-02-25 11:38:51 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.4183
2022-02-25 11:39:24 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.4313
2022-02-25 11:39:56 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.5801
2022-02-25 11:40:29 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.1766
2022-02-25 11:41:01 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.5263
2022-02-25 11:41:34 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.1910
2022-02-25 11:42:07 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.3159
2022-02-25 11:42:40 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.6793
2022-02-25 11:43:13 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.4842
2022-02-25 11:43:44 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.6266
2022-02-25 11:44:17 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.4797
2022-02-25 11:44:50 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.4494
2022-02-25 11:45:22 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.5304
2022-02-25 11:45:55 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.7316
2022-02-25 11:46:28 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.7371
2022-02-25 11:47:00 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.5674
2022-02-25 11:47:33 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.5226
2022-02-25 11:48:06 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.4657
2022-02-25 11:48:39 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.4518
2022-02-25 11:49:11 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.6031
2022-02-25 11:49:43 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.3799
2022-02-25 11:50:17 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.4997
2022-02-25 11:50:50 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.6199
2022-02-25 11:51:23 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.5699
2022-02-25 11:51:56 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.6484
2022-02-25 11:52:28 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.4343
2022-02-25 11:52:29 - train: epoch 051, train_loss: 1.5167
2022-02-25 11:53:44 - eval: epoch: 051, acc1: 67.808%, acc5: 88.364%, test_loss: 1.3039, per_image_load_time: 2.647ms, per_image_inference_time: 0.279ms
2022-02-25 11:53:44 - until epoch: 051, best_acc1: 68.144%
2022-02-25 11:53:44 - epoch 052 lr: 0.010000000000000002
2022-02-25 11:54:21 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.5529
2022-02-25 11:54:54 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.5466
2022-02-25 11:55:26 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.3543
2022-02-25 11:55:58 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.4886
2022-02-25 11:56:31 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.6393
2022-02-25 11:57:03 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.6026
2022-02-25 11:57:35 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.5091
2022-02-25 11:58:07 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.5711
2022-02-25 11:58:40 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.6310
2022-02-25 11:59:12 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.8328
2022-02-25 11:59:44 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.3880
2022-02-25 12:00:17 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.3053
2022-02-25 12:00:49 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.4456
2022-02-25 12:01:22 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.7160
2022-02-25 12:01:55 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.4433
2022-02-25 12:02:28 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.3649
2022-02-25 12:02:59 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.4250
2022-02-25 12:03:32 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.3531
2022-02-25 12:04:04 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.2480
2022-02-25 12:04:38 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.6751
2022-02-25 12:05:11 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.5127
2022-02-25 12:05:44 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.5102
2022-02-25 12:06:16 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.4103
2022-02-25 12:06:49 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.1983
2022-02-25 12:07:21 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.4294
2022-02-25 12:07:55 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.4781
2022-02-25 12:08:26 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.3306
2022-02-25 12:09:00 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.6982
2022-02-25 12:09:32 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.4839
2022-02-25 12:10:05 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.5043
2022-02-25 12:10:37 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.5575
2022-02-25 12:11:11 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.4532
2022-02-25 12:11:43 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.6586
2022-02-25 12:12:15 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.5451
2022-02-25 12:12:48 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.6026
2022-02-25 12:13:20 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.6023
2022-02-25 12:13:53 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.6185
2022-02-25 12:14:26 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.4745
2022-02-25 12:14:58 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.2992
2022-02-25 12:15:31 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.8330
2022-02-25 12:16:03 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.3878
2022-02-25 12:16:36 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.5682
2022-02-25 12:17:08 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.4923
2022-02-25 12:17:41 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.6613
2022-02-25 12:18:13 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.6464
2022-02-25 12:18:47 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.4754
2022-02-25 12:19:18 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.3676
2022-02-25 12:19:52 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.3357
2022-02-25 12:20:25 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.6007
2022-02-25 12:20:57 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.3602
2022-02-25 12:20:58 - train: epoch 052, train_loss: 1.5137
2022-02-25 12:22:11 - eval: epoch: 052, acc1: 67.638%, acc5: 88.308%, test_loss: 1.3124, per_image_load_time: 1.012ms, per_image_inference_time: 0.285ms
2022-02-25 12:22:11 - until epoch: 052, best_acc1: 68.144%
2022-02-25 12:22:11 - epoch 053 lr: 0.010000000000000002
2022-02-25 12:22:49 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.6419
2022-02-25 12:23:21 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.3946
2022-02-25 12:23:53 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.4103
2022-02-25 12:24:26 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.6181
2022-02-25 12:24:58 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.4630
2022-02-25 12:25:31 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.6180
2022-02-25 12:26:03 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.4597
2022-02-25 12:26:35 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.5482
2022-02-25 12:27:08 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.5015
2022-02-25 12:27:41 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.4225
2022-02-25 12:28:13 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.5399
2022-02-25 12:28:45 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.2006
2022-02-25 12:29:18 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.6560
2022-02-25 12:29:50 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.7857
2022-02-25 12:30:23 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.4239
2022-02-25 12:30:56 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.7860
2022-02-25 12:31:29 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.5645
2022-02-25 12:32:00 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.4315
2022-02-25 12:32:34 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.3584
2022-02-25 12:33:06 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.4531
2022-02-25 12:33:39 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.6448
2022-02-25 12:34:11 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.3453
2022-02-25 12:34:43 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.3877
2022-02-25 12:35:17 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.3043
2022-02-25 12:35:50 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.6004
2022-02-25 12:36:22 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.7752
2022-02-25 12:36:55 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.7170
2022-02-25 12:37:27 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.7274
2022-02-25 12:38:00 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.4415
2022-02-25 12:38:32 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.3429
2022-02-25 12:39:04 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.7142
2022-02-25 12:39:37 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.5877
2022-02-25 12:40:10 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.3570
2022-02-25 12:40:43 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.3436
2022-02-25 12:41:16 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.6461
2022-02-25 12:41:47 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.6200
2022-02-25 12:42:21 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.6039
2022-02-25 12:42:53 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.5430
2022-02-25 12:43:26 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.7760
2022-02-25 12:43:58 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.5220
2022-02-25 12:44:31 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.5584
2022-02-25 12:45:03 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.6288
2022-02-25 12:45:37 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.7782
2022-02-25 12:46:09 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.4827
2022-02-25 12:46:42 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.5077
2022-02-25 12:47:14 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.6873
2022-02-25 12:47:47 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.4498
2022-02-25 12:48:19 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.7076
2022-02-25 12:48:52 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.5265
2022-02-25 12:49:24 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.5580
2022-02-25 12:49:25 - train: epoch 053, train_loss: 1.5116
2022-02-25 12:50:38 - eval: epoch: 053, acc1: 67.562%, acc5: 88.558%, test_loss: 1.3081, per_image_load_time: 2.502ms, per_image_inference_time: 0.276ms
2022-02-25 12:50:39 - until epoch: 053, best_acc1: 68.144%
2022-02-25 12:50:39 - epoch 054 lr: 0.010000000000000002
2022-02-25 12:51:17 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.3425
2022-02-25 12:51:49 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.5490
2022-02-25 12:52:21 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.4891
2022-02-25 12:52:53 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.3561
2022-02-25 12:53:26 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.4899
2022-02-25 12:53:57 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.6151
2022-02-25 12:54:31 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.6924
2022-02-25 12:55:03 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.5871
2022-02-25 12:55:36 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.4869
2022-02-25 12:56:08 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.5119
2022-02-25 12:56:41 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.3673
2022-02-25 12:57:13 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.6591
2022-02-25 12:57:46 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.4582
2022-02-25 12:58:18 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.5075
2022-02-25 12:58:51 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.4974
2022-02-25 12:59:24 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.4239
2022-02-25 12:59:57 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.5061
2022-02-25 13:00:29 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.4926
2022-02-25 13:01:02 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.7484
2022-02-25 13:01:34 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.5221
2022-02-25 13:02:07 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.4553
2022-02-25 13:02:39 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.5439
2022-02-25 13:03:14 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.3345
2022-02-25 13:03:47 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.4416
2022-02-25 13:04:19 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.4859
2022-02-25 13:04:52 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.2061
2022-02-25 13:05:25 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.4603
2022-02-25 13:05:58 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.6929
2022-02-25 13:06:30 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.5266
2022-02-25 13:07:03 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.7512
2022-02-25 13:07:35 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.5482
2022-02-25 13:08:08 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.5609
2022-02-25 13:08:41 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.4626
2022-02-25 13:09:14 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.5542
2022-02-25 13:09:46 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.4541
2022-02-25 13:10:19 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.5936
2022-02-25 13:10:51 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.4494
2022-02-25 13:11:25 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.5056
2022-02-25 13:11:57 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.4703
2022-02-25 13:12:30 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.4298
2022-02-25 13:13:03 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.4573
2022-02-25 13:13:36 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.4939
2022-02-25 13:14:10 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.5296
2022-02-25 13:14:42 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.3260
2022-02-25 13:15:14 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.4255
2022-02-25 13:15:48 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.7102
2022-02-25 13:16:21 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.6875
2022-02-25 13:16:54 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.6132
2022-02-25 13:17:29 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.5260
2022-02-25 13:18:00 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.6440
2022-02-25 13:18:02 - train: epoch 054, train_loss: 1.5094
2022-02-25 13:19:16 - eval: epoch: 054, acc1: 67.840%, acc5: 88.474%, test_loss: 1.3017, per_image_load_time: 2.634ms, per_image_inference_time: 0.278ms
2022-02-25 13:19:17 - until epoch: 054, best_acc1: 68.144%
2022-02-25 13:19:17 - epoch 055 lr: 0.010000000000000002
2022-02-25 13:19:55 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.4261
2022-02-25 13:20:28 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.5225
2022-02-25 13:21:01 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.2437
2022-02-25 13:21:34 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.3678
2022-02-25 13:22:06 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.3955
2022-02-25 13:22:38 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.4964
2022-02-25 13:23:12 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.4055
2022-02-25 13:23:45 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.3808
2022-02-25 13:24:17 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.5342
2022-02-25 13:24:51 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.4705
2022-02-25 13:25:23 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.3004
2022-02-25 13:25:57 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.5417
2022-02-25 13:26:30 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.5164
2022-02-25 13:27:04 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.4277
2022-02-25 13:27:36 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.4604
2022-02-25 13:28:10 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.5163
2022-02-25 13:28:43 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.4806
2022-02-25 13:29:17 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.5295
2022-02-25 13:29:51 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.5212
2022-02-25 13:30:24 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.4299
2022-02-25 13:30:57 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.2922
2022-02-25 13:31:30 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.6026
2022-02-25 13:32:04 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.4671
2022-02-25 13:32:37 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.4534
2022-02-25 13:33:11 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.4164
2022-02-25 13:33:44 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.3554
2022-02-25 13:34:18 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.3738
2022-02-25 13:34:51 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.6170
2022-02-25 13:35:25 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.5192
2022-02-25 13:35:59 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.5981
2022-02-25 13:36:32 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.6906
2022-02-25 13:37:06 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.6528
2022-02-25 13:37:39 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.3855
2022-02-25 13:38:13 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.3589
2022-02-25 13:38:46 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.5405
2022-02-25 13:39:20 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.6082
2022-02-25 13:39:53 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.3316
2022-02-25 13:40:27 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.6528
2022-02-25 13:41:00 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.8136
2022-02-25 13:41:34 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.4470
2022-02-25 13:42:07 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.5213
2022-02-25 13:42:40 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.7767
2022-02-25 13:43:14 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.7528
2022-02-25 13:43:47 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.7702
2022-02-25 13:44:21 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.4323
2022-02-25 13:44:53 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.4211
2022-02-25 13:45:27 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.5121
2022-02-25 13:46:01 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.6924
2022-02-25 13:46:35 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.4455
2022-02-25 13:47:07 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.4998
2022-02-25 13:47:08 - train: epoch 055, train_loss: 1.5087
2022-02-25 13:48:23 - eval: epoch: 055, acc1: 67.668%, acc5: 88.526%, test_loss: 1.3038, per_image_load_time: 0.955ms, per_image_inference_time: 0.285ms
2022-02-25 13:48:24 - until epoch: 055, best_acc1: 68.144%
2022-02-25 13:48:24 - epoch 056 lr: 0.010000000000000002
2022-02-25 13:49:02 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.4700
2022-02-25 13:49:34 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.5115
2022-02-25 13:50:08 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.3286
2022-02-25 13:50:41 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.5480
2022-02-25 13:51:15 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.4584
2022-02-25 13:51:49 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.4804
2022-02-25 13:52:21 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.4967
2022-02-25 13:52:56 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.6097
2022-02-25 13:53:30 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.6118
2022-02-25 13:54:03 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.5353
2022-02-25 13:54:36 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.2879
2022-02-25 13:55:09 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.4915
2022-02-25 13:55:43 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.7452
2022-02-25 13:56:17 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.6036
2022-02-25 13:56:50 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.5939
2022-02-25 13:57:23 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.4724
2022-02-25 13:57:56 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.4691
2022-02-25 13:58:30 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.7909
2022-02-25 13:59:03 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.5412
2022-02-25 13:59:37 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.4517
2022-02-25 14:00:10 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.4691
2022-02-25 14:00:44 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.7533
2022-02-25 14:01:17 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.5595
2022-02-25 14:01:50 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.4597
2022-02-25 14:02:25 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.7608
2022-02-25 14:02:58 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.4811
2022-02-25 14:03:31 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.5080
2022-02-25 14:04:06 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.4661
2022-02-25 14:04:38 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.6464
2022-02-25 14:05:12 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.4699
2022-02-25 14:05:45 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.3375
2022-02-25 14:06:18 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.4441
2022-02-25 14:06:52 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.5179
2022-02-25 14:07:26 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.4733
2022-02-25 14:07:59 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.6203
2022-02-25 14:08:33 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.3420
2022-02-25 14:09:07 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.4822
2022-02-25 14:09:40 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.5682
2022-02-25 14:10:14 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.7385
2022-02-25 14:10:46 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.5809
2022-02-25 14:11:22 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.5682
2022-02-25 14:11:54 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.4619
2022-02-25 14:12:28 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.5596
2022-02-25 14:13:01 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.7120
2022-02-25 14:13:34 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.3576
2022-02-25 14:14:08 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.6390
2022-02-25 14:14:41 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.4427
2022-02-25 14:15:14 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.5936
2022-02-25 14:15:48 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.5559
2022-02-25 14:16:21 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.5079
2022-02-25 14:16:22 - train: epoch 056, train_loss: 1.5028
2022-02-25 14:17:37 - eval: epoch: 056, acc1: 67.840%, acc5: 88.436%, test_loss: 1.3023, per_image_load_time: 2.592ms, per_image_inference_time: 0.312ms
2022-02-25 14:17:37 - until epoch: 056, best_acc1: 68.144%
2022-02-25 14:17:37 - epoch 057 lr: 0.010000000000000002
2022-02-25 14:18:16 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.5380
2022-02-25 14:18:49 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.5752
2022-02-25 14:19:22 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.5656
2022-02-25 14:19:57 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.3579
2022-02-25 14:20:30 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.2437
2022-02-25 14:21:03 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.6614
2022-02-25 14:21:37 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.3682
2022-02-25 14:22:10 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.4914
2022-02-25 14:22:43 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.6640
2022-02-25 14:23:16 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.3051
2022-02-25 14:23:49 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.3685
2022-02-25 14:24:23 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.6967
2022-02-25 14:24:56 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.5292
2022-02-25 14:25:30 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.5116
2022-02-25 14:26:02 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.5737
2022-02-25 14:26:36 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.4698
2022-02-25 14:27:10 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.5441
2022-02-25 14:27:42 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.6668
2022-02-25 14:28:16 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.5683
2022-02-25 14:28:50 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.4420
2022-02-25 14:29:24 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.3879
2022-02-25 14:29:55 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.5169
2022-02-25 14:30:29 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.5393
2022-02-25 14:31:03 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.5776
2022-02-25 14:31:36 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.5051
2022-02-25 14:32:09 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.3770
2022-02-25 14:32:43 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.3208
2022-02-25 14:33:15 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.3316
2022-02-25 14:33:48 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.6052
2022-02-25 14:34:23 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.6634
2022-02-25 14:34:55 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.5779
2022-02-25 14:35:29 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.5042
2022-02-25 14:36:03 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.4016
2022-02-25 14:36:36 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.6731
2022-02-25 14:37:10 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.6457
2022-02-25 14:37:43 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.5813
2022-02-25 14:38:17 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.4335
2022-02-25 14:38:49 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.5478
2022-02-25 14:39:22 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.6908
2022-02-25 14:39:56 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.4557
2022-02-25 14:40:29 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.6141
2022-02-25 14:41:02 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.6344
2022-02-25 14:41:36 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.3170
2022-02-25 14:42:09 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.6916
2022-02-25 14:42:42 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.5249
2022-02-25 14:43:16 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.5000
2022-02-25 14:43:49 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.8179
2022-02-25 14:44:23 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.8739
2022-02-25 14:44:56 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.6909
2022-02-25 14:45:29 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.6681
2022-02-25 14:45:30 - train: epoch 057, train_loss: 1.5019
2022-02-25 14:46:45 - eval: epoch: 057, acc1: 67.654%, acc5: 88.400%, test_loss: 1.3134, per_image_load_time: 2.525ms, per_image_inference_time: 0.293ms
2022-02-25 14:46:45 - until epoch: 057, best_acc1: 68.144%
2022-02-25 14:46:45 - epoch 058 lr: 0.010000000000000002
2022-02-25 14:47:24 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.5593
2022-02-25 14:47:57 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.3805
2022-02-25 14:48:31 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.4583
2022-02-25 14:49:04 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.5747
2022-02-25 14:49:37 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.4526
2022-02-25 14:50:10 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.6415
2022-02-25 14:50:43 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.3171
2022-02-25 14:51:16 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.6381
2022-02-25 14:51:49 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.4233
2022-02-25 14:52:22 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.4410
2022-02-25 14:52:56 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.4532
2022-02-25 14:53:29 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.4543
2022-02-25 14:54:02 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.3875
2022-02-25 14:54:35 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.4496
2022-02-25 14:55:09 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.4895
2022-02-25 14:55:43 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.3940
2022-02-25 14:56:16 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.6510
2022-02-25 14:56:49 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.5528
2022-02-25 14:57:23 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.3902
2022-02-25 14:57:56 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.7441
2022-02-25 14:58:30 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.3258
2022-02-25 14:59:03 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.4002
2022-02-25 14:59:36 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.4248
2022-02-25 15:00:10 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.6394
2022-02-25 15:00:44 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.5972
2022-02-25 15:01:17 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.4640
2022-02-25 15:01:50 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.6359
2022-02-25 15:02:23 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.4413
2022-02-25 15:02:57 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.2877
2022-02-25 15:03:30 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.5525
2022-02-25 15:04:04 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.5091
2022-02-25 15:04:38 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.4131
2022-02-25 15:05:11 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.4818
2022-02-25 15:05:44 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.4649
2022-02-25 15:06:18 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.3996
2022-02-25 15:06:52 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.5324
2022-02-25 15:07:25 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.4212
2022-02-25 15:07:57 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.3801
2022-02-25 15:08:32 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.4688
2022-02-25 15:09:04 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.6265
2022-02-25 15:09:38 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.3812
2022-02-25 15:10:11 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.4033
2022-02-25 15:10:45 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.5599
2022-02-25 15:11:18 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.3822
2022-02-25 15:11:52 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.4444
2022-02-25 15:12:26 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.3633
2022-02-25 15:12:59 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.3777
2022-02-25 15:13:33 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.4834
2022-02-25 15:14:08 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.4782
2022-02-25 15:14:40 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.3132
2022-02-25 15:14:41 - train: epoch 058, train_loss: 1.5012
2022-02-25 15:15:56 - eval: epoch: 058, acc1: 68.140%, acc5: 88.710%, test_loss: 1.2948, per_image_load_time: 2.533ms, per_image_inference_time: 0.287ms
2022-02-25 15:15:56 - until epoch: 058, best_acc1: 68.144%
2022-02-25 15:15:56 - epoch 059 lr: 0.010000000000000002
2022-02-25 15:16:35 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.3949
2022-02-25 15:17:08 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.4040
2022-02-25 15:17:41 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.3796
2022-02-25 15:18:15 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.6139
2022-02-25 15:18:47 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.5566
2022-02-25 15:19:21 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.4376
2022-02-25 15:19:54 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.4134
2022-02-25 15:20:27 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.6898
2022-02-25 15:21:00 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.5886
2022-02-25 15:21:34 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.5852
2022-02-25 15:22:08 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 1.7531
2022-02-25 15:22:41 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.2883
2022-02-25 15:23:15 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.8542
2022-02-25 15:23:49 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.6179
2022-02-25 15:24:22 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.3667
2022-02-25 15:24:55 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.3255
2022-02-25 15:25:29 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.3924
2022-02-25 15:26:02 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.4329
2022-02-25 15:26:35 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.3474
2022-02-25 15:27:09 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.3120
2022-02-25 15:27:42 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.5391
2022-02-25 15:28:15 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.5863
2022-02-25 15:28:48 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.3846
2022-02-25 15:29:21 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.5693
2022-02-25 15:29:55 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.6422
2022-02-25 15:30:28 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.3523
2022-02-25 15:31:01 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.5064
2022-02-25 15:31:34 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.4732
2022-02-25 15:32:09 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.5270
2022-02-25 15:32:42 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 1.7248
2022-02-25 15:33:14 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.3464
2022-02-25 15:33:48 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.4472
2022-02-25 15:34:21 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.5072
2022-02-25 15:34:54 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 1.8045
2022-02-25 15:35:28 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.4615
2022-02-25 15:36:00 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.6234
2022-02-25 15:36:34 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.5277
2022-02-25 15:37:07 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.4946
2022-02-25 15:37:41 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.4139
2022-02-25 15:38:14 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 1.6359
2022-02-25 15:38:47 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.3061
2022-02-25 15:39:20 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.5601
2022-02-25 15:39:55 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.5802
2022-02-25 15:40:27 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.5177
2022-02-25 15:41:01 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.5840
2022-02-25 15:41:35 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.4328
2022-02-25 15:42:09 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.4355
2022-02-25 15:42:44 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.5500
2022-02-25 15:43:17 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.5717
2022-02-25 15:43:50 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.6582
2022-02-25 15:43:51 - train: epoch 059, train_loss: 1.4976
2022-02-25 15:45:06 - eval: epoch: 059, acc1: 67.166%, acc5: 87.996%, test_loss: 1.3242, per_image_load_time: 1.112ms, per_image_inference_time: 0.296ms
2022-02-25 15:45:06 - until epoch: 059, best_acc1: 68.144%
2022-02-25 15:45:06 - epoch 060 lr: 0.010000000000000002
2022-02-25 15:45:44 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.5613
2022-02-25 15:46:18 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.5117
2022-02-25 15:46:51 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.5273
2022-02-25 15:47:23 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.7734
2022-02-25 15:47:57 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.5690
2022-02-25 15:48:29 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.8292
2022-02-25 15:49:03 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.3860
2022-02-25 15:49:37 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.5734
2022-02-25 15:50:11 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.1826
2022-02-25 15:50:45 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.2870
2022-02-25 15:51:18 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.3985
2022-02-25 15:51:51 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.4395
2022-02-25 15:52:25 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.4269
2022-02-25 15:52:58 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.6911
2022-02-25 15:53:31 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.6809
2022-02-25 15:54:05 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.6171
2022-02-25 15:54:38 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.6553
2022-02-25 15:55:13 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.5661
2022-02-25 15:55:46 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.6137
2022-02-25 15:56:19 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.3723
2022-02-25 15:56:53 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.5667
2022-02-25 15:57:27 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.5525
2022-02-25 15:58:02 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.4928
2022-02-25 15:58:35 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.5623
2022-02-25 15:59:09 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.5366
2022-02-25 15:59:42 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.6993
2022-02-25 16:00:16 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.3582
2022-02-25 16:00:49 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.6227
2022-02-25 16:01:23 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.4561
2022-02-25 16:01:56 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.6604
2022-02-25 16:02:30 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.5493
2022-02-25 16:03:03 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.5800
2022-02-25 16:03:38 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.2031
2022-02-25 16:04:11 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.5509
2022-02-25 16:04:45 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.5260
2022-02-25 16:05:17 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.5769
2022-02-25 16:05:51 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.5235
2022-02-25 16:06:25 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.3587
2022-02-25 16:06:58 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.7689
2022-02-25 16:07:31 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.5856
2022-02-25 16:08:05 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.6392
2022-02-25 16:08:38 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.7190
2022-02-25 16:09:13 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.3381
2022-02-25 16:09:46 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.6505
2022-02-25 16:10:20 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.6234
2022-02-25 16:10:54 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.2880
2022-02-25 16:11:27 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.5312
2022-02-25 16:12:00 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.4385
2022-02-25 16:12:35 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.5586
2022-02-25 16:13:07 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.6256
2022-02-25 16:13:08 - train: epoch 060, train_loss: 1.4943
2022-02-25 16:14:24 - eval: epoch: 060, acc1: 67.676%, acc5: 88.462%, test_loss: 1.3028, per_image_load_time: 2.587ms, per_image_inference_time: 0.301ms
2022-02-25 16:14:24 - until epoch: 060, best_acc1: 68.144%
2022-02-25 16:14:24 - epoch 061 lr: 0.0010000000000000002
2022-02-25 16:15:03 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.2618
2022-02-25 16:15:37 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.2936
2022-02-25 16:16:08 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.1194
2022-02-25 16:16:43 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.5461
2022-02-25 16:17:17 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.4801
2022-02-25 16:17:50 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.2129
2022-02-25 16:18:24 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.1780
2022-02-25 16:18:57 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.3513
2022-02-25 16:19:31 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.2353
2022-02-25 16:20:03 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.3560
2022-02-25 16:20:38 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.1192
2022-02-25 16:21:11 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.3113
2022-02-25 16:21:44 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.2983
2022-02-25 16:22:18 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.2901
2022-02-25 16:22:51 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.2616
2022-02-25 16:23:25 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.1921
2022-02-25 16:23:58 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.4139
2022-02-25 16:24:31 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.2345
2022-02-25 16:25:04 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.4659
2022-02-25 16:25:39 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.2088
2022-02-25 16:26:12 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.5112
2022-02-25 16:26:46 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.1313
2022-02-25 16:27:18 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.2089
2022-02-25 16:27:52 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.0720
2022-02-25 16:28:25 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.4579
2022-02-25 16:28:59 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.4705
2022-02-25 16:29:32 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.5516
2022-02-25 16:30:07 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.3604
2022-02-25 16:30:40 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.2230
2022-02-25 16:31:14 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.1941
2022-02-25 16:31:46 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.3606
2022-02-25 16:32:20 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.3014
2022-02-25 16:32:54 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.1035
2022-02-25 16:33:28 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.1068
2022-02-25 16:34:01 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.3997
2022-02-25 16:34:36 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.2885
2022-02-25 16:35:10 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.3114
2022-02-25 16:35:44 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.3253
2022-02-25 16:36:18 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.3312
2022-02-25 16:36:50 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.2373
2022-02-25 16:37:25 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.3933
2022-02-25 16:37:58 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.2620
2022-02-25 16:38:31 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.3309
2022-02-25 16:39:05 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.2301
2022-02-25 16:39:38 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.1422
2022-02-25 16:40:13 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.3094
2022-02-25 16:40:47 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.2222
2022-02-25 16:41:21 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.2064
2022-02-25 16:41:54 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.3516
2022-02-25 16:42:27 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.2478
2022-02-25 16:42:28 - train: epoch 061, train_loss: 1.2774
2022-02-25 16:43:45 - eval: epoch: 061, acc1: 71.926%, acc5: 90.738%, test_loss: 1.1118, per_image_load_time: 1.182ms, per_image_inference_time: 0.282ms
2022-02-25 16:43:45 - until epoch: 061, best_acc1: 71.926%
2022-02-25 16:43:45 - epoch 062 lr: 0.0010000000000000002
2022-02-25 16:44:24 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.3647
2022-02-25 16:44:57 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.7179
2022-02-25 16:45:31 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.2647
2022-02-25 16:46:04 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.2038
2022-02-25 16:46:37 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.2236
2022-02-25 16:47:10 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.3046
2022-02-25 16:47:43 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.1718
2022-02-25 16:48:17 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.3624
2022-02-25 16:48:50 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.4218
2022-02-25 16:49:23 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.1969
2022-02-25 16:49:57 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.2615
2022-02-25 16:50:29 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.3663
2022-02-25 16:51:03 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.1652
2022-02-25 16:51:36 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.0852
2022-02-25 16:52:10 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.3474
2022-02-25 16:52:43 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.2506
2022-02-25 16:53:18 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.2544
2022-02-25 16:53:51 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.2441
2022-02-25 16:54:25 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.1876
2022-02-25 16:54:58 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.2497
2022-02-25 16:55:32 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.3190
2022-02-25 16:56:04 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.2524
2022-02-25 16:56:38 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.1591
2022-02-25 16:57:11 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.2421
2022-02-25 16:57:45 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.2825
2022-02-25 16:58:17 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.2478
2022-02-25 16:58:52 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.2651
2022-02-25 16:59:25 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.4973
2022-02-25 16:59:58 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.1386
2022-02-25 17:00:31 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.2767
2022-02-25 17:01:04 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.2598
2022-02-25 17:01:38 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.0815
2022-02-25 17:02:11 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.2592
2022-02-25 17:02:46 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.3609
2022-02-25 17:03:19 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.3355
2022-02-25 17:03:52 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.2709
2022-02-25 17:04:25 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.1054
2022-02-25 17:04:58 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.1690
2022-02-25 17:05:32 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.2522
2022-02-25 17:06:04 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.1604
2022-02-25 17:06:38 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.4003
2022-02-25 17:07:11 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 1.2012
2022-02-25 17:07:46 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.0922
2022-02-25 17:08:18 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.1321
2022-02-25 17:08:51 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.1792
2022-02-25 17:09:24 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 0.9506
2022-02-25 17:09:58 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.0908
2022-02-25 17:10:32 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.3269
2022-02-25 17:11:07 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.1720
2022-02-25 17:11:40 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.2662
2022-02-25 17:11:41 - train: epoch 062, train_loss: 1.2298
2022-02-25 17:12:58 - eval: epoch: 062, acc1: 72.284%, acc5: 90.896%, test_loss: 1.0975, per_image_load_time: 2.701ms, per_image_inference_time: 0.278ms
2022-02-25 17:12:58 - until epoch: 062, best_acc1: 72.284%
2022-02-25 17:12:58 - epoch 063 lr: 0.0010000000000000002
2022-02-25 17:13:36 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 1.0878
2022-02-25 17:14:09 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.0483
2022-02-25 17:14:42 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.2140
2022-02-25 17:15:16 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.0810
2022-02-25 17:15:49 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.1887
2022-02-25 17:16:22 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.1397
2022-02-25 17:16:55 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.2997
2022-02-25 17:17:28 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.1437
2022-02-25 17:18:01 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.3866
2022-02-25 17:18:34 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.2204
2022-02-25 17:19:08 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.2412
2022-02-25 17:19:43 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.3436
2022-02-25 17:20:17 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.0984
2022-02-25 17:20:51 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.2830
2022-02-25 17:21:25 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.1701
2022-02-25 17:21:57 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.1477
2022-02-25 17:22:31 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 0.9641
2022-02-25 17:23:05 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.1437
2022-02-25 17:23:38 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.2659
2022-02-25 17:24:11 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 1.1101
2022-02-25 17:24:44 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.1314
2022-02-25 17:25:19 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.3635
2022-02-25 17:25:51 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.3516
2022-02-25 17:26:24 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.3621
2022-02-25 17:26:58 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.2343
2022-02-25 17:27:31 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.1818
2022-02-25 17:28:05 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.5173
2022-02-25 17:28:40 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.4252
2022-02-25 17:29:12 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.1885
2022-02-25 17:29:46 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.3595
2022-02-25 17:30:19 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.4514
2022-02-25 17:30:53 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.2453
2022-02-25 17:31:26 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.1706
2022-02-25 17:32:00 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.1892
2022-02-25 17:32:32 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.2114
2022-02-25 17:33:06 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.2838
2022-02-25 17:33:39 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.3111
2022-02-25 17:34:13 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.4767
2022-02-25 17:34:46 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 1.2655
2022-02-25 17:35:20 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 1.0792
2022-02-25 17:35:53 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.1151
2022-02-25 17:36:27 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.1613
2022-02-25 17:37:00 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.2672
2022-02-25 17:37:35 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.2594
2022-02-25 17:38:08 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.2749
2022-02-25 17:38:42 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.2310
2022-02-25 17:39:16 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.1800
2022-02-25 17:39:50 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 0.9282
2022-02-25 17:40:23 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.3388
2022-02-25 17:40:56 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.2482
2022-02-25 17:40:57 - train: epoch 063, train_loss: 1.2090
2022-02-25 17:42:12 - eval: epoch: 063, acc1: 72.548%, acc5: 91.076%, test_loss: 1.0922, per_image_load_time: 2.626ms, per_image_inference_time: 0.301ms
2022-02-25 17:42:13 - until epoch: 063, best_acc1: 72.548%
2022-02-25 17:42:13 - epoch 064 lr: 0.0010000000000000002
2022-02-25 17:42:51 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.1325
2022-02-25 17:43:25 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.3047
2022-02-25 17:43:57 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.2696
2022-02-25 17:44:31 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.0953
2022-02-25 17:45:04 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.0875
2022-02-25 17:45:38 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.1848
2022-02-25 17:46:12 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.3572
2022-02-25 17:46:46 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 1.0917
2022-02-25 17:47:19 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.0630
2022-02-25 17:47:52 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.2046
2022-02-25 17:48:25 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.2030
2022-02-25 17:48:58 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 0.9952
2022-02-25 17:49:33 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.1804
2022-02-25 17:50:05 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.3820
2022-02-25 17:50:38 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.2779
2022-02-25 17:51:12 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.1349
2022-02-25 17:51:44 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.0500
2022-02-25 17:52:19 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.1201
2022-02-25 17:52:51 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.3440
2022-02-25 17:53:26 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.1025
2022-02-25 17:53:59 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 1.1369
2022-02-25 17:54:32 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.2786
2022-02-25 17:55:05 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.2087
2022-02-25 17:55:39 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.1537
2022-02-25 17:56:12 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 1.2796
2022-02-25 17:56:46 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.1057
2022-02-25 17:57:19 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.2324
2022-02-25 17:57:53 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.2786
2022-02-25 17:58:26 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.1264
2022-02-25 17:59:01 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.0504
2022-02-25 17:59:33 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.1687
2022-02-25 18:00:07 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.2646
2022-02-25 18:00:40 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.1856
2022-02-25 18:01:15 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.0493
2022-02-25 18:01:48 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.1089
2022-02-25 18:02:22 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.1583
2022-02-25 18:02:56 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 1.0510
2022-02-25 18:03:30 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.3117
2022-02-25 18:04:04 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.3487
2022-02-25 18:04:37 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.1965
2022-02-25 18:05:11 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.2780
2022-02-25 18:05:45 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 0.9981
2022-02-25 18:06:19 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.1758
2022-02-25 18:06:52 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.0923
2022-02-25 18:07:27 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 0.9995
2022-02-25 18:08:01 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.4099
2022-02-25 18:08:35 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.4107
2022-02-25 18:09:09 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.3402
2022-02-25 18:09:45 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.3041
2022-02-25 18:10:18 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.0948
2022-02-25 18:10:20 - train: epoch 064, train_loss: 1.1988
2022-02-25 18:11:36 - eval: epoch: 064, acc1: 72.612%, acc5: 91.144%, test_loss: 1.0865, per_image_load_time: 2.629ms, per_image_inference_time: 0.321ms
2022-02-25 18:11:37 - until epoch: 064, best_acc1: 72.612%
2022-02-25 18:11:37 - epoch 065 lr: 0.0010000000000000002
2022-02-25 18:12:15 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.1998
2022-02-25 18:12:50 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.2778
2022-02-25 18:13:23 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.0440
2022-02-25 18:13:56 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.2173
2022-02-25 18:14:29 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.2583
2022-02-25 18:15:03 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.1634
2022-02-25 18:15:37 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.3007
2022-02-25 18:16:09 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.1644
2022-02-25 18:16:44 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.2847
2022-02-25 18:17:16 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.1647
2022-02-25 18:17:50 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.2939
2022-02-25 18:18:24 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.5956
2022-02-25 18:18:57 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.2923
2022-02-25 18:19:31 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 1.2195
2022-02-25 18:20:05 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.2560
2022-02-25 18:20:39 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.1429
2022-02-25 18:21:12 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.1534
2022-02-25 18:21:46 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.1795
2022-02-25 18:22:19 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 0.9637
2022-02-25 18:22:54 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.1481
2022-02-25 18:23:26 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.1257
2022-02-25 18:24:01 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 0.9932
2022-02-25 18:24:34 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.1684
2022-02-25 18:25:08 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.1726
2022-02-25 18:25:41 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.3477
2022-02-25 18:26:16 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.3584
2022-02-25 18:26:49 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.0985
2022-02-25 18:27:24 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.1486
2022-02-25 18:27:56 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.1483
2022-02-25 18:28:29 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.1561
2022-02-25 18:29:03 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.2414
2022-02-25 18:29:37 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.3543
2022-02-25 18:30:11 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.1532
2022-02-25 18:30:44 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.1772
2022-02-25 18:31:18 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.4297
2022-02-25 18:31:52 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.2537
2022-02-25 18:32:26 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.2055
2022-02-25 18:32:59 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.2502
2022-02-25 18:33:33 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.1870
2022-02-25 18:34:07 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.2607
2022-02-25 18:34:41 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.1957
2022-02-25 18:35:14 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.2958
2022-02-25 18:35:47 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.0627
2022-02-25 18:36:22 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.0793
2022-02-25 18:36:55 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.2771
2022-02-25 18:37:30 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.1756
2022-02-25 18:38:03 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.0790
2022-02-25 18:38:39 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 1.1085
2022-02-25 18:39:12 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.2640
2022-02-25 18:39:46 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.1725
2022-02-25 18:39:47 - train: epoch 065, train_loss: 1.1850
2022-02-25 18:41:04 - eval: epoch: 065, acc1: 72.826%, acc5: 91.156%, test_loss: 1.0807, per_image_load_time: 2.680ms, per_image_inference_time: 0.288ms
2022-02-25 18:41:05 - until epoch: 065, best_acc1: 72.826%
2022-02-25 18:41:05 - epoch 066 lr: 0.0010000000000000002
2022-02-25 18:41:43 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.0853
2022-02-25 18:42:17 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.3118
2022-02-25 18:42:51 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 1.1174
2022-02-25 18:43:24 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 1.0629
2022-02-25 18:43:58 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.3406
2022-02-25 18:44:31 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.1454
2022-02-25 18:45:05 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.0715
2022-02-25 18:45:38 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.3089
2022-02-25 18:46:11 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.0081
2022-02-25 18:46:45 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.1670
2022-02-25 18:47:19 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.3318
2022-02-25 18:47:53 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.1838
2022-02-25 18:48:26 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.1665
2022-02-25 18:49:00 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 1.0372
2022-02-25 18:49:34 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.3047
2022-02-25 18:50:07 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.3419
2022-02-25 18:50:41 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.0982
2022-02-25 18:51:14 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.2375
2022-02-25 18:51:48 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.1748
2022-02-25 18:52:21 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.1496
2022-02-25 18:52:56 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.3519
2022-02-25 18:53:29 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 1.1601
2022-02-25 18:54:03 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.2284
2022-02-25 18:54:37 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.3485
2022-02-25 18:55:11 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.0132
2022-02-25 18:55:44 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.1241
2022-02-25 18:56:18 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.3396
2022-02-25 18:56:51 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.1145
2022-02-25 18:57:25 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.1224
2022-02-25 18:57:58 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.1129
2022-02-25 18:58:32 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.3077
2022-02-25 18:59:05 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.2107
2022-02-25 18:59:39 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.1008
2022-02-25 19:00:13 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.0292
2022-02-25 19:00:48 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.3676
2022-02-25 19:01:21 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.1548
2022-02-25 19:01:54 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 0.9546
2022-02-25 19:02:30 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.2262
2022-02-25 19:03:03 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.2094
2022-02-25 19:03:36 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.2481
2022-02-25 19:04:10 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 1.1922
2022-02-25 19:04:44 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 1.0273
2022-02-25 19:05:17 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.1984
2022-02-25 19:05:52 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.3439
2022-02-25 19:06:24 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.2421
2022-02-25 19:06:59 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.3510
2022-02-25 19:07:33 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.0419
2022-02-25 19:08:08 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.2450
2022-02-25 19:08:42 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.2568
2022-02-25 19:09:15 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 1.1831
2022-02-25 19:09:17 - train: epoch 066, train_loss: 1.1752
2022-02-25 19:10:34 - eval: epoch: 066, acc1: 72.800%, acc5: 91.302%, test_loss: 1.0780, per_image_load_time: 2.666ms, per_image_inference_time: 0.295ms
2022-02-25 19:10:34 - until epoch: 066, best_acc1: 72.826%
2022-02-25 19:10:34 - epoch 067 lr: 0.0010000000000000002
2022-02-25 19:11:13 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.0896
2022-02-25 19:11:47 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.3029
2022-02-25 19:12:20 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.2333
2022-02-25 19:12:53 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.0643
2022-02-25 19:13:27 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.0246
2022-02-25 19:14:00 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 0.9488
2022-02-25 19:14:33 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 0.9823
2022-02-25 19:15:07 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.1917
2022-02-25 19:15:41 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.2066
2022-02-25 19:16:14 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 1.0428
2022-02-25 19:16:48 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 1.2770
2022-02-25 19:17:21 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.1821
2022-02-25 19:17:55 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.2560
2022-02-25 19:18:29 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.3123
2022-02-25 19:19:03 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 0.8908
2022-02-25 19:19:36 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.3270
2022-02-25 19:20:10 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.2187
2022-02-25 19:20:44 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.3865
2022-02-25 19:21:17 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.2798
2022-02-25 19:21:50 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.0939
2022-02-25 19:22:24 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 1.1004
2022-02-25 19:22:58 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 0.9904
2022-02-25 19:23:32 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.1606
2022-02-25 19:24:06 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 1.0783
2022-02-25 19:24:38 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 1.0734
2022-02-25 19:25:11 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.3383
2022-02-25 19:25:46 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.0995
2022-02-25 19:26:20 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.2054
2022-02-25 19:26:55 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.2425
2022-02-25 19:27:27 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.2668
2022-02-25 19:28:01 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 0.9997
2022-02-25 19:28:36 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.1907
2022-02-25 19:29:10 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.2696
2022-02-25 19:29:43 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.2135
2022-02-25 19:30:17 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.1762
2022-02-25 19:30:52 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.2189
2022-02-25 19:31:25 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.3263
2022-02-25 19:31:59 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.3332
2022-02-25 19:32:32 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 1.2181
2022-02-25 19:33:07 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.4181
2022-02-25 19:33:40 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.2979
2022-02-25 19:34:14 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.2423
2022-02-25 19:34:48 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.0997
2022-02-25 19:35:22 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.1378
2022-02-25 19:35:56 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 1.2654
2022-02-25 19:36:30 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.1179
2022-02-25 19:37:04 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.2537
2022-02-25 19:37:40 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.0383
2022-02-25 19:38:14 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.0695
2022-02-25 19:38:48 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.1564
2022-02-25 19:38:50 - train: epoch 067, train_loss: 1.1682
2022-02-25 19:40:06 - eval: epoch: 067, acc1: 72.942%, acc5: 91.240%, test_loss: 1.0766, per_image_load_time: 2.244ms, per_image_inference_time: 0.347ms
2022-02-25 19:40:06 - until epoch: 067, best_acc1: 72.942%
2022-02-25 19:40:06 - epoch 068 lr: 0.0010000000000000002
2022-02-25 19:40:45 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.1192
2022-02-25 19:41:18 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.1470
2022-02-25 19:41:52 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.1355
2022-02-25 19:42:25 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.2389
2022-02-25 19:42:59 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.0652
2022-02-25 19:43:32 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.0164
2022-02-25 19:44:07 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.2992
2022-02-25 19:44:40 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.1078
2022-02-25 19:45:14 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.1218
2022-02-25 19:45:47 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.0989
2022-02-25 19:46:20 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.1499
2022-02-25 19:46:54 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 0.9734
2022-02-25 19:47:27 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.1611
2022-02-25 19:48:01 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.1180
2022-02-25 19:48:34 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.1994
2022-02-25 19:49:08 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.1637
2022-02-25 19:49:43 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.0030
2022-02-25 19:50:16 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.0607
2022-02-25 19:50:49 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.1316
2022-02-25 19:51:23 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.3758
2022-02-25 19:51:57 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.1451
2022-02-25 19:52:31 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.1148
2022-02-25 19:53:06 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 0.9687
2022-02-25 19:53:39 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.2366
2022-02-25 19:54:13 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.1492
2022-02-25 19:54:46 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.0341
2022-02-25 19:55:20 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.1204
2022-02-25 19:55:53 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.3543
2022-02-25 19:56:28 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.3313
2022-02-25 19:57:02 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.1499
2022-02-25 19:57:35 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.0297
2022-02-25 19:58:09 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.0880
2022-02-25 19:58:44 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.1464
2022-02-25 19:59:17 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 1.1592
2022-02-25 19:59:51 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.2288
2022-02-25 20:00:26 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.1002
2022-02-25 20:00:59 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.2439
2022-02-25 20:01:33 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.2966
2022-02-25 20:02:07 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.2154
2022-02-25 20:02:40 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.1657
2022-02-25 20:03:15 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.0246
2022-02-25 20:03:48 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.2063
2022-02-25 20:04:23 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 1.1246
2022-02-25 20:04:55 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.0749
2022-02-25 20:05:29 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.3287
2022-02-25 20:06:04 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.2497
2022-02-25 20:06:39 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.3674
2022-02-25 20:07:13 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.1530
2022-02-25 20:07:48 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.0536
2022-02-25 20:08:22 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.1170
2022-02-25 20:08:23 - train: epoch 068, train_loss: 1.1629
2022-02-25 20:09:39 - eval: epoch: 068, acc1: 72.798%, acc5: 91.318%, test_loss: 1.0725, per_image_load_time: 2.028ms, per_image_inference_time: 0.331ms
2022-02-25 20:09:39 - until epoch: 068, best_acc1: 72.942%
2022-02-25 20:09:39 - epoch 069 lr: 0.0010000000000000002
2022-02-25 20:10:18 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.3544
2022-02-25 20:10:51 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.1237
2022-02-25 20:11:24 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.3168
2022-02-25 20:11:58 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.1109
2022-02-25 20:12:33 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.1532
2022-02-25 20:13:06 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 0.9183
2022-02-25 20:13:40 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.2780
2022-02-25 20:14:13 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.1168
2022-02-25 20:14:47 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.0697
2022-02-25 20:15:20 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.1446
2022-02-25 20:15:55 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.2498
2022-02-25 20:16:28 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.1620
2022-02-25 20:17:02 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.2597
2022-02-25 20:17:36 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.1150
2022-02-25 20:18:09 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.0321
2022-02-25 20:18:43 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.2219
2022-02-25 20:19:17 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.1092
2022-02-25 20:19:51 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.3003
2022-02-25 20:20:26 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.2092
2022-02-25 20:20:59 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 0.9591
2022-02-25 20:21:34 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.1807
2022-02-25 20:22:07 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.1455
2022-02-25 20:22:41 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.1382
2022-02-25 20:23:14 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.1949
2022-02-25 20:23:48 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.0032
2022-02-25 20:24:22 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.1539
2022-02-25 20:24:55 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.2582
2022-02-25 20:25:30 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.2070
2022-02-25 20:26:03 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 1.1769
2022-02-25 20:26:38 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.0484
2022-02-25 20:27:11 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 1.0957
2022-02-25 20:27:46 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 1.1083
2022-02-25 20:28:19 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.0272
2022-02-25 20:28:54 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 1.1282
2022-02-25 20:29:27 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 1.1353
2022-02-25 20:30:01 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.0179
2022-02-25 20:30:35 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.1759
2022-02-25 20:31:10 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.0683
2022-02-25 20:31:42 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.2167
2022-02-25 20:32:17 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.1570
2022-02-25 20:32:51 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.2424
2022-02-25 20:33:24 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.0275
2022-02-25 20:33:58 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.1103
2022-02-25 20:34:32 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.1424
2022-02-25 20:35:06 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.2691
2022-02-25 20:35:40 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.3391
2022-02-25 20:36:14 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.1900
2022-02-25 20:36:51 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.2302
2022-02-25 20:37:24 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 0.9858
2022-02-25 20:37:59 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.2621
2022-02-25 20:38:00 - train: epoch 069, train_loss: 1.1579
2022-02-25 20:39:17 - eval: epoch: 069, acc1: 73.018%, acc5: 91.286%, test_loss: 1.0710, per_image_load_time: 2.633ms, per_image_inference_time: 0.281ms
2022-02-25 20:39:17 - until epoch: 069, best_acc1: 73.018%
2022-02-25 20:39:17 - epoch 070 lr: 0.0010000000000000002
2022-02-25 20:39:56 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.0796
2022-02-25 20:40:30 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.1985
2022-02-25 20:41:04 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.2377
2022-02-25 20:41:37 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.0665
2022-02-25 20:42:11 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.1552
2022-02-25 20:42:44 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.2063
2022-02-25 20:43:19 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.1148
2022-02-25 20:43:52 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.1081
2022-02-25 20:44:26 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.0693
2022-02-25 20:44:59 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.1890
2022-02-25 20:45:34 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.3044
2022-02-25 20:46:07 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 1.0715
2022-02-25 20:46:40 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.0621
2022-02-25 20:47:14 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.2079
2022-02-25 20:47:48 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 0.9775
2022-02-25 20:48:22 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.2735
2022-02-25 20:48:57 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 0.9702
2022-02-25 20:49:31 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.0669
2022-02-25 20:50:03 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.1879
2022-02-25 20:50:38 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.0808
2022-02-25 20:51:12 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 1.1591
2022-02-25 20:51:46 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.0763
2022-02-25 20:52:19 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.2021
2022-02-25 20:52:53 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.2185
2022-02-25 20:53:28 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.1191
2022-02-25 20:54:01 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.0175
2022-02-25 20:54:35 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.1205
2022-02-25 20:55:09 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.4134
2022-02-25 20:55:42 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.0991
2022-02-25 20:56:17 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.1543
2022-02-25 20:56:50 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.1963
2022-02-25 20:57:24 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.2616
2022-02-25 20:57:58 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 0.9619
2022-02-25 20:58:31 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.1160
2022-02-25 20:59:06 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.0583
2022-02-25 20:59:40 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.0232
2022-02-25 21:00:14 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.1639
2022-02-25 21:00:48 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.0070
2022-02-25 21:01:21 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 1.0348
2022-02-25 21:01:54 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.1043
2022-02-25 21:02:28 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 1.1628
2022-02-25 21:03:03 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.0972
2022-02-25 21:03:36 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.1299
2022-02-25 21:04:12 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.0111
2022-02-25 21:04:44 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.0604
2022-02-25 21:05:19 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.3558
2022-02-25 21:05:52 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.3182
2022-02-25 21:06:27 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.2831
2022-02-25 21:07:01 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.1793
2022-02-25 21:07:35 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.0841
2022-02-25 21:07:36 - train: epoch 070, train_loss: 1.1511
2022-02-25 21:08:54 - eval: epoch: 070, acc1: 72.928%, acc5: 91.214%, test_loss: 1.0708, per_image_load_time: 1.005ms, per_image_inference_time: 0.284ms
2022-02-25 21:08:54 - until epoch: 070, best_acc1: 73.018%
2022-02-25 21:08:54 - epoch 071 lr: 0.0010000000000000002
2022-02-25 21:09:33 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.0798
2022-02-25 21:10:07 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.1236
2022-02-25 21:10:40 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.2951
2022-02-25 21:11:14 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.1408
2022-02-25 21:11:47 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.2424
2022-02-25 21:12:21 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.3021
2022-02-25 21:12:54 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.1821
2022-02-25 21:13:28 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.2593
2022-02-25 21:14:01 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.1393
2022-02-25 21:14:34 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.1179
2022-02-25 21:15:07 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.1941
2022-02-25 21:15:41 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.1693
2022-02-25 21:16:14 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.2391
2022-02-25 21:16:48 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.0048
2022-02-25 21:17:22 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 0.9741
2022-02-25 21:17:55 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 0.9203
2022-02-25 21:18:28 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.0878
2022-02-25 21:19:03 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.3176
2022-02-25 21:19:36 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 1.1543
2022-02-25 21:20:10 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.1816
2022-02-25 21:20:43 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.0127
2022-02-25 21:21:17 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 1.0737
2022-02-25 21:21:51 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 0.9924
2022-02-25 21:22:25 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.0950
2022-02-25 21:22:59 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.3642
2022-02-25 21:23:32 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.0861
2022-02-25 21:24:06 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.0862
2022-02-25 21:24:40 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.2276
2022-02-25 21:25:14 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.1926
2022-02-25 21:25:49 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.2805
2022-02-25 21:26:21 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.0556
2022-02-25 21:26:54 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.0686
2022-02-25 21:27:26 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.0624
2022-02-25 21:27:59 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.0533
2022-02-25 21:28:31 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.2337
2022-02-25 21:29:04 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.2237
2022-02-25 21:29:36 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.1868
2022-02-25 21:30:10 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.1611
2022-02-25 21:30:41 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.1761
2022-02-25 21:31:14 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.1286
2022-02-25 21:31:46 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.0047
2022-02-25 21:32:20 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.3402
2022-02-25 21:32:51 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.1108
2022-02-25 21:33:25 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.2023
2022-02-25 21:33:58 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.1986
2022-02-25 21:34:30 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.2359
2022-02-25 21:35:03 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.2038
2022-02-25 21:35:36 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 1.1519
2022-02-25 21:36:08 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 1.0328
2022-02-25 21:36:41 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.1828
2022-02-25 21:36:42 - train: epoch 071, train_loss: 1.1478
2022-02-25 21:37:55 - eval: epoch: 071, acc1: 73.200%, acc5: 91.316%, test_loss: 1.0676, per_image_load_time: 1.132ms, per_image_inference_time: 0.273ms
2022-02-25 21:37:56 - until epoch: 071, best_acc1: 73.200%
2022-02-25 21:37:56 - epoch 072 lr: 0.0010000000000000002
2022-02-25 21:38:33 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.1248
2022-02-25 21:39:06 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.2052
2022-02-25 21:39:38 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.1094
2022-02-25 21:40:11 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.1876
2022-02-25 21:40:43 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.0487
2022-02-25 21:41:15 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.1024
2022-02-25 21:41:48 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.3282
2022-02-25 21:42:21 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.2091
2022-02-25 21:42:54 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.0662
2022-02-25 21:43:27 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.1183
2022-02-25 21:44:00 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 1.2009
2022-02-25 21:44:32 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 1.0781
2022-02-25 21:45:07 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.1922
2022-02-25 21:45:39 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.1406
2022-02-25 21:46:12 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.1140
2022-02-25 21:46:45 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.1343
2022-02-25 21:47:18 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 0.9701
2022-02-25 21:47:50 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.1051
2022-02-25 21:48:23 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.2371
2022-02-25 21:48:56 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.2202
2022-02-25 21:49:29 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.1059
2022-02-25 21:50:02 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.3479
2022-02-25 21:50:34 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.2893
2022-02-25 21:51:08 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 1.0309
2022-02-25 21:51:40 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.1388
2022-02-25 21:52:14 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 1.1542
2022-02-25 21:52:47 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 1.0209
2022-02-25 21:53:20 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.1287
2022-02-25 21:53:52 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.0403
2022-02-25 21:54:25 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.0971
2022-02-25 21:54:59 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.1815
2022-02-25 21:55:32 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.0357
2022-02-25 21:56:03 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.1959
2022-02-25 21:56:37 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 1.1305
2022-02-25 21:57:09 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 1.1075
2022-02-25 21:57:44 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.0782
2022-02-25 21:58:18 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.1941
2022-02-25 21:58:52 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.0266
2022-02-25 21:59:27 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.0915
2022-02-25 22:00:01 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.1276
2022-02-25 22:00:35 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.2891
2022-02-25 22:01:08 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.0627
2022-02-25 22:01:40 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.1725
2022-02-25 22:02:14 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 1.0849
2022-02-25 22:02:46 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.2209
2022-02-25 22:03:19 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.0160
2022-02-25 22:03:52 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.0482
2022-02-25 22:04:25 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.2769
2022-02-25 22:04:58 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.2715
2022-02-25 22:05:30 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.1258
2022-02-25 22:05:32 - train: epoch 072, train_loss: 1.1422
2022-02-25 22:06:45 - eval: epoch: 072, acc1: 73.094%, acc5: 91.310%, test_loss: 1.0696, per_image_load_time: 1.707ms, per_image_inference_time: 0.310ms
2022-02-25 22:06:45 - until epoch: 072, best_acc1: 73.200%
2022-02-25 22:06:45 - epoch 073 lr: 0.0010000000000000002
2022-02-25 22:07:22 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.2437
2022-02-25 22:07:56 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.1941
2022-02-25 22:08:29 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.0788
2022-02-25 22:09:01 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 0.8683
2022-02-25 22:09:34 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.1189
2022-02-25 22:10:06 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.1501
2022-02-25 22:10:40 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.0287
2022-02-25 22:11:12 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.0026
2022-02-25 22:11:45 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.1541
2022-02-25 22:12:17 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 0.9912
2022-02-25 22:12:50 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.2409
2022-02-25 22:13:23 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.2147
2022-02-25 22:13:56 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.2168
2022-02-25 22:14:29 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.0997
2022-02-25 22:15:02 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.0824
2022-02-25 22:15:34 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.0951
2022-02-25 22:16:08 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.4064
2022-02-25 22:16:41 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 1.1089
2022-02-25 22:17:13 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.3514
2022-02-25 22:17:45 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.1822
2022-02-25 22:18:19 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.1572
2022-02-25 22:18:51 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.1673
2022-02-25 22:19:24 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.0969
2022-02-25 22:19:57 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.1670
2022-02-25 22:20:31 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.1229
2022-02-25 22:21:03 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 0.9816
2022-02-25 22:21:36 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.2241
2022-02-25 22:22:10 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.1175
2022-02-25 22:22:43 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.2400
2022-02-25 22:23:15 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 0.9702
2022-02-25 22:23:48 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.0990
2022-02-25 22:24:20 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 0.9401
2022-02-25 22:24:54 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 0.9058
2022-02-25 22:25:26 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 0.9911
2022-02-25 22:25:59 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.0441
2022-02-25 22:26:32 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 1.0814
2022-02-25 22:27:05 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.2995
2022-02-25 22:27:38 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.3125
2022-02-25 22:28:12 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.0670
2022-02-25 22:28:44 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.2438
2022-02-25 22:29:18 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 1.0565
2022-02-25 22:29:50 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.2600
2022-02-25 22:30:23 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.1335
2022-02-25 22:30:56 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.1771
2022-02-25 22:31:29 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.1344
2022-02-25 22:32:03 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.1667
2022-02-25 22:32:35 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 0.8673
2022-02-25 22:33:09 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.0958
2022-02-25 22:33:42 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.0331
2022-02-25 22:34:13 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.1674
2022-02-25 22:34:15 - train: epoch 073, train_loss: 1.1363
2022-02-25 22:35:29 - eval: epoch: 073, acc1: 73.000%, acc5: 91.268%, test_loss: 1.0681, per_image_load_time: 1.689ms, per_image_inference_time: 0.305ms
2022-02-25 22:35:30 - until epoch: 073, best_acc1: 73.200%
2022-02-25 22:35:30 - epoch 074 lr: 0.0010000000000000002
2022-02-25 22:36:08 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.0415
2022-02-25 22:36:40 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 1.2459
2022-02-25 22:37:13 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.1711
2022-02-25 22:37:46 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.2871
2022-02-25 22:38:20 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.2321
2022-02-25 22:38:51 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 0.8993
2022-02-25 22:39:25 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 1.0903
2022-02-25 22:39:58 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.2255
2022-02-25 22:40:31 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.1082
2022-02-25 22:41:05 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.3439
2022-02-25 22:41:37 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.1688
2022-02-25 22:42:11 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 1.1089
2022-02-25 22:42:45 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.1476
2022-02-25 22:43:18 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 1.0143
2022-02-25 22:43:51 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.0611
2022-02-25 22:44:24 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 1.0848
2022-02-25 22:44:57 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.2348
2022-02-25 22:45:31 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.1822
2022-02-25 22:46:04 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.2193
2022-02-25 22:46:37 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 0.8650
2022-02-25 22:47:09 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.2968
2022-02-25 22:47:43 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.3167
2022-02-25 22:48:17 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.3195
2022-02-25 22:48:51 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.3188
2022-02-25 22:49:25 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 0.9635
2022-02-25 22:49:59 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.0363
2022-02-25 22:50:33 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 0.9589
2022-02-25 22:51:06 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.5709
2022-02-25 22:51:40 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.3415
2022-02-25 22:52:12 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.2954
2022-02-25 22:52:45 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.2629
2022-02-25 22:53:19 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 1.1152
2022-02-25 22:53:53 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.1861
2022-02-25 22:54:25 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.2431
2022-02-25 22:54:58 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.0791
2022-02-25 22:55:32 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 1.3505
2022-02-25 22:56:04 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.1292
2022-02-25 22:56:38 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.0499
2022-02-25 22:57:11 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 1.1153
2022-02-25 22:57:44 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 1.2055
2022-02-25 22:58:17 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.2082
2022-02-25 22:58:50 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.0386
2022-02-25 22:59:23 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.2082
2022-02-25 22:59:57 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 1.0107
2022-02-25 23:00:29 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 1.1070
2022-02-25 23:01:03 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.3077
2022-02-25 23:01:36 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.0885
2022-02-25 23:02:10 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.1812
2022-02-25 23:02:43 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.1011
2022-02-25 23:03:16 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.1258
2022-02-25 23:03:17 - train: epoch 074, train_loss: 1.1359
2022-02-25 23:04:32 - eval: epoch: 074, acc1: 73.132%, acc5: 91.382%, test_loss: 1.0666, per_image_load_time: 2.575ms, per_image_inference_time: 0.305ms
2022-02-25 23:04:33 - until epoch: 074, best_acc1: 73.200%
2022-02-25 23:04:33 - epoch 075 lr: 0.0010000000000000002
2022-02-25 23:05:11 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 1.2895
2022-02-25 23:05:43 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 1.2252
2022-02-25 23:06:16 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.0556
2022-02-25 23:06:49 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 0.9720
2022-02-25 23:07:21 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.2233
2022-02-25 23:07:54 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.0680
2022-02-25 23:08:28 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.1254
2022-02-25 23:09:01 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.0485
2022-02-25 23:09:35 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.1967
2022-02-25 23:10:07 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 0.9493
2022-02-25 23:10:40 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.1272
2022-02-25 23:11:14 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.0377
2022-02-25 23:11:46 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.1485
2022-02-25 23:12:21 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.1486
2022-02-25 23:12:54 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.2295
2022-02-25 23:13:27 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 1.0446
2022-02-25 23:14:01 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 1.0418
2022-02-25 23:14:33 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.0115
2022-02-25 23:15:07 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 1.0132
2022-02-25 23:15:40 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.0167
2022-02-25 23:16:14 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.0877
2022-02-25 23:16:47 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.0316
2022-02-25 23:17:21 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 0.9523
2022-02-25 23:17:54 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.0912
2022-02-25 23:18:27 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.0888
2022-02-25 23:19:01 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.0763
2022-02-25 23:19:35 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 0.9215
2022-02-25 23:20:07 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.0893
2022-02-25 23:20:40 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.2139
2022-02-25 23:21:14 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.1860
2022-02-25 23:21:47 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.0067
2022-02-25 23:22:20 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.2203
2022-02-25 23:22:54 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.0808
2022-02-25 23:23:26 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 1.0369
2022-02-25 23:23:59 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.1414
2022-02-25 23:24:33 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.2355
2022-02-25 23:25:06 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.1535
2022-02-25 23:25:40 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 1.2167
2022-02-25 23:26:13 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.1722
2022-02-25 23:26:46 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.1123
2022-02-25 23:27:20 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 0.9629
2022-02-25 23:27:52 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.1516
2022-02-25 23:28:25 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.2394
2022-02-25 23:28:59 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 1.1108
2022-02-25 23:29:33 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.1414
2022-02-25 23:30:06 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 1.0558
2022-02-25 23:30:39 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.5719
2022-02-25 23:31:14 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.0155
2022-02-25 23:31:47 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.1743
2022-02-25 23:32:19 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.0825
2022-02-25 23:32:21 - train: epoch 075, train_loss: 1.1292
2022-02-25 23:33:36 - eval: epoch: 075, acc1: 73.066%, acc5: 91.332%, test_loss: 1.0679, per_image_load_time: 1.633ms, per_image_inference_time: 0.304ms
2022-02-25 23:33:36 - until epoch: 075, best_acc1: 73.200%
2022-02-25 23:33:36 - epoch 076 lr: 0.0010000000000000002
2022-02-25 23:34:15 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 1.0661
2022-02-25 23:34:48 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.0603
2022-02-25 23:35:20 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.2913
2022-02-25 23:35:53 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.1915
2022-02-25 23:36:26 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.1994
2022-02-25 23:36:59 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.1822
2022-02-25 23:37:32 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.0382
2022-02-25 23:38:06 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.0977
2022-02-25 23:38:40 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 1.0215
2022-02-25 23:39:15 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.0901
2022-02-25 23:39:49 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.2036
2022-02-25 23:40:23 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.1002
2022-02-25 23:40:56 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.1831
2022-02-25 23:41:30 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 0.9956
2022-02-25 23:42:03 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.0891
2022-02-25 23:42:36 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.3813
2022-02-25 23:43:08 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.0106
2022-02-25 23:43:41 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 1.0476
2022-02-25 23:44:15 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.1647
2022-02-25 23:44:48 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 1.1021
2022-02-25 23:45:22 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.1608
2022-02-25 23:45:55 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.1926
2022-02-25 23:46:27 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 0.9384
2022-02-25 23:47:01 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.3368
2022-02-25 23:47:33 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 0.9858
2022-02-25 23:48:07 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.2887
2022-02-25 23:48:40 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.0688
2022-02-25 23:49:13 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 1.1040
2022-02-25 23:49:46 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 1.0718
2022-02-25 23:50:21 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 1.0736
2022-02-25 23:50:53 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 1.1415
2022-02-25 23:51:27 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.0408
2022-02-25 23:52:01 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.0498
2022-02-25 23:52:34 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.2592
2022-02-25 23:53:07 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 0.9542
2022-02-25 23:53:41 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 1.0283
2022-02-25 23:54:15 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 1.0744
2022-02-25 23:54:49 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 0.9142
2022-02-25 23:55:22 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 0.9527
2022-02-25 23:55:55 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 1.0781
2022-02-25 23:56:30 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 1.0537
2022-02-25 23:57:03 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.1707
2022-02-25 23:57:37 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 1.2779
2022-02-25 23:58:10 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 1.3062
2022-02-25 23:58:43 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 1.2350
2022-02-25 23:59:17 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 1.0606
2022-02-25 23:59:51 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.0863
2022-02-26 00:00:25 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 1.1293
2022-02-26 00:01:00 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 1.1855
2022-02-26 00:01:32 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.1976
2022-02-26 00:01:34 - train: epoch 076, train_loss: 1.1276
2022-02-26 00:02:47 - eval: epoch: 076, acc1: 73.208%, acc5: 91.400%, test_loss: 1.0641, per_image_load_time: 2.201ms, per_image_inference_time: 0.322ms
2022-02-26 00:02:48 - until epoch: 076, best_acc1: 73.208%
2022-02-26 00:02:48 - epoch 077 lr: 0.0010000000000000002
2022-02-26 00:03:26 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 1.2356
2022-02-26 00:03:59 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 1.2679
2022-02-26 00:04:32 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 1.0664
2022-02-26 00:05:06 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.1238
2022-02-26 00:05:38 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 1.0190
2022-02-26 00:06:12 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 1.0238
2022-02-26 00:06:44 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 0.9822
2022-02-26 00:07:18 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.0572
2022-02-26 00:07:53 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.1962
2022-02-26 00:08:25 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 0.8426
2022-02-26 00:09:00 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 1.1280
2022-02-26 00:09:33 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 1.3195
2022-02-26 00:10:08 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 1.0678
2022-02-26 00:10:40 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.0095
2022-02-26 00:11:14 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 1.0010
2022-02-26 00:11:47 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 1.0435
2022-02-26 00:12:20 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.1385
2022-02-26 00:12:54 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 1.0155
2022-02-26 00:13:26 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 0.8580
2022-02-26 00:14:00 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 1.1314
2022-02-26 00:14:33 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 1.3383
2022-02-26 00:15:07 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 1.0366
2022-02-26 00:15:41 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.3452
2022-02-26 00:16:15 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 1.1243
2022-02-26 00:16:48 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.1644
2022-02-26 00:17:22 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 0.9352
2022-02-26 00:17:56 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 1.1507
2022-02-26 00:18:29 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.1900
2022-02-26 00:19:03 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.3478
2022-02-26 00:19:36 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 1.1865
2022-02-26 00:20:10 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 1.1010
2022-02-26 00:20:42 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.2421
2022-02-26 00:21:16 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 1.0406
2022-02-26 00:21:50 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.2134
2022-02-26 00:22:23 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.0609
2022-02-26 00:22:57 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 0.9696
2022-02-26 00:23:30 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 0.9725
2022-02-26 00:24:03 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 1.1443
2022-02-26 00:24:37 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 0.9672
2022-02-26 00:25:10 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 1.1640
2022-02-26 00:25:45 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 0.9776
2022-02-26 00:26:18 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.1492
2022-02-26 00:26:51 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 1.1336
2022-02-26 00:27:24 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 1.1950
2022-02-26 00:27:59 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.1382
2022-02-26 00:28:32 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 0.8919
2022-02-26 00:29:06 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 1.0519
2022-02-26 00:29:41 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 1.0720
2022-02-26 00:30:15 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.4216
2022-02-26 00:30:48 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.0281
2022-02-26 00:30:50 - train: epoch 077, train_loss: 1.1230
2022-02-26 00:32:05 - eval: epoch: 077, acc1: 73.206%, acc5: 91.402%, test_loss: 1.0626, per_image_load_time: 2.580ms, per_image_inference_time: 0.323ms
2022-02-26 00:32:06 - until epoch: 077, best_acc1: 73.208%
2022-02-26 00:32:06 - epoch 078 lr: 0.0010000000000000002
2022-02-26 00:32:44 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 1.1633
2022-02-26 00:33:18 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.1761
2022-02-26 00:33:50 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.1890
2022-02-26 00:34:23 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 1.0430
2022-02-26 00:34:56 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.0507
2022-02-26 00:35:29 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 1.2057
2022-02-26 00:36:02 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.3057
2022-02-26 00:36:35 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.2740
2022-02-26 00:37:08 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 1.0486
2022-02-26 00:37:42 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 1.0735
2022-02-26 00:38:14 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 1.1588
2022-02-26 00:38:48 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 0.8969
2022-02-26 00:39:22 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 0.9312
2022-02-26 00:39:55 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 0.9009
2022-02-26 00:40:29 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 1.1624
2022-02-26 00:41:01 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 1.0296
2022-02-26 00:41:35 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 1.0784
2022-02-26 00:42:09 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 0.9815
2022-02-26 00:42:42 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 1.1729
2022-02-26 00:43:16 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 1.0697
2022-02-26 00:43:48 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.2554
2022-02-26 00:44:23 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 1.0587
2022-02-26 00:44:55 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 0.9382
2022-02-26 00:45:29 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 1.1277
2022-02-26 00:46:03 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.1259
2022-02-26 00:46:36 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 1.2035
2022-02-26 00:47:10 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 1.2160
2022-02-26 00:47:43 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 1.2113
2022-02-26 00:48:17 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 1.0494
2022-02-26 00:48:49 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 1.2876
2022-02-26 00:49:23 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.1262
2022-02-26 00:49:56 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 1.1713
2022-02-26 00:50:30 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.3504
2022-02-26 00:51:02 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 1.1746
2022-02-26 00:51:37 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 1.2403
2022-02-26 00:52:09 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.1737
2022-02-26 00:52:43 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 1.0919
2022-02-26 00:53:17 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 1.1115
2022-02-26 00:53:49 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 0.9583
2022-02-26 00:54:23 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 0.9755
2022-02-26 00:54:56 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.0941
2022-02-26 00:55:30 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.3268
2022-02-26 00:56:03 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 1.2240
2022-02-26 00:56:38 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 1.1659
2022-02-26 00:57:12 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.2300
2022-02-26 00:57:45 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 1.0563
2022-02-26 00:58:19 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 1.1636
2022-02-26 00:58:52 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 1.1302
2022-02-26 00:59:25 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 0.9415
2022-02-26 00:59:57 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 1.1654
2022-02-26 00:59:59 - train: epoch 078, train_loss: 1.1196
2022-02-26 01:01:13 - eval: epoch: 078, acc1: 73.114%, acc5: 91.360%, test_loss: 1.0672, per_image_load_time: 2.488ms, per_image_inference_time: 0.336ms
2022-02-26 01:01:14 - until epoch: 078, best_acc1: 73.208%
2022-02-26 01:01:14 - epoch 079 lr: 0.0010000000000000002
2022-02-26 01:01:52 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 0.9767
2022-02-26 01:02:26 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 1.0425
2022-02-26 01:02:59 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 1.0908
2022-02-26 01:03:32 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 1.1220
2022-02-26 01:04:07 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 0.9954
2022-02-26 01:04:39 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 1.0457
2022-02-26 01:05:14 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 0.9632
2022-02-26 01:05:47 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 1.1323
2022-02-26 01:06:21 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 1.0929
2022-02-26 01:06:55 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 1.1220
2022-02-26 01:07:29 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 1.2151
2022-02-26 01:08:03 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.2743
2022-02-26 01:08:36 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 0.9846
2022-02-26 01:09:11 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 1.1410
2022-02-26 01:09:44 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 0.9849
2022-02-26 01:10:18 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 1.0153
2022-02-26 01:10:51 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 1.1768
2022-02-26 01:11:26 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 1.1466
2022-02-26 01:11:59 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.3211
2022-02-26 01:12:33 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.1365
2022-02-26 01:13:07 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 1.1279
2022-02-26 01:13:41 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 1.1541
2022-02-26 01:14:15 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 1.2086
2022-02-26 01:14:48 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.0839
2022-02-26 01:15:23 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 1.0542
2022-02-26 01:15:56 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 1.0266
2022-02-26 01:16:31 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 0.9583
2022-02-26 01:17:04 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 0.9852
2022-02-26 01:17:38 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 1.0069
2022-02-26 01:18:12 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.1364
2022-02-26 01:18:45 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 1.0414
2022-02-26 01:19:20 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.1593
2022-02-26 01:19:53 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 1.0530
2022-02-26 01:20:27 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 1.1124
2022-02-26 01:21:00 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.0876
2022-02-26 01:21:34 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 1.1016
2022-02-26 01:22:08 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 1.0540
2022-02-26 01:22:42 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 1.1386
2022-02-26 01:23:16 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 1.1142
2022-02-26 01:23:50 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 0.8625
2022-02-26 01:24:24 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.3611
2022-02-26 01:24:58 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.1290
2022-02-26 01:25:32 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 0.8612
2022-02-26 01:26:06 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.3473
2022-02-26 01:26:40 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.2693
2022-02-26 01:27:13 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.0547
2022-02-26 01:27:48 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 1.1262
2022-02-26 01:28:22 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.0999
2022-02-26 01:28:57 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.1453
2022-02-26 01:29:31 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 1.0750
2022-02-26 01:29:33 - train: epoch 079, train_loss: 1.1154
2022-02-26 01:30:49 - eval: epoch: 079, acc1: 73.048%, acc5: 91.444%, test_loss: 1.0651, per_image_load_time: 2.473ms, per_image_inference_time: 0.320ms
2022-02-26 01:30:49 - until epoch: 079, best_acc1: 73.208%
2022-02-26 01:30:49 - epoch 080 lr: 0.0010000000000000002
2022-02-26 01:31:28 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 1.1286
2022-02-26 01:32:01 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 1.0512
2022-02-26 01:32:35 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.3942
2022-02-26 01:33:08 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 0.9096
2022-02-26 01:33:42 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 0.9988
2022-02-26 01:34:16 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 1.2729
2022-02-26 01:34:49 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 1.0728
2022-02-26 01:35:23 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 1.0554
2022-02-26 01:35:57 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 1.1052
2022-02-26 01:36:30 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 1.0644
2022-02-26 01:37:03 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 1.1537
2022-02-26 01:37:38 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 1.0169
2022-02-26 01:38:11 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 1.0660
2022-02-26 01:38:45 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 1.1302
2022-02-26 01:39:19 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 1.1473
2022-02-26 01:39:53 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 1.1072
2022-02-26 01:40:26 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 1.1370
2022-02-26 01:41:00 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 1.1662
2022-02-26 01:41:33 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 1.2123
2022-02-26 01:42:07 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 1.0937
2022-02-26 01:42:41 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 1.2559
2022-02-26 01:43:15 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 1.0393
2022-02-26 01:43:48 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 1.2463
2022-02-26 01:44:22 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 1.2384
2022-02-26 01:44:55 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 1.0843
2022-02-26 01:45:29 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 1.1921
2022-02-26 01:46:02 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 1.0592
2022-02-26 01:46:36 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 1.0391
2022-02-26 01:47:09 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 1.1476
2022-02-26 01:47:43 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 1.0062
2022-02-26 01:48:17 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 1.1659
2022-02-26 01:48:51 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 1.0300
2022-02-26 01:49:24 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 1.1005
2022-02-26 01:49:58 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 1.0208
2022-02-26 01:50:32 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 0.9601
2022-02-26 01:51:05 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 1.1031
2022-02-26 01:51:39 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 1.0795
2022-02-26 01:52:12 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 1.2565
2022-02-26 01:52:46 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 0.9385
2022-02-26 01:53:20 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 1.2150
2022-02-26 01:53:54 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 1.3671
2022-02-26 01:54:28 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 1.0145
2022-02-26 01:55:01 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 1.3879
2022-02-26 01:55:34 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 1.1499
2022-02-26 01:56:08 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 1.0699
2022-02-26 01:56:42 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 0.9622
2022-02-26 01:57:16 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 1.1547
2022-02-26 01:57:51 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 1.2001
2022-02-26 01:58:25 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.2906
2022-02-26 01:58:58 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 1.1513
2022-02-26 01:59:01 - train: epoch 080, train_loss: 1.1140
2022-02-26 02:00:15 - eval: epoch: 080, acc1: 73.230%, acc5: 91.382%, test_loss: 1.0660, per_image_load_time: 2.493ms, per_image_inference_time: 0.316ms
2022-02-26 02:00:16 - until epoch: 080, best_acc1: 73.230%
2022-02-26 02:00:16 - epoch 081 lr: 0.0010000000000000002
2022-02-26 02:00:55 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 0.9955
2022-02-26 02:01:28 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 0.9549
2022-02-26 02:02:02 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 0.9979
2022-02-26 02:02:35 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 1.0959
2022-02-26 02:03:08 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 1.2575
2022-02-26 02:03:41 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 1.0336
2022-02-26 02:04:15 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 1.0254
2022-02-26 02:04:49 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 1.2412
2022-02-26 02:05:22 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 1.0960
2022-02-26 02:05:56 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 1.0367
2022-02-26 02:06:30 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 1.1136
2022-02-26 02:07:03 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 1.2706
2022-02-26 02:07:36 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 1.0373
2022-02-26 02:08:10 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 0.7810
2022-02-26 02:08:44 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 1.3048
2022-02-26 02:09:18 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 0.9529
2022-02-26 02:09:52 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 1.1844
2022-02-26 02:10:25 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 1.2732
2022-02-26 02:10:58 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 1.1352
2022-02-26 02:11:33 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 1.1446
2022-02-26 02:12:07 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 1.0468
2022-02-26 02:12:41 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 0.9451
2022-02-26 02:13:15 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 0.9080
2022-02-26 02:13:48 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 1.1445
2022-02-26 02:14:22 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 1.3248
2022-02-26 02:14:56 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.0355
2022-02-26 02:15:29 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 1.1761
2022-02-26 02:16:03 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 1.0538
2022-02-26 02:16:37 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 1.1641
2022-02-26 02:17:10 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 1.1093
2022-02-26 02:17:44 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 1.2464
2022-02-26 02:18:17 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 1.0508
2022-02-26 02:18:52 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 1.0930
2022-02-26 02:19:25 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 1.0414
2022-02-26 02:20:00 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 1.1457
2022-02-26 02:20:33 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 0.9162
2022-02-26 02:21:07 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 1.0887
2022-02-26 02:21:40 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 0.9703
2022-02-26 02:22:15 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 1.1728
2022-02-26 02:22:48 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 0.9576
2022-02-26 02:23:22 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 0.9689
2022-02-26 02:23:56 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 1.2606
2022-02-26 02:24:29 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 1.0710
2022-02-26 02:25:03 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 1.0263
2022-02-26 02:25:38 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 1.1743
2022-02-26 02:26:10 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 1.0274
2022-02-26 02:26:45 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 1.1834
2022-02-26 02:27:19 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 1.1012
2022-02-26 02:27:53 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 0.9709
2022-02-26 02:28:27 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 1.0975
2022-02-26 02:28:29 - train: epoch 081, train_loss: 1.1099
2022-02-26 02:29:44 - eval: epoch: 081, acc1: 73.214%, acc5: 91.436%, test_loss: 1.0661, per_image_load_time: 2.475ms, per_image_inference_time: 0.331ms
2022-02-26 02:29:45 - until epoch: 081, best_acc1: 73.230%
2022-02-26 02:29:45 - epoch 082 lr: 0.0010000000000000002
2022-02-26 02:30:24 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 0.9756
2022-02-26 02:30:57 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 1.0390
2022-02-26 02:31:29 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 1.1158
2022-02-26 02:32:04 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 1.1898
2022-02-26 02:32:37 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 1.2063
2022-02-26 02:33:11 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 1.1399
2022-02-26 02:33:44 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 1.0662
2022-02-26 02:34:18 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 1.3094
2022-02-26 02:34:52 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 1.2257
2022-02-26 02:35:25 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 1.0631
2022-02-26 02:35:59 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 1.2560
2022-02-26 02:36:33 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 1.1391
2022-02-26 02:37:07 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 1.3595
2022-02-26 02:37:41 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 1.0526
2022-02-26 02:38:15 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 1.1866
2022-02-26 02:38:49 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 1.1005
2022-02-26 02:39:23 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 1.0215
2022-02-26 02:39:56 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 1.0082
2022-02-26 02:40:30 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 1.1770
2022-02-26 02:41:04 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 1.0152
2022-02-26 02:41:38 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 1.1968
2022-02-26 02:42:11 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 1.1530
2022-02-26 02:42:45 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 1.1965
2022-02-26 02:43:18 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 1.1205
2022-02-26 02:43:52 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 1.1586
2022-02-26 02:44:26 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 1.1531
2022-02-26 02:45:00 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 0.9923
2022-02-26 02:45:33 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 1.1054
2022-02-26 02:46:07 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 0.9375
2022-02-26 02:46:40 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 1.0579
2022-02-26 02:47:14 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 1.2014
2022-02-26 02:47:47 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 1.2711
2022-02-26 02:48:21 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 1.0313
2022-02-26 02:48:54 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 1.0663
2022-02-26 02:49:28 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 0.9259
2022-02-26 02:50:01 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 1.0632
2022-02-26 02:50:35 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 1.0668
2022-02-26 02:51:09 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 1.2065
2022-02-26 02:51:43 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 1.1517
2022-02-26 02:52:16 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 1.0236
2022-02-26 02:52:50 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 1.1468
2022-02-26 02:53:24 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 1.1380
2022-02-26 02:53:59 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 1.1523
2022-02-26 02:54:32 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 0.9384
2022-02-26 02:55:06 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 0.9509
2022-02-26 02:55:40 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 1.1609
2022-02-26 02:56:14 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 1.1042
2022-02-26 02:56:47 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 1.0956
2022-02-26 02:57:22 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 1.2130
2022-02-26 02:57:54 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 1.0611
2022-02-26 02:57:56 - train: epoch 082, train_loss: 1.1089
2022-02-26 02:59:12 - eval: epoch: 082, acc1: 73.318%, acc5: 91.458%, test_loss: 1.0652, per_image_load_time: 1.357ms, per_image_inference_time: 0.291ms
2022-02-26 02:59:13 - until epoch: 082, best_acc1: 73.318%
2022-02-26 02:59:13 - epoch 083 lr: 0.0010000000000000002
2022-02-26 02:59:52 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 0.8558
2022-02-26 03:00:24 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 0.8856
2022-02-26 03:00:58 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 1.2183
2022-02-26 03:01:31 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 1.2245
2022-02-26 03:02:06 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 1.1568
2022-02-26 03:02:38 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 0.9405
2022-02-26 03:03:12 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 1.0419
2022-02-26 03:03:46 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 1.1171
2022-02-26 03:04:20 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 1.2400
2022-02-26 03:04:53 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 1.1638
2022-02-26 03:05:27 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 0.9964
2022-02-26 03:06:00 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 1.0509
2022-02-26 03:06:34 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 1.1673
2022-02-26 03:07:07 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 1.1131
2022-02-26 03:07:41 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 1.2242
2022-02-26 03:08:15 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 1.2820
2022-02-26 03:08:49 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 1.0605
2022-02-26 03:09:22 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 1.2490
2022-02-26 03:09:56 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 1.0397
2022-02-26 03:10:29 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 0.8297
2022-02-26 03:11:03 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 1.0596
2022-02-26 03:11:37 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 1.1050
2022-02-26 03:12:10 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 1.0787
2022-02-26 03:12:44 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 1.1362
2022-02-26 03:13:18 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 0.8545
2022-02-26 03:13:52 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 1.0973
2022-02-26 03:14:26 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 1.1030
2022-02-26 03:14:59 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 0.9861
2022-02-26 03:15:33 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 1.0908
2022-02-26 03:16:07 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 1.2077
2022-02-26 03:16:40 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 1.0478
2022-02-26 03:17:13 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 1.0935
2022-02-26 03:17:48 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 1.1694
2022-02-26 03:18:21 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 1.0984
2022-02-26 03:18:55 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 1.2400
2022-02-26 03:19:29 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 1.3060
2022-02-26 03:20:02 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 1.0397
2022-02-26 03:20:36 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 0.9492
2022-02-26 03:21:11 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 1.2932
2022-02-26 03:21:44 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 1.2349
2022-02-26 03:22:18 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 1.1337
2022-02-26 03:22:51 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 1.2913
2022-02-26 03:23:26 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 1.1657
2022-02-26 03:23:59 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 1.1457
2022-02-26 03:24:34 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 1.0020
2022-02-26 03:25:07 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 1.0684
2022-02-26 03:25:42 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.3250
2022-02-26 03:26:16 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 1.1580
2022-02-26 03:26:50 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 1.1072
2022-02-26 03:27:23 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 0.9593
2022-02-26 03:27:26 - train: epoch 083, train_loss: 1.1040
2022-02-26 03:28:41 - eval: epoch: 083, acc1: 73.302%, acc5: 91.436%, test_loss: 1.0624, per_image_load_time: 1.914ms, per_image_inference_time: 0.311ms
2022-02-26 03:28:42 - until epoch: 083, best_acc1: 73.318%
2022-02-26 03:28:42 - epoch 084 lr: 0.0010000000000000002
2022-02-26 03:29:20 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 0.9838
2022-02-26 03:29:54 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 1.0616
2022-02-26 03:30:27 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 0.9431
2022-02-26 03:31:01 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 1.0384
2022-02-26 03:31:35 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 1.2499
2022-02-26 03:32:09 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 1.2638
2022-02-26 03:32:41 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 1.3116
2022-02-26 03:33:15 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 1.2149
2022-02-26 03:33:49 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 1.2262
2022-02-26 03:34:22 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 1.1600
2022-02-26 03:34:56 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 1.0613
2022-02-26 03:35:30 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 1.1095
2022-02-26 03:36:04 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 0.9390
2022-02-26 03:36:38 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 1.0227
2022-02-26 03:37:12 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 1.0729
2022-02-26 03:37:45 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 1.0759
2022-02-26 03:38:19 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 1.4117
2022-02-26 03:38:52 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 1.1321
2022-02-26 03:39:26 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 1.1774
2022-02-26 03:39:59 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 1.0921
2022-02-26 03:40:33 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 1.0126
2022-02-26 03:41:07 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 1.0053
2022-02-26 03:41:41 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 1.0335
2022-02-26 03:42:14 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 1.0159
2022-02-26 03:42:48 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 1.0712
2022-02-26 03:43:22 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 1.1130
2022-02-26 03:43:56 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 1.1374
2022-02-26 03:44:29 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 1.1360
2022-02-26 03:45:04 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 1.2980
2022-02-26 03:45:37 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 1.2164
2022-02-26 03:46:10 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 1.0609
2022-02-26 03:46:43 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 1.0565
2022-02-26 03:47:17 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 1.2303
2022-02-26 03:47:51 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 1.2503
2022-02-26 03:48:24 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 1.1407
2022-02-26 03:48:57 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 1.0999
2022-02-26 03:49:32 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 1.3588
2022-02-26 03:50:05 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 1.2322
2022-02-26 03:50:40 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 1.2020
2022-02-26 03:51:13 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 0.9135
2022-02-26 03:51:47 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 1.1239
2022-02-26 03:52:21 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 1.1773
2022-02-26 03:52:55 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 1.0966
2022-02-26 03:53:28 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.4657
2022-02-26 03:54:03 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 1.0699
2022-02-26 03:54:36 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 0.9635
2022-02-26 03:55:10 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 0.9762
2022-02-26 03:55:44 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 1.1359
2022-02-26 03:56:18 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 1.0255
2022-02-26 03:56:52 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 1.3489
2022-02-26 03:56:54 - train: epoch 084, train_loss: 1.1007
2022-02-26 03:58:10 - eval: epoch: 084, acc1: 73.192%, acc5: 91.388%, test_loss: 1.0663, per_image_load_time: 1.497ms, per_image_inference_time: 0.317ms
2022-02-26 03:58:11 - until epoch: 084, best_acc1: 73.318%
2022-02-26 03:58:11 - epoch 085 lr: 0.0010000000000000002
2022-02-26 03:58:49 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 1.0858
2022-02-26 03:59:22 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 0.9364
2022-02-26 03:59:56 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 1.1067
2022-02-26 04:00:29 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 1.0594
2022-02-26 04:01:03 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 1.2868
2022-02-26 04:01:36 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 0.8984
2022-02-26 04:02:10 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 1.0144
2022-02-26 04:02:44 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 0.9945
2022-02-26 04:03:17 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 1.0747
2022-02-26 04:03:51 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 1.0663
2022-02-26 04:04:25 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 1.0325
2022-02-26 04:04:58 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 1.0633
2022-02-26 04:05:32 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 1.1785
2022-02-26 04:06:06 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 1.0633
2022-02-26 04:06:39 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 1.2445
2022-02-26 04:07:13 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 1.0843
2022-02-26 04:07:46 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 1.3817
2022-02-26 04:08:19 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 0.9363
2022-02-26 04:08:53 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 1.1387
2022-02-26 04:09:27 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 1.0341
2022-02-26 04:10:01 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 1.0538
2022-02-26 04:10:34 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 1.1485
2022-02-26 04:11:08 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 0.9218
2022-02-26 04:11:41 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 1.2239
2022-02-26 04:12:16 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 1.1518
2022-02-26 04:12:49 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 1.3071
2022-02-26 04:13:23 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 1.0320
2022-02-26 04:13:56 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 1.3295
2022-02-26 04:14:30 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 1.2802
2022-02-26 04:15:03 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 1.2488
2022-02-26 04:15:37 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 1.1716
2022-02-26 04:16:12 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 0.9893
2022-02-26 04:16:45 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 0.8993
2022-02-26 04:17:19 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 1.1730
2022-02-26 04:17:53 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 1.1937
2022-02-26 04:18:26 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 1.1369
2022-02-26 04:19:00 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 1.0590
2022-02-26 04:19:34 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 1.1507
2022-02-26 04:20:07 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 1.3065
2022-02-26 04:20:41 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 1.2583
2022-02-26 04:21:15 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 1.0701
2022-02-26 04:21:49 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 1.0953
2022-02-26 04:22:22 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 1.1474
2022-02-26 04:22:57 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 1.1590
2022-02-26 04:23:30 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 1.0492
2022-02-26 04:24:04 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 1.0419
2022-02-26 04:24:38 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.3263
2022-02-26 04:25:13 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 0.9660
2022-02-26 04:25:47 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 1.2311
2022-02-26 04:26:21 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 0.9920
2022-02-26 04:26:23 - train: epoch 085, train_loss: 1.0979
2022-02-26 04:27:38 - eval: epoch: 085, acc1: 73.206%, acc5: 91.378%, test_loss: 1.0676, per_image_load_time: 2.569ms, per_image_inference_time: 0.316ms
2022-02-26 04:27:39 - until epoch: 085, best_acc1: 73.318%
2022-02-26 04:27:39 - epoch 086 lr: 0.0010000000000000002
2022-02-26 04:28:18 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 1.0200
2022-02-26 04:28:51 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 1.0836
2022-02-26 04:29:25 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 1.1649
2022-02-26 04:29:58 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 1.1402
2022-02-26 04:30:31 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 1.0717
2022-02-26 04:31:05 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 1.1199
2022-02-26 04:31:39 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 0.9537
2022-02-26 04:32:13 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 1.0895
2022-02-26 04:32:46 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 0.8807
2022-02-26 04:33:20 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 1.0520
2022-02-26 04:33:54 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 1.0591
2022-02-26 04:34:27 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 1.0089
2022-02-26 04:35:00 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 1.2206
2022-02-26 04:35:35 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 1.0677
2022-02-26 04:36:09 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 0.9798
2022-02-26 04:36:42 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 0.9799
2022-02-26 04:37:16 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 1.0513
2022-02-26 04:37:50 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 1.1700
2022-02-26 04:38:23 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 1.0592
2022-02-26 04:38:57 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 1.1730
2022-02-26 04:39:31 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 0.8508
2022-02-26 04:40:04 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 1.0115
2022-02-26 04:40:39 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 1.0160
2022-02-26 04:41:12 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 0.9515
2022-02-26 04:41:47 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 1.2003
2022-02-26 04:42:20 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 1.2156
2022-02-26 04:42:54 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 1.1238
2022-02-26 04:43:28 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 1.0760
2022-02-26 04:44:02 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 1.0190
2022-02-26 04:44:35 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 0.9608
2022-02-26 04:45:09 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 1.0885
2022-02-26 04:45:42 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 1.3542
2022-02-26 04:46:17 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 1.1844
2022-02-26 04:46:50 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 1.2228
2022-02-26 04:47:24 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 1.1903
2022-02-26 04:47:58 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 1.1529
2022-02-26 04:48:32 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 1.0434
2022-02-26 04:49:05 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 1.1646
2022-02-26 04:49:40 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 0.9935
2022-02-26 04:50:13 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 1.0520
2022-02-26 04:50:47 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 1.0451
2022-02-26 04:51:21 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 1.0568
2022-02-26 04:51:55 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 1.1081
2022-02-26 04:52:29 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 0.9540
2022-02-26 04:53:02 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 0.9492
2022-02-26 04:53:37 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 1.0987
2022-02-26 04:54:11 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 1.2275
2022-02-26 04:54:45 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 1.1823
2022-02-26 04:55:20 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 1.2623
2022-02-26 04:55:53 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 1.2568
2022-02-26 04:55:56 - train: epoch 086, train_loss: 1.0967
2022-02-26 04:57:11 - eval: epoch: 086, acc1: 73.190%, acc5: 91.462%, test_loss: 1.0648, per_image_load_time: 1.736ms, per_image_inference_time: 0.318ms
2022-02-26 04:57:11 - until epoch: 086, best_acc1: 73.318%
2022-02-26 04:57:11 - epoch 087 lr: 0.0010000000000000002
2022-02-26 04:57:50 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 1.0597
2022-02-26 04:58:23 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 1.0574
2022-02-26 04:58:57 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 1.0563
2022-02-26 04:59:29 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 1.2323
2022-02-26 05:00:04 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 1.0814
2022-02-26 05:00:36 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 1.1624
2022-02-26 05:01:11 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 1.1137
2022-02-26 05:01:44 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 1.0488
2022-02-26 05:02:19 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 0.9885
2022-02-26 05:02:51 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 0.9649
2022-02-26 05:03:25 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 1.0432
2022-02-26 05:03:59 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 1.2584
2022-02-26 05:04:32 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 1.0798
2022-02-26 05:05:06 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 1.0330
2022-02-26 05:05:40 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 1.0191
2022-02-26 05:06:13 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 1.0649
2022-02-26 05:06:46 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 1.3257
2022-02-26 05:07:20 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 1.0938
2022-02-26 05:07:54 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 1.3409
2022-02-26 05:08:27 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 1.1081
2022-02-26 05:09:01 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 1.0405
2022-02-26 05:09:34 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 1.1999
2022-02-26 05:10:08 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 1.3310
2022-02-26 05:10:42 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 1.1491
2022-02-26 05:11:16 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 1.3055
2022-02-26 05:11:49 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 1.0858
2022-02-26 05:12:24 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 1.0043
2022-02-26 05:12:57 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 0.9787
2022-02-26 05:13:31 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 0.9866
2022-02-26 05:14:04 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 1.0660
2022-02-26 05:14:38 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 1.0045
2022-02-26 05:15:12 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 1.0135
2022-02-26 05:15:46 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 1.1029
2022-02-26 05:16:19 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 0.9787
2022-02-26 05:16:53 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 0.9628
2022-02-26 05:17:26 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 1.0497
2022-02-26 05:18:00 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 0.9506
2022-02-26 05:18:34 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 1.2317
2022-02-26 05:19:07 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 1.0553
2022-02-26 05:19:41 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 1.0397
2022-02-26 05:20:15 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 1.3189
2022-02-26 05:20:48 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 1.0736
2022-02-26 05:21:22 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 1.0393
2022-02-26 05:21:55 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 0.9969
2022-02-26 05:22:29 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 1.1909
2022-02-26 05:23:02 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 1.1384
2022-02-26 05:23:36 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 1.0593
2022-02-26 05:24:10 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 0.9205
2022-02-26 05:24:43 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 0.9915
2022-02-26 05:25:16 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 1.0706
2022-02-26 05:25:18 - train: epoch 087, train_loss: 1.0937
2022-02-26 05:26:35 - eval: epoch: 087, acc1: 73.192%, acc5: 91.384%, test_loss: 1.0673, per_image_load_time: 1.925ms, per_image_inference_time: 0.304ms
2022-02-26 05:26:36 - until epoch: 087, best_acc1: 73.318%
2022-02-26 05:26:36 - epoch 088 lr: 0.0010000000000000002
2022-02-26 05:27:14 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 1.0663
2022-02-26 05:27:48 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 1.0693
2022-02-26 05:28:21 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 1.0846
2022-02-26 05:28:55 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 1.0569
2022-02-26 05:29:28 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 1.1445
2022-02-26 05:30:01 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 0.9584
2022-02-26 05:30:35 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 1.1756
2022-02-26 05:31:08 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 1.0899
2022-02-26 05:31:42 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 1.0118
2022-02-26 05:32:15 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 1.0921
2022-02-26 05:32:49 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 0.9219
2022-02-26 05:33:23 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 1.1897
2022-02-26 05:33:57 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 1.1097
2022-02-26 05:34:31 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 1.0606
2022-02-26 05:35:04 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 1.1968
2022-02-26 05:35:38 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 0.9142
2022-02-26 05:36:12 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 1.3001
2022-02-26 05:36:46 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 0.9064
2022-02-26 05:37:20 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 1.2134
2022-02-26 05:37:53 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 0.8552
2022-02-26 05:38:26 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 1.0344
2022-02-26 05:39:01 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 1.0144
2022-02-26 05:39:35 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 1.0626
2022-02-26 05:40:08 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 1.0554
2022-02-26 05:40:41 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 1.2818
2022-02-26 05:41:15 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 1.0999
2022-02-26 05:41:49 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 1.0684
2022-02-26 05:42:23 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 1.1011
2022-02-26 05:42:56 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 1.1995
2022-02-26 05:43:31 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 1.0705
2022-02-26 05:44:04 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 1.0188
2022-02-26 05:44:38 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 1.0930
2022-02-26 05:45:12 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 1.1601
2022-02-26 05:45:46 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 1.1315
2022-02-26 05:46:21 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 1.0241
2022-02-26 05:46:54 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 1.1568
2022-02-26 05:47:28 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 1.2122
2022-02-26 05:48:01 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 1.2489
2022-02-26 05:48:36 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 0.9350
2022-02-26 05:49:09 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 0.8919
2022-02-26 05:49:44 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 1.0008
2022-02-26 05:50:16 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 1.3422
2022-02-26 05:50:52 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 0.9873
2022-02-26 05:51:25 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 1.0503
2022-02-26 05:51:59 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 1.3127
2022-02-26 05:52:33 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 1.4782
2022-02-26 05:53:07 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 1.3455
2022-02-26 05:53:41 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 1.1419
2022-02-26 05:54:16 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 0.9763
2022-02-26 05:54:49 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 1.1151
2022-02-26 05:54:51 - train: epoch 088, train_loss: 1.0915
2022-02-26 05:56:08 - eval: epoch: 088, acc1: 73.194%, acc5: 91.436%, test_loss: 1.0664, per_image_load_time: 2.607ms, per_image_inference_time: 0.301ms
2022-02-26 05:56:08 - until epoch: 088, best_acc1: 73.318%
2022-02-26 05:56:08 - epoch 089 lr: 0.0010000000000000002
2022-02-26 05:56:46 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 1.1201
2022-02-26 05:57:19 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 1.0143
2022-02-26 05:57:53 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 1.2783
2022-02-26 05:58:26 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 0.9189
2022-02-26 05:59:00 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 1.1570
2022-02-26 05:59:33 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 1.1068
2022-02-26 06:00:07 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 1.0238
2022-02-26 06:00:40 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 1.2936
2022-02-26 06:01:14 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 0.9225
2022-02-26 06:01:47 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 1.1533
2022-02-26 06:02:21 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 0.8814
2022-02-26 06:02:54 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 1.3406
2022-02-26 06:03:28 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 0.9953
2022-02-26 06:04:02 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 1.1795
2022-02-26 06:04:36 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 1.1599
2022-02-26 06:05:10 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 0.9278
2022-02-26 06:05:44 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 1.2442
2022-02-26 06:06:18 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 1.0651
2022-02-26 06:06:51 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 1.1606
2022-02-26 06:07:26 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 0.9860
2022-02-26 06:07:59 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 1.1432
2022-02-26 06:08:33 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 1.0823
2022-02-26 06:09:07 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 1.1280
2022-02-26 06:09:42 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 1.0943
2022-02-26 06:10:15 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 1.0748
2022-02-26 06:10:49 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 0.9173
2022-02-26 06:11:22 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 1.0490
2022-02-26 06:11:56 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 1.2417
2022-02-26 06:12:29 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 1.1691
2022-02-26 06:13:03 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 1.1448
2022-02-26 06:13:36 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 0.9641
2022-02-26 06:14:10 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 1.3235
2022-02-26 06:14:44 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 1.1773
2022-02-26 06:15:16 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 1.0650
2022-02-26 06:15:50 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 0.9702
2022-02-26 06:16:22 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 1.0825
2022-02-26 06:16:55 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 1.2488
2022-02-26 06:17:30 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 0.9500
2022-02-26 06:18:04 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 1.3577
2022-02-26 06:18:37 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 0.9664
2022-02-26 06:19:11 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 1.1465
2022-02-26 06:19:45 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 1.1619
2022-02-26 06:20:20 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 1.1015
2022-02-26 06:20:53 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 1.1887
2022-02-26 06:21:27 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 0.9701
2022-02-26 06:22:01 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 1.0107
2022-02-26 06:22:35 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 1.0226
2022-02-26 06:23:10 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 1.0167
2022-02-26 06:23:45 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 0.9265
2022-02-26 06:24:18 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 0.8547
2022-02-26 06:24:20 - train: epoch 089, train_loss: 1.0885
2022-02-26 06:25:38 - eval: epoch: 089, acc1: 73.138%, acc5: 91.246%, test_loss: 1.0662, per_image_load_time: 2.666ms, per_image_inference_time: 0.280ms
2022-02-26 06:25:38 - until epoch: 089, best_acc1: 73.318%
2022-02-26 06:25:38 - epoch 090 lr: 0.0010000000000000002
2022-02-26 06:26:17 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 1.1607
2022-02-26 06:26:50 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 1.0481
2022-02-26 06:27:23 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 1.0438
2022-02-26 06:27:58 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 1.0090
2022-02-26 06:28:31 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 1.1396
2022-02-26 06:29:04 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 1.1710
2022-02-26 06:29:38 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 1.1423
2022-02-26 06:30:11 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 1.1384
2022-02-26 06:30:45 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 1.1754
2022-02-26 06:31:19 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 1.0157
2022-02-26 06:31:52 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 1.1294
2022-02-26 06:32:26 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 0.9445
2022-02-26 06:32:59 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 1.0810
2022-02-26 06:33:33 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 1.0669
2022-02-26 06:34:06 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 1.3833
2022-02-26 06:34:40 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 1.1896
2022-02-26 06:35:14 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 0.9401
2022-02-26 06:35:47 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 0.9719
2022-02-26 06:36:21 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 1.0503
2022-02-26 06:36:54 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 1.0805
2022-02-26 06:37:28 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 1.2095
2022-02-26 06:38:02 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 1.0754
2022-02-26 06:38:36 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 1.2218
2022-02-26 06:39:10 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 1.0699
2022-02-26 06:39:45 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 1.1361
2022-02-26 06:40:17 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 1.1210
2022-02-26 06:40:51 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 1.1218
2022-02-26 06:41:24 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 1.1212
2022-02-26 06:41:58 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 1.1005
2022-02-26 06:42:32 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 1.2043
2022-02-26 06:43:06 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 0.9796
2022-02-26 06:43:39 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 1.1839
2022-02-26 06:44:13 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 1.1246
2022-02-26 06:44:47 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 1.1381
2022-02-26 06:45:20 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 0.9876
2022-02-26 06:45:54 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 1.0675
2022-02-26 06:46:28 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 1.2143
2022-02-26 06:47:01 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 1.1614
2022-02-26 06:47:35 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 1.0496
2022-02-26 06:48:09 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 1.0560
2022-02-26 06:48:43 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 1.1991
2022-02-26 06:49:17 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 1.1661
2022-02-26 06:49:50 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 1.1946
2022-02-26 06:50:24 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 1.0817
2022-02-26 06:50:58 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 0.9337
2022-02-26 06:51:32 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 1.1212
2022-02-26 06:52:06 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 1.1281
2022-02-26 06:52:40 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 1.4262
2022-02-26 06:53:15 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 0.9474
2022-02-26 06:53:48 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 1.0385
2022-02-26 06:53:50 - train: epoch 090, train_loss: 1.0855
2022-02-26 06:55:06 - eval: epoch: 090, acc1: 73.094%, acc5: 91.348%, test_loss: 1.0685, per_image_load_time: 2.557ms, per_image_inference_time: 0.324ms
2022-02-26 06:55:07 - until epoch: 090, best_acc1: 73.318%
2022-02-26 06:55:07 - epoch 091 lr: 0.00010000000000000003
2022-02-26 06:55:45 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 0.9687
2022-02-26 06:56:19 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 1.0758
2022-02-26 06:56:52 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 1.0202
2022-02-26 06:57:25 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 1.1141
2022-02-26 06:57:59 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 1.0565
2022-02-26 06:58:33 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 1.0895
2022-02-26 06:59:06 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 1.2181
2022-02-26 06:59:40 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 0.9770
2022-02-26 07:00:14 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 1.1135
2022-02-26 07:00:47 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 0.9264
2022-02-26 07:01:22 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 0.7115
2022-02-26 07:01:55 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 1.1204
2022-02-26 07:02:30 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 1.1467
2022-02-26 07:03:03 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 1.2496
2022-02-26 07:03:36 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 0.9841
2022-02-26 07:04:10 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 0.9695
2022-02-26 07:04:44 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 0.9939
2022-02-26 07:05:17 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 1.0547
2022-02-26 07:05:52 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 0.9321
2022-02-26 07:06:25 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 1.0096
2022-02-26 07:06:59 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 1.0386
2022-02-26 07:07:34 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 1.2031
2022-02-26 07:08:07 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 1.0507
2022-02-26 07:08:41 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 0.7517
2022-02-26 07:09:15 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 1.1452
2022-02-26 07:09:48 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 1.0565
2022-02-26 07:10:22 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 0.9873
2022-02-26 07:10:56 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 1.0098
2022-02-26 07:11:31 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 1.2074
2022-02-26 07:12:04 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 1.2418
2022-02-26 07:12:38 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 0.8736
2022-02-26 07:13:11 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 1.4608
2022-02-26 07:13:45 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 0.9326
2022-02-26 07:14:19 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 1.1044
2022-02-26 07:14:53 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 1.0941
2022-02-26 07:15:26 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 1.1771
2022-02-26 07:16:01 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 1.2430
2022-02-26 07:16:35 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 1.1312
2022-02-26 07:17:08 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 1.1107
2022-02-26 07:17:42 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 0.9793
2022-02-26 07:18:17 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 1.0954
2022-02-26 07:18:50 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 1.0693
2022-02-26 07:19:24 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 1.0712
2022-02-26 07:19:58 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 0.8739
2022-02-26 07:20:32 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 0.9805
2022-02-26 07:21:06 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 1.0594
2022-02-26 07:21:41 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 1.1123
2022-02-26 07:22:15 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 1.0036
2022-02-26 07:22:50 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 0.8457
2022-02-26 07:23:23 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 1.0970
2022-02-26 07:23:26 - train: epoch 091, train_loss: 1.0528
2022-02-26 07:24:43 - eval: epoch: 091, acc1: 73.670%, acc5: 91.620%, test_loss: 1.0444, per_image_load_time: 2.658ms, per_image_inference_time: 0.312ms
2022-02-26 07:24:43 - until epoch: 091, best_acc1: 73.670%
2022-02-26 07:24:43 - epoch 092 lr: 0.00010000000000000003
2022-02-26 07:25:22 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 1.0244
2022-02-26 07:25:56 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 1.1803
2022-02-26 07:26:30 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 1.1173
2022-02-26 07:27:03 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 1.0038
2022-02-26 07:27:36 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 1.0474
2022-02-26 07:28:10 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 1.0914
2022-02-26 07:28:43 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 0.9743
2022-02-26 07:29:17 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 1.1283
2022-02-26 07:29:50 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 1.1338
2022-02-26 07:30:24 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 1.0626
2022-02-26 07:30:56 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 0.9750
2022-02-26 07:31:30 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 0.9803
2022-02-26 07:32:04 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 0.9719
2022-02-26 07:32:37 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 0.9124
2022-02-26 07:33:11 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 0.9556
2022-02-26 07:33:45 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 0.9137
2022-02-26 07:34:18 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 1.0413
2022-02-26 07:34:50 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 1.0077
2022-02-26 07:35:25 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 0.8715
2022-02-26 07:35:59 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 0.8368
2022-02-26 07:36:32 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 1.0537
2022-02-26 07:37:06 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 1.1306
2022-02-26 07:37:39 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 1.0769
2022-02-26 07:38:13 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 0.9926
2022-02-26 07:38:47 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 1.0902
2022-02-26 07:39:21 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 1.1170
2022-02-26 07:39:55 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 0.9097
2022-02-26 07:40:29 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 0.9591
2022-02-26 07:41:02 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 1.2206
2022-02-26 07:41:36 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 0.9391
2022-02-26 07:42:09 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 1.0699
2022-02-26 07:42:44 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 1.0264
2022-02-26 07:43:17 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 0.9626
2022-02-26 07:43:50 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 1.1074
2022-02-26 07:44:25 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 0.9922
2022-02-26 07:44:59 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 1.0053
2022-02-26 07:45:32 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 1.0515
2022-02-26 07:46:07 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 1.1116
2022-02-26 07:46:40 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 1.0479
2022-02-26 07:47:14 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 1.1286
2022-02-26 07:47:47 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 1.0894
2022-02-26 07:48:21 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 1.0161
2022-02-26 07:48:56 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 1.1957
2022-02-26 07:49:29 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 0.9792
2022-02-26 07:50:03 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 0.9615
2022-02-26 07:50:38 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 1.0363
2022-02-26 07:51:11 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 0.8256
2022-02-26 07:51:46 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 1.0072
2022-02-26 07:52:20 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 0.8489
2022-02-26 07:52:52 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 1.0546
2022-02-26 07:52:54 - train: epoch 092, train_loss: 1.0428
2022-02-26 07:54:10 - eval: epoch: 092, acc1: 73.680%, acc5: 91.654%, test_loss: 1.0434, per_image_load_time: 2.555ms, per_image_inference_time: 0.308ms
2022-02-26 07:54:10 - until epoch: 092, best_acc1: 73.680%
2022-02-26 20:00:07 - epoch 093 lr: 0.00010000000000000003
2022-02-26 20:00:45 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 0.7864
2022-02-26 20:01:18 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 1.0263
2022-02-26 20:01:52 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 1.1089
2022-02-26 20:02:26 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 1.0584
2022-02-26 20:02:59 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 1.2695
2022-02-26 20:03:33 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 0.9396
2022-02-26 20:04:06 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 0.9563
2022-02-26 20:04:40 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 0.9826
2022-02-26 20:05:12 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 1.0898
2022-02-26 20:05:46 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 1.0032
2022-02-26 20:06:19 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 1.0221
2022-02-26 20:06:52 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 1.0084
2022-02-26 20:07:26 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 1.0233
2022-02-26 20:08:00 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 1.1762
2022-02-26 20:08:33 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 0.9873
2022-02-26 20:09:07 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 1.0990
2022-02-26 20:09:41 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 1.0455
2022-02-26 20:10:13 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 1.1365
2022-02-26 20:10:48 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 0.8531
2022-02-26 20:11:21 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 0.9290
2022-02-26 20:11:54 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 0.8943
2022-02-26 20:12:27 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 0.9720
2022-02-26 20:13:01 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 1.1071
2022-02-26 20:13:35 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 1.1015
2022-02-26 20:14:09 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 1.1512
2022-02-26 20:14:41 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 1.3873
2022-02-26 20:15:15 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 1.0298
2022-02-26 20:15:49 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 1.1808
2022-02-26 20:16:22 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 0.8002
2022-02-26 20:16:57 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 0.9638
2022-02-26 20:17:29 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 1.0807
2022-02-26 20:18:03 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 0.9981
2022-02-26 20:18:37 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 1.0113
2022-02-26 20:19:10 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 1.1381
2022-02-26 20:19:44 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 1.0567
2022-02-26 20:20:17 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 1.1953
2022-02-26 20:20:51 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 1.1188
2022-02-26 20:21:24 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 0.9798
2022-02-26 20:21:58 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 1.1387
2022-02-26 20:22:31 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 1.4038
2022-02-26 20:23:04 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 0.9882
2022-02-26 20:23:38 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 1.0234
2022-02-26 20:24:11 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 1.1414
2022-02-26 20:24:45 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 1.0936
2022-02-26 20:25:18 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 0.9243
2022-02-26 20:25:52 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 0.8821
2022-02-26 20:26:25 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 1.3056
2022-02-26 20:26:59 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 1.0815
2022-02-26 20:27:33 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 1.0925
2022-02-26 20:28:05 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 0.9223
2022-02-26 20:28:06 - train: epoch 093, train_loss: 1.0394
2022-02-26 20:29:22 - eval: epoch: 093, acc1: 73.708%, acc5: 91.758%, test_loss: 1.0418, per_image_load_time: 2.665ms, per_image_inference_time: 0.281ms
2022-02-26 20:29:23 - until epoch: 093, best_acc1: 73.708%
2022-02-26 20:29:23 - epoch 094 lr: 0.00010000000000000003
2022-02-26 20:30:01 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 1.0156
2022-02-26 20:30:34 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 1.1571
2022-02-26 20:31:08 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 1.1360
2022-02-26 20:31:41 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 1.2021
2022-02-26 20:32:15 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 0.9434
2022-02-26 20:32:47 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 0.7860
2022-02-26 20:33:21 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 1.1388
2022-02-26 20:33:54 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 0.9903
2022-02-26 20:34:26 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 1.1082
2022-02-26 20:34:59 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 0.9916
2022-02-26 20:35:33 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 0.9690
2022-02-26 20:36:06 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 1.0538
2022-02-26 20:36:40 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 1.0706
2022-02-26 20:37:12 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 0.9221
2022-02-26 20:37:46 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 1.0160
2022-02-26 20:38:18 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.2234
2022-02-26 20:38:51 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 1.0236
2022-02-26 20:39:24 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 1.0584
2022-02-26 20:39:57 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 0.9492
2022-02-26 20:40:30 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 0.7994
2022-02-26 20:41:03 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 0.8394
2022-02-26 20:41:37 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 0.9854
2022-02-26 20:42:10 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 1.0171
2022-02-26 20:42:45 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 1.0524
2022-02-26 20:43:17 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 0.9886
2022-02-26 20:43:51 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 1.0976
2022-02-26 20:44:23 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 0.9731
2022-02-26 20:44:58 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 1.1043
2022-02-26 20:45:30 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 1.0034
2022-02-26 20:46:03 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 1.1174
2022-02-26 20:46:36 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 1.1414
2022-02-26 20:47:08 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 1.0928
2022-02-26 20:47:43 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 1.0417
2022-02-26 20:48:17 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 0.9771
2022-02-26 20:48:49 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 1.2231
2022-02-26 20:49:22 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 1.0704
2022-02-26 20:49:55 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 1.2014
2022-02-26 20:50:28 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 1.0841
2022-02-26 20:51:02 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 0.9854
2022-02-26 20:51:35 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 0.9718
2022-02-26 20:52:08 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 1.1332
2022-02-26 20:52:41 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 0.9443
2022-02-26 20:53:14 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 1.2462
2022-02-26 20:53:48 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 1.1652
2022-02-26 20:54:22 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 1.0751
2022-02-26 20:54:54 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 1.1734
2022-02-26 20:55:29 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 1.1969
2022-02-26 20:56:01 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 0.8975
2022-02-26 20:56:36 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 0.9285
2022-02-26 20:57:07 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 1.0727
2022-02-26 20:57:08 - train: epoch 094, train_loss: 1.0361
2022-02-26 20:58:23 - eval: epoch: 094, acc1: 73.780%, acc5: 91.664%, test_loss: 1.0412, per_image_load_time: 1.252ms, per_image_inference_time: 0.272ms
2022-02-26 20:58:24 - until epoch: 094, best_acc1: 73.780%
2022-02-26 20:58:24 - epoch 095 lr: 0.00010000000000000003
2022-02-26 20:59:01 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 1.1841
2022-02-26 20:59:34 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 1.0594
2022-02-26 21:00:07 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 0.8394
2022-02-26 21:00:41 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 1.0655
2022-02-26 21:01:13 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 1.0223
2022-02-26 21:01:47 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 1.1693
2022-02-26 21:02:20 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 1.1203
2022-02-26 21:02:54 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 1.0110
2022-02-26 21:03:26 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 1.2447
2022-02-26 21:03:59 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 1.1234
2022-02-26 21:04:32 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 0.9917
2022-02-26 21:05:05 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 1.1075
2022-02-26 21:05:39 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 0.9076
2022-02-26 21:06:13 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 1.0752
2022-02-26 21:06:44 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 1.0865
2022-02-26 21:07:18 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 0.8860
2022-02-26 21:07:52 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 1.0217
2022-02-26 21:08:25 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 1.0338
2022-02-26 21:08:58 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 1.0712
2022-02-26 21:09:32 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 1.0759
2022-02-26 21:10:05 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 0.9678
2022-02-26 21:10:39 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 1.0001
2022-02-26 21:11:12 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 1.0973
2022-02-26 21:11:46 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 0.9943
2022-02-26 21:12:19 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 0.9774
2022-02-26 21:12:52 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 0.9722
2022-02-26 21:13:25 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 1.1244
2022-02-26 21:13:58 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 0.8364
2022-02-26 21:14:32 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 0.9271
2022-02-26 21:15:04 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 1.0995
2022-02-26 21:15:38 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 1.0877
2022-02-26 21:16:12 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 0.9988
2022-02-26 21:16:44 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 1.1761
2022-02-26 21:17:19 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 1.0088
2022-02-26 21:17:53 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 0.8937
2022-02-26 21:18:28 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 0.7741
2022-02-26 21:19:02 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 1.1092
2022-02-26 21:19:37 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 0.8958
2022-02-26 21:20:11 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 1.0343
2022-02-26 21:20:44 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 0.9270
2022-02-26 21:21:16 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 1.0339
2022-02-26 21:21:51 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 0.7335
2022-02-26 21:22:24 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 1.3275
2022-02-26 21:22:58 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 1.1994
2022-02-26 21:23:31 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 0.8994
2022-02-26 21:24:05 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 0.8606
2022-02-26 21:24:38 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 0.9809
2022-02-26 21:25:11 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 1.0219
2022-02-26 21:25:44 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 0.9387
2022-02-26 21:26:17 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 0.9924
2022-02-26 21:26:18 - train: epoch 095, train_loss: 1.0341
2022-02-26 21:27:33 - eval: epoch: 095, acc1: 73.716%, acc5: 91.706%, test_loss: 1.0414, per_image_load_time: 2.624ms, per_image_inference_time: 0.295ms
2022-02-26 21:27:33 - until epoch: 095, best_acc1: 73.780%
2022-02-26 21:27:33 - epoch 096 lr: 0.00010000000000000003
2022-02-26 21:28:11 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 1.0172
2022-02-26 21:28:44 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 1.0796
2022-02-26 21:29:17 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 0.9349
2022-02-26 21:29:50 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 0.8606
2022-02-26 21:30:23 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 1.0623
2022-02-26 21:30:57 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 1.0521
2022-02-26 21:31:30 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 1.0025
2022-02-26 21:32:04 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 1.0480
2022-02-26 21:32:37 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 1.0317
2022-02-26 21:33:10 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 1.0255
2022-02-26 21:33:44 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 0.9678
2022-02-26 21:34:17 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 1.0810
2022-02-26 21:34:51 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 0.9232
2022-02-26 21:35:25 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 1.0683
2022-02-26 21:35:58 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 1.0791
2022-02-26 21:36:32 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 0.7466
2022-02-26 21:37:06 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 0.9313
2022-02-26 21:37:39 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 1.0202
2022-02-26 21:38:13 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 1.2785
2022-02-26 21:38:47 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 1.0117
2022-02-26 21:39:20 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 0.8703
2022-02-26 21:39:55 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 0.9736
2022-02-26 21:40:29 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 1.0786
2022-02-26 21:41:03 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 1.0901
2022-02-26 21:41:36 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 1.1491
2022-02-26 21:42:10 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 1.0250
2022-02-26 21:42:43 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 0.9585
2022-02-26 21:43:18 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 1.1129
2022-02-26 21:43:51 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 1.0469
2022-02-26 21:44:25 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 1.1443
2022-02-26 21:45:00 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 1.2436
2022-02-26 21:45:33 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 0.9334
2022-02-26 21:46:07 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 1.1370
2022-02-26 21:46:41 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 0.9866
2022-02-26 21:47:14 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 1.0968
2022-02-26 21:47:49 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 0.7764
2022-02-26 21:48:22 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 0.9236
2022-02-26 21:48:56 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 0.8684
2022-02-26 21:49:29 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 0.9954
2022-02-26 21:50:04 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 0.9990
2022-02-26 21:50:36 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 1.0220
2022-02-26 21:51:10 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 0.9722
2022-02-26 21:51:44 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 0.8831
2022-02-26 21:52:17 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 0.9277
2022-02-26 21:52:52 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 1.1142
2022-02-26 21:53:24 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 0.9030
2022-02-26 21:53:58 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 1.0331
2022-02-26 21:54:32 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 1.1937
2022-02-26 21:55:06 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 0.9972
2022-02-26 21:55:40 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 1.0967
2022-02-26 21:55:41 - train: epoch 096, train_loss: 1.0334
2022-02-26 21:56:56 - eval: epoch: 096, acc1: 73.700%, acc5: 91.696%, test_loss: 1.0390, per_image_load_time: 2.595ms, per_image_inference_time: 0.286ms
2022-02-26 21:56:56 - until epoch: 096, best_acc1: 73.780%
2022-02-26 21:56:56 - epoch 097 lr: 0.00010000000000000003
2022-02-26 21:57:36 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 1.1224
2022-02-26 21:58:10 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 1.1049
2022-02-26 21:58:42 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 1.0629
2022-02-26 21:59:16 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 1.2572
2022-02-26 21:59:50 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 0.9480
2022-02-26 22:00:23 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 0.9270
2022-02-26 22:00:57 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 0.9352
2022-02-26 22:01:29 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 0.9550
2022-02-26 22:02:05 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 1.0148
2022-02-26 22:02:37 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 1.1401
2022-02-26 22:03:11 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 0.9279
2022-02-26 22:03:45 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 1.2005
2022-02-26 22:04:18 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 0.9008
2022-02-26 22:04:53 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 0.9819
2022-02-26 22:05:26 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 0.9813
2022-02-26 22:06:00 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 0.9774
2022-02-26 22:06:34 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 0.9948
2022-02-26 22:07:07 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 1.2577
2022-02-26 22:07:43 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 1.0744
2022-02-26 22:08:17 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 0.8153
2022-02-26 22:08:51 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 0.9273
2022-02-26 22:09:25 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 0.9481
2022-02-26 22:10:00 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 1.0472
2022-02-26 22:10:34 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 1.1421
2022-02-26 22:11:08 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 0.9584
2022-02-26 22:11:41 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 1.0842
2022-02-26 22:12:15 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 1.0396
2022-02-26 22:12:49 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 0.9436
2022-02-26 22:13:23 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 0.9907
2022-02-26 22:13:57 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 1.1637
2022-02-26 22:14:30 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 0.9952
2022-02-26 22:15:05 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 1.2713
2022-02-26 22:15:38 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 0.9598
2022-02-26 22:16:12 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 1.1500
2022-02-26 22:16:45 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 0.9454
2022-02-26 22:17:19 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 1.0478
2022-02-26 22:17:52 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 0.7298
2022-02-26 22:18:26 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 0.9672
2022-02-26 22:18:59 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 0.9111
2022-02-26 22:19:33 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 1.0253
2022-02-26 22:20:07 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 1.0801
2022-02-26 22:20:40 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 1.0197
2022-02-26 22:21:14 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 1.2472
2022-02-26 22:21:48 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 0.9434
2022-02-26 22:22:21 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 1.1030
2022-02-26 22:22:56 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 1.0516
2022-02-26 22:23:29 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 1.0040
2022-02-26 22:24:03 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 0.9271
2022-02-26 22:24:37 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 1.1945
2022-02-26 22:25:10 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 0.9650
2022-02-26 22:25:11 - train: epoch 097, train_loss: 1.0333
2022-02-26 22:26:27 - eval: epoch: 097, acc1: 73.704%, acc5: 91.734%, test_loss: 1.0397, per_image_load_time: 2.414ms, per_image_inference_time: 0.287ms
2022-02-26 22:26:28 - until epoch: 097, best_acc1: 73.780%
2022-02-26 22:26:28 - epoch 098 lr: 0.00010000000000000003
2022-02-26 22:27:06 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 1.2074
2022-02-26 22:27:40 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 1.1230
2022-02-26 22:28:13 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 0.8749
2022-02-26 22:28:47 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 1.0917
2022-02-26 22:29:19 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 1.0144
2022-02-26 22:29:53 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 1.0064
2022-02-26 22:30:25 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 1.2470
2022-02-26 22:31:00 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 1.0981
2022-02-26 22:31:32 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 1.1604
2022-02-26 22:32:07 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 1.0441
2022-02-26 22:32:39 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 1.0076
2022-02-26 22:33:12 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 1.0338
2022-02-26 22:33:46 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 1.2311
2022-02-26 22:34:20 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 1.0413
2022-02-26 22:34:54 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 0.9794
2022-02-26 22:35:27 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 1.0406
2022-02-26 22:36:01 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 1.0396
2022-02-26 22:36:35 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 1.0065
2022-02-26 22:37:08 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 1.0017
2022-02-26 22:37:42 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 1.1721
2022-02-26 22:38:15 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 1.1544
2022-02-26 22:38:49 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 0.9502
2022-02-26 22:39:22 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 0.9615
2022-02-26 22:39:56 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 1.0036
2022-02-26 22:40:29 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 0.9880
2022-02-26 22:41:03 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 0.9780
2022-02-26 22:41:37 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 1.1539
2022-02-26 22:42:11 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 1.1983
2022-02-26 22:42:43 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 0.9791
2022-02-26 22:43:17 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 1.0427
2022-02-26 22:43:51 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 1.0466
2022-02-26 22:44:26 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 0.8682
2022-02-26 22:44:59 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 1.1399
2022-02-26 22:45:33 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 0.9932
2022-02-26 22:46:06 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 0.9778
2022-02-26 22:46:40 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 1.1067
2022-02-26 22:47:13 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 1.0385
2022-02-26 22:47:47 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 0.9830
2022-02-26 22:48:21 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 0.9971
2022-02-26 22:48:55 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 1.0568
2022-02-26 22:49:28 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 1.0638
2022-02-26 22:50:02 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 1.1547
2022-02-26 22:50:36 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 0.8811
2022-02-26 22:51:09 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 1.3241
2022-02-26 22:51:43 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 1.1098
2022-02-26 22:52:17 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 1.1671
2022-02-26 22:52:50 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 0.8842
2022-02-26 22:53:25 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 0.9255
2022-02-26 22:53:58 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 1.0227
2022-02-26 22:54:32 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 1.0192
2022-02-26 22:54:33 - train: epoch 098, train_loss: 1.0310
2022-02-26 22:55:49 - eval: epoch: 098, acc1: 73.810%, acc5: 91.648%, test_loss: 1.0402, per_image_load_time: 1.999ms, per_image_inference_time: 0.286ms
2022-02-26 22:55:50 - until epoch: 098, best_acc1: 73.810%
2022-02-26 22:55:50 - epoch 099 lr: 0.00010000000000000003
2022-02-26 22:56:29 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 1.0424
2022-02-26 22:57:03 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 1.0338
2022-02-26 22:57:36 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 1.0803
2022-02-26 22:58:10 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 1.2226
2022-02-26 22:58:44 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 0.9507
2022-02-26 22:59:17 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 1.0315
2022-02-26 22:59:53 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 1.2908
2022-02-26 23:00:27 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 0.9493
2022-02-26 23:01:02 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 1.1215
2022-02-26 23:01:37 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 0.9087
2022-02-26 23:02:12 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 1.2089
2022-02-26 23:02:45 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 0.9603
2022-02-26 23:03:18 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 0.9375
2022-02-26 23:03:52 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 1.0871
2022-02-26 23:04:25 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 1.0690
2022-02-26 23:04:58 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 1.1481
2022-02-26 23:05:32 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 0.9256
2022-02-26 23:06:06 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 1.0348
2022-02-26 23:06:39 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 1.1206
2022-02-26 23:07:13 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 1.0413
2022-02-26 23:07:46 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 0.8253
2022-02-26 23:08:20 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 0.9799
2022-02-26 23:08:53 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 1.0444
2022-02-26 23:09:27 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 1.1131
2022-02-26 23:10:00 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 0.9993
2022-02-26 23:10:34 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 1.0017
2022-02-26 23:11:07 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 1.0211
2022-02-26 23:11:42 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 1.1259
2022-02-26 23:12:15 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 0.8305
2022-02-26 23:12:49 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 1.1370
2022-02-26 23:13:21 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 0.9568
2022-02-26 23:13:55 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 1.0560
2022-02-26 23:14:29 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 1.0619
2022-02-26 23:15:03 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 1.0518
2022-02-26 23:15:36 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 1.0664
2022-02-26 23:16:10 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 0.9468
2022-02-26 23:16:43 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 0.8115
2022-02-26 23:17:17 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 1.0646
2022-02-26 23:17:51 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 0.9626
2022-02-26 23:18:24 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 0.8948
2022-02-26 23:18:58 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 0.9709
2022-02-26 23:19:31 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 1.1417
2022-02-26 23:20:06 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 1.0288
2022-02-26 23:20:39 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 1.1303
2022-02-26 23:21:13 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 1.1628
2022-02-26 23:21:46 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 1.0610
2022-02-26 23:22:21 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 1.1718
2022-02-26 23:22:55 - train: epoch 0099, iter [04800, 05004], lr: 0.000100, loss: 1.1403
2022-02-26 23:23:28 - train: epoch 0099, iter [04900, 05004], lr: 0.000100, loss: 1.0048
2022-02-26 23:24:01 - train: epoch 0099, iter [05000, 05004], lr: 0.000100, loss: 1.2621
2022-02-26 23:24:02 - train: epoch 099, train_loss: 1.0298
2022-02-26 23:25:19 - eval: epoch: 099, acc1: 73.800%, acc5: 91.720%, test_loss: 1.0410, per_image_load_time: 1.790ms, per_image_inference_time: 0.284ms
2022-02-26 23:25:19 - until epoch: 099, best_acc1: 73.810%
2022-02-26 23:25:19 - epoch 100 lr: 0.00010000000000000003
2022-02-26 23:25:58 - train: epoch 0100, iter [00100, 05004], lr: 0.000100, loss: 1.1134
2022-02-26 23:26:32 - train: epoch 0100, iter [00200, 05004], lr: 0.000100, loss: 1.0533
2022-02-26 23:27:05 - train: epoch 0100, iter [00300, 05004], lr: 0.000100, loss: 0.9321
2022-02-26 23:27:38 - train: epoch 0100, iter [00400, 05004], lr: 0.000100, loss: 1.0822
2022-02-26 23:28:11 - train: epoch 0100, iter [00500, 05004], lr: 0.000100, loss: 1.0548
2022-02-26 23:28:46 - train: epoch 0100, iter [00600, 05004], lr: 0.000100, loss: 1.1910
2022-02-26 23:29:19 - train: epoch 0100, iter [00700, 05004], lr: 0.000100, loss: 0.9398
2022-02-26 23:29:53 - train: epoch 0100, iter [00800, 05004], lr: 0.000100, loss: 0.8909
2022-02-26 23:30:26 - train: epoch 0100, iter [00900, 05004], lr: 0.000100, loss: 0.8202
2022-02-26 23:30:59 - train: epoch 0100, iter [01000, 05004], lr: 0.000100, loss: 0.9331
2022-02-26 23:31:33 - train: epoch 0100, iter [01100, 05004], lr: 0.000100, loss: 0.8800
2022-02-26 23:32:07 - train: epoch 0100, iter [01200, 05004], lr: 0.000100, loss: 0.9198
2022-02-26 23:32:40 - train: epoch 0100, iter [01300, 05004], lr: 0.000100, loss: 0.9010
2022-02-26 23:33:14 - train: epoch 0100, iter [01400, 05004], lr: 0.000100, loss: 1.0372
2022-02-26 23:33:47 - train: epoch 0100, iter [01500, 05004], lr: 0.000100, loss: 1.1047
2022-02-26 23:34:21 - train: epoch 0100, iter [01600, 05004], lr: 0.000100, loss: 1.1332
2022-02-26 23:34:54 - train: epoch 0100, iter [01700, 05004], lr: 0.000100, loss: 0.9599
2022-02-26 23:35:28 - train: epoch 0100, iter [01800, 05004], lr: 0.000100, loss: 1.0620
2022-02-26 23:36:01 - train: epoch 0100, iter [01900, 05004], lr: 0.000100, loss: 0.8995
2022-02-26 23:36:36 - train: epoch 0100, iter [02000, 05004], lr: 0.000100, loss: 1.2196
2022-02-26 23:37:09 - train: epoch 0100, iter [02100, 05004], lr: 0.000100, loss: 0.9524
2022-02-26 23:37:43 - train: epoch 0100, iter [02200, 05004], lr: 0.000100, loss: 1.1420
2022-02-26 23:38:17 - train: epoch 0100, iter [02300, 05004], lr: 0.000100, loss: 1.1272
2022-02-26 23:38:50 - train: epoch 0100, iter [02400, 05004], lr: 0.000100, loss: 1.0378
2022-02-26 23:39:24 - train: epoch 0100, iter [02500, 05004], lr: 0.000100, loss: 1.0134
2022-02-26 23:39:58 - train: epoch 0100, iter [02600, 05004], lr: 0.000100, loss: 1.0570
2022-02-26 23:40:31 - train: epoch 0100, iter [02700, 05004], lr: 0.000100, loss: 0.9688
2022-02-26 23:41:04 - train: epoch 0100, iter [02800, 05004], lr: 0.000100, loss: 0.9514
2022-02-26 23:41:38 - train: epoch 0100, iter [02900, 05004], lr: 0.000100, loss: 1.0775
2022-02-26 23:42:11 - train: epoch 0100, iter [03000, 05004], lr: 0.000100, loss: 0.9098
2022-02-26 23:42:45 - train: epoch 0100, iter [03100, 05004], lr: 0.000100, loss: 0.8137
2022-02-26 23:43:19 - train: epoch 0100, iter [03200, 05004], lr: 0.000100, loss: 1.0898
2022-02-26 23:43:52 - train: epoch 0100, iter [03300, 05004], lr: 0.000100, loss: 1.0253
2022-02-26 23:44:27 - train: epoch 0100, iter [03400, 05004], lr: 0.000100, loss: 1.0872
2022-02-26 23:44:59 - train: epoch 0100, iter [03500, 05004], lr: 0.000100, loss: 0.9442
2022-02-26 23:45:34 - train: epoch 0100, iter [03600, 05004], lr: 0.000100, loss: 0.9074
2022-02-26 23:46:07 - train: epoch 0100, iter [03700, 05004], lr: 0.000100, loss: 1.0155
2022-02-26 23:46:40 - train: epoch 0100, iter [03800, 05004], lr: 0.000100, loss: 0.9218
2022-02-26 23:47:14 - train: epoch 0100, iter [03900, 05004], lr: 0.000100, loss: 0.9752
2022-02-26 23:47:48 - train: epoch 0100, iter [04000, 05004], lr: 0.000100, loss: 0.9962
2022-02-26 23:48:21 - train: epoch 0100, iter [04100, 05004], lr: 0.000100, loss: 1.0838
2022-02-26 23:48:55 - train: epoch 0100, iter [04200, 05004], lr: 0.000100, loss: 1.3332
2022-02-26 23:49:29 - train: epoch 0100, iter [04300, 05004], lr: 0.000100, loss: 0.8956
2022-02-26 23:50:05 - train: epoch 0100, iter [04400, 05004], lr: 0.000100, loss: 1.3056
2022-02-26 23:50:39 - train: epoch 0100, iter [04500, 05004], lr: 0.000100, loss: 0.9613
2022-02-26 23:51:15 - train: epoch 0100, iter [04600, 05004], lr: 0.000100, loss: 1.1162
2022-02-26 23:51:49 - train: epoch 0100, iter [04700, 05004], lr: 0.000100, loss: 1.0834
2022-02-26 23:52:24 - train: epoch 0100, iter [04800, 05004], lr: 0.000100, loss: 0.9976
2022-02-26 23:52:59 - train: epoch 0100, iter [04900, 05004], lr: 0.000100, loss: 1.0808
2022-02-26 23:53:33 - train: epoch 0100, iter [05000, 05004], lr: 0.000100, loss: 1.1965
2022-02-26 23:53:34 - train: epoch 100, train_loss: 1.0305
2022-02-26 23:54:50 - eval: epoch: 100, acc1: 73.824%, acc5: 91.738%, test_loss: 1.0397, per_image_load_time: 2.678ms, per_image_inference_time: 0.282ms
2022-02-26 23:54:51 - until epoch: 100, best_acc1: 73.824%
2022-02-26 23:54:51 - train done. model: darknet19, train time: 48.923 hours, best_acc1: 73.824%
