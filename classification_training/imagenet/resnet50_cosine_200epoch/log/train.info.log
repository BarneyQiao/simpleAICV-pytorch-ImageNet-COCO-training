2022-03-04 22:34:52 - train: epoch 0190, iter [02700, 05004], lr: 0.000783, loss: 0.7491
2022-03-04 22:35:25 - train: epoch 0190, iter [02800, 05004], lr: 0.000783, loss: 0.6635
2022-03-04 22:35:57 - train: epoch 0190, iter [02900, 05004], lr: 0.000783, loss: 0.7721
2022-03-04 22:36:30 - train: epoch 0190, iter [03000, 05004], lr: 0.000783, loss: 0.7381
2022-03-04 22:37:04 - train: epoch 0190, iter [03100, 05004], lr: 0.000783, loss: 0.7156
2022-03-04 22:37:37 - train: epoch 0190, iter [03200, 05004], lr: 0.000783, loss: 0.9383
2022-03-04 22:38:10 - train: epoch 0190, iter [03300, 05004], lr: 0.000783, loss: 0.8086
2022-03-04 22:38:43 - train: epoch 0190, iter [03400, 05004], lr: 0.000783, loss: 0.7750
2022-03-04 22:39:16 - train: epoch 0190, iter [03500, 05004], lr: 0.000783, loss: 0.6076
2022-03-04 22:39:49 - train: epoch 0190, iter [03600, 05004], lr: 0.000783, loss: 0.6288
2022-03-04 22:40:23 - train: epoch 0190, iter [03700, 05004], lr: 0.000783, loss: 0.6426
2022-03-04 22:40:55 - train: epoch 0190, iter [03800, 05004], lr: 0.000783, loss: 0.6727
2022-03-04 22:41:28 - train: epoch 0190, iter [03900, 05004], lr: 0.000783, loss: 0.7071
2022-03-04 22:42:00 - train: epoch 0190, iter [04000, 05004], lr: 0.000783, loss: 0.7732
2022-03-04 22:42:33 - train: epoch 0190, iter [04100, 05004], lr: 0.000783, loss: 0.7827
2022-03-04 22:43:07 - train: epoch 0190, iter [04200, 05004], lr: 0.000783, loss: 0.7037
2022-03-04 22:43:40 - train: epoch 0190, iter [04300, 05004], lr: 0.000783, loss: 0.7729
2022-03-04 22:44:13 - train: epoch 0190, iter [04400, 05004], lr: 0.000783, loss: 0.6799
2022-03-04 22:44:47 - train: epoch 0190, iter [04500, 05004], lr: 0.000783, loss: 0.7855
2022-03-04 22:45:19 - train: epoch 0190, iter [04600, 05004], lr: 0.000783, loss: 0.9053
2022-03-04 22:45:53 - train: epoch 0190, iter [04700, 05004], lr: 0.000783, loss: 0.8380
2022-03-04 22:46:26 - train: epoch 0190, iter [04800, 05004], lr: 0.000783, loss: 0.7794
2022-03-04 22:47:00 - train: epoch 0190, iter [04900, 05004], lr: 0.000783, loss: 0.6789
2022-03-04 22:47:31 - train: epoch 0190, iter [05000, 05004], lr: 0.000783, loss: 0.9025
2022-03-04 22:47:32 - train: epoch 190, train_loss: 0.7644
2022-03-04 22:48:46 - eval: epoch: 190, acc1: 76.888%, acc5: 93.446%, test_loss: 0.9245, per_image_load_time: 2.361ms, per_image_inference_time: 0.525ms
2022-03-04 22:48:47 - until epoch: 190, best_acc1: 76.888%
2022-03-04 22:48:47 - epoch 191 lr: 0.0006474868681043578
2022-03-04 22:49:25 - train: epoch 0191, iter [00100, 05004], lr: 0.000647, loss: 0.7047
2022-03-04 22:49:58 - train: epoch 0191, iter [00200, 05004], lr: 0.000647, loss: 0.7747
2022-03-04 22:50:31 - train: epoch 0191, iter [00300, 05004], lr: 0.000647, loss: 0.7990
2022-03-04 22:51:04 - train: epoch 0191, iter [00400, 05004], lr: 0.000647, loss: 0.8428
2022-03-04 22:51:38 - train: epoch 0191, iter [00500, 05004], lr: 0.000647, loss: 0.7345
2022-03-04 22:52:10 - train: epoch 0191, iter [00600, 05004], lr: 0.000647, loss: 0.7729
2022-03-04 22:52:43 - train: epoch 0191, iter [00700, 05004], lr: 0.000647, loss: 1.0322
2022-03-04 22:53:17 - train: epoch 0191, iter [00800, 05004], lr: 0.000647, loss: 0.6594
2022-03-04 22:53:50 - train: epoch 0191, iter [00900, 05004], lr: 0.000647, loss: 0.6973
2022-03-04 22:54:23 - train: epoch 0191, iter [01000, 05004], lr: 0.000647, loss: 0.7178
2022-03-04 22:54:57 - train: epoch 0191, iter [01100, 05004], lr: 0.000647, loss: 0.6934
2022-03-04 22:55:31 - train: epoch 0191, iter [01200, 05004], lr: 0.000647, loss: 0.8469
2022-03-04 22:56:04 - train: epoch 0191, iter [01300, 05004], lr: 0.000647, loss: 0.7422
2022-03-04 22:56:37 - train: epoch 0191, iter [01400, 05004], lr: 0.000647, loss: 0.6308
2022-03-04 22:57:10 - train: epoch 0191, iter [01500, 05004], lr: 0.000647, loss: 0.6608
2022-03-04 22:57:44 - train: epoch 0191, iter [01600, 05004], lr: 0.000647, loss: 0.8057
2022-03-04 22:58:18 - train: epoch 0191, iter [01700, 05004], lr: 0.000647, loss: 0.8616
2022-03-04 22:58:51 - train: epoch 0191, iter [01800, 05004], lr: 0.000647, loss: 0.8395
2022-03-04 22:59:23 - train: epoch 0191, iter [01900, 05004], lr: 0.000647, loss: 0.7243
2022-03-04 22:59:57 - train: epoch 0191, iter [02000, 05004], lr: 0.000647, loss: 0.8348
2022-03-04 23:00:30 - train: epoch 0191, iter [02100, 05004], lr: 0.000647, loss: 0.8673
2022-03-04 23:01:04 - train: epoch 0191, iter [02200, 05004], lr: 0.000647, loss: 0.6698
2022-03-04 23:01:37 - train: epoch 0191, iter [02300, 05004], lr: 0.000647, loss: 0.5598
2022-03-04 23:02:10 - train: epoch 0191, iter [02400, 05004], lr: 0.000647, loss: 0.6962
2022-03-04 23:02:44 - train: epoch 0191, iter [02500, 05004], lr: 0.000647, loss: 0.6361
2022-03-04 23:03:17 - train: epoch 0191, iter [02600, 05004], lr: 0.000647, loss: 0.8753
2022-03-04 23:03:50 - train: epoch 0191, iter [02700, 05004], lr: 0.000647, loss: 0.8457
2022-03-04 23:04:23 - train: epoch 0191, iter [02800, 05004], lr: 0.000647, loss: 0.8102
2022-03-04 23:04:56 - train: epoch 0191, iter [02900, 05004], lr: 0.000647, loss: 0.5495
2022-03-04 23:05:30 - train: epoch 0191, iter [03000, 05004], lr: 0.000647, loss: 0.7989
2022-03-04 23:06:03 - train: epoch 0191, iter [03100, 05004], lr: 0.000647, loss: 0.7926
2022-03-04 23:06:37 - train: epoch 0191, iter [03200, 05004], lr: 0.000647, loss: 0.8681
2022-03-04 23:07:10 - train: epoch 0191, iter [03300, 05004], lr: 0.000647, loss: 0.7291
2022-03-04 23:07:44 - train: epoch 0191, iter [03400, 05004], lr: 0.000647, loss: 0.6887
2022-03-04 23:08:17 - train: epoch 0191, iter [03500, 05004], lr: 0.000647, loss: 0.7987
2022-03-04 23:08:50 - train: epoch 0191, iter [03600, 05004], lr: 0.000647, loss: 0.7228
2022-03-04 23:09:24 - train: epoch 0191, iter [03700, 05004], lr: 0.000647, loss: 0.6585
2022-03-04 23:09:56 - train: epoch 0191, iter [03800, 05004], lr: 0.000647, loss: 0.6313
2022-03-04 23:10:30 - train: epoch 0191, iter [03900, 05004], lr: 0.000647, loss: 0.8461
2022-03-04 23:11:03 - train: epoch 0191, iter [04000, 05004], lr: 0.000647, loss: 0.7729
2022-03-04 23:11:37 - train: epoch 0191, iter [04100, 05004], lr: 0.000647, loss: 0.7853
2022-03-04 23:12:10 - train: epoch 0191, iter [04200, 05004], lr: 0.000647, loss: 0.7275
2022-03-04 23:12:43 - train: epoch 0191, iter [04300, 05004], lr: 0.000647, loss: 0.7101
2022-03-04 23:13:17 - train: epoch 0191, iter [04400, 05004], lr: 0.000647, loss: 0.8719
2022-03-04 23:13:50 - train: epoch 0191, iter [04500, 05004], lr: 0.000647, loss: 0.6183
2022-03-04 23:14:23 - train: epoch 0191, iter [04600, 05004], lr: 0.000647, loss: 0.6823
2022-03-04 23:14:56 - train: epoch 0191, iter [04700, 05004], lr: 0.000647, loss: 0.7280
2022-03-04 23:15:29 - train: epoch 0191, iter [04800, 05004], lr: 0.000647, loss: 0.8003
2022-03-04 23:16:02 - train: epoch 0191, iter [04900, 05004], lr: 0.000647, loss: 0.6925
2022-03-04 23:16:34 - train: epoch 0191, iter [05000, 05004], lr: 0.000647, loss: 0.8510
2022-03-04 23:16:36 - train: epoch 191, train_loss: 0.7550
2022-03-04 23:17:49 - eval: epoch: 191, acc1: 76.834%, acc5: 93.452%, test_loss: 0.9260, per_image_load_time: 2.301ms, per_image_inference_time: 0.512ms
2022-03-04 23:17:50 - until epoch: 191, best_acc1: 76.888%
2022-03-04 23:17:50 - epoch 192 lr: 0.000524680027447444
2022-03-04 23:18:27 - train: epoch 0192, iter [00100, 05004], lr: 0.000525, loss: 0.8080
2022-03-04 23:19:00 - train: epoch 0192, iter [00200, 05004], lr: 0.000525, loss: 0.8592
2022-03-04 23:19:34 - train: epoch 0192, iter [00300, 05004], lr: 0.000525, loss: 0.6124
2022-03-04 23:20:06 - train: epoch 0192, iter [00400, 05004], lr: 0.000525, loss: 0.7435
2022-03-04 23:20:40 - train: epoch 0192, iter [00500, 05004], lr: 0.000525, loss: 0.7606
2022-03-04 23:21:12 - train: epoch 0192, iter [00600, 05004], lr: 0.000525, loss: 0.7678
2022-03-04 23:21:46 - train: epoch 0192, iter [00700, 05004], lr: 0.000525, loss: 1.0333
2022-03-04 23:22:19 - train: epoch 0192, iter [00800, 05004], lr: 0.000525, loss: 0.6469
2022-03-04 23:22:52 - train: epoch 0192, iter [00900, 05004], lr: 0.000525, loss: 0.7515
2022-03-04 23:23:26 - train: epoch 0192, iter [01000, 05004], lr: 0.000525, loss: 0.7302
2022-03-04 23:23:59 - train: epoch 0192, iter [01100, 05004], lr: 0.000525, loss: 0.7094
2022-03-04 23:24:32 - train: epoch 0192, iter [01200, 05004], lr: 0.000525, loss: 0.7827
2022-03-04 23:25:06 - train: epoch 0192, iter [01300, 05004], lr: 0.000525, loss: 0.7896
2022-03-04 23:25:39 - train: epoch 0192, iter [01400, 05004], lr: 0.000525, loss: 0.6770
2022-03-04 23:26:12 - train: epoch 0192, iter [01500, 05004], lr: 0.000525, loss: 0.7465
2022-03-04 23:26:45 - train: epoch 0192, iter [01600, 05004], lr: 0.000525, loss: 0.6557
2022-03-04 23:27:19 - train: epoch 0192, iter [01700, 05004], lr: 0.000525, loss: 0.6570
2022-03-04 23:27:52 - train: epoch 0192, iter [01800, 05004], lr: 0.000525, loss: 0.5916
2022-03-04 23:28:25 - train: epoch 0192, iter [01900, 05004], lr: 0.000525, loss: 0.7926
2022-03-04 23:28:58 - train: epoch 0192, iter [02000, 05004], lr: 0.000525, loss: 0.8635
2022-03-04 23:29:31 - train: epoch 0192, iter [02100, 05004], lr: 0.000525, loss: 0.6874
2022-03-04 23:30:05 - train: epoch 0192, iter [02200, 05004], lr: 0.000525, loss: 0.9231
2022-03-04 23:30:38 - train: epoch 0192, iter [02300, 05004], lr: 0.000525, loss: 0.6780
2022-03-04 23:31:12 - train: epoch 0192, iter [02400, 05004], lr: 0.000525, loss: 0.6770
2022-03-04 23:31:45 - train: epoch 0192, iter [02500, 05004], lr: 0.000525, loss: 0.7619
2022-03-04 23:32:19 - train: epoch 0192, iter [02600, 05004], lr: 0.000525, loss: 0.8928
2022-03-04 23:32:52 - train: epoch 0192, iter [02700, 05004], lr: 0.000525, loss: 0.7487
2022-03-04 23:33:25 - train: epoch 0192, iter [02800, 05004], lr: 0.000525, loss: 0.8930
2022-03-04 23:33:59 - train: epoch 0192, iter [02900, 05004], lr: 0.000525, loss: 0.7380
2022-03-04 23:34:32 - train: epoch 0192, iter [03000, 05004], lr: 0.000525, loss: 0.8279
2022-03-04 23:35:05 - train: epoch 0192, iter [03100, 05004], lr: 0.000525, loss: 0.7378
2022-03-04 23:35:39 - train: epoch 0192, iter [03200, 05004], lr: 0.000525, loss: 0.5791
2022-03-04 23:36:13 - train: epoch 0192, iter [03300, 05004], lr: 0.000525, loss: 0.7528
2022-03-04 23:36:46 - train: epoch 0192, iter [03400, 05004], lr: 0.000525, loss: 0.8965
2022-03-04 23:37:19 - train: epoch 0192, iter [03500, 05004], lr: 0.000525, loss: 0.7226
2022-03-04 23:37:52 - train: epoch 0192, iter [03600, 05004], lr: 0.000525, loss: 0.8460
2022-03-04 23:38:25 - train: epoch 0192, iter [03700, 05004], lr: 0.000525, loss: 0.7506
2022-03-04 23:38:58 - train: epoch 0192, iter [03800, 05004], lr: 0.000525, loss: 0.7037
2022-03-04 23:39:32 - train: epoch 0192, iter [03900, 05004], lr: 0.000525, loss: 0.7462
2022-03-04 23:40:05 - train: epoch 0192, iter [04000, 05004], lr: 0.000525, loss: 0.7484
2022-03-04 23:40:39 - train: epoch 0192, iter [04100, 05004], lr: 0.000525, loss: 0.6817
2022-03-04 23:41:12 - train: epoch 0192, iter [04200, 05004], lr: 0.000525, loss: 0.8768
2022-03-04 23:41:46 - train: epoch 0192, iter [04300, 05004], lr: 0.000525, loss: 0.6806
2022-03-04 23:42:20 - train: epoch 0192, iter [04400, 05004], lr: 0.000525, loss: 0.6991
2022-03-04 23:42:52 - train: epoch 0192, iter [04500, 05004], lr: 0.000525, loss: 0.7944
2022-03-04 23:43:25 - train: epoch 0192, iter [04600, 05004], lr: 0.000525, loss: 0.7888
2022-03-04 23:43:59 - train: epoch 0192, iter [04700, 05004], lr: 0.000525, loss: 0.6937
2022-03-04 23:44:33 - train: epoch 0192, iter [04800, 05004], lr: 0.000525, loss: 0.6981
2022-03-04 23:45:05 - train: epoch 0192, iter [04900, 05004], lr: 0.000525, loss: 0.8346
2022-03-04 23:45:38 - train: epoch 0192, iter [05000, 05004], lr: 0.000525, loss: 0.7146
2022-03-04 23:45:39 - train: epoch 192, train_loss: 0.7429
2022-03-04 23:46:53 - eval: epoch: 192, acc1: 76.870%, acc5: 93.472%, test_loss: 0.9225, per_image_load_time: 2.318ms, per_image_inference_time: 0.508ms
2022-03-04 23:46:53 - until epoch: 192, best_acc1: 76.888%
2022-03-04 23:46:53 - epoch 193 lr: 0.00041471450892189844
2022-03-04 23:47:31 - train: epoch 0193, iter [00100, 05004], lr: 0.000415, loss: 0.8308
2022-03-04 23:48:05 - train: epoch 0193, iter [00200, 05004], lr: 0.000415, loss: 0.7674
2022-03-04 23:48:38 - train: epoch 0193, iter [00300, 05004], lr: 0.000415, loss: 0.5970
2022-03-04 23:49:11 - train: epoch 0193, iter [00400, 05004], lr: 0.000415, loss: 0.7291
2022-03-04 23:49:45 - train: epoch 0193, iter [00500, 05004], lr: 0.000415, loss: 0.7833
2022-03-04 23:50:18 - train: epoch 0193, iter [00600, 05004], lr: 0.000415, loss: 0.7086
2022-03-04 23:50:51 - train: epoch 0193, iter [00700, 05004], lr: 0.000415, loss: 0.9675
2022-03-04 23:51:25 - train: epoch 0193, iter [00800, 05004], lr: 0.000415, loss: 0.7550
2022-03-04 23:51:57 - train: epoch 0193, iter [00900, 05004], lr: 0.000415, loss: 0.7875
2022-03-04 23:52:31 - train: epoch 0193, iter [01000, 05004], lr: 0.000415, loss: 0.9303
2022-03-04 23:53:04 - train: epoch 0193, iter [01100, 05004], lr: 0.000415, loss: 0.7820
2022-03-04 23:53:37 - train: epoch 0193, iter [01200, 05004], lr: 0.000415, loss: 0.8409
2022-03-04 23:54:11 - train: epoch 0193, iter [01300, 05004], lr: 0.000415, loss: 0.5922
2022-03-04 23:54:44 - train: epoch 0193, iter [01400, 05004], lr: 0.000415, loss: 0.7584
2022-03-04 23:55:18 - train: epoch 0193, iter [01500, 05004], lr: 0.000415, loss: 0.7121
2022-03-04 23:55:51 - train: epoch 0193, iter [01600, 05004], lr: 0.000415, loss: 0.5945
2022-03-04 23:56:24 - train: epoch 0193, iter [01700, 05004], lr: 0.000415, loss: 0.7630
2022-03-04 23:56:57 - train: epoch 0193, iter [01800, 05004], lr: 0.000415, loss: 0.7164
2022-03-04 23:57:30 - train: epoch 0193, iter [01900, 05004], lr: 0.000415, loss: 0.6873
2022-03-04 23:58:03 - train: epoch 0193, iter [02000, 05004], lr: 0.000415, loss: 0.7937
2022-03-04 23:58:36 - train: epoch 0193, iter [02100, 05004], lr: 0.000415, loss: 0.6357
2022-03-04 23:59:10 - train: epoch 0193, iter [02200, 05004], lr: 0.000415, loss: 0.7134
2022-03-04 23:59:42 - train: epoch 0193, iter [02300, 05004], lr: 0.000415, loss: 0.7752
2022-03-05 00:00:16 - train: epoch 0193, iter [02400, 05004], lr: 0.000415, loss: 0.6224
2022-03-05 00:00:49 - train: epoch 0193, iter [02500, 05004], lr: 0.000415, loss: 0.8150
2022-03-05 00:01:22 - train: epoch 0193, iter [02600, 05004], lr: 0.000415, loss: 0.7351
2022-03-05 00:01:55 - train: epoch 0193, iter [02700, 05004], lr: 0.000415, loss: 0.6536
2022-03-05 00:02:28 - train: epoch 0193, iter [02800, 05004], lr: 0.000415, loss: 0.7789
2022-03-05 00:03:02 - train: epoch 0193, iter [02900, 05004], lr: 0.000415, loss: 0.8119
2022-03-05 00:03:35 - train: epoch 0193, iter [03000, 05004], lr: 0.000415, loss: 0.6198
2022-03-05 00:04:08 - train: epoch 0193, iter [03100, 05004], lr: 0.000415, loss: 0.6422
2022-03-05 00:04:40 - train: epoch 0193, iter [03200, 05004], lr: 0.000415, loss: 0.7189
2022-03-05 00:05:14 - train: epoch 0193, iter [03300, 05004], lr: 0.000415, loss: 0.6980
2022-03-05 00:05:46 - train: epoch 0193, iter [03400, 05004], lr: 0.000415, loss: 0.6069
2022-03-05 00:06:19 - train: epoch 0193, iter [03500, 05004], lr: 0.000415, loss: 0.8742
2022-03-05 00:06:52 - train: epoch 0193, iter [03600, 05004], lr: 0.000415, loss: 0.6866
2022-03-05 00:07:26 - train: epoch 0193, iter [03700, 05004], lr: 0.000415, loss: 0.6235
2022-03-05 00:07:59 - train: epoch 0193, iter [03800, 05004], lr: 0.000415, loss: 0.7491
2022-03-05 00:08:32 - train: epoch 0193, iter [03900, 05004], lr: 0.000415, loss: 0.7628
2022-03-05 00:09:05 - train: epoch 0193, iter [04000, 05004], lr: 0.000415, loss: 0.5547
2022-03-05 00:09:39 - train: epoch 0193, iter [04100, 05004], lr: 0.000415, loss: 0.9539
2022-03-05 00:10:11 - train: epoch 0193, iter [04200, 05004], lr: 0.000415, loss: 0.7537
2022-03-05 00:10:45 - train: epoch 0193, iter [04300, 05004], lr: 0.000415, loss: 0.8610
2022-03-05 00:11:18 - train: epoch 0193, iter [04400, 05004], lr: 0.000415, loss: 0.8098
2022-03-05 00:11:51 - train: epoch 0193, iter [04500, 05004], lr: 0.000415, loss: 0.7096
2022-03-05 00:12:23 - train: epoch 0193, iter [04600, 05004], lr: 0.000415, loss: 0.8199
2022-03-05 00:12:56 - train: epoch 0193, iter [04700, 05004], lr: 0.000415, loss: 0.8223
2022-03-05 00:13:29 - train: epoch 0193, iter [04800, 05004], lr: 0.000415, loss: 0.7400
2022-03-05 00:14:02 - train: epoch 0193, iter [04900, 05004], lr: 0.000415, loss: 0.5828
2022-03-05 00:14:35 - train: epoch 0193, iter [05000, 05004], lr: 0.000415, loss: 0.8260
2022-03-05 00:14:36 - train: epoch 193, train_loss: 0.7356
2022-03-05 00:15:50 - eval: epoch: 193, acc1: 77.050%, acc5: 93.536%, test_loss: 0.9203, per_image_load_time: 2.349ms, per_image_inference_time: 0.493ms
2022-03-05 00:15:51 - until epoch: 193, best_acc1: 77.050%
2022-03-05 00:15:51 - epoch 194 lr: 0.00031761885408435053
2022-03-05 00:16:29 - train: epoch 0194, iter [00100, 05004], lr: 0.000318, loss: 0.7029
2022-03-05 00:17:01 - train: epoch 0194, iter [00200, 05004], lr: 0.000318, loss: 0.6654
2022-03-05 00:17:35 - train: epoch 0194, iter [00300, 05004], lr: 0.000318, loss: 0.7355
2022-03-05 00:18:08 - train: epoch 0194, iter [00400, 05004], lr: 0.000318, loss: 0.8230
2022-03-05 00:18:41 - train: epoch 0194, iter [00500, 05004], lr: 0.000318, loss: 0.9224
2022-03-05 00:19:14 - train: epoch 0194, iter [00600, 05004], lr: 0.000318, loss: 0.6379
2022-03-05 00:19:47 - train: epoch 0194, iter [00700, 05004], lr: 0.000318, loss: 0.7903
2022-03-05 00:20:20 - train: epoch 0194, iter [00800, 05004], lr: 0.000318, loss: 0.7125
2022-03-05 00:20:53 - train: epoch 0194, iter [00900, 05004], lr: 0.000318, loss: 0.6539
2022-03-05 00:21:27 - train: epoch 0194, iter [01000, 05004], lr: 0.000318, loss: 0.6024
2022-03-05 00:22:01 - train: epoch 0194, iter [01100, 05004], lr: 0.000318, loss: 0.7359
2022-03-05 00:22:34 - train: epoch 0194, iter [01200, 05004], lr: 0.000318, loss: 0.7995
2022-03-05 00:23:07 - train: epoch 0194, iter [01300, 05004], lr: 0.000318, loss: 0.6910
2022-03-05 00:23:40 - train: epoch 0194, iter [01400, 05004], lr: 0.000318, loss: 0.7390
2022-03-05 00:24:14 - train: epoch 0194, iter [01500, 05004], lr: 0.000318, loss: 0.7929
2022-03-05 00:24:47 - train: epoch 0194, iter [01600, 05004], lr: 0.000318, loss: 0.7556
2022-03-05 00:25:20 - train: epoch 0194, iter [01700, 05004], lr: 0.000318, loss: 0.7534
2022-03-05 00:25:53 - train: epoch 0194, iter [01800, 05004], lr: 0.000318, loss: 0.5525
2022-03-05 00:26:26 - train: epoch 0194, iter [01900, 05004], lr: 0.000318, loss: 0.7744
2022-03-05 00:27:00 - train: epoch 0194, iter [02000, 05004], lr: 0.000318, loss: 0.6844
2022-03-05 00:27:32 - train: epoch 0194, iter [02100, 05004], lr: 0.000318, loss: 0.6304
2022-03-05 00:28:06 - train: epoch 0194, iter [02200, 05004], lr: 0.000318, loss: 0.7833
2022-03-05 00:28:39 - train: epoch 0194, iter [02300, 05004], lr: 0.000318, loss: 0.5187
2022-03-05 00:29:12 - train: epoch 0194, iter [02400, 05004], lr: 0.000318, loss: 0.6479
2022-03-05 00:29:45 - train: epoch 0194, iter [02500, 05004], lr: 0.000318, loss: 0.7616
2022-03-05 00:30:18 - train: epoch 0194, iter [02600, 05004], lr: 0.000318, loss: 0.7657
2022-03-05 00:30:52 - train: epoch 0194, iter [02700, 05004], lr: 0.000318, loss: 0.7142
2022-03-05 00:31:24 - train: epoch 0194, iter [02800, 05004], lr: 0.000318, loss: 0.7813
2022-03-05 00:31:57 - train: epoch 0194, iter [02900, 05004], lr: 0.000318, loss: 0.7023
2022-03-05 00:32:30 - train: epoch 0194, iter [03000, 05004], lr: 0.000318, loss: 0.7363
2022-03-05 00:33:03 - train: epoch 0194, iter [03100, 05004], lr: 0.000318, loss: 0.6471
2022-03-05 00:33:37 - train: epoch 0194, iter [03200, 05004], lr: 0.000318, loss: 0.6979
2022-03-05 00:34:11 - train: epoch 0194, iter [03300, 05004], lr: 0.000318, loss: 0.7385
2022-03-05 00:34:43 - train: epoch 0194, iter [03400, 05004], lr: 0.000318, loss: 0.6508
2022-03-05 00:35:17 - train: epoch 0194, iter [03500, 05004], lr: 0.000318, loss: 1.0297
2022-03-05 00:35:50 - train: epoch 0194, iter [03600, 05004], lr: 0.000318, loss: 0.6122
2022-03-05 00:36:23 - train: epoch 0194, iter [03700, 05004], lr: 0.000318, loss: 0.8952
2022-03-05 00:36:56 - train: epoch 0194, iter [03800, 05004], lr: 0.000318, loss: 0.6703
2022-03-05 00:37:29 - train: epoch 0194, iter [03900, 05004], lr: 0.000318, loss: 0.5862
2022-03-05 00:38:02 - train: epoch 0194, iter [04000, 05004], lr: 0.000318, loss: 0.7419
2022-03-05 00:38:35 - train: epoch 0194, iter [04100, 05004], lr: 0.000318, loss: 0.6462
2022-03-05 00:39:09 - train: epoch 0194, iter [04200, 05004], lr: 0.000318, loss: 0.5915
2022-03-05 00:39:42 - train: epoch 0194, iter [04300, 05004], lr: 0.000318, loss: 0.5792
2022-03-05 00:40:14 - train: epoch 0194, iter [04400, 05004], lr: 0.000318, loss: 0.7836
2022-03-05 00:40:48 - train: epoch 0194, iter [04500, 05004], lr: 0.000318, loss: 0.7097
2022-03-05 00:41:21 - train: epoch 0194, iter [04600, 05004], lr: 0.000318, loss: 0.6300
2022-03-05 00:41:54 - train: epoch 0194, iter [04700, 05004], lr: 0.000318, loss: 0.6556
2022-03-05 00:42:28 - train: epoch 0194, iter [04800, 05004], lr: 0.000318, loss: 0.7095
2022-03-05 00:43:00 - train: epoch 0194, iter [04900, 05004], lr: 0.000318, loss: 0.6775
2022-03-05 00:43:32 - train: epoch 0194, iter [05000, 05004], lr: 0.000318, loss: 0.6556
2022-03-05 00:43:33 - train: epoch 194, train_loss: 0.7268
2022-03-05 00:44:47 - eval: epoch: 194, acc1: 77.032%, acc5: 93.514%, test_loss: 0.9188, per_image_load_time: 2.286ms, per_image_inference_time: 0.531ms
2022-03-05 00:44:48 - until epoch: 194, best_acc1: 77.050%
2022-03-05 00:44:48 - epoch 195 lr: 0.00023341826411756863
2022-03-05 00:45:24 - train: epoch 0195, iter [00100, 05004], lr: 0.000233, loss: 0.5820
2022-03-05 00:45:58 - train: epoch 0195, iter [00200, 05004], lr: 0.000233, loss: 0.5994
2022-03-05 00:46:31 - train: epoch 0195, iter [00300, 05004], lr: 0.000233, loss: 0.6756
2022-03-05 00:47:05 - train: epoch 0195, iter [00400, 05004], lr: 0.000233, loss: 0.7994
2022-03-05 00:47:38 - train: epoch 0195, iter [00500, 05004], lr: 0.000233, loss: 0.7520
2022-03-05 00:48:11 - train: epoch 0195, iter [00600, 05004], lr: 0.000233, loss: 0.7377
2022-03-05 00:48:44 - train: epoch 0195, iter [00700, 05004], lr: 0.000233, loss: 0.6700
2022-03-05 00:49:17 - train: epoch 0195, iter [00800, 05004], lr: 0.000233, loss: 0.7768
2022-03-05 00:49:51 - train: epoch 0195, iter [00900, 05004], lr: 0.000233, loss: 0.5883
2022-03-05 00:50:24 - train: epoch 0195, iter [01000, 05004], lr: 0.000233, loss: 0.7591
2022-03-05 00:50:57 - train: epoch 0195, iter [01100, 05004], lr: 0.000233, loss: 0.7434
2022-03-05 00:51:30 - train: epoch 0195, iter [01200, 05004], lr: 0.000233, loss: 0.8485
2022-03-05 00:52:03 - train: epoch 0195, iter [01300, 05004], lr: 0.000233, loss: 0.7109
2022-03-05 00:52:36 - train: epoch 0195, iter [01400, 05004], lr: 0.000233, loss: 0.6953
2022-03-05 00:53:09 - train: epoch 0195, iter [01500, 05004], lr: 0.000233, loss: 0.7458
2022-03-05 00:53:42 - train: epoch 0195, iter [01600, 05004], lr: 0.000233, loss: 0.7257
2022-03-05 00:54:15 - train: epoch 0195, iter [01700, 05004], lr: 0.000233, loss: 0.5988
2022-03-05 00:54:48 - train: epoch 0195, iter [01800, 05004], lr: 0.000233, loss: 0.6666
2022-03-05 00:55:20 - train: epoch 0195, iter [01900, 05004], lr: 0.000233, loss: 0.8166
2022-03-05 00:55:54 - train: epoch 0195, iter [02000, 05004], lr: 0.000233, loss: 0.7400
2022-03-05 00:56:27 - train: epoch 0195, iter [02100, 05004], lr: 0.000233, loss: 0.8747
2022-03-05 00:57:00 - train: epoch 0195, iter [02200, 05004], lr: 0.000233, loss: 0.6127
2022-03-05 00:57:33 - train: epoch 0195, iter [02300, 05004], lr: 0.000233, loss: 0.7317
2022-03-05 00:58:06 - train: epoch 0195, iter [02400, 05004], lr: 0.000233, loss: 0.7247
2022-03-05 00:58:39 - train: epoch 0195, iter [02500, 05004], lr: 0.000233, loss: 0.7159
2022-03-05 00:59:13 - train: epoch 0195, iter [02600, 05004], lr: 0.000233, loss: 0.6636
2022-03-05 00:59:46 - train: epoch 0195, iter [02700, 05004], lr: 0.000233, loss: 0.8949
2022-03-05 01:00:19 - train: epoch 0195, iter [02800, 05004], lr: 0.000233, loss: 0.6363
2022-03-05 01:00:52 - train: epoch 0195, iter [02900, 05004], lr: 0.000233, loss: 0.6580
2022-03-05 01:01:26 - train: epoch 0195, iter [03000, 05004], lr: 0.000233, loss: 0.7201
2022-03-05 01:01:59 - train: epoch 0195, iter [03100, 05004], lr: 0.000233, loss: 0.7794
2022-03-05 01:02:32 - train: epoch 0195, iter [03200, 05004], lr: 0.000233, loss: 0.7666
2022-03-05 01:03:04 - train: epoch 0195, iter [03300, 05004], lr: 0.000233, loss: 0.7925
2022-03-05 01:03:39 - train: epoch 0195, iter [03400, 05004], lr: 0.000233, loss: 0.8774
2022-03-05 01:04:12 - train: epoch 0195, iter [03500, 05004], lr: 0.000233, loss: 0.8579
2022-03-05 01:04:45 - train: epoch 0195, iter [03600, 05004], lr: 0.000233, loss: 0.7006
2022-03-05 01:05:19 - train: epoch 0195, iter [03700, 05004], lr: 0.000233, loss: 0.7726
2022-03-05 01:05:52 - train: epoch 0195, iter [03800, 05004], lr: 0.000233, loss: 0.6712
2022-03-05 01:06:24 - train: epoch 0195, iter [03900, 05004], lr: 0.000233, loss: 0.7418
2022-03-05 01:06:58 - train: epoch 0195, iter [04000, 05004], lr: 0.000233, loss: 0.6805
2022-03-05 01:07:32 - train: epoch 0195, iter [04100, 05004], lr: 0.000233, loss: 0.6016
2022-03-05 01:08:04 - train: epoch 0195, iter [04200, 05004], lr: 0.000233, loss: 0.8932
2022-03-05 01:08:37 - train: epoch 0195, iter [04300, 05004], lr: 0.000233, loss: 0.7846
2022-03-05 01:09:11 - train: epoch 0195, iter [04400, 05004], lr: 0.000233, loss: 0.6702
2022-03-05 01:09:44 - train: epoch 0195, iter [04500, 05004], lr: 0.000233, loss: 0.6811
2022-03-05 01:10:17 - train: epoch 0195, iter [04600, 05004], lr: 0.000233, loss: 0.7176
2022-03-05 01:10:50 - train: epoch 0195, iter [04700, 05004], lr: 0.000233, loss: 0.7111
2022-03-05 01:11:23 - train: epoch 0195, iter [04800, 05004], lr: 0.000233, loss: 0.6501
2022-03-05 01:11:56 - train: epoch 0195, iter [04900, 05004], lr: 0.000233, loss: 0.7963
2022-03-05 01:12:28 - train: epoch 0195, iter [05000, 05004], lr: 0.000233, loss: 0.5458
2022-03-05 01:12:29 - train: epoch 195, train_loss: 0.7225
2022-03-05 01:13:42 - eval: epoch: 195, acc1: 77.200%, acc5: 93.544%, test_loss: 0.9177, per_image_load_time: 2.345ms, per_image_inference_time: 0.510ms
2022-03-05 01:13:43 - until epoch: 195, best_acc1: 77.200%
2022-03-05 01:13:43 - epoch 196 lr: 0.00016213459328950355
2022-03-05 01:14:20 - train: epoch 0196, iter [00100, 05004], lr: 0.000162, loss: 0.7230
2022-03-05 01:14:53 - train: epoch 0196, iter [00200, 05004], lr: 0.000162, loss: 0.6927
2022-03-05 01:15:27 - train: epoch 0196, iter [00300, 05004], lr: 0.000162, loss: 0.8328
2022-03-05 01:16:00 - train: epoch 0196, iter [00400, 05004], lr: 0.000162, loss: 0.8538
2022-03-05 01:16:33 - train: epoch 0196, iter [00500, 05004], lr: 0.000162, loss: 0.6885
2022-03-05 01:17:06 - train: epoch 0196, iter [00600, 05004], lr: 0.000162, loss: 0.7004
2022-03-05 01:17:39 - train: epoch 0196, iter [00700, 05004], lr: 0.000162, loss: 0.7038
2022-03-05 01:18:12 - train: epoch 0196, iter [00800, 05004], lr: 0.000162, loss: 0.6601
2022-03-05 01:18:45 - train: epoch 0196, iter [00900, 05004], lr: 0.000162, loss: 0.6253
2022-03-05 01:19:18 - train: epoch 0196, iter [01000, 05004], lr: 0.000162, loss: 0.8097
2022-03-05 01:19:51 - train: epoch 0196, iter [01100, 05004], lr: 0.000162, loss: 0.6504
2022-03-05 01:20:25 - train: epoch 0196, iter [01200, 05004], lr: 0.000162, loss: 0.7419
2022-03-05 01:20:58 - train: epoch 0196, iter [01300, 05004], lr: 0.000162, loss: 0.6166
2022-03-05 01:21:31 - train: epoch 0196, iter [01400, 05004], lr: 0.000162, loss: 0.6770
2022-03-05 01:22:04 - train: epoch 0196, iter [01500, 05004], lr: 0.000162, loss: 0.6867
2022-03-05 01:22:37 - train: epoch 0196, iter [01600, 05004], lr: 0.000162, loss: 0.7218
2022-03-05 01:23:11 - train: epoch 0196, iter [01700, 05004], lr: 0.000162, loss: 0.7043
2022-03-05 01:23:44 - train: epoch 0196, iter [01800, 05004], lr: 0.000162, loss: 0.6700
2022-03-05 01:24:17 - train: epoch 0196, iter [01900, 05004], lr: 0.000162, loss: 0.7929
2022-03-05 01:24:50 - train: epoch 0196, iter [02000, 05004], lr: 0.000162, loss: 0.7904
2022-03-05 01:25:22 - train: epoch 0196, iter [02100, 05004], lr: 0.000162, loss: 0.8051
2022-03-05 01:25:56 - train: epoch 0196, iter [02200, 05004], lr: 0.000162, loss: 0.8315
2022-03-05 01:26:29 - train: epoch 0196, iter [02300, 05004], lr: 0.000162, loss: 0.6743
2022-03-05 01:27:01 - train: epoch 0196, iter [02400, 05004], lr: 0.000162, loss: 0.7783
2022-03-05 01:27:35 - train: epoch 0196, iter [02500, 05004], lr: 0.000162, loss: 0.7484
2022-03-05 01:28:08 - train: epoch 0196, iter [02600, 05004], lr: 0.000162, loss: 0.6353
2022-03-05 01:28:41 - train: epoch 0196, iter [02700, 05004], lr: 0.000162, loss: 0.7164
2022-03-05 01:29:15 - train: epoch 0196, iter [02800, 05004], lr: 0.000162, loss: 0.6139
2022-03-05 01:29:48 - train: epoch 0196, iter [02900, 05004], lr: 0.000162, loss: 0.8483
2022-03-05 01:30:21 - train: epoch 0196, iter [03000, 05004], lr: 0.000162, loss: 0.8037
2022-03-05 01:30:54 - train: epoch 0196, iter [03100, 05004], lr: 0.000162, loss: 0.7579
2022-03-05 01:31:27 - train: epoch 0196, iter [03200, 05004], lr: 0.000162, loss: 0.5410
2022-03-05 01:32:00 - train: epoch 0196, iter [03300, 05004], lr: 0.000162, loss: 0.6590
2022-03-05 01:32:34 - train: epoch 0196, iter [03400, 05004], lr: 0.000162, loss: 0.7863
2022-03-05 01:33:06 - train: epoch 0196, iter [03500, 05004], lr: 0.000162, loss: 0.7230
2022-03-05 01:33:39 - train: epoch 0196, iter [03600, 05004], lr: 0.000162, loss: 0.6393
2022-03-05 01:34:13 - train: epoch 0196, iter [03700, 05004], lr: 0.000162, loss: 0.7356
2022-03-05 01:34:46 - train: epoch 0196, iter [03800, 05004], lr: 0.000162, loss: 0.7828
2022-03-05 01:35:19 - train: epoch 0196, iter [03900, 05004], lr: 0.000162, loss: 0.6712
2022-03-05 01:35:52 - train: epoch 0196, iter [04000, 05004], lr: 0.000162, loss: 0.7343
2022-03-05 01:36:26 - train: epoch 0196, iter [04100, 05004], lr: 0.000162, loss: 0.7500
2022-03-05 01:36:58 - train: epoch 0196, iter [04200, 05004], lr: 0.000162, loss: 0.7210
2022-03-05 01:37:32 - train: epoch 0196, iter [04300, 05004], lr: 0.000162, loss: 0.7881
2022-03-05 01:38:05 - train: epoch 0196, iter [04400, 05004], lr: 0.000162, loss: 0.6620
2022-03-05 01:38:38 - train: epoch 0196, iter [04500, 05004], lr: 0.000162, loss: 0.7548
2022-03-05 01:39:12 - train: epoch 0196, iter [04600, 05004], lr: 0.000162, loss: 0.7130
2022-03-05 01:39:45 - train: epoch 0196, iter [04700, 05004], lr: 0.000162, loss: 0.6171
2022-03-05 01:40:19 - train: epoch 0196, iter [04800, 05004], lr: 0.000162, loss: 0.5996
2022-03-05 01:40:52 - train: epoch 0196, iter [04900, 05004], lr: 0.000162, loss: 0.6485
2022-03-05 01:41:25 - train: epoch 0196, iter [05000, 05004], lr: 0.000162, loss: 0.6652
2022-03-05 01:41:26 - train: epoch 196, train_loss: 0.7166
2022-03-05 01:42:39 - eval: epoch: 196, acc1: 77.166%, acc5: 93.508%, test_loss: 0.9171, per_image_load_time: 2.296ms, per_image_inference_time: 0.496ms
2022-03-05 01:42:39 - until epoch: 196, best_acc1: 77.200%
2022-03-05 01:42:39 - epoch 197 lr: 0.00010378634328099269
2022-03-05 01:43:17 - train: epoch 0197, iter [00100, 05004], lr: 0.000104, loss: 0.7797
2022-03-05 01:43:50 - train: epoch 0197, iter [00200, 05004], lr: 0.000104, loss: 0.7693
2022-03-05 01:44:23 - train: epoch 0197, iter [00300, 05004], lr: 0.000104, loss: 0.6009
2022-03-05 01:44:56 - train: epoch 0197, iter [00400, 05004], lr: 0.000104, loss: 0.7331
2022-03-05 01:45:30 - train: epoch 0197, iter [00500, 05004], lr: 0.000104, loss: 0.6123
2022-03-05 01:46:03 - train: epoch 0197, iter [00600, 05004], lr: 0.000104, loss: 0.8092
2022-03-05 01:46:36 - train: epoch 0197, iter [00700, 05004], lr: 0.000104, loss: 0.7672
2022-03-05 01:47:10 - train: epoch 0197, iter [00800, 05004], lr: 0.000104, loss: 0.7470
2022-03-05 01:47:43 - train: epoch 0197, iter [00900, 05004], lr: 0.000104, loss: 0.6037
2022-03-05 01:48:16 - train: epoch 0197, iter [01000, 05004], lr: 0.000104, loss: 0.8425
2022-03-05 01:48:49 - train: epoch 0197, iter [01100, 05004], lr: 0.000104, loss: 0.7665
2022-03-05 01:49:23 - train: epoch 0197, iter [01200, 05004], lr: 0.000104, loss: 0.8186
2022-03-05 01:49:55 - train: epoch 0197, iter [01300, 05004], lr: 0.000104, loss: 0.6684
2022-03-05 01:50:29 - train: epoch 0197, iter [01400, 05004], lr: 0.000104, loss: 0.7857
2022-03-05 01:51:02 - train: epoch 0197, iter [01500, 05004], lr: 0.000104, loss: 0.7191
2022-03-05 01:51:36 - train: epoch 0197, iter [01600, 05004], lr: 0.000104, loss: 0.6562
2022-03-05 01:52:09 - train: epoch 0197, iter [01700, 05004], lr: 0.000104, loss: 0.7330
2022-03-05 01:52:43 - train: epoch 0197, iter [01800, 05004], lr: 0.000104, loss: 0.6457
2022-03-05 01:53:16 - train: epoch 0197, iter [01900, 05004], lr: 0.000104, loss: 0.7266
2022-03-05 01:53:49 - train: epoch 0197, iter [02000, 05004], lr: 0.000104, loss: 0.7757
2022-03-05 01:54:22 - train: epoch 0197, iter [02100, 05004], lr: 0.000104, loss: 0.7610
2022-03-05 01:54:56 - train: epoch 0197, iter [02200, 05004], lr: 0.000104, loss: 0.7404
2022-03-05 01:55:29 - train: epoch 0197, iter [02300, 05004], lr: 0.000104, loss: 0.9397
2022-03-05 01:56:03 - train: epoch 0197, iter [02400, 05004], lr: 0.000104, loss: 0.6698
2022-03-05 01:56:36 - train: epoch 0197, iter [02500, 05004], lr: 0.000104, loss: 0.7681
2022-03-05 01:57:09 - train: epoch 0197, iter [02600, 05004], lr: 0.000104, loss: 0.8850
2022-03-05 01:57:42 - train: epoch 0197, iter [02700, 05004], lr: 0.000104, loss: 0.5937
2022-03-05 01:58:17 - train: epoch 0197, iter [02800, 05004], lr: 0.000104, loss: 0.6913
2022-03-05 01:58:49 - train: epoch 0197, iter [02900, 05004], lr: 0.000104, loss: 0.6596
2022-03-05 01:59:23 - train: epoch 0197, iter [03000, 05004], lr: 0.000104, loss: 0.7196
2022-03-05 01:59:56 - train: epoch 0197, iter [03100, 05004], lr: 0.000104, loss: 0.7800
2022-03-05 02:00:29 - train: epoch 0197, iter [03200, 05004], lr: 0.000104, loss: 0.6122
2022-03-05 02:01:04 - train: epoch 0197, iter [03300, 05004], lr: 0.000104, loss: 0.6899
2022-03-05 02:01:36 - train: epoch 0197, iter [03400, 05004], lr: 0.000104, loss: 0.7433
2022-03-05 02:02:10 - train: epoch 0197, iter [03500, 05004], lr: 0.000104, loss: 0.8070
2022-03-05 02:02:43 - train: epoch 0197, iter [03600, 05004], lr: 0.000104, loss: 0.5669
2022-03-05 02:03:15 - train: epoch 0197, iter [03700, 05004], lr: 0.000104, loss: 0.5537
2022-03-05 02:03:49 - train: epoch 0197, iter [03800, 05004], lr: 0.000104, loss: 0.8603
2022-03-05 02:04:23 - train: epoch 0197, iter [03900, 05004], lr: 0.000104, loss: 0.8469
2022-03-05 02:04:56 - train: epoch 0197, iter [04000, 05004], lr: 0.000104, loss: 0.7105
2022-03-05 02:05:29 - train: epoch 0197, iter [04100, 05004], lr: 0.000104, loss: 0.5393
2022-03-05 02:06:03 - train: epoch 0197, iter [04200, 05004], lr: 0.000104, loss: 0.6431
2022-03-05 02:06:36 - train: epoch 0197, iter [04300, 05004], lr: 0.000104, loss: 0.7011
2022-03-05 02:07:10 - train: epoch 0197, iter [04400, 05004], lr: 0.000104, loss: 0.8507
2022-03-05 02:07:43 - train: epoch 0197, iter [04500, 05004], lr: 0.000104, loss: 0.6657
2022-03-05 02:08:16 - train: epoch 0197, iter [04600, 05004], lr: 0.000104, loss: 0.8417
2022-03-05 02:08:49 - train: epoch 0197, iter [04700, 05004], lr: 0.000104, loss: 0.8598
2022-03-05 02:09:23 - train: epoch 0197, iter [04800, 05004], lr: 0.000104, loss: 0.6541
2022-03-05 02:09:56 - train: epoch 0197, iter [04900, 05004], lr: 0.000104, loss: 0.6864
2022-03-05 02:10:28 - train: epoch 0197, iter [05000, 05004], lr: 0.000104, loss: 0.6780
2022-03-05 02:10:29 - train: epoch 197, train_loss: 0.7139
2022-03-05 02:11:43 - eval: epoch: 197, acc1: 77.164%, acc5: 93.534%, test_loss: 0.9157, per_image_load_time: 2.335ms, per_image_inference_time: 0.509ms
2022-03-05 02:11:44 - until epoch: 197, best_acc1: 77.200%
2022-03-05 02:11:44 - epoch 198 lr: 5.8388658383667914e-05
2022-03-05 02:12:22 - train: epoch 0198, iter [00100, 05004], lr: 0.000058, loss: 0.6872
2022-03-05 02:12:55 - train: epoch 0198, iter [00200, 05004], lr: 0.000058, loss: 0.4989
2022-03-05 02:13:28 - train: epoch 0198, iter [00300, 05004], lr: 0.000058, loss: 0.6654
2022-03-05 02:14:01 - train: epoch 0198, iter [00400, 05004], lr: 0.000058, loss: 0.6355
2022-03-05 02:14:34 - train: epoch 0198, iter [00500, 05004], lr: 0.000058, loss: 0.7977
2022-03-05 02:15:07 - train: epoch 0198, iter [00600, 05004], lr: 0.000058, loss: 0.7752
2022-03-05 02:15:41 - train: epoch 0198, iter [00700, 05004], lr: 0.000058, loss: 0.5301
2022-03-05 02:16:14 - train: epoch 0198, iter [00800, 05004], lr: 0.000058, loss: 0.7200
2022-03-05 02:16:47 - train: epoch 0198, iter [00900, 05004], lr: 0.000058, loss: 0.6725
2022-03-05 02:17:19 - train: epoch 0198, iter [01000, 05004], lr: 0.000058, loss: 0.8162
2022-03-05 02:17:53 - train: epoch 0198, iter [01100, 05004], lr: 0.000058, loss: 0.5972
2022-03-05 02:18:26 - train: epoch 0198, iter [01200, 05004], lr: 0.000058, loss: 0.5981
2022-03-05 02:18:58 - train: epoch 0198, iter [01300, 05004], lr: 0.000058, loss: 0.5519
2022-03-05 02:19:30 - train: epoch 0198, iter [01400, 05004], lr: 0.000058, loss: 0.6859
2022-03-05 02:20:04 - train: epoch 0198, iter [01500, 05004], lr: 0.000058, loss: 0.6066
2022-03-05 02:20:37 - train: epoch 0198, iter [01600, 05004], lr: 0.000058, loss: 0.6580
2022-03-05 02:21:10 - train: epoch 0198, iter [01700, 05004], lr: 0.000058, loss: 0.5379
2022-03-05 02:21:43 - train: epoch 0198, iter [01800, 05004], lr: 0.000058, loss: 0.6610
2022-03-05 02:22:17 - train: epoch 0198, iter [01900, 05004], lr: 0.000058, loss: 0.8109
2022-03-05 02:22:50 - train: epoch 0198, iter [02000, 05004], lr: 0.000058, loss: 0.7247
2022-03-05 02:23:22 - train: epoch 0198, iter [02100, 05004], lr: 0.000058, loss: 0.7147
2022-03-05 02:23:55 - train: epoch 0198, iter [02200, 05004], lr: 0.000058, loss: 0.7306
2022-03-05 02:24:29 - train: epoch 0198, iter [02300, 05004], lr: 0.000058, loss: 0.6158
2022-03-05 02:25:02 - train: epoch 0198, iter [02400, 05004], lr: 0.000058, loss: 0.6920
2022-03-05 02:25:35 - train: epoch 0198, iter [02500, 05004], lr: 0.000058, loss: 0.7669
2022-03-05 02:26:08 - train: epoch 0198, iter [02600, 05004], lr: 0.000058, loss: 0.5852
2022-03-05 02:26:41 - train: epoch 0198, iter [02700, 05004], lr: 0.000058, loss: 0.8465
2022-03-05 02:27:14 - train: epoch 0198, iter [02800, 05004], lr: 0.000058, loss: 0.7267
2022-03-05 02:27:48 - train: epoch 0198, iter [02900, 05004], lr: 0.000058, loss: 0.7890
2022-03-05 02:28:22 - train: epoch 0198, iter [03000, 05004], lr: 0.000058, loss: 0.8315
2022-03-05 02:28:54 - train: epoch 0198, iter [03100, 05004], lr: 0.000058, loss: 0.7571
2022-03-05 02:29:28 - train: epoch 0198, iter [03200, 05004], lr: 0.000058, loss: 0.6455
2022-03-05 02:30:00 - train: epoch 0198, iter [03300, 05004], lr: 0.000058, loss: 0.5578
2022-03-05 02:30:34 - train: epoch 0198, iter [03400, 05004], lr: 0.000058, loss: 0.7687
2022-03-05 02:31:07 - train: epoch 0198, iter [03500, 05004], lr: 0.000058, loss: 0.7344
2022-03-05 02:31:41 - train: epoch 0198, iter [03600, 05004], lr: 0.000058, loss: 0.6961
2022-03-05 02:32:14 - train: epoch 0198, iter [03700, 05004], lr: 0.000058, loss: 0.7212
2022-03-05 02:32:47 - train: epoch 0198, iter [03800, 05004], lr: 0.000058, loss: 0.6864
2022-03-05 02:33:20 - train: epoch 0198, iter [03900, 05004], lr: 0.000058, loss: 0.6905
2022-03-05 02:33:53 - train: epoch 0198, iter [04000, 05004], lr: 0.000058, loss: 0.7292
2022-03-05 02:34:26 - train: epoch 0198, iter [04100, 05004], lr: 0.000058, loss: 0.6674
2022-03-05 02:34:59 - train: epoch 0198, iter [04200, 05004], lr: 0.000058, loss: 0.6296
2022-03-05 02:35:33 - train: epoch 0198, iter [04300, 05004], lr: 0.000058, loss: 0.6793
2022-03-05 02:36:06 - train: epoch 0198, iter [04400, 05004], lr: 0.000058, loss: 0.8127
2022-03-05 02:36:39 - train: epoch 0198, iter [04500, 05004], lr: 0.000058, loss: 0.7656
2022-03-05 02:37:11 - train: epoch 0198, iter [04600, 05004], lr: 0.000058, loss: 0.7467
2022-03-05 02:37:45 - train: epoch 0198, iter [04700, 05004], lr: 0.000058, loss: 0.7667
2022-03-05 02:38:18 - train: epoch 0198, iter [04800, 05004], lr: 0.000058, loss: 0.7049
2022-03-05 02:38:51 - train: epoch 0198, iter [04900, 05004], lr: 0.000058, loss: 0.7950
2022-03-05 02:39:23 - train: epoch 0198, iter [05000, 05004], lr: 0.000058, loss: 0.7780
2022-03-05 02:39:24 - train: epoch 198, train_loss: 0.7100
2022-03-05 02:40:39 - eval: epoch: 198, acc1: 77.258%, acc5: 93.544%, test_loss: 0.9154, per_image_load_time: 2.052ms, per_image_inference_time: 0.540ms
2022-03-05 02:40:40 - until epoch: 198, best_acc1: 77.258%
2022-03-05 02:40:40 - epoch 199 lr: 2.595332156925534e-05
2022-03-05 02:41:18 - train: epoch 0199, iter [00100, 05004], lr: 0.000026, loss: 0.7595
2022-03-05 02:41:51 - train: epoch 0199, iter [00200, 05004], lr: 0.000026, loss: 0.5686
2022-03-05 02:42:24 - train: epoch 0199, iter [00300, 05004], lr: 0.000026, loss: 0.8022
2022-03-05 02:42:56 - train: epoch 0199, iter [00400, 05004], lr: 0.000026, loss: 0.6534
2022-03-05 02:43:30 - train: epoch 0199, iter [00500, 05004], lr: 0.000026, loss: 0.6746
2022-03-05 02:44:03 - train: epoch 0199, iter [00600, 05004], lr: 0.000026, loss: 0.7371
2022-03-05 02:44:35 - train: epoch 0199, iter [00700, 05004], lr: 0.000026, loss: 0.6170
2022-03-05 02:45:08 - train: epoch 0199, iter [00800, 05004], lr: 0.000026, loss: 0.6722
2022-03-05 02:45:42 - train: epoch 0199, iter [00900, 05004], lr: 0.000026, loss: 0.8520
2022-03-05 02:46:16 - train: epoch 0199, iter [01000, 05004], lr: 0.000026, loss: 0.7238
2022-03-05 02:46:48 - train: epoch 0199, iter [01100, 05004], lr: 0.000026, loss: 0.6835
2022-03-05 02:47:22 - train: epoch 0199, iter [01200, 05004], lr: 0.000026, loss: 0.7765
2022-03-05 02:47:55 - train: epoch 0199, iter [01300, 05004], lr: 0.000026, loss: 0.7174
2022-03-05 02:48:28 - train: epoch 0199, iter [01400, 05004], lr: 0.000026, loss: 0.6735
2022-03-05 02:49:02 - train: epoch 0199, iter [01500, 05004], lr: 0.000026, loss: 0.8594
2022-03-05 02:49:36 - train: epoch 0199, iter [01600, 05004], lr: 0.000026, loss: 0.7157
2022-03-05 02:50:10 - train: epoch 0199, iter [01700, 05004], lr: 0.000026, loss: 0.6896
2022-03-05 02:50:42 - train: epoch 0199, iter [01800, 05004], lr: 0.000026, loss: 0.7915
2022-03-05 02:51:16 - train: epoch 0199, iter [01900, 05004], lr: 0.000026, loss: 0.6292
2022-03-05 02:51:49 - train: epoch 0199, iter [02000, 05004], lr: 0.000026, loss: 0.8250
2022-03-05 02:52:22 - train: epoch 0199, iter [02100, 05004], lr: 0.000026, loss: 0.7461
2022-03-05 02:52:56 - train: epoch 0199, iter [02200, 05004], lr: 0.000026, loss: 0.7709
2022-03-05 02:53:29 - train: epoch 0199, iter [02300, 05004], lr: 0.000026, loss: 0.7622
2022-03-05 02:54:02 - train: epoch 0199, iter [02400, 05004], lr: 0.000026, loss: 0.6724
2022-03-05 02:54:35 - train: epoch 0199, iter [02500, 05004], lr: 0.000026, loss: 0.7395
2022-03-05 02:55:08 - train: epoch 0199, iter [02600, 05004], lr: 0.000026, loss: 0.5674
2022-03-05 02:55:42 - train: epoch 0199, iter [02700, 05004], lr: 0.000026, loss: 0.8288
2022-03-05 02:56:14 - train: epoch 0199, iter [02800, 05004], lr: 0.000026, loss: 0.6799
2022-03-05 02:56:48 - train: epoch 0199, iter [02900, 05004], lr: 0.000026, loss: 0.7664
2022-03-05 02:57:21 - train: epoch 0199, iter [03000, 05004], lr: 0.000026, loss: 0.7125
2022-03-05 02:57:55 - train: epoch 0199, iter [03100, 05004], lr: 0.000026, loss: 0.7726
2022-03-05 02:58:27 - train: epoch 0199, iter [03200, 05004], lr: 0.000026, loss: 0.6572
2022-03-05 02:59:01 - train: epoch 0199, iter [03300, 05004], lr: 0.000026, loss: 0.7783
2022-03-05 02:59:34 - train: epoch 0199, iter [03400, 05004], lr: 0.000026, loss: 0.8718
2022-03-05 03:00:08 - train: epoch 0199, iter [03500, 05004], lr: 0.000026, loss: 0.5929
2022-03-05 03:00:40 - train: epoch 0199, iter [03600, 05004], lr: 0.000026, loss: 0.6515
2022-03-05 03:01:14 - train: epoch 0199, iter [03700, 05004], lr: 0.000026, loss: 0.5574
2022-03-05 03:01:46 - train: epoch 0199, iter [03800, 05004], lr: 0.000026, loss: 0.7690
2022-03-05 03:02:20 - train: epoch 0199, iter [03900, 05004], lr: 0.000026, loss: 0.7151
2022-03-05 03:02:53 - train: epoch 0199, iter [04000, 05004], lr: 0.000026, loss: 0.6320
2022-03-05 03:03:26 - train: epoch 0199, iter [04100, 05004], lr: 0.000026, loss: 0.7396
2022-03-05 03:04:00 - train: epoch 0199, iter [04200, 05004], lr: 0.000026, loss: 0.6101
2022-03-05 03:04:32 - train: epoch 0199, iter [04300, 05004], lr: 0.000026, loss: 0.6102
2022-03-05 03:05:05 - train: epoch 0199, iter [04400, 05004], lr: 0.000026, loss: 0.6350
2022-03-05 03:05:38 - train: epoch 0199, iter [04500, 05004], lr: 0.000026, loss: 0.7117
2022-03-05 03:06:12 - train: epoch 0199, iter [04600, 05004], lr: 0.000026, loss: 0.7628
2022-03-05 03:06:45 - train: epoch 0199, iter [04700, 05004], lr: 0.000026, loss: 0.6961
2022-03-05 03:07:17 - train: epoch 0199, iter [04800, 05004], lr: 0.000026, loss: 0.6826
2022-03-05 03:07:51 - train: epoch 0199, iter [04900, 05004], lr: 0.000026, loss: 0.6024
2022-03-05 03:08:22 - train: epoch 0199, iter [05000, 05004], lr: 0.000026, loss: 0.6758
2022-03-05 03:08:24 - train: epoch 199, train_loss: 0.7053
2022-03-05 03:09:37 - eval: epoch: 199, acc1: 77.242%, acc5: 93.560%, test_loss: 0.9142, per_image_load_time: 2.326ms, per_image_inference_time: 0.512ms
2022-03-05 03:09:38 - until epoch: 199, best_acc1: 77.258%
2022-03-05 03:09:38 - epoch 200 lr: 6.488751431266149e-06
2022-03-05 03:10:16 - train: epoch 0200, iter [00100, 05004], lr: 0.000006, loss: 0.7150
2022-03-05 03:10:50 - train: epoch 0200, iter [00200, 05004], lr: 0.000006, loss: 0.6983
2022-03-05 03:11:22 - train: epoch 0200, iter [00300, 05004], lr: 0.000006, loss: 0.5830
2022-03-05 03:11:56 - train: epoch 0200, iter [00400, 05004], lr: 0.000006, loss: 0.7771
2022-03-05 03:12:28 - train: epoch 0200, iter [00500, 05004], lr: 0.000006, loss: 0.6397
2022-03-05 03:13:02 - train: epoch 0200, iter [00600, 05004], lr: 0.000006, loss: 0.7907
2022-03-05 03:13:35 - train: epoch 0200, iter [00700, 05004], lr: 0.000006, loss: 0.6685
2022-03-05 03:14:08 - train: epoch 0200, iter [00800, 05004], lr: 0.000006, loss: 0.7679
2022-03-05 03:14:42 - train: epoch 0200, iter [00900, 05004], lr: 0.000006, loss: 0.6489
2022-03-05 03:15:15 - train: epoch 0200, iter [01000, 05004], lr: 0.000006, loss: 0.7615
2022-03-05 03:15:49 - train: epoch 0200, iter [01100, 05004], lr: 0.000006, loss: 0.6441
2022-03-05 03:16:22 - train: epoch 0200, iter [01200, 05004], lr: 0.000006, loss: 0.6827
2022-03-05 03:16:55 - train: epoch 0200, iter [01300, 05004], lr: 0.000006, loss: 0.6165
2022-03-05 03:17:29 - train: epoch 0200, iter [01400, 05004], lr: 0.000006, loss: 1.0114
2022-03-05 03:18:02 - train: epoch 0200, iter [01500, 05004], lr: 0.000006, loss: 0.6371
2022-03-05 03:18:36 - train: epoch 0200, iter [01600, 05004], lr: 0.000006, loss: 0.7766
2022-03-05 03:19:09 - train: epoch 0200, iter [01700, 05004], lr: 0.000006, loss: 0.6756
2022-03-05 03:19:43 - train: epoch 0200, iter [01800, 05004], lr: 0.000006, loss: 0.7422
2022-03-05 03:20:15 - train: epoch 0200, iter [01900, 05004], lr: 0.000006, loss: 0.9378
2022-03-05 03:20:49 - train: epoch 0200, iter [02000, 05004], lr: 0.000006, loss: 0.9580
2022-03-05 03:21:22 - train: epoch 0200, iter [02100, 05004], lr: 0.000006, loss: 0.7379
2022-03-05 03:21:54 - train: epoch 0200, iter [02200, 05004], lr: 0.000006, loss: 0.8237
2022-03-05 03:22:28 - train: epoch 0200, iter [02300, 05004], lr: 0.000006, loss: 0.7649
2022-03-05 03:23:01 - train: epoch 0200, iter [02400, 05004], lr: 0.000006, loss: 0.7140
2022-03-05 03:23:34 - train: epoch 0200, iter [02500, 05004], lr: 0.000006, loss: 0.7545
2022-03-05 03:24:08 - train: epoch 0200, iter [02600, 05004], lr: 0.000006, loss: 0.7262
2022-03-05 03:24:40 - train: epoch 0200, iter [02700, 05004], lr: 0.000006, loss: 0.7283
2022-03-05 03:25:14 - train: epoch 0200, iter [02800, 05004], lr: 0.000006, loss: 0.5426
2022-03-05 03:25:47 - train: epoch 0200, iter [02900, 05004], lr: 0.000006, loss: 0.7184
2022-03-05 03:26:21 - train: epoch 0200, iter [03000, 05004], lr: 0.000006, loss: 0.7322
2022-03-05 03:26:54 - train: epoch 0200, iter [03100, 05004], lr: 0.000006, loss: 0.6297
2022-03-05 03:27:27 - train: epoch 0200, iter [03200, 05004], lr: 0.000006, loss: 0.7744
2022-03-05 03:28:01 - train: epoch 0200, iter [03300, 05004], lr: 0.000006, loss: 0.6873
2022-03-05 03:28:35 - train: epoch 0200, iter [03400, 05004], lr: 0.000006, loss: 0.6890
2022-03-05 03:29:08 - train: epoch 0200, iter [03500, 05004], lr: 0.000006, loss: 0.7277
2022-03-05 03:29:42 - train: epoch 0200, iter [03600, 05004], lr: 0.000006, loss: 0.6986
2022-03-05 03:30:15 - train: epoch 0200, iter [03700, 05004], lr: 0.000006, loss: 0.7618
2022-03-05 03:30:48 - train: epoch 0200, iter [03800, 05004], lr: 0.000006, loss: 0.6146
2022-03-05 03:31:21 - train: epoch 0200, iter [03900, 05004], lr: 0.000006, loss: 0.6634
2022-03-05 03:31:55 - train: epoch 0200, iter [04000, 05004], lr: 0.000006, loss: 0.6909
2022-03-05 03:32:28 - train: epoch 0200, iter [04100, 05004], lr: 0.000006, loss: 0.6043
2022-03-05 03:33:02 - train: epoch 0200, iter [04200, 05004], lr: 0.000006, loss: 0.7692
2022-03-05 03:33:34 - train: epoch 0200, iter [04300, 05004], lr: 0.000006, loss: 0.7230
2022-03-05 03:34:07 - train: epoch 0200, iter [04400, 05004], lr: 0.000006, loss: 0.6812
2022-03-05 03:34:41 - train: epoch 0200, iter [04500, 05004], lr: 0.000006, loss: 0.7245
2022-03-05 03:35:14 - train: epoch 0200, iter [04600, 05004], lr: 0.000006, loss: 0.6631
2022-03-05 03:35:47 - train: epoch 0200, iter [04700, 05004], lr: 0.000006, loss: 0.7592
2022-03-05 03:36:20 - train: epoch 0200, iter [04800, 05004], lr: 0.000006, loss: 0.6536
2022-03-05 03:36:53 - train: epoch 0200, iter [04900, 05004], lr: 0.000006, loss: 0.8788
2022-03-05 03:37:25 - train: epoch 0200, iter [05000, 05004], lr: 0.000006, loss: 0.7298
2022-03-05 03:37:26 - train: epoch 200, train_loss: 0.7052
2022-03-05 03:38:40 - eval: epoch: 200, acc1: 77.180%, acc5: 93.524%, test_loss: 0.9144, per_image_load_time: 2.329ms, per_image_inference_time: 0.535ms
2022-03-05 03:38:41 - until epoch: 200, best_acc1: 77.258%
2022-03-05 03:38:41 - train done. model: resnet50, train time: 101.029 hours, best_acc1: 77.258%
2022-03-05 13:22:42 - network: resnet50
2022-03-05 13:22:42 - num_classes: 1000
2022-03-05 13:22:42 - input_image_size: 224
2022-03-05 13:22:42 - scale: 1.1428571428571428
2022-03-05 13:22:42 - trained_model_path: 
2022-03-05 13:22:42 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-05 13:22:42 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f7ec4054b20>
2022-03-05 13:22:42 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f7ec4054df0>
2022-03-05 13:22:42 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f7ec4054e20>
2022-03-05 13:22:42 - seed: 0
2022-03-05 13:22:42 - batch_size: 256
2022-03-05 13:22:42 - num_workers: 16
2022-03-05 13:22:42 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-05 13:22:42 - scheduler: ('CosineLR', {'warm_up_epochs': 5})
2022-03-05 13:22:42 - epochs: 200
2022-03-05 13:22:42 - print_interval: 100
2022-03-05 13:22:42 - distributed: True
2022-03-05 13:22:42 - sync_bn: False
2022-03-05 13:22:42 - apex: True
2022-03-05 13:22:42 - gpus_type: NVIDIA RTX A5000
2022-03-05 13:22:42 - gpus_num: 2
2022-03-05 13:22:42 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f7ea307e8f0>
2022-03-05 13:22:42 - --------------------parameters--------------------
2022-03-05 13:22:42 - name: conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-03-05 13:22:42 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-03-05 13:22:42 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-03-05 13:22:42 - name: fc.weight, grad: True
2022-03-05 13:22:42 - name: fc.bias, grad: True
2022-03-05 13:22:42 - --------------------buffers--------------------
2022-03-05 13:22:42 - name: conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:42 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-05 13:22:42 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-03-05 13:22:42 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:22:43 - resuming model from ./checkpoints/latest.pth. resume_epoch: 200, used_time: 101.029 hours, best_acc1: 77.258%, test_loss: 0.9144, lr: 0.000000
2022-03-05 13:22:43 - train done. model: resnet50, train time: 101.029 hours, best_acc1: 77.258%
