2022-02-28 22:34:29 - network: resnet50
2022-02-28 22:34:29 - num_classes: 1000
2022-02-28 22:34:29 - input_image_size: 224
2022-02-28 22:34:29 - scale: 1.1428571428571428
2022-02-28 22:34:29 - trained_model_path: 
2022-02-28 22:34:29 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-28 22:34:29 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f6ff7e6b940>
2022-02-28 22:34:29 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f6fd0034160>
2022-02-28 22:34:29 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f6fd0034190>
2022-02-28 22:34:29 - seed: 0
2022-02-28 22:34:29 - batch_size: 256
2022-02-28 22:34:29 - num_workers: 16
2022-02-28 22:34:29 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-28 22:34:29 - scheduler: ('CosineLR', {'warm_up_epochs': 5})
2022-02-28 22:34:29 - epochs: 200
2022-02-28 22:34:29 - print_interval: 100
2022-02-28 22:34:29 - distributed: True
2022-02-28 22:34:29 - sync_bn: False
2022-02-28 22:34:29 - apex: True
2022-02-28 22:34:29 - gpus_type: NVIDIA GeForce RTX 3090
2022-02-28 22:34:29 - gpus_num: 2
2022-02-28 22:34:29 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f6fcd1e0ef0>
2022-02-28 22:34:34 - --------------------parameters--------------------
2022-02-28 22:34:34 - name: conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-02-28 22:34:34 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-02-28 22:34:34 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-02-28 22:34:34 - name: fc.weight, grad: True
2022-02-28 22:34:34 - name: fc.bias, grad: True
2022-02-28 22:34:34 - --------------------buffers--------------------
2022-02-28 22:34:34 - name: conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-02-28 22:34:34 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-02-28 22:34:34 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-28 22:34:34 - epoch 001 lr: 0.020000000000000004
2022-02-28 22:35:16 - train: epoch 0001, iter [00100, 05004], lr: 0.020000, loss: 6.9991
2022-02-28 22:35:52 - train: epoch 0001, iter [00200, 05004], lr: 0.020000, loss: 6.8638
2022-02-28 22:36:27 - train: epoch 0001, iter [00300, 05004], lr: 0.020000, loss: 6.7499
2022-02-28 22:37:03 - train: epoch 0001, iter [00400, 05004], lr: 0.020000, loss: 6.7100
2022-02-28 22:37:38 - train: epoch 0001, iter [00500, 05004], lr: 0.020000, loss: 6.6354
2022-02-28 22:38:14 - train: epoch 0001, iter [00600, 05004], lr: 0.020000, loss: 6.4587
2022-02-28 22:38:47 - train: epoch 0001, iter [00700, 05004], lr: 0.020000, loss: 6.4715
2022-02-28 22:39:20 - train: epoch 0001, iter [00800, 05004], lr: 0.020000, loss: 6.2919
2022-02-28 22:39:53 - train: epoch 0001, iter [00900, 05004], lr: 0.020000, loss: 6.2042
2022-02-28 22:40:29 - train: epoch 0001, iter [01000, 05004], lr: 0.020000, loss: 6.2468
2022-02-28 22:41:04 - train: epoch 0001, iter [01100, 05004], lr: 0.020000, loss: 6.1217
2022-02-28 22:41:39 - train: epoch 0001, iter [01200, 05004], lr: 0.020000, loss: 5.9844
2022-02-28 22:42:15 - train: epoch 0001, iter [01300, 05004], lr: 0.020000, loss: 5.9971
2022-02-28 22:42:50 - train: epoch 0001, iter [01400, 05004], lr: 0.020000, loss: 5.9589
2022-02-28 22:43:26 - train: epoch 0001, iter [01500, 05004], lr: 0.020000, loss: 5.7962
2022-02-28 22:44:01 - train: epoch 0001, iter [01600, 05004], lr: 0.020000, loss: 5.8053
2022-02-28 22:44:36 - train: epoch 0001, iter [01700, 05004], lr: 0.020000, loss: 5.6480
2022-02-28 22:45:10 - train: epoch 0001, iter [01800, 05004], lr: 0.020000, loss: 5.6670
2022-02-28 22:45:41 - train: epoch 0001, iter [01900, 05004], lr: 0.020000, loss: 5.5948
2022-02-28 22:46:16 - train: epoch 0001, iter [02000, 05004], lr: 0.020000, loss: 5.5536
2022-02-28 22:46:51 - train: epoch 0001, iter [02100, 05004], lr: 0.020000, loss: 5.4698
2022-02-28 22:47:27 - train: epoch 0001, iter [02200, 05004], lr: 0.020000, loss: 5.3992
2022-02-28 22:48:02 - train: epoch 0001, iter [02300, 05004], lr: 0.020000, loss: 5.3717
2022-02-28 22:48:37 - train: epoch 0001, iter [02400, 05004], lr: 0.020000, loss: 5.3357
2022-02-28 22:49:12 - train: epoch 0001, iter [02500, 05004], lr: 0.020000, loss: 5.3566
2022-02-28 22:49:47 - train: epoch 0001, iter [02600, 05004], lr: 0.020000, loss: 5.4040
2022-02-28 22:50:22 - train: epoch 0001, iter [02700, 05004], lr: 0.020000, loss: 5.4633
2022-02-28 22:51:01 - train: epoch 0001, iter [02800, 05004], lr: 0.020000, loss: 5.2456
2022-02-28 22:52:08 - train: epoch 0001, iter [02900, 05004], lr: 0.020000, loss: 5.1529
2022-02-28 22:52:47 - train: epoch 0001, iter [03000, 05004], lr: 0.020000, loss: 5.2366
2022-02-28 22:53:25 - train: epoch 0001, iter [03100, 05004], lr: 0.020000, loss: 5.3125
2022-02-28 22:54:02 - train: epoch 0001, iter [03200, 05004], lr: 0.020000, loss: 5.1397
2022-02-28 22:54:38 - train: epoch 0001, iter [03300, 05004], lr: 0.020000, loss: 5.0706
2022-02-28 22:55:15 - train: epoch 0001, iter [03400, 05004], lr: 0.020000, loss: 5.0277
2022-02-28 22:55:51 - train: epoch 0001, iter [03500, 05004], lr: 0.020000, loss: 4.9750
2022-02-28 22:56:26 - train: epoch 0001, iter [03600, 05004], lr: 0.020000, loss: 5.0216
2022-02-28 22:57:07 - train: epoch 0001, iter [03700, 05004], lr: 0.020000, loss: 5.1791
2022-02-28 22:58:03 - train: epoch 0001, iter [03800, 05004], lr: 0.020000, loss: 4.9014
2022-02-28 22:58:42 - train: epoch 0001, iter [03900, 05004], lr: 0.020000, loss: 4.8816
2022-02-28 22:59:17 - train: epoch 0001, iter [04000, 05004], lr: 0.020000, loss: 4.8898
2022-02-28 22:59:54 - train: epoch 0001, iter [04100, 05004], lr: 0.020000, loss: 4.9543
2022-02-28 23:00:30 - train: epoch 0001, iter [04200, 05004], lr: 0.020000, loss: 4.8326
2022-02-28 23:01:05 - train: epoch 0001, iter [04300, 05004], lr: 0.020000, loss: 4.7619
2022-02-28 23:01:41 - train: epoch 0001, iter [04400, 05004], lr: 0.020000, loss: 4.5627
2022-02-28 23:02:15 - train: epoch 0001, iter [04500, 05004], lr: 0.020000, loss: 4.9180
2022-02-28 23:02:52 - train: epoch 0001, iter [04600, 05004], lr: 0.020000, loss: 4.9599
2022-02-28 23:03:32 - train: epoch 0001, iter [04700, 05004], lr: 0.020000, loss: 4.5434
2022-02-28 23:04:08 - train: epoch 0001, iter [04800, 05004], lr: 0.020000, loss: 4.8083
2022-02-28 23:04:44 - train: epoch 0001, iter [04900, 05004], lr: 0.020000, loss: 4.5937
2022-02-28 23:05:19 - train: epoch 0001, iter [05000, 05004], lr: 0.020000, loss: 4.5255
2022-02-28 23:05:20 - train: epoch 001, train_loss: 5.4972
2022-02-28 23:07:13 - eval: epoch: 001, acc1: 13.574%, acc5: 32.364%, test_loss: 4.5922, per_image_load_time: 0.664ms, per_image_inference_time: 0.340ms
2022-02-28 23:07:13 - until epoch: 001, best_acc1: 13.574%
2022-02-28 23:07:13 - epoch 002 lr: 0.04000000000000001
2022-02-28 23:08:53 - train: epoch 0002, iter [00100, 05004], lr: 0.040000, loss: 4.8877
2022-02-28 23:10:14 - train: epoch 0002, iter [00200, 05004], lr: 0.040000, loss: 4.7673
2022-02-28 23:10:55 - train: epoch 0002, iter [00300, 05004], lr: 0.040000, loss: 4.8917
2022-02-28 23:11:36 - train: epoch 0002, iter [00400, 05004], lr: 0.040000, loss: 4.7176
2022-02-28 23:12:21 - train: epoch 0002, iter [00500, 05004], lr: 0.040000, loss: 4.5552
2022-02-28 23:13:02 - train: epoch 0002, iter [00600, 05004], lr: 0.040000, loss: 4.6224
2022-02-28 23:13:47 - train: epoch 0002, iter [00700, 05004], lr: 0.040000, loss: 4.7495
2022-02-28 23:14:22 - train: epoch 0002, iter [00800, 05004], lr: 0.040000, loss: 4.3242
2022-02-28 23:15:00 - train: epoch 0002, iter [00900, 05004], lr: 0.040000, loss: 4.0584
2022-02-28 23:16:18 - train: epoch 0002, iter [01000, 05004], lr: 0.040000, loss: 4.5581
2022-02-28 23:16:53 - train: epoch 0002, iter [01100, 05004], lr: 0.040000, loss: 4.4629
2022-02-28 23:17:32 - train: epoch 0002, iter [01200, 05004], lr: 0.040000, loss: 4.3149
2022-02-28 23:18:11 - train: epoch 0002, iter [01300, 05004], lr: 0.040000, loss: 4.3558
2022-02-28 23:18:48 - train: epoch 0002, iter [01400, 05004], lr: 0.040000, loss: 4.4711
2022-02-28 23:19:29 - train: epoch 0002, iter [01500, 05004], lr: 0.040000, loss: 4.2160
2022-02-28 23:20:04 - train: epoch 0002, iter [01600, 05004], lr: 0.040000, loss: 4.1674
2022-02-28 23:20:50 - train: epoch 0002, iter [01700, 05004], lr: 0.040000, loss: 4.1567
2022-02-28 23:22:14 - train: epoch 0002, iter [01800, 05004], lr: 0.040000, loss: 4.3220
2022-02-28 23:23:46 - train: epoch 0002, iter [01900, 05004], lr: 0.040000, loss: 4.0730
2022-02-28 23:25:18 - train: epoch 0002, iter [02000, 05004], lr: 0.040000, loss: 3.8463
2022-02-28 23:26:27 - train: epoch 0002, iter [02100, 05004], lr: 0.040000, loss: 4.1243
2022-02-28 23:27:30 - train: epoch 0002, iter [02200, 05004], lr: 0.040000, loss: 3.8124
2022-02-28 23:28:02 - train: epoch 0002, iter [02300, 05004], lr: 0.040000, loss: 4.0837
2022-02-28 23:29:16 - train: epoch 0002, iter [02400, 05004], lr: 0.040000, loss: 3.8706
2022-02-28 23:30:22 - train: epoch 0002, iter [02500, 05004], lr: 0.040000, loss: 3.9713
2022-02-28 23:31:22 - train: epoch 0002, iter [02600, 05004], lr: 0.040000, loss: 3.9028
2022-02-28 23:32:12 - train: epoch 0002, iter [02700, 05004], lr: 0.040000, loss: 4.0488
2022-02-28 23:33:17 - train: epoch 0002, iter [02800, 05004], lr: 0.040000, loss: 3.9438
2022-02-28 23:34:04 - train: epoch 0002, iter [02900, 05004], lr: 0.040000, loss: 3.8681
2022-02-28 23:34:57 - train: epoch 0002, iter [03000, 05004], lr: 0.040000, loss: 3.7596
2022-02-28 23:35:38 - train: epoch 0002, iter [03100, 05004], lr: 0.040000, loss: 3.8472
2022-02-28 23:36:20 - train: epoch 0002, iter [03200, 05004], lr: 0.040000, loss: 3.7531
2022-02-28 23:37:05 - train: epoch 0002, iter [03300, 05004], lr: 0.040000, loss: 3.7906
2022-02-28 23:37:47 - train: epoch 0002, iter [03400, 05004], lr: 0.040000, loss: 4.0485
2022-02-28 23:38:26 - train: epoch 0002, iter [03500, 05004], lr: 0.040000, loss: 3.8301
2022-02-28 23:39:10 - train: epoch 0002, iter [03600, 05004], lr: 0.040000, loss: 3.7666
2022-02-28 23:40:05 - train: epoch 0002, iter [03700, 05004], lr: 0.040000, loss: 3.8361
2022-02-28 23:40:51 - train: epoch 0002, iter [03800, 05004], lr: 0.040000, loss: 3.5547
2022-02-28 23:41:29 - train: epoch 0002, iter [03900, 05004], lr: 0.040000, loss: 3.7364
2022-02-28 23:42:09 - train: epoch 0002, iter [04000, 05004], lr: 0.040000, loss: 3.5797
2022-02-28 23:42:43 - train: epoch 0002, iter [04100, 05004], lr: 0.040000, loss: 3.6899
2022-02-28 23:43:20 - train: epoch 0002, iter [04200, 05004], lr: 0.040000, loss: 3.6153
2022-02-28 23:43:56 - train: epoch 0002, iter [04300, 05004], lr: 0.040000, loss: 3.6088
2022-02-28 23:44:32 - train: epoch 0002, iter [04400, 05004], lr: 0.040000, loss: 3.4600
2022-02-28 23:45:10 - train: epoch 0002, iter [04500, 05004], lr: 0.040000, loss: 3.4913
2022-02-28 23:45:54 - train: epoch 0002, iter [04600, 05004], lr: 0.040000, loss: 3.5211
2022-02-28 23:46:36 - train: epoch 0002, iter [04700, 05004], lr: 0.040000, loss: 3.5663
2022-02-28 23:47:21 - train: epoch 0002, iter [04800, 05004], lr: 0.040000, loss: 3.6063
2022-02-28 23:48:08 - train: epoch 0002, iter [04900, 05004], lr: 0.040000, loss: 3.5497
2022-02-28 23:48:52 - train: epoch 0002, iter [05000, 05004], lr: 0.040000, loss: 3.4012
2022-02-28 23:48:53 - train: epoch 002, train_loss: 4.0479
2022-02-28 23:50:12 - eval: epoch: 002, acc1: 26.554%, acc5: 51.630%, test_loss: 3.6759, per_image_load_time: 1.140ms, per_image_inference_time: 0.406ms
2022-02-28 23:50:13 - until epoch: 002, best_acc1: 26.554%
2022-02-28 23:50:13 - epoch 003 lr: 0.06
2022-02-28 23:50:53 - train: epoch 0003, iter [00100, 05004], lr: 0.060000, loss: 3.7620
2022-02-28 23:51:29 - train: epoch 0003, iter [00200, 05004], lr: 0.060000, loss: 3.6814
2022-02-28 23:52:10 - train: epoch 0003, iter [00300, 05004], lr: 0.060000, loss: 3.6388
2022-02-28 23:52:43 - train: epoch 0003, iter [00400, 05004], lr: 0.060000, loss: 3.6200
2022-02-28 23:53:16 - train: epoch 0003, iter [00500, 05004], lr: 0.060000, loss: 3.8671
2022-02-28 23:53:51 - train: epoch 0003, iter [00600, 05004], lr: 0.060000, loss: 3.5015
2022-02-28 23:54:27 - train: epoch 0003, iter [00700, 05004], lr: 0.060000, loss: 3.8056
2022-02-28 23:55:03 - train: epoch 0003, iter [00800, 05004], lr: 0.060000, loss: 3.7675
2022-02-28 23:55:46 - train: epoch 0003, iter [00900, 05004], lr: 0.060000, loss: 3.4584
2022-02-28 23:56:27 - train: epoch 0003, iter [01000, 05004], lr: 0.060000, loss: 3.5222
2022-02-28 23:57:05 - train: epoch 0003, iter [01100, 05004], lr: 0.060000, loss: 3.3395
2022-02-28 23:57:42 - train: epoch 0003, iter [01200, 05004], lr: 0.060000, loss: 3.5063
2022-02-28 23:58:22 - train: epoch 0003, iter [01300, 05004], lr: 0.060000, loss: 3.5194
2022-02-28 23:58:53 - train: epoch 0003, iter [01400, 05004], lr: 0.060000, loss: 3.3774
2022-02-28 23:59:32 - train: epoch 0003, iter [01500, 05004], lr: 0.060000, loss: 3.7850
2022-03-01 00:00:13 - train: epoch 0003, iter [01600, 05004], lr: 0.060000, loss: 3.4409
2022-03-01 00:00:52 - train: epoch 0003, iter [01700, 05004], lr: 0.060000, loss: 3.3029
2022-03-01 00:01:32 - train: epoch 0003, iter [01800, 05004], lr: 0.060000, loss: 3.2415
2022-03-01 00:02:08 - train: epoch 0003, iter [01900, 05004], lr: 0.060000, loss: 3.5804
2022-03-01 00:02:44 - train: epoch 0003, iter [02000, 05004], lr: 0.060000, loss: 3.6786
2022-03-01 00:03:22 - train: epoch 0003, iter [02100, 05004], lr: 0.060000, loss: 3.6117
2022-03-01 00:04:02 - train: epoch 0003, iter [02200, 05004], lr: 0.060000, loss: 3.7723
2022-03-01 00:04:49 - train: epoch 0003, iter [02300, 05004], lr: 0.060000, loss: 3.3064
2022-03-01 00:06:03 - train: epoch 0003, iter [02400, 05004], lr: 0.060000, loss: 3.2703
2022-03-01 00:06:52 - train: epoch 0003, iter [02500, 05004], lr: 0.060000, loss: 3.3390
2022-03-01 00:07:41 - train: epoch 0003, iter [02600, 05004], lr: 0.060000, loss: 3.3777
2022-03-01 00:08:29 - train: epoch 0003, iter [02700, 05004], lr: 0.060000, loss: 3.6236
2022-03-01 00:09:19 - train: epoch 0003, iter [02800, 05004], lr: 0.060000, loss: 3.1870
2022-03-01 00:10:08 - train: epoch 0003, iter [02900, 05004], lr: 0.060000, loss: 3.3203
2022-03-01 00:10:55 - train: epoch 0003, iter [03000, 05004], lr: 0.060000, loss: 3.6210
2022-03-01 00:13:03 - train: epoch 0003, iter [03100, 05004], lr: 0.060000, loss: 3.5219
2022-03-01 00:14:56 - train: epoch 0003, iter [03200, 05004], lr: 0.060000, loss: 3.3074
2022-03-01 00:16:23 - train: epoch 0003, iter [03300, 05004], lr: 0.060000, loss: 3.2029
2022-03-01 00:18:43 - train: epoch 0003, iter [03400, 05004], lr: 0.060000, loss: 3.3664
2022-03-01 00:20:04 - train: epoch 0003, iter [03500, 05004], lr: 0.060000, loss: 3.0863
2022-03-01 00:20:36 - train: epoch 0003, iter [03600, 05004], lr: 0.060000, loss: 3.1692
2022-03-01 00:21:11 - train: epoch 0003, iter [03700, 05004], lr: 0.060000, loss: 3.2192
2022-03-01 00:21:45 - train: epoch 0003, iter [03800, 05004], lr: 0.060000, loss: 3.2726
2022-03-01 00:22:19 - train: epoch 0003, iter [03900, 05004], lr: 0.060000, loss: 3.4144
2022-03-01 00:22:51 - train: epoch 0003, iter [04000, 05004], lr: 0.060000, loss: 3.2106
2022-03-01 00:24:00 - train: epoch 0003, iter [04100, 05004], lr: 0.060000, loss: 3.1627
2022-03-01 00:24:55 - train: epoch 0003, iter [04200, 05004], lr: 0.060000, loss: 3.0829
2022-03-01 00:25:46 - train: epoch 0003, iter [04300, 05004], lr: 0.060000, loss: 2.8888
2022-03-01 00:26:24 - train: epoch 0003, iter [04400, 05004], lr: 0.060000, loss: 3.0656
2022-03-01 00:27:11 - train: epoch 0003, iter [04500, 05004], lr: 0.060000, loss: 3.1161
2022-03-01 00:27:52 - train: epoch 0003, iter [04600, 05004], lr: 0.060000, loss: 3.1080
2022-03-01 00:28:30 - train: epoch 0003, iter [04700, 05004], lr: 0.060000, loss: 3.0324
2022-03-01 00:29:07 - train: epoch 0003, iter [04800, 05004], lr: 0.060000, loss: 3.2533
2022-03-01 00:29:45 - train: epoch 0003, iter [04900, 05004], lr: 0.060000, loss: 3.1694
2022-03-01 00:30:20 - train: epoch 0003, iter [05000, 05004], lr: 0.060000, loss: 3.1178
2022-03-01 00:30:21 - train: epoch 003, train_loss: 3.3426
2022-03-01 00:31:37 - eval: epoch: 003, acc1: 35.812%, acc5: 61.900%, test_loss: 2.9623, per_image_load_time: 2.544ms, per_image_inference_time: 0.408ms
2022-03-01 00:31:38 - until epoch: 003, best_acc1: 35.812%
2022-03-01 00:31:38 - epoch 004 lr: 0.08000000000000002
2022-03-01 00:32:21 - train: epoch 0004, iter [00100, 05004], lr: 0.080000, loss: 3.1593
2022-03-01 00:32:58 - train: epoch 0004, iter [00200, 05004], lr: 0.080000, loss: 3.1385
2022-03-01 00:33:37 - train: epoch 0004, iter [00300, 05004], lr: 0.080000, loss: 3.2459
2022-03-01 00:34:13 - train: epoch 0004, iter [00400, 05004], lr: 0.080000, loss: 3.0949
2022-03-01 00:34:51 - train: epoch 0004, iter [00500, 05004], lr: 0.080000, loss: 2.9804
2022-03-01 00:35:26 - train: epoch 0004, iter [00600, 05004], lr: 0.080000, loss: 3.2668
2022-03-01 00:36:08 - train: epoch 0004, iter [00700, 05004], lr: 0.080000, loss: 3.2388
2022-03-01 00:36:46 - train: epoch 0004, iter [00800, 05004], lr: 0.080000, loss: 2.9470
2022-03-01 00:37:25 - train: epoch 0004, iter [00900, 05004], lr: 0.080000, loss: 2.8699
2022-03-01 00:38:07 - train: epoch 0004, iter [01000, 05004], lr: 0.080000, loss: 3.0541
2022-03-01 00:38:54 - train: epoch 0004, iter [01100, 05004], lr: 0.080000, loss: 3.1429
2022-03-01 00:39:33 - train: epoch 0004, iter [01200, 05004], lr: 0.080000, loss: 2.8964
2022-03-01 00:40:09 - train: epoch 0004, iter [01300, 05004], lr: 0.080000, loss: 2.8534
2022-03-01 00:40:48 - train: epoch 0004, iter [01400, 05004], lr: 0.080000, loss: 3.1143
2022-03-01 00:41:27 - train: epoch 0004, iter [01500, 05004], lr: 0.080000, loss: 3.1709
2022-03-01 00:42:08 - train: epoch 0004, iter [01600, 05004], lr: 0.080000, loss: 2.9749
2022-03-01 00:42:45 - train: epoch 0004, iter [01700, 05004], lr: 0.080000, loss: 3.0833
2022-03-01 00:43:41 - train: epoch 0004, iter [01800, 05004], lr: 0.080000, loss: 3.2542
2022-03-01 00:45:15 - train: epoch 0004, iter [01900, 05004], lr: 0.080000, loss: 3.1548
2022-03-01 00:46:24 - train: epoch 0004, iter [02000, 05004], lr: 0.080000, loss: 2.9012
2022-03-01 00:47:26 - train: epoch 0004, iter [02100, 05004], lr: 0.080000, loss: 3.1212
2022-03-01 00:48:25 - train: epoch 0004, iter [02200, 05004], lr: 0.080000, loss: 2.9747
2022-03-01 00:49:29 - train: epoch 0004, iter [02300, 05004], lr: 0.080000, loss: 2.9192
2022-03-01 00:50:31 - train: epoch 0004, iter [02400, 05004], lr: 0.080000, loss: 2.8532
2022-03-01 00:51:18 - train: epoch 0004, iter [02500, 05004], lr: 0.080000, loss: 2.9926
2022-03-01 00:52:04 - train: epoch 0004, iter [02600, 05004], lr: 0.080000, loss: 3.0402
2022-03-01 00:52:52 - train: epoch 0004, iter [02700, 05004], lr: 0.080000, loss: 2.7382
2022-03-01 00:53:33 - train: epoch 0004, iter [02800, 05004], lr: 0.080000, loss: 2.8058
2022-03-01 00:54:26 - train: epoch 0004, iter [02900, 05004], lr: 0.080000, loss: 2.8785
2022-03-01 00:55:01 - train: epoch 0004, iter [03000, 05004], lr: 0.080000, loss: 2.9148
2022-03-01 00:55:38 - train: epoch 0004, iter [03100, 05004], lr: 0.080000, loss: 2.9587
2022-03-01 00:56:21 - train: epoch 0004, iter [03200, 05004], lr: 0.080000, loss: 2.9186
2022-03-01 00:57:01 - train: epoch 0004, iter [03300, 05004], lr: 0.080000, loss: 3.0520
2022-03-01 00:57:44 - train: epoch 0004, iter [03400, 05004], lr: 0.080000, loss: 3.0069
2022-03-01 00:58:29 - train: epoch 0004, iter [03500, 05004], lr: 0.080000, loss: 2.8081
2022-03-01 00:59:19 - train: epoch 0004, iter [03600, 05004], lr: 0.080000, loss: 2.7470
2022-03-01 01:00:05 - train: epoch 0004, iter [03700, 05004], lr: 0.080000, loss: 2.8728
2022-03-01 01:00:48 - train: epoch 0004, iter [03800, 05004], lr: 0.080000, loss: 2.8956
2022-03-01 01:01:32 - train: epoch 0004, iter [03900, 05004], lr: 0.080000, loss: 2.8210
2022-03-01 01:02:30 - train: epoch 0004, iter [04000, 05004], lr: 0.080000, loss: 2.6294
2022-03-01 01:03:09 - train: epoch 0004, iter [04100, 05004], lr: 0.080000, loss: 2.8590
2022-03-01 01:03:47 - train: epoch 0004, iter [04200, 05004], lr: 0.080000, loss: 2.7320
2022-03-01 01:04:32 - train: epoch 0004, iter [04300, 05004], lr: 0.080000, loss: 2.6472
2022-03-01 01:05:18 - train: epoch 0004, iter [04400, 05004], lr: 0.080000, loss: 2.7458
2022-03-01 01:06:08 - train: epoch 0004, iter [04500, 05004], lr: 0.080000, loss: 2.3386
2022-03-01 01:06:57 - train: epoch 0004, iter [04600, 05004], lr: 0.080000, loss: 2.7948
2022-03-01 01:07:58 - train: epoch 0004, iter [04700, 05004], lr: 0.080000, loss: 2.7270
2022-03-01 01:08:38 - train: epoch 0004, iter [04800, 05004], lr: 0.080000, loss: 2.7107
2022-03-01 01:09:28 - train: epoch 0004, iter [04900, 05004], lr: 0.080000, loss: 2.8532
2022-03-01 01:10:07 - train: epoch 0004, iter [05000, 05004], lr: 0.080000, loss: 2.8971
2022-03-01 01:10:08 - train: epoch 004, train_loss: 2.9876
2022-03-01 01:11:27 - eval: epoch: 004, acc1: 39.540%, acc5: 66.712%, test_loss: 2.7142, per_image_load_time: 2.662ms, per_image_inference_time: 0.407ms
2022-03-01 01:11:28 - until epoch: 004, best_acc1: 39.540%
2022-03-01 01:11:28 - epoch 005 lr: 0.1
2022-03-01 01:12:16 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.9513
2022-03-01 01:12:59 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.9782
2022-03-01 01:13:45 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.0266
2022-03-01 01:14:17 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.9556
2022-03-01 01:14:57 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.6987
2022-03-01 01:15:37 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.8510
2022-03-01 01:16:17 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.9566
2022-03-01 01:16:54 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.1746
2022-03-01 01:17:32 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.9420
2022-03-01 01:18:07 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.0279
2022-03-01 01:18:41 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.9071
2022-03-01 01:19:15 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.9849
2022-03-01 01:19:50 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.8620
2022-03-01 01:20:23 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.9317
2022-03-01 01:20:54 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.7048
2022-03-01 01:21:29 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.6420
2022-03-01 01:22:04 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.8343
2022-03-01 01:22:43 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.8092
2022-03-01 01:23:18 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.7039
2022-03-01 01:24:01 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.9082
2022-03-01 01:24:38 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.6518
2022-03-01 01:25:13 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.6684
2022-03-01 01:25:50 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.6561
2022-03-01 01:26:40 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.7213
2022-03-01 01:27:24 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.9008
2022-03-01 01:28:18 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.9270
2022-03-01 01:29:01 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.9102
2022-03-01 01:29:42 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.8275
2022-03-01 01:30:23 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.7242
2022-03-01 01:31:07 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.6988
2022-03-01 01:31:46 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.7808
2022-03-01 01:32:24 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.9189
2022-03-01 01:33:24 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.6157
2022-03-01 01:34:24 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.7550
2022-03-01 01:34:57 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.8004
2022-03-01 01:35:30 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.8943
2022-03-01 01:36:02 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.7493
2022-03-01 01:36:35 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.5558
2022-03-01 01:37:07 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.9432
2022-03-01 01:37:40 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.7474
2022-03-01 01:38:13 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.7195
2022-03-01 01:38:46 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.6890
2022-03-01 01:39:18 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.6166
2022-03-01 01:39:59 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.7329
2022-03-01 01:40:31 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.8109
2022-03-01 01:41:04 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.6469
2022-03-01 01:41:37 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.5472
2022-03-01 01:42:18 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.5805
2022-03-01 01:42:57 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.8800
2022-03-01 01:43:28 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.6273
2022-03-01 01:43:29 - train: epoch 005, train_loss: 2.8193
2022-03-01 01:44:43 - eval: epoch: 005, acc1: 38.914%, acc5: 66.094%, test_loss: 2.7420, per_image_load_time: 1.795ms, per_image_inference_time: 0.470ms
2022-03-01 01:44:43 - until epoch: 005, best_acc1: 39.540%
2022-03-01 01:44:43 - epoch 006 lr: 0.1
2022-03-01 01:45:21 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.6379
2022-03-01 01:45:53 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.7671
2022-03-01 01:46:27 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.5562
2022-03-01 01:46:59 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.9104
2022-03-01 01:47:33 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.6826
2022-03-01 01:48:05 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.7955
2022-03-01 01:48:39 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.7155
2022-03-01 01:49:11 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.6861
2022-03-01 01:49:45 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.6058
2022-03-01 01:50:17 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.5908
2022-03-01 01:51:58 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.5571
2022-03-01 01:52:31 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.7889
2022-03-01 01:53:04 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.8830
2022-03-01 01:53:37 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.7440
2022-03-01 01:54:10 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.7930
2022-03-01 01:54:44 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.4976
2022-03-01 01:55:17 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.8618
2022-03-01 01:56:01 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.7647
2022-03-01 01:58:30 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.5960
2022-03-01 01:59:32 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.7983
2022-03-01 02:00:16 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.7069
2022-03-01 02:03:32 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.5784
2022-03-01 02:08:44 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.4647
2022-03-01 02:13:36 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.6306
2022-03-01 02:17:59 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.9617
2022-03-01 02:22:12 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.5480
2022-03-01 02:24:53 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.7658
2022-03-01 02:26:28 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.4835
2022-03-01 02:28:45 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.7475
2022-03-01 02:31:36 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.6199
2022-03-01 02:35:38 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.4311
2022-03-01 02:37:54 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.5378
2022-03-01 02:40:50 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.4772
2022-03-01 02:42:25 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.8687
2022-03-01 02:43:53 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.6925
2022-03-01 02:47:06 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.5988
2022-03-01 02:51:56 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.6391
2022-03-01 02:53:54 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.5097
2022-03-01 02:57:18 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.5290
2022-03-01 03:08:18 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.8630
2022-03-01 03:25:00 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.5922
2022-03-01 03:44:23 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.4971
2022-03-01 04:01:04 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.6787
2022-03-01 04:18:23 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.6734
2022-03-01 04:35:24 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.6692
2022-03-01 04:51:42 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.6867
2022-03-01 04:59:35 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.5698
2022-03-01 05:13:34 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.6806
2022-03-01 05:31:58 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.6819
2022-03-01 05:45:36 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.5029
2022-03-01 05:45:37 - train: epoch 006, train_loss: 2.6546
2022-03-01 05:46:50 - eval: epoch: 006, acc1: 45.116%, acc5: 71.796%, test_loss: 2.4191, per_image_load_time: 2.326ms, per_image_inference_time: 0.505ms
2022-03-01 05:46:51 - until epoch: 006, best_acc1: 45.116%
2022-03-01 05:46:51 - epoch 007 lr: 0.09999351124856874
2022-03-01 05:47:28 - train: epoch 0007, iter [00100, 05004], lr: 0.099994, loss: 2.4904
2022-03-01 05:48:01 - train: epoch 0007, iter [00200, 05004], lr: 0.099994, loss: 2.7298
2022-03-01 05:48:34 - train: epoch 0007, iter [00300, 05004], lr: 0.099994, loss: 2.8151
2022-03-01 05:49:08 - train: epoch 0007, iter [00400, 05004], lr: 0.099994, loss: 2.6467
2022-03-01 05:49:40 - train: epoch 0007, iter [00500, 05004], lr: 0.099994, loss: 2.5483
2022-03-01 05:50:13 - train: epoch 0007, iter [00600, 05004], lr: 0.099994, loss: 2.7674
2022-03-01 05:50:46 - train: epoch 0007, iter [00700, 05004], lr: 0.099994, loss: 2.6175
2022-03-01 05:51:20 - train: epoch 0007, iter [00800, 05004], lr: 0.099994, loss: 2.6599
2022-03-01 05:51:53 - train: epoch 0007, iter [00900, 05004], lr: 0.099994, loss: 2.6342
2022-03-01 05:52:26 - train: epoch 0007, iter [01000, 05004], lr: 0.099994, loss: 2.5353
2022-03-01 05:52:58 - train: epoch 0007, iter [01100, 05004], lr: 0.099994, loss: 2.3799
2022-03-01 05:53:32 - train: epoch 0007, iter [01200, 05004], lr: 0.099994, loss: 2.5627
2022-03-01 05:54:05 - train: epoch 0007, iter [01300, 05004], lr: 0.099994, loss: 2.4812
2022-03-01 05:54:39 - train: epoch 0007, iter [01400, 05004], lr: 0.099994, loss: 2.6452
2022-03-01 05:55:13 - train: epoch 0007, iter [01500, 05004], lr: 0.099994, loss: 2.7081
2022-03-01 05:55:46 - train: epoch 0007, iter [01600, 05004], lr: 0.099994, loss: 2.5844
2022-03-01 05:56:19 - train: epoch 0007, iter [01700, 05004], lr: 0.099994, loss: 2.5148
2022-03-01 05:56:52 - train: epoch 0007, iter [01800, 05004], lr: 0.099994, loss: 2.4265
2022-03-01 05:57:26 - train: epoch 0007, iter [01900, 05004], lr: 0.099994, loss: 2.6352
2022-03-01 05:57:59 - train: epoch 0007, iter [02000, 05004], lr: 0.099994, loss: 2.3733
2022-03-01 05:58:33 - train: epoch 0007, iter [02100, 05004], lr: 0.099994, loss: 2.7343
2022-03-01 05:59:06 - train: epoch 0007, iter [02200, 05004], lr: 0.099994, loss: 2.4488
2022-03-01 05:59:39 - train: epoch 0007, iter [02300, 05004], lr: 0.099994, loss: 2.5381
2022-03-01 06:00:12 - train: epoch 0007, iter [02400, 05004], lr: 0.099994, loss: 2.5772
2022-03-01 06:00:46 - train: epoch 0007, iter [02500, 05004], lr: 0.099994, loss: 2.4755
2022-03-01 06:01:19 - train: epoch 0007, iter [02600, 05004], lr: 0.099994, loss: 2.5628
2022-03-01 06:01:53 - train: epoch 0007, iter [02700, 05004], lr: 0.099994, loss: 2.3774
2022-03-01 06:02:26 - train: epoch 0007, iter [02800, 05004], lr: 0.099994, loss: 2.5079
2022-03-01 06:02:58 - train: epoch 0007, iter [02900, 05004], lr: 0.099994, loss: 2.4491
2022-03-01 06:03:32 - train: epoch 0007, iter [03000, 05004], lr: 0.099994, loss: 2.6248
2022-03-01 06:04:05 - train: epoch 0007, iter [03100, 05004], lr: 0.099994, loss: 2.4327
2022-03-01 06:04:39 - train: epoch 0007, iter [03200, 05004], lr: 0.099994, loss: 2.3836
2022-03-01 06:05:12 - train: epoch 0007, iter [03300, 05004], lr: 0.099994, loss: 2.7673
2022-03-01 06:05:46 - train: epoch 0007, iter [03400, 05004], lr: 0.099994, loss: 2.4260
2022-03-01 06:06:20 - train: epoch 0007, iter [03500, 05004], lr: 0.099994, loss: 2.5739
2022-03-01 06:06:53 - train: epoch 0007, iter [03600, 05004], lr: 0.099994, loss: 2.3666
2022-03-01 06:07:26 - train: epoch 0007, iter [03700, 05004], lr: 0.099994, loss: 2.4725
2022-03-01 06:07:58 - train: epoch 0007, iter [03800, 05004], lr: 0.099994, loss: 2.7064
2022-03-01 06:08:33 - train: epoch 0007, iter [03900, 05004], lr: 0.099994, loss: 2.4464
2022-03-01 06:09:06 - train: epoch 0007, iter [04000, 05004], lr: 0.099994, loss: 2.6399
2022-03-01 06:09:40 - train: epoch 0007, iter [04100, 05004], lr: 0.099994, loss: 2.5156
2022-03-01 06:10:13 - train: epoch 0007, iter [04200, 05004], lr: 0.099994, loss: 2.3731
2022-03-01 06:10:46 - train: epoch 0007, iter [04300, 05004], lr: 0.099994, loss: 2.5769
2022-03-01 06:11:20 - train: epoch 0007, iter [04400, 05004], lr: 0.099994, loss: 2.4072
2022-03-01 06:11:53 - train: epoch 0007, iter [04500, 05004], lr: 0.099994, loss: 2.6913
2022-03-01 06:12:26 - train: epoch 0007, iter [04600, 05004], lr: 0.099994, loss: 2.5505
2022-03-01 06:13:00 - train: epoch 0007, iter [04700, 05004], lr: 0.099994, loss: 2.6201
2022-03-01 06:13:33 - train: epoch 0007, iter [04800, 05004], lr: 0.099994, loss: 2.6987
2022-03-01 06:14:07 - train: epoch 0007, iter [04900, 05004], lr: 0.099994, loss: 2.5156
2022-03-01 06:14:39 - train: epoch 0007, iter [05000, 05004], lr: 0.099994, loss: 2.4974
2022-03-01 06:14:40 - train: epoch 007, train_loss: 2.5499
2022-03-01 06:15:54 - eval: epoch: 007, acc1: 44.730%, acc5: 71.814%, test_loss: 2.4137, per_image_load_time: 2.288ms, per_image_inference_time: 0.541ms
2022-03-01 06:15:54 - until epoch: 007, best_acc1: 45.116%
2022-03-01 06:15:54 - epoch 008 lr: 0.09997404667843075
2022-03-01 06:16:32 - train: epoch 0008, iter [00100, 05004], lr: 0.099974, loss: 2.4347
2022-03-01 06:17:06 - train: epoch 0008, iter [00200, 05004], lr: 0.099974, loss: 2.5840
2022-03-01 06:17:38 - train: epoch 0008, iter [00300, 05004], lr: 0.099974, loss: 2.2802
2022-03-01 06:18:12 - train: epoch 0008, iter [00400, 05004], lr: 0.099974, loss: 2.3084
2022-03-01 06:18:44 - train: epoch 0008, iter [00500, 05004], lr: 0.099974, loss: 2.3855
2022-03-01 06:19:18 - train: epoch 0008, iter [00600, 05004], lr: 0.099974, loss: 2.4284
2022-03-01 06:19:51 - train: epoch 0008, iter [00700, 05004], lr: 0.099974, loss: 2.7884
2022-03-01 06:20:24 - train: epoch 0008, iter [00800, 05004], lr: 0.099974, loss: 2.2748
2022-03-01 06:20:58 - train: epoch 0008, iter [00900, 05004], lr: 0.099974, loss: 2.4825
2022-03-01 06:21:31 - train: epoch 0008, iter [01000, 05004], lr: 0.099974, loss: 2.4874
2022-03-01 06:22:04 - train: epoch 0008, iter [01100, 05004], lr: 0.099974, loss: 2.2818
2022-03-01 06:22:37 - train: epoch 0008, iter [01200, 05004], lr: 0.099974, loss: 2.4058
2022-03-01 06:23:10 - train: epoch 0008, iter [01300, 05004], lr: 0.099974, loss: 2.4995
2022-03-01 06:23:44 - train: epoch 0008, iter [01400, 05004], lr: 0.099974, loss: 2.4438
2022-03-01 06:24:17 - train: epoch 0008, iter [01500, 05004], lr: 0.099974, loss: 2.5461
2022-03-01 06:24:49 - train: epoch 0008, iter [01600, 05004], lr: 0.099974, loss: 2.5306
2022-03-01 06:25:23 - train: epoch 0008, iter [01700, 05004], lr: 0.099974, loss: 2.3379
2022-03-01 06:25:56 - train: epoch 0008, iter [01800, 05004], lr: 0.099974, loss: 2.6148
2022-03-01 06:26:29 - train: epoch 0008, iter [01900, 05004], lr: 0.099974, loss: 2.2093
2022-03-01 06:27:03 - train: epoch 0008, iter [02000, 05004], lr: 0.099974, loss: 2.5547
2022-03-01 06:27:35 - train: epoch 0008, iter [02100, 05004], lr: 0.099974, loss: 2.4204
2022-03-01 06:28:09 - train: epoch 0008, iter [02200, 05004], lr: 0.099974, loss: 2.3468
2022-03-01 06:28:42 - train: epoch 0008, iter [02300, 05004], lr: 0.099974, loss: 2.5918
2022-03-01 06:29:16 - train: epoch 0008, iter [02400, 05004], lr: 0.099974, loss: 2.3942
2022-03-01 06:29:49 - train: epoch 0008, iter [02500, 05004], lr: 0.099974, loss: 2.5345
2022-03-01 06:30:22 - train: epoch 0008, iter [02600, 05004], lr: 0.099974, loss: 2.5949
2022-03-01 06:30:55 - train: epoch 0008, iter [02700, 05004], lr: 0.099974, loss: 2.5651
2022-03-01 06:31:29 - train: epoch 0008, iter [02800, 05004], lr: 0.099974, loss: 2.5273
2022-03-01 06:32:01 - train: epoch 0008, iter [02900, 05004], lr: 0.099974, loss: 2.5250
2022-03-01 06:32:35 - train: epoch 0008, iter [03000, 05004], lr: 0.099974, loss: 2.5724
2022-03-01 06:33:08 - train: epoch 0008, iter [03100, 05004], lr: 0.099974, loss: 2.4277
2022-03-01 06:33:42 - train: epoch 0008, iter [03200, 05004], lr: 0.099974, loss: 2.7322
2022-03-01 06:34:15 - train: epoch 0008, iter [03300, 05004], lr: 0.099974, loss: 2.6734
2022-03-01 06:34:47 - train: epoch 0008, iter [03400, 05004], lr: 0.099974, loss: 2.6383
2022-03-01 06:35:21 - train: epoch 0008, iter [03500, 05004], lr: 0.099974, loss: 2.3600
2022-03-01 06:35:54 - train: epoch 0008, iter [03600, 05004], lr: 0.099974, loss: 2.5795
2022-03-01 06:36:27 - train: epoch 0008, iter [03700, 05004], lr: 0.099974, loss: 2.4058
2022-03-01 06:37:00 - train: epoch 0008, iter [03800, 05004], lr: 0.099974, loss: 2.3896
2022-03-01 06:37:34 - train: epoch 0008, iter [03900, 05004], lr: 0.099974, loss: 2.6085
2022-03-01 06:38:06 - train: epoch 0008, iter [04000, 05004], lr: 0.099974, loss: 2.7314
2022-03-01 06:38:39 - train: epoch 0008, iter [04100, 05004], lr: 0.099974, loss: 2.4653
2022-03-01 06:39:12 - train: epoch 0008, iter [04200, 05004], lr: 0.099974, loss: 2.3907
2022-03-01 06:39:45 - train: epoch 0008, iter [04300, 05004], lr: 0.099974, loss: 2.2636
2022-03-01 06:40:19 - train: epoch 0008, iter [04400, 05004], lr: 0.099974, loss: 2.3632
2022-03-01 06:40:52 - train: epoch 0008, iter [04500, 05004], lr: 0.099974, loss: 2.6379
2022-03-01 06:41:24 - train: epoch 0008, iter [04600, 05004], lr: 0.099974, loss: 2.5993
2022-03-01 06:41:57 - train: epoch 0008, iter [04700, 05004], lr: 0.099974, loss: 2.3189
2022-03-01 06:42:29 - train: epoch 0008, iter [04800, 05004], lr: 0.099974, loss: 2.5050
2022-03-01 06:43:02 - train: epoch 0008, iter [04900, 05004], lr: 0.099974, loss: 2.5566
2022-03-01 06:43:35 - train: epoch 0008, iter [05000, 05004], lr: 0.099974, loss: 2.4181
2022-03-01 06:43:36 - train: epoch 008, train_loss: 2.4774
2022-03-01 06:44:50 - eval: epoch: 008, acc1: 49.636%, acc5: 75.818%, test_loss: 2.1687, per_image_load_time: 1.443ms, per_image_inference_time: 0.529ms
2022-03-01 06:44:51 - until epoch: 008, best_acc1: 49.636%
2022-03-01 06:44:51 - epoch 009 lr: 0.09994161134161633
2022-03-01 06:45:29 - train: epoch 0009, iter [00100, 05004], lr: 0.099942, loss: 2.1346
2022-03-01 06:46:02 - train: epoch 0009, iter [00200, 05004], lr: 0.099942, loss: 2.2237
2022-03-01 06:46:34 - train: epoch 0009, iter [00300, 05004], lr: 0.099942, loss: 2.2328
2022-03-01 06:47:08 - train: epoch 0009, iter [00400, 05004], lr: 0.099942, loss: 2.5607
2022-03-01 06:47:41 - train: epoch 0009, iter [00500, 05004], lr: 0.099942, loss: 2.4088
2022-03-01 06:48:15 - train: epoch 0009, iter [00600, 05004], lr: 0.099942, loss: 2.4025
2022-03-01 06:48:48 - train: epoch 0009, iter [00700, 05004], lr: 0.099942, loss: 2.3462
2022-03-01 06:49:21 - train: epoch 0009, iter [00800, 05004], lr: 0.099942, loss: 2.3139
2022-03-01 06:49:55 - train: epoch 0009, iter [00900, 05004], lr: 0.099942, loss: 2.3477
2022-03-01 06:50:27 - train: epoch 0009, iter [01000, 05004], lr: 0.099942, loss: 2.2996
2022-03-01 06:51:01 - train: epoch 0009, iter [01100, 05004], lr: 0.099942, loss: 2.7172
2022-03-01 06:51:34 - train: epoch 0009, iter [01200, 05004], lr: 0.099942, loss: 2.6014
2022-03-01 06:52:08 - train: epoch 0009, iter [01300, 05004], lr: 0.099942, loss: 2.6604
2022-03-01 06:52:41 - train: epoch 0009, iter [01400, 05004], lr: 0.099942, loss: 2.0906
2022-03-01 06:53:15 - train: epoch 0009, iter [01500, 05004], lr: 0.099942, loss: 2.3263
2022-03-01 06:53:48 - train: epoch 0009, iter [01600, 05004], lr: 0.099942, loss: 2.4245
2022-03-01 06:54:21 - train: epoch 0009, iter [01700, 05004], lr: 0.099942, loss: 2.5818
2022-03-01 06:54:54 - train: epoch 0009, iter [01800, 05004], lr: 0.099942, loss: 2.3392
2022-03-01 06:55:27 - train: epoch 0009, iter [01900, 05004], lr: 0.099942, loss: 2.2398
2022-03-01 06:56:00 - train: epoch 0009, iter [02000, 05004], lr: 0.099942, loss: 2.1646
2022-03-01 06:56:33 - train: epoch 0009, iter [02100, 05004], lr: 0.099942, loss: 2.5048
2022-03-01 06:57:07 - train: epoch 0009, iter [02200, 05004], lr: 0.099942, loss: 2.4590
2022-03-01 06:57:40 - train: epoch 0009, iter [02300, 05004], lr: 0.099942, loss: 2.2221
2022-03-01 06:58:13 - train: epoch 0009, iter [02400, 05004], lr: 0.099942, loss: 2.3656
2022-03-01 06:58:48 - train: epoch 0009, iter [02500, 05004], lr: 0.099942, loss: 2.2585
2022-03-01 06:59:20 - train: epoch 0009, iter [02600, 05004], lr: 0.099942, loss: 2.4674
2022-03-01 06:59:54 - train: epoch 0009, iter [02700, 05004], lr: 0.099942, loss: 2.3825
2022-03-01 07:00:27 - train: epoch 0009, iter [02800, 05004], lr: 0.099942, loss: 2.4178
2022-03-01 07:01:00 - train: epoch 0009, iter [02900, 05004], lr: 0.099942, loss: 2.1814
2022-03-01 07:01:33 - train: epoch 0009, iter [03000, 05004], lr: 0.099942, loss: 2.2911
2022-03-01 07:02:06 - train: epoch 0009, iter [03100, 05004], lr: 0.099942, loss: 2.4573
2022-03-01 07:02:40 - train: epoch 0009, iter [03200, 05004], lr: 0.099942, loss: 2.4449
2022-03-01 07:03:13 - train: epoch 0009, iter [03300, 05004], lr: 0.099942, loss: 2.3600
2022-03-01 07:03:46 - train: epoch 0009, iter [03400, 05004], lr: 0.099942, loss: 2.5700
2022-03-01 07:04:19 - train: epoch 0009, iter [03500, 05004], lr: 0.099942, loss: 2.5140
2022-03-01 07:04:52 - train: epoch 0009, iter [03600, 05004], lr: 0.099942, loss: 2.2901
2022-03-01 07:05:26 - train: epoch 0009, iter [03700, 05004], lr: 0.099942, loss: 2.5971
2022-03-01 07:05:59 - train: epoch 0009, iter [03800, 05004], lr: 0.099942, loss: 2.5832
2022-03-01 07:06:32 - train: epoch 0009, iter [03900, 05004], lr: 0.099942, loss: 2.0685
2022-03-01 07:07:05 - train: epoch 0009, iter [04000, 05004], lr: 0.099942, loss: 2.6422
2022-03-01 07:07:38 - train: epoch 0009, iter [04100, 05004], lr: 0.099942, loss: 2.5114
2022-03-01 07:08:11 - train: epoch 0009, iter [04200, 05004], lr: 0.099942, loss: 2.3476
2022-03-01 07:08:44 - train: epoch 0009, iter [04300, 05004], lr: 0.099942, loss: 2.5835
2022-03-01 07:09:17 - train: epoch 0009, iter [04400, 05004], lr: 0.099942, loss: 2.3213
2022-03-01 07:09:50 - train: epoch 0009, iter [04500, 05004], lr: 0.099942, loss: 2.3319
2022-03-01 07:10:23 - train: epoch 0009, iter [04600, 05004], lr: 0.099942, loss: 2.5013
2022-03-01 07:10:57 - train: epoch 0009, iter [04700, 05004], lr: 0.099942, loss: 2.5610
2022-03-01 07:11:30 - train: epoch 0009, iter [04800, 05004], lr: 0.099942, loss: 2.5808
2022-03-01 07:12:03 - train: epoch 0009, iter [04900, 05004], lr: 0.099942, loss: 2.4219
2022-03-01 07:12:35 - train: epoch 0009, iter [05000, 05004], lr: 0.099942, loss: 2.3257
2022-03-01 07:12:36 - train: epoch 009, train_loss: 2.4247
2022-03-01 07:13:50 - eval: epoch: 009, acc1: 50.814%, acc5: 76.558%, test_loss: 2.1246, per_image_load_time: 2.333ms, per_image_inference_time: 0.516ms
2022-03-01 07:13:51 - until epoch: 009, best_acc1: 50.814%
2022-03-01 07:13:51 - epoch 010 lr: 0.09989621365671902
2022-03-01 07:14:29 - train: epoch 0010, iter [00100, 05004], lr: 0.099896, loss: 2.2560
2022-03-01 07:15:01 - train: epoch 0010, iter [00200, 05004], lr: 0.099896, loss: 2.5106
2022-03-01 07:15:35 - train: epoch 0010, iter [00300, 05004], lr: 0.099896, loss: 2.2740
2022-03-01 07:16:08 - train: epoch 0010, iter [00400, 05004], lr: 0.099896, loss: 2.4766
2022-03-01 07:16:41 - train: epoch 0010, iter [00500, 05004], lr: 0.099896, loss: 2.3776
2022-03-01 07:17:14 - train: epoch 0010, iter [00600, 05004], lr: 0.099896, loss: 2.4047
2022-03-01 07:17:47 - train: epoch 0010, iter [00700, 05004], lr: 0.099896, loss: 2.4479
2022-03-01 07:18:20 - train: epoch 0010, iter [00800, 05004], lr: 0.099896, loss: 2.2032
2022-03-01 07:18:53 - train: epoch 0010, iter [00900, 05004], lr: 0.099896, loss: 2.2501
2022-03-01 07:19:26 - train: epoch 0010, iter [01000, 05004], lr: 0.099896, loss: 2.2482
2022-03-01 07:19:59 - train: epoch 0010, iter [01100, 05004], lr: 0.099896, loss: 2.3650
2022-03-01 07:20:32 - train: epoch 0010, iter [01200, 05004], lr: 0.099896, loss: 2.1896
2022-03-01 07:21:06 - train: epoch 0010, iter [01300, 05004], lr: 0.099896, loss: 2.2422
2022-03-01 07:21:39 - train: epoch 0010, iter [01400, 05004], lr: 0.099896, loss: 2.3945
2022-03-01 07:22:12 - train: epoch 0010, iter [01500, 05004], lr: 0.099896, loss: 2.1139
2022-03-01 07:22:44 - train: epoch 0010, iter [01600, 05004], lr: 0.099896, loss: 2.4187
2022-03-01 07:23:18 - train: epoch 0010, iter [01700, 05004], lr: 0.099896, loss: 2.4377
2022-03-01 07:23:50 - train: epoch 0010, iter [01800, 05004], lr: 0.099896, loss: 2.3902
2022-03-01 07:24:24 - train: epoch 0010, iter [01900, 05004], lr: 0.099896, loss: 2.3071
2022-03-01 07:24:57 - train: epoch 0010, iter [02000, 05004], lr: 0.099896, loss: 2.4577
2022-03-01 07:25:31 - train: epoch 0010, iter [02100, 05004], lr: 0.099896, loss: 2.2170
2022-03-01 07:26:04 - train: epoch 0010, iter [02200, 05004], lr: 0.099896, loss: 2.6359
2022-03-01 07:26:36 - train: epoch 0010, iter [02300, 05004], lr: 0.099896, loss: 2.6247
2022-03-01 07:27:10 - train: epoch 0010, iter [02400, 05004], lr: 0.099896, loss: 2.5307
2022-03-01 07:27:43 - train: epoch 0010, iter [02500, 05004], lr: 0.099896, loss: 2.3795
2022-03-01 07:28:16 - train: epoch 0010, iter [02600, 05004], lr: 0.099896, loss: 2.4556
2022-03-01 07:28:49 - train: epoch 0010, iter [02700, 05004], lr: 0.099896, loss: 2.1295
2022-03-01 07:29:21 - train: epoch 0010, iter [02800, 05004], lr: 0.099896, loss: 2.3739
2022-03-01 07:29:55 - train: epoch 0010, iter [02900, 05004], lr: 0.099896, loss: 2.5396
2022-03-01 07:30:28 - train: epoch 0010, iter [03000, 05004], lr: 0.099896, loss: 2.3482
2022-03-01 07:31:01 - train: epoch 0010, iter [03100, 05004], lr: 0.099896, loss: 2.5502
2022-03-01 07:31:34 - train: epoch 0010, iter [03200, 05004], lr: 0.099896, loss: 2.3487
2022-03-01 07:32:07 - train: epoch 0010, iter [03300, 05004], lr: 0.099896, loss: 2.5565
2022-03-01 07:32:41 - train: epoch 0010, iter [03400, 05004], lr: 0.099896, loss: 2.6267
2022-03-01 07:33:13 - train: epoch 0010, iter [03500, 05004], lr: 0.099896, loss: 2.6667
2022-03-01 07:33:46 - train: epoch 0010, iter [03600, 05004], lr: 0.099896, loss: 2.5886
2022-03-01 07:34:19 - train: epoch 0010, iter [03700, 05004], lr: 0.099896, loss: 2.2472
2022-03-01 07:34:52 - train: epoch 0010, iter [03800, 05004], lr: 0.099896, loss: 2.5529
2022-03-01 07:35:25 - train: epoch 0010, iter [03900, 05004], lr: 0.099896, loss: 2.0035
2022-03-01 07:35:58 - train: epoch 0010, iter [04000, 05004], lr: 0.099896, loss: 2.3655
2022-03-01 07:36:31 - train: epoch 0010, iter [04100, 05004], lr: 0.099896, loss: 2.2187
2022-03-01 07:37:05 - train: epoch 0010, iter [04200, 05004], lr: 0.099896, loss: 2.4887
2022-03-01 07:37:38 - train: epoch 0010, iter [04300, 05004], lr: 0.099896, loss: 2.3462
2022-03-01 07:38:11 - train: epoch 0010, iter [04400, 05004], lr: 0.099896, loss: 2.3806
2022-03-01 07:38:44 - train: epoch 0010, iter [04500, 05004], lr: 0.099896, loss: 2.2596
2022-03-01 07:39:17 - train: epoch 0010, iter [04600, 05004], lr: 0.099896, loss: 2.3920
2022-03-01 07:39:50 - train: epoch 0010, iter [04700, 05004], lr: 0.099896, loss: 2.3665
2022-03-01 07:40:23 - train: epoch 0010, iter [04800, 05004], lr: 0.099896, loss: 2.2275
2022-03-01 07:40:55 - train: epoch 0010, iter [04900, 05004], lr: 0.099896, loss: 2.2566
2022-03-01 07:41:27 - train: epoch 0010, iter [05000, 05004], lr: 0.099896, loss: 2.2069
2022-03-01 07:41:28 - train: epoch 010, train_loss: 2.3810
2022-03-01 07:42:41 - eval: epoch: 010, acc1: 48.524%, acc5: 74.850%, test_loss: 2.2324, per_image_load_time: 1.339ms, per_image_inference_time: 0.517ms
2022-03-01 07:42:42 - until epoch: 010, best_acc1: 50.814%
2022-03-01 07:42:42 - epoch 011 lr: 0.09983786540671051
2022-03-01 07:43:21 - train: epoch 0011, iter [00100, 05004], lr: 0.099838, loss: 2.2077
2022-03-01 07:43:53 - train: epoch 0011, iter [00200, 05004], lr: 0.099838, loss: 2.4711
2022-03-01 07:44:26 - train: epoch 0011, iter [00300, 05004], lr: 0.099838, loss: 2.1365
2022-03-01 07:44:59 - train: epoch 0011, iter [00400, 05004], lr: 0.099838, loss: 2.4048
2022-03-01 07:45:32 - train: epoch 0011, iter [00500, 05004], lr: 0.099838, loss: 2.2801
2022-03-01 07:46:05 - train: epoch 0011, iter [00600, 05004], lr: 0.099838, loss: 2.3090
2022-03-01 07:46:38 - train: epoch 0011, iter [00700, 05004], lr: 0.099838, loss: 2.3337
2022-03-01 07:47:11 - train: epoch 0011, iter [00800, 05004], lr: 0.099838, loss: 2.3623
2022-03-01 07:47:44 - train: epoch 0011, iter [00900, 05004], lr: 0.099838, loss: 2.4717
2022-03-01 07:48:18 - train: epoch 0011, iter [01000, 05004], lr: 0.099838, loss: 2.2982
2022-03-01 07:48:50 - train: epoch 0011, iter [01100, 05004], lr: 0.099838, loss: 2.4985
2022-03-01 07:49:23 - train: epoch 0011, iter [01200, 05004], lr: 0.099838, loss: 2.6523
2022-03-01 07:49:56 - train: epoch 0011, iter [01300, 05004], lr: 0.099838, loss: 2.4509
2022-03-01 07:50:29 - train: epoch 0011, iter [01400, 05004], lr: 0.099838, loss: 2.3362
2022-03-01 07:51:03 - train: epoch 0011, iter [01500, 05004], lr: 0.099838, loss: 2.2126
2022-03-01 07:51:37 - train: epoch 0011, iter [01600, 05004], lr: 0.099838, loss: 2.3740
2022-03-01 07:52:09 - train: epoch 0011, iter [01700, 05004], lr: 0.099838, loss: 2.4349
2022-03-01 07:52:43 - train: epoch 0011, iter [01800, 05004], lr: 0.099838, loss: 2.2910
2022-03-01 07:53:17 - train: epoch 0011, iter [01900, 05004], lr: 0.099838, loss: 2.2393
2022-03-01 07:53:49 - train: epoch 0011, iter [02000, 05004], lr: 0.099838, loss: 2.5141
2022-03-01 07:54:22 - train: epoch 0011, iter [02100, 05004], lr: 0.099838, loss: 2.3758
2022-03-01 07:54:56 - train: epoch 0011, iter [02200, 05004], lr: 0.099838, loss: 2.2252
2022-03-01 07:55:28 - train: epoch 0011, iter [02300, 05004], lr: 0.099838, loss: 2.5854
2022-03-01 07:56:02 - train: epoch 0011, iter [02400, 05004], lr: 0.099838, loss: 2.0400
2022-03-01 07:56:35 - train: epoch 0011, iter [02500, 05004], lr: 0.099838, loss: 2.6620
2022-03-01 07:57:08 - train: epoch 0011, iter [02600, 05004], lr: 0.099838, loss: 2.3778
2022-03-01 07:57:41 - train: epoch 0011, iter [02700, 05004], lr: 0.099838, loss: 2.3537
2022-03-01 07:58:14 - train: epoch 0011, iter [02800, 05004], lr: 0.099838, loss: 2.0185
2022-03-01 07:58:47 - train: epoch 0011, iter [02900, 05004], lr: 0.099838, loss: 2.5047
2022-03-01 07:59:20 - train: epoch 0011, iter [03000, 05004], lr: 0.099838, loss: 2.6208
2022-03-01 07:59:53 - train: epoch 0011, iter [03100, 05004], lr: 0.099838, loss: 2.4161
2022-03-01 08:00:26 - train: epoch 0011, iter [03200, 05004], lr: 0.099838, loss: 2.2071
2022-03-01 08:00:59 - train: epoch 0011, iter [03300, 05004], lr: 0.099838, loss: 2.4770
2022-03-01 08:01:33 - train: epoch 0011, iter [03400, 05004], lr: 0.099838, loss: 2.2495
2022-03-01 08:02:06 - train: epoch 0011, iter [03500, 05004], lr: 0.099838, loss: 2.3207
2022-03-01 08:02:39 - train: epoch 0011, iter [03600, 05004], lr: 0.099838, loss: 2.3349
2022-03-01 08:03:12 - train: epoch 0011, iter [03700, 05004], lr: 0.099838, loss: 2.3681
2022-03-01 08:03:45 - train: epoch 0011, iter [03800, 05004], lr: 0.099838, loss: 2.1076
2022-03-01 08:04:18 - train: epoch 0011, iter [03900, 05004], lr: 0.099838, loss: 2.3548
2022-03-01 08:04:52 - train: epoch 0011, iter [04000, 05004], lr: 0.099838, loss: 2.3163
2022-03-01 08:05:24 - train: epoch 0011, iter [04100, 05004], lr: 0.099838, loss: 2.1889
2022-03-01 08:05:57 - train: epoch 0011, iter [04200, 05004], lr: 0.099838, loss: 2.3073
2022-03-01 08:06:30 - train: epoch 0011, iter [04300, 05004], lr: 0.099838, loss: 2.4449
2022-03-01 08:07:04 - train: epoch 0011, iter [04400, 05004], lr: 0.099838, loss: 2.2519
2022-03-01 08:07:37 - train: epoch 0011, iter [04500, 05004], lr: 0.099838, loss: 2.2223
2022-03-01 08:08:09 - train: epoch 0011, iter [04600, 05004], lr: 0.099838, loss: 2.1953
2022-03-01 08:08:43 - train: epoch 0011, iter [04700, 05004], lr: 0.099838, loss: 2.1328
2022-03-01 08:09:16 - train: epoch 0011, iter [04800, 05004], lr: 0.099838, loss: 2.1311
2022-03-01 08:09:49 - train: epoch 0011, iter [04900, 05004], lr: 0.099838, loss: 2.0867
2022-03-01 08:10:21 - train: epoch 0011, iter [05000, 05004], lr: 0.099838, loss: 2.2719
2022-03-01 08:10:22 - train: epoch 011, train_loss: 2.3458
2022-03-01 08:11:36 - eval: epoch: 011, acc1: 48.842%, acc5: 75.072%, test_loss: 2.2065, per_image_load_time: 2.343ms, per_image_inference_time: 0.508ms
2022-03-01 08:11:36 - until epoch: 011, best_acc1: 50.814%
2022-03-01 08:11:36 - epoch 012 lr: 0.09976658173588243
2022-03-01 08:12:15 - train: epoch 0012, iter [00100, 05004], lr: 0.099767, loss: 2.1925
2022-03-01 08:12:48 - train: epoch 0012, iter [00200, 05004], lr: 0.099767, loss: 2.2711
2022-03-01 08:13:21 - train: epoch 0012, iter [00300, 05004], lr: 0.099767, loss: 2.3041
2022-03-01 08:13:54 - train: epoch 0012, iter [00400, 05004], lr: 0.099767, loss: 2.3370
2022-03-01 08:14:26 - train: epoch 0012, iter [00500, 05004], lr: 0.099767, loss: 2.5619
2022-03-01 08:15:00 - train: epoch 0012, iter [00600, 05004], lr: 0.099767, loss: 2.1414
2022-03-01 08:15:34 - train: epoch 0012, iter [00700, 05004], lr: 0.099767, loss: 2.2278
2022-03-01 08:16:06 - train: epoch 0012, iter [00800, 05004], lr: 0.099767, loss: 2.3509
2022-03-01 08:16:39 - train: epoch 0012, iter [00900, 05004], lr: 0.099767, loss: 2.4389
2022-03-01 08:17:12 - train: epoch 0012, iter [01000, 05004], lr: 0.099767, loss: 2.1013
2022-03-01 08:17:45 - train: epoch 0012, iter [01100, 05004], lr: 0.099767, loss: 2.6394
2022-03-01 08:18:19 - train: epoch 0012, iter [01200, 05004], lr: 0.099767, loss: 2.0694
2022-03-01 08:18:52 - train: epoch 0012, iter [01300, 05004], lr: 0.099767, loss: 2.2561
2022-03-01 08:19:25 - train: epoch 0012, iter [01400, 05004], lr: 0.099767, loss: 2.4780
2022-03-01 08:19:58 - train: epoch 0012, iter [01500, 05004], lr: 0.099767, loss: 2.0924
2022-03-01 08:20:32 - train: epoch 0012, iter [01600, 05004], lr: 0.099767, loss: 2.2984
2022-03-01 08:21:05 - train: epoch 0012, iter [01700, 05004], lr: 0.099767, loss: 2.1461
2022-03-01 08:21:38 - train: epoch 0012, iter [01800, 05004], lr: 0.099767, loss: 2.3779
2022-03-01 08:22:12 - train: epoch 0012, iter [01900, 05004], lr: 0.099767, loss: 2.4099
2022-03-01 08:22:45 - train: epoch 0012, iter [02000, 05004], lr: 0.099767, loss: 2.4623
2022-03-01 08:23:17 - train: epoch 0012, iter [02100, 05004], lr: 0.099767, loss: 2.3316
2022-03-01 08:23:50 - train: epoch 0012, iter [02200, 05004], lr: 0.099767, loss: 2.5165
2022-03-01 08:24:24 - train: epoch 0012, iter [02300, 05004], lr: 0.099767, loss: 2.3126
2022-03-01 08:24:56 - train: epoch 0012, iter [02400, 05004], lr: 0.099767, loss: 2.3562
2022-03-01 08:25:30 - train: epoch 0012, iter [02500, 05004], lr: 0.099767, loss: 2.1522
2022-03-01 08:26:02 - train: epoch 0012, iter [02600, 05004], lr: 0.099767, loss: 2.1406
2022-03-01 08:26:36 - train: epoch 0012, iter [02700, 05004], lr: 0.099767, loss: 2.1397
2022-03-01 08:27:09 - train: epoch 0012, iter [02800, 05004], lr: 0.099767, loss: 2.2838
2022-03-01 08:27:42 - train: epoch 0012, iter [02900, 05004], lr: 0.099767, loss: 2.1149
2022-03-01 08:28:15 - train: epoch 0012, iter [03000, 05004], lr: 0.099767, loss: 2.1713
2022-03-01 08:28:48 - train: epoch 0012, iter [03100, 05004], lr: 0.099767, loss: 2.3542
2022-03-01 08:29:21 - train: epoch 0012, iter [03200, 05004], lr: 0.099767, loss: 2.0860
2022-03-01 08:29:54 - train: epoch 0012, iter [03300, 05004], lr: 0.099767, loss: 2.2237
2022-03-01 08:30:27 - train: epoch 0012, iter [03400, 05004], lr: 0.099767, loss: 2.3118
2022-03-01 08:31:00 - train: epoch 0012, iter [03500, 05004], lr: 0.099767, loss: 2.4567
2022-03-01 08:31:33 - train: epoch 0012, iter [03600, 05004], lr: 0.099767, loss: 2.3896
2022-03-01 08:32:06 - train: epoch 0012, iter [03700, 05004], lr: 0.099767, loss: 2.3588
2022-03-01 08:32:39 - train: epoch 0012, iter [03800, 05004], lr: 0.099767, loss: 2.2881
2022-03-01 08:33:12 - train: epoch 0012, iter [03900, 05004], lr: 0.099767, loss: 2.1786
2022-03-01 08:33:46 - train: epoch 0012, iter [04000, 05004], lr: 0.099767, loss: 2.1880
2022-03-01 08:34:19 - train: epoch 0012, iter [04100, 05004], lr: 0.099767, loss: 2.2247
2022-03-01 08:34:53 - train: epoch 0012, iter [04200, 05004], lr: 0.099767, loss: 2.1283
2022-03-01 08:35:26 - train: epoch 0012, iter [04300, 05004], lr: 0.099767, loss: 2.3677
2022-03-01 08:35:59 - train: epoch 0012, iter [04400, 05004], lr: 0.099767, loss: 2.1690
2022-03-01 08:36:32 - train: epoch 0012, iter [04500, 05004], lr: 0.099767, loss: 2.2388
2022-03-01 08:37:05 - train: epoch 0012, iter [04600, 05004], lr: 0.099767, loss: 2.6985
2022-03-01 08:37:38 - train: epoch 0012, iter [04700, 05004], lr: 0.099767, loss: 2.3056
2022-03-01 08:38:12 - train: epoch 0012, iter [04800, 05004], lr: 0.099767, loss: 2.3380
2022-03-01 08:38:45 - train: epoch 0012, iter [04900, 05004], lr: 0.099767, loss: 2.2593
2022-03-01 08:39:17 - train: epoch 0012, iter [05000, 05004], lr: 0.099767, loss: 2.0194
2022-03-01 08:39:18 - train: epoch 012, train_loss: 2.3197
2022-03-01 08:40:31 - eval: epoch: 012, acc1: 49.328%, acc5: 75.790%, test_loss: 2.1785, per_image_load_time: 2.293ms, per_image_inference_time: 0.547ms
2022-03-01 08:40:31 - until epoch: 012, best_acc1: 50.814%
2022-03-01 08:40:31 - epoch 013 lr: 0.09968238114591566
2022-03-01 08:41:09 - train: epoch 0013, iter [00100, 05004], lr: 0.099682, loss: 2.1314
2022-03-01 08:41:43 - train: epoch 0013, iter [00200, 05004], lr: 0.099682, loss: 2.2503
2022-03-01 08:42:16 - train: epoch 0013, iter [00300, 05004], lr: 0.099682, loss: 2.1794
2022-03-01 08:42:48 - train: epoch 0013, iter [00400, 05004], lr: 0.099682, loss: 2.2016
2022-03-01 08:43:22 - train: epoch 0013, iter [00500, 05004], lr: 0.099682, loss: 2.2970
2022-03-01 08:43:55 - train: epoch 0013, iter [00600, 05004], lr: 0.099682, loss: 2.3450
2022-03-01 08:44:27 - train: epoch 0013, iter [00700, 05004], lr: 0.099682, loss: 2.0834
2022-03-01 08:45:01 - train: epoch 0013, iter [00800, 05004], lr: 0.099682, loss: 2.4355
2022-03-01 08:45:35 - train: epoch 0013, iter [00900, 05004], lr: 0.099682, loss: 2.2087
2022-03-01 08:46:08 - train: epoch 0013, iter [01000, 05004], lr: 0.099682, loss: 2.3401
2022-03-01 08:46:41 - train: epoch 0013, iter [01100, 05004], lr: 0.099682, loss: 2.2871
2022-03-01 08:47:15 - train: epoch 0013, iter [01200, 05004], lr: 0.099682, loss: 2.3909
2022-03-01 08:47:48 - train: epoch 0013, iter [01300, 05004], lr: 0.099682, loss: 2.3600
2022-03-01 08:48:21 - train: epoch 0013, iter [01400, 05004], lr: 0.099682, loss: 2.1856
2022-03-01 08:48:54 - train: epoch 0013, iter [01500, 05004], lr: 0.099682, loss: 2.5092
2022-03-01 08:49:27 - train: epoch 0013, iter [01600, 05004], lr: 0.099682, loss: 2.0518
2022-03-01 08:50:01 - train: epoch 0013, iter [01700, 05004], lr: 0.099682, loss: 2.2879
2022-03-01 08:50:34 - train: epoch 0013, iter [01800, 05004], lr: 0.099682, loss: 2.4188
2022-03-01 08:51:08 - train: epoch 0013, iter [01900, 05004], lr: 0.099682, loss: 2.2641
2022-03-01 08:51:40 - train: epoch 0013, iter [02000, 05004], lr: 0.099682, loss: 2.4834
2022-03-01 08:52:14 - train: epoch 0013, iter [02100, 05004], lr: 0.099682, loss: 2.5705
2022-03-01 08:52:47 - train: epoch 0013, iter [02200, 05004], lr: 0.099682, loss: 2.2199
2022-03-01 08:53:21 - train: epoch 0013, iter [02300, 05004], lr: 0.099682, loss: 2.2981
2022-03-01 08:53:54 - train: epoch 0013, iter [02400, 05004], lr: 0.099682, loss: 2.2979
2022-03-01 08:54:27 - train: epoch 0013, iter [02500, 05004], lr: 0.099682, loss: 2.0992
2022-03-01 08:55:00 - train: epoch 0013, iter [02600, 05004], lr: 0.099682, loss: 2.2506
2022-03-01 08:55:34 - train: epoch 0013, iter [02700, 05004], lr: 0.099682, loss: 2.1284
2022-03-01 08:56:07 - train: epoch 0013, iter [02800, 05004], lr: 0.099682, loss: 2.3371
2022-03-01 08:56:41 - train: epoch 0013, iter [02900, 05004], lr: 0.099682, loss: 2.3844
2022-03-01 08:57:14 - train: epoch 0013, iter [03000, 05004], lr: 0.099682, loss: 2.1585
2022-03-01 08:57:47 - train: epoch 0013, iter [03100, 05004], lr: 0.099682, loss: 2.1637
2022-03-01 08:58:20 - train: epoch 0013, iter [03200, 05004], lr: 0.099682, loss: 2.2082
2022-03-01 08:58:53 - train: epoch 0013, iter [03300, 05004], lr: 0.099682, loss: 2.1419
2022-03-01 08:59:26 - train: epoch 0013, iter [03400, 05004], lr: 0.099682, loss: 2.1763
2022-03-01 09:00:00 - train: epoch 0013, iter [03500, 05004], lr: 0.099682, loss: 2.1291
2022-03-01 09:00:33 - train: epoch 0013, iter [03600, 05004], lr: 0.099682, loss: 2.6768
2022-03-01 09:01:06 - train: epoch 0013, iter [03700, 05004], lr: 0.099682, loss: 2.0453
2022-03-01 09:01:39 - train: epoch 0013, iter [03800, 05004], lr: 0.099682, loss: 2.3820
2022-03-01 09:02:12 - train: epoch 0013, iter [03900, 05004], lr: 0.099682, loss: 2.3690
2022-03-01 09:02:45 - train: epoch 0013, iter [04000, 05004], lr: 0.099682, loss: 2.3909
2022-03-01 09:03:18 - train: epoch 0013, iter [04100, 05004], lr: 0.099682, loss: 2.1283
2022-03-01 09:03:52 - train: epoch 0013, iter [04200, 05004], lr: 0.099682, loss: 2.1882
2022-03-01 09:04:24 - train: epoch 0013, iter [04300, 05004], lr: 0.099682, loss: 2.2190
2022-03-01 09:04:57 - train: epoch 0013, iter [04400, 05004], lr: 0.099682, loss: 2.1644
2022-03-01 09:05:31 - train: epoch 0013, iter [04500, 05004], lr: 0.099682, loss: 2.2216
2022-03-01 09:06:05 - train: epoch 0013, iter [04600, 05004], lr: 0.099682, loss: 2.3798
2022-03-01 09:06:37 - train: epoch 0013, iter [04700, 05004], lr: 0.099682, loss: 2.4213
2022-03-01 09:07:11 - train: epoch 0013, iter [04800, 05004], lr: 0.099682, loss: 2.3832
2022-03-01 09:07:44 - train: epoch 0013, iter [04900, 05004], lr: 0.099682, loss: 2.3533
2022-03-01 09:08:16 - train: epoch 0013, iter [05000, 05004], lr: 0.099682, loss: 2.5139
2022-03-01 09:08:17 - train: epoch 013, train_loss: 2.2955
2022-03-01 09:09:30 - eval: epoch: 013, acc1: 50.796%, acc5: 76.560%, test_loss: 2.1261, per_image_load_time: 1.397ms, per_image_inference_time: 0.547ms
2022-03-01 09:09:31 - until epoch: 013, best_acc1: 50.814%
2022-03-01 09:09:31 - epoch 014 lr: 0.0995852854910781
2022-03-01 09:10:09 - train: epoch 0014, iter [00100, 05004], lr: 0.099585, loss: 2.3394
2022-03-01 09:10:41 - train: epoch 0014, iter [00200, 05004], lr: 0.099585, loss: 2.3398
2022-03-01 09:11:15 - train: epoch 0014, iter [00300, 05004], lr: 0.099585, loss: 2.0029
2022-03-01 09:11:47 - train: epoch 0014, iter [00400, 05004], lr: 0.099585, loss: 2.1174
2022-03-01 09:12:20 - train: epoch 0014, iter [00500, 05004], lr: 0.099585, loss: 2.2480
2022-03-01 09:12:53 - train: epoch 0014, iter [00600, 05004], lr: 0.099585, loss: 2.3072
2022-03-01 09:13:27 - train: epoch 0014, iter [00700, 05004], lr: 0.099585, loss: 2.2366
2022-03-01 09:14:00 - train: epoch 0014, iter [00800, 05004], lr: 0.099585, loss: 2.2294
2022-03-01 09:14:33 - train: epoch 0014, iter [00900, 05004], lr: 0.099585, loss: 2.3003
2022-03-01 09:15:05 - train: epoch 0014, iter [01000, 05004], lr: 0.099585, loss: 2.4480
2022-03-01 09:15:39 - train: epoch 0014, iter [01100, 05004], lr: 0.099585, loss: 2.1921
2022-03-01 09:16:12 - train: epoch 0014, iter [01200, 05004], lr: 0.099585, loss: 2.2709
2022-03-01 09:16:46 - train: epoch 0014, iter [01300, 05004], lr: 0.099585, loss: 2.4103
2022-03-01 09:17:18 - train: epoch 0014, iter [01400, 05004], lr: 0.099585, loss: 2.3157
2022-03-01 09:17:52 - train: epoch 0014, iter [01500, 05004], lr: 0.099585, loss: 2.3276
2022-03-01 09:18:25 - train: epoch 0014, iter [01600, 05004], lr: 0.099585, loss: 2.2376
2022-03-01 09:18:58 - train: epoch 0014, iter [01700, 05004], lr: 0.099585, loss: 2.5762
2022-03-01 09:19:32 - train: epoch 0014, iter [01800, 05004], lr: 0.099585, loss: 2.3756
2022-03-01 09:20:05 - train: epoch 0014, iter [01900, 05004], lr: 0.099585, loss: 2.1598
2022-03-01 09:20:38 - train: epoch 0014, iter [02000, 05004], lr: 0.099585, loss: 2.1891
2022-03-01 09:21:12 - train: epoch 0014, iter [02100, 05004], lr: 0.099585, loss: 2.4282
2022-03-01 09:21:45 - train: epoch 0014, iter [02200, 05004], lr: 0.099585, loss: 2.3358
2022-03-01 09:22:18 - train: epoch 0014, iter [02300, 05004], lr: 0.099585, loss: 2.2933
2022-03-01 09:22:51 - train: epoch 0014, iter [02400, 05004], lr: 0.099585, loss: 2.4392
2022-03-01 09:23:24 - train: epoch 0014, iter [02500, 05004], lr: 0.099585, loss: 2.3181
2022-03-01 09:23:58 - train: epoch 0014, iter [02600, 05004], lr: 0.099585, loss: 2.2704
2022-03-01 09:24:31 - train: epoch 0014, iter [02700, 05004], lr: 0.099585, loss: 2.2918
2022-03-01 09:25:04 - train: epoch 0014, iter [02800, 05004], lr: 0.099585, loss: 2.5571
2022-03-01 09:25:37 - train: epoch 0014, iter [02900, 05004], lr: 0.099585, loss: 2.2746
2022-03-01 09:26:12 - train: epoch 0014, iter [03000, 05004], lr: 0.099585, loss: 2.3635
2022-03-01 09:26:44 - train: epoch 0014, iter [03100, 05004], lr: 0.099585, loss: 2.0677
2022-03-01 09:27:18 - train: epoch 0014, iter [03200, 05004], lr: 0.099585, loss: 2.2347
2022-03-01 09:27:50 - train: epoch 0014, iter [03300, 05004], lr: 0.099585, loss: 2.2214
2022-03-01 09:28:23 - train: epoch 0014, iter [03400, 05004], lr: 0.099585, loss: 2.1898
2022-03-01 09:28:56 - train: epoch 0014, iter [03500, 05004], lr: 0.099585, loss: 2.2493
2022-03-01 09:29:29 - train: epoch 0014, iter [03600, 05004], lr: 0.099585, loss: 2.1646
2022-03-01 09:30:02 - train: epoch 0014, iter [03700, 05004], lr: 0.099585, loss: 2.1347
2022-03-01 09:30:35 - train: epoch 0014, iter [03800, 05004], lr: 0.099585, loss: 2.4064
2022-03-01 09:31:08 - train: epoch 0014, iter [03900, 05004], lr: 0.099585, loss: 2.1156
2022-03-01 09:31:41 - train: epoch 0014, iter [04000, 05004], lr: 0.099585, loss: 2.4679
2022-03-01 09:32:14 - train: epoch 0014, iter [04100, 05004], lr: 0.099585, loss: 2.1197
2022-03-01 09:32:48 - train: epoch 0014, iter [04200, 05004], lr: 0.099585, loss: 2.2003
2022-03-01 09:33:20 - train: epoch 0014, iter [04300, 05004], lr: 0.099585, loss: 2.1438
2022-03-01 09:33:53 - train: epoch 0014, iter [04400, 05004], lr: 0.099585, loss: 2.1651
2022-03-01 09:34:27 - train: epoch 0014, iter [04500, 05004], lr: 0.099585, loss: 2.2547
2022-03-01 09:35:00 - train: epoch 0014, iter [04600, 05004], lr: 0.099585, loss: 2.2025
2022-03-01 09:35:32 - train: epoch 0014, iter [04700, 05004], lr: 0.099585, loss: 2.1774
2022-03-01 09:36:05 - train: epoch 0014, iter [04800, 05004], lr: 0.099585, loss: 2.2229
2022-03-01 09:36:38 - train: epoch 0014, iter [04900, 05004], lr: 0.099585, loss: 2.0217
2022-03-01 09:37:10 - train: epoch 0014, iter [05000, 05004], lr: 0.099585, loss: 2.3319
2022-03-01 09:37:11 - train: epoch 014, train_loss: 2.2766
2022-03-01 09:38:25 - eval: epoch: 014, acc1: 52.022%, acc5: 77.984%, test_loss: 2.0575, per_image_load_time: 1.807ms, per_image_inference_time: 0.522ms
2022-03-01 09:38:26 - until epoch: 014, best_acc1: 52.022%
2022-03-01 09:38:26 - epoch 015 lr: 0.09947531997255256
2022-03-01 09:39:04 - train: epoch 0015, iter [00100, 05004], lr: 0.099475, loss: 2.0399
2022-03-01 09:39:36 - train: epoch 0015, iter [00200, 05004], lr: 0.099475, loss: 2.3476
2022-03-01 09:40:09 - train: epoch 0015, iter [00300, 05004], lr: 0.099475, loss: 2.4762
2022-03-01 09:40:43 - train: epoch 0015, iter [00400, 05004], lr: 0.099475, loss: 2.2379
2022-03-01 09:41:15 - train: epoch 0015, iter [00500, 05004], lr: 0.099475, loss: 2.1724
2022-03-01 09:41:48 - train: epoch 0015, iter [00600, 05004], lr: 0.099475, loss: 2.4103
2022-03-01 09:42:21 - train: epoch 0015, iter [00700, 05004], lr: 0.099475, loss: 2.2356
2022-03-01 09:42:54 - train: epoch 0015, iter [00800, 05004], lr: 0.099475, loss: 2.1024
2022-03-01 09:43:27 - train: epoch 0015, iter [00900, 05004], lr: 0.099475, loss: 2.0478
2022-03-01 09:43:59 - train: epoch 0015, iter [01000, 05004], lr: 0.099475, loss: 2.3843
2022-03-01 09:44:33 - train: epoch 0015, iter [01100, 05004], lr: 0.099475, loss: 2.1191
2022-03-01 09:45:06 - train: epoch 0015, iter [01200, 05004], lr: 0.099475, loss: 2.2639
2022-03-01 09:45:39 - train: epoch 0015, iter [01300, 05004], lr: 0.099475, loss: 2.6081
2022-03-01 09:46:12 - train: epoch 0015, iter [01400, 05004], lr: 0.099475, loss: 2.0825
2022-03-01 09:46:45 - train: epoch 0015, iter [01500, 05004], lr: 0.099475, loss: 2.0094
2022-03-01 09:47:18 - train: epoch 0015, iter [01600, 05004], lr: 0.099475, loss: 2.3772
2022-03-01 09:47:52 - train: epoch 0015, iter [01700, 05004], lr: 0.099475, loss: 2.4634
2022-03-01 09:48:25 - train: epoch 0015, iter [01800, 05004], lr: 0.099475, loss: 2.1466
2022-03-01 09:48:58 - train: epoch 0015, iter [01900, 05004], lr: 0.099475, loss: 2.1252
2022-03-01 09:49:31 - train: epoch 0015, iter [02000, 05004], lr: 0.099475, loss: 2.1781
2022-03-01 09:50:05 - train: epoch 0015, iter [02100, 05004], lr: 0.099475, loss: 2.1144
2022-03-01 09:50:38 - train: epoch 0015, iter [02200, 05004], lr: 0.099475, loss: 2.4709
2022-03-01 09:51:11 - train: epoch 0015, iter [02300, 05004], lr: 0.099475, loss: 2.1813
2022-03-01 09:51:44 - train: epoch 0015, iter [02400, 05004], lr: 0.099475, loss: 2.3429
2022-03-01 09:52:18 - train: epoch 0015, iter [02500, 05004], lr: 0.099475, loss: 2.3206
2022-03-01 09:52:52 - train: epoch 0015, iter [02600, 05004], lr: 0.099475, loss: 2.1072
2022-03-01 09:53:25 - train: epoch 0015, iter [02700, 05004], lr: 0.099475, loss: 2.3515
2022-03-01 09:53:59 - train: epoch 0015, iter [02800, 05004], lr: 0.099475, loss: 2.3720
2022-03-01 09:54:32 - train: epoch 0015, iter [02900, 05004], lr: 0.099475, loss: 2.1728
2022-03-01 09:55:06 - train: epoch 0015, iter [03000, 05004], lr: 0.099475, loss: 2.0252
2022-03-01 09:55:39 - train: epoch 0015, iter [03100, 05004], lr: 0.099475, loss: 2.2389
2022-03-01 09:56:13 - train: epoch 0015, iter [03200, 05004], lr: 0.099475, loss: 2.3818
2022-03-01 09:56:49 - train: epoch 0015, iter [03300, 05004], lr: 0.099475, loss: 2.0897
2022-03-01 09:57:22 - train: epoch 0015, iter [03400, 05004], lr: 0.099475, loss: 2.2574
2022-03-01 09:57:56 - train: epoch 0015, iter [03500, 05004], lr: 0.099475, loss: 2.4420
2022-03-01 09:58:28 - train: epoch 0015, iter [03600, 05004], lr: 0.099475, loss: 2.2698
2022-03-01 09:59:01 - train: epoch 0015, iter [03700, 05004], lr: 0.099475, loss: 2.1338
2022-03-01 09:59:34 - train: epoch 0015, iter [03800, 05004], lr: 0.099475, loss: 2.2728
2022-03-01 10:00:07 - train: epoch 0015, iter [03900, 05004], lr: 0.099475, loss: 2.3884
2022-03-01 10:00:41 - train: epoch 0015, iter [04000, 05004], lr: 0.099475, loss: 2.3257
2022-03-01 10:01:14 - train: epoch 0015, iter [04100, 05004], lr: 0.099475, loss: 2.4572
2022-03-01 10:01:48 - train: epoch 0015, iter [04200, 05004], lr: 0.099475, loss: 2.0336
2022-03-01 10:02:21 - train: epoch 0015, iter [04300, 05004], lr: 0.099475, loss: 2.3441
2022-03-01 10:02:54 - train: epoch 0015, iter [04400, 05004], lr: 0.099475, loss: 2.3136
2022-03-01 10:03:27 - train: epoch 0015, iter [04500, 05004], lr: 0.099475, loss: 2.2926
2022-03-01 10:04:00 - train: epoch 0015, iter [04600, 05004], lr: 0.099475, loss: 2.1870
2022-03-01 10:04:34 - train: epoch 0015, iter [04700, 05004], lr: 0.099475, loss: 2.3731
2022-03-01 10:05:06 - train: epoch 0015, iter [04800, 05004], lr: 0.099475, loss: 2.0783
2022-03-01 10:05:39 - train: epoch 0015, iter [04900, 05004], lr: 0.099475, loss: 2.2210
2022-03-01 10:06:10 - train: epoch 0015, iter [05000, 05004], lr: 0.099475, loss: 2.2993
2022-03-01 10:06:12 - train: epoch 015, train_loss: 2.2603
2022-03-01 10:07:25 - eval: epoch: 015, acc1: 50.328%, acc5: 76.246%, test_loss: 2.1429, per_image_load_time: 2.313ms, per_image_inference_time: 0.460ms
2022-03-01 10:07:26 - until epoch: 015, best_acc1: 52.022%
2022-03-01 10:07:26 - epoch 016 lr: 0.09935251313189564
2022-03-01 10:08:04 - train: epoch 0016, iter [00100, 05004], lr: 0.099353, loss: 2.1945
2022-03-01 10:08:38 - train: epoch 0016, iter [00200, 05004], lr: 0.099353, loss: 2.1109
2022-03-01 10:09:11 - train: epoch 0016, iter [00300, 05004], lr: 0.099353, loss: 2.2416
2022-03-01 10:09:44 - train: epoch 0016, iter [00400, 05004], lr: 0.099353, loss: 2.4823
2022-03-01 10:10:17 - train: epoch 0016, iter [00500, 05004], lr: 0.099353, loss: 2.0329
2022-03-01 10:10:51 - train: epoch 0016, iter [00600, 05004], lr: 0.099353, loss: 2.4903
2022-03-01 10:11:22 - train: epoch 0016, iter [00700, 05004], lr: 0.099353, loss: 1.9980
2022-03-01 10:11:57 - train: epoch 0016, iter [00800, 05004], lr: 0.099353, loss: 2.3072
2022-03-01 10:12:29 - train: epoch 0016, iter [00900, 05004], lr: 0.099353, loss: 2.2160
2022-03-01 10:13:03 - train: epoch 0016, iter [01000, 05004], lr: 0.099353, loss: 2.1414
2022-03-01 10:13:36 - train: epoch 0016, iter [01100, 05004], lr: 0.099353, loss: 2.2448
2022-03-01 10:14:08 - train: epoch 0016, iter [01200, 05004], lr: 0.099353, loss: 2.2107
2022-03-01 10:14:41 - train: epoch 0016, iter [01300, 05004], lr: 0.099353, loss: 2.3228
2022-03-01 10:15:15 - train: epoch 0016, iter [01400, 05004], lr: 0.099353, loss: 2.0717
2022-03-01 10:15:48 - train: epoch 0016, iter [01500, 05004], lr: 0.099353, loss: 2.3826
2022-03-01 10:16:22 - train: epoch 0016, iter [01600, 05004], lr: 0.099353, loss: 2.3124
2022-03-01 10:16:54 - train: epoch 0016, iter [01700, 05004], lr: 0.099353, loss: 2.0640
2022-03-01 10:17:26 - train: epoch 0016, iter [01800, 05004], lr: 0.099353, loss: 2.2820
2022-03-01 10:17:59 - train: epoch 0016, iter [01900, 05004], lr: 0.099353, loss: 2.1740
2022-03-01 10:18:32 - train: epoch 0016, iter [02000, 05004], lr: 0.099353, loss: 2.0126
2022-03-01 10:19:05 - train: epoch 0016, iter [02100, 05004], lr: 0.099353, loss: 2.3831
2022-03-01 10:19:38 - train: epoch 0016, iter [02200, 05004], lr: 0.099353, loss: 2.2891
2022-03-01 10:20:11 - train: epoch 0016, iter [02300, 05004], lr: 0.099353, loss: 2.4504
2022-03-01 10:20:43 - train: epoch 0016, iter [02400, 05004], lr: 0.099353, loss: 2.3971
2022-03-01 10:21:16 - train: epoch 0016, iter [02500, 05004], lr: 0.099353, loss: 2.1764
2022-03-01 10:21:48 - train: epoch 0016, iter [02600, 05004], lr: 0.099353, loss: 2.3513
2022-03-01 10:22:22 - train: epoch 0016, iter [02700, 05004], lr: 0.099353, loss: 2.1876
2022-03-01 10:22:54 - train: epoch 0016, iter [02800, 05004], lr: 0.099353, loss: 2.1832
2022-03-01 10:23:27 - train: epoch 0016, iter [02900, 05004], lr: 0.099353, loss: 2.3440
2022-03-01 10:24:00 - train: epoch 0016, iter [03000, 05004], lr: 0.099353, loss: 2.5586
2022-03-01 10:24:33 - train: epoch 0016, iter [03100, 05004], lr: 0.099353, loss: 2.4194
2022-03-01 10:25:05 - train: epoch 0016, iter [03200, 05004], lr: 0.099353, loss: 2.3084
2022-03-01 10:25:38 - train: epoch 0016, iter [03300, 05004], lr: 0.099353, loss: 2.3834
2022-03-01 10:26:11 - train: epoch 0016, iter [03400, 05004], lr: 0.099353, loss: 2.1446
2022-03-01 10:26:44 - train: epoch 0016, iter [03500, 05004], lr: 0.099353, loss: 2.0966
2022-03-01 10:27:16 - train: epoch 0016, iter [03600, 05004], lr: 0.099353, loss: 2.0818
2022-03-01 10:27:50 - train: epoch 0016, iter [03700, 05004], lr: 0.099353, loss: 2.2831
2022-03-01 10:28:22 - train: epoch 0016, iter [03800, 05004], lr: 0.099353, loss: 2.5824
2022-03-01 10:28:55 - train: epoch 0016, iter [03900, 05004], lr: 0.099353, loss: 2.4023
2022-03-01 10:29:27 - train: epoch 0016, iter [04000, 05004], lr: 0.099353, loss: 2.3854
2022-03-01 10:30:00 - train: epoch 0016, iter [04100, 05004], lr: 0.099353, loss: 2.2523
2022-03-01 10:30:33 - train: epoch 0016, iter [04200, 05004], lr: 0.099353, loss: 2.2289
2022-03-01 10:31:06 - train: epoch 0016, iter [04300, 05004], lr: 0.099353, loss: 2.0987
2022-03-01 10:31:39 - train: epoch 0016, iter [04400, 05004], lr: 0.099353, loss: 2.1628
2022-03-01 10:32:12 - train: epoch 0016, iter [04500, 05004], lr: 0.099353, loss: 2.2755
2022-03-01 10:32:45 - train: epoch 0016, iter [04600, 05004], lr: 0.099353, loss: 2.1805
2022-03-01 10:33:16 - train: epoch 0016, iter [04700, 05004], lr: 0.099353, loss: 2.4700
2022-03-01 10:33:50 - train: epoch 0016, iter [04800, 05004], lr: 0.099353, loss: 2.1010
2022-03-01 10:34:23 - train: epoch 0016, iter [04900, 05004], lr: 0.099353, loss: 2.3372
2022-03-01 10:34:54 - train: epoch 0016, iter [05000, 05004], lr: 0.099353, loss: 2.2718
2022-03-01 10:34:55 - train: epoch 016, train_loss: 2.2455
2022-03-01 10:36:10 - eval: epoch: 016, acc1: 51.380%, acc5: 77.470%, test_loss: 2.0825, per_image_load_time: 2.469ms, per_image_inference_time: 0.417ms
2022-03-01 10:36:11 - until epoch: 016, best_acc1: 52.022%
2022-03-01 10:36:11 - epoch 017 lr: 0.09921689684362989
2022-03-01 10:36:48 - train: epoch 0017, iter [00100, 05004], lr: 0.099217, loss: 2.2156
2022-03-01 10:37:22 - train: epoch 0017, iter [00200, 05004], lr: 0.099217, loss: 2.3163
2022-03-01 10:37:54 - train: epoch 0017, iter [00300, 05004], lr: 0.099217, loss: 2.4291
2022-03-01 10:38:27 - train: epoch 0017, iter [00400, 05004], lr: 0.099217, loss: 1.9281
2022-03-01 10:38:59 - train: epoch 0017, iter [00500, 05004], lr: 0.099217, loss: 2.2887
2022-03-01 10:39:32 - train: epoch 0017, iter [00600, 05004], lr: 0.099217, loss: 2.5318
2022-03-01 10:40:04 - train: epoch 0017, iter [00700, 05004], lr: 0.099217, loss: 2.3130
2022-03-01 10:40:37 - train: epoch 0017, iter [00800, 05004], lr: 0.099217, loss: 2.1539
2022-03-01 10:41:10 - train: epoch 0017, iter [00900, 05004], lr: 0.099217, loss: 2.1471
2022-03-01 10:41:43 - train: epoch 0017, iter [01000, 05004], lr: 0.099217, loss: 2.1306
2022-03-01 10:42:16 - train: epoch 0017, iter [01100, 05004], lr: 0.099217, loss: 2.5537
2022-03-01 10:42:49 - train: epoch 0017, iter [01200, 05004], lr: 0.099217, loss: 2.3471
2022-03-01 10:43:22 - train: epoch 0017, iter [01300, 05004], lr: 0.099217, loss: 2.2084
2022-03-01 10:43:54 - train: epoch 0017, iter [01400, 05004], lr: 0.099217, loss: 2.3104
2022-03-01 10:44:27 - train: epoch 0017, iter [01500, 05004], lr: 0.099217, loss: 1.9446
2022-03-01 10:44:58 - train: epoch 0017, iter [01600, 05004], lr: 0.099217, loss: 2.0114
2022-03-01 10:45:33 - train: epoch 0017, iter [01700, 05004], lr: 0.099217, loss: 2.2652
2022-03-01 10:46:05 - train: epoch 0017, iter [01800, 05004], lr: 0.099217, loss: 2.2445
2022-03-01 10:46:39 - train: epoch 0017, iter [01900, 05004], lr: 0.099217, loss: 2.2292
2022-03-01 10:47:11 - train: epoch 0017, iter [02000, 05004], lr: 0.099217, loss: 2.4213
2022-03-01 10:47:44 - train: epoch 0017, iter [02100, 05004], lr: 0.099217, loss: 2.3068
2022-03-01 10:48:16 - train: epoch 0017, iter [02200, 05004], lr: 0.099217, loss: 2.1144
2022-03-01 10:48:49 - train: epoch 0017, iter [02300, 05004], lr: 0.099217, loss: 2.1534
2022-03-01 10:49:21 - train: epoch 0017, iter [02400, 05004], lr: 0.099217, loss: 2.1628
2022-03-01 10:49:54 - train: epoch 0017, iter [02500, 05004], lr: 0.099217, loss: 2.4042
2022-03-01 10:50:27 - train: epoch 0017, iter [02600, 05004], lr: 0.099217, loss: 2.1092
2022-03-01 10:50:59 - train: epoch 0017, iter [02700, 05004], lr: 0.099217, loss: 2.1601
2022-03-01 10:51:33 - train: epoch 0017, iter [02800, 05004], lr: 0.099217, loss: 2.4327
2022-03-01 10:52:05 - train: epoch 0017, iter [02900, 05004], lr: 0.099217, loss: 2.4663
2022-03-01 10:52:38 - train: epoch 0017, iter [03000, 05004], lr: 0.099217, loss: 2.0904
2022-03-01 10:53:11 - train: epoch 0017, iter [03100, 05004], lr: 0.099217, loss: 2.3607
2022-03-01 10:53:44 - train: epoch 0017, iter [03200, 05004], lr: 0.099217, loss: 2.1467
2022-03-01 10:54:17 - train: epoch 0017, iter [03300, 05004], lr: 0.099217, loss: 2.3940
2022-03-01 10:54:50 - train: epoch 0017, iter [03400, 05004], lr: 0.099217, loss: 2.0812
2022-03-01 10:55:23 - train: epoch 0017, iter [03500, 05004], lr: 0.099217, loss: 2.2477
2022-03-01 10:55:56 - train: epoch 0017, iter [03600, 05004], lr: 0.099217, loss: 2.4966
2022-03-01 10:56:29 - train: epoch 0017, iter [03700, 05004], lr: 0.099217, loss: 2.1772
2022-03-01 10:57:02 - train: epoch 0017, iter [03800, 05004], lr: 0.099217, loss: 2.3689
2022-03-01 10:57:34 - train: epoch 0017, iter [03900, 05004], lr: 0.099217, loss: 2.0644
2022-03-01 10:58:08 - train: epoch 0017, iter [04000, 05004], lr: 0.099217, loss: 2.1138
2022-03-01 10:58:40 - train: epoch 0017, iter [04100, 05004], lr: 0.099217, loss: 2.2504
2022-03-01 10:59:15 - train: epoch 0017, iter [04200, 05004], lr: 0.099217, loss: 2.2316
2022-03-01 10:59:47 - train: epoch 0017, iter [04300, 05004], lr: 0.099217, loss: 2.2830
2022-03-01 11:00:21 - train: epoch 0017, iter [04400, 05004], lr: 0.099217, loss: 2.1099
2022-03-01 11:00:53 - train: epoch 0017, iter [04500, 05004], lr: 0.099217, loss: 2.4380
2022-03-01 11:01:27 - train: epoch 0017, iter [04600, 05004], lr: 0.099217, loss: 2.2698
2022-03-01 11:02:00 - train: epoch 0017, iter [04700, 05004], lr: 0.099217, loss: 2.2737
2022-03-01 11:02:32 - train: epoch 0017, iter [04800, 05004], lr: 0.099217, loss: 2.2167
2022-03-01 11:03:06 - train: epoch 0017, iter [04900, 05004], lr: 0.099217, loss: 1.9810
2022-03-01 11:03:38 - train: epoch 0017, iter [05000, 05004], lr: 0.099217, loss: 2.0197
2022-03-01 11:03:39 - train: epoch 017, train_loss: 2.2321
2022-03-01 11:04:53 - eval: epoch: 017, acc1: 50.298%, acc5: 76.478%, test_loss: 2.1395, per_image_load_time: 2.080ms, per_image_inference_time: 0.443ms
2022-03-01 11:04:54 - until epoch: 017, best_acc1: 52.022%
2022-03-01 11:04:54 - epoch 018 lr: 0.09906850630697067
2022-03-01 11:05:33 - train: epoch 0018, iter [00100, 05004], lr: 0.099069, loss: 2.1644
2022-03-01 11:06:07 - train: epoch 0018, iter [00200, 05004], lr: 0.099069, loss: 2.2149
2022-03-01 11:06:42 - train: epoch 0018, iter [00300, 05004], lr: 0.099069, loss: 2.3436
2022-03-01 11:07:15 - train: epoch 0018, iter [00400, 05004], lr: 0.099069, loss: 2.3719
2022-03-01 11:07:49 - train: epoch 0018, iter [00500, 05004], lr: 0.099069, loss: 2.0550
2022-03-01 11:08:23 - train: epoch 0018, iter [00600, 05004], lr: 0.099069, loss: 2.2833
2022-03-01 11:08:57 - train: epoch 0018, iter [00700, 05004], lr: 0.099069, loss: 1.9890
2022-03-01 11:09:31 - train: epoch 0018, iter [00800, 05004], lr: 0.099069, loss: 2.2468
2022-03-01 11:10:05 - train: epoch 0018, iter [00900, 05004], lr: 0.099069, loss: 2.2463
2022-03-01 11:10:38 - train: epoch 0018, iter [01000, 05004], lr: 0.099069, loss: 2.0260
2022-03-01 11:11:11 - train: epoch 0018, iter [01100, 05004], lr: 0.099069, loss: 2.4506
2022-03-01 11:11:46 - train: epoch 0018, iter [01200, 05004], lr: 0.099069, loss: 2.2624
2022-03-01 11:12:21 - train: epoch 0018, iter [01300, 05004], lr: 0.099069, loss: 2.4772
2022-03-01 11:12:57 - train: epoch 0018, iter [01400, 05004], lr: 0.099069, loss: 2.2379
2022-03-01 11:13:32 - train: epoch 0018, iter [01500, 05004], lr: 0.099069, loss: 2.3715
2022-03-01 11:14:06 - train: epoch 0018, iter [01600, 05004], lr: 0.099069, loss: 2.3004
2022-03-01 11:14:42 - train: epoch 0018, iter [01700, 05004], lr: 0.099069, loss: 2.1230
2022-03-01 11:15:16 - train: epoch 0018, iter [01800, 05004], lr: 0.099069, loss: 1.9422
2022-03-01 11:15:51 - train: epoch 0018, iter [01900, 05004], lr: 0.099069, loss: 2.1917
2022-03-01 11:16:25 - train: epoch 0018, iter [02000, 05004], lr: 0.099069, loss: 2.5038
2022-03-01 11:16:59 - train: epoch 0018, iter [02100, 05004], lr: 0.099069, loss: 2.4683
2022-03-01 11:17:32 - train: epoch 0018, iter [02200, 05004], lr: 0.099069, loss: 2.1392
2022-03-01 11:18:07 - train: epoch 0018, iter [02300, 05004], lr: 0.099069, loss: 2.2989
2022-03-01 11:18:42 - train: epoch 0018, iter [02400, 05004], lr: 0.099069, loss: 2.1277
2022-03-01 11:19:16 - train: epoch 0018, iter [02500, 05004], lr: 0.099069, loss: 1.9895
2022-03-01 11:19:52 - train: epoch 0018, iter [02600, 05004], lr: 0.099069, loss: 2.0911
2022-03-01 11:20:25 - train: epoch 0018, iter [02700, 05004], lr: 0.099069, loss: 2.3464
2022-03-01 11:21:00 - train: epoch 0018, iter [02800, 05004], lr: 0.099069, loss: 1.9489
2022-03-01 11:21:34 - train: epoch 0018, iter [02900, 05004], lr: 0.099069, loss: 2.3338
2022-03-01 11:22:09 - train: epoch 0018, iter [03000, 05004], lr: 0.099069, loss: 2.1750
2022-03-01 11:22:43 - train: epoch 0018, iter [03100, 05004], lr: 0.099069, loss: 2.4737
2022-03-01 11:23:17 - train: epoch 0018, iter [03200, 05004], lr: 0.099069, loss: 2.0704
2022-03-01 11:23:49 - train: epoch 0018, iter [03300, 05004], lr: 0.099069, loss: 2.2258
2022-03-01 11:24:25 - train: epoch 0018, iter [03400, 05004], lr: 0.099069, loss: 2.1645
2022-03-01 11:24:59 - train: epoch 0018, iter [03500, 05004], lr: 0.099069, loss: 2.4330
2022-03-01 11:25:33 - train: epoch 0018, iter [03600, 05004], lr: 0.099069, loss: 2.2713
2022-03-01 11:26:07 - train: epoch 0018, iter [03700, 05004], lr: 0.099069, loss: 2.6177
2022-03-01 11:26:42 - train: epoch 0018, iter [03800, 05004], lr: 0.099069, loss: 2.2513
2022-03-01 11:27:16 - train: epoch 0018, iter [03900, 05004], lr: 0.099069, loss: 2.1782
2022-03-01 11:27:51 - train: epoch 0018, iter [04000, 05004], lr: 0.099069, loss: 2.0573
2022-03-01 11:28:25 - train: epoch 0018, iter [04100, 05004], lr: 0.099069, loss: 2.2435
2022-03-01 11:28:58 - train: epoch 0018, iter [04200, 05004], lr: 0.099069, loss: 2.2530
2022-03-01 11:29:32 - train: epoch 0018, iter [04300, 05004], lr: 0.099069, loss: 1.9311
2022-03-01 11:30:06 - train: epoch 0018, iter [04400, 05004], lr: 0.099069, loss: 2.3661
2022-03-01 11:30:40 - train: epoch 0018, iter [04500, 05004], lr: 0.099069, loss: 2.1981
2022-03-01 11:31:15 - train: epoch 0018, iter [04600, 05004], lr: 0.099069, loss: 2.0255
2022-03-01 11:31:50 - train: epoch 0018, iter [04700, 05004], lr: 0.099069, loss: 2.4917
2022-03-01 11:32:26 - train: epoch 0018, iter [04800, 05004], lr: 0.099069, loss: 2.3267
2022-03-01 11:33:00 - train: epoch 0018, iter [04900, 05004], lr: 0.099069, loss: 2.2176
2022-03-01 11:33:34 - train: epoch 0018, iter [05000, 05004], lr: 0.099069, loss: 2.3026
2022-03-01 11:33:35 - train: epoch 018, train_loss: 2.2195
2022-03-01 11:34:54 - eval: epoch: 018, acc1: 53.692%, acc5: 78.804%, test_loss: 1.9796, per_image_load_time: 2.681ms, per_image_inference_time: 0.413ms
2022-03-01 11:34:55 - until epoch: 018, best_acc1: 53.692%
2022-03-01 11:34:55 - epoch 019 lr: 0.09890738003669029
2022-03-01 11:35:35 - train: epoch 0019, iter [00100, 05004], lr: 0.098907, loss: 2.0495
2022-03-01 11:36:07 - train: epoch 0019, iter [00200, 05004], lr: 0.098907, loss: 2.4474
2022-03-01 11:36:43 - train: epoch 0019, iter [00300, 05004], lr: 0.098907, loss: 2.4203
2022-03-01 11:37:19 - train: epoch 0019, iter [00400, 05004], lr: 0.098907, loss: 2.1841
2022-03-01 11:37:53 - train: epoch 0019, iter [00500, 05004], lr: 0.098907, loss: 2.2054
2022-03-01 11:38:29 - train: epoch 0019, iter [00600, 05004], lr: 0.098907, loss: 2.2418
2022-03-01 11:39:03 - train: epoch 0019, iter [00700, 05004], lr: 0.098907, loss: 2.0635
2022-03-01 11:39:38 - train: epoch 0019, iter [00800, 05004], lr: 0.098907, loss: 2.3356
2022-03-01 11:40:13 - train: epoch 0019, iter [00900, 05004], lr: 0.098907, loss: 2.2495
2022-03-01 11:40:49 - train: epoch 0019, iter [01000, 05004], lr: 0.098907, loss: 2.2961
2022-03-01 11:41:22 - train: epoch 0019, iter [01100, 05004], lr: 0.098907, loss: 2.0972
2022-03-01 11:41:56 - train: epoch 0019, iter [01200, 05004], lr: 0.098907, loss: 2.2650
2022-03-01 11:42:30 - train: epoch 0019, iter [01300, 05004], lr: 0.098907, loss: 2.3591
2022-03-01 11:43:05 - train: epoch 0019, iter [01400, 05004], lr: 0.098907, loss: 2.0548
2022-03-01 11:43:40 - train: epoch 0019, iter [01500, 05004], lr: 0.098907, loss: 2.5418
2022-03-01 11:44:15 - train: epoch 0019, iter [01600, 05004], lr: 0.098907, loss: 2.2364
2022-03-01 11:44:50 - train: epoch 0019, iter [01700, 05004], lr: 0.098907, loss: 2.3106
2022-03-01 11:45:25 - train: epoch 0019, iter [01800, 05004], lr: 0.098907, loss: 2.2090
2022-03-01 11:46:00 - train: epoch 0019, iter [01900, 05004], lr: 0.098907, loss: 2.3667
2022-03-01 11:46:36 - train: epoch 0019, iter [02000, 05004], lr: 0.098907, loss: 2.1661
2022-03-01 11:47:09 - train: epoch 0019, iter [02100, 05004], lr: 0.098907, loss: 2.0235
2022-03-01 11:47:44 - train: epoch 0019, iter [02200, 05004], lr: 0.098907, loss: 2.1450
2022-03-01 11:48:16 - train: epoch 0019, iter [02300, 05004], lr: 0.098907, loss: 2.0955
2022-03-01 11:48:51 - train: epoch 0019, iter [02400, 05004], lr: 0.098907, loss: 2.2576
2022-03-01 11:49:26 - train: epoch 0019, iter [02500, 05004], lr: 0.098907, loss: 2.0722
2022-03-01 11:50:01 - train: epoch 0019, iter [02600, 05004], lr: 0.098907, loss: 2.2559
2022-03-01 11:50:36 - train: epoch 0019, iter [02700, 05004], lr: 0.098907, loss: 2.2968
2022-03-01 11:51:11 - train: epoch 0019, iter [02800, 05004], lr: 0.098907, loss: 2.2724
2022-03-01 11:51:46 - train: epoch 0019, iter [02900, 05004], lr: 0.098907, loss: 2.2157
2022-03-01 11:52:21 - train: epoch 0019, iter [03000, 05004], lr: 0.098907, loss: 2.3805
2022-03-01 11:52:56 - train: epoch 0019, iter [03100, 05004], lr: 0.098907, loss: 2.2942
2022-03-01 11:53:29 - train: epoch 0019, iter [03200, 05004], lr: 0.098907, loss: 1.9488
2022-03-01 11:54:03 - train: epoch 0019, iter [03300, 05004], lr: 0.098907, loss: 2.0985
2022-03-01 11:54:37 - train: epoch 0019, iter [03400, 05004], lr: 0.098907, loss: 2.2241
2022-03-01 11:55:12 - train: epoch 0019, iter [03500, 05004], lr: 0.098907, loss: 2.2541
2022-03-01 11:55:45 - train: epoch 0019, iter [03600, 05004], lr: 0.098907, loss: 2.1303
2022-03-01 11:56:20 - train: epoch 0019, iter [03700, 05004], lr: 0.098907, loss: 2.2586
2022-03-01 11:56:55 - train: epoch 0019, iter [03800, 05004], lr: 0.098907, loss: 2.3003
2022-03-01 11:57:30 - train: epoch 0019, iter [03900, 05004], lr: 0.098907, loss: 2.1692
2022-03-01 11:58:05 - train: epoch 0019, iter [04000, 05004], lr: 0.098907, loss: 2.0268
2022-03-01 11:58:39 - train: epoch 0019, iter [04100, 05004], lr: 0.098907, loss: 2.2627
2022-03-01 11:59:14 - train: epoch 0019, iter [04200, 05004], lr: 0.098907, loss: 2.1935
2022-03-01 11:59:47 - train: epoch 0019, iter [04300, 05004], lr: 0.098907, loss: 2.0400
2022-03-01 12:00:19 - train: epoch 0019, iter [04400, 05004], lr: 0.098907, loss: 2.3607
2022-03-01 12:00:54 - train: epoch 0019, iter [04500, 05004], lr: 0.098907, loss: 2.4990
2022-03-01 12:01:29 - train: epoch 0019, iter [04600, 05004], lr: 0.098907, loss: 2.1773
2022-03-01 12:02:03 - train: epoch 0019, iter [04700, 05004], lr: 0.098907, loss: 2.0572
2022-03-01 12:02:39 - train: epoch 0019, iter [04800, 05004], lr: 0.098907, loss: 2.3156
2022-03-01 12:03:13 - train: epoch 0019, iter [04900, 05004], lr: 0.098907, loss: 2.2471
2022-03-01 12:03:45 - train: epoch 0019, iter [05000, 05004], lr: 0.098907, loss: 2.2284
2022-03-01 12:03:46 - train: epoch 019, train_loss: 2.2137
2022-03-01 12:05:00 - eval: epoch: 019, acc1: 53.250%, acc5: 78.624%, test_loss: 1.9945, per_image_load_time: 2.311ms, per_image_inference_time: 0.469ms
2022-03-01 12:05:01 - until epoch: 019, best_acc1: 53.692%
2022-03-01 12:05:01 - epoch 020 lr: 0.0987335598531214
2022-03-01 12:05:40 - train: epoch 0020, iter [00100, 05004], lr: 0.098734, loss: 2.3781
2022-03-01 12:06:12 - train: epoch 0020, iter [00200, 05004], lr: 0.098734, loss: 2.0235
2022-03-01 12:06:45 - train: epoch 0020, iter [00300, 05004], lr: 0.098734, loss: 2.3106
2022-03-01 12:07:19 - train: epoch 0020, iter [00400, 05004], lr: 0.098734, loss: 2.0379
2022-03-01 12:07:53 - train: epoch 0020, iter [00500, 05004], lr: 0.098734, loss: 2.1320
2022-03-01 12:08:25 - train: epoch 0020, iter [00600, 05004], lr: 0.098734, loss: 2.2977
2022-03-01 12:08:59 - train: epoch 0020, iter [00700, 05004], lr: 0.098734, loss: 2.1696
2022-03-01 12:09:34 - train: epoch 0020, iter [00800, 05004], lr: 0.098734, loss: 2.2792
2022-03-01 12:10:06 - train: epoch 0020, iter [00900, 05004], lr: 0.098734, loss: 2.5905
2022-03-01 12:10:40 - train: epoch 0020, iter [01000, 05004], lr: 0.098734, loss: 2.2579
2022-03-01 12:11:13 - train: epoch 0020, iter [01100, 05004], lr: 0.098734, loss: 2.0405
2022-03-01 12:11:46 - train: epoch 0020, iter [01200, 05004], lr: 0.098734, loss: 1.9714
2022-03-01 12:12:19 - train: epoch 0020, iter [01300, 05004], lr: 0.098734, loss: 2.0988
2022-03-01 12:12:52 - train: epoch 0020, iter [01400, 05004], lr: 0.098734, loss: 2.4158
2022-03-01 12:13:25 - train: epoch 0020, iter [01500, 05004], lr: 0.098734, loss: 2.2948
2022-03-01 12:14:00 - train: epoch 0020, iter [01600, 05004], lr: 0.098734, loss: 2.0744
2022-03-01 12:14:35 - train: epoch 0020, iter [01700, 05004], lr: 0.098734, loss: 1.8428
2022-03-01 12:15:09 - train: epoch 0020, iter [01800, 05004], lr: 0.098734, loss: 2.1929
2022-03-01 12:15:44 - train: epoch 0020, iter [01900, 05004], lr: 0.098734, loss: 2.0710
2022-03-01 12:16:19 - train: epoch 0020, iter [02000, 05004], lr: 0.098734, loss: 2.0508
2022-03-01 12:16:52 - train: epoch 0020, iter [02100, 05004], lr: 0.098734, loss: 2.2702
2022-03-01 12:17:25 - train: epoch 0020, iter [02200, 05004], lr: 0.098734, loss: 2.0654
2022-03-01 12:17:58 - train: epoch 0020, iter [02300, 05004], lr: 0.098734, loss: 2.1115
2022-03-01 12:18:33 - train: epoch 0020, iter [02400, 05004], lr: 0.098734, loss: 2.3101
2022-03-01 12:19:09 - train: epoch 0020, iter [02500, 05004], lr: 0.098734, loss: 2.1534
2022-03-01 12:19:44 - train: epoch 0020, iter [02600, 05004], lr: 0.098734, loss: 2.0354
2022-03-01 12:20:19 - train: epoch 0020, iter [02700, 05004], lr: 0.098734, loss: 2.2433
2022-03-01 12:20:55 - train: epoch 0020, iter [02800, 05004], lr: 0.098734, loss: 2.2422
2022-03-01 12:21:31 - train: epoch 0020, iter [02900, 05004], lr: 0.098734, loss: 2.2869
2022-03-01 12:22:05 - train: epoch 0020, iter [03000, 05004], lr: 0.098734, loss: 2.1601
2022-03-01 12:22:41 - train: epoch 0020, iter [03100, 05004], lr: 0.098734, loss: 2.3292
2022-03-01 12:23:14 - train: epoch 0020, iter [03200, 05004], lr: 0.098734, loss: 2.3985
2022-03-01 12:23:46 - train: epoch 0020, iter [03300, 05004], lr: 0.098734, loss: 2.1000
2022-03-01 12:24:21 - train: epoch 0020, iter [03400, 05004], lr: 0.098734, loss: 2.2774
2022-03-01 12:24:57 - train: epoch 0020, iter [03500, 05004], lr: 0.098734, loss: 2.1342
2022-03-01 12:25:32 - train: epoch 0020, iter [03600, 05004], lr: 0.098734, loss: 2.2102
2022-03-01 12:26:07 - train: epoch 0020, iter [03700, 05004], lr: 0.098734, loss: 2.1003
2022-03-01 12:26:43 - train: epoch 0020, iter [03800, 05004], lr: 0.098734, loss: 2.2533
2022-03-01 12:27:18 - train: epoch 0020, iter [03900, 05004], lr: 0.098734, loss: 2.3107
2022-03-01 12:27:53 - train: epoch 0020, iter [04000, 05004], lr: 0.098734, loss: 2.1012
2022-03-01 12:28:28 - train: epoch 0020, iter [04100, 05004], lr: 0.098734, loss: 2.0404
2022-03-01 12:29:03 - train: epoch 0020, iter [04200, 05004], lr: 0.098734, loss: 2.1523
2022-03-01 12:29:36 - train: epoch 0020, iter [04300, 05004], lr: 0.098734, loss: 2.2945
2022-03-01 12:30:08 - train: epoch 0020, iter [04400, 05004], lr: 0.098734, loss: 2.1676
2022-03-01 12:30:43 - train: epoch 0020, iter [04500, 05004], lr: 0.098734, loss: 2.2709
2022-03-01 12:31:19 - train: epoch 0020, iter [04600, 05004], lr: 0.098734, loss: 2.3422
2022-03-01 12:31:54 - train: epoch 0020, iter [04700, 05004], lr: 0.098734, loss: 1.9919
2022-03-01 12:32:30 - train: epoch 0020, iter [04800, 05004], lr: 0.098734, loss: 2.1433
2022-03-01 12:33:05 - train: epoch 0020, iter [04900, 05004], lr: 0.098734, loss: 2.3528
2022-03-01 12:33:39 - train: epoch 0020, iter [05000, 05004], lr: 0.098734, loss: 2.0150
2022-03-01 12:33:40 - train: epoch 020, train_loss: 2.1982
2022-03-01 12:35:01 - eval: epoch: 020, acc1: 53.918%, acc5: 79.156%, test_loss: 1.9707, per_image_load_time: 0.944ms, per_image_inference_time: 0.402ms
2022-03-01 12:35:01 - until epoch: 020, best_acc1: 53.918%
2022-03-01 12:35:01 - epoch 021 lr: 0.0985470908713026
2022-03-01 12:35:41 - train: epoch 0021, iter [00100, 05004], lr: 0.098547, loss: 2.0198
2022-03-01 12:36:13 - train: epoch 0021, iter [00200, 05004], lr: 0.098547, loss: 2.2345
2022-03-01 12:36:47 - train: epoch 0021, iter [00300, 05004], lr: 0.098547, loss: 1.8734
2022-03-01 12:37:24 - train: epoch 0021, iter [00400, 05004], lr: 0.098547, loss: 2.3887
2022-03-01 12:38:00 - train: epoch 0021, iter [00500, 05004], lr: 0.098547, loss: 2.1742
2022-03-01 12:38:35 - train: epoch 0021, iter [00600, 05004], lr: 0.098547, loss: 2.0433
2022-03-01 12:39:10 - train: epoch 0021, iter [00700, 05004], lr: 0.098547, loss: 1.9726
2022-03-01 12:39:46 - train: epoch 0021, iter [00800, 05004], lr: 0.098547, loss: 2.2890
2022-03-01 12:40:21 - train: epoch 0021, iter [00900, 05004], lr: 0.098547, loss: 2.1524
2022-03-01 12:40:55 - train: epoch 0021, iter [01000, 05004], lr: 0.098547, loss: 2.0121
2022-03-01 12:41:30 - train: epoch 0021, iter [01100, 05004], lr: 0.098547, loss: 2.1348
2022-03-01 12:42:03 - train: epoch 0021, iter [01200, 05004], lr: 0.098547, loss: 2.1317
2022-03-01 12:42:35 - train: epoch 0021, iter [01300, 05004], lr: 0.098547, loss: 2.0583
2022-03-01 12:43:10 - train: epoch 0021, iter [01400, 05004], lr: 0.098547, loss: 2.1844
2022-03-01 12:43:46 - train: epoch 0021, iter [01500, 05004], lr: 0.098547, loss: 2.0024
2022-03-01 12:44:21 - train: epoch 0021, iter [01600, 05004], lr: 0.098547, loss: 2.1377
2022-03-01 12:44:56 - train: epoch 0021, iter [01700, 05004], lr: 0.098547, loss: 2.1888
2022-03-01 12:45:31 - train: epoch 0021, iter [01800, 05004], lr: 0.098547, loss: 2.0524
2022-03-01 12:46:07 - train: epoch 0021, iter [01900, 05004], lr: 0.098547, loss: 2.1733
2022-03-01 12:46:41 - train: epoch 0021, iter [02000, 05004], lr: 0.098547, loss: 2.3805
2022-03-01 12:47:16 - train: epoch 0021, iter [02100, 05004], lr: 0.098547, loss: 2.0419
2022-03-01 12:47:50 - train: epoch 0021, iter [02200, 05004], lr: 0.098547, loss: 2.2200
2022-03-01 12:48:25 - train: epoch 0021, iter [02300, 05004], lr: 0.098547, loss: 2.0145
2022-03-01 12:48:57 - train: epoch 0021, iter [02400, 05004], lr: 0.098547, loss: 1.9712
2022-03-01 12:49:28 - train: epoch 0021, iter [02500, 05004], lr: 0.098547, loss: 2.2535
2022-03-01 12:50:01 - train: epoch 0021, iter [02600, 05004], lr: 0.098547, loss: 2.4205
2022-03-01 12:50:33 - train: epoch 0021, iter [02700, 05004], lr: 0.098547, loss: 2.0795
2022-03-01 12:51:05 - train: epoch 0021, iter [02800, 05004], lr: 0.098547, loss: 2.1478
2022-03-01 12:51:36 - train: epoch 0021, iter [02900, 05004], lr: 0.098547, loss: 2.0894
2022-03-01 12:52:08 - train: epoch 0021, iter [03000, 05004], lr: 0.098547, loss: 2.4517
2022-03-01 12:52:40 - train: epoch 0021, iter [03100, 05004], lr: 0.098547, loss: 2.2739
2022-03-01 12:53:12 - train: epoch 0021, iter [03200, 05004], lr: 0.098547, loss: 2.2519
2022-03-01 12:53:43 - train: epoch 0021, iter [03300, 05004], lr: 0.098547, loss: 2.4588
2022-03-01 12:54:15 - train: epoch 0021, iter [03400, 05004], lr: 0.098547, loss: 2.3584
2022-03-01 12:54:48 - train: epoch 0021, iter [03500, 05004], lr: 0.098547, loss: 1.9945
2022-03-01 12:55:24 - train: epoch 0021, iter [03600, 05004], lr: 0.098547, loss: 2.2055
2022-03-01 12:55:59 - train: epoch 0021, iter [03700, 05004], lr: 0.098547, loss: 2.2262
2022-03-01 12:56:36 - train: epoch 0021, iter [03800, 05004], lr: 0.098547, loss: 2.0809
2022-03-01 12:57:08 - train: epoch 0021, iter [03900, 05004], lr: 0.098547, loss: 2.0954
2022-03-01 12:57:40 - train: epoch 0021, iter [04000, 05004], lr: 0.098547, loss: 2.4812
2022-03-01 12:58:12 - train: epoch 0021, iter [04100, 05004], lr: 0.098547, loss: 2.0568
2022-03-01 12:58:43 - train: epoch 0021, iter [04200, 05004], lr: 0.098547, loss: 2.1920
2022-03-01 12:59:17 - train: epoch 0021, iter [04300, 05004], lr: 0.098547, loss: 2.1112
2022-03-01 12:59:53 - train: epoch 0021, iter [04400, 05004], lr: 0.098547, loss: 2.1767
2022-03-01 13:00:29 - train: epoch 0021, iter [04500, 05004], lr: 0.098547, loss: 2.4120
2022-03-01 13:01:05 - train: epoch 0021, iter [04600, 05004], lr: 0.098547, loss: 2.1250
2022-03-01 13:01:40 - train: epoch 0021, iter [04700, 05004], lr: 0.098547, loss: 2.5115
2022-03-01 13:02:17 - train: epoch 0021, iter [04800, 05004], lr: 0.098547, loss: 2.3213
2022-03-01 13:02:53 - train: epoch 0021, iter [04900, 05004], lr: 0.098547, loss: 1.9893
2022-03-01 13:03:27 - train: epoch 0021, iter [05000, 05004], lr: 0.098547, loss: 1.9977
2022-03-01 13:03:28 - train: epoch 021, train_loss: 2.1941
2022-03-01 13:04:43 - eval: epoch: 021, acc1: 51.864%, acc5: 77.846%, test_loss: 2.0589, per_image_load_time: 1.729ms, per_image_inference_time: 0.463ms
2022-03-01 13:04:44 - until epoch: 021, best_acc1: 53.918%
2022-03-01 13:04:44 - epoch 022 lr: 0.09834802148926883
2022-03-01 13:05:26 - train: epoch 0022, iter [00100, 05004], lr: 0.098348, loss: 1.9624
2022-03-01 13:06:03 - train: epoch 0022, iter [00200, 05004], lr: 0.098348, loss: 2.1338
2022-03-01 13:06:39 - train: epoch 0022, iter [00300, 05004], lr: 0.098348, loss: 1.8678
2022-03-01 13:07:16 - train: epoch 0022, iter [00400, 05004], lr: 0.098348, loss: 2.0703
2022-03-01 13:07:51 - train: epoch 0022, iter [00500, 05004], lr: 0.098348, loss: 2.1916
2022-03-01 13:08:29 - train: epoch 0022, iter [00600, 05004], lr: 0.098348, loss: 2.2777
2022-03-01 13:09:04 - train: epoch 0022, iter [00700, 05004], lr: 0.098348, loss: 2.2481
2022-03-01 13:09:38 - train: epoch 0022, iter [00800, 05004], lr: 0.098348, loss: 2.3186
2022-03-01 13:10:11 - train: epoch 0022, iter [00900, 05004], lr: 0.098348, loss: 2.2703
2022-03-01 13:10:47 - train: epoch 0022, iter [01000, 05004], lr: 0.098348, loss: 2.2462
2022-03-01 13:11:23 - train: epoch 0022, iter [01100, 05004], lr: 0.098348, loss: 2.2121
2022-03-01 13:11:59 - train: epoch 0022, iter [01200, 05004], lr: 0.098348, loss: 1.8735
2022-03-01 13:12:35 - train: epoch 0022, iter [01300, 05004], lr: 0.098348, loss: 2.1832
2022-03-01 13:13:11 - train: epoch 0022, iter [01400, 05004], lr: 0.098348, loss: 2.2444
2022-03-01 13:13:46 - train: epoch 0022, iter [01500, 05004], lr: 0.098348, loss: 2.1480
2022-03-01 13:14:24 - train: epoch 0022, iter [01600, 05004], lr: 0.098348, loss: 1.9953
2022-03-01 13:14:59 - train: epoch 0022, iter [01700, 05004], lr: 0.098348, loss: 1.9185
2022-03-01 13:15:32 - train: epoch 0022, iter [01800, 05004], lr: 0.098348, loss: 2.3523
2022-03-01 13:16:05 - train: epoch 0022, iter [01900, 05004], lr: 0.098348, loss: 2.1149
2022-03-01 13:16:41 - train: epoch 0022, iter [02000, 05004], lr: 0.098348, loss: 2.2555
2022-03-01 13:17:17 - train: epoch 0022, iter [02100, 05004], lr: 0.098348, loss: 2.2403
2022-03-01 13:17:52 - train: epoch 0022, iter [02200, 05004], lr: 0.098348, loss: 2.0848
2022-03-01 13:18:29 - train: epoch 0022, iter [02300, 05004], lr: 0.098348, loss: 2.3441
2022-03-01 13:19:05 - train: epoch 0022, iter [02400, 05004], lr: 0.098348, loss: 2.2320
2022-03-01 13:19:41 - train: epoch 0022, iter [02500, 05004], lr: 0.098348, loss: 2.1380
2022-03-01 13:20:17 - train: epoch 0022, iter [02600, 05004], lr: 0.098348, loss: 1.9438
2022-03-01 13:20:52 - train: epoch 0022, iter [02700, 05004], lr: 0.098348, loss: 2.0040
2022-03-01 13:21:24 - train: epoch 0022, iter [02800, 05004], lr: 0.098348, loss: 2.5330
2022-03-01 13:21:58 - train: epoch 0022, iter [02900, 05004], lr: 0.098348, loss: 2.0071
2022-03-01 13:22:35 - train: epoch 0022, iter [03000, 05004], lr: 0.098348, loss: 2.1635
2022-03-01 13:23:10 - train: epoch 0022, iter [03100, 05004], lr: 0.098348, loss: 2.4344
2022-03-01 13:23:46 - train: epoch 0022, iter [03200, 05004], lr: 0.098348, loss: 2.2298
2022-03-01 13:24:22 - train: epoch 0022, iter [03300, 05004], lr: 0.098348, loss: 2.1538
2022-03-01 13:24:58 - train: epoch 0022, iter [03400, 05004], lr: 0.098348, loss: 2.0010
2022-03-01 13:25:34 - train: epoch 0022, iter [03500, 05004], lr: 0.098348, loss: 2.3542
2022-03-01 13:26:11 - train: epoch 0022, iter [03600, 05004], lr: 0.098348, loss: 2.2434
2022-03-01 13:26:44 - train: epoch 0022, iter [03700, 05004], lr: 0.098348, loss: 2.2667
2022-03-01 13:27:16 - train: epoch 0022, iter [03800, 05004], lr: 0.098348, loss: 2.2644
2022-03-01 13:27:52 - train: epoch 0022, iter [03900, 05004], lr: 0.098348, loss: 2.1667
2022-03-01 13:28:28 - train: epoch 0022, iter [04000, 05004], lr: 0.098348, loss: 2.1997
2022-03-01 13:29:03 - train: epoch 0022, iter [04100, 05004], lr: 0.098348, loss: 2.0588
2022-03-01 13:29:39 - train: epoch 0022, iter [04200, 05004], lr: 0.098348, loss: 2.1660
2022-03-01 13:30:15 - train: epoch 0022, iter [04300, 05004], lr: 0.098348, loss: 2.3571
2022-03-01 13:30:50 - train: epoch 0022, iter [04400, 05004], lr: 0.098348, loss: 2.3329
2022-03-01 13:31:25 - train: epoch 0022, iter [04500, 05004], lr: 0.098348, loss: 2.1418
2022-03-01 13:32:00 - train: epoch 0022, iter [04600, 05004], lr: 0.098348, loss: 2.4058
2022-03-01 13:32:32 - train: epoch 0022, iter [04700, 05004], lr: 0.098348, loss: 2.2539
2022-03-01 13:33:05 - train: epoch 0022, iter [04800, 05004], lr: 0.098348, loss: 1.9771
2022-03-01 13:33:38 - train: epoch 0022, iter [04900, 05004], lr: 0.098348, loss: 2.0743
2022-03-01 13:34:11 - train: epoch 0022, iter [05000, 05004], lr: 0.098348, loss: 2.0994
2022-03-01 13:34:12 - train: epoch 022, train_loss: 2.1874
2022-03-01 13:35:26 - eval: epoch: 022, acc1: 52.764%, acc5: 78.694%, test_loss: 2.0073, per_image_load_time: 1.582ms, per_image_inference_time: 0.455ms
2022-03-01 13:35:27 - until epoch: 022, best_acc1: 53.918%
2022-03-01 13:35:27 - epoch 023 lr: 0.09813640337548954
2022-03-01 13:36:06 - train: epoch 0023, iter [00100, 05004], lr: 0.098136, loss: 2.0470
2022-03-01 13:36:40 - train: epoch 0023, iter [00200, 05004], lr: 0.098136, loss: 1.8540
2022-03-01 13:37:13 - train: epoch 0023, iter [00300, 05004], lr: 0.098136, loss: 2.0490
2022-03-01 13:37:47 - train: epoch 0023, iter [00400, 05004], lr: 0.098136, loss: 2.1884
2022-03-01 13:38:19 - train: epoch 0023, iter [00500, 05004], lr: 0.098136, loss: 2.2381
2022-03-01 13:38:53 - train: epoch 0023, iter [00600, 05004], lr: 0.098136, loss: 2.0601
2022-03-01 13:39:27 - train: epoch 0023, iter [00700, 05004], lr: 0.098136, loss: 2.0112
2022-03-01 13:40:00 - train: epoch 0023, iter [00800, 05004], lr: 0.098136, loss: 2.0829
2022-03-01 13:40:33 - train: epoch 0023, iter [00900, 05004], lr: 0.098136, loss: 2.1705
2022-03-01 13:41:08 - train: epoch 0023, iter [01000, 05004], lr: 0.098136, loss: 2.0127
2022-03-01 13:41:42 - train: epoch 0023, iter [01100, 05004], lr: 0.098136, loss: 2.1912
2022-03-01 13:42:15 - train: epoch 0023, iter [01200, 05004], lr: 0.098136, loss: 2.0359
2022-03-01 13:42:48 - train: epoch 0023, iter [01300, 05004], lr: 0.098136, loss: 2.1166
2022-03-01 13:43:23 - train: epoch 0023, iter [01400, 05004], lr: 0.098136, loss: 2.1141
2022-03-01 13:43:55 - train: epoch 0023, iter [01500, 05004], lr: 0.098136, loss: 2.0321
2022-03-01 13:44:28 - train: epoch 0023, iter [01600, 05004], lr: 0.098136, loss: 2.1424
2022-03-01 13:45:01 - train: epoch 0023, iter [01700, 05004], lr: 0.098136, loss: 2.3029
2022-03-01 13:45:35 - train: epoch 0023, iter [01800, 05004], lr: 0.098136, loss: 2.0677
2022-03-01 13:46:08 - train: epoch 0023, iter [01900, 05004], lr: 0.098136, loss: 2.1932
2022-03-01 13:46:42 - train: epoch 0023, iter [02000, 05004], lr: 0.098136, loss: 1.9850
2022-03-01 13:47:16 - train: epoch 0023, iter [02100, 05004], lr: 0.098136, loss: 2.2753
2022-03-01 13:47:51 - train: epoch 0023, iter [02200, 05004], lr: 0.098136, loss: 1.8599
2022-03-01 13:48:27 - train: epoch 0023, iter [02300, 05004], lr: 0.098136, loss: 2.0189
2022-03-01 13:49:01 - train: epoch 0023, iter [02400, 05004], lr: 0.098136, loss: 2.1664
2022-03-01 13:49:33 - train: epoch 0023, iter [02500, 05004], lr: 0.098136, loss: 2.2080
2022-03-01 13:50:08 - train: epoch 0023, iter [02600, 05004], lr: 0.098136, loss: 2.2769
2022-03-01 13:50:45 - train: epoch 0023, iter [02700, 05004], lr: 0.098136, loss: 2.1176
2022-03-01 13:51:20 - train: epoch 0023, iter [02800, 05004], lr: 0.098136, loss: 2.1252
2022-03-01 13:51:56 - train: epoch 0023, iter [02900, 05004], lr: 0.098136, loss: 2.2006
2022-03-01 13:52:32 - train: epoch 0023, iter [03000, 05004], lr: 0.098136, loss: 2.3884
2022-03-01 13:53:08 - train: epoch 0023, iter [03100, 05004], lr: 0.098136, loss: 2.3891
2022-03-01 13:53:44 - train: epoch 0023, iter [03200, 05004], lr: 0.098136, loss: 2.3305
2022-03-01 13:54:20 - train: epoch 0023, iter [03300, 05004], lr: 0.098136, loss: 2.1630
2022-03-01 13:54:53 - train: epoch 0023, iter [03400, 05004], lr: 0.098136, loss: 2.3515
2022-03-01 13:55:26 - train: epoch 0023, iter [03500, 05004], lr: 0.098136, loss: 2.1005
2022-03-01 13:56:01 - train: epoch 0023, iter [03600, 05004], lr: 0.098136, loss: 1.9343
2022-03-01 13:56:37 - train: epoch 0023, iter [03700, 05004], lr: 0.098136, loss: 2.1956
2022-03-01 13:57:13 - train: epoch 0023, iter [03800, 05004], lr: 0.098136, loss: 2.1418
2022-03-01 13:57:48 - train: epoch 0023, iter [03900, 05004], lr: 0.098136, loss: 2.2283
2022-03-01 13:58:24 - train: epoch 0023, iter [04000, 05004], lr: 0.098136, loss: 2.2012
2022-03-01 13:59:00 - train: epoch 0023, iter [04100, 05004], lr: 0.098136, loss: 2.0357
2022-03-01 13:59:35 - train: epoch 0023, iter [04200, 05004], lr: 0.098136, loss: 2.1009
2022-03-01 14:00:10 - train: epoch 0023, iter [04300, 05004], lr: 0.098136, loss: 1.9661
2022-03-01 14:00:43 - train: epoch 0023, iter [04400, 05004], lr: 0.098136, loss: 2.0211
2022-03-01 14:01:15 - train: epoch 0023, iter [04500, 05004], lr: 0.098136, loss: 2.0281
2022-03-01 14:01:52 - train: epoch 0023, iter [04600, 05004], lr: 0.098136, loss: 2.2657
2022-03-01 14:02:27 - train: epoch 0023, iter [04700, 05004], lr: 0.098136, loss: 1.9752
2022-03-01 14:03:03 - train: epoch 0023, iter [04800, 05004], lr: 0.098136, loss: 2.0794
2022-03-01 14:03:38 - train: epoch 0023, iter [04900, 05004], lr: 0.098136, loss: 2.0489
2022-03-01 14:04:12 - train: epoch 0023, iter [05000, 05004], lr: 0.098136, loss: 2.2767
2022-03-01 14:04:13 - train: epoch 023, train_loss: 2.1772
2022-03-01 14:05:35 - eval: epoch: 023, acc1: 54.512%, acc5: 79.698%, test_loss: 1.9295, per_image_load_time: 2.753ms, per_image_inference_time: 0.426ms
2022-03-01 14:05:36 - until epoch: 023, best_acc1: 54.512%
2022-03-01 14:05:36 - epoch 024 lr: 0.09791229145545832
2022-03-01 14:06:17 - train: epoch 0024, iter [00100, 05004], lr: 0.097912, loss: 2.0930
2022-03-01 14:06:50 - train: epoch 0024, iter [00200, 05004], lr: 0.097912, loss: 2.2203
2022-03-01 14:07:24 - train: epoch 0024, iter [00300, 05004], lr: 0.097912, loss: 2.0510
2022-03-01 14:08:00 - train: epoch 0024, iter [00400, 05004], lr: 0.097912, loss: 2.2275
2022-03-01 14:08:36 - train: epoch 0024, iter [00500, 05004], lr: 0.097912, loss: 2.1183
2022-03-01 14:09:12 - train: epoch 0024, iter [00600, 05004], lr: 0.097912, loss: 1.9592
2022-03-01 14:09:48 - train: epoch 0024, iter [00700, 05004], lr: 0.097912, loss: 2.1299
2022-03-01 14:10:24 - train: epoch 0024, iter [00800, 05004], lr: 0.097912, loss: 1.9893
2022-03-01 14:10:59 - train: epoch 0024, iter [00900, 05004], lr: 0.097912, loss: 2.1300
2022-03-01 14:11:34 - train: epoch 0024, iter [01000, 05004], lr: 0.097912, loss: 2.2142
2022-03-01 14:12:09 - train: epoch 0024, iter [01100, 05004], lr: 0.097912, loss: 1.8788
2022-03-01 14:12:41 - train: epoch 0024, iter [01200, 05004], lr: 0.097912, loss: 1.9333
2022-03-01 14:13:15 - train: epoch 0024, iter [01300, 05004], lr: 0.097912, loss: 2.4757
2022-03-01 14:13:51 - train: epoch 0024, iter [01400, 05004], lr: 0.097912, loss: 2.0895
2022-03-01 14:14:27 - train: epoch 0024, iter [01500, 05004], lr: 0.097912, loss: 2.3974
2022-03-01 14:15:02 - train: epoch 0024, iter [01600, 05004], lr: 0.097912, loss: 2.1908
2022-03-01 14:15:38 - train: epoch 0024, iter [01700, 05004], lr: 0.097912, loss: 2.0745
2022-03-01 14:16:13 - train: epoch 0024, iter [01800, 05004], lr: 0.097912, loss: 2.3337
2022-03-01 14:16:49 - train: epoch 0024, iter [01900, 05004], lr: 0.097912, loss: 2.0414
2022-03-01 14:17:24 - train: epoch 0024, iter [02000, 05004], lr: 0.097912, loss: 2.0805
2022-03-01 14:17:59 - train: epoch 0024, iter [02100, 05004], lr: 0.097912, loss: 2.1797
2022-03-01 14:18:31 - train: epoch 0024, iter [02200, 05004], lr: 0.097912, loss: 2.0285
2022-03-01 14:19:06 - train: epoch 0024, iter [02300, 05004], lr: 0.097912, loss: 2.2426
2022-03-01 14:19:42 - train: epoch 0024, iter [02400, 05004], lr: 0.097912, loss: 2.0994
2022-03-01 14:20:18 - train: epoch 0024, iter [02500, 05004], lr: 0.097912, loss: 2.0617
2022-03-01 14:20:53 - train: epoch 0024, iter [02600, 05004], lr: 0.097912, loss: 1.9897
2022-03-01 14:21:30 - train: epoch 0024, iter [02700, 05004], lr: 0.097912, loss: 2.2684
2022-03-01 14:22:05 - train: epoch 0024, iter [02800, 05004], lr: 0.097912, loss: 2.1849
2022-03-01 14:22:41 - train: epoch 0024, iter [02900, 05004], lr: 0.097912, loss: 2.2181
2022-03-01 14:23:16 - train: epoch 0024, iter [03000, 05004], lr: 0.097912, loss: 2.1029
2022-03-01 14:23:51 - train: epoch 0024, iter [03100, 05004], lr: 0.097912, loss: 2.0419
2022-03-01 14:24:23 - train: epoch 0024, iter [03200, 05004], lr: 0.097912, loss: 2.3517
2022-03-01 14:24:58 - train: epoch 0024, iter [03300, 05004], lr: 0.097912, loss: 1.9701
2022-03-01 14:25:34 - train: epoch 0024, iter [03400, 05004], lr: 0.097912, loss: 2.1670
2022-03-01 14:26:10 - train: epoch 0024, iter [03500, 05004], lr: 0.097912, loss: 2.0465
2022-03-01 14:26:47 - train: epoch 0024, iter [03600, 05004], lr: 0.097912, loss: 2.3130
2022-03-01 14:27:23 - train: epoch 0024, iter [03700, 05004], lr: 0.097912, loss: 2.1339
2022-03-01 14:27:59 - train: epoch 0024, iter [03800, 05004], lr: 0.097912, loss: 2.3778
2022-03-01 14:28:35 - train: epoch 0024, iter [03900, 05004], lr: 0.097912, loss: 2.1563
2022-03-01 14:29:10 - train: epoch 0024, iter [04000, 05004], lr: 0.097912, loss: 2.2346
2022-03-01 14:29:44 - train: epoch 0024, iter [04100, 05004], lr: 0.097912, loss: 2.1763
2022-03-01 14:30:16 - train: epoch 0024, iter [04200, 05004], lr: 0.097912, loss: 2.1177
2022-03-01 14:30:53 - train: epoch 0024, iter [04300, 05004], lr: 0.097912, loss: 2.2137
2022-03-01 14:31:29 - train: epoch 0024, iter [04400, 05004], lr: 0.097912, loss: 2.2571
2022-03-01 14:32:04 - train: epoch 0024, iter [04500, 05004], lr: 0.097912, loss: 2.0073
2022-03-01 14:32:41 - train: epoch 0024, iter [04600, 05004], lr: 0.097912, loss: 2.2140
2022-03-01 14:33:17 - train: epoch 0024, iter [04700, 05004], lr: 0.097912, loss: 2.0901
2022-03-01 14:33:53 - train: epoch 0024, iter [04800, 05004], lr: 0.097912, loss: 2.0804
2022-03-01 14:34:29 - train: epoch 0024, iter [04900, 05004], lr: 0.097912, loss: 2.1570
2022-03-01 14:35:03 - train: epoch 0024, iter [05000, 05004], lr: 0.097912, loss: 2.2964
2022-03-01 14:35:04 - train: epoch 024, train_loss: 2.1733
2022-03-01 14:36:20 - eval: epoch: 024, acc1: 54.152%, acc5: 79.250%, test_loss: 1.9597, per_image_load_time: 2.093ms, per_image_inference_time: 0.450ms
2022-03-01 14:36:20 - until epoch: 024, best_acc1: 54.512%
2022-03-01 14:36:20 - epoch 025 lr: 0.09767574389743683
2022-03-01 14:37:03 - train: epoch 0025, iter [00100, 05004], lr: 0.097676, loss: 1.9887
2022-03-01 14:37:39 - train: epoch 0025, iter [00200, 05004], lr: 0.097676, loss: 1.8774
2022-03-01 14:38:15 - train: epoch 0025, iter [00300, 05004], lr: 0.097676, loss: 2.0654
2022-03-01 14:38:52 - train: epoch 0025, iter [00400, 05004], lr: 0.097676, loss: 2.0612
2022-03-01 14:39:28 - train: epoch 0025, iter [00500, 05004], lr: 0.097676, loss: 1.9899
2022-03-01 14:40:05 - train: epoch 0025, iter [00600, 05004], lr: 0.097676, loss: 2.2330
2022-03-01 14:40:40 - train: epoch 0025, iter [00700, 05004], lr: 0.097676, loss: 2.2336
2022-03-01 14:41:12 - train: epoch 0025, iter [00800, 05004], lr: 0.097676, loss: 2.1343
2022-03-01 14:41:46 - train: epoch 0025, iter [00900, 05004], lr: 0.097676, loss: 1.9432
2022-03-01 14:42:23 - train: epoch 0025, iter [01000, 05004], lr: 0.097676, loss: 2.0580
2022-03-01 14:42:59 - train: epoch 0025, iter [01100, 05004], lr: 0.097676, loss: 2.1082
2022-03-01 14:43:36 - train: epoch 0025, iter [01200, 05004], lr: 0.097676, loss: 2.2598
2022-03-01 14:44:12 - train: epoch 0025, iter [01300, 05004], lr: 0.097676, loss: 2.1871
2022-03-01 14:44:48 - train: epoch 0025, iter [01400, 05004], lr: 0.097676, loss: 2.3221
2022-03-01 14:45:24 - train: epoch 0025, iter [01500, 05004], lr: 0.097676, loss: 2.0555
2022-03-01 14:46:00 - train: epoch 0025, iter [01600, 05004], lr: 0.097676, loss: 1.9008
2022-03-01 14:46:35 - train: epoch 0025, iter [01700, 05004], lr: 0.097676, loss: 2.0303
2022-03-01 14:47:08 - train: epoch 0025, iter [01800, 05004], lr: 0.097676, loss: 1.9506
2022-03-01 14:47:43 - train: epoch 0025, iter [01900, 05004], lr: 0.097676, loss: 2.0717
2022-03-01 14:48:19 - train: epoch 0025, iter [02000, 05004], lr: 0.097676, loss: 2.1410
2022-03-01 14:48:56 - train: epoch 0025, iter [02100, 05004], lr: 0.097676, loss: 2.0326
2022-03-01 14:49:31 - train: epoch 0025, iter [02200, 05004], lr: 0.097676, loss: 1.8663
2022-03-01 14:50:07 - train: epoch 0025, iter [02300, 05004], lr: 0.097676, loss: 2.2260
2022-03-01 14:50:42 - train: epoch 0025, iter [02400, 05004], lr: 0.097676, loss: 2.0242
2022-03-01 14:51:19 - train: epoch 0025, iter [02500, 05004], lr: 0.097676, loss: 2.2223
2022-03-01 14:51:55 - train: epoch 0025, iter [02600, 05004], lr: 0.097676, loss: 2.2800
2022-03-01 14:52:29 - train: epoch 0025, iter [02700, 05004], lr: 0.097676, loss: 2.2533
2022-03-01 14:53:01 - train: epoch 0025, iter [02800, 05004], lr: 0.097676, loss: 2.1252
2022-03-01 14:53:37 - train: epoch 0025, iter [02900, 05004], lr: 0.097676, loss: 2.3617
2022-03-01 14:54:13 - train: epoch 0025, iter [03000, 05004], lr: 0.097676, loss: 2.2723
2022-03-01 14:54:48 - train: epoch 0025, iter [03100, 05004], lr: 0.097676, loss: 2.0540
2022-03-01 14:55:24 - train: epoch 0025, iter [03200, 05004], lr: 0.097676, loss: 2.3616
2022-03-01 14:56:00 - train: epoch 0025, iter [03300, 05004], lr: 0.097676, loss: 2.1445
2022-03-01 14:56:36 - train: epoch 0025, iter [03400, 05004], lr: 0.097676, loss: 2.2633
2022-03-01 14:57:12 - train: epoch 0025, iter [03500, 05004], lr: 0.097676, loss: 1.9715
2022-03-01 14:57:48 - train: epoch 0025, iter [03600, 05004], lr: 0.097676, loss: 2.3175
2022-03-01 14:58:21 - train: epoch 0025, iter [03700, 05004], lr: 0.097676, loss: 2.0883
2022-03-01 14:58:54 - train: epoch 0025, iter [03800, 05004], lr: 0.097676, loss: 2.2255
2022-03-01 14:59:30 - train: epoch 0025, iter [03900, 05004], lr: 0.097676, loss: 2.3284
2022-03-01 15:00:06 - train: epoch 0025, iter [04000, 05004], lr: 0.097676, loss: 2.3238
2022-03-01 15:00:42 - train: epoch 0025, iter [04100, 05004], lr: 0.097676, loss: 2.4424
2022-03-01 15:01:19 - train: epoch 0025, iter [04200, 05004], lr: 0.097676, loss: 2.2704
2022-03-01 15:01:55 - train: epoch 0025, iter [04300, 05004], lr: 0.097676, loss: 2.0735
2022-03-01 15:02:32 - train: epoch 0025, iter [04400, 05004], lr: 0.097676, loss: 2.0750
2022-03-01 15:03:08 - train: epoch 0025, iter [04500, 05004], lr: 0.097676, loss: 2.1350
2022-03-01 15:03:42 - train: epoch 0025, iter [04600, 05004], lr: 0.097676, loss: 2.2137
2022-03-01 15:04:15 - train: epoch 0025, iter [04700, 05004], lr: 0.097676, loss: 2.0840
2022-03-01 15:04:48 - train: epoch 0025, iter [04800, 05004], lr: 0.097676, loss: 1.9908
2022-03-01 15:05:21 - train: epoch 0025, iter [04900, 05004], lr: 0.097676, loss: 2.0548
2022-03-01 15:05:54 - train: epoch 0025, iter [05000, 05004], lr: 0.097676, loss: 2.3490
2022-03-01 15:05:55 - train: epoch 025, train_loss: 2.1674
2022-03-01 15:07:10 - eval: epoch: 025, acc1: 54.888%, acc5: 80.270%, test_loss: 1.9018, per_image_load_time: 2.478ms, per_image_inference_time: 0.460ms
2022-03-01 15:07:11 - until epoch: 025, best_acc1: 54.888%
2022-03-01 15:07:11 - epoch 026 lr: 0.09742682209735727
2022-03-01 15:07:50 - train: epoch 0026, iter [00100, 05004], lr: 0.097427, loss: 1.9566
2022-03-01 15:08:23 - train: epoch 0026, iter [00200, 05004], lr: 0.097427, loss: 1.9484
2022-03-01 15:08:57 - train: epoch 0026, iter [00300, 05004], lr: 0.097427, loss: 2.0645
2022-03-01 15:09:30 - train: epoch 0026, iter [00400, 05004], lr: 0.097427, loss: 2.1319
2022-03-01 15:10:03 - train: epoch 0026, iter [00500, 05004], lr: 0.097427, loss: 2.2034
2022-03-01 15:10:37 - train: epoch 0026, iter [00600, 05004], lr: 0.097427, loss: 2.3769
2022-03-01 15:11:10 - train: epoch 0026, iter [00700, 05004], lr: 0.097427, loss: 2.0274
2022-03-01 15:11:44 - train: epoch 0026, iter [00800, 05004], lr: 0.097427, loss: 1.8931
2022-03-01 15:12:18 - train: epoch 0026, iter [00900, 05004], lr: 0.097427, loss: 2.3940
2022-03-01 15:12:51 - train: epoch 0026, iter [01000, 05004], lr: 0.097427, loss: 2.0119
2022-03-01 15:13:26 - train: epoch 0026, iter [01100, 05004], lr: 0.097427, loss: 2.0786
2022-03-01 15:14:00 - train: epoch 0026, iter [01200, 05004], lr: 0.097427, loss: 2.1451
2022-03-01 15:14:32 - train: epoch 0026, iter [01300, 05004], lr: 0.097427, loss: 2.0104
2022-03-01 15:15:05 - train: epoch 0026, iter [01400, 05004], lr: 0.097427, loss: 2.2622
2022-03-01 15:15:39 - train: epoch 0026, iter [01500, 05004], lr: 0.097427, loss: 2.1163
2022-03-01 15:16:13 - train: epoch 0026, iter [01600, 05004], lr: 0.097427, loss: 2.3508
2022-03-01 15:16:46 - train: epoch 0026, iter [01700, 05004], lr: 0.097427, loss: 2.0146
2022-03-01 15:17:20 - train: epoch 0026, iter [01800, 05004], lr: 0.097427, loss: 2.1822
2022-03-01 15:17:53 - train: epoch 0026, iter [01900, 05004], lr: 0.097427, loss: 2.4515
2022-03-01 15:18:27 - train: epoch 0026, iter [02000, 05004], lr: 0.097427, loss: 2.3492
2022-03-01 15:18:59 - train: epoch 0026, iter [02100, 05004], lr: 0.097427, loss: 2.3414
2022-03-01 15:19:34 - train: epoch 0026, iter [02200, 05004], lr: 0.097427, loss: 2.2276
2022-03-01 15:20:07 - train: epoch 0026, iter [02300, 05004], lr: 0.097427, loss: 2.1450
2022-03-01 15:20:39 - train: epoch 0026, iter [02400, 05004], lr: 0.097427, loss: 2.4215
2022-03-01 15:21:12 - train: epoch 0026, iter [02500, 05004], lr: 0.097427, loss: 2.2929
2022-03-01 15:21:46 - train: epoch 0026, iter [02600, 05004], lr: 0.097427, loss: 2.2176
2022-03-01 15:22:19 - train: epoch 0026, iter [02700, 05004], lr: 0.097427, loss: 2.1241
2022-03-01 15:22:52 - train: epoch 0026, iter [02800, 05004], lr: 0.097427, loss: 2.0006
2022-03-01 15:23:27 - train: epoch 0026, iter [02900, 05004], lr: 0.097427, loss: 2.3772
2022-03-01 15:24:00 - train: epoch 0026, iter [03000, 05004], lr: 0.097427, loss: 2.0462
2022-03-01 15:24:33 - train: epoch 0026, iter [03100, 05004], lr: 0.097427, loss: 2.2026
2022-03-01 15:25:07 - train: epoch 0026, iter [03200, 05004], lr: 0.097427, loss: 2.1030
2022-03-01 15:25:40 - train: epoch 0026, iter [03300, 05004], lr: 0.097427, loss: 2.0922
2022-03-01 15:26:12 - train: epoch 0026, iter [03400, 05004], lr: 0.097427, loss: 2.3506
2022-03-01 15:26:45 - train: epoch 0026, iter [03500, 05004], lr: 0.097427, loss: 2.3208
2022-03-01 15:27:19 - train: epoch 0026, iter [03600, 05004], lr: 0.097427, loss: 2.0233
2022-03-01 15:27:52 - train: epoch 0026, iter [03700, 05004], lr: 0.097427, loss: 2.2848
2022-03-01 15:28:26 - train: epoch 0026, iter [03800, 05004], lr: 0.097427, loss: 2.3680
2022-03-01 15:29:00 - train: epoch 0026, iter [03900, 05004], lr: 0.097427, loss: 2.3308
2022-03-01 15:29:33 - train: epoch 0026, iter [04000, 05004], lr: 0.097427, loss: 2.3944
2022-03-01 15:30:06 - train: epoch 0026, iter [04100, 05004], lr: 0.097427, loss: 2.2333
2022-03-01 15:30:40 - train: epoch 0026, iter [04200, 05004], lr: 0.097427, loss: 2.2671
2022-03-01 15:31:14 - train: epoch 0026, iter [04300, 05004], lr: 0.097427, loss: 2.1919
2022-03-01 15:31:46 - train: epoch 0026, iter [04400, 05004], lr: 0.097427, loss: 2.4132
2022-03-01 15:32:19 - train: epoch 0026, iter [04500, 05004], lr: 0.097427, loss: 2.3434
2022-03-01 15:32:53 - train: epoch 0026, iter [04600, 05004], lr: 0.097427, loss: 2.0835
2022-03-01 15:33:27 - train: epoch 0026, iter [04700, 05004], lr: 0.097427, loss: 2.0749
2022-03-01 15:34:00 - train: epoch 0026, iter [04800, 05004], lr: 0.097427, loss: 2.0840
2022-03-01 15:34:34 - train: epoch 0026, iter [04900, 05004], lr: 0.097427, loss: 2.2166
2022-03-01 15:35:06 - train: epoch 0026, iter [05000, 05004], lr: 0.097427, loss: 2.4095
2022-03-01 15:35:08 - train: epoch 026, train_loss: 2.1604
2022-03-01 15:36:22 - eval: epoch: 026, acc1: 54.870%, acc5: 80.276%, test_loss: 1.8911, per_image_load_time: 1.789ms, per_image_inference_time: 0.447ms
2022-03-01 15:36:23 - until epoch: 026, best_acc1: 54.888%
2022-03-01 15:36:23 - epoch 027 lr: 0.09716559066288716
2022-03-01 15:37:02 - train: epoch 0027, iter [00100, 05004], lr: 0.097166, loss: 2.3329
2022-03-01 15:37:35 - train: epoch 0027, iter [00200, 05004], lr: 0.097166, loss: 2.1236
2022-03-01 15:38:08 - train: epoch 0027, iter [00300, 05004], lr: 0.097166, loss: 2.2858
2022-03-01 15:38:42 - train: epoch 0027, iter [00400, 05004], lr: 0.097166, loss: 2.2806
2022-03-01 15:39:15 - train: epoch 0027, iter [00500, 05004], lr: 0.097166, loss: 2.2445
2022-03-01 15:39:49 - train: epoch 0027, iter [00600, 05004], lr: 0.097166, loss: 2.4445
2022-03-01 15:40:22 - train: epoch 0027, iter [00700, 05004], lr: 0.097166, loss: 2.2857
2022-03-01 15:40:56 - train: epoch 0027, iter [00800, 05004], lr: 0.097166, loss: 2.1564
2022-03-01 15:41:30 - train: epoch 0027, iter [00900, 05004], lr: 0.097166, loss: 2.0019
2022-03-01 15:42:03 - train: epoch 0027, iter [01000, 05004], lr: 0.097166, loss: 2.1355
2022-03-01 15:42:37 - train: epoch 0027, iter [01100, 05004], lr: 0.097166, loss: 2.0971
2022-03-01 15:43:09 - train: epoch 0027, iter [01200, 05004], lr: 0.097166, loss: 2.3666
2022-03-01 15:43:41 - train: epoch 0027, iter [01300, 05004], lr: 0.097166, loss: 2.2604
2022-03-01 15:44:15 - train: epoch 0027, iter [01400, 05004], lr: 0.097166, loss: 2.3161
2022-03-01 15:44:48 - train: epoch 0027, iter [01500, 05004], lr: 0.097166, loss: 2.1856
2022-03-01 15:45:21 - train: epoch 0027, iter [01600, 05004], lr: 0.097166, loss: 2.1586
2022-03-01 15:45:55 - train: epoch 0027, iter [01700, 05004], lr: 0.097166, loss: 2.1406
2022-03-01 15:46:29 - train: epoch 0027, iter [01800, 05004], lr: 0.097166, loss: 2.1117
2022-03-01 15:47:02 - train: epoch 0027, iter [01900, 05004], lr: 0.097166, loss: 2.3376
2022-03-01 15:47:35 - train: epoch 0027, iter [02000, 05004], lr: 0.097166, loss: 2.1330
2022-03-01 15:48:09 - train: epoch 0027, iter [02100, 05004], lr: 0.097166, loss: 2.3840
2022-03-01 15:48:40 - train: epoch 0027, iter [02200, 05004], lr: 0.097166, loss: 2.2521
2022-03-01 15:49:14 - train: epoch 0027, iter [02300, 05004], lr: 0.097166, loss: 2.4737
2022-03-01 15:49:48 - train: epoch 0027, iter [02400, 05004], lr: 0.097166, loss: 2.1308
2022-03-01 15:50:21 - train: epoch 0027, iter [02500, 05004], lr: 0.097166, loss: 2.1275
2022-03-01 15:50:55 - train: epoch 0027, iter [02600, 05004], lr: 0.097166, loss: 2.3704
2022-03-01 15:51:28 - train: epoch 0027, iter [02700, 05004], lr: 0.097166, loss: 2.1183
2022-03-01 15:52:02 - train: epoch 0027, iter [02800, 05004], lr: 0.097166, loss: 2.3920
2022-03-01 15:52:35 - train: epoch 0027, iter [02900, 05004], lr: 0.097166, loss: 2.1695
2022-03-01 15:53:09 - train: epoch 0027, iter [03000, 05004], lr: 0.097166, loss: 2.0119
2022-03-01 15:53:42 - train: epoch 0027, iter [03100, 05004], lr: 0.097166, loss: 2.0269
2022-03-01 15:54:14 - train: epoch 0027, iter [03200, 05004], lr: 0.097166, loss: 1.9629
2022-03-01 15:54:47 - train: epoch 0027, iter [03300, 05004], lr: 0.097166, loss: 2.2272
2022-03-01 15:55:20 - train: epoch 0027, iter [03400, 05004], lr: 0.097166, loss: 2.1400
2022-03-01 15:55:54 - train: epoch 0027, iter [03500, 05004], lr: 0.097166, loss: 2.3854
2022-03-01 15:56:28 - train: epoch 0027, iter [03600, 05004], lr: 0.097166, loss: 1.8765
2022-03-01 15:57:01 - train: epoch 0027, iter [03700, 05004], lr: 0.097166, loss: 2.0979
2022-03-01 15:57:35 - train: epoch 0027, iter [03800, 05004], lr: 0.097166, loss: 1.9801
2022-03-01 15:58:09 - train: epoch 0027, iter [03900, 05004], lr: 0.097166, loss: 2.0252
2022-03-01 15:58:42 - train: epoch 0027, iter [04000, 05004], lr: 0.097166, loss: 2.1595
2022-03-01 15:59:16 - train: epoch 0027, iter [04100, 05004], lr: 0.097166, loss: 2.1509
2022-03-01 15:59:47 - train: epoch 0027, iter [04200, 05004], lr: 0.097166, loss: 2.3052
2022-03-01 16:00:20 - train: epoch 0027, iter [04300, 05004], lr: 0.097166, loss: 2.0743
2022-03-01 16:00:55 - train: epoch 0027, iter [04400, 05004], lr: 0.097166, loss: 2.1378
2022-03-01 16:01:27 - train: epoch 0027, iter [04500, 05004], lr: 0.097166, loss: 1.9106
2022-03-01 16:02:02 - train: epoch 0027, iter [04600, 05004], lr: 0.097166, loss: 2.1625
2022-03-01 16:02:35 - train: epoch 0027, iter [04700, 05004], lr: 0.097166, loss: 2.2521
2022-03-01 16:03:09 - train: epoch 0027, iter [04800, 05004], lr: 0.097166, loss: 2.3290
2022-03-01 16:03:43 - train: epoch 0027, iter [04900, 05004], lr: 0.097166, loss: 2.2234
2022-03-01 16:04:15 - train: epoch 0027, iter [05000, 05004], lr: 0.097166, loss: 1.8250
2022-03-01 16:04:16 - train: epoch 027, train_loss: 2.1552
2022-03-01 16:05:30 - eval: epoch: 027, acc1: 54.680%, acc5: 79.644%, test_loss: 1.9319, per_image_load_time: 1.931ms, per_image_inference_time: 0.506ms
2022-03-01 16:05:31 - until epoch: 027, best_acc1: 54.888%
2022-03-01 16:05:31 - epoch 028 lr: 0.09689211739666023
2022-03-01 16:06:10 - train: epoch 0028, iter [00100, 05004], lr: 0.096892, loss: 1.8546
2022-03-01 16:06:44 - train: epoch 0028, iter [00200, 05004], lr: 0.096892, loss: 2.1133
2022-03-01 16:07:17 - train: epoch 0028, iter [00300, 05004], lr: 0.096892, loss: 2.0309
2022-03-01 16:07:51 - train: epoch 0028, iter [00400, 05004], lr: 0.096892, loss: 1.9859
2022-03-01 16:08:24 - train: epoch 0028, iter [00500, 05004], lr: 0.096892, loss: 2.0933
2022-03-01 16:08:59 - train: epoch 0028, iter [00600, 05004], lr: 0.096892, loss: 2.2524
2022-03-01 16:09:32 - train: epoch 0028, iter [00700, 05004], lr: 0.096892, loss: 2.4193
2022-03-01 16:10:06 - train: epoch 0028, iter [00800, 05004], lr: 0.096892, loss: 1.8521
2022-03-01 16:10:38 - train: epoch 0028, iter [00900, 05004], lr: 0.096892, loss: 1.9444
2022-03-01 16:11:11 - train: epoch 0028, iter [01000, 05004], lr: 0.096892, loss: 2.2274
2022-03-01 16:11:45 - train: epoch 0028, iter [01100, 05004], lr: 0.096892, loss: 2.0977
2022-03-01 16:12:19 - train: epoch 0028, iter [01200, 05004], lr: 0.096892, loss: 2.0611
2022-03-01 16:12:53 - train: epoch 0028, iter [01300, 05004], lr: 0.096892, loss: 2.0981
2022-03-01 16:13:26 - train: epoch 0028, iter [01400, 05004], lr: 0.096892, loss: 2.4513
2022-03-01 16:14:01 - train: epoch 0028, iter [01500, 05004], lr: 0.096892, loss: 2.1244
2022-03-01 16:14:35 - train: epoch 0028, iter [01600, 05004], lr: 0.096892, loss: 2.0989
2022-03-01 16:15:08 - train: epoch 0028, iter [01700, 05004], lr: 0.096892, loss: 2.0370
2022-03-01 16:15:42 - train: epoch 0028, iter [01800, 05004], lr: 0.096892, loss: 2.1053
2022-03-01 16:16:14 - train: epoch 0028, iter [01900, 05004], lr: 0.096892, loss: 2.0730
2022-03-01 16:16:47 - train: epoch 0028, iter [02000, 05004], lr: 0.096892, loss: 2.3496
2022-03-01 16:17:20 - train: epoch 0028, iter [02100, 05004], lr: 0.096892, loss: 2.1874
2022-03-01 16:17:54 - train: epoch 0028, iter [02200, 05004], lr: 0.096892, loss: 2.1655
2022-03-01 16:18:28 - train: epoch 0028, iter [02300, 05004], lr: 0.096892, loss: 2.3886
2022-03-01 16:19:02 - train: epoch 0028, iter [02400, 05004], lr: 0.096892, loss: 2.3830
2022-03-01 16:19:36 - train: epoch 0028, iter [02500, 05004], lr: 0.096892, loss: 2.1278
2022-03-01 16:20:09 - train: epoch 0028, iter [02600, 05004], lr: 0.096892, loss: 1.9918
2022-03-01 16:20:43 - train: epoch 0028, iter [02700, 05004], lr: 0.096892, loss: 2.0473
2022-03-01 16:21:17 - train: epoch 0028, iter [02800, 05004], lr: 0.096892, loss: 2.1839
2022-03-01 16:21:50 - train: epoch 0028, iter [02900, 05004], lr: 0.096892, loss: 2.1185
2022-03-01 16:22:22 - train: epoch 0028, iter [03000, 05004], lr: 0.096892, loss: 2.1944
2022-03-01 16:22:55 - train: epoch 0028, iter [03100, 05004], lr: 0.096892, loss: 2.3276
2022-03-01 16:23:29 - train: epoch 0028, iter [03200, 05004], lr: 0.096892, loss: 2.2191
2022-03-01 16:24:03 - train: epoch 0028, iter [03300, 05004], lr: 0.096892, loss: 2.1945
2022-03-01 16:24:37 - train: epoch 0028, iter [03400, 05004], lr: 0.096892, loss: 2.1036
2022-03-01 16:25:10 - train: epoch 0028, iter [03500, 05004], lr: 0.096892, loss: 2.1804
2022-03-01 16:25:43 - train: epoch 0028, iter [03600, 05004], lr: 0.096892, loss: 2.0795
2022-03-01 16:26:17 - train: epoch 0028, iter [03700, 05004], lr: 0.096892, loss: 2.1571
2022-03-01 16:26:50 - train: epoch 0028, iter [03800, 05004], lr: 0.096892, loss: 1.8388
2022-03-01 16:27:23 - train: epoch 0028, iter [03900, 05004], lr: 0.096892, loss: 2.0056
2022-03-01 16:27:56 - train: epoch 0028, iter [04000, 05004], lr: 0.096892, loss: 2.3078
2022-03-01 16:28:29 - train: epoch 0028, iter [04100, 05004], lr: 0.096892, loss: 2.2822
2022-03-01 16:29:02 - train: epoch 0028, iter [04200, 05004], lr: 0.096892, loss: 2.3178
2022-03-01 16:29:36 - train: epoch 0028, iter [04300, 05004], lr: 0.096892, loss: 2.0607
2022-03-01 16:30:09 - train: epoch 0028, iter [04400, 05004], lr: 0.096892, loss: 2.1173
2022-03-01 16:30:43 - train: epoch 0028, iter [04500, 05004], lr: 0.096892, loss: 2.1301
2022-03-01 16:31:16 - train: epoch 0028, iter [04600, 05004], lr: 0.096892, loss: 2.2256
2022-03-01 16:31:50 - train: epoch 0028, iter [04700, 05004], lr: 0.096892, loss: 2.1611
2022-03-01 16:32:23 - train: epoch 0028, iter [04800, 05004], lr: 0.096892, loss: 2.0867
2022-03-01 16:32:56 - train: epoch 0028, iter [04900, 05004], lr: 0.096892, loss: 1.9582
2022-03-01 16:33:27 - train: epoch 0028, iter [05000, 05004], lr: 0.096892, loss: 2.0672
2022-03-01 16:33:28 - train: epoch 028, train_loss: 2.1464
2022-03-01 16:34:43 - eval: epoch: 028, acc1: 55.758%, acc5: 80.346%, test_loss: 1.8823, per_image_load_time: 2.336ms, per_image_inference_time: 0.438ms
2022-03-01 16:34:44 - until epoch: 028, best_acc1: 55.758%
2022-03-01 16:34:44 - epoch 029 lr: 0.0966064732786784
2022-03-01 16:35:23 - train: epoch 0029, iter [00100, 05004], lr: 0.096606, loss: 2.2275
2022-03-01 16:35:57 - train: epoch 0029, iter [00200, 05004], lr: 0.096606, loss: 2.2378
2022-03-01 16:36:32 - train: epoch 0029, iter [00300, 05004], lr: 0.096606, loss: 2.3086
2022-03-01 16:37:05 - train: epoch 0029, iter [00400, 05004], lr: 0.096606, loss: 1.9538
2022-03-01 16:37:39 - train: epoch 0029, iter [00500, 05004], lr: 0.096606, loss: 2.1297
2022-03-01 16:38:13 - train: epoch 0029, iter [00600, 05004], lr: 0.096606, loss: 2.4705
2022-03-01 16:38:45 - train: epoch 0029, iter [00700, 05004], lr: 0.096606, loss: 1.6523
2022-03-01 16:39:19 - train: epoch 0029, iter [00800, 05004], lr: 0.096606, loss: 2.3325
2022-03-01 16:39:52 - train: epoch 0029, iter [00900, 05004], lr: 0.096606, loss: 2.0670
2022-03-01 16:40:26 - train: epoch 0029, iter [01000, 05004], lr: 0.096606, loss: 1.9695
2022-03-01 16:41:00 - train: epoch 0029, iter [01100, 05004], lr: 0.096606, loss: 2.0787
2022-03-01 16:41:34 - train: epoch 0029, iter [01200, 05004], lr: 0.096606, loss: 2.2215
2022-03-01 16:42:08 - train: epoch 0029, iter [01300, 05004], lr: 0.096606, loss: 2.2012
2022-03-01 16:42:41 - train: epoch 0029, iter [01400, 05004], lr: 0.096606, loss: 2.3373
2022-03-01 16:43:16 - train: epoch 0029, iter [01500, 05004], lr: 0.096606, loss: 2.2265
2022-03-01 16:43:49 - train: epoch 0029, iter [01600, 05004], lr: 0.096606, loss: 2.1198
2022-03-01 16:44:20 - train: epoch 0029, iter [01700, 05004], lr: 0.096606, loss: 2.1739
2022-03-01 16:44:54 - train: epoch 0029, iter [01800, 05004], lr: 0.096606, loss: 2.2001
2022-03-01 16:45:28 - train: epoch 0029, iter [01900, 05004], lr: 0.096606, loss: 1.9636
2022-03-01 16:46:01 - train: epoch 0029, iter [02000, 05004], lr: 0.096606, loss: 2.3257
2022-03-01 16:46:34 - train: epoch 0029, iter [02100, 05004], lr: 0.096606, loss: 2.0479
2022-03-01 16:47:09 - train: epoch 0029, iter [02200, 05004], lr: 0.096606, loss: 2.1897
2022-03-01 16:47:43 - train: epoch 0029, iter [02300, 05004], lr: 0.096606, loss: 2.1545
2022-03-01 16:48:16 - train: epoch 0029, iter [02400, 05004], lr: 0.096606, loss: 2.0454
2022-03-01 16:48:49 - train: epoch 0029, iter [02500, 05004], lr: 0.096606, loss: 2.0119
2022-03-01 16:49:23 - train: epoch 0029, iter [02600, 05004], lr: 0.096606, loss: 2.0397
2022-03-01 16:49:54 - train: epoch 0029, iter [02700, 05004], lr: 0.096606, loss: 1.9912
2022-03-01 16:50:29 - train: epoch 0029, iter [02800, 05004], lr: 0.096606, loss: 2.1513
2022-03-01 16:51:02 - train: epoch 0029, iter [02900, 05004], lr: 0.096606, loss: 2.0945
2022-03-01 16:51:36 - train: epoch 0029, iter [03000, 05004], lr: 0.096606, loss: 2.1025
2022-03-01 16:52:09 - train: epoch 0029, iter [03100, 05004], lr: 0.096606, loss: 2.0828
2022-03-01 16:52:43 - train: epoch 0029, iter [03200, 05004], lr: 0.096606, loss: 2.2028
2022-03-01 16:53:17 - train: epoch 0029, iter [03300, 05004], lr: 0.096606, loss: 2.1192
2022-03-01 16:53:50 - train: epoch 0029, iter [03400, 05004], lr: 0.096606, loss: 2.0157
2022-03-01 16:54:24 - train: epoch 0029, iter [03500, 05004], lr: 0.096606, loss: 2.3669
2022-03-01 16:54:57 - train: epoch 0029, iter [03600, 05004], lr: 0.096606, loss: 2.0893
2022-03-01 16:55:29 - train: epoch 0029, iter [03700, 05004], lr: 0.096606, loss: 2.1976
2022-03-01 16:56:03 - train: epoch 0029, iter [03800, 05004], lr: 0.096606, loss: 2.0438
2022-03-01 16:56:36 - train: epoch 0029, iter [03900, 05004], lr: 0.096606, loss: 2.0127
2022-03-01 16:57:10 - train: epoch 0029, iter [04000, 05004], lr: 0.096606, loss: 2.0619
2022-03-01 16:57:44 - train: epoch 0029, iter [04100, 05004], lr: 0.096606, loss: 2.1159
2022-03-01 16:58:18 - train: epoch 0029, iter [04200, 05004], lr: 0.096606, loss: 1.9580
2022-03-01 16:58:51 - train: epoch 0029, iter [04300, 05004], lr: 0.096606, loss: 2.2136
2022-03-01 16:59:25 - train: epoch 0029, iter [04400, 05004], lr: 0.096606, loss: 2.1562
2022-03-01 16:59:58 - train: epoch 0029, iter [04500, 05004], lr: 0.096606, loss: 2.2637
2022-03-01 17:00:32 - train: epoch 0029, iter [04600, 05004], lr: 0.096606, loss: 2.3387
2022-03-01 17:01:04 - train: epoch 0029, iter [04700, 05004], lr: 0.096606, loss: 1.9615
2022-03-01 17:01:37 - train: epoch 0029, iter [04800, 05004], lr: 0.096606, loss: 2.2130
2022-03-01 17:02:12 - train: epoch 0029, iter [04900, 05004], lr: 0.096606, loss: 2.5221
2022-03-01 17:02:44 - train: epoch 0029, iter [05000, 05004], lr: 0.096606, loss: 1.7609
2022-03-01 17:02:45 - train: epoch 029, train_loss: 2.1437
2022-03-01 17:04:00 - eval: epoch: 029, acc1: 52.308%, acc5: 77.902%, test_loss: 2.0546, per_image_load_time: 2.483ms, per_image_inference_time: 0.452ms
2022-03-01 17:04:01 - until epoch: 029, best_acc1: 55.758%
2022-03-01 17:04:01 - epoch 030 lr: 0.09630873244788883
2022-03-01 17:04:40 - train: epoch 0030, iter [00100, 05004], lr: 0.096309, loss: 2.2521
2022-03-01 17:05:14 - train: epoch 0030, iter [00200, 05004], lr: 0.096309, loss: 2.2914
2022-03-01 17:05:47 - train: epoch 0030, iter [00300, 05004], lr: 0.096309, loss: 2.0551
2022-03-01 17:06:20 - train: epoch 0030, iter [00400, 05004], lr: 0.096309, loss: 1.8431
2022-03-01 17:06:53 - train: epoch 0030, iter [00500, 05004], lr: 0.096309, loss: 2.4086
2022-03-01 17:07:26 - train: epoch 0030, iter [00600, 05004], lr: 0.096309, loss: 1.9345
2022-03-01 17:08:00 - train: epoch 0030, iter [00700, 05004], lr: 0.096309, loss: 2.1614
2022-03-01 17:08:34 - train: epoch 0030, iter [00800, 05004], lr: 0.096309, loss: 2.2845
2022-03-01 17:09:08 - train: epoch 0030, iter [00900, 05004], lr: 0.096309, loss: 2.1085
2022-03-01 17:09:42 - train: epoch 0030, iter [01000, 05004], lr: 0.096309, loss: 1.9125
2022-03-01 17:10:16 - train: epoch 0030, iter [01100, 05004], lr: 0.096309, loss: 1.9814
2022-03-01 17:10:50 - train: epoch 0030, iter [01200, 05004], lr: 0.096309, loss: 2.1641
2022-03-01 17:11:23 - train: epoch 0030, iter [01300, 05004], lr: 0.096309, loss: 1.9646
2022-03-01 17:11:56 - train: epoch 0030, iter [01400, 05004], lr: 0.096309, loss: 2.1140
2022-03-01 17:12:28 - train: epoch 0030, iter [01500, 05004], lr: 0.096309, loss: 2.0769
2022-03-01 17:13:02 - train: epoch 0030, iter [01600, 05004], lr: 0.096309, loss: 2.1897
2022-03-01 17:13:36 - train: epoch 0030, iter [01700, 05004], lr: 0.096309, loss: 2.2908
2022-03-01 17:14:09 - train: epoch 0030, iter [01800, 05004], lr: 0.096309, loss: 2.1538
2022-03-01 17:14:44 - train: epoch 0030, iter [01900, 05004], lr: 0.096309, loss: 2.3083
2022-03-01 17:15:17 - train: epoch 0030, iter [02000, 05004], lr: 0.096309, loss: 2.1581
2022-03-01 17:15:50 - train: epoch 0030, iter [02100, 05004], lr: 0.096309, loss: 2.2208
2022-03-01 17:16:23 - train: epoch 0030, iter [02200, 05004], lr: 0.096309, loss: 2.0603
2022-03-01 17:16:57 - train: epoch 0030, iter [02300, 05004], lr: 0.096309, loss: 2.1303
2022-03-01 17:17:30 - train: epoch 0030, iter [02400, 05004], lr: 0.096309, loss: 2.2998
2022-03-01 17:18:02 - train: epoch 0030, iter [02500, 05004], lr: 0.096309, loss: 2.0755
2022-03-01 17:18:36 - train: epoch 0030, iter [02600, 05004], lr: 0.096309, loss: 2.0974
2022-03-01 17:19:09 - train: epoch 0030, iter [02700, 05004], lr: 0.096309, loss: 2.0221
2022-03-01 17:19:44 - train: epoch 0030, iter [02800, 05004], lr: 0.096309, loss: 2.1259
2022-03-01 17:20:17 - train: epoch 0030, iter [02900, 05004], lr: 0.096309, loss: 2.1989
2022-03-01 17:20:51 - train: epoch 0030, iter [03000, 05004], lr: 0.096309, loss: 2.3463
2022-03-01 17:21:24 - train: epoch 0030, iter [03100, 05004], lr: 0.096309, loss: 2.1265
2022-03-01 17:21:58 - train: epoch 0030, iter [03200, 05004], lr: 0.096309, loss: 2.0299
2022-03-01 17:22:31 - train: epoch 0030, iter [03300, 05004], lr: 0.096309, loss: 2.2842
2022-03-01 17:23:04 - train: epoch 0030, iter [03400, 05004], lr: 0.096309, loss: 2.1330
2022-03-01 17:23:37 - train: epoch 0030, iter [03500, 05004], lr: 0.096309, loss: 2.2906
2022-03-01 17:24:11 - train: epoch 0030, iter [03600, 05004], lr: 0.096309, loss: 1.9689
2022-03-01 17:24:45 - train: epoch 0030, iter [03700, 05004], lr: 0.096309, loss: 2.0970
2022-03-01 17:25:18 - train: epoch 0030, iter [03800, 05004], lr: 0.096309, loss: 2.2883
2022-03-01 17:25:53 - train: epoch 0030, iter [03900, 05004], lr: 0.096309, loss: 2.2678
2022-03-01 17:26:26 - train: epoch 0030, iter [04000, 05004], lr: 0.096309, loss: 2.0400
2022-03-01 17:27:00 - train: epoch 0030, iter [04100, 05004], lr: 0.096309, loss: 2.0528
2022-03-01 17:27:34 - train: epoch 0030, iter [04200, 05004], lr: 0.096309, loss: 2.4354
2022-03-01 17:28:07 - train: epoch 0030, iter [04300, 05004], lr: 0.096309, loss: 2.0798
2022-03-01 17:28:39 - train: epoch 0030, iter [04400, 05004], lr: 0.096309, loss: 2.2228
2022-03-01 17:29:13 - train: epoch 0030, iter [04500, 05004], lr: 0.096309, loss: 2.2594
2022-03-01 17:29:46 - train: epoch 0030, iter [04600, 05004], lr: 0.096309, loss: 1.9257
2022-03-01 17:30:20 - train: epoch 0030, iter [04700, 05004], lr: 0.096309, loss: 2.1783
2022-03-01 17:30:54 - train: epoch 0030, iter [04800, 05004], lr: 0.096309, loss: 2.1299
2022-03-01 17:31:27 - train: epoch 0030, iter [04900, 05004], lr: 0.096309, loss: 2.2812
2022-03-01 17:32:00 - train: epoch 0030, iter [05000, 05004], lr: 0.096309, loss: 2.2072
2022-03-01 17:32:01 - train: epoch 030, train_loss: 2.1419
2022-03-01 17:33:17 - eval: epoch: 030, acc1: 54.992%, acc5: 79.878%, test_loss: 1.9033, per_image_load_time: 2.446ms, per_image_inference_time: 0.450ms
2022-03-01 17:33:18 - until epoch: 030, best_acc1: 55.758%
2022-03-01 17:33:18 - epoch 031 lr: 0.09599897218294122
2022-03-01 17:33:56 - train: epoch 0031, iter [00100, 05004], lr: 0.095999, loss: 2.2225
2022-03-01 17:34:29 - train: epoch 0031, iter [00200, 05004], lr: 0.095999, loss: 2.0668
2022-03-01 17:35:02 - train: epoch 0031, iter [00300, 05004], lr: 0.095999, loss: 1.9506
2022-03-01 17:35:36 - train: epoch 0031, iter [00400, 05004], lr: 0.095999, loss: 2.2157
2022-03-01 17:36:10 - train: epoch 0031, iter [00500, 05004], lr: 0.095999, loss: 2.0221
2022-03-01 17:36:44 - train: epoch 0031, iter [00600, 05004], lr: 0.095999, loss: 2.1744
2022-03-01 17:37:18 - train: epoch 0031, iter [00700, 05004], lr: 0.095999, loss: 2.2065
2022-03-01 17:37:51 - train: epoch 0031, iter [00800, 05004], lr: 0.095999, loss: 2.0201
2022-03-01 17:38:25 - train: epoch 0031, iter [00900, 05004], lr: 0.095999, loss: 2.1420
2022-03-01 17:38:59 - train: epoch 0031, iter [01000, 05004], lr: 0.095999, loss: 2.4717
2022-03-01 17:39:32 - train: epoch 0031, iter [01100, 05004], lr: 0.095999, loss: 2.3349
2022-03-01 17:40:04 - train: epoch 0031, iter [01200, 05004], lr: 0.095999, loss: 2.1850
2022-03-01 17:40:38 - train: epoch 0031, iter [01300, 05004], lr: 0.095999, loss: 1.8389
2022-03-01 17:41:12 - train: epoch 0031, iter [01400, 05004], lr: 0.095999, loss: 2.1587
2022-03-01 17:41:45 - train: epoch 0031, iter [01500, 05004], lr: 0.095999, loss: 2.2711
2022-03-01 17:42:19 - train: epoch 0031, iter [01600, 05004], lr: 0.095999, loss: 2.0036
2022-03-01 17:42:53 - train: epoch 0031, iter [01700, 05004], lr: 0.095999, loss: 1.9836
2022-03-01 17:43:27 - train: epoch 0031, iter [01800, 05004], lr: 0.095999, loss: 2.0834
2022-03-01 17:44:00 - train: epoch 0031, iter [01900, 05004], lr: 0.095999, loss: 2.1150
2022-03-01 17:44:34 - train: epoch 0031, iter [02000, 05004], lr: 0.095999, loss: 2.1828
2022-03-01 17:45:07 - train: epoch 0031, iter [02100, 05004], lr: 0.095999, loss: 1.9077
2022-03-01 17:45:38 - train: epoch 0031, iter [02200, 05004], lr: 0.095999, loss: 2.0516
2022-03-01 17:46:13 - train: epoch 0031, iter [02300, 05004], lr: 0.095999, loss: 1.8875
2022-03-01 17:46:46 - train: epoch 0031, iter [02400, 05004], lr: 0.095999, loss: 2.1834
2022-03-01 17:47:20 - train: epoch 0031, iter [02500, 05004], lr: 0.095999, loss: 2.0343
2022-03-01 17:47:53 - train: epoch 0031, iter [02600, 05004], lr: 0.095999, loss: 2.0162
2022-03-01 17:48:28 - train: epoch 0031, iter [02700, 05004], lr: 0.095999, loss: 2.2293
2022-03-01 17:49:01 - train: epoch 0031, iter [02800, 05004], lr: 0.095999, loss: 2.4779
2022-03-01 17:49:35 - train: epoch 0031, iter [02900, 05004], lr: 0.095999, loss: 2.1783
2022-03-01 17:50:08 - train: epoch 0031, iter [03000, 05004], lr: 0.095999, loss: 2.2804
2022-03-01 17:50:40 - train: epoch 0031, iter [03100, 05004], lr: 0.095999, loss: 2.1195
2022-03-01 17:51:13 - train: epoch 0031, iter [03200, 05004], lr: 0.095999, loss: 2.2652
2022-03-01 17:51:46 - train: epoch 0031, iter [03300, 05004], lr: 0.095999, loss: 2.1967
2022-03-01 17:52:20 - train: epoch 0031, iter [03400, 05004], lr: 0.095999, loss: 2.2473
2022-03-01 17:52:53 - train: epoch 0031, iter [03500, 05004], lr: 0.095999, loss: 2.3343
2022-03-01 17:53:28 - train: epoch 0031, iter [03600, 05004], lr: 0.095999, loss: 2.1616
2022-03-01 17:54:01 - train: epoch 0031, iter [03700, 05004], lr: 0.095999, loss: 2.0359
2022-03-01 17:54:35 - train: epoch 0031, iter [03800, 05004], lr: 0.095999, loss: 1.9028
2022-03-01 17:55:09 - train: epoch 0031, iter [03900, 05004], lr: 0.095999, loss: 2.1802
2022-03-01 17:55:42 - train: epoch 0031, iter [04000, 05004], lr: 0.095999, loss: 2.0924
2022-03-01 17:56:15 - train: epoch 0031, iter [04100, 05004], lr: 0.095999, loss: 1.9931
2022-03-01 17:56:48 - train: epoch 0031, iter [04200, 05004], lr: 0.095999, loss: 2.1617
2022-03-01 17:57:21 - train: epoch 0031, iter [04300, 05004], lr: 0.095999, loss: 2.0473
2022-03-01 17:57:55 - train: epoch 0031, iter [04400, 05004], lr: 0.095999, loss: 2.2597
2022-03-01 17:58:28 - train: epoch 0031, iter [04500, 05004], lr: 0.095999, loss: 2.4313
2022-03-01 17:59:02 - train: epoch 0031, iter [04600, 05004], lr: 0.095999, loss: 2.2173
2022-03-01 17:59:36 - train: epoch 0031, iter [04700, 05004], lr: 0.095999, loss: 2.1734
2022-03-01 18:00:09 - train: epoch 0031, iter [04800, 05004], lr: 0.095999, loss: 2.1260
2022-03-01 18:00:43 - train: epoch 0031, iter [04900, 05004], lr: 0.095999, loss: 1.9118
2022-03-01 18:01:15 - train: epoch 0031, iter [05000, 05004], lr: 0.095999, loss: 2.1480
2022-03-01 18:01:16 - train: epoch 031, train_loss: 2.1345
2022-03-01 18:02:30 - eval: epoch: 031, acc1: 52.046%, acc5: 77.380%, test_loss: 2.0788, per_image_load_time: 1.195ms, per_image_inference_time: 0.486ms
2022-03-01 18:02:31 - until epoch: 031, best_acc1: 55.758%
2022-03-01 18:02:31 - epoch 032 lr: 0.09567727288213004
2022-03-01 18:03:10 - train: epoch 0032, iter [00100, 05004], lr: 0.095677, loss: 2.0018
2022-03-01 18:03:44 - train: epoch 0032, iter [00200, 05004], lr: 0.095677, loss: 2.2246
2022-03-01 18:04:19 - train: epoch 0032, iter [00300, 05004], lr: 0.095677, loss: 2.0379
2022-03-01 18:04:52 - train: epoch 0032, iter [00400, 05004], lr: 0.095677, loss: 2.2426
2022-03-01 18:05:26 - train: epoch 0032, iter [00500, 05004], lr: 0.095677, loss: 2.1335
2022-03-01 18:05:59 - train: epoch 0032, iter [00600, 05004], lr: 0.095677, loss: 2.1565
2022-03-01 18:06:34 - train: epoch 0032, iter [00700, 05004], lr: 0.095677, loss: 2.1487
2022-03-01 18:07:06 - train: epoch 0032, iter [00800, 05004], lr: 0.095677, loss: 2.1791
2022-03-01 18:07:39 - train: epoch 0032, iter [00900, 05004], lr: 0.095677, loss: 2.0468
2022-03-01 18:08:11 - train: epoch 0032, iter [01000, 05004], lr: 0.095677, loss: 2.1690
2022-03-01 18:08:46 - train: epoch 0032, iter [01100, 05004], lr: 0.095677, loss: 2.2924
2022-03-01 18:09:20 - train: epoch 0032, iter [01200, 05004], lr: 0.095677, loss: 2.2274
2022-03-01 18:09:54 - train: epoch 0032, iter [01300, 05004], lr: 0.095677, loss: 2.1193
2022-03-01 18:10:28 - train: epoch 0032, iter [01400, 05004], lr: 0.095677, loss: 2.2329
2022-03-01 18:11:01 - train: epoch 0032, iter [01500, 05004], lr: 0.095677, loss: 2.2202
2022-03-01 18:11:34 - train: epoch 0032, iter [01600, 05004], lr: 0.095677, loss: 2.1817
2022-03-01 18:12:08 - train: epoch 0032, iter [01700, 05004], lr: 0.095677, loss: 2.2647
2022-03-01 18:12:41 - train: epoch 0032, iter [01800, 05004], lr: 0.095677, loss: 2.3946
2022-03-01 18:13:14 - train: epoch 0032, iter [01900, 05004], lr: 0.095677, loss: 1.9984
2022-03-01 18:13:46 - train: epoch 0032, iter [02000, 05004], lr: 0.095677, loss: 2.0648
2022-03-01 18:14:20 - train: epoch 0032, iter [02100, 05004], lr: 0.095677, loss: 1.9836
2022-03-01 18:14:54 - train: epoch 0032, iter [02200, 05004], lr: 0.095677, loss: 1.9816
2022-03-01 18:15:27 - train: epoch 0032, iter [02300, 05004], lr: 0.095677, loss: 2.1899
2022-03-01 18:16:00 - train: epoch 0032, iter [02400, 05004], lr: 0.095677, loss: 2.0184
2022-03-01 18:16:34 - train: epoch 0032, iter [02500, 05004], lr: 0.095677, loss: 2.3834
2022-03-01 18:17:08 - train: epoch 0032, iter [02600, 05004], lr: 0.095677, loss: 1.8793
2022-03-01 18:17:41 - train: epoch 0032, iter [02700, 05004], lr: 0.095677, loss: 2.1047
2022-03-01 18:18:15 - train: epoch 0032, iter [02800, 05004], lr: 0.095677, loss: 2.2545
2022-03-01 18:18:47 - train: epoch 0032, iter [02900, 05004], lr: 0.095677, loss: 1.9756
2022-03-01 18:19:20 - train: epoch 0032, iter [03000, 05004], lr: 0.095677, loss: 1.9846
2022-03-01 18:19:54 - train: epoch 0032, iter [03100, 05004], lr: 0.095677, loss: 2.2499
2022-03-01 18:20:27 - train: epoch 0032, iter [03200, 05004], lr: 0.095677, loss: 2.2478
2022-03-01 18:21:01 - train: epoch 0032, iter [03300, 05004], lr: 0.095677, loss: 2.0370
2022-03-01 18:21:35 - train: epoch 0032, iter [03400, 05004], lr: 0.095677, loss: 1.9239
2022-03-01 18:22:08 - train: epoch 0032, iter [03500, 05004], lr: 0.095677, loss: 2.0994
2022-03-01 18:22:42 - train: epoch 0032, iter [03600, 05004], lr: 0.095677, loss: 2.1992
2022-03-01 18:23:15 - train: epoch 0032, iter [03700, 05004], lr: 0.095677, loss: 2.2411
2022-03-01 18:23:49 - train: epoch 0032, iter [03800, 05004], lr: 0.095677, loss: 2.0401
2022-03-01 18:24:21 - train: epoch 0032, iter [03900, 05004], lr: 0.095677, loss: 2.3494
2022-03-01 18:24:54 - train: epoch 0032, iter [04000, 05004], lr: 0.095677, loss: 1.9333
2022-03-01 18:25:27 - train: epoch 0032, iter [04100, 05004], lr: 0.095677, loss: 2.0976
2022-03-01 18:26:02 - train: epoch 0032, iter [04200, 05004], lr: 0.095677, loss: 1.9968
2022-03-01 18:26:35 - train: epoch 0032, iter [04300, 05004], lr: 0.095677, loss: 2.4341
2022-03-01 18:27:09 - train: epoch 0032, iter [04400, 05004], lr: 0.095677, loss: 2.1064
2022-03-01 18:27:42 - train: epoch 0032, iter [04500, 05004], lr: 0.095677, loss: 2.2582
2022-03-01 18:28:16 - train: epoch 0032, iter [04600, 05004], lr: 0.095677, loss: 2.1168
2022-03-01 18:28:50 - train: epoch 0032, iter [04700, 05004], lr: 0.095677, loss: 2.1860
2022-03-01 18:29:23 - train: epoch 0032, iter [04800, 05004], lr: 0.095677, loss: 1.9382
2022-03-01 18:29:54 - train: epoch 0032, iter [04900, 05004], lr: 0.095677, loss: 2.2742
2022-03-01 18:30:26 - train: epoch 0032, iter [05000, 05004], lr: 0.095677, loss: 2.1524
2022-03-01 18:30:28 - train: epoch 032, train_loss: 2.1312
2022-03-01 18:31:43 - eval: epoch: 032, acc1: 56.262%, acc5: 80.886%, test_loss: 1.8609, per_image_load_time: 2.075ms, per_image_inference_time: 0.464ms
2022-03-01 18:31:44 - until epoch: 032, best_acc1: 56.262%
2022-03-01 18:31:44 - epoch 033 lr: 0.09534371804252728
2022-03-01 18:32:22 - train: epoch 0033, iter [00100, 05004], lr: 0.095344, loss: 1.9800
2022-03-01 18:32:57 - train: epoch 0033, iter [00200, 05004], lr: 0.095344, loss: 2.1571
2022-03-01 18:33:30 - train: epoch 0033, iter [00300, 05004], lr: 0.095344, loss: 1.9927
2022-03-01 18:34:04 - train: epoch 0033, iter [00400, 05004], lr: 0.095344, loss: 1.8037
2022-03-01 18:34:38 - train: epoch 0033, iter [00500, 05004], lr: 0.095344, loss: 2.1977
2022-03-01 18:35:10 - train: epoch 0033, iter [00600, 05004], lr: 0.095344, loss: 2.0233
2022-03-01 18:35:43 - train: epoch 0033, iter [00700, 05004], lr: 0.095344, loss: 2.2199
2022-03-01 18:36:17 - train: epoch 0033, iter [00800, 05004], lr: 0.095344, loss: 2.2805
2022-03-01 18:36:51 - train: epoch 0033, iter [00900, 05004], lr: 0.095344, loss: 2.1855
2022-03-01 18:37:25 - train: epoch 0033, iter [01000, 05004], lr: 0.095344, loss: 2.0265
2022-03-01 18:37:58 - train: epoch 0033, iter [01100, 05004], lr: 0.095344, loss: 2.0315
2022-03-01 18:38:32 - train: epoch 0033, iter [01200, 05004], lr: 0.095344, loss: 2.2259
2022-03-01 18:39:07 - train: epoch 0033, iter [01300, 05004], lr: 0.095344, loss: 2.0873
2022-03-01 18:39:41 - train: epoch 0033, iter [01400, 05004], lr: 0.095344, loss: 2.2301
2022-03-01 18:40:14 - train: epoch 0033, iter [01500, 05004], lr: 0.095344, loss: 2.4289
2022-03-01 18:40:47 - train: epoch 0033, iter [01600, 05004], lr: 0.095344, loss: 2.3930
2022-03-01 18:41:19 - train: epoch 0033, iter [01700, 05004], lr: 0.095344, loss: 2.0093
2022-03-01 18:41:52 - train: epoch 0033, iter [01800, 05004], lr: 0.095344, loss: 2.3598
2022-03-01 18:42:26 - train: epoch 0033, iter [01900, 05004], lr: 0.095344, loss: 2.2078
2022-03-01 18:43:00 - train: epoch 0033, iter [02000, 05004], lr: 0.095344, loss: 1.9985
2022-03-01 18:43:33 - train: epoch 0033, iter [02100, 05004], lr: 0.095344, loss: 2.1609
2022-03-01 18:44:07 - train: epoch 0033, iter [02200, 05004], lr: 0.095344, loss: 2.2244
2022-03-01 18:44:41 - train: epoch 0033, iter [02300, 05004], lr: 0.095344, loss: 2.0943
2022-03-01 18:45:15 - train: epoch 0033, iter [02400, 05004], lr: 0.095344, loss: 2.1758
2022-03-01 18:45:49 - train: epoch 0033, iter [02500, 05004], lr: 0.095344, loss: 2.1985
2022-03-01 18:46:21 - train: epoch 0033, iter [02600, 05004], lr: 0.095344, loss: 1.9690
2022-03-01 18:46:53 - train: epoch 0033, iter [02700, 05004], lr: 0.095344, loss: 2.2253
2022-03-01 18:47:27 - train: epoch 0033, iter [02800, 05004], lr: 0.095344, loss: 2.2119
2022-03-01 18:48:00 - train: epoch 0033, iter [02900, 05004], lr: 0.095344, loss: 2.0990
2022-03-01 18:48:34 - train: epoch 0033, iter [03000, 05004], lr: 0.095344, loss: 2.2114
2022-03-01 18:49:08 - train: epoch 0033, iter [03100, 05004], lr: 0.095344, loss: 2.1774
2022-03-01 18:49:41 - train: epoch 0033, iter [03200, 05004], lr: 0.095344, loss: 2.0494
2022-03-01 18:50:15 - train: epoch 0033, iter [03300, 05004], lr: 0.095344, loss: 2.2453
2022-03-01 18:50:49 - train: epoch 0033, iter [03400, 05004], lr: 0.095344, loss: 2.0232
2022-03-01 18:51:23 - train: epoch 0033, iter [03500, 05004], lr: 0.095344, loss: 2.1652
2022-03-01 18:51:56 - train: epoch 0033, iter [03600, 05004], lr: 0.095344, loss: 2.3807
2022-03-01 18:52:28 - train: epoch 0033, iter [03700, 05004], lr: 0.095344, loss: 2.1170
2022-03-01 18:53:02 - train: epoch 0033, iter [03800, 05004], lr: 0.095344, loss: 2.1448
2022-03-01 18:53:35 - train: epoch 0033, iter [03900, 05004], lr: 0.095344, loss: 2.2393
2022-03-01 18:54:10 - train: epoch 0033, iter [04000, 05004], lr: 0.095344, loss: 2.2257
2022-03-01 18:54:43 - train: epoch 0033, iter [04100, 05004], lr: 0.095344, loss: 2.2628
2022-03-01 18:55:17 - train: epoch 0033, iter [04200, 05004], lr: 0.095344, loss: 2.1186
2022-03-01 18:55:49 - train: epoch 0033, iter [04300, 05004], lr: 0.095344, loss: 2.2780
2022-03-01 18:56:23 - train: epoch 0033, iter [04400, 05004], lr: 0.095344, loss: 2.1927
2022-03-01 18:56:57 - train: epoch 0033, iter [04500, 05004], lr: 0.095344, loss: 2.5148
2022-03-01 18:57:29 - train: epoch 0033, iter [04600, 05004], lr: 0.095344, loss: 2.0530
2022-03-01 18:58:02 - train: epoch 0033, iter [04700, 05004], lr: 0.095344, loss: 2.0623
2022-03-01 18:58:35 - train: epoch 0033, iter [04800, 05004], lr: 0.095344, loss: 2.6308
2022-03-01 18:59:09 - train: epoch 0033, iter [04900, 05004], lr: 0.095344, loss: 1.9480
2022-03-01 18:59:42 - train: epoch 0033, iter [05000, 05004], lr: 0.095344, loss: 2.0447
2022-03-01 18:59:43 - train: epoch 033, train_loss: 2.1244
2022-03-01 19:00:59 - eval: epoch: 033, acc1: 48.316%, acc5: 73.904%, test_loss: 2.2787, per_image_load_time: 2.468ms, per_image_inference_time: 0.456ms
2022-03-01 19:01:00 - until epoch: 033, best_acc1: 56.262%
2022-03-01 19:01:00 - epoch 034 lr: 0.09499839423831061
2022-03-01 19:01:39 - train: epoch 0034, iter [00100, 05004], lr: 0.094998, loss: 2.1261
2022-03-01 19:02:13 - train: epoch 0034, iter [00200, 05004], lr: 0.094998, loss: 2.0163
2022-03-01 19:02:47 - train: epoch 0034, iter [00300, 05004], lr: 0.094998, loss: 2.0168
2022-03-01 19:03:18 - train: epoch 0034, iter [00400, 05004], lr: 0.094998, loss: 1.9423
2022-03-01 19:03:52 - train: epoch 0034, iter [00500, 05004], lr: 0.094998, loss: 2.0869
2022-03-01 19:04:25 - train: epoch 0034, iter [00600, 05004], lr: 0.094998, loss: 2.4256
2022-03-01 19:04:59 - train: epoch 0034, iter [00700, 05004], lr: 0.094998, loss: 2.1339
2022-03-01 19:05:33 - train: epoch 0034, iter [00800, 05004], lr: 0.094998, loss: 2.0672
2022-03-01 19:06:07 - train: epoch 0034, iter [00900, 05004], lr: 0.094998, loss: 2.1880
2022-03-01 19:06:41 - train: epoch 0034, iter [01000, 05004], lr: 0.094998, loss: 2.0736
2022-03-01 19:07:14 - train: epoch 0034, iter [01100, 05004], lr: 0.094998, loss: 2.0981
2022-03-01 19:07:49 - train: epoch 0034, iter [01200, 05004], lr: 0.094998, loss: 2.1131
2022-03-01 19:08:22 - train: epoch 0034, iter [01300, 05004], lr: 0.094998, loss: 1.9376
2022-03-01 19:08:55 - train: epoch 0034, iter [01400, 05004], lr: 0.094998, loss: 2.0989
2022-03-01 19:09:28 - train: epoch 0034, iter [01500, 05004], lr: 0.094998, loss: 2.0405
2022-03-01 19:10:02 - train: epoch 0034, iter [01600, 05004], lr: 0.094998, loss: 2.0812
2022-03-01 19:10:35 - train: epoch 0034, iter [01700, 05004], lr: 0.094998, loss: 2.0376
2022-03-01 19:11:09 - train: epoch 0034, iter [01800, 05004], lr: 0.094998, loss: 2.3668
2022-03-01 19:11:43 - train: epoch 0034, iter [01900, 05004], lr: 0.094998, loss: 2.2496
2022-03-01 19:12:16 - train: epoch 0034, iter [02000, 05004], lr: 0.094998, loss: 2.1423
2022-03-01 19:12:51 - train: epoch 0034, iter [02100, 05004], lr: 0.094998, loss: 2.3607
2022-03-01 19:13:24 - train: epoch 0034, iter [02200, 05004], lr: 0.094998, loss: 1.8531
2022-03-01 19:13:58 - train: epoch 0034, iter [02300, 05004], lr: 0.094998, loss: 2.1730
2022-03-01 19:14:30 - train: epoch 0034, iter [02400, 05004], lr: 0.094998, loss: 2.0230
2022-03-01 19:15:03 - train: epoch 0034, iter [02500, 05004], lr: 0.094998, loss: 2.1505
2022-03-01 19:15:37 - train: epoch 0034, iter [02600, 05004], lr: 0.094998, loss: 2.1848
2022-03-01 19:16:10 - train: epoch 0034, iter [02700, 05004], lr: 0.094998, loss: 2.1337
2022-03-01 19:16:44 - train: epoch 0034, iter [02800, 05004], lr: 0.094998, loss: 2.0025
2022-03-01 19:17:18 - train: epoch 0034, iter [02900, 05004], lr: 0.094998, loss: 1.7623
2022-03-01 19:17:51 - train: epoch 0034, iter [03000, 05004], lr: 0.094998, loss: 1.8581
2022-03-01 19:18:24 - train: epoch 0034, iter [03100, 05004], lr: 0.094998, loss: 2.0778
2022-03-01 19:18:59 - train: epoch 0034, iter [03200, 05004], lr: 0.094998, loss: 2.1311
2022-03-01 19:19:32 - train: epoch 0034, iter [03300, 05004], lr: 0.094998, loss: 2.1008
2022-03-01 19:20:05 - train: epoch 0034, iter [03400, 05004], lr: 0.094998, loss: 2.3136
2022-03-01 19:20:37 - train: epoch 0034, iter [03500, 05004], lr: 0.094998, loss: 1.9904
2022-03-01 19:21:10 - train: epoch 0034, iter [03600, 05004], lr: 0.094998, loss: 2.0104
2022-03-01 19:21:44 - train: epoch 0034, iter [03700, 05004], lr: 0.094998, loss: 2.0048
2022-03-01 19:22:17 - train: epoch 0034, iter [03800, 05004], lr: 0.094998, loss: 2.0646
2022-03-01 19:22:52 - train: epoch 0034, iter [03900, 05004], lr: 0.094998, loss: 2.1263
2022-03-01 19:23:26 - train: epoch 0034, iter [04000, 05004], lr: 0.094998, loss: 1.9989
2022-03-01 19:23:59 - train: epoch 0034, iter [04100, 05004], lr: 0.094998, loss: 2.1362
2022-03-01 19:24:33 - train: epoch 0034, iter [04200, 05004], lr: 0.094998, loss: 1.9891
2022-03-01 19:25:07 - train: epoch 0034, iter [04300, 05004], lr: 0.094998, loss: 2.0939
2022-03-01 19:25:39 - train: epoch 0034, iter [04400, 05004], lr: 0.094998, loss: 2.1961
2022-03-01 19:26:12 - train: epoch 0034, iter [04500, 05004], lr: 0.094998, loss: 2.2398
2022-03-01 19:26:46 - train: epoch 0034, iter [04600, 05004], lr: 0.094998, loss: 2.1985
2022-03-01 19:27:19 - train: epoch 0034, iter [04700, 05004], lr: 0.094998, loss: 2.0506
2022-03-01 19:27:53 - train: epoch 0034, iter [04800, 05004], lr: 0.094998, loss: 2.1204
2022-03-01 19:28:27 - train: epoch 0034, iter [04900, 05004], lr: 0.094998, loss: 2.1154
2022-03-01 19:28:59 - train: epoch 0034, iter [05000, 05004], lr: 0.094998, loss: 2.1109
2022-03-01 19:29:00 - train: epoch 034, train_loss: 2.1194
2022-03-01 19:30:15 - eval: epoch: 034, acc1: 52.438%, acc5: 78.010%, test_loss: 2.0443, per_image_load_time: 2.395ms, per_image_inference_time: 0.461ms
2022-03-01 19:30:16 - until epoch: 034, best_acc1: 56.262%
2022-03-01 19:30:16 - epoch 035 lr: 0.09464139109829321
2022-03-01 19:30:54 - train: epoch 0035, iter [00100, 05004], lr: 0.094641, loss: 1.9616
2022-03-01 19:31:28 - train: epoch 0035, iter [00200, 05004], lr: 0.094641, loss: 2.0059
2022-03-01 19:32:00 - train: epoch 0035, iter [00300, 05004], lr: 0.094641, loss: 2.2574
2022-03-01 19:32:34 - train: epoch 0035, iter [00400, 05004], lr: 0.094641, loss: 1.9838
2022-03-01 19:33:08 - train: epoch 0035, iter [00500, 05004], lr: 0.094641, loss: 2.0120
2022-03-01 19:33:42 - train: epoch 0035, iter [00600, 05004], lr: 0.094641, loss: 2.0786
2022-03-01 19:34:15 - train: epoch 0035, iter [00700, 05004], lr: 0.094641, loss: 1.9828
2022-03-01 19:34:49 - train: epoch 0035, iter [00800, 05004], lr: 0.094641, loss: 2.1256
2022-03-01 19:35:23 - train: epoch 0035, iter [00900, 05004], lr: 0.094641, loss: 2.3150
2022-03-01 19:35:58 - train: epoch 0035, iter [01000, 05004], lr: 0.094641, loss: 2.2084
2022-03-01 19:36:30 - train: epoch 0035, iter [01100, 05004], lr: 0.094641, loss: 2.2772
2022-03-01 19:37:02 - train: epoch 0035, iter [01200, 05004], lr: 0.094641, loss: 2.1205
2022-03-01 19:37:36 - train: epoch 0035, iter [01300, 05004], lr: 0.094641, loss: 2.3520
2022-03-01 19:38:10 - train: epoch 0035, iter [01400, 05004], lr: 0.094641, loss: 2.0275
2022-03-01 19:38:44 - train: epoch 0035, iter [01500, 05004], lr: 0.094641, loss: 2.2806
2022-03-01 19:39:18 - train: epoch 0035, iter [01600, 05004], lr: 0.094641, loss: 2.0135
2022-03-01 19:39:52 - train: epoch 0035, iter [01700, 05004], lr: 0.094641, loss: 1.9334
2022-03-01 19:40:25 - train: epoch 0035, iter [01800, 05004], lr: 0.094641, loss: 2.1303
2022-03-01 19:40:59 - train: epoch 0035, iter [01900, 05004], lr: 0.094641, loss: 2.2262
2022-03-01 19:41:33 - train: epoch 0035, iter [02000, 05004], lr: 0.094641, loss: 2.2058
2022-03-01 19:42:05 - train: epoch 0035, iter [02100, 05004], lr: 0.094641, loss: 2.0213
2022-03-01 19:42:37 - train: epoch 0035, iter [02200, 05004], lr: 0.094641, loss: 2.3011
2022-03-01 19:43:11 - train: epoch 0035, iter [02300, 05004], lr: 0.094641, loss: 2.1434
2022-03-01 19:43:45 - train: epoch 0035, iter [02400, 05004], lr: 0.094641, loss: 2.2310
2022-03-01 19:44:19 - train: epoch 0035, iter [02500, 05004], lr: 0.094641, loss: 1.9956
2022-03-01 19:44:53 - train: epoch 0035, iter [02600, 05004], lr: 0.094641, loss: 2.2681
2022-03-01 19:45:26 - train: epoch 0035, iter [02700, 05004], lr: 0.094641, loss: 2.2601
2022-03-01 19:46:00 - train: epoch 0035, iter [02800, 05004], lr: 0.094641, loss: 2.1613
2022-03-01 19:46:33 - train: epoch 0035, iter [02900, 05004], lr: 0.094641, loss: 2.0634
2022-03-01 19:47:07 - train: epoch 0035, iter [03000, 05004], lr: 0.094641, loss: 2.1317
2022-03-01 19:47:39 - train: epoch 0035, iter [03100, 05004], lr: 0.094641, loss: 2.0775
2022-03-01 19:48:12 - train: epoch 0035, iter [03200, 05004], lr: 0.094641, loss: 1.9166
2022-03-01 19:48:45 - train: epoch 0035, iter [03300, 05004], lr: 0.094641, loss: 1.9629
2022-03-01 19:49:18 - train: epoch 0035, iter [03400, 05004], lr: 0.094641, loss: 2.1456
2022-03-01 19:49:52 - train: epoch 0035, iter [03500, 05004], lr: 0.094641, loss: 1.8818
2022-03-01 19:50:26 - train: epoch 0035, iter [03600, 05004], lr: 0.094641, loss: 2.1967
2022-03-01 19:51:00 - train: epoch 0035, iter [03700, 05004], lr: 0.094641, loss: 1.9454
2022-03-01 19:51:34 - train: epoch 0035, iter [03800, 05004], lr: 0.094641, loss: 2.1664
2022-03-01 19:52:08 - train: epoch 0035, iter [03900, 05004], lr: 0.094641, loss: 2.3120
2022-03-01 19:52:41 - train: epoch 0035, iter [04000, 05004], lr: 0.094641, loss: 1.8363
2022-03-01 19:53:13 - train: epoch 0035, iter [04100, 05004], lr: 0.094641, loss: 2.4067
2022-03-01 19:53:46 - train: epoch 0035, iter [04200, 05004], lr: 0.094641, loss: 2.1400
2022-03-01 19:54:20 - train: epoch 0035, iter [04300, 05004], lr: 0.094641, loss: 2.0469
2022-03-01 19:54:53 - train: epoch 0035, iter [04400, 05004], lr: 0.094641, loss: 2.1494
2022-03-01 19:55:27 - train: epoch 0035, iter [04500, 05004], lr: 0.094641, loss: 2.2407
2022-03-01 19:56:00 - train: epoch 0035, iter [04600, 05004], lr: 0.094641, loss: 1.9499
2022-03-01 19:56:33 - train: epoch 0035, iter [04700, 05004], lr: 0.094641, loss: 2.4672
2022-03-01 19:57:07 - train: epoch 0035, iter [04800, 05004], lr: 0.094641, loss: 2.3220
2022-03-01 19:57:41 - train: epoch 0035, iter [04900, 05004], lr: 0.094641, loss: 2.2173
2022-03-01 19:58:13 - train: epoch 0035, iter [05000, 05004], lr: 0.094641, loss: 1.9116
2022-03-01 19:58:14 - train: epoch 035, train_loss: 2.1152
2022-03-01 19:59:29 - eval: epoch: 035, acc1: 55.014%, acc5: 80.250%, test_loss: 1.8981, per_image_load_time: 2.364ms, per_image_inference_time: 0.475ms
2022-03-01 19:59:29 - until epoch: 035, best_acc1: 56.262%
2022-03-01 19:59:29 - epoch 036 lr: 0.0942728012826605
2022-03-01 20:00:09 - train: epoch 0036, iter [00100, 05004], lr: 0.094273, loss: 2.1408
2022-03-01 20:00:42 - train: epoch 0036, iter [00200, 05004], lr: 0.094273, loss: 1.8725
2022-03-01 20:01:16 - train: epoch 0036, iter [00300, 05004], lr: 0.094273, loss: 1.9704
2022-03-01 20:01:49 - train: epoch 0036, iter [00400, 05004], lr: 0.094273, loss: 2.1087
2022-03-01 20:02:24 - train: epoch 0036, iter [00500, 05004], lr: 0.094273, loss: 2.0604
2022-03-01 20:02:58 - train: epoch 0036, iter [00600, 05004], lr: 0.094273, loss: 1.9899
2022-03-01 20:03:32 - train: epoch 0036, iter [00700, 05004], lr: 0.094273, loss: 1.8749
2022-03-01 20:04:05 - train: epoch 0036, iter [00800, 05004], lr: 0.094273, loss: 1.9367
2022-03-01 20:04:38 - train: epoch 0036, iter [00900, 05004], lr: 0.094273, loss: 2.0841
2022-03-01 20:05:10 - train: epoch 0036, iter [01000, 05004], lr: 0.094273, loss: 2.1257
2022-03-01 20:05:44 - train: epoch 0036, iter [01100, 05004], lr: 0.094273, loss: 2.2694
2022-03-01 20:06:18 - train: epoch 0036, iter [01200, 05004], lr: 0.094273, loss: 2.1658
2022-03-01 20:06:52 - train: epoch 0036, iter [01300, 05004], lr: 0.094273, loss: 2.1260
2022-03-01 20:07:26 - train: epoch 0036, iter [01400, 05004], lr: 0.094273, loss: 2.2444
2022-03-01 20:08:00 - train: epoch 0036, iter [01500, 05004], lr: 0.094273, loss: 2.3542
2022-03-01 20:08:35 - train: epoch 0036, iter [01600, 05004], lr: 0.094273, loss: 2.1866
2022-03-01 20:09:08 - train: epoch 0036, iter [01700, 05004], lr: 0.094273, loss: 2.0059
2022-03-01 20:09:41 - train: epoch 0036, iter [01800, 05004], lr: 0.094273, loss: 1.9483
2022-03-01 20:10:13 - train: epoch 0036, iter [01900, 05004], lr: 0.094273, loss: 2.2299
2022-03-01 20:10:47 - train: epoch 0036, iter [02000, 05004], lr: 0.094273, loss: 2.0029
2022-03-01 20:11:21 - train: epoch 0036, iter [02100, 05004], lr: 0.094273, loss: 2.1335
2022-03-01 20:11:55 - train: epoch 0036, iter [02200, 05004], lr: 0.094273, loss: 2.0579
2022-03-01 20:12:28 - train: epoch 0036, iter [02300, 05004], lr: 0.094273, loss: 2.1441
2022-03-01 20:13:02 - train: epoch 0036, iter [02400, 05004], lr: 0.094273, loss: 2.2025
2022-03-01 20:13:36 - train: epoch 0036, iter [02500, 05004], lr: 0.094273, loss: 1.9795
2022-03-01 20:14:10 - train: epoch 0036, iter [02600, 05004], lr: 0.094273, loss: 2.2320
2022-03-01 20:14:43 - train: epoch 0036, iter [02700, 05004], lr: 0.094273, loss: 1.9804
2022-03-01 20:15:16 - train: epoch 0036, iter [02800, 05004], lr: 0.094273, loss: 2.0317
2022-03-01 20:15:47 - train: epoch 0036, iter [02900, 05004], lr: 0.094273, loss: 1.8497
2022-03-01 20:16:22 - train: epoch 0036, iter [03000, 05004], lr: 0.094273, loss: 2.3319
2022-03-01 20:16:56 - train: epoch 0036, iter [03100, 05004], lr: 0.094273, loss: 2.1466
2022-03-01 20:17:29 - train: epoch 0036, iter [03200, 05004], lr: 0.094273, loss: 2.0771
2022-03-01 20:18:04 - train: epoch 0036, iter [03300, 05004], lr: 0.094273, loss: 2.0170
2022-03-01 20:18:37 - train: epoch 0036, iter [03400, 05004], lr: 0.094273, loss: 1.9446
2022-03-01 20:19:10 - train: epoch 0036, iter [03500, 05004], lr: 0.094273, loss: 2.1898
2022-03-01 20:19:44 - train: epoch 0036, iter [03600, 05004], lr: 0.094273, loss: 2.2242
2022-03-01 20:20:18 - train: epoch 0036, iter [03700, 05004], lr: 0.094273, loss: 2.1016
2022-03-01 20:20:50 - train: epoch 0036, iter [03800, 05004], lr: 0.094273, loss: 1.9645
2022-03-01 20:21:23 - train: epoch 0036, iter [03900, 05004], lr: 0.094273, loss: 2.0476
2022-03-01 20:21:57 - train: epoch 0036, iter [04000, 05004], lr: 0.094273, loss: 2.1746
2022-03-01 20:22:31 - train: epoch 0036, iter [04100, 05004], lr: 0.094273, loss: 2.0894
2022-03-01 20:23:05 - train: epoch 0036, iter [04200, 05004], lr: 0.094273, loss: 2.0820
2022-03-01 20:23:39 - train: epoch 0036, iter [04300, 05004], lr: 0.094273, loss: 2.1997
2022-03-01 20:24:13 - train: epoch 0036, iter [04400, 05004], lr: 0.094273, loss: 2.0947
2022-03-01 20:24:45 - train: epoch 0036, iter [04500, 05004], lr: 0.094273, loss: 1.8576
2022-03-01 20:25:19 - train: epoch 0036, iter [04600, 05004], lr: 0.094273, loss: 2.0761
2022-03-01 20:25:53 - train: epoch 0036, iter [04700, 05004], lr: 0.094273, loss: 1.9892
2022-03-01 20:26:25 - train: epoch 0036, iter [04800, 05004], lr: 0.094273, loss: 2.2150
2022-03-01 20:26:58 - train: epoch 0036, iter [04900, 05004], lr: 0.094273, loss: 2.1187
2022-03-01 20:27:30 - train: epoch 0036, iter [05000, 05004], lr: 0.094273, loss: 2.1201
2022-03-01 20:27:32 - train: epoch 036, train_loss: 2.1078
2022-03-01 20:28:47 - eval: epoch: 036, acc1: 55.578%, acc5: 80.466%, test_loss: 1.8906, per_image_load_time: 2.309ms, per_image_inference_time: 0.466ms
2022-03-01 20:28:47 - until epoch: 036, best_acc1: 56.262%
2022-03-01 20:28:47 - epoch 037 lr: 0.09389272045892023
2022-03-01 20:29:28 - train: epoch 0037, iter [00100, 05004], lr: 0.093893, loss: 1.9706
2022-03-01 20:30:01 - train: epoch 0037, iter [00200, 05004], lr: 0.093893, loss: 1.8352
2022-03-01 20:30:35 - train: epoch 0037, iter [00300, 05004], lr: 0.093893, loss: 1.8998
2022-03-01 20:31:09 - train: epoch 0037, iter [00400, 05004], lr: 0.093893, loss: 2.1491
2022-03-01 20:31:42 - train: epoch 0037, iter [00500, 05004], lr: 0.093893, loss: 2.1341
2022-03-01 20:32:15 - train: epoch 0037, iter [00600, 05004], lr: 0.093893, loss: 2.3036
2022-03-01 20:32:48 - train: epoch 0037, iter [00700, 05004], lr: 0.093893, loss: 2.2415
2022-03-01 20:33:22 - train: epoch 0037, iter [00800, 05004], lr: 0.093893, loss: 2.1542
2022-03-01 20:33:56 - train: epoch 0037, iter [00900, 05004], lr: 0.093893, loss: 2.4331
2022-03-01 20:34:30 - train: epoch 0037, iter [01000, 05004], lr: 0.093893, loss: 1.9635
2022-03-01 20:35:04 - train: epoch 0037, iter [01100, 05004], lr: 0.093893, loss: 2.1452
2022-03-01 20:35:37 - train: epoch 0037, iter [01200, 05004], lr: 0.093893, loss: 2.0690
2022-03-01 20:36:12 - train: epoch 0037, iter [01300, 05004], lr: 0.093893, loss: 2.1851
2022-03-01 20:36:46 - train: epoch 0037, iter [01400, 05004], lr: 0.093893, loss: 2.1377
2022-03-01 20:37:19 - train: epoch 0037, iter [01500, 05004], lr: 0.093893, loss: 1.9862
2022-03-01 20:37:51 - train: epoch 0037, iter [01600, 05004], lr: 0.093893, loss: 1.8598
2022-03-01 20:38:25 - train: epoch 0037, iter [01700, 05004], lr: 0.093893, loss: 2.2671
2022-03-01 20:38:58 - train: epoch 0037, iter [01800, 05004], lr: 0.093893, loss: 2.1921
2022-03-01 20:39:31 - train: epoch 0037, iter [01900, 05004], lr: 0.093893, loss: 2.1645
2022-03-01 20:40:05 - train: epoch 0037, iter [02000, 05004], lr: 0.093893, loss: 2.2618
2022-03-01 20:40:39 - train: epoch 0037, iter [02100, 05004], lr: 0.093893, loss: 2.1855
2022-03-01 20:41:13 - train: epoch 0037, iter [02200, 05004], lr: 0.093893, loss: 1.9973
2022-03-01 20:41:47 - train: epoch 0037, iter [02300, 05004], lr: 0.093893, loss: 1.9938
2022-03-01 20:42:20 - train: epoch 0037, iter [02400, 05004], lr: 0.093893, loss: 2.1410
2022-03-01 20:42:54 - train: epoch 0037, iter [02500, 05004], lr: 0.093893, loss: 2.1861
2022-03-01 20:43:26 - train: epoch 0037, iter [02600, 05004], lr: 0.093893, loss: 2.2374
2022-03-01 20:43:59 - train: epoch 0037, iter [02700, 05004], lr: 0.093893, loss: 2.3008
2022-03-01 20:44:32 - train: epoch 0037, iter [02800, 05004], lr: 0.093893, loss: 2.1806
2022-03-01 20:45:06 - train: epoch 0037, iter [02900, 05004], lr: 0.093893, loss: 2.1955
2022-03-01 20:45:40 - train: epoch 0037, iter [03000, 05004], lr: 0.093893, loss: 2.2833
2022-03-01 20:46:13 - train: epoch 0037, iter [03100, 05004], lr: 0.093893, loss: 2.0346
2022-03-01 20:46:47 - train: epoch 0037, iter [03200, 05004], lr: 0.093893, loss: 2.1495
2022-03-01 20:47:21 - train: epoch 0037, iter [03300, 05004], lr: 0.093893, loss: 2.0062
2022-03-01 20:47:55 - train: epoch 0037, iter [03400, 05004], lr: 0.093893, loss: 1.8910
2022-03-01 20:48:28 - train: epoch 0037, iter [03500, 05004], lr: 0.093893, loss: 2.0553
2022-03-01 20:49:00 - train: epoch 0037, iter [03600, 05004], lr: 0.093893, loss: 2.1692
2022-03-01 20:49:34 - train: epoch 0037, iter [03700, 05004], lr: 0.093893, loss: 2.2282
2022-03-01 20:50:07 - train: epoch 0037, iter [03800, 05004], lr: 0.093893, loss: 2.0471
2022-03-01 20:50:41 - train: epoch 0037, iter [03900, 05004], lr: 0.093893, loss: 2.2008
2022-03-01 20:51:15 - train: epoch 0037, iter [04000, 05004], lr: 0.093893, loss: 2.0773
2022-03-01 20:51:49 - train: epoch 0037, iter [04100, 05004], lr: 0.093893, loss: 2.0549
2022-03-01 20:52:22 - train: epoch 0037, iter [04200, 05004], lr: 0.093893, loss: 2.3569
2022-03-01 20:52:56 - train: epoch 0037, iter [04300, 05004], lr: 0.093893, loss: 2.1619
2022-03-01 20:53:30 - train: epoch 0037, iter [04400, 05004], lr: 0.093893, loss: 2.1101
2022-03-01 20:54:02 - train: epoch 0037, iter [04500, 05004], lr: 0.093893, loss: 2.1150
2022-03-01 20:54:35 - train: epoch 0037, iter [04600, 05004], lr: 0.093893, loss: 2.1059
2022-03-01 20:55:09 - train: epoch 0037, iter [04700, 05004], lr: 0.093893, loss: 2.0355
2022-03-01 20:55:42 - train: epoch 0037, iter [04800, 05004], lr: 0.093893, loss: 2.0595
2022-03-01 20:56:16 - train: epoch 0037, iter [04900, 05004], lr: 0.093893, loss: 2.2368
2022-03-01 20:56:49 - train: epoch 0037, iter [05000, 05004], lr: 0.093893, loss: 2.0550
2022-03-01 20:56:50 - train: epoch 037, train_loss: 2.1078
2022-03-01 20:58:06 - eval: epoch: 037, acc1: 55.544%, acc5: 80.710%, test_loss: 1.8773, per_image_load_time: 2.446ms, per_image_inference_time: 0.438ms
2022-03-01 20:58:07 - until epoch: 037, best_acc1: 56.262%
2022-03-01 20:58:07 - epoch 038 lr: 0.09350124727707197
2022-03-01 20:58:46 - train: epoch 0038, iter [00100, 05004], lr: 0.093501, loss: 2.0603
2022-03-01 20:59:19 - train: epoch 0038, iter [00200, 05004], lr: 0.093501, loss: 1.8443
2022-03-01 20:59:51 - train: epoch 0038, iter [00300, 05004], lr: 0.093501, loss: 1.8611
2022-03-01 21:00:25 - train: epoch 0038, iter [00400, 05004], lr: 0.093501, loss: 2.0364
2022-03-01 21:00:59 - train: epoch 0038, iter [00500, 05004], lr: 0.093501, loss: 1.9144
2022-03-01 21:01:32 - train: epoch 0038, iter [00600, 05004], lr: 0.093501, loss: 2.2314
2022-03-01 21:02:05 - train: epoch 0038, iter [00700, 05004], lr: 0.093501, loss: 1.8950
2022-03-01 21:02:40 - train: epoch 0038, iter [00800, 05004], lr: 0.093501, loss: 1.8936
2022-03-01 21:03:14 - train: epoch 0038, iter [00900, 05004], lr: 0.093501, loss: 1.9351
2022-03-01 21:03:47 - train: epoch 0038, iter [01000, 05004], lr: 0.093501, loss: 2.2345
2022-03-01 21:04:22 - train: epoch 0038, iter [01100, 05004], lr: 0.093501, loss: 2.1393
2022-03-01 21:04:55 - train: epoch 0038, iter [01200, 05004], lr: 0.093501, loss: 2.0688
2022-03-01 21:05:28 - train: epoch 0038, iter [01300, 05004], lr: 0.093501, loss: 2.3083
2022-03-01 21:06:01 - train: epoch 0038, iter [01400, 05004], lr: 0.093501, loss: 2.1544
2022-03-01 21:06:35 - train: epoch 0038, iter [01500, 05004], lr: 0.093501, loss: 2.2316
2022-03-01 21:07:08 - train: epoch 0038, iter [01600, 05004], lr: 0.093501, loss: 2.2221
2022-03-01 21:07:43 - train: epoch 0038, iter [01700, 05004], lr: 0.093501, loss: 2.3019
2022-03-01 21:08:16 - train: epoch 0038, iter [01800, 05004], lr: 0.093501, loss: 2.2502
2022-03-01 21:08:50 - train: epoch 0038, iter [01900, 05004], lr: 0.093501, loss: 2.1636
2022-03-01 21:09:24 - train: epoch 0038, iter [02000, 05004], lr: 0.093501, loss: 1.9931
2022-03-01 21:09:58 - train: epoch 0038, iter [02100, 05004], lr: 0.093501, loss: 2.1562
2022-03-01 21:10:32 - train: epoch 0038, iter [02200, 05004], lr: 0.093501, loss: 2.0729
2022-03-01 21:11:04 - train: epoch 0038, iter [02300, 05004], lr: 0.093501, loss: 2.3392
2022-03-01 21:11:37 - train: epoch 0038, iter [02400, 05004], lr: 0.093501, loss: 2.2549
2022-03-01 21:12:11 - train: epoch 0038, iter [02500, 05004], lr: 0.093501, loss: 2.0070
2022-03-01 21:12:44 - train: epoch 0038, iter [02600, 05004], lr: 0.093501, loss: 2.1208
2022-03-01 21:13:18 - train: epoch 0038, iter [02700, 05004], lr: 0.093501, loss: 2.1550
2022-03-01 21:13:52 - train: epoch 0038, iter [02800, 05004], lr: 0.093501, loss: 2.2622
2022-03-01 21:14:27 - train: epoch 0038, iter [02900, 05004], lr: 0.093501, loss: 2.2393
2022-03-01 21:15:00 - train: epoch 0038, iter [03000, 05004], lr: 0.093501, loss: 2.0638
2022-03-01 21:15:33 - train: epoch 0038, iter [03100, 05004], lr: 0.093501, loss: 2.0651
2022-03-01 21:16:07 - train: epoch 0038, iter [03200, 05004], lr: 0.093501, loss: 1.8081
2022-03-01 21:16:39 - train: epoch 0038, iter [03300, 05004], lr: 0.093501, loss: 1.9663
2022-03-01 21:17:12 - train: epoch 0038, iter [03400, 05004], lr: 0.093501, loss: 2.0097
2022-03-01 21:17:45 - train: epoch 0038, iter [03500, 05004], lr: 0.093501, loss: 2.1263
2022-03-01 21:18:18 - train: epoch 0038, iter [03600, 05004], lr: 0.093501, loss: 2.1744
2022-03-01 21:18:52 - train: epoch 0038, iter [03700, 05004], lr: 0.093501, loss: 1.8547
2022-03-01 21:19:25 - train: epoch 0038, iter [03800, 05004], lr: 0.093501, loss: 2.1739
2022-03-01 21:19:59 - train: epoch 0038, iter [03900, 05004], lr: 0.093501, loss: 2.1415
2022-03-01 21:20:32 - train: epoch 0038, iter [04000, 05004], lr: 0.093501, loss: 2.1491
2022-03-01 21:21:05 - train: epoch 0038, iter [04100, 05004], lr: 0.093501, loss: 2.0304
2022-03-01 21:21:38 - train: epoch 0038, iter [04200, 05004], lr: 0.093501, loss: 1.8206
2022-03-01 21:22:11 - train: epoch 0038, iter [04300, 05004], lr: 0.093501, loss: 2.1889
2022-03-01 21:22:44 - train: epoch 0038, iter [04400, 05004], lr: 0.093501, loss: 2.2538
2022-03-01 21:23:16 - train: epoch 0038, iter [04500, 05004], lr: 0.093501, loss: 2.1894
2022-03-01 21:23:49 - train: epoch 0038, iter [04600, 05004], lr: 0.093501, loss: 2.2704
2022-03-01 21:24:21 - train: epoch 0038, iter [04700, 05004], lr: 0.093501, loss: 1.8947
2022-03-01 21:24:54 - train: epoch 0038, iter [04800, 05004], lr: 0.093501, loss: 2.2234
2022-03-01 21:25:27 - train: epoch 0038, iter [04900, 05004], lr: 0.093501, loss: 2.0717
2022-03-01 21:25:58 - train: epoch 0038, iter [05000, 05004], lr: 0.093501, loss: 1.8131
2022-03-01 21:25:59 - train: epoch 038, train_loss: 2.1018
2022-03-01 21:27:14 - eval: epoch: 038, acc1: 52.944%, acc5: 78.276%, test_loss: 2.0146, per_image_load_time: 2.390ms, per_image_inference_time: 0.505ms
2022-03-01 21:27:15 - until epoch: 038, best_acc1: 56.262%
2022-03-01 21:27:15 - epoch 039 lr: 0.09309848334400246
2022-03-01 21:27:52 - train: epoch 0039, iter [00100, 05004], lr: 0.093098, loss: 2.0948
2022-03-01 21:28:25 - train: epoch 0039, iter [00200, 05004], lr: 0.093098, loss: 2.2700
2022-03-01 21:28:58 - train: epoch 0039, iter [00300, 05004], lr: 0.093098, loss: 1.8697
2022-03-01 21:29:31 - train: epoch 0039, iter [00400, 05004], lr: 0.093098, loss: 2.0925
2022-03-01 21:30:04 - train: epoch 0039, iter [00500, 05004], lr: 0.093098, loss: 2.0319
2022-03-01 21:30:37 - train: epoch 0039, iter [00600, 05004], lr: 0.093098, loss: 1.9968
2022-03-01 21:31:10 - train: epoch 0039, iter [00700, 05004], lr: 0.093098, loss: 2.2619
2022-03-01 21:31:43 - train: epoch 0039, iter [00800, 05004], lr: 0.093098, loss: 2.0311
2022-03-01 21:32:15 - train: epoch 0039, iter [00900, 05004], lr: 0.093098, loss: 2.1442
2022-03-01 21:32:49 - train: epoch 0039, iter [01000, 05004], lr: 0.093098, loss: 1.9465
2022-03-01 21:33:21 - train: epoch 0039, iter [01100, 05004], lr: 0.093098, loss: 2.1451
2022-03-01 21:33:55 - train: epoch 0039, iter [01200, 05004], lr: 0.093098, loss: 2.2043
2022-03-01 21:34:29 - train: epoch 0039, iter [01300, 05004], lr: 0.093098, loss: 2.3357
2022-03-01 21:35:02 - train: epoch 0039, iter [01400, 05004], lr: 0.093098, loss: 2.2342
2022-03-01 21:35:35 - train: epoch 0039, iter [01500, 05004], lr: 0.093098, loss: 2.0641
2022-03-01 21:36:08 - train: epoch 0039, iter [01600, 05004], lr: 0.093098, loss: 2.2041
2022-03-01 21:36:42 - train: epoch 0039, iter [01700, 05004], lr: 0.093098, loss: 1.7200
2022-03-01 21:37:15 - train: epoch 0039, iter [01800, 05004], lr: 0.093098, loss: 2.0347
2022-03-01 21:37:48 - train: epoch 0039, iter [01900, 05004], lr: 0.093098, loss: 1.9365
2022-03-01 21:38:22 - train: epoch 0039, iter [02000, 05004], lr: 0.093098, loss: 2.0087
2022-03-01 21:38:55 - train: epoch 0039, iter [02100, 05004], lr: 0.093098, loss: 2.2236
2022-03-01 21:39:28 - train: epoch 0039, iter [02200, 05004], lr: 0.093098, loss: 1.9698
2022-03-01 21:40:01 - train: epoch 0039, iter [02300, 05004], lr: 0.093098, loss: 2.2369
2022-03-01 21:40:34 - train: epoch 0039, iter [02400, 05004], lr: 0.093098, loss: 2.3047
2022-03-01 21:41:08 - train: epoch 0039, iter [02500, 05004], lr: 0.093098, loss: 1.9176
2022-03-01 21:41:41 - train: epoch 0039, iter [02600, 05004], lr: 0.093098, loss: 2.1897
2022-03-01 21:42:14 - train: epoch 0039, iter [02700, 05004], lr: 0.093098, loss: 2.2198
2022-03-01 21:42:47 - train: epoch 0039, iter [02800, 05004], lr: 0.093098, loss: 2.0763
2022-03-01 21:43:21 - train: epoch 0039, iter [02900, 05004], lr: 0.093098, loss: 1.8942
2022-03-01 21:43:54 - train: epoch 0039, iter [03000, 05004], lr: 0.093098, loss: 2.0379
2022-03-01 21:44:27 - train: epoch 0039, iter [03100, 05004], lr: 0.093098, loss: 2.0821
2022-03-01 21:45:01 - train: epoch 0039, iter [03200, 05004], lr: 0.093098, loss: 2.2314
2022-03-01 21:45:34 - train: epoch 0039, iter [03300, 05004], lr: 0.093098, loss: 2.0713
2022-03-01 21:46:08 - train: epoch 0039, iter [03400, 05004], lr: 0.093098, loss: 2.1231
2022-03-01 21:46:41 - train: epoch 0039, iter [03500, 05004], lr: 0.093098, loss: 2.3890
2022-03-01 21:47:14 - train: epoch 0039, iter [03600, 05004], lr: 0.093098, loss: 2.2107
2022-03-01 21:47:47 - train: epoch 0039, iter [03700, 05004], lr: 0.093098, loss: 2.0160
2022-03-01 21:48:20 - train: epoch 0039, iter [03800, 05004], lr: 0.093098, loss: 2.0228
2022-03-01 21:48:53 - train: epoch 0039, iter [03900, 05004], lr: 0.093098, loss: 2.1860
2022-03-01 21:49:27 - train: epoch 0039, iter [04000, 05004], lr: 0.093098, loss: 2.0565
2022-03-01 21:50:00 - train: epoch 0039, iter [04100, 05004], lr: 0.093098, loss: 2.2622
2022-03-01 21:50:33 - train: epoch 0039, iter [04200, 05004], lr: 0.093098, loss: 2.1685
2022-03-01 21:51:07 - train: epoch 0039, iter [04300, 05004], lr: 0.093098, loss: 2.0862
2022-03-01 21:51:40 - train: epoch 0039, iter [04400, 05004], lr: 0.093098, loss: 1.7940
2022-03-01 21:52:13 - train: epoch 0039, iter [04500, 05004], lr: 0.093098, loss: 1.9282
2022-03-01 21:52:47 - train: epoch 0039, iter [04600, 05004], lr: 0.093098, loss: 2.2111
2022-03-01 21:53:19 - train: epoch 0039, iter [04700, 05004], lr: 0.093098, loss: 2.0817
2022-03-01 21:53:53 - train: epoch 0039, iter [04800, 05004], lr: 0.093098, loss: 2.1192
2022-03-01 21:54:26 - train: epoch 0039, iter [04900, 05004], lr: 0.093098, loss: 1.9864
2022-03-01 21:54:58 - train: epoch 0039, iter [05000, 05004], lr: 0.093098, loss: 1.9847
2022-03-01 21:54:59 - train: epoch 039, train_loss: 2.1000
2022-03-01 21:56:13 - eval: epoch: 039, acc1: 52.894%, acc5: 78.370%, test_loss: 2.0094, per_image_load_time: 2.337ms, per_image_inference_time: 0.514ms
2022-03-01 21:56:14 - until epoch: 039, best_acc1: 56.262%
2022-03-01 21:56:14 - epoch 040 lr: 0.09268453319711362
2022-03-01 21:56:52 - train: epoch 0040, iter [00100, 05004], lr: 0.092685, loss: 2.2897
2022-03-01 21:57:24 - train: epoch 0040, iter [00200, 05004], lr: 0.092685, loss: 2.3744
2022-03-01 21:57:57 - train: epoch 0040, iter [00300, 05004], lr: 0.092685, loss: 2.2284
2022-03-01 21:58:31 - train: epoch 0040, iter [00400, 05004], lr: 0.092685, loss: 2.1723
2022-03-01 21:59:05 - train: epoch 0040, iter [00500, 05004], lr: 0.092685, loss: 1.9573
2022-03-01 21:59:37 - train: epoch 0040, iter [00600, 05004], lr: 0.092685, loss: 2.1997
2022-03-01 22:00:11 - train: epoch 0040, iter [00700, 05004], lr: 0.092685, loss: 2.0602
2022-03-01 22:00:44 - train: epoch 0040, iter [00800, 05004], lr: 0.092685, loss: 2.2108
2022-03-01 22:01:16 - train: epoch 0040, iter [00900, 05004], lr: 0.092685, loss: 2.0092
2022-03-01 22:01:50 - train: epoch 0040, iter [01000, 05004], lr: 0.092685, loss: 1.8174
2022-03-01 22:02:23 - train: epoch 0040, iter [01100, 05004], lr: 0.092685, loss: 2.0194
2022-03-01 22:02:56 - train: epoch 0040, iter [01200, 05004], lr: 0.092685, loss: 2.0052
2022-03-01 22:03:30 - train: epoch 0040, iter [01300, 05004], lr: 0.092685, loss: 1.9599
2022-03-01 22:04:03 - train: epoch 0040, iter [01400, 05004], lr: 0.092685, loss: 1.9389
2022-03-01 22:04:36 - train: epoch 0040, iter [01500, 05004], lr: 0.092685, loss: 2.1924
2022-03-01 22:05:09 - train: epoch 0040, iter [01600, 05004], lr: 0.092685, loss: 2.1025
2022-03-01 22:05:42 - train: epoch 0040, iter [01700, 05004], lr: 0.092685, loss: 2.2034
2022-03-01 22:06:15 - train: epoch 0040, iter [01800, 05004], lr: 0.092685, loss: 1.9136
2022-03-01 22:06:48 - train: epoch 0040, iter [01900, 05004], lr: 0.092685, loss: 1.9965
2022-03-01 22:07:21 - train: epoch 0040, iter [02000, 05004], lr: 0.092685, loss: 2.0753
2022-03-01 22:07:55 - train: epoch 0040, iter [02100, 05004], lr: 0.092685, loss: 1.9692
2022-03-01 22:08:28 - train: epoch 0040, iter [02200, 05004], lr: 0.092685, loss: 1.8189
2022-03-01 22:09:00 - train: epoch 0040, iter [02300, 05004], lr: 0.092685, loss: 2.1042
2022-03-01 22:09:34 - train: epoch 0040, iter [02400, 05004], lr: 0.092685, loss: 2.1196
2022-03-01 22:10:07 - train: epoch 0040, iter [02500, 05004], lr: 0.092685, loss: 2.1950
2022-03-01 22:10:40 - train: epoch 0040, iter [02600, 05004], lr: 0.092685, loss: 2.0205
2022-03-01 22:11:13 - train: epoch 0040, iter [02700, 05004], lr: 0.092685, loss: 2.1866
2022-03-01 22:11:45 - train: epoch 0040, iter [02800, 05004], lr: 0.092685, loss: 2.2220
2022-03-01 22:12:18 - train: epoch 0040, iter [02900, 05004], lr: 0.092685, loss: 2.4357
2022-03-01 22:12:53 - train: epoch 0040, iter [03000, 05004], lr: 0.092685, loss: 2.2046
2022-03-01 22:13:25 - train: epoch 0040, iter [03100, 05004], lr: 0.092685, loss: 2.0088
2022-03-01 22:13:58 - train: epoch 0040, iter [03200, 05004], lr: 0.092685, loss: 2.4012
2022-03-01 22:14:32 - train: epoch 0040, iter [03300, 05004], lr: 0.092685, loss: 2.2307
2022-03-01 22:15:04 - train: epoch 0040, iter [03400, 05004], lr: 0.092685, loss: 2.2066
2022-03-01 22:15:37 - train: epoch 0040, iter [03500, 05004], lr: 0.092685, loss: 2.1755
2022-03-01 22:16:10 - train: epoch 0040, iter [03600, 05004], lr: 0.092685, loss: 2.1467
2022-03-01 22:16:43 - train: epoch 0040, iter [03700, 05004], lr: 0.092685, loss: 2.0384
2022-03-01 22:17:16 - train: epoch 0040, iter [03800, 05004], lr: 0.092685, loss: 1.9613
2022-03-01 22:17:49 - train: epoch 0040, iter [03900, 05004], lr: 0.092685, loss: 2.1690
2022-03-01 22:18:22 - train: epoch 0040, iter [04000, 05004], lr: 0.092685, loss: 2.2062
2022-03-01 22:18:55 - train: epoch 0040, iter [04100, 05004], lr: 0.092685, loss: 2.1616
2022-03-01 22:19:29 - train: epoch 0040, iter [04200, 05004], lr: 0.092685, loss: 2.0315
2022-03-01 22:20:01 - train: epoch 0040, iter [04300, 05004], lr: 0.092685, loss: 2.1518
2022-03-01 22:20:34 - train: epoch 0040, iter [04400, 05004], lr: 0.092685, loss: 2.1593
2022-03-01 22:21:07 - train: epoch 0040, iter [04500, 05004], lr: 0.092685, loss: 1.9540
2022-03-01 22:21:40 - train: epoch 0040, iter [04600, 05004], lr: 0.092685, loss: 2.0413
2022-03-01 22:22:13 - train: epoch 0040, iter [04700, 05004], lr: 0.092685, loss: 2.1023
2022-03-01 22:22:47 - train: epoch 0040, iter [04800, 05004], lr: 0.092685, loss: 1.9892
2022-03-01 22:23:20 - train: epoch 0040, iter [04900, 05004], lr: 0.092685, loss: 2.1895
2022-03-01 22:23:51 - train: epoch 0040, iter [05000, 05004], lr: 0.092685, loss: 2.1838
2022-03-01 22:23:52 - train: epoch 040, train_loss: 2.0939
2022-03-01 22:25:05 - eval: epoch: 040, acc1: 53.906%, acc5: 79.278%, test_loss: 1.9503, per_image_load_time: 1.487ms, per_image_inference_time: 0.530ms
2022-03-01 22:25:06 - until epoch: 040, best_acc1: 56.262%
2022-03-01 22:25:06 - epoch 041 lr: 0.09225950427718975
2022-03-01 22:25:44 - train: epoch 0041, iter [00100, 05004], lr: 0.092260, loss: 2.1421
2022-03-01 22:26:17 - train: epoch 0041, iter [00200, 05004], lr: 0.092260, loss: 2.3628
2022-03-01 22:26:50 - train: epoch 0041, iter [00300, 05004], lr: 0.092260, loss: 2.0963
2022-03-01 22:27:22 - train: epoch 0041, iter [00400, 05004], lr: 0.092260, loss: 2.1260
2022-03-01 22:27:55 - train: epoch 0041, iter [00500, 05004], lr: 0.092260, loss: 1.9281
2022-03-01 22:28:28 - train: epoch 0041, iter [00600, 05004], lr: 0.092260, loss: 2.2295
2022-03-01 22:29:01 - train: epoch 0041, iter [00700, 05004], lr: 0.092260, loss: 2.0338
2022-03-01 22:29:34 - train: epoch 0041, iter [00800, 05004], lr: 0.092260, loss: 1.8536
2022-03-01 22:30:07 - train: epoch 0041, iter [00900, 05004], lr: 0.092260, loss: 1.8311
2022-03-01 22:30:41 - train: epoch 0041, iter [01000, 05004], lr: 0.092260, loss: 2.3196
2022-03-01 22:31:14 - train: epoch 0041, iter [01100, 05004], lr: 0.092260, loss: 2.0210
2022-03-01 22:31:46 - train: epoch 0041, iter [01200, 05004], lr: 0.092260, loss: 1.8299
2022-03-01 22:32:19 - train: epoch 0041, iter [01300, 05004], lr: 0.092260, loss: 2.0633
2022-03-01 22:32:53 - train: epoch 0041, iter [01400, 05004], lr: 0.092260, loss: 2.2259
2022-03-01 22:33:25 - train: epoch 0041, iter [01500, 05004], lr: 0.092260, loss: 2.3789
2022-03-01 22:33:59 - train: epoch 0041, iter [01600, 05004], lr: 0.092260, loss: 1.9894
