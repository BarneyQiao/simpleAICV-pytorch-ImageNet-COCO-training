2022-03-02 22:34:34 - train: epoch 0091, iter [00400, 05004], lr: 0.060001, loss: 1.8312
2022-03-02 22:35:07 - train: epoch 0091, iter [00500, 05004], lr: 0.060001, loss: 1.7479
2022-03-02 22:35:40 - train: epoch 0091, iter [00600, 05004], lr: 0.060001, loss: 1.7756
2022-03-02 22:36:12 - train: epoch 0091, iter [00700, 05004], lr: 0.060001, loss: 1.8995
2022-03-02 22:36:46 - train: epoch 0091, iter [00800, 05004], lr: 0.060001, loss: 1.6008
2022-03-02 22:37:19 - train: epoch 0091, iter [00900, 05004], lr: 0.060001, loss: 1.9015
2022-03-02 22:37:52 - train: epoch 0091, iter [01000, 05004], lr: 0.060001, loss: 1.7208
2022-03-02 22:38:24 - train: epoch 0091, iter [01100, 05004], lr: 0.060001, loss: 1.4975
2022-03-02 22:38:58 - train: epoch 0091, iter [01200, 05004], lr: 0.060001, loss: 1.7424
2022-03-02 22:39:31 - train: epoch 0091, iter [01300, 05004], lr: 0.060001, loss: 1.6930
2022-03-02 22:40:04 - train: epoch 0091, iter [01400, 05004], lr: 0.060001, loss: 1.8870
2022-03-02 22:40:37 - train: epoch 0091, iter [01500, 05004], lr: 0.060001, loss: 1.8222
2022-03-02 22:41:11 - train: epoch 0091, iter [01600, 05004], lr: 0.060001, loss: 1.9337
2022-03-02 22:41:44 - train: epoch 0091, iter [01700, 05004], lr: 0.060001, loss: 1.8391
2022-03-02 22:42:17 - train: epoch 0091, iter [01800, 05004], lr: 0.060001, loss: 1.8569
2022-03-02 22:42:50 - train: epoch 0091, iter [01900, 05004], lr: 0.060001, loss: 1.8353
2022-03-02 22:43:23 - train: epoch 0091, iter [02000, 05004], lr: 0.060001, loss: 1.8887
2022-03-02 22:43:56 - train: epoch 0091, iter [02100, 05004], lr: 0.060001, loss: 1.6822
2022-03-02 22:44:29 - train: epoch 0091, iter [02200, 05004], lr: 0.060001, loss: 1.9950
2022-03-02 22:45:03 - train: epoch 0091, iter [02300, 05004], lr: 0.060001, loss: 1.9024
2022-03-02 22:45:36 - train: epoch 0091, iter [02400, 05004], lr: 0.060001, loss: 1.6987
2022-03-02 22:46:08 - train: epoch 0091, iter [02500, 05004], lr: 0.060001, loss: 1.7995
2022-03-02 22:46:42 - train: epoch 0091, iter [02600, 05004], lr: 0.060001, loss: 1.8051
2022-03-02 22:47:16 - train: epoch 0091, iter [02700, 05004], lr: 0.060001, loss: 1.7378
2022-03-02 22:47:48 - train: epoch 0091, iter [02800, 05004], lr: 0.060001, loss: 1.8751
2022-03-02 22:48:21 - train: epoch 0091, iter [02900, 05004], lr: 0.060001, loss: 1.9644
2022-03-02 22:48:54 - train: epoch 0091, iter [03000, 05004], lr: 0.060001, loss: 1.9799
2022-03-02 22:49:28 - train: epoch 0091, iter [03100, 05004], lr: 0.060001, loss: 1.8536
2022-03-02 22:50:00 - train: epoch 0091, iter [03200, 05004], lr: 0.060001, loss: 2.0062
2022-03-02 22:50:33 - train: epoch 0091, iter [03300, 05004], lr: 0.060001, loss: 1.8501
2022-03-02 22:51:06 - train: epoch 0091, iter [03400, 05004], lr: 0.060001, loss: 1.8308
2022-03-02 22:51:40 - train: epoch 0091, iter [03500, 05004], lr: 0.060001, loss: 1.8646
2022-03-02 22:52:13 - train: epoch 0091, iter [03600, 05004], lr: 0.060001, loss: 1.7330
2022-03-02 22:52:46 - train: epoch 0091, iter [03700, 05004], lr: 0.060001, loss: 1.9471
2022-03-02 22:53:19 - train: epoch 0091, iter [03800, 05004], lr: 0.060001, loss: 1.8579
2022-03-02 22:53:52 - train: epoch 0091, iter [03900, 05004], lr: 0.060001, loss: 1.8504
2022-03-02 22:54:25 - train: epoch 0091, iter [04000, 05004], lr: 0.060001, loss: 1.9055
2022-03-02 22:54:58 - train: epoch 0091, iter [04100, 05004], lr: 0.060001, loss: 1.8691
2022-03-02 22:55:31 - train: epoch 0091, iter [04200, 05004], lr: 0.060001, loss: 1.9472
2022-03-02 22:56:05 - train: epoch 0091, iter [04300, 05004], lr: 0.060001, loss: 1.9043
2022-03-02 22:56:38 - train: epoch 0091, iter [04400, 05004], lr: 0.060001, loss: 1.7082
2022-03-02 22:57:11 - train: epoch 0091, iter [04500, 05004], lr: 0.060001, loss: 1.7482
2022-03-02 22:57:44 - train: epoch 0091, iter [04600, 05004], lr: 0.060001, loss: 1.8113
2022-03-02 22:58:17 - train: epoch 0091, iter [04700, 05004], lr: 0.060001, loss: 1.9927
2022-03-02 22:58:49 - train: epoch 0091, iter [04800, 05004], lr: 0.060001, loss: 1.8529
2022-03-02 22:59:23 - train: epoch 0091, iter [04900, 05004], lr: 0.060001, loss: 1.8005
2022-03-02 22:59:54 - train: epoch 0091, iter [05000, 05004], lr: 0.060001, loss: 2.1823
2022-03-02 22:59:56 - train: epoch 091, train_loss: 1.8629
2022-03-02 23:01:09 - eval: epoch: 091, acc1: 59.654%, acc5: 83.512%, test_loss: 1.6751, per_image_load_time: 1.915ms, per_image_inference_time: 0.546ms
2022-03-02 23:01:10 - until epoch: 091, best_acc1: 60.566%
2022-03-02 23:01:10 - epoch 092 lr: 0.05921076370520058
2022-03-02 23:01:47 - train: epoch 0092, iter [00100, 05004], lr: 0.059211, loss: 1.8906
2022-03-02 23:02:21 - train: epoch 0092, iter [00200, 05004], lr: 0.059211, loss: 1.8357
2022-03-02 23:02:54 - train: epoch 0092, iter [00300, 05004], lr: 0.059211, loss: 1.7620
2022-03-02 23:03:28 - train: epoch 0092, iter [00400, 05004], lr: 0.059211, loss: 1.6989
2022-03-02 23:03:59 - train: epoch 0092, iter [00500, 05004], lr: 0.059211, loss: 1.9556
2022-03-02 23:04:33 - train: epoch 0092, iter [00600, 05004], lr: 0.059211, loss: 1.7875
2022-03-02 23:05:06 - train: epoch 0092, iter [00700, 05004], lr: 0.059211, loss: 1.8231
2022-03-02 23:05:39 - train: epoch 0092, iter [00800, 05004], lr: 0.059211, loss: 1.9978
2022-03-02 23:06:12 - train: epoch 0092, iter [00900, 05004], lr: 0.059211, loss: 1.8300
2022-03-02 23:06:46 - train: epoch 0092, iter [01000, 05004], lr: 0.059211, loss: 2.0522
2022-03-02 23:07:18 - train: epoch 0092, iter [01100, 05004], lr: 0.059211, loss: 1.8256
2022-03-02 23:07:51 - train: epoch 0092, iter [01200, 05004], lr: 0.059211, loss: 1.6616
2022-03-02 23:08:24 - train: epoch 0092, iter [01300, 05004], lr: 0.059211, loss: 1.7133
2022-03-02 23:08:58 - train: epoch 0092, iter [01400, 05004], lr: 0.059211, loss: 1.7416
2022-03-02 23:09:31 - train: epoch 0092, iter [01500, 05004], lr: 0.059211, loss: 1.8696
2022-03-02 23:10:04 - train: epoch 0092, iter [01600, 05004], lr: 0.059211, loss: 1.8111
2022-03-02 23:10:37 - train: epoch 0092, iter [01700, 05004], lr: 0.059211, loss: 1.8074
2022-03-02 23:11:11 - train: epoch 0092, iter [01800, 05004], lr: 0.059211, loss: 1.8844
2022-03-02 23:11:44 - train: epoch 0092, iter [01900, 05004], lr: 0.059211, loss: 1.7536
2022-03-02 23:12:17 - train: epoch 0092, iter [02000, 05004], lr: 0.059211, loss: 1.7083
2022-03-02 23:12:50 - train: epoch 0092, iter [02100, 05004], lr: 0.059211, loss: 1.7537
2022-03-02 23:13:23 - train: epoch 0092, iter [02200, 05004], lr: 0.059211, loss: 1.8298
2022-03-02 23:13:56 - train: epoch 0092, iter [02300, 05004], lr: 0.059211, loss: 1.8629
2022-03-02 23:14:29 - train: epoch 0092, iter [02400, 05004], lr: 0.059211, loss: 1.7110
2022-03-02 23:15:03 - train: epoch 0092, iter [02500, 05004], lr: 0.059211, loss: 1.8697
2022-03-02 23:15:36 - train: epoch 0092, iter [02600, 05004], lr: 0.059211, loss: 1.9004
2022-03-02 23:16:10 - train: epoch 0092, iter [02700, 05004], lr: 0.059211, loss: 1.8405
2022-03-02 23:16:43 - train: epoch 0092, iter [02800, 05004], lr: 0.059211, loss: 1.7501
2022-03-02 23:17:16 - train: epoch 0092, iter [02900, 05004], lr: 0.059211, loss: 1.9842
2022-03-02 23:17:49 - train: epoch 0092, iter [03000, 05004], lr: 0.059211, loss: 1.7566
2022-03-02 23:18:22 - train: epoch 0092, iter [03100, 05004], lr: 0.059211, loss: 1.8671
2022-03-02 23:18:56 - train: epoch 0092, iter [03200, 05004], lr: 0.059211, loss: 1.6806
2022-03-02 23:19:29 - train: epoch 0092, iter [03300, 05004], lr: 0.059211, loss: 1.9134
2022-03-02 23:20:02 - train: epoch 0092, iter [03400, 05004], lr: 0.059211, loss: 2.0112
2022-03-02 23:20:35 - train: epoch 0092, iter [03500, 05004], lr: 0.059211, loss: 1.7924
2022-03-02 23:21:08 - train: epoch 0092, iter [03600, 05004], lr: 0.059211, loss: 1.8508
2022-03-02 23:21:42 - train: epoch 0092, iter [03700, 05004], lr: 0.059211, loss: 1.7514
2022-03-02 23:22:15 - train: epoch 0092, iter [03800, 05004], lr: 0.059211, loss: 2.2176
2022-03-02 23:22:48 - train: epoch 0092, iter [03900, 05004], lr: 0.059211, loss: 1.9697
2022-03-02 23:23:21 - train: epoch 0092, iter [04000, 05004], lr: 0.059211, loss: 1.9602
2022-03-02 23:23:54 - train: epoch 0092, iter [04100, 05004], lr: 0.059211, loss: 1.6971
2022-03-02 23:24:28 - train: epoch 0092, iter [04200, 05004], lr: 0.059211, loss: 1.8806
2022-03-02 23:25:01 - train: epoch 0092, iter [04300, 05004], lr: 0.059211, loss: 1.9264
2022-03-02 23:25:34 - train: epoch 0092, iter [04400, 05004], lr: 0.059211, loss: 1.7506
2022-03-02 23:26:07 - train: epoch 0092, iter [04500, 05004], lr: 0.059211, loss: 1.8210
2022-03-02 23:26:40 - train: epoch 0092, iter [04600, 05004], lr: 0.059211, loss: 1.7775
2022-03-02 23:27:13 - train: epoch 0092, iter [04700, 05004], lr: 0.059211, loss: 1.4999
2022-03-02 23:27:47 - train: epoch 0092, iter [04800, 05004], lr: 0.059211, loss: 1.7921
2022-03-02 23:28:20 - train: epoch 0092, iter [04900, 05004], lr: 0.059211, loss: 1.8253
2022-03-02 23:28:52 - train: epoch 0092, iter [05000, 05004], lr: 0.059211, loss: 2.0130
2022-03-02 23:28:53 - train: epoch 092, train_loss: 1.8535
2022-03-02 23:30:07 - eval: epoch: 092, acc1: 59.508%, acc5: 83.170%, test_loss: 1.6914, per_image_load_time: 2.264ms, per_image_inference_time: 0.502ms
2022-03-02 23:30:08 - until epoch: 092, best_acc1: 60.566%
2022-03-02 23:30:08 - epoch 093 lr: 0.05841785206735192
2022-03-02 23:30:47 - train: epoch 0093, iter [00100, 05004], lr: 0.058418, loss: 1.6887
2022-03-02 23:31:19 - train: epoch 0093, iter [00200, 05004], lr: 0.058418, loss: 1.6456
2022-03-02 23:31:52 - train: epoch 0093, iter [00300, 05004], lr: 0.058418, loss: 1.9073
2022-03-02 23:32:25 - train: epoch 0093, iter [00400, 05004], lr: 0.058418, loss: 1.7882
2022-03-02 23:32:59 - train: epoch 0093, iter [00500, 05004], lr: 0.058418, loss: 2.0497
2022-03-02 23:33:32 - train: epoch 0093, iter [00600, 05004], lr: 0.058418, loss: 1.7277
2022-03-02 23:34:05 - train: epoch 0093, iter [00700, 05004], lr: 0.058418, loss: 1.9548
2022-03-02 23:34:38 - train: epoch 0093, iter [00800, 05004], lr: 0.058418, loss: 1.7765
2022-03-02 23:35:11 - train: epoch 0093, iter [00900, 05004], lr: 0.058418, loss: 1.7010
2022-03-02 23:35:44 - train: epoch 0093, iter [01000, 05004], lr: 0.058418, loss: 1.6232
2022-03-02 23:36:18 - train: epoch 0093, iter [01100, 05004], lr: 0.058418, loss: 1.6966
2022-03-02 23:36:50 - train: epoch 0093, iter [01200, 05004], lr: 0.058418, loss: 1.7065
2022-03-02 23:37:24 - train: epoch 0093, iter [01300, 05004], lr: 0.058418, loss: 1.8503
2022-03-02 23:37:58 - train: epoch 0093, iter [01400, 05004], lr: 0.058418, loss: 1.8591
2022-03-02 23:38:31 - train: epoch 0093, iter [01500, 05004], lr: 0.058418, loss: 1.9264
2022-03-02 23:39:04 - train: epoch 0093, iter [01600, 05004], lr: 0.058418, loss: 1.8768
2022-03-02 23:39:38 - train: epoch 0093, iter [01700, 05004], lr: 0.058418, loss: 1.6990
2022-03-02 23:40:10 - train: epoch 0093, iter [01800, 05004], lr: 0.058418, loss: 1.8446
2022-03-02 23:40:43 - train: epoch 0093, iter [01900, 05004], lr: 0.058418, loss: 1.7580
2022-03-02 23:41:16 - train: epoch 0093, iter [02000, 05004], lr: 0.058418, loss: 1.7474
2022-03-02 23:41:49 - train: epoch 0093, iter [02100, 05004], lr: 0.058418, loss: 1.6953
2022-03-02 23:42:23 - train: epoch 0093, iter [02200, 05004], lr: 0.058418, loss: 1.9950
2022-03-02 23:42:56 - train: epoch 0093, iter [02300, 05004], lr: 0.058418, loss: 1.9048
2022-03-02 23:43:30 - train: epoch 0093, iter [02400, 05004], lr: 0.058418, loss: 1.6345
2022-03-02 23:44:03 - train: epoch 0093, iter [02500, 05004], lr: 0.058418, loss: 1.9086
2022-03-02 23:44:36 - train: epoch 0093, iter [02600, 05004], lr: 0.058418, loss: 2.1355
2022-03-02 23:45:09 - train: epoch 0093, iter [02700, 05004], lr: 0.058418, loss: 1.7608
2022-03-02 23:45:42 - train: epoch 0093, iter [02800, 05004], lr: 0.058418, loss: 1.8754
2022-03-02 23:46:15 - train: epoch 0093, iter [02900, 05004], lr: 0.058418, loss: 1.8988
2022-03-02 23:46:49 - train: epoch 0093, iter [03000, 05004], lr: 0.058418, loss: 1.6467
2022-03-02 23:47:22 - train: epoch 0093, iter [03100, 05004], lr: 0.058418, loss: 1.9718
2022-03-02 23:47:55 - train: epoch 0093, iter [03200, 05004], lr: 0.058418, loss: 1.7361
2022-03-02 23:48:28 - train: epoch 0093, iter [03300, 05004], lr: 0.058418, loss: 1.8681
2022-03-02 23:49:01 - train: epoch 0093, iter [03400, 05004], lr: 0.058418, loss: 2.0876
2022-03-02 23:49:35 - train: epoch 0093, iter [03500, 05004], lr: 0.058418, loss: 1.6480
2022-03-02 23:50:08 - train: epoch 0093, iter [03600, 05004], lr: 0.058418, loss: 1.9792
2022-03-02 23:50:41 - train: epoch 0093, iter [03700, 05004], lr: 0.058418, loss: 1.7403
2022-03-02 23:51:14 - train: epoch 0093, iter [03800, 05004], lr: 0.058418, loss: 1.6977
2022-03-02 23:51:48 - train: epoch 0093, iter [03900, 05004], lr: 0.058418, loss: 1.8425
2022-03-02 23:52:21 - train: epoch 0093, iter [04000, 05004], lr: 0.058418, loss: 2.1454
2022-03-02 23:52:53 - train: epoch 0093, iter [04100, 05004], lr: 0.058418, loss: 2.0143
2022-03-02 23:53:27 - train: epoch 0093, iter [04200, 05004], lr: 0.058418, loss: 1.9913
2022-03-02 23:54:00 - train: epoch 0093, iter [04300, 05004], lr: 0.058418, loss: 2.1246
2022-03-02 23:54:33 - train: epoch 0093, iter [04400, 05004], lr: 0.058418, loss: 1.7376
2022-03-02 23:55:06 - train: epoch 0093, iter [04500, 05004], lr: 0.058418, loss: 1.7449
2022-03-02 23:55:39 - train: epoch 0093, iter [04600, 05004], lr: 0.058418, loss: 1.7472
2022-03-02 23:56:12 - train: epoch 0093, iter [04700, 05004], lr: 0.058418, loss: 2.3632
2022-03-02 23:56:46 - train: epoch 0093, iter [04800, 05004], lr: 0.058418, loss: 1.8117
2022-03-02 23:57:19 - train: epoch 0093, iter [04900, 05004], lr: 0.058418, loss: 1.8765
2022-03-02 23:57:51 - train: epoch 0093, iter [05000, 05004], lr: 0.058418, loss: 1.7667
2022-03-02 23:57:52 - train: epoch 093, train_loss: 1.8483
2022-03-02 23:59:05 - eval: epoch: 093, acc1: 61.836%, acc5: 84.782%, test_loss: 1.5827, per_image_load_time: 2.122ms, per_image_inference_time: 0.542ms
2022-03-02 23:59:06 - until epoch: 093, best_acc1: 61.836%
2022-03-02 23:59:06 - epoch 094 lr: 0.05762275557551727
2022-03-02 23:59:43 - train: epoch 0094, iter [00100, 05004], lr: 0.057623, loss: 1.9074
2022-03-03 00:00:17 - train: epoch 0094, iter [00200, 05004], lr: 0.057623, loss: 1.8788
2022-03-03 00:00:49 - train: epoch 0094, iter [00300, 05004], lr: 0.057623, loss: 1.9310
2022-03-03 00:01:23 - train: epoch 0094, iter [00400, 05004], lr: 0.057623, loss: 2.0151
2022-03-03 00:01:56 - train: epoch 0094, iter [00500, 05004], lr: 0.057623, loss: 1.6200
2022-03-03 00:02:28 - train: epoch 0094, iter [00600, 05004], lr: 0.057623, loss: 1.6509
2022-03-03 00:03:02 - train: epoch 0094, iter [00700, 05004], lr: 0.057623, loss: 1.8834
2022-03-03 00:03:35 - train: epoch 0094, iter [00800, 05004], lr: 0.057623, loss: 1.8061
2022-03-03 00:04:09 - train: epoch 0094, iter [00900, 05004], lr: 0.057623, loss: 1.7740
2022-03-03 00:04:42 - train: epoch 0094, iter [01000, 05004], lr: 0.057623, loss: 1.8725
2022-03-03 00:05:14 - train: epoch 0094, iter [01100, 05004], lr: 0.057623, loss: 2.0915
2022-03-03 00:05:47 - train: epoch 0094, iter [01200, 05004], lr: 0.057623, loss: 1.8663
2022-03-03 00:06:21 - train: epoch 0094, iter [01300, 05004], lr: 0.057623, loss: 1.7052
2022-03-03 00:06:54 - train: epoch 0094, iter [01400, 05004], lr: 0.057623, loss: 1.6787
2022-03-03 00:07:27 - train: epoch 0094, iter [01500, 05004], lr: 0.057623, loss: 1.8576
2022-03-03 00:08:01 - train: epoch 0094, iter [01600, 05004], lr: 0.057623, loss: 2.2249
2022-03-03 00:08:34 - train: epoch 0094, iter [01700, 05004], lr: 0.057623, loss: 1.7991
2022-03-03 00:09:07 - train: epoch 0094, iter [01800, 05004], lr: 0.057623, loss: 1.7173
2022-03-03 00:09:41 - train: epoch 0094, iter [01900, 05004], lr: 0.057623, loss: 1.8301
2022-03-03 00:10:14 - train: epoch 0094, iter [02000, 05004], lr: 0.057623, loss: 1.5722
2022-03-03 00:10:46 - train: epoch 0094, iter [02100, 05004], lr: 0.057623, loss: 1.8813
2022-03-03 00:11:20 - train: epoch 0094, iter [02200, 05004], lr: 0.057623, loss: 1.7470
2022-03-03 00:11:53 - train: epoch 0094, iter [02300, 05004], lr: 0.057623, loss: 1.6470
2022-03-03 00:12:25 - train: epoch 0094, iter [02400, 05004], lr: 0.057623, loss: 1.8371
2022-03-03 00:12:59 - train: epoch 0094, iter [02500, 05004], lr: 0.057623, loss: 1.8008
2022-03-03 00:13:32 - train: epoch 0094, iter [02600, 05004], lr: 0.057623, loss: 1.6957
2022-03-03 00:14:05 - train: epoch 0094, iter [02700, 05004], lr: 0.057623, loss: 1.8456
2022-03-03 00:14:38 - train: epoch 0094, iter [02800, 05004], lr: 0.057623, loss: 1.9003
2022-03-03 00:15:11 - train: epoch 0094, iter [02900, 05004], lr: 0.057623, loss: 1.8121
2022-03-03 00:15:44 - train: epoch 0094, iter [03000, 05004], lr: 0.057623, loss: 1.7813
2022-03-03 00:16:17 - train: epoch 0094, iter [03100, 05004], lr: 0.057623, loss: 2.0492
2022-03-03 00:16:50 - train: epoch 0094, iter [03200, 05004], lr: 0.057623, loss: 1.7651
2022-03-03 00:17:23 - train: epoch 0094, iter [03300, 05004], lr: 0.057623, loss: 1.7523
2022-03-03 00:17:56 - train: epoch 0094, iter [03400, 05004], lr: 0.057623, loss: 1.8349
2022-03-03 00:18:29 - train: epoch 0094, iter [03500, 05004], lr: 0.057623, loss: 2.0909
2022-03-03 00:19:02 - train: epoch 0094, iter [03600, 05004], lr: 0.057623, loss: 1.7813
2022-03-03 00:19:36 - train: epoch 0094, iter [03700, 05004], lr: 0.057623, loss: 2.0125
2022-03-03 00:20:09 - train: epoch 0094, iter [03800, 05004], lr: 0.057623, loss: 1.7407
2022-03-03 00:20:42 - train: epoch 0094, iter [03900, 05004], lr: 0.057623, loss: 1.8127
2022-03-03 00:21:15 - train: epoch 0094, iter [04000, 05004], lr: 0.057623, loss: 1.9070
2022-03-03 00:21:47 - train: epoch 0094, iter [04100, 05004], lr: 0.057623, loss: 2.1231
2022-03-03 00:22:21 - train: epoch 0094, iter [04200, 05004], lr: 0.057623, loss: 1.6842
2022-03-03 00:22:54 - train: epoch 0094, iter [04300, 05004], lr: 0.057623, loss: 2.0752
2022-03-03 00:23:27 - train: epoch 0094, iter [04400, 05004], lr: 0.057623, loss: 1.9648
2022-03-03 00:24:00 - train: epoch 0094, iter [04500, 05004], lr: 0.057623, loss: 1.8550
2022-03-03 00:24:33 - train: epoch 0094, iter [04600, 05004], lr: 0.057623, loss: 2.0142
2022-03-03 00:25:07 - train: epoch 0094, iter [04700, 05004], lr: 0.057623, loss: 1.9330
2022-03-03 00:25:40 - train: epoch 0094, iter [04800, 05004], lr: 0.057623, loss: 1.8408
2022-03-03 00:26:13 - train: epoch 0094, iter [04900, 05004], lr: 0.057623, loss: 1.7082
2022-03-03 00:26:44 - train: epoch 0094, iter [05000, 05004], lr: 0.057623, loss: 1.7191
2022-03-03 00:26:45 - train: epoch 094, train_loss: 1.8431
2022-03-03 00:27:59 - eval: epoch: 094, acc1: 61.058%, acc5: 84.422%, test_loss: 1.6146, per_image_load_time: 1.902ms, per_image_inference_time: 0.538ms
2022-03-03 00:27:59 - until epoch: 094, best_acc1: 61.836%
2022-03-03 00:27:59 - epoch 095 lr: 0.05682568059703659
2022-03-03 00:28:37 - train: epoch 0095, iter [00100, 05004], lr: 0.056826, loss: 1.7495
2022-03-03 00:29:10 - train: epoch 0095, iter [00200, 05004], lr: 0.056826, loss: 1.9160
2022-03-03 00:29:43 - train: epoch 0095, iter [00300, 05004], lr: 0.056826, loss: 1.7782
2022-03-03 00:30:16 - train: epoch 0095, iter [00400, 05004], lr: 0.056826, loss: 1.9262
2022-03-03 00:30:49 - train: epoch 0095, iter [00500, 05004], lr: 0.056826, loss: 1.8339
2022-03-03 00:31:22 - train: epoch 0095, iter [00600, 05004], lr: 0.056826, loss: 1.7541
2022-03-03 00:31:56 - train: epoch 0095, iter [00700, 05004], lr: 0.056826, loss: 1.9878
2022-03-03 00:32:28 - train: epoch 0095, iter [00800, 05004], lr: 0.056826, loss: 2.0477
2022-03-03 00:33:02 - train: epoch 0095, iter [00900, 05004], lr: 0.056826, loss: 2.0318
2022-03-03 00:33:36 - train: epoch 0095, iter [01000, 05004], lr: 0.056826, loss: 2.0486
2022-03-03 00:34:08 - train: epoch 0095, iter [01100, 05004], lr: 0.056826, loss: 1.6625
2022-03-03 00:34:42 - train: epoch 0095, iter [01200, 05004], lr: 0.056826, loss: 1.8241
2022-03-03 00:35:15 - train: epoch 0095, iter [01300, 05004], lr: 0.056826, loss: 1.8052
2022-03-03 00:35:49 - train: epoch 0095, iter [01400, 05004], lr: 0.056826, loss: 1.8947
2022-03-03 00:36:21 - train: epoch 0095, iter [01500, 05004], lr: 0.056826, loss: 1.7590
2022-03-03 00:36:55 - train: epoch 0095, iter [01600, 05004], lr: 0.056826, loss: 1.5660
2022-03-03 00:37:28 - train: epoch 0095, iter [01700, 05004], lr: 0.056826, loss: 1.7274
2022-03-03 00:38:02 - train: epoch 0095, iter [01800, 05004], lr: 0.056826, loss: 1.7659
2022-03-03 00:38:35 - train: epoch 0095, iter [01900, 05004], lr: 0.056826, loss: 1.7300
2022-03-03 00:39:08 - train: epoch 0095, iter [02000, 05004], lr: 0.056826, loss: 1.9333
2022-03-03 00:39:42 - train: epoch 0095, iter [02100, 05004], lr: 0.056826, loss: 1.9458
2022-03-03 00:40:15 - train: epoch 0095, iter [02200, 05004], lr: 0.056826, loss: 1.4510
2022-03-03 00:40:48 - train: epoch 0095, iter [02300, 05004], lr: 0.056826, loss: 1.6948
2022-03-03 00:41:21 - train: epoch 0095, iter [02400, 05004], lr: 0.056826, loss: 2.1580
2022-03-03 00:41:54 - train: epoch 0095, iter [02500, 05004], lr: 0.056826, loss: 1.6733
2022-03-03 00:42:28 - train: epoch 0095, iter [02600, 05004], lr: 0.056826, loss: 1.9193
2022-03-03 00:43:01 - train: epoch 0095, iter [02700, 05004], lr: 0.056826, loss: 1.6955
2022-03-03 00:43:35 - train: epoch 0095, iter [02800, 05004], lr: 0.056826, loss: 1.6990
2022-03-03 00:44:08 - train: epoch 0095, iter [02900, 05004], lr: 0.056826, loss: 1.7050
2022-03-03 00:44:41 - train: epoch 0095, iter [03000, 05004], lr: 0.056826, loss: 1.9797
2022-03-03 00:45:14 - train: epoch 0095, iter [03100, 05004], lr: 0.056826, loss: 1.9591
2022-03-03 00:45:47 - train: epoch 0095, iter [03200, 05004], lr: 0.056826, loss: 1.8263
2022-03-03 00:46:19 - train: epoch 0095, iter [03300, 05004], lr: 0.056826, loss: 1.9277
2022-03-03 00:46:53 - train: epoch 0095, iter [03400, 05004], lr: 0.056826, loss: 1.7031
2022-03-03 00:47:27 - train: epoch 0095, iter [03500, 05004], lr: 0.056826, loss: 1.8293
2022-03-03 00:47:59 - train: epoch 0095, iter [03600, 05004], lr: 0.056826, loss: 1.8265
2022-03-03 00:48:33 - train: epoch 0095, iter [03700, 05004], lr: 0.056826, loss: 1.6688
2022-03-03 00:49:05 - train: epoch 0095, iter [03800, 05004], lr: 0.056826, loss: 1.6804
2022-03-03 00:49:38 - train: epoch 0095, iter [03900, 05004], lr: 0.056826, loss: 1.8957
2022-03-03 00:50:11 - train: epoch 0095, iter [04000, 05004], lr: 0.056826, loss: 1.5290
2022-03-03 00:50:45 - train: epoch 0095, iter [04100, 05004], lr: 0.056826, loss: 1.8866
2022-03-03 00:51:17 - train: epoch 0095, iter [04200, 05004], lr: 0.056826, loss: 1.8098
2022-03-03 00:51:50 - train: epoch 0095, iter [04300, 05004], lr: 0.056826, loss: 1.9443
2022-03-03 00:52:24 - train: epoch 0095, iter [04400, 05004], lr: 0.056826, loss: 1.9546
2022-03-03 00:52:57 - train: epoch 0095, iter [04500, 05004], lr: 0.056826, loss: 1.7096
2022-03-03 00:53:30 - train: epoch 0095, iter [04600, 05004], lr: 0.056826, loss: 1.7285
2022-03-03 00:54:02 - train: epoch 0095, iter [04700, 05004], lr: 0.056826, loss: 1.8531
2022-03-03 00:54:35 - train: epoch 0095, iter [04800, 05004], lr: 0.056826, loss: 1.9562
2022-03-03 00:55:09 - train: epoch 0095, iter [04900, 05004], lr: 0.056826, loss: 1.7707
2022-03-03 00:55:40 - train: epoch 0095, iter [05000, 05004], lr: 0.056826, loss: 1.6308
2022-03-03 00:55:42 - train: epoch 095, train_loss: 1.8395
2022-03-03 00:56:55 - eval: epoch: 095, acc1: 61.446%, acc5: 84.784%, test_loss: 1.5906, per_image_load_time: 2.287ms, per_image_inference_time: 0.554ms
2022-03-03 00:56:56 - until epoch: 095, best_acc1: 61.836%
2022-03-03 00:56:56 - epoch 096 lr: 0.05602683401276615
2022-03-03 00:57:33 - train: epoch 0096, iter [00100, 05004], lr: 0.056027, loss: 1.8655
2022-03-03 00:58:07 - train: epoch 0096, iter [00200, 05004], lr: 0.056027, loss: 1.7443
2022-03-03 00:58:40 - train: epoch 0096, iter [00300, 05004], lr: 0.056027, loss: 1.8173
2022-03-03 00:59:13 - train: epoch 0096, iter [00400, 05004], lr: 0.056027, loss: 1.6050
2022-03-03 00:59:46 - train: epoch 0096, iter [00500, 05004], lr: 0.056027, loss: 1.7792
2022-03-03 01:00:18 - train: epoch 0096, iter [00600, 05004], lr: 0.056027, loss: 2.0058
2022-03-03 01:00:51 - train: epoch 0096, iter [00700, 05004], lr: 0.056027, loss: 1.7218
2022-03-03 01:01:24 - train: epoch 0096, iter [00800, 05004], lr: 0.056027, loss: 1.6615
2022-03-03 01:01:58 - train: epoch 0096, iter [00900, 05004], lr: 0.056027, loss: 1.8774
2022-03-03 01:02:32 - train: epoch 0096, iter [01000, 05004], lr: 0.056027, loss: 1.7660
2022-03-03 01:03:05 - train: epoch 0096, iter [01100, 05004], lr: 0.056027, loss: 1.8692
2022-03-03 01:03:38 - train: epoch 0096, iter [01200, 05004], lr: 0.056027, loss: 2.0155
2022-03-03 01:04:11 - train: epoch 0096, iter [01300, 05004], lr: 0.056027, loss: 1.9232
2022-03-03 01:04:44 - train: epoch 0096, iter [01400, 05004], lr: 0.056027, loss: 1.8049
2022-03-03 01:05:17 - train: epoch 0096, iter [01500, 05004], lr: 0.056027, loss: 1.6758
2022-03-03 01:05:50 - train: epoch 0096, iter [01600, 05004], lr: 0.056027, loss: 1.4895
2022-03-03 01:06:23 - train: epoch 0096, iter [01700, 05004], lr: 0.056027, loss: 1.7609
2022-03-03 01:06:56 - train: epoch 0096, iter [01800, 05004], lr: 0.056027, loss: 2.0164
2022-03-03 01:07:29 - train: epoch 0096, iter [01900, 05004], lr: 0.056027, loss: 2.0077
2022-03-03 01:08:03 - train: epoch 0096, iter [02000, 05004], lr: 0.056027, loss: 1.8418
2022-03-03 01:08:35 - train: epoch 0096, iter [02100, 05004], lr: 0.056027, loss: 1.7932
2022-03-03 01:09:09 - train: epoch 0096, iter [02200, 05004], lr: 0.056027, loss: 1.6417
2022-03-03 01:09:43 - train: epoch 0096, iter [02300, 05004], lr: 0.056027, loss: 1.7805
2022-03-03 01:10:15 - train: epoch 0096, iter [02400, 05004], lr: 0.056027, loss: 1.5932
2022-03-03 01:10:49 - train: epoch 0096, iter [02500, 05004], lr: 0.056027, loss: 1.7344
2022-03-03 01:11:22 - train: epoch 0096, iter [02600, 05004], lr: 0.056027, loss: 1.6878
2022-03-03 01:11:55 - train: epoch 0096, iter [02700, 05004], lr: 0.056027, loss: 1.8426
2022-03-03 01:12:28 - train: epoch 0096, iter [02800, 05004], lr: 0.056027, loss: 2.0953
2022-03-03 01:13:01 - train: epoch 0096, iter [02900, 05004], lr: 0.056027, loss: 1.7428
2022-03-03 01:13:35 - train: epoch 0096, iter [03000, 05004], lr: 0.056027, loss: 1.9322
2022-03-03 01:14:08 - train: epoch 0096, iter [03100, 05004], lr: 0.056027, loss: 2.0096
2022-03-03 01:14:41 - train: epoch 0096, iter [03200, 05004], lr: 0.056027, loss: 1.8730
2022-03-03 01:15:15 - train: epoch 0096, iter [03300, 05004], lr: 0.056027, loss: 2.0141
2022-03-03 01:15:48 - train: epoch 0096, iter [03400, 05004], lr: 0.056027, loss: 1.6395
2022-03-03 01:16:21 - train: epoch 0096, iter [03500, 05004], lr: 0.056027, loss: 1.7450
2022-03-03 01:16:55 - train: epoch 0096, iter [03600, 05004], lr: 0.056027, loss: 1.7375
2022-03-03 01:17:28 - train: epoch 0096, iter [03700, 05004], lr: 0.056027, loss: 1.8134
2022-03-03 01:18:00 - train: epoch 0096, iter [03800, 05004], lr: 0.056027, loss: 1.5971
2022-03-03 01:18:33 - train: epoch 0096, iter [03900, 05004], lr: 0.056027, loss: 1.8245
2022-03-03 01:19:07 - train: epoch 0096, iter [04000, 05004], lr: 0.056027, loss: 1.5508
2022-03-03 01:19:40 - train: epoch 0096, iter [04100, 05004], lr: 0.056027, loss: 1.6740
2022-03-03 01:20:13 - train: epoch 0096, iter [04200, 05004], lr: 0.056027, loss: 1.8003
2022-03-03 01:20:46 - train: epoch 0096, iter [04300, 05004], lr: 0.056027, loss: 1.5276
2022-03-03 01:21:19 - train: epoch 0096, iter [04400, 05004], lr: 0.056027, loss: 1.7178
2022-03-03 01:21:52 - train: epoch 0096, iter [04500, 05004], lr: 0.056027, loss: 1.7921
2022-03-03 01:22:26 - train: epoch 0096, iter [04600, 05004], lr: 0.056027, loss: 1.7904
2022-03-03 01:22:59 - train: epoch 0096, iter [04700, 05004], lr: 0.056027, loss: 1.9409
2022-03-03 01:23:32 - train: epoch 0096, iter [04800, 05004], lr: 0.056027, loss: 2.0210
2022-03-03 01:24:05 - train: epoch 0096, iter [04900, 05004], lr: 0.056027, loss: 2.1530
2022-03-03 01:24:37 - train: epoch 0096, iter [05000, 05004], lr: 0.056027, loss: 1.7932
2022-03-03 01:24:38 - train: epoch 096, train_loss: 1.8309
2022-03-03 01:25:52 - eval: epoch: 096, acc1: 60.236%, acc5: 83.908%, test_loss: 1.6514, per_image_load_time: 2.170ms, per_image_inference_time: 0.570ms
2022-03-03 01:25:53 - until epoch: 096, best_acc1: 61.836%
2022-03-03 01:25:53 - epoch 097 lr: 0.05522642316338268
2022-03-03 01:26:32 - train: epoch 0097, iter [00100, 05004], lr: 0.055226, loss: 1.9495
2022-03-03 01:27:04 - train: epoch 0097, iter [00200, 05004], lr: 0.055226, loss: 1.5263
2022-03-03 01:27:38 - train: epoch 0097, iter [00300, 05004], lr: 0.055226, loss: 1.9676
2022-03-03 01:28:11 - train: epoch 0097, iter [00400, 05004], lr: 0.055226, loss: 2.1066
2022-03-03 01:28:44 - train: epoch 0097, iter [00500, 05004], lr: 0.055226, loss: 1.8466
2022-03-03 01:29:18 - train: epoch 0097, iter [00600, 05004], lr: 0.055226, loss: 1.8668
2022-03-03 01:29:50 - train: epoch 0097, iter [00700, 05004], lr: 0.055226, loss: 1.7097
2022-03-03 01:30:23 - train: epoch 0097, iter [00800, 05004], lr: 0.055226, loss: 1.6773
2022-03-03 01:30:56 - train: epoch 0097, iter [00900, 05004], lr: 0.055226, loss: 1.6815
2022-03-03 01:31:30 - train: epoch 0097, iter [01000, 05004], lr: 0.055226, loss: 1.7750
2022-03-03 01:32:03 - train: epoch 0097, iter [01100, 05004], lr: 0.055226, loss: 1.5997
2022-03-03 01:32:36 - train: epoch 0097, iter [01200, 05004], lr: 0.055226, loss: 1.6742
2022-03-03 01:33:10 - train: epoch 0097, iter [01300, 05004], lr: 0.055226, loss: 1.7495
2022-03-03 01:33:44 - train: epoch 0097, iter [01400, 05004], lr: 0.055226, loss: 1.7100
2022-03-03 01:34:17 - train: epoch 0097, iter [01500, 05004], lr: 0.055226, loss: 1.7300
2022-03-03 01:34:50 - train: epoch 0097, iter [01600, 05004], lr: 0.055226, loss: 1.7167
2022-03-03 01:35:23 - train: epoch 0097, iter [01700, 05004], lr: 0.055226, loss: 1.7742
2022-03-03 01:35:56 - train: epoch 0097, iter [01800, 05004], lr: 0.055226, loss: 1.8937
2022-03-03 01:36:30 - train: epoch 0097, iter [01900, 05004], lr: 0.055226, loss: 1.7025
2022-03-03 01:37:03 - train: epoch 0097, iter [02000, 05004], lr: 0.055226, loss: 1.7901
2022-03-03 01:37:36 - train: epoch 0097, iter [02100, 05004], lr: 0.055226, loss: 1.7433
2022-03-03 01:38:10 - train: epoch 0097, iter [02200, 05004], lr: 0.055226, loss: 2.0078
2022-03-03 01:38:42 - train: epoch 0097, iter [02300, 05004], lr: 0.055226, loss: 1.7461
2022-03-03 01:39:16 - train: epoch 0097, iter [02400, 05004], lr: 0.055226, loss: 1.9264
2022-03-03 01:39:49 - train: epoch 0097, iter [02500, 05004], lr: 0.055226, loss: 1.8068
2022-03-03 01:40:22 - train: epoch 0097, iter [02600, 05004], lr: 0.055226, loss: 2.0246
2022-03-03 01:40:55 - train: epoch 0097, iter [02700, 05004], lr: 0.055226, loss: 1.6443
2022-03-03 01:41:28 - train: epoch 0097, iter [02800, 05004], lr: 0.055226, loss: 1.7137
2022-03-03 01:42:02 - train: epoch 0097, iter [02900, 05004], lr: 0.055226, loss: 1.9933
2022-03-03 01:42:35 - train: epoch 0097, iter [03000, 05004], lr: 0.055226, loss: 1.8345
2022-03-03 01:43:08 - train: epoch 0097, iter [03100, 05004], lr: 0.055226, loss: 1.8612
2022-03-03 01:43:41 - train: epoch 0097, iter [03200, 05004], lr: 0.055226, loss: 1.8155
2022-03-03 01:44:14 - train: epoch 0097, iter [03300, 05004], lr: 0.055226, loss: 2.0071
2022-03-03 01:44:48 - train: epoch 0097, iter [03400, 05004], lr: 0.055226, loss: 1.9893
2022-03-03 01:45:20 - train: epoch 0097, iter [03500, 05004], lr: 0.055226, loss: 1.9051
2022-03-03 01:45:54 - train: epoch 0097, iter [03600, 05004], lr: 0.055226, loss: 1.8034
2022-03-03 01:46:26 - train: epoch 0097, iter [03700, 05004], lr: 0.055226, loss: 1.4922
2022-03-03 01:46:59 - train: epoch 0097, iter [03800, 05004], lr: 0.055226, loss: 1.6881
2022-03-03 01:47:32 - train: epoch 0097, iter [03900, 05004], lr: 0.055226, loss: 1.8342
2022-03-03 01:48:05 - train: epoch 0097, iter [04000, 05004], lr: 0.055226, loss: 1.8986
2022-03-03 01:48:38 - train: epoch 0097, iter [04100, 05004], lr: 0.055226, loss: 1.7660
2022-03-03 01:49:11 - train: epoch 0097, iter [04200, 05004], lr: 0.055226, loss: 1.7743
2022-03-03 01:49:44 - train: epoch 0097, iter [04300, 05004], lr: 0.055226, loss: 1.8547
2022-03-03 01:50:17 - train: epoch 0097, iter [04400, 05004], lr: 0.055226, loss: 1.7646
2022-03-03 01:50:50 - train: epoch 0097, iter [04500, 05004], lr: 0.055226, loss: 1.8865
2022-03-03 01:51:23 - train: epoch 0097, iter [04600, 05004], lr: 0.055226, loss: 1.9839
2022-03-03 01:51:56 - train: epoch 0097, iter [04700, 05004], lr: 0.055226, loss: 1.8377
2022-03-03 01:52:29 - train: epoch 0097, iter [04800, 05004], lr: 0.055226, loss: 1.7855
2022-03-03 01:53:03 - train: epoch 0097, iter [04900, 05004], lr: 0.055226, loss: 2.0499
2022-03-03 01:53:34 - train: epoch 0097, iter [05000, 05004], lr: 0.055226, loss: 1.8650
2022-03-03 01:53:36 - train: epoch 097, train_loss: 1.8245
2022-03-03 01:54:49 - eval: epoch: 097, acc1: 60.840%, acc5: 84.252%, test_loss: 1.6168, per_image_load_time: 1.868ms, per_image_inference_time: 0.550ms
2022-03-03 01:54:49 - until epoch: 097, best_acc1: 61.836%
2022-03-03 01:54:49 - epoch 098 lr: 0.054424655795567926
2022-03-03 01:55:27 - train: epoch 0098, iter [00100, 05004], lr: 0.054425, loss: 1.9345
2022-03-03 01:56:01 - train: epoch 0098, iter [00200, 05004], lr: 0.054425, loss: 1.9820
2022-03-03 01:56:33 - train: epoch 0098, iter [00300, 05004], lr: 0.054425, loss: 1.9835
2022-03-03 01:57:06 - train: epoch 0098, iter [00400, 05004], lr: 0.054425, loss: 1.7047
2022-03-03 01:57:40 - train: epoch 0098, iter [00500, 05004], lr: 0.054425, loss: 1.8078
2022-03-03 01:58:12 - train: epoch 0098, iter [00600, 05004], lr: 0.054425, loss: 1.9453
2022-03-03 01:58:46 - train: epoch 0098, iter [00700, 05004], lr: 0.054425, loss: 1.7411
2022-03-03 01:59:19 - train: epoch 0098, iter [00800, 05004], lr: 0.054425, loss: 1.8632
2022-03-03 01:59:52 - train: epoch 0098, iter [00900, 05004], lr: 0.054425, loss: 1.7526
2022-03-03 02:00:25 - train: epoch 0098, iter [01000, 05004], lr: 0.054425, loss: 1.6065
2022-03-03 02:00:59 - train: epoch 0098, iter [01100, 05004], lr: 0.054425, loss: 1.4642
2022-03-03 02:01:32 - train: epoch 0098, iter [01200, 05004], lr: 0.054425, loss: 1.6592
2022-03-03 02:02:05 - train: epoch 0098, iter [01300, 05004], lr: 0.054425, loss: 1.9237
2022-03-03 02:02:37 - train: epoch 0098, iter [01400, 05004], lr: 0.054425, loss: 1.8116
2022-03-03 02:03:11 - train: epoch 0098, iter [01500, 05004], lr: 0.054425, loss: 1.6434
2022-03-03 02:03:44 - train: epoch 0098, iter [01600, 05004], lr: 0.054425, loss: 1.7024
2022-03-03 02:04:17 - train: epoch 0098, iter [01700, 05004], lr: 0.054425, loss: 1.8238
2022-03-03 02:04:50 - train: epoch 0098, iter [01800, 05004], lr: 0.054425, loss: 1.8010
2022-03-03 02:05:23 - train: epoch 0098, iter [01900, 05004], lr: 0.054425, loss: 1.6435
2022-03-03 02:05:56 - train: epoch 0098, iter [02000, 05004], lr: 0.054425, loss: 1.9581
2022-03-03 02:06:29 - train: epoch 0098, iter [02100, 05004], lr: 0.054425, loss: 1.7987
2022-03-03 02:07:02 - train: epoch 0098, iter [02200, 05004], lr: 0.054425, loss: 1.7790
2022-03-03 02:07:35 - train: epoch 0098, iter [02300, 05004], lr: 0.054425, loss: 1.7063
2022-03-03 02:08:08 - train: epoch 0098, iter [02400, 05004], lr: 0.054425, loss: 1.7345
2022-03-03 02:08:41 - train: epoch 0098, iter [02500, 05004], lr: 0.054425, loss: 1.9781
2022-03-03 02:09:14 - train: epoch 0098, iter [02600, 05004], lr: 0.054425, loss: 1.7953
2022-03-03 02:09:47 - train: epoch 0098, iter [02700, 05004], lr: 0.054425, loss: 1.8943
2022-03-03 02:10:19 - train: epoch 0098, iter [02800, 05004], lr: 0.054425, loss: 1.8350
2022-03-03 02:10:53 - train: epoch 0098, iter [02900, 05004], lr: 0.054425, loss: 1.6857
2022-03-03 02:11:26 - train: epoch 0098, iter [03000, 05004], lr: 0.054425, loss: 1.8279
2022-03-03 02:11:59 - train: epoch 0098, iter [03100, 05004], lr: 0.054425, loss: 1.8396
2022-03-03 02:12:32 - train: epoch 0098, iter [03200, 05004], lr: 0.054425, loss: 1.5895
2022-03-03 02:13:05 - train: epoch 0098, iter [03300, 05004], lr: 0.054425, loss: 1.8582
2022-03-03 02:13:38 - train: epoch 0098, iter [03400, 05004], lr: 0.054425, loss: 1.9064
2022-03-03 02:14:11 - train: epoch 0098, iter [03500, 05004], lr: 0.054425, loss: 1.8449
2022-03-03 02:14:44 - train: epoch 0098, iter [03600, 05004], lr: 0.054425, loss: 2.0649
2022-03-03 02:15:17 - train: epoch 0098, iter [03700, 05004], lr: 0.054425, loss: 1.8563
2022-03-03 02:15:50 - train: epoch 0098, iter [03800, 05004], lr: 0.054425, loss: 1.8171
2022-03-03 02:16:24 - train: epoch 0098, iter [03900, 05004], lr: 0.054425, loss: 1.8175
2022-03-03 02:16:57 - train: epoch 0098, iter [04000, 05004], lr: 0.054425, loss: 1.7632
2022-03-03 02:17:29 - train: epoch 0098, iter [04100, 05004], lr: 0.054425, loss: 2.0401
2022-03-03 02:18:02 - train: epoch 0098, iter [04200, 05004], lr: 0.054425, loss: 1.8231
2022-03-03 02:18:36 - train: epoch 0098, iter [04300, 05004], lr: 0.054425, loss: 1.6281
2022-03-03 02:19:09 - train: epoch 0098, iter [04400, 05004], lr: 0.054425, loss: 2.2333
2022-03-03 02:19:42 - train: epoch 0098, iter [04500, 05004], lr: 0.054425, loss: 1.9805
2022-03-03 02:20:15 - train: epoch 0098, iter [04600, 05004], lr: 0.054425, loss: 1.9141
2022-03-03 02:20:48 - train: epoch 0098, iter [04700, 05004], lr: 0.054425, loss: 1.6219
2022-03-03 02:21:21 - train: epoch 0098, iter [04800, 05004], lr: 0.054425, loss: 1.6463
2022-03-03 02:21:55 - train: epoch 0098, iter [04900, 05004], lr: 0.054425, loss: 1.9240
2022-03-03 02:22:27 - train: epoch 0098, iter [05000, 05004], lr: 0.054425, loss: 1.8697
2022-03-03 02:22:28 - train: epoch 098, train_loss: 1.8175
2022-03-03 02:23:41 - eval: epoch: 098, acc1: 61.548%, acc5: 84.998%, test_loss: 1.5851, per_image_load_time: 2.351ms, per_image_inference_time: 0.535ms
2022-03-03 02:23:42 - until epoch: 098, best_acc1: 61.836%
2022-03-03 02:23:42 - epoch 099 lr: 0.05362174000808813
2022-03-03 02:24:20 - train: epoch 0099, iter [00100, 05004], lr: 0.053622, loss: 1.5985
2022-03-03 02:24:53 - train: epoch 0099, iter [00200, 05004], lr: 0.053622, loss: 1.6672
2022-03-03 02:25:26 - train: epoch 0099, iter [00300, 05004], lr: 0.053622, loss: 1.6861
2022-03-03 02:25:59 - train: epoch 0099, iter [00400, 05004], lr: 0.053622, loss: 1.9463
2022-03-03 02:26:32 - train: epoch 0099, iter [00500, 05004], lr: 0.053622, loss: 1.9565
2022-03-03 02:27:05 - train: epoch 0099, iter [00600, 05004], lr: 0.053622, loss: 1.7945
2022-03-03 02:27:37 - train: epoch 0099, iter [00700, 05004], lr: 0.053622, loss: 1.8019
2022-03-03 02:28:12 - train: epoch 0099, iter [00800, 05004], lr: 0.053622, loss: 1.8210
2022-03-03 02:28:45 - train: epoch 0099, iter [00900, 05004], lr: 0.053622, loss: 1.6944
2022-03-03 02:29:19 - train: epoch 0099, iter [01000, 05004], lr: 0.053622, loss: 1.8856
2022-03-03 02:29:52 - train: epoch 0099, iter [01100, 05004], lr: 0.053622, loss: 1.8960
2022-03-03 02:30:25 - train: epoch 0099, iter [01200, 05004], lr: 0.053622, loss: 1.7149
2022-03-03 02:30:59 - train: epoch 0099, iter [01300, 05004], lr: 0.053622, loss: 1.8554
2022-03-03 02:31:31 - train: epoch 0099, iter [01400, 05004], lr: 0.053622, loss: 1.8984
2022-03-03 02:32:05 - train: epoch 0099, iter [01500, 05004], lr: 0.053622, loss: 1.7062
2022-03-03 02:32:38 - train: epoch 0099, iter [01600, 05004], lr: 0.053622, loss: 1.9675
2022-03-03 02:33:11 - train: epoch 0099, iter [01700, 05004], lr: 0.053622, loss: 1.7787
2022-03-03 02:33:44 - train: epoch 0099, iter [01800, 05004], lr: 0.053622, loss: 1.9558
2022-03-03 02:34:17 - train: epoch 0099, iter [01900, 05004], lr: 0.053622, loss: 1.6834
2022-03-03 02:34:50 - train: epoch 0099, iter [02000, 05004], lr: 0.053622, loss: 1.5911
2022-03-03 02:35:24 - train: epoch 0099, iter [02100, 05004], lr: 0.053622, loss: 1.6688
2022-03-03 02:35:57 - train: epoch 0099, iter [02200, 05004], lr: 0.053622, loss: 1.8474
2022-03-03 02:36:31 - train: epoch 0099, iter [02300, 05004], lr: 0.053622, loss: 1.8412
2022-03-03 02:37:04 - train: epoch 0099, iter [02400, 05004], lr: 0.053622, loss: 2.0582
2022-03-03 02:37:36 - train: epoch 0099, iter [02500, 05004], lr: 0.053622, loss: 1.8466
2022-03-03 02:38:10 - train: epoch 0099, iter [02600, 05004], lr: 0.053622, loss: 1.6422
2022-03-03 02:38:43 - train: epoch 0099, iter [02700, 05004], lr: 0.053622, loss: 1.8049
2022-03-03 02:39:17 - train: epoch 0099, iter [02800, 05004], lr: 0.053622, loss: 1.8824
2022-03-03 02:39:50 - train: epoch 0099, iter [02900, 05004], lr: 0.053622, loss: 1.9421
2022-03-03 02:40:23 - train: epoch 0099, iter [03000, 05004], lr: 0.053622, loss: 1.9601
2022-03-03 02:40:56 - train: epoch 0099, iter [03100, 05004], lr: 0.053622, loss: 1.7057
2022-03-03 02:41:29 - train: epoch 0099, iter [03200, 05004], lr: 0.053622, loss: 2.0071
2022-03-03 02:42:02 - train: epoch 0099, iter [03300, 05004], lr: 0.053622, loss: 1.6951
2022-03-03 02:42:35 - train: epoch 0099, iter [03400, 05004], lr: 0.053622, loss: 1.8463
2022-03-03 02:43:07 - train: epoch 0099, iter [03500, 05004], lr: 0.053622, loss: 1.8577
2022-03-03 02:43:42 - train: epoch 0099, iter [03600, 05004], lr: 0.053622, loss: 1.7406
2022-03-03 02:44:15 - train: epoch 0099, iter [03700, 05004], lr: 0.053622, loss: 1.6536
2022-03-03 02:44:48 - train: epoch 0099, iter [03800, 05004], lr: 0.053622, loss: 1.9104
2022-03-03 02:45:22 - train: epoch 0099, iter [03900, 05004], lr: 0.053622, loss: 1.8430
2022-03-03 02:45:55 - train: epoch 0099, iter [04000, 05004], lr: 0.053622, loss: 1.9245
2022-03-03 02:46:28 - train: epoch 0099, iter [04100, 05004], lr: 0.053622, loss: 1.7046
2022-03-03 02:47:02 - train: epoch 0099, iter [04200, 05004], lr: 0.053622, loss: 1.9166
2022-03-03 02:47:34 - train: epoch 0099, iter [04300, 05004], lr: 0.053622, loss: 1.8127
2022-03-03 02:48:07 - train: epoch 0099, iter [04400, 05004], lr: 0.053622, loss: 1.9255
2022-03-03 02:48:40 - train: epoch 0099, iter [04500, 05004], lr: 0.053622, loss: 1.9679
2022-03-03 02:49:14 - train: epoch 0099, iter [04600, 05004], lr: 0.053622, loss: 2.0977
2022-03-03 02:49:47 - train: epoch 0099, iter [04700, 05004], lr: 0.053622, loss: 1.6986
2022-03-03 02:50:21 - train: epoch 0099, iter [04800, 05004], lr: 0.053622, loss: 1.9089
2022-03-03 02:50:54 - train: epoch 0099, iter [04900, 05004], lr: 0.053622, loss: 1.6851
2022-03-03 02:51:26 - train: epoch 0099, iter [05000, 05004], lr: 0.053622, loss: 1.8687
2022-03-03 02:51:27 - train: epoch 099, train_loss: 1.8116
2022-03-03 02:52:40 - eval: epoch: 099, acc1: 60.780%, acc5: 84.240%, test_loss: 1.6236, per_image_load_time: 0.868ms, per_image_inference_time: 0.492ms
2022-03-03 02:52:40 - until epoch: 099, best_acc1: 61.836%
2022-03-03 02:52:40 - epoch 100 lr: 0.05281788419778188
2022-03-03 02:53:18 - train: epoch 0100, iter [00100, 05004], lr: 0.052818, loss: 1.7505
2022-03-03 02:53:51 - train: epoch 0100, iter [00200, 05004], lr: 0.052818, loss: 1.8265
2022-03-03 02:54:25 - train: epoch 0100, iter [00300, 05004], lr: 0.052818, loss: 1.6588
2022-03-03 02:54:58 - train: epoch 0100, iter [00400, 05004], lr: 0.052818, loss: 1.5364
2022-03-03 02:55:31 - train: epoch 0100, iter [00500, 05004], lr: 0.052818, loss: 1.8888
2022-03-03 02:56:03 - train: epoch 0100, iter [00600, 05004], lr: 0.052818, loss: 1.9050
2022-03-03 02:56:37 - train: epoch 0100, iter [00700, 05004], lr: 0.052818, loss: 1.6273
2022-03-03 02:57:10 - train: epoch 0100, iter [00800, 05004], lr: 0.052818, loss: 1.8319
2022-03-03 02:57:42 - train: epoch 0100, iter [00900, 05004], lr: 0.052818, loss: 1.6216
2022-03-03 02:58:16 - train: epoch 0100, iter [01000, 05004], lr: 0.052818, loss: 1.8433
2022-03-03 02:58:49 - train: epoch 0100, iter [01100, 05004], lr: 0.052818, loss: 1.6895
2022-03-03 02:59:22 - train: epoch 0100, iter [01200, 05004], lr: 0.052818, loss: 1.9126
2022-03-03 02:59:55 - train: epoch 0100, iter [01300, 05004], lr: 0.052818, loss: 2.0029
2022-03-03 03:00:28 - train: epoch 0100, iter [01400, 05004], lr: 0.052818, loss: 1.8617
2022-03-03 03:01:02 - train: epoch 0100, iter [01500, 05004], lr: 0.052818, loss: 1.8494
2022-03-03 03:01:35 - train: epoch 0100, iter [01600, 05004], lr: 0.052818, loss: 1.6010
2022-03-03 03:02:08 - train: epoch 0100, iter [01700, 05004], lr: 0.052818, loss: 1.6696
2022-03-03 03:02:41 - train: epoch 0100, iter [01800, 05004], lr: 0.052818, loss: 1.9550
2022-03-03 03:03:15 - train: epoch 0100, iter [01900, 05004], lr: 0.052818, loss: 1.7590
2022-03-03 03:03:48 - train: epoch 0100, iter [02000, 05004], lr: 0.052818, loss: 1.8047
2022-03-03 03:04:21 - train: epoch 0100, iter [02100, 05004], lr: 0.052818, loss: 1.7503
2022-03-03 03:04:54 - train: epoch 0100, iter [02200, 05004], lr: 0.052818, loss: 1.9840
2022-03-03 03:05:27 - train: epoch 0100, iter [02300, 05004], lr: 0.052818, loss: 1.9068
2022-03-03 03:06:00 - train: epoch 0100, iter [02400, 05004], lr: 0.052818, loss: 1.7537
2022-03-03 03:06:33 - train: epoch 0100, iter [02500, 05004], lr: 0.052818, loss: 1.7772
2022-03-03 03:07:06 - train: epoch 0100, iter [02600, 05004], lr: 0.052818, loss: 1.8206
2022-03-03 03:07:39 - train: epoch 0100, iter [02700, 05004], lr: 0.052818, loss: 1.7312
2022-03-03 03:08:12 - train: epoch 0100, iter [02800, 05004], lr: 0.052818, loss: 1.8013
2022-03-03 03:08:45 - train: epoch 0100, iter [02900, 05004], lr: 0.052818, loss: 1.9675
2022-03-03 03:09:18 - train: epoch 0100, iter [03000, 05004], lr: 0.052818, loss: 1.7204
2022-03-03 03:09:51 - train: epoch 0100, iter [03100, 05004], lr: 0.052818, loss: 1.7432
2022-03-03 03:10:24 - train: epoch 0100, iter [03200, 05004], lr: 0.052818, loss: 1.9806
2022-03-03 03:10:57 - train: epoch 0100, iter [03300, 05004], lr: 0.052818, loss: 2.0568
2022-03-03 03:11:30 - train: epoch 0100, iter [03400, 05004], lr: 0.052818, loss: 2.0324
2022-03-03 03:12:04 - train: epoch 0100, iter [03500, 05004], lr: 0.052818, loss: 1.5936
2022-03-03 03:12:38 - train: epoch 0100, iter [03600, 05004], lr: 0.052818, loss: 1.8792
2022-03-03 03:13:10 - train: epoch 0100, iter [03700, 05004], lr: 0.052818, loss: 1.6366
2022-03-03 03:13:43 - train: epoch 0100, iter [03800, 05004], lr: 0.052818, loss: 1.6884
2022-03-03 03:14:16 - train: epoch 0100, iter [03900, 05004], lr: 0.052818, loss: 1.9471
2022-03-03 03:14:50 - train: epoch 0100, iter [04000, 05004], lr: 0.052818, loss: 1.7018
2022-03-03 03:15:23 - train: epoch 0100, iter [04100, 05004], lr: 0.052818, loss: 1.9503
2022-03-03 03:15:56 - train: epoch 0100, iter [04200, 05004], lr: 0.052818, loss: 1.8121
2022-03-03 03:16:29 - train: epoch 0100, iter [04300, 05004], lr: 0.052818, loss: 1.6632
2022-03-03 03:17:02 - train: epoch 0100, iter [04400, 05004], lr: 0.052818, loss: 1.9654
2022-03-03 03:17:35 - train: epoch 0100, iter [04500, 05004], lr: 0.052818, loss: 1.6671
2022-03-03 03:18:08 - train: epoch 0100, iter [04600, 05004], lr: 0.052818, loss: 1.5638
2022-03-03 03:18:41 - train: epoch 0100, iter [04700, 05004], lr: 0.052818, loss: 1.8360
2022-03-03 03:19:15 - train: epoch 0100, iter [04800, 05004], lr: 0.052818, loss: 1.5578
2022-03-03 03:19:47 - train: epoch 0100, iter [04900, 05004], lr: 0.052818, loss: 1.8201
2022-03-03 03:20:20 - train: epoch 0100, iter [05000, 05004], lr: 0.052818, loss: 1.8753
2022-03-03 03:20:21 - train: epoch 100, train_loss: 1.8049
2022-03-03 03:21:34 - eval: epoch: 100, acc1: 61.080%, acc5: 84.390%, test_loss: 1.6160, per_image_load_time: 2.245ms, per_image_inference_time: 0.554ms
2022-03-03 03:21:34 - until epoch: 100, best_acc1: 61.836%
2022-03-03 03:21:34 - epoch 101 lr: 0.05201329700547076
2022-03-03 03:22:12 - train: epoch 0101, iter [00100, 05004], lr: 0.052013, loss: 1.6585
2022-03-03 03:22:45 - train: epoch 0101, iter [00200, 05004], lr: 0.052013, loss: 1.7255
2022-03-03 03:23:19 - train: epoch 0101, iter [00300, 05004], lr: 0.052013, loss: 1.6034
2022-03-03 03:23:52 - train: epoch 0101, iter [00400, 05004], lr: 0.052013, loss: 1.6884
2022-03-03 03:24:24 - train: epoch 0101, iter [00500, 05004], lr: 0.052013, loss: 1.7100
2022-03-03 03:24:58 - train: epoch 0101, iter [00600, 05004], lr: 0.052013, loss: 1.7387
2022-03-03 03:25:31 - train: epoch 0101, iter [00700, 05004], lr: 0.052013, loss: 2.0295
2022-03-03 03:26:05 - train: epoch 0101, iter [00800, 05004], lr: 0.052013, loss: 1.8914
2022-03-03 03:26:37 - train: epoch 0101, iter [00900, 05004], lr: 0.052013, loss: 1.8437
2022-03-03 03:27:11 - train: epoch 0101, iter [01000, 05004], lr: 0.052013, loss: 1.9014
2022-03-03 03:27:44 - train: epoch 0101, iter [01100, 05004], lr: 0.052013, loss: 1.8161
2022-03-03 03:28:17 - train: epoch 0101, iter [01200, 05004], lr: 0.052013, loss: 1.6647
2022-03-03 03:28:50 - train: epoch 0101, iter [01300, 05004], lr: 0.052013, loss: 2.0733
2022-03-03 03:29:24 - train: epoch 0101, iter [01400, 05004], lr: 0.052013, loss: 1.7200
2022-03-03 03:29:57 - train: epoch 0101, iter [01500, 05004], lr: 0.052013, loss: 1.6838
2022-03-03 03:30:31 - train: epoch 0101, iter [01600, 05004], lr: 0.052013, loss: 1.6890
2022-03-03 03:31:04 - train: epoch 0101, iter [01700, 05004], lr: 0.052013, loss: 1.5874
2022-03-03 03:31:38 - train: epoch 0101, iter [01800, 05004], lr: 0.052013, loss: 2.0779
2022-03-03 03:32:10 - train: epoch 0101, iter [01900, 05004], lr: 0.052013, loss: 1.5015
2022-03-03 03:32:44 - train: epoch 0101, iter [02000, 05004], lr: 0.052013, loss: 1.6653
2022-03-03 03:33:17 - train: epoch 0101, iter [02100, 05004], lr: 0.052013, loss: 1.8073
2022-03-03 03:33:51 - train: epoch 0101, iter [02200, 05004], lr: 0.052013, loss: 1.6713
2022-03-03 03:34:24 - train: epoch 0101, iter [02300, 05004], lr: 0.052013, loss: 1.6920
2022-03-03 03:34:58 - train: epoch 0101, iter [02400, 05004], lr: 0.052013, loss: 1.6929
2022-03-03 03:35:31 - train: epoch 0101, iter [02500, 05004], lr: 0.052013, loss: 1.9452
2022-03-03 03:36:04 - train: epoch 0101, iter [02600, 05004], lr: 0.052013, loss: 1.9035
2022-03-03 03:36:38 - train: epoch 0101, iter [02700, 05004], lr: 0.052013, loss: 1.9745
2022-03-03 03:37:10 - train: epoch 0101, iter [02800, 05004], lr: 0.052013, loss: 1.7832
2022-03-03 03:37:44 - train: epoch 0101, iter [02900, 05004], lr: 0.052013, loss: 1.8921
2022-03-03 03:38:17 - train: epoch 0101, iter [03000, 05004], lr: 0.052013, loss: 1.9081
2022-03-03 03:38:49 - train: epoch 0101, iter [03100, 05004], lr: 0.052013, loss: 1.9480
2022-03-03 03:39:22 - train: epoch 0101, iter [03200, 05004], lr: 0.052013, loss: 2.0126
2022-03-03 03:39:56 - train: epoch 0101, iter [03300, 05004], lr: 0.052013, loss: 1.9473
2022-03-03 03:40:29 - train: epoch 0101, iter [03400, 05004], lr: 0.052013, loss: 1.8490
2022-03-03 03:41:02 - train: epoch 0101, iter [03500, 05004], lr: 0.052013, loss: 1.6426
2022-03-03 03:41:35 - train: epoch 0101, iter [03600, 05004], lr: 0.052013, loss: 1.8437
2022-03-03 03:42:08 - train: epoch 0101, iter [03700, 05004], lr: 0.052013, loss: 2.0260
2022-03-03 03:42:41 - train: epoch 0101, iter [03800, 05004], lr: 0.052013, loss: 1.9164
2022-03-03 03:43:14 - train: epoch 0101, iter [03900, 05004], lr: 0.052013, loss: 1.9548
2022-03-03 03:43:48 - train: epoch 0101, iter [04000, 05004], lr: 0.052013, loss: 1.9086
2022-03-03 03:44:21 - train: epoch 0101, iter [04100, 05004], lr: 0.052013, loss: 1.7508
2022-03-03 03:44:53 - train: epoch 0101, iter [04200, 05004], lr: 0.052013, loss: 1.7932
2022-03-03 03:45:28 - train: epoch 0101, iter [04300, 05004], lr: 0.052013, loss: 1.8224
2022-03-03 03:46:00 - train: epoch 0101, iter [04400, 05004], lr: 0.052013, loss: 1.7811
2022-03-03 03:46:34 - train: epoch 0101, iter [04500, 05004], lr: 0.052013, loss: 2.0622
2022-03-03 03:47:07 - train: epoch 0101, iter [04600, 05004], lr: 0.052013, loss: 1.8739
2022-03-03 03:47:40 - train: epoch 0101, iter [04700, 05004], lr: 0.052013, loss: 1.8687
2022-03-03 03:48:13 - train: epoch 0101, iter [04800, 05004], lr: 0.052013, loss: 1.8517
2022-03-03 03:48:47 - train: epoch 0101, iter [04900, 05004], lr: 0.052013, loss: 1.6842
2022-03-03 03:49:19 - train: epoch 0101, iter [05000, 05004], lr: 0.052013, loss: 1.8789
2022-03-03 03:49:20 - train: epoch 101, train_loss: 1.7965
2022-03-03 03:50:33 - eval: epoch: 101, acc1: 61.296%, acc5: 84.468%, test_loss: 1.5995, per_image_load_time: 1.389ms, per_image_inference_time: 0.534ms
2022-03-03 03:50:33 - until epoch: 101, best_acc1: 61.836%
2022-03-03 03:50:33 - epoch 102 lr: 0.05120818726180662
2022-03-03 03:51:11 - train: epoch 0102, iter [00100, 05004], lr: 0.051208, loss: 1.7628
2022-03-03 03:51:45 - train: epoch 0102, iter [00200, 05004], lr: 0.051208, loss: 1.6914
2022-03-03 03:52:18 - train: epoch 0102, iter [00300, 05004], lr: 0.051208, loss: 1.7977
2022-03-03 03:52:51 - train: epoch 0102, iter [00400, 05004], lr: 0.051208, loss: 1.6749
2022-03-03 03:53:24 - train: epoch 0102, iter [00500, 05004], lr: 0.051208, loss: 1.8908
2022-03-03 03:53:57 - train: epoch 0102, iter [00600, 05004], lr: 0.051208, loss: 1.7084
2022-03-03 03:54:30 - train: epoch 0102, iter [00700, 05004], lr: 0.051208, loss: 1.7044
2022-03-03 03:55:04 - train: epoch 0102, iter [00800, 05004], lr: 0.051208, loss: 1.7666
2022-03-03 03:55:37 - train: epoch 0102, iter [00900, 05004], lr: 0.051208, loss: 1.6926
2022-03-03 03:56:11 - train: epoch 0102, iter [01000, 05004], lr: 0.051208, loss: 2.0338
2022-03-03 03:56:43 - train: epoch 0102, iter [01100, 05004], lr: 0.051208, loss: 1.6632
2022-03-03 03:57:17 - train: epoch 0102, iter [01200, 05004], lr: 0.051208, loss: 1.9868
2022-03-03 03:57:50 - train: epoch 0102, iter [01300, 05004], lr: 0.051208, loss: 1.8043
2022-03-03 03:58:23 - train: epoch 0102, iter [01400, 05004], lr: 0.051208, loss: 1.8894
2022-03-03 03:58:56 - train: epoch 0102, iter [01500, 05004], lr: 0.051208, loss: 1.8032
2022-03-03 03:59:30 - train: epoch 0102, iter [01600, 05004], lr: 0.051208, loss: 1.9872
2022-03-03 04:00:03 - train: epoch 0102, iter [01700, 05004], lr: 0.051208, loss: 1.7587
2022-03-03 04:00:36 - train: epoch 0102, iter [01800, 05004], lr: 0.051208, loss: 1.8370
2022-03-03 04:01:09 - train: epoch 0102, iter [01900, 05004], lr: 0.051208, loss: 1.7207
2022-03-03 04:01:43 - train: epoch 0102, iter [02000, 05004], lr: 0.051208, loss: 1.7019
2022-03-03 04:02:16 - train: epoch 0102, iter [02100, 05004], lr: 0.051208, loss: 1.6952
2022-03-03 04:02:49 - train: epoch 0102, iter [02200, 05004], lr: 0.051208, loss: 1.6276
2022-03-03 04:03:22 - train: epoch 0102, iter [02300, 05004], lr: 0.051208, loss: 1.7244
2022-03-03 04:03:56 - train: epoch 0102, iter [02400, 05004], lr: 0.051208, loss: 1.9502
2022-03-03 04:04:28 - train: epoch 0102, iter [02500, 05004], lr: 0.051208, loss: 1.7234
2022-03-03 04:05:02 - train: epoch 0102, iter [02600, 05004], lr: 0.051208, loss: 1.8699
2022-03-03 04:05:35 - train: epoch 0102, iter [02700, 05004], lr: 0.051208, loss: 2.0757
2022-03-03 04:06:09 - train: epoch 0102, iter [02800, 05004], lr: 0.051208, loss: 1.8955
2022-03-03 04:06:42 - train: epoch 0102, iter [02900, 05004], lr: 0.051208, loss: 1.7131
2022-03-03 04:07:15 - train: epoch 0102, iter [03000, 05004], lr: 0.051208, loss: 1.6979
2022-03-03 04:07:48 - train: epoch 0102, iter [03100, 05004], lr: 0.051208, loss: 1.7166
2022-03-03 04:08:21 - train: epoch 0102, iter [03200, 05004], lr: 0.051208, loss: 1.7088
2022-03-03 04:08:55 - train: epoch 0102, iter [03300, 05004], lr: 0.051208, loss: 1.7947
2022-03-03 04:09:27 - train: epoch 0102, iter [03400, 05004], lr: 0.051208, loss: 1.7793
2022-03-03 04:10:00 - train: epoch 0102, iter [03500, 05004], lr: 0.051208, loss: 1.9021
2022-03-03 04:10:34 - train: epoch 0102, iter [03600, 05004], lr: 0.051208, loss: 1.8378
2022-03-03 04:11:07 - train: epoch 0102, iter [03700, 05004], lr: 0.051208, loss: 1.7994
2022-03-03 04:11:40 - train: epoch 0102, iter [03800, 05004], lr: 0.051208, loss: 1.8164
2022-03-03 04:12:13 - train: epoch 0102, iter [03900, 05004], lr: 0.051208, loss: 1.9189
2022-03-03 04:12:47 - train: epoch 0102, iter [04000, 05004], lr: 0.051208, loss: 1.6894
2022-03-03 04:13:20 - train: epoch 0102, iter [04100, 05004], lr: 0.051208, loss: 1.7027
2022-03-03 04:13:53 - train: epoch 0102, iter [04200, 05004], lr: 0.051208, loss: 2.1016
2022-03-03 04:14:26 - train: epoch 0102, iter [04300, 05004], lr: 0.051208, loss: 1.6654
2022-03-03 04:14:59 - train: epoch 0102, iter [04400, 05004], lr: 0.051208, loss: 1.7238
2022-03-03 04:15:32 - train: epoch 0102, iter [04500, 05004], lr: 0.051208, loss: 2.0238
2022-03-03 04:16:05 - train: epoch 0102, iter [04600, 05004], lr: 0.051208, loss: 1.8950
2022-03-03 04:16:38 - train: epoch 0102, iter [04700, 05004], lr: 0.051208, loss: 1.8140
2022-03-03 04:17:12 - train: epoch 0102, iter [04800, 05004], lr: 0.051208, loss: 1.9289
2022-03-03 04:17:45 - train: epoch 0102, iter [04900, 05004], lr: 0.051208, loss: 1.8658
2022-03-03 04:18:17 - train: epoch 0102, iter [05000, 05004], lr: 0.051208, loss: 1.6838
2022-03-03 04:18:18 - train: epoch 102, train_loss: 1.7899
2022-03-03 04:19:31 - eval: epoch: 102, acc1: 60.932%, acc5: 84.520%, test_loss: 1.6125, per_image_load_time: 2.301ms, per_image_inference_time: 0.541ms
2022-03-03 04:19:32 - until epoch: 102, best_acc1: 61.836%
2022-03-03 04:19:32 - epoch 103 lr: 0.0504027639330695
2022-03-03 04:20:09 - train: epoch 0103, iter [00100, 05004], lr: 0.050403, loss: 1.5901
2022-03-03 04:20:43 - train: epoch 0103, iter [00200, 05004], lr: 0.050403, loss: 1.7462
2022-03-03 04:21:16 - train: epoch 0103, iter [00300, 05004], lr: 0.050403, loss: 1.6351
2022-03-03 04:21:49 - train: epoch 0103, iter [00400, 05004], lr: 0.050403, loss: 2.0047
2022-03-03 04:22:23 - train: epoch 0103, iter [00500, 05004], lr: 0.050403, loss: 1.7932
2022-03-03 04:22:56 - train: epoch 0103, iter [00600, 05004], lr: 0.050403, loss: 1.6816
2022-03-03 04:23:30 - train: epoch 0103, iter [00700, 05004], lr: 0.050403, loss: 1.7394
2022-03-03 04:24:03 - train: epoch 0103, iter [00800, 05004], lr: 0.050403, loss: 1.8458
2022-03-03 04:24:37 - train: epoch 0103, iter [00900, 05004], lr: 0.050403, loss: 1.7433
2022-03-03 04:25:10 - train: epoch 0103, iter [01000, 05004], lr: 0.050403, loss: 1.6917
2022-03-03 04:25:43 - train: epoch 0103, iter [01100, 05004], lr: 0.050403, loss: 1.6929
2022-03-03 04:26:17 - train: epoch 0103, iter [01200, 05004], lr: 0.050403, loss: 1.8150
2022-03-03 04:26:50 - train: epoch 0103, iter [01300, 05004], lr: 0.050403, loss: 1.9445
2022-03-03 04:27:24 - train: epoch 0103, iter [01400, 05004], lr: 0.050403, loss: 1.8388
2022-03-03 04:27:56 - train: epoch 0103, iter [01500, 05004], lr: 0.050403, loss: 1.9634
2022-03-03 04:28:30 - train: epoch 0103, iter [01600, 05004], lr: 0.050403, loss: 1.4033
2022-03-03 04:29:04 - train: epoch 0103, iter [01700, 05004], lr: 0.050403, loss: 1.7284
2022-03-03 04:29:36 - train: epoch 0103, iter [01800, 05004], lr: 0.050403, loss: 1.6471
2022-03-03 04:30:10 - train: epoch 0103, iter [01900, 05004], lr: 0.050403, loss: 1.7168
2022-03-03 04:30:43 - train: epoch 0103, iter [02000, 05004], lr: 0.050403, loss: 1.5483
2022-03-03 04:31:16 - train: epoch 0103, iter [02100, 05004], lr: 0.050403, loss: 1.8377
2022-03-03 04:31:50 - train: epoch 0103, iter [02200, 05004], lr: 0.050403, loss: 1.7563
2022-03-03 04:32:24 - train: epoch 0103, iter [02300, 05004], lr: 0.050403, loss: 1.7995
2022-03-03 04:32:56 - train: epoch 0103, iter [02400, 05004], lr: 0.050403, loss: 1.8681
2022-03-03 04:33:30 - train: epoch 0103, iter [02500, 05004], lr: 0.050403, loss: 1.8223
2022-03-03 04:34:03 - train: epoch 0103, iter [02600, 05004], lr: 0.050403, loss: 1.7817
2022-03-03 04:34:36 - train: epoch 0103, iter [02700, 05004], lr: 0.050403, loss: 1.8346
2022-03-03 04:35:09 - train: epoch 0103, iter [02800, 05004], lr: 0.050403, loss: 1.4150
2022-03-03 04:35:43 - train: epoch 0103, iter [02900, 05004], lr: 0.050403, loss: 1.7247
2022-03-03 04:36:16 - train: epoch 0103, iter [03000, 05004], lr: 0.050403, loss: 1.8952
2022-03-03 04:36:49 - train: epoch 0103, iter [03100, 05004], lr: 0.050403, loss: 1.8020
2022-03-03 04:37:22 - train: epoch 0103, iter [03200, 05004], lr: 0.050403, loss: 1.6960
2022-03-03 04:37:55 - train: epoch 0103, iter [03300, 05004], lr: 0.050403, loss: 1.6954
2022-03-03 04:38:28 - train: epoch 0103, iter [03400, 05004], lr: 0.050403, loss: 1.8091
2022-03-03 04:39:01 - train: epoch 0103, iter [03500, 05004], lr: 0.050403, loss: 1.7971
2022-03-03 04:39:35 - train: epoch 0103, iter [03600, 05004], lr: 0.050403, loss: 1.7675
2022-03-03 04:40:07 - train: epoch 0103, iter [03700, 05004], lr: 0.050403, loss: 1.6888
2022-03-03 04:40:41 - train: epoch 0103, iter [03800, 05004], lr: 0.050403, loss: 1.7090
2022-03-03 04:41:14 - train: epoch 0103, iter [03900, 05004], lr: 0.050403, loss: 1.6920
2022-03-03 04:41:47 - train: epoch 0103, iter [04000, 05004], lr: 0.050403, loss: 1.8301
2022-03-03 04:42:20 - train: epoch 0103, iter [04100, 05004], lr: 0.050403, loss: 1.8696
2022-03-03 04:42:55 - train: epoch 0103, iter [04200, 05004], lr: 0.050403, loss: 1.9004
2022-03-03 04:43:28 - train: epoch 0103, iter [04300, 05004], lr: 0.050403, loss: 1.8757
2022-03-03 04:44:01 - train: epoch 0103, iter [04400, 05004], lr: 0.050403, loss: 1.9258
2022-03-03 04:44:34 - train: epoch 0103, iter [04500, 05004], lr: 0.050403, loss: 1.8736
2022-03-03 04:45:08 - train: epoch 0103, iter [04600, 05004], lr: 0.050403, loss: 1.9339
2022-03-03 04:45:40 - train: epoch 0103, iter [04700, 05004], lr: 0.050403, loss: 1.6276
2022-03-03 04:46:14 - train: epoch 0103, iter [04800, 05004], lr: 0.050403, loss: 2.0366
2022-03-03 04:46:47 - train: epoch 0103, iter [04900, 05004], lr: 0.050403, loss: 1.7578
2022-03-03 04:47:19 - train: epoch 0103, iter [05000, 05004], lr: 0.050403, loss: 1.8012
2022-03-03 04:47:20 - train: epoch 103, train_loss: 1.7834
2022-03-03 04:48:34 - eval: epoch: 103, acc1: 62.326%, acc5: 85.044%, test_loss: 1.5561, per_image_load_time: 2.092ms, per_image_inference_time: 0.545ms
2022-03-03 04:48:35 - until epoch: 103, best_acc1: 62.326%
2022-03-03 04:48:35 - epoch 104 lr: 0.04959723606693051
2022-03-03 04:49:13 - train: epoch 0104, iter [00100, 05004], lr: 0.049597, loss: 1.6913
2022-03-03 04:49:46 - train: epoch 0104, iter [00200, 05004], lr: 0.049597, loss: 1.6357
2022-03-03 04:50:18 - train: epoch 0104, iter [00300, 05004], lr: 0.049597, loss: 1.8962
2022-03-03 04:50:51 - train: epoch 0104, iter [00400, 05004], lr: 0.049597, loss: 1.8373
2022-03-03 04:51:25 - train: epoch 0104, iter [00500, 05004], lr: 0.049597, loss: 1.7155
2022-03-03 04:51:58 - train: epoch 0104, iter [00600, 05004], lr: 0.049597, loss: 1.8631
2022-03-03 04:52:32 - train: epoch 0104, iter [00700, 05004], lr: 0.049597, loss: 1.9826
2022-03-03 04:53:04 - train: epoch 0104, iter [00800, 05004], lr: 0.049597, loss: 1.8288
2022-03-03 04:53:38 - train: epoch 0104, iter [00900, 05004], lr: 0.049597, loss: 1.7056
2022-03-03 04:54:10 - train: epoch 0104, iter [01000, 05004], lr: 0.049597, loss: 1.6790
2022-03-03 04:54:44 - train: epoch 0104, iter [01100, 05004], lr: 0.049597, loss: 1.6981
2022-03-03 04:55:16 - train: epoch 0104, iter [01200, 05004], lr: 0.049597, loss: 1.6562
2022-03-03 04:55:50 - train: epoch 0104, iter [01300, 05004], lr: 0.049597, loss: 1.7869
2022-03-03 04:56:24 - train: epoch 0104, iter [01400, 05004], lr: 0.049597, loss: 1.6564
2022-03-03 04:56:57 - train: epoch 0104, iter [01500, 05004], lr: 0.049597, loss: 1.8766
2022-03-03 04:57:31 - train: epoch 0104, iter [01600, 05004], lr: 0.049597, loss: 1.5063
2022-03-03 04:58:04 - train: epoch 0104, iter [01700, 05004], lr: 0.049597, loss: 1.7056
2022-03-03 04:58:38 - train: epoch 0104, iter [01800, 05004], lr: 0.049597, loss: 1.8609
2022-03-03 04:59:10 - train: epoch 0104, iter [01900, 05004], lr: 0.049597, loss: 1.5945
2022-03-03 04:59:44 - train: epoch 0104, iter [02000, 05004], lr: 0.049597, loss: 1.8126
2022-03-03 05:00:17 - train: epoch 0104, iter [02100, 05004], lr: 0.049597, loss: 1.5449
2022-03-03 05:00:49 - train: epoch 0104, iter [02200, 05004], lr: 0.049597, loss: 1.8904
2022-03-03 05:01:23 - train: epoch 0104, iter [02300, 05004], lr: 0.049597, loss: 1.8352
2022-03-03 05:01:56 - train: epoch 0104, iter [02400, 05004], lr: 0.049597, loss: 2.0067
2022-03-03 05:02:30 - train: epoch 0104, iter [02500, 05004], lr: 0.049597, loss: 1.7501
2022-03-03 05:03:03 - train: epoch 0104, iter [02600, 05004], lr: 0.049597, loss: 1.6408
2022-03-03 05:03:37 - train: epoch 0104, iter [02700, 05004], lr: 0.049597, loss: 1.7980
2022-03-03 05:04:09 - train: epoch 0104, iter [02800, 05004], lr: 0.049597, loss: 1.9567
2022-03-03 05:04:43 - train: epoch 0104, iter [02900, 05004], lr: 0.049597, loss: 1.9082
2022-03-03 05:05:16 - train: epoch 0104, iter [03000, 05004], lr: 0.049597, loss: 1.8566
2022-03-03 05:05:49 - train: epoch 0104, iter [03100, 05004], lr: 0.049597, loss: 1.5779
2022-03-03 05:06:22 - train: epoch 0104, iter [03200, 05004], lr: 0.049597, loss: 1.8880
2022-03-03 05:06:56 - train: epoch 0104, iter [03300, 05004], lr: 0.049597, loss: 1.9853
2022-03-03 05:07:29 - train: epoch 0104, iter [03400, 05004], lr: 0.049597, loss: 1.6543
2022-03-03 05:08:02 - train: epoch 0104, iter [03500, 05004], lr: 0.049597, loss: 1.7717
2022-03-03 05:08:35 - train: epoch 0104, iter [03600, 05004], lr: 0.049597, loss: 1.7262
2022-03-03 05:09:09 - train: epoch 0104, iter [03700, 05004], lr: 0.049597, loss: 1.8001
2022-03-03 05:09:42 - train: epoch 0104, iter [03800, 05004], lr: 0.049597, loss: 1.4989
2022-03-03 05:10:15 - train: epoch 0104, iter [03900, 05004], lr: 0.049597, loss: 1.6909
2022-03-03 05:10:48 - train: epoch 0104, iter [04000, 05004], lr: 0.049597, loss: 1.7038
2022-03-03 05:11:22 - train: epoch 0104, iter [04100, 05004], lr: 0.049597, loss: 1.8184
2022-03-03 05:11:55 - train: epoch 0104, iter [04200, 05004], lr: 0.049597, loss: 1.7561
2022-03-03 05:12:28 - train: epoch 0104, iter [04300, 05004], lr: 0.049597, loss: 1.6724
2022-03-03 05:13:01 - train: epoch 0104, iter [04400, 05004], lr: 0.049597, loss: 1.6548
2022-03-03 05:13:34 - train: epoch 0104, iter [04500, 05004], lr: 0.049597, loss: 1.9486
2022-03-03 05:14:07 - train: epoch 0104, iter [04600, 05004], lr: 0.049597, loss: 1.7714
2022-03-03 05:14:39 - train: epoch 0104, iter [04700, 05004], lr: 0.049597, loss: 1.7578
2022-03-03 05:15:13 - train: epoch 0104, iter [04800, 05004], lr: 0.049597, loss: 1.8564
2022-03-03 05:15:46 - train: epoch 0104, iter [04900, 05004], lr: 0.049597, loss: 1.6901
2022-03-03 05:16:19 - train: epoch 0104, iter [05000, 05004], lr: 0.049597, loss: 1.8518
2022-03-03 05:16:20 - train: epoch 104, train_loss: 1.7768
2022-03-03 05:17:33 - eval: epoch: 104, acc1: 62.704%, acc5: 85.690%, test_loss: 1.5338, per_image_load_time: 0.892ms, per_image_inference_time: 0.497ms
2022-03-03 05:17:34 - until epoch: 104, best_acc1: 62.704%
2022-03-03 05:17:34 - epoch 105 lr: 0.0487918127381934
2022-03-03 05:18:13 - train: epoch 0105, iter [00100, 05004], lr: 0.048792, loss: 1.9324
2022-03-03 05:18:45 - train: epoch 0105, iter [00200, 05004], lr: 0.048792, loss: 1.9135
2022-03-03 05:19:17 - train: epoch 0105, iter [00300, 05004], lr: 0.048792, loss: 1.9495
2022-03-03 05:19:51 - train: epoch 0105, iter [00400, 05004], lr: 0.048792, loss: 1.8385
2022-03-03 05:20:24 - train: epoch 0105, iter [00500, 05004], lr: 0.048792, loss: 1.8051
2022-03-03 05:20:58 - train: epoch 0105, iter [00600, 05004], lr: 0.048792, loss: 1.9901
2022-03-03 05:21:30 - train: epoch 0105, iter [00700, 05004], lr: 0.048792, loss: 1.6730
2022-03-03 05:22:04 - train: epoch 0105, iter [00800, 05004], lr: 0.048792, loss: 1.7074
2022-03-03 05:22:37 - train: epoch 0105, iter [00900, 05004], lr: 0.048792, loss: 1.7603
2022-03-03 05:23:10 - train: epoch 0105, iter [01000, 05004], lr: 0.048792, loss: 1.7809
2022-03-03 05:23:43 - train: epoch 0105, iter [01100, 05004], lr: 0.048792, loss: 2.0375
2022-03-03 05:24:17 - train: epoch 0105, iter [01200, 05004], lr: 0.048792, loss: 1.8594
2022-03-03 05:24:50 - train: epoch 0105, iter [01300, 05004], lr: 0.048792, loss: 1.5160
2022-03-03 05:25:23 - train: epoch 0105, iter [01400, 05004], lr: 0.048792, loss: 1.8267
2022-03-03 05:25:55 - train: epoch 0105, iter [01500, 05004], lr: 0.048792, loss: 1.6999
2022-03-03 05:26:29 - train: epoch 0105, iter [01600, 05004], lr: 0.048792, loss: 1.4645
2022-03-03 05:27:03 - train: epoch 0105, iter [01700, 05004], lr: 0.048792, loss: 1.6989
2022-03-03 05:27:35 - train: epoch 0105, iter [01800, 05004], lr: 0.048792, loss: 1.7959
2022-03-03 05:28:08 - train: epoch 0105, iter [01900, 05004], lr: 0.048792, loss: 1.8723
2022-03-03 05:28:41 - train: epoch 0105, iter [02000, 05004], lr: 0.048792, loss: 1.7096
2022-03-03 05:29:14 - train: epoch 0105, iter [02100, 05004], lr: 0.048792, loss: 1.8031
2022-03-03 05:29:47 - train: epoch 0105, iter [02200, 05004], lr: 0.048792, loss: 2.0307
2022-03-03 05:30:20 - train: epoch 0105, iter [02300, 05004], lr: 0.048792, loss: 1.8133
2022-03-03 05:30:53 - train: epoch 0105, iter [02400, 05004], lr: 0.048792, loss: 1.6712
2022-03-03 05:31:27 - train: epoch 0105, iter [02500, 05004], lr: 0.048792, loss: 1.6069
2022-03-03 05:31:59 - train: epoch 0105, iter [02600, 05004], lr: 0.048792, loss: 1.7801
2022-03-03 05:32:32 - train: epoch 0105, iter [02700, 05004], lr: 0.048792, loss: 2.0779
2022-03-03 05:33:05 - train: epoch 0105, iter [02800, 05004], lr: 0.048792, loss: 1.9629
2022-03-03 05:33:38 - train: epoch 0105, iter [02900, 05004], lr: 0.048792, loss: 1.8801
2022-03-03 05:34:11 - train: epoch 0105, iter [03000, 05004], lr: 0.048792, loss: 1.7440
2022-03-03 05:34:44 - train: epoch 0105, iter [03100, 05004], lr: 0.048792, loss: 1.8227
2022-03-03 05:35:17 - train: epoch 0105, iter [03200, 05004], lr: 0.048792, loss: 2.1357
2022-03-03 05:35:50 - train: epoch 0105, iter [03300, 05004], lr: 0.048792, loss: 1.6116
2022-03-03 05:36:23 - train: epoch 0105, iter [03400, 05004], lr: 0.048792, loss: 1.7637
2022-03-03 05:36:56 - train: epoch 0105, iter [03500, 05004], lr: 0.048792, loss: 1.8792
2022-03-03 05:37:30 - train: epoch 0105, iter [03600, 05004], lr: 0.048792, loss: 1.9874
2022-03-03 05:38:02 - train: epoch 0105, iter [03700, 05004], lr: 0.048792, loss: 1.6490
2022-03-03 05:38:36 - train: epoch 0105, iter [03800, 05004], lr: 0.048792, loss: 1.8850
2022-03-03 05:39:09 - train: epoch 0105, iter [03900, 05004], lr: 0.048792, loss: 1.8658
2022-03-03 05:39:41 - train: epoch 0105, iter [04000, 05004], lr: 0.048792, loss: 1.8086
2022-03-03 05:40:14 - train: epoch 0105, iter [04100, 05004], lr: 0.048792, loss: 1.6532
2022-03-03 05:40:47 - train: epoch 0105, iter [04200, 05004], lr: 0.048792, loss: 1.8239
2022-03-03 05:41:20 - train: epoch 0105, iter [04300, 05004], lr: 0.048792, loss: 2.0839
2022-03-03 05:41:53 - train: epoch 0105, iter [04400, 05004], lr: 0.048792, loss: 1.6176
2022-03-03 05:42:26 - train: epoch 0105, iter [04500, 05004], lr: 0.048792, loss: 2.0609
2022-03-03 05:42:59 - train: epoch 0105, iter [04600, 05004], lr: 0.048792, loss: 1.8794
2022-03-03 05:43:32 - train: epoch 0105, iter [04700, 05004], lr: 0.048792, loss: 1.7704
2022-03-03 05:44:05 - train: epoch 0105, iter [04800, 05004], lr: 0.048792, loss: 1.8700
2022-03-03 05:44:38 - train: epoch 0105, iter [04900, 05004], lr: 0.048792, loss: 1.7999
2022-03-03 05:45:09 - train: epoch 0105, iter [05000, 05004], lr: 0.048792, loss: 1.5918
2022-03-03 05:45:11 - train: epoch 105, train_loss: 1.7698
2022-03-03 05:46:24 - eval: epoch: 105, acc1: 63.334%, acc5: 85.456%, test_loss: 1.5235, per_image_load_time: 2.200ms, per_image_inference_time: 0.537ms
2022-03-03 05:46:25 - until epoch: 105, best_acc1: 63.334%
2022-03-03 05:46:25 - epoch 106 lr: 0.04798670299452926
2022-03-03 05:47:02 - train: epoch 0106, iter [00100, 05004], lr: 0.047987, loss: 1.5968
2022-03-03 05:47:35 - train: epoch 0106, iter [00200, 05004], lr: 0.047987, loss: 1.5838
2022-03-03 05:48:08 - train: epoch 0106, iter [00300, 05004], lr: 0.047987, loss: 1.5298
2022-03-03 05:48:41 - train: epoch 0106, iter [00400, 05004], lr: 0.047987, loss: 1.7855
2022-03-03 05:49:14 - train: epoch 0106, iter [00500, 05004], lr: 0.047987, loss: 1.8560
2022-03-03 05:49:47 - train: epoch 0106, iter [00600, 05004], lr: 0.047987, loss: 1.7562
2022-03-03 05:50:20 - train: epoch 0106, iter [00700, 05004], lr: 0.047987, loss: 1.7099
2022-03-03 05:50:54 - train: epoch 0106, iter [00800, 05004], lr: 0.047987, loss: 1.4967
2022-03-03 05:51:26 - train: epoch 0106, iter [00900, 05004], lr: 0.047987, loss: 1.9640
2022-03-03 05:52:00 - train: epoch 0106, iter [01000, 05004], lr: 0.047987, loss: 1.9539
2022-03-03 05:52:33 - train: epoch 0106, iter [01100, 05004], lr: 0.047987, loss: 1.5111
2022-03-03 05:53:06 - train: epoch 0106, iter [01200, 05004], lr: 0.047987, loss: 1.5920
2022-03-03 05:53:39 - train: epoch 0106, iter [01300, 05004], lr: 0.047987, loss: 1.7214
2022-03-03 05:54:12 - train: epoch 0106, iter [01400, 05004], lr: 0.047987, loss: 1.7250
2022-03-03 05:54:45 - train: epoch 0106, iter [01500, 05004], lr: 0.047987, loss: 1.8011
2022-03-03 05:55:19 - train: epoch 0106, iter [01600, 05004], lr: 0.047987, loss: 1.7039
2022-03-03 05:55:52 - train: epoch 0106, iter [01700, 05004], lr: 0.047987, loss: 1.6544
2022-03-03 05:56:26 - train: epoch 0106, iter [01800, 05004], lr: 0.047987, loss: 1.6740
2022-03-03 05:56:59 - train: epoch 0106, iter [01900, 05004], lr: 0.047987, loss: 1.7412
2022-03-03 05:57:32 - train: epoch 0106, iter [02000, 05004], lr: 0.047987, loss: 1.7219
2022-03-03 05:58:05 - train: epoch 0106, iter [02100, 05004], lr: 0.047987, loss: 1.6639
2022-03-03 05:58:38 - train: epoch 0106, iter [02200, 05004], lr: 0.047987, loss: 1.7232
2022-03-03 05:59:11 - train: epoch 0106, iter [02300, 05004], lr: 0.047987, loss: 1.6408
2022-03-03 05:59:44 - train: epoch 0106, iter [02400, 05004], lr: 0.047987, loss: 1.8432
2022-03-03 06:00:17 - train: epoch 0106, iter [02500, 05004], lr: 0.047987, loss: 1.7512
2022-03-03 06:00:50 - train: epoch 0106, iter [02600, 05004], lr: 0.047987, loss: 1.4933
2022-03-03 06:01:24 - train: epoch 0106, iter [02700, 05004], lr: 0.047987, loss: 1.7891
2022-03-03 06:01:56 - train: epoch 0106, iter [02800, 05004], lr: 0.047987, loss: 1.7785
2022-03-03 06:02:30 - train: epoch 0106, iter [02900, 05004], lr: 0.047987, loss: 1.5596
2022-03-03 06:03:03 - train: epoch 0106, iter [03000, 05004], lr: 0.047987, loss: 1.9555
2022-03-03 06:03:35 - train: epoch 0106, iter [03100, 05004], lr: 0.047987, loss: 1.7258
2022-03-03 06:04:08 - train: epoch 0106, iter [03200, 05004], lr: 0.047987, loss: 1.5622
2022-03-03 06:04:41 - train: epoch 0106, iter [03300, 05004], lr: 0.047987, loss: 1.7476
2022-03-03 06:05:14 - train: epoch 0106, iter [03400, 05004], lr: 0.047987, loss: 1.7341
2022-03-03 06:05:48 - train: epoch 0106, iter [03500, 05004], lr: 0.047987, loss: 1.6396
2022-03-03 06:06:21 - train: epoch 0106, iter [03600, 05004], lr: 0.047987, loss: 1.7699
2022-03-03 06:06:54 - train: epoch 0106, iter [03700, 05004], lr: 0.047987, loss: 1.8375
2022-03-03 06:07:27 - train: epoch 0106, iter [03800, 05004], lr: 0.047987, loss: 1.9584
2022-03-03 06:07:59 - train: epoch 0106, iter [03900, 05004], lr: 0.047987, loss: 2.0475
2022-03-03 06:08:33 - train: epoch 0106, iter [04000, 05004], lr: 0.047987, loss: 1.6461
2022-03-03 06:09:06 - train: epoch 0106, iter [04100, 05004], lr: 0.047987, loss: 1.8437
2022-03-03 06:09:39 - train: epoch 0106, iter [04200, 05004], lr: 0.047987, loss: 1.6239
2022-03-03 06:10:12 - train: epoch 0106, iter [04300, 05004], lr: 0.047987, loss: 1.3935
2022-03-03 06:10:46 - train: epoch 0106, iter [04400, 05004], lr: 0.047987, loss: 1.7632
2022-03-03 06:11:19 - train: epoch 0106, iter [04500, 05004], lr: 0.047987, loss: 1.9785
2022-03-03 06:11:50 - train: epoch 0106, iter [04600, 05004], lr: 0.047987, loss: 1.9657
2022-03-03 06:12:25 - train: epoch 0106, iter [04700, 05004], lr: 0.047987, loss: 1.7443
2022-03-03 06:12:57 - train: epoch 0106, iter [04800, 05004], lr: 0.047987, loss: 1.8932
2022-03-03 06:13:31 - train: epoch 0106, iter [04900, 05004], lr: 0.047987, loss: 1.7458
2022-03-03 06:14:03 - train: epoch 0106, iter [05000, 05004], lr: 0.047987, loss: 1.8819
2022-03-03 06:14:04 - train: epoch 106, train_loss: 1.7621
2022-03-03 06:15:18 - eval: epoch: 106, acc1: 64.168%, acc5: 86.332%, test_loss: 1.4742, per_image_load_time: 1.485ms, per_image_inference_time: 0.506ms
2022-03-03 06:15:19 - until epoch: 106, best_acc1: 64.168%
2022-03-03 06:15:19 - epoch 107 lr: 0.04718211580221812
2022-03-03 06:15:57 - train: epoch 0107, iter [00100, 05004], lr: 0.047182, loss: 1.5473
2022-03-03 06:16:30 - train: epoch 0107, iter [00200, 05004], lr: 0.047182, loss: 1.8678
2022-03-03 06:17:03 - train: epoch 0107, iter [00300, 05004], lr: 0.047182, loss: 1.7815
2022-03-03 06:17:36 - train: epoch 0107, iter [00400, 05004], lr: 0.047182, loss: 1.6744
2022-03-03 06:18:09 - train: epoch 0107, iter [00500, 05004], lr: 0.047182, loss: 1.5188
2022-03-03 06:18:43 - train: epoch 0107, iter [00600, 05004], lr: 0.047182, loss: 1.4313
2022-03-03 06:19:15 - train: epoch 0107, iter [00700, 05004], lr: 0.047182, loss: 1.6196
2022-03-03 06:19:48 - train: epoch 0107, iter [00800, 05004], lr: 0.047182, loss: 1.4786
2022-03-03 06:20:21 - train: epoch 0107, iter [00900, 05004], lr: 0.047182, loss: 1.8369
2022-03-03 06:20:55 - train: epoch 0107, iter [01000, 05004], lr: 0.047182, loss: 1.8195
2022-03-03 06:21:28 - train: epoch 0107, iter [01100, 05004], lr: 0.047182, loss: 1.8368
2022-03-03 06:22:01 - train: epoch 0107, iter [01200, 05004], lr: 0.047182, loss: 1.7381
2022-03-03 06:22:35 - train: epoch 0107, iter [01300, 05004], lr: 0.047182, loss: 1.6402
2022-03-03 06:23:07 - train: epoch 0107, iter [01400, 05004], lr: 0.047182, loss: 1.7476
2022-03-03 06:23:40 - train: epoch 0107, iter [01500, 05004], lr: 0.047182, loss: 1.7492
2022-03-03 06:24:13 - train: epoch 0107, iter [01600, 05004], lr: 0.047182, loss: 1.5667
2022-03-03 06:24:47 - train: epoch 0107, iter [01700, 05004], lr: 0.047182, loss: 1.6204
2022-03-03 06:25:20 - train: epoch 0107, iter [01800, 05004], lr: 0.047182, loss: 1.8721
2022-03-03 06:25:54 - train: epoch 0107, iter [01900, 05004], lr: 0.047182, loss: 1.8999
2022-03-03 06:26:26 - train: epoch 0107, iter [02000, 05004], lr: 0.047182, loss: 1.5682
2022-03-03 06:27:00 - train: epoch 0107, iter [02100, 05004], lr: 0.047182, loss: 1.7057
2022-03-03 06:27:33 - train: epoch 0107, iter [02200, 05004], lr: 0.047182, loss: 1.7535
2022-03-03 06:28:06 - train: epoch 0107, iter [02300, 05004], lr: 0.047182, loss: 1.6409
2022-03-03 06:28:39 - train: epoch 0107, iter [02400, 05004], lr: 0.047182, loss: 1.7602
2022-03-03 06:29:12 - train: epoch 0107, iter [02500, 05004], lr: 0.047182, loss: 1.6824
2022-03-03 06:29:45 - train: epoch 0107, iter [02600, 05004], lr: 0.047182, loss: 1.9172
2022-03-03 06:30:18 - train: epoch 0107, iter [02700, 05004], lr: 0.047182, loss: 1.6819
2022-03-03 06:30:51 - train: epoch 0107, iter [02800, 05004], lr: 0.047182, loss: 1.8822
2022-03-03 06:31:24 - train: epoch 0107, iter [02900, 05004], lr: 0.047182, loss: 1.6714
2022-03-03 06:31:57 - train: epoch 0107, iter [03000, 05004], lr: 0.047182, loss: 1.7374
2022-03-03 06:32:31 - train: epoch 0107, iter [03100, 05004], lr: 0.047182, loss: 2.0349
2022-03-03 06:33:05 - train: epoch 0107, iter [03200, 05004], lr: 0.047182, loss: 1.7716
2022-03-03 06:33:37 - train: epoch 0107, iter [03300, 05004], lr: 0.047182, loss: 1.7637
2022-03-03 06:34:11 - train: epoch 0107, iter [03400, 05004], lr: 0.047182, loss: 1.7154
2022-03-03 06:34:44 - train: epoch 0107, iter [03500, 05004], lr: 0.047182, loss: 1.5446
2022-03-03 06:35:17 - train: epoch 0107, iter [03600, 05004], lr: 0.047182, loss: 1.8643
2022-03-03 06:35:49 - train: epoch 0107, iter [03700, 05004], lr: 0.047182, loss: 1.7598
2022-03-03 06:36:23 - train: epoch 0107, iter [03800, 05004], lr: 0.047182, loss: 1.7653
2022-03-03 06:36:55 - train: epoch 0107, iter [03900, 05004], lr: 0.047182, loss: 1.7661
2022-03-03 06:37:29 - train: epoch 0107, iter [04000, 05004], lr: 0.047182, loss: 1.7824
2022-03-03 06:38:02 - train: epoch 0107, iter [04100, 05004], lr: 0.047182, loss: 1.6428
2022-03-03 06:38:35 - train: epoch 0107, iter [04200, 05004], lr: 0.047182, loss: 1.8864
2022-03-03 06:39:08 - train: epoch 0107, iter [04300, 05004], lr: 0.047182, loss: 1.9296
2022-03-03 06:39:42 - train: epoch 0107, iter [04400, 05004], lr: 0.047182, loss: 1.7869
2022-03-03 06:40:14 - train: epoch 0107, iter [04500, 05004], lr: 0.047182, loss: 1.7547
2022-03-03 06:40:48 - train: epoch 0107, iter [04600, 05004], lr: 0.047182, loss: 1.7212
2022-03-03 06:41:22 - train: epoch 0107, iter [04700, 05004], lr: 0.047182, loss: 1.8641
2022-03-03 06:41:55 - train: epoch 0107, iter [04800, 05004], lr: 0.047182, loss: 1.7985
2022-03-03 06:42:27 - train: epoch 0107, iter [04900, 05004], lr: 0.047182, loss: 1.6139
2022-03-03 06:43:00 - train: epoch 0107, iter [05000, 05004], lr: 0.047182, loss: 1.5408
2022-03-03 06:43:01 - train: epoch 107, train_loss: 1.7562
2022-03-03 06:44:15 - eval: epoch: 107, acc1: 62.746%, acc5: 85.340%, test_loss: 1.5311, per_image_load_time: 1.408ms, per_image_inference_time: 0.539ms
2022-03-03 06:44:16 - until epoch: 107, best_acc1: 64.168%
2022-03-03 06:44:16 - epoch 108 lr: 0.04637825999191189
2022-03-03 06:44:53 - train: epoch 0108, iter [00100, 05004], lr: 0.046378, loss: 1.6069
2022-03-03 06:45:27 - train: epoch 0108, iter [00200, 05004], lr: 0.046378, loss: 1.4410
2022-03-03 06:46:00 - train: epoch 0108, iter [00300, 05004], lr: 0.046378, loss: 1.8402
2022-03-03 06:46:33 - train: epoch 0108, iter [00400, 05004], lr: 0.046378, loss: 1.9647
2022-03-03 06:47:05 - train: epoch 0108, iter [00500, 05004], lr: 0.046378, loss: 1.6325
2022-03-03 06:47:39 - train: epoch 0108, iter [00600, 05004], lr: 0.046378, loss: 1.9362
2022-03-03 06:48:11 - train: epoch 0108, iter [00700, 05004], lr: 0.046378, loss: 1.6702
2022-03-03 06:48:44 - train: epoch 0108, iter [00800, 05004], lr: 0.046378, loss: 1.7130
2022-03-03 06:49:18 - train: epoch 0108, iter [00900, 05004], lr: 0.046378, loss: 1.7211
2022-03-03 06:49:51 - train: epoch 0108, iter [01000, 05004], lr: 0.046378, loss: 1.6943
2022-03-03 06:50:24 - train: epoch 0108, iter [01100, 05004], lr: 0.046378, loss: 1.7938
2022-03-03 06:50:58 - train: epoch 0108, iter [01200, 05004], lr: 0.046378, loss: 1.7782
2022-03-03 06:51:30 - train: epoch 0108, iter [01300, 05004], lr: 0.046378, loss: 1.7204
2022-03-03 06:52:04 - train: epoch 0108, iter [01400, 05004], lr: 0.046378, loss: 1.7940
2022-03-03 06:52:37 - train: epoch 0108, iter [01500, 05004], lr: 0.046378, loss: 1.8674
2022-03-03 06:53:10 - train: epoch 0108, iter [01600, 05004], lr: 0.046378, loss: 1.8552
2022-03-03 06:53:43 - train: epoch 0108, iter [01700, 05004], lr: 0.046378, loss: 1.7739
2022-03-03 06:54:17 - train: epoch 0108, iter [01800, 05004], lr: 0.046378, loss: 1.6677
2022-03-03 06:54:50 - train: epoch 0108, iter [01900, 05004], lr: 0.046378, loss: 1.8030
2022-03-03 06:55:23 - train: epoch 0108, iter [02000, 05004], lr: 0.046378, loss: 1.8549
2022-03-03 06:55:56 - train: epoch 0108, iter [02100, 05004], lr: 0.046378, loss: 1.8079
2022-03-03 06:56:30 - train: epoch 0108, iter [02200, 05004], lr: 0.046378, loss: 1.7273
2022-03-03 06:57:02 - train: epoch 0108, iter [02300, 05004], lr: 0.046378, loss: 1.7435
2022-03-03 06:57:36 - train: epoch 0108, iter [02400, 05004], lr: 0.046378, loss: 1.9465
2022-03-03 06:58:09 - train: epoch 0108, iter [02500, 05004], lr: 0.046378, loss: 2.0215
2022-03-03 06:58:43 - train: epoch 0108, iter [02600, 05004], lr: 0.046378, loss: 2.0155
2022-03-03 06:59:15 - train: epoch 0108, iter [02700, 05004], lr: 0.046378, loss: 1.8185
2022-03-03 06:59:49 - train: epoch 0108, iter [02800, 05004], lr: 0.046378, loss: 1.8241
2022-03-03 07:00:21 - train: epoch 0108, iter [02900, 05004], lr: 0.046378, loss: 1.6195
2022-03-03 07:00:55 - train: epoch 0108, iter [03000, 05004], lr: 0.046378, loss: 1.7334
2022-03-03 07:01:28 - train: epoch 0108, iter [03100, 05004], lr: 0.046378, loss: 1.7407
2022-03-03 07:02:01 - train: epoch 0108, iter [03200, 05004], lr: 0.046378, loss: 1.6568
2022-03-03 07:02:34 - train: epoch 0108, iter [03300, 05004], lr: 0.046378, loss: 1.6280
2022-03-03 07:03:07 - train: epoch 0108, iter [03400, 05004], lr: 0.046378, loss: 2.0076
2022-03-03 07:03:40 - train: epoch 0108, iter [03500, 05004], lr: 0.046378, loss: 1.7044
2022-03-03 07:04:13 - train: epoch 0108, iter [03600, 05004], lr: 0.046378, loss: 1.8424
2022-03-03 07:04:47 - train: epoch 0108, iter [03700, 05004], lr: 0.046378, loss: 1.8528
2022-03-03 07:05:20 - train: epoch 0108, iter [03800, 05004], lr: 0.046378, loss: 1.7877
2022-03-03 07:05:53 - train: epoch 0108, iter [03900, 05004], lr: 0.046378, loss: 1.8808
2022-03-03 07:06:26 - train: epoch 0108, iter [04000, 05004], lr: 0.046378, loss: 1.7249
2022-03-03 07:06:59 - train: epoch 0108, iter [04100, 05004], lr: 0.046378, loss: 1.6783
2022-03-03 07:07:33 - train: epoch 0108, iter [04200, 05004], lr: 0.046378, loss: 1.6955
2022-03-03 07:08:05 - train: epoch 0108, iter [04300, 05004], lr: 0.046378, loss: 1.7164
2022-03-03 07:08:39 - train: epoch 0108, iter [04400, 05004], lr: 0.046378, loss: 1.7878
2022-03-03 07:09:12 - train: epoch 0108, iter [04500, 05004], lr: 0.046378, loss: 1.9237
2022-03-03 07:09:45 - train: epoch 0108, iter [04600, 05004], lr: 0.046378, loss: 1.7895
2022-03-03 07:10:19 - train: epoch 0108, iter [04700, 05004], lr: 0.046378, loss: 1.7845
2022-03-03 07:10:53 - train: epoch 0108, iter [04800, 05004], lr: 0.046378, loss: 1.6375
2022-03-03 07:11:25 - train: epoch 0108, iter [04900, 05004], lr: 0.046378, loss: 1.8252
2022-03-03 07:11:57 - train: epoch 0108, iter [05000, 05004], lr: 0.046378, loss: 1.7504
2022-03-03 07:11:58 - train: epoch 108, train_loss: 1.7483
2022-03-03 07:13:13 - eval: epoch: 108, acc1: 59.604%, acc5: 83.058%, test_loss: 1.6942, per_image_load_time: 2.362ms, per_image_inference_time: 0.518ms
2022-03-03 07:13:13 - until epoch: 108, best_acc1: 64.168%
2022-03-03 07:13:13 - epoch 109 lr: 0.045575344204432086
2022-03-03 07:13:51 - train: epoch 0109, iter [00100, 05004], lr: 0.045575, loss: 1.6639
2022-03-03 07:14:24 - train: epoch 0109, iter [00200, 05004], lr: 0.045575, loss: 1.8655
2022-03-03 07:14:56 - train: epoch 0109, iter [00300, 05004], lr: 0.045575, loss: 1.7214
2022-03-03 07:15:30 - train: epoch 0109, iter [00400, 05004], lr: 0.045575, loss: 1.7192
2022-03-03 07:16:03 - train: epoch 0109, iter [00500, 05004], lr: 0.045575, loss: 1.8959
2022-03-03 07:16:36 - train: epoch 0109, iter [00600, 05004], lr: 0.045575, loss: 1.6045
2022-03-03 07:17:09 - train: epoch 0109, iter [00700, 05004], lr: 0.045575, loss: 1.6699
2022-03-03 07:17:43 - train: epoch 0109, iter [00800, 05004], lr: 0.045575, loss: 1.7958
2022-03-03 07:18:16 - train: epoch 0109, iter [00900, 05004], lr: 0.045575, loss: 1.6444
2022-03-03 07:18:50 - train: epoch 0109, iter [01000, 05004], lr: 0.045575, loss: 1.5156
2022-03-03 07:19:23 - train: epoch 0109, iter [01100, 05004], lr: 0.045575, loss: 1.5796
2022-03-03 07:19:56 - train: epoch 0109, iter [01200, 05004], lr: 0.045575, loss: 1.6792
2022-03-03 07:20:29 - train: epoch 0109, iter [01300, 05004], lr: 0.045575, loss: 1.4695
2022-03-03 07:21:03 - train: epoch 0109, iter [01400, 05004], lr: 0.045575, loss: 1.6790
2022-03-03 07:21:36 - train: epoch 0109, iter [01500, 05004], lr: 0.045575, loss: 1.9148
2022-03-03 07:22:10 - train: epoch 0109, iter [01600, 05004], lr: 0.045575, loss: 1.5503
2022-03-03 07:22:43 - train: epoch 0109, iter [01700, 05004], lr: 0.045575, loss: 1.9526
2022-03-03 07:23:16 - train: epoch 0109, iter [01800, 05004], lr: 0.045575, loss: 1.7743
2022-03-03 07:23:49 - train: epoch 0109, iter [01900, 05004], lr: 0.045575, loss: 1.8133
2022-03-03 07:24:23 - train: epoch 0109, iter [02000, 05004], lr: 0.045575, loss: 1.7504
2022-03-03 07:24:56 - train: epoch 0109, iter [02100, 05004], lr: 0.045575, loss: 1.6191
2022-03-03 07:25:29 - train: epoch 0109, iter [02200, 05004], lr: 0.045575, loss: 1.6173
2022-03-03 07:26:02 - train: epoch 0109, iter [02300, 05004], lr: 0.045575, loss: 1.6452
2022-03-03 07:26:35 - train: epoch 0109, iter [02400, 05004], lr: 0.045575, loss: 1.7432
2022-03-03 07:27:09 - train: epoch 0109, iter [02500, 05004], lr: 0.045575, loss: 1.5760
2022-03-03 07:27:42 - train: epoch 0109, iter [02600, 05004], lr: 0.045575, loss: 1.7796
2022-03-03 07:28:15 - train: epoch 0109, iter [02700, 05004], lr: 0.045575, loss: 1.5754
2022-03-03 07:28:49 - train: epoch 0109, iter [02800, 05004], lr: 0.045575, loss: 1.8476
2022-03-03 07:29:22 - train: epoch 0109, iter [02900, 05004], lr: 0.045575, loss: 1.6830
2022-03-03 07:29:55 - train: epoch 0109, iter [03000, 05004], lr: 0.045575, loss: 1.6661
2022-03-03 07:30:28 - train: epoch 0109, iter [03100, 05004], lr: 0.045575, loss: 1.7955
2022-03-03 07:31:02 - train: epoch 0109, iter [03200, 05004], lr: 0.045575, loss: 1.6200
2022-03-03 07:31:35 - train: epoch 0109, iter [03300, 05004], lr: 0.045575, loss: 1.8128
2022-03-03 07:32:08 - train: epoch 0109, iter [03400, 05004], lr: 0.045575, loss: 1.6997
2022-03-03 07:32:41 - train: epoch 0109, iter [03500, 05004], lr: 0.045575, loss: 1.9282
2022-03-03 07:33:14 - train: epoch 0109, iter [03600, 05004], lr: 0.045575, loss: 1.7520
2022-03-03 07:33:47 - train: epoch 0109, iter [03700, 05004], lr: 0.045575, loss: 1.3014
2022-03-03 07:34:20 - train: epoch 0109, iter [03800, 05004], lr: 0.045575, loss: 1.8223
2022-03-03 07:34:53 - train: epoch 0109, iter [03900, 05004], lr: 0.045575, loss: 1.7332
2022-03-03 07:35:26 - train: epoch 0109, iter [04000, 05004], lr: 0.045575, loss: 1.7495
2022-03-03 07:35:59 - train: epoch 0109, iter [04100, 05004], lr: 0.045575, loss: 1.7800
2022-03-03 07:36:32 - train: epoch 0109, iter [04200, 05004], lr: 0.045575, loss: 1.7863
2022-03-03 07:37:05 - train: epoch 0109, iter [04300, 05004], lr: 0.045575, loss: 1.7878
2022-03-03 07:37:39 - train: epoch 0109, iter [04400, 05004], lr: 0.045575, loss: 1.6309
2022-03-03 07:38:11 - train: epoch 0109, iter [04500, 05004], lr: 0.045575, loss: 1.7376
2022-03-03 07:38:45 - train: epoch 0109, iter [04600, 05004], lr: 0.045575, loss: 1.7307
2022-03-03 07:39:18 - train: epoch 0109, iter [04700, 05004], lr: 0.045575, loss: 1.6128
2022-03-03 07:39:51 - train: epoch 0109, iter [04800, 05004], lr: 0.045575, loss: 1.8205
2022-03-03 07:40:25 - train: epoch 0109, iter [04900, 05004], lr: 0.045575, loss: 1.7131
2022-03-03 07:40:56 - train: epoch 0109, iter [05000, 05004], lr: 0.045575, loss: 1.5552
2022-03-03 07:40:57 - train: epoch 109, train_loss: 1.7401
2022-03-03 07:42:11 - eval: epoch: 109, acc1: 63.254%, acc5: 85.960%, test_loss: 1.5104, per_image_load_time: 2.025ms, per_image_inference_time: 0.543ms
2022-03-03 07:42:12 - until epoch: 109, best_acc1: 64.168%
2022-03-03 07:42:12 - epoch 110 lr: 0.04477357683661734
2022-03-03 07:42:49 - train: epoch 0110, iter [00100, 05004], lr: 0.044774, loss: 1.4855
2022-03-03 07:43:23 - train: epoch 0110, iter [00200, 05004], lr: 0.044774, loss: 1.5249
2022-03-03 07:43:55 - train: epoch 0110, iter [00300, 05004], lr: 0.044774, loss: 1.9614
2022-03-03 07:44:28 - train: epoch 0110, iter [00400, 05004], lr: 0.044774, loss: 1.6767
2022-03-03 07:45:02 - train: epoch 0110, iter [00500, 05004], lr: 0.044774, loss: 1.7057
2022-03-03 07:45:35 - train: epoch 0110, iter [00600, 05004], lr: 0.044774, loss: 1.8032
2022-03-03 07:46:08 - train: epoch 0110, iter [00700, 05004], lr: 0.044774, loss: 1.7755
2022-03-03 07:46:41 - train: epoch 0110, iter [00800, 05004], lr: 0.044774, loss: 1.8274
2022-03-03 07:47:15 - train: epoch 0110, iter [00900, 05004], lr: 0.044774, loss: 1.6955
2022-03-03 07:47:48 - train: epoch 0110, iter [01000, 05004], lr: 0.044774, loss: 1.5071
2022-03-03 07:48:21 - train: epoch 0110, iter [01100, 05004], lr: 0.044774, loss: 2.0485
2022-03-03 07:48:55 - train: epoch 0110, iter [01200, 05004], lr: 0.044774, loss: 1.7074
2022-03-03 07:49:28 - train: epoch 0110, iter [01300, 05004], lr: 0.044774, loss: 1.6954
2022-03-03 07:50:03 - train: epoch 0110, iter [01400, 05004], lr: 0.044774, loss: 1.4967
2022-03-03 07:50:36 - train: epoch 0110, iter [01500, 05004], lr: 0.044774, loss: 1.9382
2022-03-03 07:51:10 - train: epoch 0110, iter [01600, 05004], lr: 0.044774, loss: 1.7032
2022-03-03 07:51:43 - train: epoch 0110, iter [01700, 05004], lr: 0.044774, loss: 1.6859
2022-03-03 07:52:17 - train: epoch 0110, iter [01800, 05004], lr: 0.044774, loss: 1.8518
2022-03-03 07:52:50 - train: epoch 0110, iter [01900, 05004], lr: 0.044774, loss: 1.7923
2022-03-03 07:53:23 - train: epoch 0110, iter [02000, 05004], lr: 0.044774, loss: 1.7775
2022-03-03 07:53:56 - train: epoch 0110, iter [02100, 05004], lr: 0.044774, loss: 1.8439
2022-03-03 07:54:29 - train: epoch 0110, iter [02200, 05004], lr: 0.044774, loss: 1.6356
2022-03-03 07:55:03 - train: epoch 0110, iter [02300, 05004], lr: 0.044774, loss: 1.8051
2022-03-03 07:55:35 - train: epoch 0110, iter [02400, 05004], lr: 0.044774, loss: 1.7106
2022-03-03 07:56:09 - train: epoch 0110, iter [02500, 05004], lr: 0.044774, loss: 1.7369
2022-03-03 07:56:43 - train: epoch 0110, iter [02600, 05004], lr: 0.044774, loss: 1.9027
2022-03-03 07:57:16 - train: epoch 0110, iter [02700, 05004], lr: 0.044774, loss: 1.6546
2022-03-03 07:57:49 - train: epoch 0110, iter [02800, 05004], lr: 0.044774, loss: 1.7535
2022-03-03 07:58:23 - train: epoch 0110, iter [02900, 05004], lr: 0.044774, loss: 1.8812
2022-03-03 07:58:55 - train: epoch 0110, iter [03000, 05004], lr: 0.044774, loss: 1.6121
2022-03-03 07:59:29 - train: epoch 0110, iter [03100, 05004], lr: 0.044774, loss: 1.8334
2022-03-03 08:00:02 - train: epoch 0110, iter [03200, 05004], lr: 0.044774, loss: 1.7645
2022-03-03 08:00:36 - train: epoch 0110, iter [03300, 05004], lr: 0.044774, loss: 1.7835
2022-03-03 08:01:09 - train: epoch 0110, iter [03400, 05004], lr: 0.044774, loss: 1.6276
2022-03-03 08:01:42 - train: epoch 0110, iter [03500, 05004], lr: 0.044774, loss: 2.0864
2022-03-03 08:02:16 - train: epoch 0110, iter [03600, 05004], lr: 0.044774, loss: 1.9659
2022-03-03 08:02:49 - train: epoch 0110, iter [03700, 05004], lr: 0.044774, loss: 1.8319
2022-03-03 08:03:22 - train: epoch 0110, iter [03800, 05004], lr: 0.044774, loss: 1.8277
2022-03-03 08:03:55 - train: epoch 0110, iter [03900, 05004], lr: 0.044774, loss: 1.8796
2022-03-03 08:04:28 - train: epoch 0110, iter [04000, 05004], lr: 0.044774, loss: 1.6865
2022-03-03 08:05:00 - train: epoch 0110, iter [04100, 05004], lr: 0.044774, loss: 1.7059
2022-03-03 08:05:34 - train: epoch 0110, iter [04200, 05004], lr: 0.044774, loss: 1.6448
2022-03-03 08:06:06 - train: epoch 0110, iter [04300, 05004], lr: 0.044774, loss: 1.5760
2022-03-03 08:06:40 - train: epoch 0110, iter [04400, 05004], lr: 0.044774, loss: 1.8092
2022-03-03 08:07:13 - train: epoch 0110, iter [04500, 05004], lr: 0.044774, loss: 1.6891
2022-03-03 08:07:47 - train: epoch 0110, iter [04600, 05004], lr: 0.044774, loss: 1.4912
2022-03-03 08:08:20 - train: epoch 0110, iter [04700, 05004], lr: 0.044774, loss: 1.8530
2022-03-03 08:08:54 - train: epoch 0110, iter [04800, 05004], lr: 0.044774, loss: 1.9965
2022-03-03 08:09:26 - train: epoch 0110, iter [04900, 05004], lr: 0.044774, loss: 1.7827
2022-03-03 08:09:59 - train: epoch 0110, iter [05000, 05004], lr: 0.044774, loss: 1.9158
2022-03-03 08:10:00 - train: epoch 110, train_loss: 1.7339
2022-03-03 08:11:14 - eval: epoch: 110, acc1: 63.482%, acc5: 86.130%, test_loss: 1.4904, per_image_load_time: 2.364ms, per_image_inference_time: 0.524ms
2022-03-03 08:11:14 - until epoch: 110, best_acc1: 64.168%
2022-03-03 08:11:14 - epoch 111 lr: 0.04397316598723386
2022-03-03 08:11:53 - train: epoch 0111, iter [00100, 05004], lr: 0.043973, loss: 1.5157
2022-03-03 08:12:26 - train: epoch 0111, iter [00200, 05004], lr: 0.043973, loss: 1.7239
2022-03-03 08:12:58 - train: epoch 0111, iter [00300, 05004], lr: 0.043973, loss: 1.7281
2022-03-03 08:13:31 - train: epoch 0111, iter [00400, 05004], lr: 0.043973, loss: 1.6760
2022-03-03 08:14:05 - train: epoch 0111, iter [00500, 05004], lr: 0.043973, loss: 1.7180
2022-03-03 08:14:37 - train: epoch 0111, iter [00600, 05004], lr: 0.043973, loss: 1.8596
2022-03-03 08:15:10 - train: epoch 0111, iter [00700, 05004], lr: 0.043973, loss: 1.7981
2022-03-03 08:15:43 - train: epoch 0111, iter [00800, 05004], lr: 0.043973, loss: 1.6878
2022-03-03 08:16:16 - train: epoch 0111, iter [00900, 05004], lr: 0.043973, loss: 1.5967
2022-03-03 08:16:49 - train: epoch 0111, iter [01000, 05004], lr: 0.043973, loss: 1.4977
2022-03-03 08:17:23 - train: epoch 0111, iter [01100, 05004], lr: 0.043973, loss: 1.7319
2022-03-03 08:17:56 - train: epoch 0111, iter [01200, 05004], lr: 0.043973, loss: 1.6506
2022-03-03 08:18:29 - train: epoch 0111, iter [01300, 05004], lr: 0.043973, loss: 1.7592
2022-03-03 08:19:02 - train: epoch 0111, iter [01400, 05004], lr: 0.043973, loss: 1.5285
2022-03-03 08:19:36 - train: epoch 0111, iter [01500, 05004], lr: 0.043973, loss: 1.7207
2022-03-03 08:20:09 - train: epoch 0111, iter [01600, 05004], lr: 0.043973, loss: 1.5793
2022-03-03 08:20:42 - train: epoch 0111, iter [01700, 05004], lr: 0.043973, loss: 1.7115
2022-03-03 08:21:16 - train: epoch 0111, iter [01800, 05004], lr: 0.043973, loss: 1.5551
2022-03-03 08:21:48 - train: epoch 0111, iter [01900, 05004], lr: 0.043973, loss: 1.5963
2022-03-03 08:22:21 - train: epoch 0111, iter [02000, 05004], lr: 0.043973, loss: 1.8089
2022-03-03 08:22:55 - train: epoch 0111, iter [02100, 05004], lr: 0.043973, loss: 2.0711
2022-03-03 08:23:27 - train: epoch 0111, iter [02200, 05004], lr: 0.043973, loss: 1.8156
2022-03-03 08:24:00 - train: epoch 0111, iter [02300, 05004], lr: 0.043973, loss: 1.5969
2022-03-03 08:24:34 - train: epoch 0111, iter [02400, 05004], lr: 0.043973, loss: 1.6889
2022-03-03 08:25:07 - train: epoch 0111, iter [02500, 05004], lr: 0.043973, loss: 1.6380
2022-03-03 08:25:40 - train: epoch 0111, iter [02600, 05004], lr: 0.043973, loss: 1.6226
2022-03-03 08:26:13 - train: epoch 0111, iter [02700, 05004], lr: 0.043973, loss: 1.9144
2022-03-03 08:26:46 - train: epoch 0111, iter [02800, 05004], lr: 0.043973, loss: 1.6643
2022-03-03 08:27:19 - train: epoch 0111, iter [02900, 05004], lr: 0.043973, loss: 1.6498
2022-03-03 08:27:52 - train: epoch 0111, iter [03000, 05004], lr: 0.043973, loss: 1.6945
2022-03-03 08:28:25 - train: epoch 0111, iter [03100, 05004], lr: 0.043973, loss: 1.7438
2022-03-03 08:28:58 - train: epoch 0111, iter [03200, 05004], lr: 0.043973, loss: 1.8410
2022-03-03 08:29:31 - train: epoch 0111, iter [03300, 05004], lr: 0.043973, loss: 1.8859
2022-03-03 08:30:05 - train: epoch 0111, iter [03400, 05004], lr: 0.043973, loss: 1.9500
2022-03-03 08:30:38 - train: epoch 0111, iter [03500, 05004], lr: 0.043973, loss: 2.0071
2022-03-03 08:31:11 - train: epoch 0111, iter [03600, 05004], lr: 0.043973, loss: 1.5558
2022-03-03 08:31:45 - train: epoch 0111, iter [03700, 05004], lr: 0.043973, loss: 1.8653
2022-03-03 08:32:19 - train: epoch 0111, iter [03800, 05004], lr: 0.043973, loss: 1.7275
2022-03-03 08:32:52 - train: epoch 0111, iter [03900, 05004], lr: 0.043973, loss: 1.5625
2022-03-03 08:33:25 - train: epoch 0111, iter [04000, 05004], lr: 0.043973, loss: 1.8778
2022-03-03 08:33:58 - train: epoch 0111, iter [04100, 05004], lr: 0.043973, loss: 1.9632
2022-03-03 08:34:31 - train: epoch 0111, iter [04200, 05004], lr: 0.043973, loss: 1.7386
2022-03-03 08:35:04 - train: epoch 0111, iter [04300, 05004], lr: 0.043973, loss: 1.7557
2022-03-03 08:35:38 - train: epoch 0111, iter [04400, 05004], lr: 0.043973, loss: 1.6833
2022-03-03 08:36:11 - train: epoch 0111, iter [04500, 05004], lr: 0.043973, loss: 1.6980
2022-03-03 08:36:44 - train: epoch 0111, iter [04600, 05004], lr: 0.043973, loss: 1.6376
2022-03-03 08:37:18 - train: epoch 0111, iter [04700, 05004], lr: 0.043973, loss: 1.7550
2022-03-03 08:37:51 - train: epoch 0111, iter [04800, 05004], lr: 0.043973, loss: 1.7043
2022-03-03 08:38:24 - train: epoch 0111, iter [04900, 05004], lr: 0.043973, loss: 1.7718
2022-03-03 08:38:56 - train: epoch 0111, iter [05000, 05004], lr: 0.043973, loss: 1.8026
2022-03-03 08:38:57 - train: epoch 111, train_loss: 1.7260
2022-03-03 08:40:10 - eval: epoch: 111, acc1: 63.826%, acc5: 86.074%, test_loss: 1.4828, per_image_load_time: 1.363ms, per_image_inference_time: 0.534ms
2022-03-03 08:40:11 - until epoch: 111, best_acc1: 64.168%
2022-03-03 08:40:11 - epoch 112 lr: 0.04317431940296343
2022-03-03 08:40:49 - train: epoch 0112, iter [00100, 05004], lr: 0.043174, loss: 1.6473
2022-03-03 08:41:22 - train: epoch 0112, iter [00200, 05004], lr: 0.043174, loss: 1.6721
2022-03-03 08:41:54 - train: epoch 0112, iter [00300, 05004], lr: 0.043174, loss: 1.4062
2022-03-03 08:42:27 - train: epoch 0112, iter [00400, 05004], lr: 0.043174, loss: 1.6039
2022-03-03 08:43:01 - train: epoch 0112, iter [00500, 05004], lr: 0.043174, loss: 1.7923
2022-03-03 08:43:34 - train: epoch 0112, iter [00600, 05004], lr: 0.043174, loss: 1.7578
2022-03-03 08:44:07 - train: epoch 0112, iter [00700, 05004], lr: 0.043174, loss: 1.8967
2022-03-03 08:44:41 - train: epoch 0112, iter [00800, 05004], lr: 0.043174, loss: 1.4705
2022-03-03 08:45:13 - train: epoch 0112, iter [00900, 05004], lr: 0.043174, loss: 1.5560
2022-03-03 08:45:46 - train: epoch 0112, iter [01000, 05004], lr: 0.043174, loss: 1.7363
2022-03-03 08:46:19 - train: epoch 0112, iter [01100, 05004], lr: 0.043174, loss: 1.6308
2022-03-03 08:46:53 - train: epoch 0112, iter [01200, 05004], lr: 0.043174, loss: 1.8988
2022-03-03 08:47:26 - train: epoch 0112, iter [01300, 05004], lr: 0.043174, loss: 1.8608
2022-03-03 08:47:59 - train: epoch 0112, iter [01400, 05004], lr: 0.043174, loss: 1.6528
2022-03-03 08:48:32 - train: epoch 0112, iter [01500, 05004], lr: 0.043174, loss: 1.9484
2022-03-03 08:49:06 - train: epoch 0112, iter [01600, 05004], lr: 0.043174, loss: 1.8699
2022-03-03 08:49:39 - train: epoch 0112, iter [01700, 05004], lr: 0.043174, loss: 1.6137
2022-03-03 08:50:12 - train: epoch 0112, iter [01800, 05004], lr: 0.043174, loss: 1.4741
2022-03-03 08:50:46 - train: epoch 0112, iter [01900, 05004], lr: 0.043174, loss: 1.4789
2022-03-03 08:51:19 - train: epoch 0112, iter [02000, 05004], lr: 0.043174, loss: 1.5533
2022-03-03 08:51:52 - train: epoch 0112, iter [02100, 05004], lr: 0.043174, loss: 1.7749
2022-03-03 08:52:26 - train: epoch 0112, iter [02200, 05004], lr: 0.043174, loss: 1.6197
2022-03-03 08:52:59 - train: epoch 0112, iter [02300, 05004], lr: 0.043174, loss: 1.4870
2022-03-03 08:53:31 - train: epoch 0112, iter [02400, 05004], lr: 0.043174, loss: 1.5947
2022-03-03 08:54:05 - train: epoch 0112, iter [02500, 05004], lr: 0.043174, loss: 1.6752
2022-03-03 08:54:38 - train: epoch 0112, iter [02600, 05004], lr: 0.043174, loss: 1.4935
2022-03-03 08:55:11 - train: epoch 0112, iter [02700, 05004], lr: 0.043174, loss: 1.9075
2022-03-03 08:55:44 - train: epoch 0112, iter [02800, 05004], lr: 0.043174, loss: 1.7591
2022-03-03 08:56:18 - train: epoch 0112, iter [02900, 05004], lr: 0.043174, loss: 1.8816
2022-03-03 08:56:51 - train: epoch 0112, iter [03000, 05004], lr: 0.043174, loss: 1.7866
2022-03-03 08:57:23 - train: epoch 0112, iter [03100, 05004], lr: 0.043174, loss: 1.7768
2022-03-03 08:57:57 - train: epoch 0112, iter [03200, 05004], lr: 0.043174, loss: 1.5864
2022-03-03 08:58:30 - train: epoch 0112, iter [03300, 05004], lr: 0.043174, loss: 1.6843
2022-03-03 08:59:03 - train: epoch 0112, iter [03400, 05004], lr: 0.043174, loss: 1.7142
2022-03-03 08:59:36 - train: epoch 0112, iter [03500, 05004], lr: 0.043174, loss: 1.4958
2022-03-03 09:00:09 - train: epoch 0112, iter [03600, 05004], lr: 0.043174, loss: 1.8472
2022-03-03 09:00:43 - train: epoch 0112, iter [03700, 05004], lr: 0.043174, loss: 1.8775
2022-03-03 09:01:15 - train: epoch 0112, iter [03800, 05004], lr: 0.043174, loss: 1.7985
2022-03-03 09:01:48 - train: epoch 0112, iter [03900, 05004], lr: 0.043174, loss: 1.5124
2022-03-03 09:02:22 - train: epoch 0112, iter [04000, 05004], lr: 0.043174, loss: 1.6517
2022-03-03 09:02:55 - train: epoch 0112, iter [04100, 05004], lr: 0.043174, loss: 1.5990
2022-03-03 09:03:28 - train: epoch 0112, iter [04200, 05004], lr: 0.043174, loss: 1.5404
2022-03-03 09:04:01 - train: epoch 0112, iter [04300, 05004], lr: 0.043174, loss: 1.8423
2022-03-03 09:04:35 - train: epoch 0112, iter [04400, 05004], lr: 0.043174, loss: 1.9519
2022-03-03 09:05:07 - train: epoch 0112, iter [04500, 05004], lr: 0.043174, loss: 1.5498
2022-03-03 09:05:41 - train: epoch 0112, iter [04600, 05004], lr: 0.043174, loss: 1.8730
2022-03-03 09:06:14 - train: epoch 0112, iter [04700, 05004], lr: 0.043174, loss: 1.7525
2022-03-03 09:06:48 - train: epoch 0112, iter [04800, 05004], lr: 0.043174, loss: 1.6445
2022-03-03 09:07:21 - train: epoch 0112, iter [04900, 05004], lr: 0.043174, loss: 1.5794
2022-03-03 09:07:53 - train: epoch 0112, iter [05000, 05004], lr: 0.043174, loss: 1.5449
2022-03-03 09:07:54 - train: epoch 112, train_loss: 1.7198
2022-03-03 09:09:08 - eval: epoch: 112, acc1: 63.282%, acc5: 85.846%, test_loss: 1.5098, per_image_load_time: 1.345ms, per_image_inference_time: 0.517ms
2022-03-03 09:09:08 - until epoch: 112, best_acc1: 64.168%
2022-03-03 09:09:08 - epoch 113 lr: 0.042377244424482735
2022-03-03 09:09:46 - train: epoch 0113, iter [00100, 05004], lr: 0.042377, loss: 1.8023
2022-03-03 09:10:20 - train: epoch 0113, iter [00200, 05004], lr: 0.042377, loss: 1.5802
2022-03-03 09:10:54 - train: epoch 0113, iter [00300, 05004], lr: 0.042377, loss: 1.6014
2022-03-03 09:11:26 - train: epoch 0113, iter [00400, 05004], lr: 0.042377, loss: 1.6508
2022-03-03 09:12:00 - train: epoch 0113, iter [00500, 05004], lr: 0.042377, loss: 1.7691
2022-03-03 09:12:34 - train: epoch 0113, iter [00600, 05004], lr: 0.042377, loss: 1.7626
2022-03-03 09:13:07 - train: epoch 0113, iter [00700, 05004], lr: 0.042377, loss: 1.6551
2022-03-03 09:13:40 - train: epoch 0113, iter [00800, 05004], lr: 0.042377, loss: 1.7903
2022-03-03 09:14:13 - train: epoch 0113, iter [00900, 05004], lr: 0.042377, loss: 1.6473
2022-03-03 09:14:47 - train: epoch 0113, iter [01000, 05004], lr: 0.042377, loss: 1.8010
2022-03-03 09:15:21 - train: epoch 0113, iter [01100, 05004], lr: 0.042377, loss: 1.7265
2022-03-03 09:15:54 - train: epoch 0113, iter [01200, 05004], lr: 0.042377, loss: 1.7374
2022-03-03 09:16:28 - train: epoch 0113, iter [01300, 05004], lr: 0.042377, loss: 1.5313
2022-03-03 09:17:01 - train: epoch 0113, iter [01400, 05004], lr: 0.042377, loss: 1.7489
2022-03-03 09:17:34 - train: epoch 0113, iter [01500, 05004], lr: 0.042377, loss: 1.7943
2022-03-03 09:18:07 - train: epoch 0113, iter [01600, 05004], lr: 0.042377, loss: 1.7246
2022-03-03 09:18:41 - train: epoch 0113, iter [01700, 05004], lr: 0.042377, loss: 1.6545
2022-03-03 09:19:14 - train: epoch 0113, iter [01800, 05004], lr: 0.042377, loss: 1.4933
2022-03-03 09:19:47 - train: epoch 0113, iter [01900, 05004], lr: 0.042377, loss: 1.6668
2022-03-03 09:20:20 - train: epoch 0113, iter [02000, 05004], lr: 0.042377, loss: 1.6760
2022-03-03 09:20:53 - train: epoch 0113, iter [02100, 05004], lr: 0.042377, loss: 1.9676
2022-03-03 09:21:26 - train: epoch 0113, iter [02200, 05004], lr: 0.042377, loss: 1.9722
2022-03-03 09:21:59 - train: epoch 0113, iter [02300, 05004], lr: 0.042377, loss: 1.6676
2022-03-03 09:22:32 - train: epoch 0113, iter [02400, 05004], lr: 0.042377, loss: 1.8450
2022-03-03 09:23:06 - train: epoch 0113, iter [02500, 05004], lr: 0.042377, loss: 1.8228
2022-03-03 09:23:39 - train: epoch 0113, iter [02600, 05004], lr: 0.042377, loss: 1.4680
2022-03-03 09:24:12 - train: epoch 0113, iter [02700, 05004], lr: 0.042377, loss: 1.5952
2022-03-03 09:24:46 - train: epoch 0113, iter [02800, 05004], lr: 0.042377, loss: 1.6558
2022-03-03 09:25:18 - train: epoch 0113, iter [02900, 05004], lr: 0.042377, loss: 1.8185
2022-03-03 09:25:52 - train: epoch 0113, iter [03000, 05004], lr: 0.042377, loss: 1.6659
2022-03-03 09:26:24 - train: epoch 0113, iter [03100, 05004], lr: 0.042377, loss: 1.4580
2022-03-03 09:26:58 - train: epoch 0113, iter [03200, 05004], lr: 0.042377, loss: 1.5325
2022-03-03 09:27:30 - train: epoch 0113, iter [03300, 05004], lr: 0.042377, loss: 1.6215
2022-03-03 09:28:04 - train: epoch 0113, iter [03400, 05004], lr: 0.042377, loss: 1.6884
2022-03-03 09:28:36 - train: epoch 0113, iter [03500, 05004], lr: 0.042377, loss: 1.6899
2022-03-03 09:29:10 - train: epoch 0113, iter [03600, 05004], lr: 0.042377, loss: 1.9004
2022-03-03 09:29:42 - train: epoch 0113, iter [03700, 05004], lr: 0.042377, loss: 1.4274
2022-03-03 09:30:16 - train: epoch 0113, iter [03800, 05004], lr: 0.042377, loss: 1.8902
2022-03-03 09:30:49 - train: epoch 0113, iter [03900, 05004], lr: 0.042377, loss: 1.8755
2022-03-03 09:31:22 - train: epoch 0113, iter [04000, 05004], lr: 0.042377, loss: 1.6637
2022-03-03 09:31:55 - train: epoch 0113, iter [04100, 05004], lr: 0.042377, loss: 1.7681
2022-03-03 09:32:28 - train: epoch 0113, iter [04200, 05004], lr: 0.042377, loss: 1.9323
2022-03-03 09:33:01 - train: epoch 0113, iter [04300, 05004], lr: 0.042377, loss: 1.6255
2022-03-03 09:33:34 - train: epoch 0113, iter [04400, 05004], lr: 0.042377, loss: 1.8574
2022-03-03 09:34:07 - train: epoch 0113, iter [04500, 05004], lr: 0.042377, loss: 1.6624
2022-03-03 09:34:40 - train: epoch 0113, iter [04600, 05004], lr: 0.042377, loss: 1.9131
2022-03-03 09:35:13 - train: epoch 0113, iter [04700, 05004], lr: 0.042377, loss: 1.5991
2022-03-03 09:35:46 - train: epoch 0113, iter [04800, 05004], lr: 0.042377, loss: 2.1351
2022-03-03 09:36:18 - train: epoch 0113, iter [04900, 05004], lr: 0.042377, loss: 1.6919
2022-03-03 09:36:50 - train: epoch 0113, iter [05000, 05004], lr: 0.042377, loss: 1.7749
2022-03-03 09:36:51 - train: epoch 113, train_loss: 1.7105
2022-03-03 09:38:04 - eval: epoch: 113, acc1: 58.062%, acc5: 81.716%, test_loss: 1.7873, per_image_load_time: 0.751ms, per_image_inference_time: 0.479ms
2022-03-03 09:38:05 - until epoch: 113, best_acc1: 64.168%
2022-03-03 09:38:05 - epoch 114 lr: 0.04158214793264808
2022-03-03 09:38:43 - train: epoch 0114, iter [00100, 05004], lr: 0.041582, loss: 1.8160
2022-03-03 09:39:16 - train: epoch 0114, iter [00200, 05004], lr: 0.041582, loss: 1.5211
2022-03-03 09:39:48 - train: epoch 0114, iter [00300, 05004], lr: 0.041582, loss: 1.6927
2022-03-03 09:40:23 - train: epoch 0114, iter [00400, 05004], lr: 0.041582, loss: 1.8370
2022-03-03 09:40:55 - train: epoch 0114, iter [00500, 05004], lr: 0.041582, loss: 1.5200
2022-03-03 09:41:29 - train: epoch 0114, iter [00600, 05004], lr: 0.041582, loss: 1.7629
2022-03-03 09:42:02 - train: epoch 0114, iter [00700, 05004], lr: 0.041582, loss: 1.8076
2022-03-03 09:42:35 - train: epoch 0114, iter [00800, 05004], lr: 0.041582, loss: 1.6010
2022-03-03 09:43:08 - train: epoch 0114, iter [00900, 05004], lr: 0.041582, loss: 1.6169
2022-03-03 09:43:41 - train: epoch 0114, iter [01000, 05004], lr: 0.041582, loss: 1.7736
2022-03-03 09:44:14 - train: epoch 0114, iter [01100, 05004], lr: 0.041582, loss: 1.7479
2022-03-03 09:44:48 - train: epoch 0114, iter [01200, 05004], lr: 0.041582, loss: 1.6827
2022-03-03 09:45:21 - train: epoch 0114, iter [01300, 05004], lr: 0.041582, loss: 1.5390
2022-03-03 09:45:54 - train: epoch 0114, iter [01400, 05004], lr: 0.041582, loss: 1.7317
2022-03-03 09:46:27 - train: epoch 0114, iter [01500, 05004], lr: 0.041582, loss: 1.6306
2022-03-03 09:47:01 - train: epoch 0114, iter [01600, 05004], lr: 0.041582, loss: 1.6429
2022-03-03 09:47:34 - train: epoch 0114, iter [01700, 05004], lr: 0.041582, loss: 1.4892
2022-03-03 09:48:07 - train: epoch 0114, iter [01800, 05004], lr: 0.041582, loss: 1.6677
2022-03-03 09:48:40 - train: epoch 0114, iter [01900, 05004], lr: 0.041582, loss: 1.4335
2022-03-03 09:49:14 - train: epoch 0114, iter [02000, 05004], lr: 0.041582, loss: 1.8737
2022-03-03 09:49:47 - train: epoch 0114, iter [02100, 05004], lr: 0.041582, loss: 1.6631
2022-03-03 09:50:20 - train: epoch 0114, iter [02200, 05004], lr: 0.041582, loss: 1.5891
2022-03-03 09:50:53 - train: epoch 0114, iter [02300, 05004], lr: 0.041582, loss: 1.8915
2022-03-03 09:51:26 - train: epoch 0114, iter [02400, 05004], lr: 0.041582, loss: 1.6919
2022-03-03 09:51:59 - train: epoch 0114, iter [02500, 05004], lr: 0.041582, loss: 1.6824
2022-03-03 09:52:32 - train: epoch 0114, iter [02600, 05004], lr: 0.041582, loss: 1.6041
2022-03-03 09:53:05 - train: epoch 0114, iter [02700, 05004], lr: 0.041582, loss: 1.8169
2022-03-03 09:53:38 - train: epoch 0114, iter [02800, 05004], lr: 0.041582, loss: 1.7500
2022-03-03 09:54:11 - train: epoch 0114, iter [02900, 05004], lr: 0.041582, loss: 1.6847
2022-03-03 09:54:44 - train: epoch 0114, iter [03000, 05004], lr: 0.041582, loss: 1.5161
2022-03-03 09:55:17 - train: epoch 0114, iter [03100, 05004], lr: 0.041582, loss: 2.0009
2022-03-03 09:55:51 - train: epoch 0114, iter [03200, 05004], lr: 0.041582, loss: 1.7299
2022-03-03 09:56:24 - train: epoch 0114, iter [03300, 05004], lr: 0.041582, loss: 1.9356
2022-03-03 09:56:57 - train: epoch 0114, iter [03400, 05004], lr: 0.041582, loss: 1.6353
2022-03-03 09:57:30 - train: epoch 0114, iter [03500, 05004], lr: 0.041582, loss: 1.5001
2022-03-03 09:58:03 - train: epoch 0114, iter [03600, 05004], lr: 0.041582, loss: 1.7204
2022-03-03 09:58:37 - train: epoch 0114, iter [03700, 05004], lr: 0.041582, loss: 1.5855
2022-03-03 09:59:10 - train: epoch 0114, iter [03800, 05004], lr: 0.041582, loss: 1.6605
2022-03-03 09:59:43 - train: epoch 0114, iter [03900, 05004], lr: 0.041582, loss: 1.8058
2022-03-03 10:00:16 - train: epoch 0114, iter [04000, 05004], lr: 0.041582, loss: 1.6115
2022-03-03 10:00:49 - train: epoch 0114, iter [04100, 05004], lr: 0.041582, loss: 1.5908
2022-03-03 10:01:22 - train: epoch 0114, iter [04200, 05004], lr: 0.041582, loss: 1.8516
2022-03-03 10:01:55 - train: epoch 0114, iter [04300, 05004], lr: 0.041582, loss: 1.6368
2022-03-03 10:02:28 - train: epoch 0114, iter [04400, 05004], lr: 0.041582, loss: 1.8462
2022-03-03 10:03:01 - train: epoch 0114, iter [04500, 05004], lr: 0.041582, loss: 1.8534
2022-03-03 10:03:34 - train: epoch 0114, iter [04600, 05004], lr: 0.041582, loss: 1.6488
2022-03-03 10:04:07 - train: epoch 0114, iter [04700, 05004], lr: 0.041582, loss: 1.7738
2022-03-03 10:04:40 - train: epoch 0114, iter [04800, 05004], lr: 0.041582, loss: 2.0112
2022-03-03 10:05:13 - train: epoch 0114, iter [04900, 05004], lr: 0.041582, loss: 1.6296
2022-03-03 10:05:45 - train: epoch 0114, iter [05000, 05004], lr: 0.041582, loss: 1.9111
2022-03-03 10:05:46 - train: epoch 114, train_loss: 1.7019
2022-03-03 10:07:00 - eval: epoch: 114, acc1: 63.168%, acc5: 85.500%, test_loss: 1.5284, per_image_load_time: 1.510ms, per_image_inference_time: 0.525ms
2022-03-03 10:07:01 - until epoch: 114, best_acc1: 64.168%
2022-03-03 10:07:01 - epoch 115 lr: 0.04078923629479943
2022-03-03 10:07:38 - train: epoch 0115, iter [00100, 05004], lr: 0.040789, loss: 1.5737
2022-03-03 10:08:11 - train: epoch 0115, iter [00200, 05004], lr: 0.040789, loss: 1.7291
2022-03-03 10:08:44 - train: epoch 0115, iter [00300, 05004], lr: 0.040789, loss: 1.6792
2022-03-03 10:09:17 - train: epoch 0115, iter [00400, 05004], lr: 0.040789, loss: 1.4441
2022-03-03 10:09:50 - train: epoch 0115, iter [00500, 05004], lr: 0.040789, loss: 1.5696
2022-03-03 10:10:22 - train: epoch 0115, iter [00600, 05004], lr: 0.040789, loss: 1.6248
2022-03-03 10:10:56 - train: epoch 0115, iter [00700, 05004], lr: 0.040789, loss: 1.7036
2022-03-03 10:11:29 - train: epoch 0115, iter [00800, 05004], lr: 0.040789, loss: 1.6056
2022-03-03 10:12:02 - train: epoch 0115, iter [00900, 05004], lr: 0.040789, loss: 1.9011
2022-03-03 10:12:36 - train: epoch 0115, iter [01000, 05004], lr: 0.040789, loss: 1.6169
2022-03-03 10:13:09 - train: epoch 0115, iter [01100, 05004], lr: 0.040789, loss: 1.5204
2022-03-03 10:13:42 - train: epoch 0115, iter [01200, 05004], lr: 0.040789, loss: 1.7706
2022-03-03 10:14:15 - train: epoch 0115, iter [01300, 05004], lr: 0.040789, loss: 1.6036
2022-03-03 10:14:48 - train: epoch 0115, iter [01400, 05004], lr: 0.040789, loss: 1.6871
2022-03-03 10:15:21 - train: epoch 0115, iter [01500, 05004], lr: 0.040789, loss: 1.9513
2022-03-03 10:15:54 - train: epoch 0115, iter [01600, 05004], lr: 0.040789, loss: 1.5657
2022-03-03 10:16:27 - train: epoch 0115, iter [01700, 05004], lr: 0.040789, loss: 1.5884
2022-03-03 10:17:00 - train: epoch 0115, iter [01800, 05004], lr: 0.040789, loss: 1.6589
2022-03-03 10:17:34 - train: epoch 0115, iter [01900, 05004], lr: 0.040789, loss: 1.6617
2022-03-03 10:18:06 - train: epoch 0115, iter [02000, 05004], lr: 0.040789, loss: 1.9651
2022-03-03 10:18:40 - train: epoch 0115, iter [02100, 05004], lr: 0.040789, loss: 1.7330
2022-03-03 10:19:13 - train: epoch 0115, iter [02200, 05004], lr: 0.040789, loss: 1.8202
2022-03-03 10:19:46 - train: epoch 0115, iter [02300, 05004], lr: 0.040789, loss: 1.9085
2022-03-03 10:20:19 - train: epoch 0115, iter [02400, 05004], lr: 0.040789, loss: 1.6952
2022-03-03 10:20:52 - train: epoch 0115, iter [02500, 05004], lr: 0.040789, loss: 1.5702
2022-03-03 10:21:25 - train: epoch 0115, iter [02600, 05004], lr: 0.040789, loss: 1.6736
2022-03-03 10:21:58 - train: epoch 0115, iter [02700, 05004], lr: 0.040789, loss: 1.6004
2022-03-03 10:22:31 - train: epoch 0115, iter [02800, 05004], lr: 0.040789, loss: 1.7796
2022-03-03 10:23:05 - train: epoch 0115, iter [02900, 05004], lr: 0.040789, loss: 1.5788
2022-03-03 10:23:38 - train: epoch 0115, iter [03000, 05004], lr: 0.040789, loss: 1.4451
2022-03-03 10:24:11 - train: epoch 0115, iter [03100, 05004], lr: 0.040789, loss: 1.7735
2022-03-03 10:24:44 - train: epoch 0115, iter [03200, 05004], lr: 0.040789, loss: 1.7244
2022-03-03 10:25:17 - train: epoch 0115, iter [03300, 05004], lr: 0.040789, loss: 1.7647
2022-03-03 10:25:49 - train: epoch 0115, iter [03400, 05004], lr: 0.040789, loss: 1.7155
2022-03-03 10:26:23 - train: epoch 0115, iter [03500, 05004], lr: 0.040789, loss: 1.5098
2022-03-03 10:26:57 - train: epoch 0115, iter [03600, 05004], lr: 0.040789, loss: 1.6971
2022-03-03 10:27:30 - train: epoch 0115, iter [03700, 05004], lr: 0.040789, loss: 1.6876
2022-03-03 10:28:03 - train: epoch 0115, iter [03800, 05004], lr: 0.040789, loss: 1.8571
2022-03-03 10:28:36 - train: epoch 0115, iter [03900, 05004], lr: 0.040789, loss: 1.6252
2022-03-03 10:29:10 - train: epoch 0115, iter [04000, 05004], lr: 0.040789, loss: 1.7916
2022-03-03 10:29:43 - train: epoch 0115, iter [04100, 05004], lr: 0.040789, loss: 1.5606
2022-03-03 10:30:16 - train: epoch 0115, iter [04200, 05004], lr: 0.040789, loss: 1.4528
2022-03-03 10:30:49 - train: epoch 0115, iter [04300, 05004], lr: 0.040789, loss: 1.6315
2022-03-03 10:31:22 - train: epoch 0115, iter [04400, 05004], lr: 0.040789, loss: 1.7660
2022-03-03 10:31:56 - train: epoch 0115, iter [04500, 05004], lr: 0.040789, loss: 1.6956
2022-03-03 10:32:29 - train: epoch 0115, iter [04600, 05004], lr: 0.040789, loss: 1.8007
2022-03-03 10:33:02 - train: epoch 0115, iter [04700, 05004], lr: 0.040789, loss: 1.6070
2022-03-03 10:33:34 - train: epoch 0115, iter [04800, 05004], lr: 0.040789, loss: 1.5500
2022-03-03 10:34:09 - train: epoch 0115, iter [04900, 05004], lr: 0.040789, loss: 1.8571
2022-03-03 10:34:40 - train: epoch 0115, iter [05000, 05004], lr: 0.040789, loss: 1.7762
2022-03-03 10:34:41 - train: epoch 115, train_loss: 1.6959
2022-03-03 10:35:54 - eval: epoch: 115, acc1: 64.062%, acc5: 86.040%, test_loss: 1.4843, per_image_load_time: 1.032ms, per_image_inference_time: 0.497ms
2022-03-03 10:35:55 - until epoch: 115, best_acc1: 64.168%
2022-03-03 10:35:55 - epoch 116 lr: 0.03999871531119779
2022-03-03 10:36:33 - train: epoch 0116, iter [00100, 05004], lr: 0.039999, loss: 1.5616
2022-03-03 10:37:06 - train: epoch 0116, iter [00200, 05004], lr: 0.039999, loss: 1.6944
2022-03-03 10:37:40 - train: epoch 0116, iter [00300, 05004], lr: 0.039999, loss: 1.4380
2022-03-03 10:38:13 - train: epoch 0116, iter [00400, 05004], lr: 0.039999, loss: 1.8065
2022-03-03 10:38:46 - train: epoch 0116, iter [00500, 05004], lr: 0.039999, loss: 1.8271
2022-03-03 10:39:18 - train: epoch 0116, iter [00600, 05004], lr: 0.039999, loss: 1.6477
2022-03-03 10:39:52 - train: epoch 0116, iter [00700, 05004], lr: 0.039999, loss: 1.8078
2022-03-03 10:40:25 - train: epoch 0116, iter [00800, 05004], lr: 0.039999, loss: 1.7703
2022-03-03 10:40:59 - train: epoch 0116, iter [00900, 05004], lr: 0.039999, loss: 1.8672
2022-03-03 10:41:33 - train: epoch 0116, iter [01000, 05004], lr: 0.039999, loss: 1.8841
2022-03-03 10:42:05 - train: epoch 0116, iter [01100, 05004], lr: 0.039999, loss: 1.4922
2022-03-03 10:42:38 - train: epoch 0116, iter [01200, 05004], lr: 0.039999, loss: 1.5969
2022-03-03 10:43:12 - train: epoch 0116, iter [01300, 05004], lr: 0.039999, loss: 1.6768
2022-03-03 10:43:45 - train: epoch 0116, iter [01400, 05004], lr: 0.039999, loss: 1.7594
2022-03-03 10:44:18 - train: epoch 0116, iter [01500, 05004], lr: 0.039999, loss: 1.5516
2022-03-03 10:44:51 - train: epoch 0116, iter [01600, 05004], lr: 0.039999, loss: 1.8310
2022-03-03 10:45:24 - train: epoch 0116, iter [01700, 05004], lr: 0.039999, loss: 1.6482
2022-03-03 10:45:57 - train: epoch 0116, iter [01800, 05004], lr: 0.039999, loss: 1.9024
2022-03-03 10:46:31 - train: epoch 0116, iter [01900, 05004], lr: 0.039999, loss: 1.5763
2022-03-03 10:47:04 - train: epoch 0116, iter [02000, 05004], lr: 0.039999, loss: 1.5462
2022-03-03 10:47:37 - train: epoch 0116, iter [02100, 05004], lr: 0.039999, loss: 1.7752
2022-03-03 10:48:10 - train: epoch 0116, iter [02200, 05004], lr: 0.039999, loss: 1.6949
2022-03-03 10:48:44 - train: epoch 0116, iter [02300, 05004], lr: 0.039999, loss: 1.7744
2022-03-03 10:49:16 - train: epoch 0116, iter [02400, 05004], lr: 0.039999, loss: 1.7814
2022-03-03 10:49:49 - train: epoch 0116, iter [02500, 05004], lr: 0.039999, loss: 1.4139
2022-03-03 10:50:22 - train: epoch 0116, iter [02600, 05004], lr: 0.039999, loss: 1.6672
2022-03-03 10:50:55 - train: epoch 0116, iter [02700, 05004], lr: 0.039999, loss: 1.8185
2022-03-03 10:51:28 - train: epoch 0116, iter [02800, 05004], lr: 0.039999, loss: 1.7596
2022-03-03 10:52:02 - train: epoch 0116, iter [02900, 05004], lr: 0.039999, loss: 1.9593
2022-03-03 10:52:34 - train: epoch 0116, iter [03000, 05004], lr: 0.039999, loss: 1.7907
2022-03-03 10:53:08 - train: epoch 0116, iter [03100, 05004], lr: 0.039999, loss: 1.8524
2022-03-03 10:53:40 - train: epoch 0116, iter [03200, 05004], lr: 0.039999, loss: 1.7810
2022-03-03 10:54:14 - train: epoch 0116, iter [03300, 05004], lr: 0.039999, loss: 1.9228
2022-03-03 10:54:47 - train: epoch 0116, iter [03400, 05004], lr: 0.039999, loss: 1.8487
2022-03-03 10:55:20 - train: epoch 0116, iter [03500, 05004], lr: 0.039999, loss: 1.8412
2022-03-03 10:55:53 - train: epoch 0116, iter [03600, 05004], lr: 0.039999, loss: 1.8171
2022-03-03 10:56:26 - train: epoch 0116, iter [03700, 05004], lr: 0.039999, loss: 1.6702
2022-03-03 10:56:59 - train: epoch 0116, iter [03800, 05004], lr: 0.039999, loss: 1.5704
2022-03-03 10:57:33 - train: epoch 0116, iter [03900, 05004], lr: 0.039999, loss: 1.7204
2022-03-03 10:58:06 - train: epoch 0116, iter [04000, 05004], lr: 0.039999, loss: 1.5929
2022-03-03 10:58:39 - train: epoch 0116, iter [04100, 05004], lr: 0.039999, loss: 1.8411
2022-03-03 10:59:12 - train: epoch 0116, iter [04200, 05004], lr: 0.039999, loss: 1.6622
2022-03-03 10:59:45 - train: epoch 0116, iter [04300, 05004], lr: 0.039999, loss: 1.7017
2022-03-03 11:00:19 - train: epoch 0116, iter [04400, 05004], lr: 0.039999, loss: 1.5390
2022-03-03 11:00:51 - train: epoch 0116, iter [04500, 05004], lr: 0.039999, loss: 1.6810
2022-03-03 11:01:24 - train: epoch 0116, iter [04600, 05004], lr: 0.039999, loss: 1.9351
2022-03-03 11:01:57 - train: epoch 0116, iter [04700, 05004], lr: 0.039999, loss: 1.7231
2022-03-03 11:02:30 - train: epoch 0116, iter [04800, 05004], lr: 0.039999, loss: 1.5517
2022-03-03 11:03:03 - train: epoch 0116, iter [04900, 05004], lr: 0.039999, loss: 1.4979
2022-03-03 11:03:35 - train: epoch 0116, iter [05000, 05004], lr: 0.039999, loss: 1.5290
2022-03-03 11:03:36 - train: epoch 116, train_loss: 1.6879
2022-03-03 11:04:50 - eval: epoch: 116, acc1: 63.198%, acc5: 85.828%, test_loss: 1.5068, per_image_load_time: 2.216ms, per_image_inference_time: 0.549ms
2022-03-03 11:04:50 - until epoch: 116, best_acc1: 64.168%
2022-03-03 11:04:50 - epoch 117 lr: 0.0392107901616097
2022-03-03 11:05:28 - train: epoch 0117, iter [00100, 05004], lr: 0.039211, loss: 1.9740
2022-03-03 11:06:02 - train: epoch 0117, iter [00200, 05004], lr: 0.039211, loss: 1.6694
2022-03-03 11:06:34 - train: epoch 0117, iter [00300, 05004], lr: 0.039211, loss: 1.8513
2022-03-03 11:07:08 - train: epoch 0117, iter [00400, 05004], lr: 0.039211, loss: 1.4834
2022-03-03 11:07:41 - train: epoch 0117, iter [00500, 05004], lr: 0.039211, loss: 1.5820
2022-03-03 11:08:14 - train: epoch 0117, iter [00600, 05004], lr: 0.039211, loss: 1.7114
2022-03-03 11:08:47 - train: epoch 0117, iter [00700, 05004], lr: 0.039211, loss: 1.7791
2022-03-03 11:09:20 - train: epoch 0117, iter [00800, 05004], lr: 0.039211, loss: 1.5960
2022-03-03 11:09:54 - train: epoch 0117, iter [00900, 05004], lr: 0.039211, loss: 1.3805
2022-03-03 11:10:27 - train: epoch 0117, iter [01000, 05004], lr: 0.039211, loss: 1.5248
2022-03-03 11:11:00 - train: epoch 0117, iter [01100, 05004], lr: 0.039211, loss: 1.5938
2022-03-03 11:11:33 - train: epoch 0117, iter [01200, 05004], lr: 0.039211, loss: 1.8005
2022-03-03 11:12:06 - train: epoch 0117, iter [01300, 05004], lr: 0.039211, loss: 1.6876
2022-03-03 11:12:40 - train: epoch 0117, iter [01400, 05004], lr: 0.039211, loss: 1.8419
2022-03-03 11:13:12 - train: epoch 0117, iter [01500, 05004], lr: 0.039211, loss: 1.6740
2022-03-03 11:13:46 - train: epoch 0117, iter [01600, 05004], lr: 0.039211, loss: 1.3683
2022-03-03 11:14:19 - train: epoch 0117, iter [01700, 05004], lr: 0.039211, loss: 1.6076
2022-03-03 11:14:52 - train: epoch 0117, iter [01800, 05004], lr: 0.039211, loss: 1.5803
2022-03-03 11:15:25 - train: epoch 0117, iter [01900, 05004], lr: 0.039211, loss: 1.6215
2022-03-03 11:15:57 - train: epoch 0117, iter [02000, 05004], lr: 0.039211, loss: 1.7062
2022-03-03 11:16:31 - train: epoch 0117, iter [02100, 05004], lr: 0.039211, loss: 1.6297
2022-03-03 11:17:04 - train: epoch 0117, iter [02200, 05004], lr: 0.039211, loss: 1.4436
2022-03-03 11:17:37 - train: epoch 0117, iter [02300, 05004], lr: 0.039211, loss: 1.6855
2022-03-03 11:18:11 - train: epoch 0117, iter [02400, 05004], lr: 0.039211, loss: 1.4737
2022-03-03 11:18:44 - train: epoch 0117, iter [02500, 05004], lr: 0.039211, loss: 1.7622
2022-03-03 11:19:18 - train: epoch 0117, iter [02600, 05004], lr: 0.039211, loss: 1.5650
2022-03-03 11:19:51 - train: epoch 0117, iter [02700, 05004], lr: 0.039211, loss: 1.5827
2022-03-03 11:20:24 - train: epoch 0117, iter [02800, 05004], lr: 0.039211, loss: 1.6170
2022-03-03 11:20:57 - train: epoch 0117, iter [02900, 05004], lr: 0.039211, loss: 1.6160
2022-03-03 11:21:30 - train: epoch 0117, iter [03000, 05004], lr: 0.039211, loss: 1.4300
2022-03-03 11:22:04 - train: epoch 0117, iter [03100, 05004], lr: 0.039211, loss: 1.6391
2022-03-03 11:22:36 - train: epoch 0117, iter [03200, 05004], lr: 0.039211, loss: 1.6601
2022-03-03 11:23:10 - train: epoch 0117, iter [03300, 05004], lr: 0.039211, loss: 1.6939
2022-03-03 11:23:43 - train: epoch 0117, iter [03400, 05004], lr: 0.039211, loss: 1.6308
2022-03-03 11:24:16 - train: epoch 0117, iter [03500, 05004], lr: 0.039211, loss: 1.8247
2022-03-03 11:24:50 - train: epoch 0117, iter [03600, 05004], lr: 0.039211, loss: 2.0713
2022-03-03 11:25:23 - train: epoch 0117, iter [03700, 05004], lr: 0.039211, loss: 1.6495
2022-03-03 11:25:56 - train: epoch 0117, iter [03800, 05004], lr: 0.039211, loss: 1.7842
2022-03-03 11:26:29 - train: epoch 0117, iter [03900, 05004], lr: 0.039211, loss: 1.5767
2022-03-03 11:27:02 - train: epoch 0117, iter [04000, 05004], lr: 0.039211, loss: 1.6078
2022-03-03 11:27:35 - train: epoch 0117, iter [04100, 05004], lr: 0.039211, loss: 1.8000
2022-03-03 11:28:08 - train: epoch 0117, iter [04200, 05004], lr: 0.039211, loss: 1.8307
2022-03-03 11:28:42 - train: epoch 0117, iter [04300, 05004], lr: 0.039211, loss: 1.5432
2022-03-03 11:29:15 - train: epoch 0117, iter [04400, 05004], lr: 0.039211, loss: 1.4898
2022-03-03 11:29:48 - train: epoch 0117, iter [04500, 05004], lr: 0.039211, loss: 1.6992
2022-03-03 11:30:21 - train: epoch 0117, iter [04600, 05004], lr: 0.039211, loss: 1.6207
2022-03-03 11:30:54 - train: epoch 0117, iter [04700, 05004], lr: 0.039211, loss: 1.7960
2022-03-03 11:31:27 - train: epoch 0117, iter [04800, 05004], lr: 0.039211, loss: 1.8119
2022-03-03 11:32:00 - train: epoch 0117, iter [04900, 05004], lr: 0.039211, loss: 1.9436
2022-03-03 11:32:32 - train: epoch 0117, iter [05000, 05004], lr: 0.039211, loss: 1.9067
2022-03-03 11:32:33 - train: epoch 117, train_loss: 1.6797
2022-03-03 11:33:46 - eval: epoch: 117, acc1: 64.590%, acc5: 86.824%, test_loss: 1.4426, per_image_load_time: 1.126ms, per_image_inference_time: 0.538ms
2022-03-03 11:33:47 - until epoch: 117, best_acc1: 64.590%
2022-03-03 11:33:47 - epoch 118 lr: 0.03842566535205286
2022-03-03 11:34:26 - train: epoch 0118, iter [00100, 05004], lr: 0.038426, loss: 1.6586
2022-03-03 11:34:59 - train: epoch 0118, iter [00200, 05004], lr: 0.038426, loss: 1.4615
2022-03-03 11:35:32 - train: epoch 0118, iter [00300, 05004], lr: 0.038426, loss: 1.7670
2022-03-03 11:36:05 - train: epoch 0118, iter [00400, 05004], lr: 0.038426, loss: 1.6663
2022-03-03 11:36:39 - train: epoch 0118, iter [00500, 05004], lr: 0.038426, loss: 1.5140
2022-03-03 11:37:12 - train: epoch 0118, iter [00600, 05004], lr: 0.038426, loss: 1.6882
2022-03-03 11:37:44 - train: epoch 0118, iter [00700, 05004], lr: 0.038426, loss: 1.7044
2022-03-03 11:38:18 - train: epoch 0118, iter [00800, 05004], lr: 0.038426, loss: 1.6259
2022-03-03 11:38:51 - train: epoch 0118, iter [00900, 05004], lr: 0.038426, loss: 1.6066
2022-03-03 11:39:24 - train: epoch 0118, iter [01000, 05004], lr: 0.038426, loss: 1.9321
2022-03-03 11:39:58 - train: epoch 0118, iter [01100, 05004], lr: 0.038426, loss: 1.7886
2022-03-03 11:40:31 - train: epoch 0118, iter [01200, 05004], lr: 0.038426, loss: 1.9156
2022-03-03 11:41:05 - train: epoch 0118, iter [01300, 05004], lr: 0.038426, loss: 1.8851
2022-03-03 11:41:38 - train: epoch 0118, iter [01400, 05004], lr: 0.038426, loss: 1.6592
2022-03-03 11:42:12 - train: epoch 0118, iter [01500, 05004], lr: 0.038426, loss: 1.5644
2022-03-03 11:42:44 - train: epoch 0118, iter [01600, 05004], lr: 0.038426, loss: 1.6764
2022-03-03 11:43:17 - train: epoch 0118, iter [01700, 05004], lr: 0.038426, loss: 1.7212
2022-03-03 11:43:50 - train: epoch 0118, iter [01800, 05004], lr: 0.038426, loss: 1.4821
2022-03-03 11:44:25 - train: epoch 0118, iter [01900, 05004], lr: 0.038426, loss: 1.7254
2022-03-03 11:44:57 - train: epoch 0118, iter [02000, 05004], lr: 0.038426, loss: 1.5320
2022-03-03 11:45:30 - train: epoch 0118, iter [02100, 05004], lr: 0.038426, loss: 1.6075
2022-03-03 11:46:03 - train: epoch 0118, iter [02200, 05004], lr: 0.038426, loss: 1.4980
2022-03-03 11:46:36 - train: epoch 0118, iter [02300, 05004], lr: 0.038426, loss: 1.6223
2022-03-03 11:47:09 - train: epoch 0118, iter [02400, 05004], lr: 0.038426, loss: 1.8400
2022-03-03 11:47:42 - train: epoch 0118, iter [02500, 05004], lr: 0.038426, loss: 1.9660
2022-03-03 11:48:15 - train: epoch 0118, iter [02600, 05004], lr: 0.038426, loss: 1.6146
2022-03-03 11:48:49 - train: epoch 0118, iter [02700, 05004], lr: 0.038426, loss: 1.8001
2022-03-03 11:49:22 - train: epoch 0118, iter [02800, 05004], lr: 0.038426, loss: 1.6612
2022-03-03 11:49:55 - train: epoch 0118, iter [02900, 05004], lr: 0.038426, loss: 1.7142
2022-03-03 11:50:29 - train: epoch 0118, iter [03000, 05004], lr: 0.038426, loss: 1.6549
2022-03-03 11:51:01 - train: epoch 0118, iter [03100, 05004], lr: 0.038426, loss: 1.8542
2022-03-03 11:51:34 - train: epoch 0118, iter [03200, 05004], lr: 0.038426, loss: 1.7424
2022-03-03 11:52:08 - train: epoch 0118, iter [03300, 05004], lr: 0.038426, loss: 1.6376
2022-03-03 11:52:42 - train: epoch 0118, iter [03400, 05004], lr: 0.038426, loss: 1.7013
2022-03-03 11:53:15 - train: epoch 0118, iter [03500, 05004], lr: 0.038426, loss: 1.6770
2022-03-03 11:53:48 - train: epoch 0118, iter [03600, 05004], lr: 0.038426, loss: 1.6635
2022-03-03 11:54:21 - train: epoch 0118, iter [03700, 05004], lr: 0.038426, loss: 1.6672
2022-03-03 11:54:54 - train: epoch 0118, iter [03800, 05004], lr: 0.038426, loss: 1.6747
2022-03-03 11:55:27 - train: epoch 0118, iter [03900, 05004], lr: 0.038426, loss: 1.8357
2022-03-03 11:56:00 - train: epoch 0118, iter [04000, 05004], lr: 0.038426, loss: 1.7184
2022-03-03 11:56:34 - train: epoch 0118, iter [04100, 05004], lr: 0.038426, loss: 1.7368
2022-03-03 11:57:06 - train: epoch 0118, iter [04200, 05004], lr: 0.038426, loss: 1.6381
2022-03-03 11:57:40 - train: epoch 0118, iter [04300, 05004], lr: 0.038426, loss: 1.4069
2022-03-03 11:58:13 - train: epoch 0118, iter [04400, 05004], lr: 0.038426, loss: 1.8080
2022-03-03 11:58:46 - train: epoch 0118, iter [04500, 05004], lr: 0.038426, loss: 1.6079
2022-03-03 11:59:20 - train: epoch 0118, iter [04600, 05004], lr: 0.038426, loss: 1.7741
2022-03-03 11:59:53 - train: epoch 0118, iter [04700, 05004], lr: 0.038426, loss: 1.6295
2022-03-03 12:00:26 - train: epoch 0118, iter [04800, 05004], lr: 0.038426, loss: 1.5025
2022-03-03 12:00:59 - train: epoch 0118, iter [04900, 05004], lr: 0.038426, loss: 1.7549
2022-03-03 12:01:31 - train: epoch 0118, iter [05000, 05004], lr: 0.038426, loss: 1.7026
2022-03-03 12:01:32 - train: epoch 118, train_loss: 1.6693
2022-03-03 12:02:45 - eval: epoch: 118, acc1: 62.898%, acc5: 85.228%, test_loss: 1.5386, per_image_load_time: 2.344ms, per_image_inference_time: 0.522ms
2022-03-03 12:02:46 - until epoch: 118, best_acc1: 64.590%
2022-03-03 12:02:46 - epoch 119 lr: 0.037643544661716516
2022-03-03 12:03:24 - train: epoch 0119, iter [00100, 05004], lr: 0.037644, loss: 1.9106
2022-03-03 12:03:57 - train: epoch 0119, iter [00200, 05004], lr: 0.037644, loss: 1.5206
2022-03-03 12:04:30 - train: epoch 0119, iter [00300, 05004], lr: 0.037644, loss: 1.7258
2022-03-03 12:05:03 - train: epoch 0119, iter [00400, 05004], lr: 0.037644, loss: 1.4984
2022-03-03 12:05:36 - train: epoch 0119, iter [00500, 05004], lr: 0.037644, loss: 1.5572
2022-03-03 12:06:10 - train: epoch 0119, iter [00600, 05004], lr: 0.037644, loss: 1.6804
2022-03-03 12:06:43 - train: epoch 0119, iter [00700, 05004], lr: 0.037644, loss: 1.6418
2022-03-03 12:07:16 - train: epoch 0119, iter [00800, 05004], lr: 0.037644, loss: 1.7783
2022-03-03 12:07:49 - train: epoch 0119, iter [00900, 05004], lr: 0.037644, loss: 1.7703
2022-03-03 12:08:23 - train: epoch 0119, iter [01000, 05004], lr: 0.037644, loss: 1.5537
2022-03-03 12:08:55 - train: epoch 0119, iter [01100, 05004], lr: 0.037644, loss: 1.8970
2022-03-03 12:09:28 - train: epoch 0119, iter [01200, 05004], lr: 0.037644, loss: 1.6848
2022-03-03 12:10:02 - train: epoch 0119, iter [01300, 05004], lr: 0.037644, loss: 1.6255
2022-03-03 12:10:35 - train: epoch 0119, iter [01400, 05004], lr: 0.037644, loss: 1.6968
2022-03-03 12:11:08 - train: epoch 0119, iter [01500, 05004], lr: 0.037644, loss: 1.6826
2022-03-03 12:11:41 - train: epoch 0119, iter [01600, 05004], lr: 0.037644, loss: 1.7717
2022-03-03 12:12:15 - train: epoch 0119, iter [01700, 05004], lr: 0.037644, loss: 1.6099
2022-03-03 12:12:48 - train: epoch 0119, iter [01800, 05004], lr: 0.037644, loss: 1.7274
2022-03-03 12:13:21 - train: epoch 0119, iter [01900, 05004], lr: 0.037644, loss: 1.6475
2022-03-03 12:13:55 - train: epoch 0119, iter [02000, 05004], lr: 0.037644, loss: 1.4551
2022-03-03 12:14:27 - train: epoch 0119, iter [02100, 05004], lr: 0.037644, loss: 1.8101
2022-03-03 12:15:01 - train: epoch 0119, iter [02200, 05004], lr: 0.037644, loss: 1.7398
2022-03-03 12:15:34 - train: epoch 0119, iter [02300, 05004], lr: 0.037644, loss: 1.5321
2022-03-03 12:16:07 - train: epoch 0119, iter [02400, 05004], lr: 0.037644, loss: 1.6164
2022-03-03 12:16:39 - train: epoch 0119, iter [02500, 05004], lr: 0.037644, loss: 1.8200
2022-03-03 12:17:14 - train: epoch 0119, iter [02600, 05004], lr: 0.037644, loss: 1.7879
2022-03-03 12:17:46 - train: epoch 0119, iter [02700, 05004], lr: 0.037644, loss: 1.8472
2022-03-03 12:18:20 - train: epoch 0119, iter [02800, 05004], lr: 0.037644, loss: 1.8321
2022-03-03 12:18:52 - train: epoch 0119, iter [02900, 05004], lr: 0.037644, loss: 1.6505
2022-03-03 12:19:25 - train: epoch 0119, iter [03000, 05004], lr: 0.037644, loss: 1.3306
2022-03-03 12:19:58 - train: epoch 0119, iter [03100, 05004], lr: 0.037644, loss: 1.6388
2022-03-03 12:20:31 - train: epoch 0119, iter [03200, 05004], lr: 0.037644, loss: 1.5280
2022-03-03 12:21:04 - train: epoch 0119, iter [03300, 05004], lr: 0.037644, loss: 1.6029
2022-03-03 12:21:37 - train: epoch 0119, iter [03400, 05004], lr: 0.037644, loss: 1.7121
2022-03-03 12:22:10 - train: epoch 0119, iter [03500, 05004], lr: 0.037644, loss: 1.5883
2022-03-03 12:22:43 - train: epoch 0119, iter [03600, 05004], lr: 0.037644, loss: 1.8844
2022-03-03 12:23:16 - train: epoch 0119, iter [03700, 05004], lr: 0.037644, loss: 1.7311
2022-03-03 12:23:49 - train: epoch 0119, iter [03800, 05004], lr: 0.037644, loss: 1.5749
2022-03-03 12:24:22 - train: epoch 0119, iter [03900, 05004], lr: 0.037644, loss: 1.9774
2022-03-03 12:24:55 - train: epoch 0119, iter [04000, 05004], lr: 0.037644, loss: 1.5942
2022-03-03 12:25:29 - train: epoch 0119, iter [04100, 05004], lr: 0.037644, loss: 2.0113
2022-03-03 12:26:02 - train: epoch 0119, iter [04200, 05004], lr: 0.037644, loss: 1.7114
2022-03-03 12:26:35 - train: epoch 0119, iter [04300, 05004], lr: 0.037644, loss: 1.6387
2022-03-03 12:27:08 - train: epoch 0119, iter [04400, 05004], lr: 0.037644, loss: 1.6325
2022-03-03 12:27:42 - train: epoch 0119, iter [04500, 05004], lr: 0.037644, loss: 1.6600
2022-03-03 12:28:15 - train: epoch 0119, iter [04600, 05004], lr: 0.037644, loss: 1.6499
2022-03-03 12:28:48 - train: epoch 0119, iter [04700, 05004], lr: 0.037644, loss: 1.6874
2022-03-03 12:29:21 - train: epoch 0119, iter [04800, 05004], lr: 0.037644, loss: 1.6509
2022-03-03 12:29:54 - train: epoch 0119, iter [04900, 05004], lr: 0.037644, loss: 1.9484
2022-03-03 12:30:26 - train: epoch 0119, iter [05000, 05004], lr: 0.037644, loss: 1.8371
2022-03-03 12:30:27 - train: epoch 119, train_loss: 1.6613
2022-03-03 12:31:39 - eval: epoch: 119, acc1: 63.840%, acc5: 86.194%, test_loss: 1.4834, per_image_load_time: 1.203ms, per_image_inference_time: 0.537ms
2022-03-03 12:31:40 - until epoch: 119, best_acc1: 64.590%
2022-03-03 12:31:40 - epoch 120 lr: 0.036864631090070654
2022-03-03 12:32:18 - train: epoch 0120, iter [00100, 05004], lr: 0.036865, loss: 1.6179
2022-03-03 12:32:51 - train: epoch 0120, iter [00200, 05004], lr: 0.036865, loss: 1.5927
2022-03-03 12:33:24 - train: epoch 0120, iter [00300, 05004], lr: 0.036865, loss: 1.6786
2022-03-03 12:33:57 - train: epoch 0120, iter [00400, 05004], lr: 0.036865, loss: 1.6726
2022-03-03 12:34:30 - train: epoch 0120, iter [00500, 05004], lr: 0.036865, loss: 1.4187
2022-03-03 12:35:03 - train: epoch 0120, iter [00600, 05004], lr: 0.036865, loss: 1.5447
2022-03-03 12:35:36 - train: epoch 0120, iter [00700, 05004], lr: 0.036865, loss: 1.8206
2022-03-03 12:36:09 - train: epoch 0120, iter [00800, 05004], lr: 0.036865, loss: 1.4773
2022-03-03 12:36:43 - train: epoch 0120, iter [00900, 05004], lr: 0.036865, loss: 1.5340
2022-03-03 12:37:15 - train: epoch 0120, iter [01000, 05004], lr: 0.036865, loss: 1.6141
2022-03-03 12:37:49 - train: epoch 0120, iter [01100, 05004], lr: 0.036865, loss: 1.5809
2022-03-03 12:38:22 - train: epoch 0120, iter [01200, 05004], lr: 0.036865, loss: 1.7586
2022-03-03 12:38:56 - train: epoch 0120, iter [01300, 05004], lr: 0.036865, loss: 1.5705
2022-03-03 12:39:29 - train: epoch 0120, iter [01400, 05004], lr: 0.036865, loss: 1.6607
2022-03-03 12:40:02 - train: epoch 0120, iter [01500, 05004], lr: 0.036865, loss: 1.6915
2022-03-03 12:40:35 - train: epoch 0120, iter [01600, 05004], lr: 0.036865, loss: 1.5051
2022-03-03 12:41:08 - train: epoch 0120, iter [01700, 05004], lr: 0.036865, loss: 1.5738
2022-03-03 12:41:42 - train: epoch 0120, iter [01800, 05004], lr: 0.036865, loss: 1.6634
2022-03-03 12:42:15 - train: epoch 0120, iter [01900, 05004], lr: 0.036865, loss: 1.8732
2022-03-03 12:42:48 - train: epoch 0120, iter [02000, 05004], lr: 0.036865, loss: 1.6349
2022-03-03 12:43:21 - train: epoch 0120, iter [02100, 05004], lr: 0.036865, loss: 1.8806
2022-03-03 12:43:54 - train: epoch 0120, iter [02200, 05004], lr: 0.036865, loss: 1.6721
2022-03-03 12:44:27 - train: epoch 0120, iter [02300, 05004], lr: 0.036865, loss: 1.5370
2022-03-03 12:45:01 - train: epoch 0120, iter [02400, 05004], lr: 0.036865, loss: 1.7935
2022-03-03 12:45:34 - train: epoch 0120, iter [02500, 05004], lr: 0.036865, loss: 1.8310
2022-03-03 12:46:07 - train: epoch 0120, iter [02600, 05004], lr: 0.036865, loss: 1.7500
2022-03-03 12:46:40 - train: epoch 0120, iter [02700, 05004], lr: 0.036865, loss: 1.8155
2022-03-03 12:47:14 - train: epoch 0120, iter [02800, 05004], lr: 0.036865, loss: 1.7033
2022-03-03 12:47:48 - train: epoch 0120, iter [02900, 05004], lr: 0.036865, loss: 1.4189
2022-03-03 12:48:20 - train: epoch 0120, iter [03000, 05004], lr: 0.036865, loss: 1.5424
2022-03-03 12:48:54 - train: epoch 0120, iter [03100, 05004], lr: 0.036865, loss: 1.4585
2022-03-03 12:49:27 - train: epoch 0120, iter [03200, 05004], lr: 0.036865, loss: 1.6632
2022-03-03 12:50:00 - train: epoch 0120, iter [03300, 05004], lr: 0.036865, loss: 1.5235
2022-03-03 12:50:33 - train: epoch 0120, iter [03400, 05004], lr: 0.036865, loss: 1.5964
2022-03-03 12:51:06 - train: epoch 0120, iter [03500, 05004], lr: 0.036865, loss: 1.7938
2022-03-03 12:51:39 - train: epoch 0120, iter [03600, 05004], lr: 0.036865, loss: 1.7607
2022-03-03 12:52:13 - train: epoch 0120, iter [03700, 05004], lr: 0.036865, loss: 1.4641
2022-03-03 12:52:46 - train: epoch 0120, iter [03800, 05004], lr: 0.036865, loss: 1.5702
2022-03-03 12:53:19 - train: epoch 0120, iter [03900, 05004], lr: 0.036865, loss: 1.5912
2022-03-03 12:53:52 - train: epoch 0120, iter [04000, 05004], lr: 0.036865, loss: 1.6217
2022-03-03 12:54:25 - train: epoch 0120, iter [04100, 05004], lr: 0.036865, loss: 1.8820
2022-03-03 12:54:59 - train: epoch 0120, iter [04200, 05004], lr: 0.036865, loss: 1.7413
2022-03-03 12:55:32 - train: epoch 0120, iter [04300, 05004], lr: 0.036865, loss: 1.8011
2022-03-03 12:56:06 - train: epoch 0120, iter [04400, 05004], lr: 0.036865, loss: 1.6209
2022-03-03 12:56:38 - train: epoch 0120, iter [04500, 05004], lr: 0.036865, loss: 1.7713
2022-03-03 12:57:11 - train: epoch 0120, iter [04600, 05004], lr: 0.036865, loss: 1.6305
2022-03-03 12:57:44 - train: epoch 0120, iter [04700, 05004], lr: 0.036865, loss: 1.6743
2022-03-03 12:58:17 - train: epoch 0120, iter [04800, 05004], lr: 0.036865, loss: 1.8520
2022-03-03 12:58:51 - train: epoch 0120, iter [04900, 05004], lr: 0.036865, loss: 1.8551
2022-03-03 12:59:23 - train: epoch 0120, iter [05000, 05004], lr: 0.036865, loss: 1.5715
2022-03-03 12:59:24 - train: epoch 120, train_loss: 1.6537
2022-03-03 13:00:37 - eval: epoch: 120, acc1: 64.110%, acc5: 86.428%, test_loss: 1.4686, per_image_load_time: 1.040ms, per_image_inference_time: 0.507ms
2022-03-03 13:00:38 - until epoch: 120, best_acc1: 64.590%
2022-03-03 13:00:38 - epoch 121 lr: 0.03608912680417737
2022-03-03 13:01:16 - train: epoch 0121, iter [00100, 05004], lr: 0.036089, loss: 1.5844
2022-03-03 13:01:49 - train: epoch 0121, iter [00200, 05004], lr: 0.036089, loss: 1.5820
2022-03-03 13:02:22 - train: epoch 0121, iter [00300, 05004], lr: 0.036089, loss: 1.7365
2022-03-03 13:02:56 - train: epoch 0121, iter [00400, 05004], lr: 0.036089, loss: 1.5564
2022-03-03 13:03:29 - train: epoch 0121, iter [00500, 05004], lr: 0.036089, loss: 1.7353
2022-03-03 13:04:01 - train: epoch 0121, iter [00600, 05004], lr: 0.036089, loss: 1.6815
2022-03-03 13:04:35 - train: epoch 0121, iter [00700, 05004], lr: 0.036089, loss: 1.4217
2022-03-03 13:05:08 - train: epoch 0121, iter [00800, 05004], lr: 0.036089, loss: 1.8920
2022-03-03 13:05:41 - train: epoch 0121, iter [00900, 05004], lr: 0.036089, loss: 1.5688
2022-03-03 13:06:15 - train: epoch 0121, iter [01000, 05004], lr: 0.036089, loss: 1.6157
2022-03-03 13:06:48 - train: epoch 0121, iter [01100, 05004], lr: 0.036089, loss: 1.3716
2022-03-03 13:07:21 - train: epoch 0121, iter [01200, 05004], lr: 0.036089, loss: 1.6602
2022-03-03 13:07:55 - train: epoch 0121, iter [01300, 05004], lr: 0.036089, loss: 1.6216
2022-03-03 13:08:28 - train: epoch 0121, iter [01400, 05004], lr: 0.036089, loss: 1.6656
2022-03-03 13:09:01 - train: epoch 0121, iter [01500, 05004], lr: 0.036089, loss: 1.5558
2022-03-03 13:09:34 - train: epoch 0121, iter [01600, 05004], lr: 0.036089, loss: 1.6745
2022-03-03 13:10:07 - train: epoch 0121, iter [01700, 05004], lr: 0.036089, loss: 1.5912
2022-03-03 13:10:42 - train: epoch 0121, iter [01800, 05004], lr: 0.036089, loss: 1.7773
2022-03-03 13:11:14 - train: epoch 0121, iter [01900, 05004], lr: 0.036089, loss: 1.5412
2022-03-03 13:11:48 - train: epoch 0121, iter [02000, 05004], lr: 0.036089, loss: 1.6499
2022-03-03 13:12:21 - train: epoch 0121, iter [02100, 05004], lr: 0.036089, loss: 1.7448
2022-03-03 13:12:54 - train: epoch 0121, iter [02200, 05004], lr: 0.036089, loss: 1.3320
2022-03-03 13:13:27 - train: epoch 0121, iter [02300, 05004], lr: 0.036089, loss: 1.4581
2022-03-03 13:14:00 - train: epoch 0121, iter [02400, 05004], lr: 0.036089, loss: 1.5215
2022-03-03 13:14:33 - train: epoch 0121, iter [02500, 05004], lr: 0.036089, loss: 1.4652
2022-03-03 13:15:07 - train: epoch 0121, iter [02600, 05004], lr: 0.036089, loss: 1.6801
2022-03-03 13:15:40 - train: epoch 0121, iter [02700, 05004], lr: 0.036089, loss: 1.6717
2022-03-03 13:16:13 - train: epoch 0121, iter [02800, 05004], lr: 0.036089, loss: 2.0027
2022-03-03 13:16:46 - train: epoch 0121, iter [02900, 05004], lr: 0.036089, loss: 1.7892
2022-03-03 13:17:19 - train: epoch 0121, iter [03000, 05004], lr: 0.036089, loss: 1.6542
2022-03-03 13:17:53 - train: epoch 0121, iter [03100, 05004], lr: 0.036089, loss: 1.4820
2022-03-03 13:18:26 - train: epoch 0121, iter [03200, 05004], lr: 0.036089, loss: 1.4389
2022-03-03 13:18:59 - train: epoch 0121, iter [03300, 05004], lr: 0.036089, loss: 1.7452
2022-03-03 13:19:32 - train: epoch 0121, iter [03400, 05004], lr: 0.036089, loss: 1.7096
2022-03-03 13:20:05 - train: epoch 0121, iter [03500, 05004], lr: 0.036089, loss: 1.6646
2022-03-03 13:20:38 - train: epoch 0121, iter [03600, 05004], lr: 0.036089, loss: 1.5739
2022-03-03 13:21:12 - train: epoch 0121, iter [03700, 05004], lr: 0.036089, loss: 1.6285
2022-03-03 13:21:44 - train: epoch 0121, iter [03800, 05004], lr: 0.036089, loss: 1.6386
2022-03-03 13:22:18 - train: epoch 0121, iter [03900, 05004], lr: 0.036089, loss: 1.4666
2022-03-03 13:22:51 - train: epoch 0121, iter [04000, 05004], lr: 0.036089, loss: 1.6586
2022-03-03 13:23:25 - train: epoch 0121, iter [04100, 05004], lr: 0.036089, loss: 1.6596
2022-03-03 13:23:57 - train: epoch 0121, iter [04200, 05004], lr: 0.036089, loss: 1.5805
2022-03-03 13:24:30 - train: epoch 0121, iter [04300, 05004], lr: 0.036089, loss: 1.3858
2022-03-03 13:25:03 - train: epoch 0121, iter [04400, 05004], lr: 0.036089, loss: 1.7771
2022-03-03 13:25:37 - train: epoch 0121, iter [04500, 05004], lr: 0.036089, loss: 1.6950
2022-03-03 13:26:10 - train: epoch 0121, iter [04600, 05004], lr: 0.036089, loss: 1.5099
2022-03-03 13:26:44 - train: epoch 0121, iter [04700, 05004], lr: 0.036089, loss: 1.8682
2022-03-03 13:27:17 - train: epoch 0121, iter [04800, 05004], lr: 0.036089, loss: 1.5740
2022-03-03 13:27:50 - train: epoch 0121, iter [04900, 05004], lr: 0.036089, loss: 1.5380
2022-03-03 13:28:22 - train: epoch 0121, iter [05000, 05004], lr: 0.036089, loss: 1.7970
2022-03-03 13:28:23 - train: epoch 121, train_loss: 1.6466
2022-03-03 13:29:37 - eval: epoch: 121, acc1: 65.018%, acc5: 86.806%, test_loss: 1.4312, per_image_load_time: 1.117ms, per_image_inference_time: 0.507ms
2022-03-03 13:29:38 - until epoch: 121, best_acc1: 65.018%
2022-03-03 13:29:38 - epoch 122 lr: 0.03531723308621847
2022-03-03 13:30:16 - train: epoch 0122, iter [00100, 05004], lr: 0.035317, loss: 1.6285
2022-03-03 13:30:50 - train: epoch 0122, iter [00200, 05004], lr: 0.035317, loss: 1.3452
2022-03-03 13:31:23 - train: epoch 0122, iter [00300, 05004], lr: 0.035317, loss: 1.5021
2022-03-03 13:31:56 - train: epoch 0122, iter [00400, 05004], lr: 0.035317, loss: 1.4939
2022-03-03 13:32:29 - train: epoch 0122, iter [00500, 05004], lr: 0.035317, loss: 1.5756
2022-03-03 13:33:02 - train: epoch 0122, iter [00600, 05004], lr: 0.035317, loss: 1.7340
2022-03-03 13:33:35 - train: epoch 0122, iter [00700, 05004], lr: 0.035317, loss: 1.3790
2022-03-03 13:34:09 - train: epoch 0122, iter [00800, 05004], lr: 0.035317, loss: 1.8873
2022-03-03 13:34:42 - train: epoch 0122, iter [00900, 05004], lr: 0.035317, loss: 1.5675
2022-03-03 13:35:16 - train: epoch 0122, iter [01000, 05004], lr: 0.035317, loss: 1.5728
2022-03-03 13:35:49 - train: epoch 0122, iter [01100, 05004], lr: 0.035317, loss: 1.3994
2022-03-03 13:36:22 - train: epoch 0122, iter [01200, 05004], lr: 0.035317, loss: 1.7796
2022-03-03 13:36:56 - train: epoch 0122, iter [01300, 05004], lr: 0.035317, loss: 1.5236
2022-03-03 13:37:29 - train: epoch 0122, iter [01400, 05004], lr: 0.035317, loss: 1.5065
2022-03-03 13:38:01 - train: epoch 0122, iter [01500, 05004], lr: 0.035317, loss: 1.9331
2022-03-03 13:38:34 - train: epoch 0122, iter [01600, 05004], lr: 0.035317, loss: 1.4789
2022-03-03 13:39:08 - train: epoch 0122, iter [01700, 05004], lr: 0.035317, loss: 1.6257
2022-03-03 13:39:41 - train: epoch 0122, iter [01800, 05004], lr: 0.035317, loss: 1.4238
2022-03-03 13:40:13 - train: epoch 0122, iter [01900, 05004], lr: 0.035317, loss: 1.6145
2022-03-03 13:40:46 - train: epoch 0122, iter [02000, 05004], lr: 0.035317, loss: 1.6973
2022-03-03 13:41:20 - train: epoch 0122, iter [02100, 05004], lr: 0.035317, loss: 1.5953
2022-03-03 13:41:53 - train: epoch 0122, iter [02200, 05004], lr: 0.035317, loss: 1.9279
2022-03-03 13:42:26 - train: epoch 0122, iter [02300, 05004], lr: 0.035317, loss: 1.7814
2022-03-03 13:43:00 - train: epoch 0122, iter [02400, 05004], lr: 0.035317, loss: 1.7593
2022-03-03 13:43:33 - train: epoch 0122, iter [02500, 05004], lr: 0.035317, loss: 1.5820
2022-03-03 13:44:06 - train: epoch 0122, iter [02600, 05004], lr: 0.035317, loss: 1.5694
2022-03-03 13:44:38 - train: epoch 0122, iter [02700, 05004], lr: 0.035317, loss: 1.5372
2022-03-03 13:45:11 - train: epoch 0122, iter [02800, 05004], lr: 0.035317, loss: 1.7648
2022-03-03 13:45:44 - train: epoch 0122, iter [02900, 05004], lr: 0.035317, loss: 1.4716
2022-03-03 13:46:18 - train: epoch 0122, iter [03000, 05004], lr: 0.035317, loss: 1.8051
2022-03-03 13:46:50 - train: epoch 0122, iter [03100, 05004], lr: 0.035317, loss: 1.5774
2022-03-03 13:47:23 - train: epoch 0122, iter [03200, 05004], lr: 0.035317, loss: 1.7147
2022-03-03 13:47:57 - train: epoch 0122, iter [03300, 05004], lr: 0.035317, loss: 1.5141
2022-03-03 13:48:29 - train: epoch 0122, iter [03400, 05004], lr: 0.035317, loss: 1.8362
2022-03-03 13:49:02 - train: epoch 0122, iter [03500, 05004], lr: 0.035317, loss: 1.6744
2022-03-03 13:49:35 - train: epoch 0122, iter [03600, 05004], lr: 0.035317, loss: 1.6718
2022-03-03 13:50:09 - train: epoch 0122, iter [03700, 05004], lr: 0.035317, loss: 1.5515
2022-03-03 13:50:41 - train: epoch 0122, iter [03800, 05004], lr: 0.035317, loss: 1.5375
2022-03-03 13:51:14 - train: epoch 0122, iter [03900, 05004], lr: 0.035317, loss: 1.6474
2022-03-03 13:51:48 - train: epoch 0122, iter [04000, 05004], lr: 0.035317, loss: 1.6137
2022-03-03 13:52:20 - train: epoch 0122, iter [04100, 05004], lr: 0.035317, loss: 1.4872
2022-03-03 13:52:53 - train: epoch 0122, iter [04200, 05004], lr: 0.035317, loss: 1.4301
2022-03-03 13:53:27 - train: epoch 0122, iter [04300, 05004], lr: 0.035317, loss: 1.4801
2022-03-03 13:54:00 - train: epoch 0122, iter [04400, 05004], lr: 0.035317, loss: 1.7817
2022-03-03 13:54:34 - train: epoch 0122, iter [04500, 05004], lr: 0.035317, loss: 1.4998
2022-03-03 13:55:06 - train: epoch 0122, iter [04600, 05004], lr: 0.035317, loss: 1.6906
2022-03-03 13:55:39 - train: epoch 0122, iter [04700, 05004], lr: 0.035317, loss: 1.6263
2022-03-03 13:56:13 - train: epoch 0122, iter [04800, 05004], lr: 0.035317, loss: 1.5453
2022-03-03 13:56:45 - train: epoch 0122, iter [04900, 05004], lr: 0.035317, loss: 1.5264
2022-03-03 13:57:17 - train: epoch 0122, iter [05000, 05004], lr: 0.035317, loss: 1.6601
2022-03-03 13:57:18 - train: epoch 122, train_loss: 1.6394
2022-03-03 13:58:31 - eval: epoch: 122, acc1: 64.882%, acc5: 86.566%, test_loss: 1.4408, per_image_load_time: 2.347ms, per_image_inference_time: 0.482ms
2022-03-03 13:58:32 - until epoch: 122, best_acc1: 65.018%
2022-03-03 13:58:32 - epoch 123 lr: 0.03454915028125263
2022-03-03 13:59:10 - train: epoch 0123, iter [00100, 05004], lr: 0.034549, loss: 1.6186
2022-03-03 13:59:43 - train: epoch 0123, iter [00200, 05004], lr: 0.034549, loss: 1.5769
2022-03-03 14:00:16 - train: epoch 0123, iter [00300, 05004], lr: 0.034549, loss: 1.5511
2022-03-03 14:00:49 - train: epoch 0123, iter [00400, 05004], lr: 0.034549, loss: 1.5098
2022-03-03 14:01:22 - train: epoch 0123, iter [00500, 05004], lr: 0.034549, loss: 1.4862
2022-03-03 14:01:55 - train: epoch 0123, iter [00600, 05004], lr: 0.034549, loss: 1.4715
2022-03-03 14:02:28 - train: epoch 0123, iter [00700, 05004], lr: 0.034549, loss: 1.5699
2022-03-03 14:03:02 - train: epoch 0123, iter [00800, 05004], lr: 0.034549, loss: 1.4372
2022-03-03 14:03:34 - train: epoch 0123, iter [00900, 05004], lr: 0.034549, loss: 1.5786
2022-03-03 14:04:08 - train: epoch 0123, iter [01000, 05004], lr: 0.034549, loss: 1.7898
2022-03-03 14:04:41 - train: epoch 0123, iter [01100, 05004], lr: 0.034549, loss: 1.5396
2022-03-03 14:05:13 - train: epoch 0123, iter [01200, 05004], lr: 0.034549, loss: 1.6535
2022-03-03 14:05:46 - train: epoch 0123, iter [01300, 05004], lr: 0.034549, loss: 1.5598
2022-03-03 14:06:19 - train: epoch 0123, iter [01400, 05004], lr: 0.034549, loss: 1.5507
2022-03-03 14:06:53 - train: epoch 0123, iter [01500, 05004], lr: 0.034549, loss: 1.5852
2022-03-03 14:07:26 - train: epoch 0123, iter [01600, 05004], lr: 0.034549, loss: 1.6695
2022-03-03 14:07:59 - train: epoch 0123, iter [01700, 05004], lr: 0.034549, loss: 1.6217
2022-03-03 14:08:32 - train: epoch 0123, iter [01800, 05004], lr: 0.034549, loss: 1.6727
2022-03-03 14:09:05 - train: epoch 0123, iter [01900, 05004], lr: 0.034549, loss: 1.7957
2022-03-03 14:09:38 - train: epoch 0123, iter [02000, 05004], lr: 0.034549, loss: 1.7315
2022-03-03 14:10:11 - train: epoch 0123, iter [02100, 05004], lr: 0.034549, loss: 1.9134
2022-03-03 14:10:44 - train: epoch 0123, iter [02200, 05004], lr: 0.034549, loss: 1.7066
2022-03-03 14:11:17 - train: epoch 0123, iter [02300, 05004], lr: 0.034549, loss: 1.7232
2022-03-03 14:11:51 - train: epoch 0123, iter [02400, 05004], lr: 0.034549, loss: 1.6745
2022-03-03 14:12:24 - train: epoch 0123, iter [02500, 05004], lr: 0.034549, loss: 1.6312
2022-03-03 14:12:57 - train: epoch 0123, iter [02600, 05004], lr: 0.034549, loss: 1.7452
2022-03-03 14:13:29 - train: epoch 0123, iter [02700, 05004], lr: 0.034549, loss: 1.7013
2022-03-03 14:14:03 - train: epoch 0123, iter [02800, 05004], lr: 0.034549, loss: 1.5721
2022-03-03 14:14:36 - train: epoch 0123, iter [02900, 05004], lr: 0.034549, loss: 1.7455
2022-03-03 14:15:09 - train: epoch 0123, iter [03000, 05004], lr: 0.034549, loss: 1.6505
2022-03-03 14:15:42 - train: epoch 0123, iter [03100, 05004], lr: 0.034549, loss: 1.6215
2022-03-03 14:16:16 - train: epoch 0123, iter [03200, 05004], lr: 0.034549, loss: 1.5694
2022-03-03 14:16:48 - train: epoch 0123, iter [03300, 05004], lr: 0.034549, loss: 1.5162
2022-03-03 14:17:22 - train: epoch 0123, iter [03400, 05004], lr: 0.034549, loss: 1.6149
2022-03-03 14:17:56 - train: epoch 0123, iter [03500, 05004], lr: 0.034549, loss: 1.5177
2022-03-03 14:18:28 - train: epoch 0123, iter [03600, 05004], lr: 0.034549, loss: 1.4508
2022-03-03 14:19:02 - train: epoch 0123, iter [03700, 05004], lr: 0.034549, loss: 1.5668
2022-03-03 14:19:34 - train: epoch 0123, iter [03800, 05004], lr: 0.034549, loss: 1.9295
2022-03-03 14:20:08 - train: epoch 0123, iter [03900, 05004], lr: 0.034549, loss: 1.4496
2022-03-03 14:20:42 - train: epoch 0123, iter [04000, 05004], lr: 0.034549, loss: 1.7014
2022-03-03 14:21:15 - train: epoch 0123, iter [04100, 05004], lr: 0.034549, loss: 1.7296
2022-03-03 14:21:48 - train: epoch 0123, iter [04200, 05004], lr: 0.034549, loss: 1.7519
2022-03-03 14:22:21 - train: epoch 0123, iter [04300, 05004], lr: 0.034549, loss: 1.7629
2022-03-03 14:22:54 - train: epoch 0123, iter [04400, 05004], lr: 0.034549, loss: 1.7852
2022-03-03 14:23:27 - train: epoch 0123, iter [04500, 05004], lr: 0.034549, loss: 1.5452
2022-03-03 14:24:01 - train: epoch 0123, iter [04600, 05004], lr: 0.034549, loss: 1.6269
2022-03-03 14:24:34 - train: epoch 0123, iter [04700, 05004], lr: 0.034549, loss: 1.5547
2022-03-03 14:25:07 - train: epoch 0123, iter [04800, 05004], lr: 0.034549, loss: 1.5981
2022-03-03 14:25:39 - train: epoch 0123, iter [04900, 05004], lr: 0.034549, loss: 1.6204
2022-03-03 14:26:11 - train: epoch 0123, iter [05000, 05004], lr: 0.034549, loss: 1.8426
2022-03-03 14:26:12 - train: epoch 123, train_loss: 1.6277
2022-03-03 14:27:25 - eval: epoch: 123, acc1: 64.728%, acc5: 86.858%, test_loss: 1.4434, per_image_load_time: 1.946ms, per_image_inference_time: 0.521ms
2022-03-03 14:27:26 - until epoch: 123, best_acc1: 65.018%
2022-03-03 14:27:26 - epoch 124 lr: 0.03378507774521587
2022-03-03 14:28:05 - train: epoch 0124, iter [00100, 05004], lr: 0.033785, loss: 1.3730
2022-03-03 14:28:37 - train: epoch 0124, iter [00200, 05004], lr: 0.033785, loss: 1.5885
2022-03-03 14:29:10 - train: epoch 0124, iter [00300, 05004], lr: 0.033785, loss: 1.4758
2022-03-03 14:29:44 - train: epoch 0124, iter [00400, 05004], lr: 0.033785, loss: 1.4100
2022-03-03 14:30:17 - train: epoch 0124, iter [00500, 05004], lr: 0.033785, loss: 1.5947
2022-03-03 14:30:50 - train: epoch 0124, iter [00600, 05004], lr: 0.033785, loss: 1.7448
2022-03-03 14:31:22 - train: epoch 0124, iter [00700, 05004], lr: 0.033785, loss: 1.6632
2022-03-03 14:31:56 - train: epoch 0124, iter [00800, 05004], lr: 0.033785, loss: 1.5944
2022-03-03 14:32:30 - train: epoch 0124, iter [00900, 05004], lr: 0.033785, loss: 1.8568
2022-03-03 14:33:04 - train: epoch 0124, iter [01000, 05004], lr: 0.033785, loss: 1.4950
2022-03-03 14:33:36 - train: epoch 0124, iter [01100, 05004], lr: 0.033785, loss: 1.7392
2022-03-03 14:34:10 - train: epoch 0124, iter [01200, 05004], lr: 0.033785, loss: 1.4674
2022-03-03 14:34:43 - train: epoch 0124, iter [01300, 05004], lr: 0.033785, loss: 1.4506
2022-03-03 14:35:17 - train: epoch 0124, iter [01400, 05004], lr: 0.033785, loss: 1.8403
2022-03-03 14:35:50 - train: epoch 0124, iter [01500, 05004], lr: 0.033785, loss: 1.6756
2022-03-03 14:36:23 - train: epoch 0124, iter [01600, 05004], lr: 0.033785, loss: 1.7590
2022-03-03 14:36:56 - train: epoch 0124, iter [01700, 05004], lr: 0.033785, loss: 1.4449
2022-03-03 14:37:29 - train: epoch 0124, iter [01800, 05004], lr: 0.033785, loss: 1.7108
2022-03-03 14:38:03 - train: epoch 0124, iter [01900, 05004], lr: 0.033785, loss: 1.8993
2022-03-03 14:38:35 - train: epoch 0124, iter [02000, 05004], lr: 0.033785, loss: 1.6682
2022-03-03 14:39:09 - train: epoch 0124, iter [02100, 05004], lr: 0.033785, loss: 1.6097
2022-03-03 14:39:42 - train: epoch 0124, iter [02200, 05004], lr: 0.033785, loss: 1.6850
2022-03-03 14:40:15 - train: epoch 0124, iter [02300, 05004], lr: 0.033785, loss: 1.7349
2022-03-03 14:40:48 - train: epoch 0124, iter [02400, 05004], lr: 0.033785, loss: 1.6948
2022-03-03 14:41:21 - train: epoch 0124, iter [02500, 05004], lr: 0.033785, loss: 1.5074
2022-03-03 14:41:54 - train: epoch 0124, iter [02600, 05004], lr: 0.033785, loss: 1.5156
2022-03-03 14:42:28 - train: epoch 0124, iter [02700, 05004], lr: 0.033785, loss: 1.6747
2022-03-03 14:43:01 - train: epoch 0124, iter [02800, 05004], lr: 0.033785, loss: 1.7730
2022-03-03 14:43:34 - train: epoch 0124, iter [02900, 05004], lr: 0.033785, loss: 1.5354
2022-03-03 14:44:08 - train: epoch 0124, iter [03000, 05004], lr: 0.033785, loss: 1.4807
2022-03-03 14:44:41 - train: epoch 0124, iter [03100, 05004], lr: 0.033785, loss: 1.5927
2022-03-03 14:45:14 - train: epoch 0124, iter [03200, 05004], lr: 0.033785, loss: 1.4862
2022-03-03 14:45:47 - train: epoch 0124, iter [03300, 05004], lr: 0.033785, loss: 1.3888
2022-03-03 14:46:20 - train: epoch 0124, iter [03400, 05004], lr: 0.033785, loss: 1.5949
2022-03-03 14:46:54 - train: epoch 0124, iter [03500, 05004], lr: 0.033785, loss: 1.6279
2022-03-03 14:47:27 - train: epoch 0124, iter [03600, 05004], lr: 0.033785, loss: 1.7263
2022-03-03 14:48:00 - train: epoch 0124, iter [03700, 05004], lr: 0.033785, loss: 1.6889
2022-03-03 14:48:32 - train: epoch 0124, iter [03800, 05004], lr: 0.033785, loss: 1.6714
2022-03-03 14:49:06 - train: epoch 0124, iter [03900, 05004], lr: 0.033785, loss: 1.5777
2022-03-03 14:49:39 - train: epoch 0124, iter [04000, 05004], lr: 0.033785, loss: 1.4551
2022-03-03 14:50:13 - train: epoch 0124, iter [04100, 05004], lr: 0.033785, loss: 1.7446
2022-03-03 14:50:45 - train: epoch 0124, iter [04200, 05004], lr: 0.033785, loss: 1.6718
2022-03-03 14:51:19 - train: epoch 0124, iter [04300, 05004], lr: 0.033785, loss: 1.6311
2022-03-03 14:51:52 - train: epoch 0124, iter [04400, 05004], lr: 0.033785, loss: 1.5272
2022-03-03 14:52:25 - train: epoch 0124, iter [04500, 05004], lr: 0.033785, loss: 1.6211
2022-03-03 14:52:58 - train: epoch 0124, iter [04600, 05004], lr: 0.033785, loss: 1.7167
2022-03-03 14:53:32 - train: epoch 0124, iter [04700, 05004], lr: 0.033785, loss: 1.4266
2022-03-03 14:54:05 - train: epoch 0124, iter [04800, 05004], lr: 0.033785, loss: 1.7887
2022-03-03 14:54:39 - train: epoch 0124, iter [04900, 05004], lr: 0.033785, loss: 1.3771
2022-03-03 14:55:11 - train: epoch 0124, iter [05000, 05004], lr: 0.033785, loss: 1.8150
2022-03-03 14:55:12 - train: epoch 124, train_loss: 1.6215
2022-03-03 14:56:26 - eval: epoch: 124, acc1: 65.294%, acc5: 87.308%, test_loss: 1.4119, per_image_load_time: 2.274ms, per_image_inference_time: 0.575ms
2022-03-03 14:56:27 - until epoch: 124, best_acc1: 65.294%
2022-03-03 14:56:27 - epoch 125 lr: 0.033025213793178645
2022-03-03 14:57:04 - train: epoch 0125, iter [00100, 05004], lr: 0.033025, loss: 1.4877
2022-03-03 14:57:37 - train: epoch 0125, iter [00200, 05004], lr: 0.033025, loss: 1.7283
2022-03-03 14:58:11 - train: epoch 0125, iter [00300, 05004], lr: 0.033025, loss: 1.5452
2022-03-03 14:58:43 - train: epoch 0125, iter [00400, 05004], lr: 0.033025, loss: 1.7000
2022-03-03 14:59:17 - train: epoch 0125, iter [00500, 05004], lr: 0.033025, loss: 1.6583
2022-03-03 14:59:49 - train: epoch 0125, iter [00600, 05004], lr: 0.033025, loss: 1.4566
2022-03-03 15:00:22 - train: epoch 0125, iter [00700, 05004], lr: 0.033025, loss: 1.5100
2022-03-03 15:00:55 - train: epoch 0125, iter [00800, 05004], lr: 0.033025, loss: 1.6130
2022-03-03 15:01:28 - train: epoch 0125, iter [00900, 05004], lr: 0.033025, loss: 1.6192
2022-03-03 15:02:01 - train: epoch 0125, iter [01000, 05004], lr: 0.033025, loss: 1.5856
2022-03-03 15:02:34 - train: epoch 0125, iter [01100, 05004], lr: 0.033025, loss: 1.6572
2022-03-03 15:03:08 - train: epoch 0125, iter [01200, 05004], lr: 0.033025, loss: 1.4457
2022-03-03 15:03:40 - train: epoch 0125, iter [01300, 05004], lr: 0.033025, loss: 1.5868
2022-03-03 15:04:14 - train: epoch 0125, iter [01400, 05004], lr: 0.033025, loss: 1.7855
2022-03-03 15:04:47 - train: epoch 0125, iter [01500, 05004], lr: 0.033025, loss: 1.7514
2022-03-03 15:05:20 - train: epoch 0125, iter [01600, 05004], lr: 0.033025, loss: 1.5213
2022-03-03 15:05:53 - train: epoch 0125, iter [01700, 05004], lr: 0.033025, loss: 1.5112
2022-03-03 15:06:26 - train: epoch 0125, iter [01800, 05004], lr: 0.033025, loss: 1.4634
2022-03-03 15:06:59 - train: epoch 0125, iter [01900, 05004], lr: 0.033025, loss: 1.5904
2022-03-03 15:07:33 - train: epoch 0125, iter [02000, 05004], lr: 0.033025, loss: 1.5538
2022-03-03 15:08:05 - train: epoch 0125, iter [02100, 05004], lr: 0.033025, loss: 1.6148
2022-03-03 15:08:38 - train: epoch 0125, iter [02200, 05004], lr: 0.033025, loss: 1.6485
2022-03-03 15:09:13 - train: epoch 0125, iter [02300, 05004], lr: 0.033025, loss: 1.4129
2022-03-03 15:09:46 - train: epoch 0125, iter [02400, 05004], lr: 0.033025, loss: 1.7323
2022-03-03 15:10:19 - train: epoch 0125, iter [02500, 05004], lr: 0.033025, loss: 1.5197
2022-03-03 15:10:52 - train: epoch 0125, iter [02600, 05004], lr: 0.033025, loss: 1.6055
2022-03-03 15:11:25 - train: epoch 0125, iter [02700, 05004], lr: 0.033025, loss: 1.6416
2022-03-03 15:11:59 - train: epoch 0125, iter [02800, 05004], lr: 0.033025, loss: 1.6990
2022-03-03 15:12:32 - train: epoch 0125, iter [02900, 05004], lr: 0.033025, loss: 1.5814
2022-03-03 15:13:05 - train: epoch 0125, iter [03000, 05004], lr: 0.033025, loss: 1.4433
2022-03-03 15:13:38 - train: epoch 0125, iter [03100, 05004], lr: 0.033025, loss: 1.7155
2022-03-03 15:14:12 - train: epoch 0125, iter [03200, 05004], lr: 0.033025, loss: 1.8154
2022-03-03 15:14:45 - train: epoch 0125, iter [03300, 05004], lr: 0.033025, loss: 1.6450
2022-03-03 15:15:18 - train: epoch 0125, iter [03400, 05004], lr: 0.033025, loss: 1.5099
2022-03-03 15:15:51 - train: epoch 0125, iter [03500, 05004], lr: 0.033025, loss: 1.6803
2022-03-03 15:16:24 - train: epoch 0125, iter [03600, 05004], lr: 0.033025, loss: 1.4253
2022-03-03 15:16:56 - train: epoch 0125, iter [03700, 05004], lr: 0.033025, loss: 1.6197
2022-03-03 15:17:30 - train: epoch 0125, iter [03800, 05004], lr: 0.033025, loss: 1.7118
2022-03-03 15:18:03 - train: epoch 0125, iter [03900, 05004], lr: 0.033025, loss: 1.4945
2022-03-03 15:18:37 - train: epoch 0125, iter [04000, 05004], lr: 0.033025, loss: 1.7442
2022-03-03 15:19:10 - train: epoch 0125, iter [04100, 05004], lr: 0.033025, loss: 1.5838
2022-03-03 15:19:43 - train: epoch 0125, iter [04200, 05004], lr: 0.033025, loss: 1.6687
2022-03-03 15:20:16 - train: epoch 0125, iter [04300, 05004], lr: 0.033025, loss: 1.4554
2022-03-03 15:20:48 - train: epoch 0125, iter [04400, 05004], lr: 0.033025, loss: 1.6117
2022-03-03 15:21:22 - train: epoch 0125, iter [04500, 05004], lr: 0.033025, loss: 1.8315
2022-03-03 15:21:55 - train: epoch 0125, iter [04600, 05004], lr: 0.033025, loss: 1.3245
2022-03-03 15:22:28 - train: epoch 0125, iter [04700, 05004], lr: 0.033025, loss: 1.7461
2022-03-03 15:23:01 - train: epoch 0125, iter [04800, 05004], lr: 0.033025, loss: 1.7194
2022-03-03 15:23:34 - train: epoch 0125, iter [04900, 05004], lr: 0.033025, loss: 1.7299
2022-03-03 15:24:06 - train: epoch 0125, iter [05000, 05004], lr: 0.033025, loss: 1.4801
2022-03-03 15:24:07 - train: epoch 125, train_loss: 1.6107
2022-03-03 15:25:21 - eval: epoch: 125, acc1: 65.150%, acc5: 86.924%, test_loss: 1.4179, per_image_load_time: 2.334ms, per_image_inference_time: 0.514ms
2022-03-03 15:25:22 - until epoch: 125, best_acc1: 65.294%
2022-03-03 15:25:22 - epoch 126 lr: 0.03226975564787322
2022-03-03 15:26:00 - train: epoch 0126, iter [00100, 05004], lr: 0.032270, loss: 1.5740
2022-03-03 15:26:34 - train: epoch 0126, iter [00200, 05004], lr: 0.032270, loss: 1.5979
2022-03-03 15:27:07 - train: epoch 0126, iter [00300, 05004], lr: 0.032270, loss: 1.3985
2022-03-03 15:27:40 - train: epoch 0126, iter [00400, 05004], lr: 0.032270, loss: 1.7306
2022-03-03 15:28:13 - train: epoch 0126, iter [00500, 05004], lr: 0.032270, loss: 1.3896
2022-03-03 15:28:47 - train: epoch 0126, iter [00600, 05004], lr: 0.032270, loss: 1.6874
2022-03-03 15:29:20 - train: epoch 0126, iter [00700, 05004], lr: 0.032270, loss: 1.6034
2022-03-03 15:29:53 - train: epoch 0126, iter [00800, 05004], lr: 0.032270, loss: 1.5384
2022-03-03 15:30:27 - train: epoch 0126, iter [00900, 05004], lr: 0.032270, loss: 1.5215
2022-03-03 15:31:00 - train: epoch 0126, iter [01000, 05004], lr: 0.032270, loss: 1.6307
2022-03-03 15:31:33 - train: epoch 0126, iter [01100, 05004], lr: 0.032270, loss: 1.6887
2022-03-03 15:32:07 - train: epoch 0126, iter [01200, 05004], lr: 0.032270, loss: 1.4425
2022-03-03 15:32:40 - train: epoch 0126, iter [01300, 05004], lr: 0.032270, loss: 1.8302
2022-03-03 15:33:14 - train: epoch 0126, iter [01400, 05004], lr: 0.032270, loss: 1.7532
2022-03-03 15:33:47 - train: epoch 0126, iter [01500, 05004], lr: 0.032270, loss: 1.5328
2022-03-03 15:34:19 - train: epoch 0126, iter [01600, 05004], lr: 0.032270, loss: 1.4189
2022-03-03 15:34:53 - train: epoch 0126, iter [01700, 05004], lr: 0.032270, loss: 1.6142
2022-03-03 15:35:26 - train: epoch 0126, iter [01800, 05004], lr: 0.032270, loss: 1.5560
2022-03-03 15:36:00 - train: epoch 0126, iter [01900, 05004], lr: 0.032270, loss: 1.4024
2022-03-03 15:36:33 - train: epoch 0126, iter [02000, 05004], lr: 0.032270, loss: 1.6623
2022-03-03 15:37:06 - train: epoch 0126, iter [02100, 05004], lr: 0.032270, loss: 1.5281
2022-03-03 15:37:40 - train: epoch 0126, iter [02200, 05004], lr: 0.032270, loss: 1.9188
2022-03-03 15:38:12 - train: epoch 0126, iter [02300, 05004], lr: 0.032270, loss: 1.7826
2022-03-03 15:38:47 - train: epoch 0126, iter [02400, 05004], lr: 0.032270, loss: 1.5905
2022-03-03 15:39:20 - train: epoch 0126, iter [02500, 05004], lr: 0.032270, loss: 1.9184
2022-03-03 15:39:53 - train: epoch 0126, iter [02600, 05004], lr: 0.032270, loss: 1.8264
2022-03-03 15:40:27 - train: epoch 0126, iter [02700, 05004], lr: 0.032270, loss: 1.5429
2022-03-03 15:41:00 - train: epoch 0126, iter [02800, 05004], lr: 0.032270, loss: 1.7895
2022-03-03 15:41:33 - train: epoch 0126, iter [02900, 05004], lr: 0.032270, loss: 1.6685
2022-03-03 15:42:07 - train: epoch 0126, iter [03000, 05004], lr: 0.032270, loss: 1.5163
2022-03-03 15:42:40 - train: epoch 0126, iter [03100, 05004], lr: 0.032270, loss: 1.4691
2022-03-03 15:43:13 - train: epoch 0126, iter [03200, 05004], lr: 0.032270, loss: 1.6279
2022-03-03 15:43:46 - train: epoch 0126, iter [03300, 05004], lr: 0.032270, loss: 1.5678
2022-03-03 15:44:20 - train: epoch 0126, iter [03400, 05004], lr: 0.032270, loss: 1.5698
2022-03-03 15:44:53 - train: epoch 0126, iter [03500, 05004], lr: 0.032270, loss: 1.6160
2022-03-03 15:45:27 - train: epoch 0126, iter [03600, 05004], lr: 0.032270, loss: 1.6738
2022-03-03 15:46:00 - train: epoch 0126, iter [03700, 05004], lr: 0.032270, loss: 1.5681
2022-03-03 15:46:34 - train: epoch 0126, iter [03800, 05004], lr: 0.032270, loss: 1.6119
2022-03-03 15:47:07 - train: epoch 0126, iter [03900, 05004], lr: 0.032270, loss: 1.5924
2022-03-03 15:47:40 - train: epoch 0126, iter [04000, 05004], lr: 0.032270, loss: 1.6514
2022-03-03 15:48:13 - train: epoch 0126, iter [04100, 05004], lr: 0.032270, loss: 1.6135
2022-03-03 15:48:47 - train: epoch 0126, iter [04200, 05004], lr: 0.032270, loss: 1.4918
2022-03-03 15:49:20 - train: epoch 0126, iter [04300, 05004], lr: 0.032270, loss: 1.6037
2022-03-03 15:49:53 - train: epoch 0126, iter [04400, 05004], lr: 0.032270, loss: 1.7051
2022-03-03 15:50:26 - train: epoch 0126, iter [04500, 05004], lr: 0.032270, loss: 1.5305
2022-03-03 15:51:00 - train: epoch 0126, iter [04600, 05004], lr: 0.032270, loss: 1.4822
2022-03-03 15:51:33 - train: epoch 0126, iter [04700, 05004], lr: 0.032270, loss: 1.6456
2022-03-03 15:52:06 - train: epoch 0126, iter [04800, 05004], lr: 0.032270, loss: 1.7090
2022-03-03 15:52:39 - train: epoch 0126, iter [04900, 05004], lr: 0.032270, loss: 1.7463
2022-03-03 15:53:11 - train: epoch 0126, iter [05000, 05004], lr: 0.032270, loss: 1.6485
2022-03-03 15:53:12 - train: epoch 126, train_loss: 1.6025
2022-03-03 15:54:26 - eval: epoch: 126, acc1: 66.512%, acc5: 87.848%, test_loss: 1.3511, per_image_load_time: 0.841ms, per_image_inference_time: 0.510ms
2022-03-03 15:54:27 - until epoch: 126, best_acc1: 66.512%
2022-03-03 15:54:27 - epoch 127 lr: 0.031518899388504454
2022-03-03 15:55:04 - train: epoch 0127, iter [00100, 05004], lr: 0.031519, loss: 1.6690
2022-03-03 15:55:37 - train: epoch 0127, iter [00200, 05004], lr: 0.031519, loss: 1.2641
2022-03-03 15:56:10 - train: epoch 0127, iter [00300, 05004], lr: 0.031519, loss: 1.4148
2022-03-03 15:56:43 - train: epoch 0127, iter [00400, 05004], lr: 0.031519, loss: 1.7513
2022-03-03 15:57:18 - train: epoch 0127, iter [00500, 05004], lr: 0.031519, loss: 1.6240
2022-03-03 15:57:50 - train: epoch 0127, iter [00600, 05004], lr: 0.031519, loss: 1.7810
2022-03-03 15:58:24 - train: epoch 0127, iter [00700, 05004], lr: 0.031519, loss: 1.6321
2022-03-03 15:58:57 - train: epoch 0127, iter [00800, 05004], lr: 0.031519, loss: 1.5332
2022-03-03 15:59:30 - train: epoch 0127, iter [00900, 05004], lr: 0.031519, loss: 1.4205
2022-03-03 16:00:04 - train: epoch 0127, iter [01000, 05004], lr: 0.031519, loss: 1.6573
2022-03-03 16:00:37 - train: epoch 0127, iter [01100, 05004], lr: 0.031519, loss: 1.4929
2022-03-03 16:01:10 - train: epoch 0127, iter [01200, 05004], lr: 0.031519, loss: 1.4819
2022-03-03 16:01:43 - train: epoch 0127, iter [01300, 05004], lr: 0.031519, loss: 1.5649
2022-03-03 16:02:17 - train: epoch 0127, iter [01400, 05004], lr: 0.031519, loss: 1.3953
2022-03-03 16:02:49 - train: epoch 0127, iter [01500, 05004], lr: 0.031519, loss: 1.5196
2022-03-03 16:03:22 - train: epoch 0127, iter [01600, 05004], lr: 0.031519, loss: 1.4707
2022-03-03 16:03:56 - train: epoch 0127, iter [01700, 05004], lr: 0.031519, loss: 1.5464
2022-03-03 16:04:29 - train: epoch 0127, iter [01800, 05004], lr: 0.031519, loss: 1.5662
2022-03-03 16:05:03 - train: epoch 0127, iter [01900, 05004], lr: 0.031519, loss: 1.5697
2022-03-03 16:05:35 - train: epoch 0127, iter [02000, 05004], lr: 0.031519, loss: 1.5044
2022-03-03 16:06:09 - train: epoch 0127, iter [02100, 05004], lr: 0.031519, loss: 1.7242
2022-03-03 16:06:43 - train: epoch 0127, iter [02200, 05004], lr: 0.031519, loss: 1.5339
2022-03-03 16:07:16 - train: epoch 0127, iter [02300, 05004], lr: 0.031519, loss: 1.4675
2022-03-03 16:07:48 - train: epoch 0127, iter [02400, 05004], lr: 0.031519, loss: 1.6148
2022-03-03 16:08:21 - train: epoch 0127, iter [02500, 05004], lr: 0.031519, loss: 1.4567
2022-03-03 16:08:55 - train: epoch 0127, iter [02600, 05004], lr: 0.031519, loss: 1.4142
2022-03-03 16:09:28 - train: epoch 0127, iter [02700, 05004], lr: 0.031519, loss: 1.4207
2022-03-03 16:10:02 - train: epoch 0127, iter [02800, 05004], lr: 0.031519, loss: 1.5418
2022-03-03 16:10:34 - train: epoch 0127, iter [02900, 05004], lr: 0.031519, loss: 1.5861
2022-03-03 16:11:07 - train: epoch 0127, iter [03000, 05004], lr: 0.031519, loss: 1.7569
2022-03-03 16:11:41 - train: epoch 0127, iter [03100, 05004], lr: 0.031519, loss: 1.6588
2022-03-03 16:12:14 - train: epoch 0127, iter [03200, 05004], lr: 0.031519, loss: 1.5883
2022-03-03 16:12:47 - train: epoch 0127, iter [03300, 05004], lr: 0.031519, loss: 1.7573
2022-03-03 16:13:20 - train: epoch 0127, iter [03400, 05004], lr: 0.031519, loss: 1.5763
2022-03-03 16:13:54 - train: epoch 0127, iter [03500, 05004], lr: 0.031519, loss: 1.7032
2022-03-03 16:14:27 - train: epoch 0127, iter [03600, 05004], lr: 0.031519, loss: 1.8240
2022-03-03 16:15:00 - train: epoch 0127, iter [03700, 05004], lr: 0.031519, loss: 1.7939
2022-03-03 16:15:32 - train: epoch 0127, iter [03800, 05004], lr: 0.031519, loss: 1.6542
2022-03-03 16:16:07 - train: epoch 0127, iter [03900, 05004], lr: 0.031519, loss: 1.4565
2022-03-03 16:16:39 - train: epoch 0127, iter [04000, 05004], lr: 0.031519, loss: 1.5781
2022-03-03 16:17:13 - train: epoch 0127, iter [04100, 05004], lr: 0.031519, loss: 1.5482
2022-03-03 16:17:46 - train: epoch 0127, iter [04200, 05004], lr: 0.031519, loss: 1.5107
2022-03-03 16:18:19 - train: epoch 0127, iter [04300, 05004], lr: 0.031519, loss: 1.6096
2022-03-03 16:18:53 - train: epoch 0127, iter [04400, 05004], lr: 0.031519, loss: 1.4177
2022-03-03 16:19:26 - train: epoch 0127, iter [04500, 05004], lr: 0.031519, loss: 1.6940
2022-03-03 16:19:58 - train: epoch 0127, iter [04600, 05004], lr: 0.031519, loss: 1.4924
2022-03-03 16:20:32 - train: epoch 0127, iter [04700, 05004], lr: 0.031519, loss: 1.4647
2022-03-03 16:21:05 - train: epoch 0127, iter [04800, 05004], lr: 0.031519, loss: 1.5671
2022-03-03 16:21:38 - train: epoch 0127, iter [04900, 05004], lr: 0.031519, loss: 1.5776
2022-03-03 16:22:10 - train: epoch 0127, iter [05000, 05004], lr: 0.031519, loss: 1.7640
2022-03-03 16:22:11 - train: epoch 127, train_loss: 1.5919
2022-03-03 16:23:25 - eval: epoch: 127, acc1: 65.834%, acc5: 87.606%, test_loss: 1.3901, per_image_load_time: 0.958ms, per_image_inference_time: 0.533ms
2022-03-03 16:23:26 - until epoch: 127, best_acc1: 66.512%
2022-03-03 16:23:26 - epoch 128 lr: 0.030772839899857463
2022-03-03 16:24:03 - train: epoch 0128, iter [00100, 05004], lr: 0.030773, loss: 1.6132
2022-03-03 16:24:38 - train: epoch 0128, iter [00200, 05004], lr: 0.030773, loss: 1.5546
2022-03-03 16:25:11 - train: epoch 0128, iter [00300, 05004], lr: 0.030773, loss: 1.7052
2022-03-03 16:25:43 - train: epoch 0128, iter [00400, 05004], lr: 0.030773, loss: 1.4004
2022-03-03 16:26:16 - train: epoch 0128, iter [00500, 05004], lr: 0.030773, loss: 1.6718
2022-03-03 16:26:49 - train: epoch 0128, iter [00600, 05004], lr: 0.030773, loss: 1.6619
2022-03-03 16:27:22 - train: epoch 0128, iter [00700, 05004], lr: 0.030773, loss: 1.5765
2022-03-03 16:27:55 - train: epoch 0128, iter [00800, 05004], lr: 0.030773, loss: 1.7606
2022-03-03 16:28:29 - train: epoch 0128, iter [00900, 05004], lr: 0.030773, loss: 1.5266
2022-03-03 16:29:01 - train: epoch 0128, iter [01000, 05004], lr: 0.030773, loss: 1.4090
2022-03-03 16:29:34 - train: epoch 0128, iter [01100, 05004], lr: 0.030773, loss: 1.5736
2022-03-03 16:30:08 - train: epoch 0128, iter [01200, 05004], lr: 0.030773, loss: 1.4949
2022-03-03 16:30:41 - train: epoch 0128, iter [01300, 05004], lr: 0.030773, loss: 1.4687
2022-03-03 16:31:14 - train: epoch 0128, iter [01400, 05004], lr: 0.030773, loss: 1.5385
2022-03-03 16:31:48 - train: epoch 0128, iter [01500, 05004], lr: 0.030773, loss: 1.7495
2022-03-03 16:32:20 - train: epoch 0128, iter [01600, 05004], lr: 0.030773, loss: 1.5156
2022-03-03 16:32:54 - train: epoch 0128, iter [01700, 05004], lr: 0.030773, loss: 1.4287
2022-03-03 16:33:27 - train: epoch 0128, iter [01800, 05004], lr: 0.030773, loss: 1.5824
2022-03-03 16:34:01 - train: epoch 0128, iter [01900, 05004], lr: 0.030773, loss: 1.5449
2022-03-03 16:34:33 - train: epoch 0128, iter [02000, 05004], lr: 0.030773, loss: 1.6110
2022-03-03 16:35:07 - train: epoch 0128, iter [02100, 05004], lr: 0.030773, loss: 1.8052
2022-03-03 16:35:40 - train: epoch 0128, iter [02200, 05004], lr: 0.030773, loss: 1.6768
2022-03-03 16:36:13 - train: epoch 0128, iter [02300, 05004], lr: 0.030773, loss: 1.5129
2022-03-03 16:36:46 - train: epoch 0128, iter [02400, 05004], lr: 0.030773, loss: 1.5897
2022-03-03 16:37:20 - train: epoch 0128, iter [02500, 05004], lr: 0.030773, loss: 1.6981
2022-03-03 16:37:53 - train: epoch 0128, iter [02600, 05004], lr: 0.030773, loss: 1.4451
2022-03-03 16:38:25 - train: epoch 0128, iter [02700, 05004], lr: 0.030773, loss: 1.3261
2022-03-03 16:38:59 - train: epoch 0128, iter [02800, 05004], lr: 0.030773, loss: 1.5667
2022-03-03 16:39:31 - train: epoch 0128, iter [02900, 05004], lr: 0.030773, loss: 1.7688
2022-03-03 16:40:05 - train: epoch 0128, iter [03000, 05004], lr: 0.030773, loss: 1.7907
2022-03-03 16:40:37 - train: epoch 0128, iter [03100, 05004], lr: 0.030773, loss: 1.5041
2022-03-03 16:41:10 - train: epoch 0128, iter [03200, 05004], lr: 0.030773, loss: 1.7156
2022-03-03 16:41:44 - train: epoch 0128, iter [03300, 05004], lr: 0.030773, loss: 1.6436
2022-03-03 16:42:17 - train: epoch 0128, iter [03400, 05004], lr: 0.030773, loss: 1.4862
2022-03-03 16:42:49 - train: epoch 0128, iter [03500, 05004], lr: 0.030773, loss: 1.6044
2022-03-03 16:43:22 - train: epoch 0128, iter [03600, 05004], lr: 0.030773, loss: 1.7307
2022-03-03 16:43:56 - train: epoch 0128, iter [03700, 05004], lr: 0.030773, loss: 1.4496
2022-03-03 16:44:29 - train: epoch 0128, iter [03800, 05004], lr: 0.030773, loss: 1.7022
2022-03-03 16:45:03 - train: epoch 0128, iter [03900, 05004], lr: 0.030773, loss: 1.6251
2022-03-03 16:45:35 - train: epoch 0128, iter [04000, 05004], lr: 0.030773, loss: 1.6667
2022-03-03 16:46:08 - train: epoch 0128, iter [04100, 05004], lr: 0.030773, loss: 1.6211
2022-03-03 16:46:42 - train: epoch 0128, iter [04200, 05004], lr: 0.030773, loss: 1.5382
2022-03-03 16:47:15 - train: epoch 0128, iter [04300, 05004], lr: 0.030773, loss: 1.7500
2022-03-03 16:47:48 - train: epoch 0128, iter [04400, 05004], lr: 0.030773, loss: 1.5566
2022-03-03 16:48:22 - train: epoch 0128, iter [04500, 05004], lr: 0.030773, loss: 1.7222
2022-03-03 16:48:55 - train: epoch 0128, iter [04600, 05004], lr: 0.030773, loss: 1.6047
2022-03-03 16:49:27 - train: epoch 0128, iter [04700, 05004], lr: 0.030773, loss: 1.6775
2022-03-03 16:50:01 - train: epoch 0128, iter [04800, 05004], lr: 0.030773, loss: 1.6399
2022-03-03 16:50:34 - train: epoch 0128, iter [04900, 05004], lr: 0.030773, loss: 1.6853
2022-03-03 16:51:06 - train: epoch 0128, iter [05000, 05004], lr: 0.030773, loss: 1.6547
2022-03-03 16:51:07 - train: epoch 128, train_loss: 1.5831
2022-03-03 16:52:21 - eval: epoch: 128, acc1: 65.892%, acc5: 87.396%, test_loss: 1.3957, per_image_load_time: 2.352ms, per_image_inference_time: 0.544ms
2022-03-03 16:52:22 - until epoch: 128, best_acc1: 66.512%
2022-03-03 16:52:22 - epoch 129 lr: 0.030031770821715233
2022-03-03 16:53:00 - train: epoch 0129, iter [00100, 05004], lr: 0.030032, loss: 1.4002
2022-03-03 16:53:32 - train: epoch 0129, iter [00200, 05004], lr: 0.030032, loss: 1.8337
2022-03-03 16:54:06 - train: epoch 0129, iter [00300, 05004], lr: 0.030032, loss: 1.4385
2022-03-03 16:54:39 - train: epoch 0129, iter [00400, 05004], lr: 0.030032, loss: 1.6790
2022-03-03 16:55:12 - train: epoch 0129, iter [00500, 05004], lr: 0.030032, loss: 1.4210
2022-03-03 16:55:46 - train: epoch 0129, iter [00600, 05004], lr: 0.030032, loss: 1.4506
2022-03-03 16:56:19 - train: epoch 0129, iter [00700, 05004], lr: 0.030032, loss: 1.5891
2022-03-03 16:56:52 - train: epoch 0129, iter [00800, 05004], lr: 0.030032, loss: 1.6953
2022-03-03 16:57:25 - train: epoch 0129, iter [00900, 05004], lr: 0.030032, loss: 1.5691
2022-03-03 16:57:59 - train: epoch 0129, iter [01000, 05004], lr: 0.030032, loss: 1.5783
2022-03-03 16:58:33 - train: epoch 0129, iter [01100, 05004], lr: 0.030032, loss: 1.5314
2022-03-03 16:59:06 - train: epoch 0129, iter [01200, 05004], lr: 0.030032, loss: 1.5233
2022-03-03 16:59:39 - train: epoch 0129, iter [01300, 05004], lr: 0.030032, loss: 1.5074
2022-03-03 17:00:13 - train: epoch 0129, iter [01400, 05004], lr: 0.030032, loss: 1.5117
2022-03-03 17:00:45 - train: epoch 0129, iter [01500, 05004], lr: 0.030032, loss: 1.3755
2022-03-03 17:01:18 - train: epoch 0129, iter [01600, 05004], lr: 0.030032, loss: 1.7001
2022-03-03 17:01:51 - train: epoch 0129, iter [01700, 05004], lr: 0.030032, loss: 1.6908
2022-03-03 17:02:24 - train: epoch 0129, iter [01800, 05004], lr: 0.030032, loss: 1.6547
2022-03-03 17:02:58 - train: epoch 0129, iter [01900, 05004], lr: 0.030032, loss: 1.5071
2022-03-03 17:03:30 - train: epoch 0129, iter [02000, 05004], lr: 0.030032, loss: 1.5369
2022-03-03 17:04:04 - train: epoch 0129, iter [02100, 05004], lr: 0.030032, loss: 1.5601
2022-03-03 17:04:37 - train: epoch 0129, iter [02200, 05004], lr: 0.030032, loss: 1.3186
2022-03-03 17:05:11 - train: epoch 0129, iter [02300, 05004], lr: 0.030032, loss: 1.6071
2022-03-03 17:05:44 - train: epoch 0129, iter [02400, 05004], lr: 0.030032, loss: 1.8385
2022-03-03 17:06:16 - train: epoch 0129, iter [02500, 05004], lr: 0.030032, loss: 1.6781
2022-03-03 17:06:50 - train: epoch 0129, iter [02600, 05004], lr: 0.030032, loss: 1.6308
2022-03-03 17:07:24 - train: epoch 0129, iter [02700, 05004], lr: 0.030032, loss: 1.4677
2022-03-03 17:07:56 - train: epoch 0129, iter [02800, 05004], lr: 0.030032, loss: 1.7298
2022-03-03 17:08:30 - train: epoch 0129, iter [02900, 05004], lr: 0.030032, loss: 1.5439
2022-03-03 17:09:03 - train: epoch 0129, iter [03000, 05004], lr: 0.030032, loss: 1.5107
2022-03-03 17:09:37 - train: epoch 0129, iter [03100, 05004], lr: 0.030032, loss: 1.6411
2022-03-03 17:10:09 - train: epoch 0129, iter [03200, 05004], lr: 0.030032, loss: 1.3834
2022-03-03 17:10:43 - train: epoch 0129, iter [03300, 05004], lr: 0.030032, loss: 1.5794
2022-03-03 17:11:16 - train: epoch 0129, iter [03400, 05004], lr: 0.030032, loss: 1.7471
2022-03-03 17:11:50 - train: epoch 0129, iter [03500, 05004], lr: 0.030032, loss: 1.3521
2022-03-03 17:12:23 - train: epoch 0129, iter [03600, 05004], lr: 0.030032, loss: 1.5871
2022-03-03 17:12:57 - train: epoch 0129, iter [03700, 05004], lr: 0.030032, loss: 1.4881
2022-03-03 17:13:30 - train: epoch 0129, iter [03800, 05004], lr: 0.030032, loss: 1.3890
2022-03-03 17:14:03 - train: epoch 0129, iter [03900, 05004], lr: 0.030032, loss: 1.5065
2022-03-03 17:14:36 - train: epoch 0129, iter [04000, 05004], lr: 0.030032, loss: 1.5775
2022-03-03 17:15:10 - train: epoch 0129, iter [04100, 05004], lr: 0.030032, loss: 1.6990
2022-03-03 17:15:43 - train: epoch 0129, iter [04200, 05004], lr: 0.030032, loss: 1.6435
2022-03-03 17:16:16 - train: epoch 0129, iter [04300, 05004], lr: 0.030032, loss: 1.6597
2022-03-03 17:16:49 - train: epoch 0129, iter [04400, 05004], lr: 0.030032, loss: 1.4644
2022-03-03 17:17:22 - train: epoch 0129, iter [04500, 05004], lr: 0.030032, loss: 1.5266
2022-03-03 17:17:55 - train: epoch 0129, iter [04600, 05004], lr: 0.030032, loss: 1.6215
2022-03-03 17:18:29 - train: epoch 0129, iter [04700, 05004], lr: 0.030032, loss: 1.6728
2022-03-03 17:19:02 - train: epoch 0129, iter [04800, 05004], lr: 0.030032, loss: 1.4336
2022-03-03 17:19:35 - train: epoch 0129, iter [04900, 05004], lr: 0.030032, loss: 1.5072
2022-03-03 17:20:06 - train: epoch 0129, iter [05000, 05004], lr: 0.030032, loss: 1.8013
2022-03-03 17:20:08 - train: epoch 129, train_loss: 1.5756
2022-03-03 17:21:21 - eval: epoch: 129, acc1: 66.314%, acc5: 87.562%, test_loss: 1.3745, per_image_load_time: 2.312ms, per_image_inference_time: 0.535ms
2022-03-03 17:21:22 - until epoch: 129, best_acc1: 66.512%
2022-03-03 17:21:22 - epoch 130 lr: 0.029295884498599413
2022-03-03 17:22:00 - train: epoch 0130, iter [00100, 05004], lr: 0.029296, loss: 1.4020
2022-03-03 17:22:33 - train: epoch 0130, iter [00200, 05004], lr: 0.029296, loss: 1.3218
2022-03-03 17:23:06 - train: epoch 0130, iter [00300, 05004], lr: 0.029296, loss: 1.5906
2022-03-03 17:23:38 - train: epoch 0130, iter [00400, 05004], lr: 0.029296, loss: 1.4695
2022-03-03 17:24:12 - train: epoch 0130, iter [00500, 05004], lr: 0.029296, loss: 1.5580
2022-03-03 17:24:44 - train: epoch 0130, iter [00600, 05004], lr: 0.029296, loss: 1.5597
2022-03-03 17:25:18 - train: epoch 0130, iter [00700, 05004], lr: 0.029296, loss: 1.5050
2022-03-03 17:25:51 - train: epoch 0130, iter [00800, 05004], lr: 0.029296, loss: 1.3479
2022-03-03 17:26:25 - train: epoch 0130, iter [00900, 05004], lr: 0.029296, loss: 1.6565
2022-03-03 17:26:57 - train: epoch 0130, iter [01000, 05004], lr: 0.029296, loss: 1.5011
2022-03-03 17:27:30 - train: epoch 0130, iter [01100, 05004], lr: 0.029296, loss: 1.5262
2022-03-03 17:28:03 - train: epoch 0130, iter [01200, 05004], lr: 0.029296, loss: 1.6568
2022-03-03 17:28:36 - train: epoch 0130, iter [01300, 05004], lr: 0.029296, loss: 1.6989
2022-03-03 17:29:09 - train: epoch 0130, iter [01400, 05004], lr: 0.029296, loss: 1.6277
2022-03-03 17:29:42 - train: epoch 0130, iter [01500, 05004], lr: 0.029296, loss: 1.4110
2022-03-03 17:30:15 - train: epoch 0130, iter [01600, 05004], lr: 0.029296, loss: 1.5455
2022-03-03 17:30:47 - train: epoch 0130, iter [01700, 05004], lr: 0.029296, loss: 1.3850
2022-03-03 17:31:21 - train: epoch 0130, iter [01800, 05004], lr: 0.029296, loss: 1.5644
2022-03-03 17:31:54 - train: epoch 0130, iter [01900, 05004], lr: 0.029296, loss: 1.5632
2022-03-03 17:32:27 - train: epoch 0130, iter [02000, 05004], lr: 0.029296, loss: 1.6743
2022-03-03 17:33:00 - train: epoch 0130, iter [02100, 05004], lr: 0.029296, loss: 1.8641
2022-03-03 17:33:33 - train: epoch 0130, iter [02200, 05004], lr: 0.029296, loss: 1.8158
2022-03-03 17:34:06 - train: epoch 0130, iter [02300, 05004], lr: 0.029296, loss: 1.5699
2022-03-03 17:34:39 - train: epoch 0130, iter [02400, 05004], lr: 0.029296, loss: 1.6075
2022-03-03 17:35:12 - train: epoch 0130, iter [02500, 05004], lr: 0.029296, loss: 1.5691
2022-03-03 17:35:45 - train: epoch 0130, iter [02600, 05004], lr: 0.029296, loss: 1.5350
2022-03-03 17:36:19 - train: epoch 0130, iter [02700, 05004], lr: 0.029296, loss: 1.7814
2022-03-03 17:36:52 - train: epoch 0130, iter [02800, 05004], lr: 0.029296, loss: 1.5528
2022-03-03 17:37:25 - train: epoch 0130, iter [02900, 05004], lr: 0.029296, loss: 1.7703
2022-03-03 17:37:58 - train: epoch 0130, iter [03000, 05004], lr: 0.029296, loss: 1.3905
2022-03-03 17:38:31 - train: epoch 0130, iter [03100, 05004], lr: 0.029296, loss: 1.6351
2022-03-03 17:39:04 - train: epoch 0130, iter [03200, 05004], lr: 0.029296, loss: 1.4125
2022-03-03 17:39:37 - train: epoch 0130, iter [03300, 05004], lr: 0.029296, loss: 1.6062
2022-03-03 17:40:10 - train: epoch 0130, iter [03400, 05004], lr: 0.029296, loss: 1.6666
2022-03-03 17:40:44 - train: epoch 0130, iter [03500, 05004], lr: 0.029296, loss: 1.4538
2022-03-03 17:41:16 - train: epoch 0130, iter [03600, 05004], lr: 0.029296, loss: 1.7110
2022-03-03 17:41:49 - train: epoch 0130, iter [03700, 05004], lr: 0.029296, loss: 1.6626
2022-03-03 17:42:22 - train: epoch 0130, iter [03800, 05004], lr: 0.029296, loss: 1.7024
2022-03-03 17:42:55 - train: epoch 0130, iter [03900, 05004], lr: 0.029296, loss: 1.5418
2022-03-03 17:43:28 - train: epoch 0130, iter [04000, 05004], lr: 0.029296, loss: 1.4326
2022-03-03 17:44:01 - train: epoch 0130, iter [04100, 05004], lr: 0.029296, loss: 1.7190
2022-03-03 17:44:34 - train: epoch 0130, iter [04200, 05004], lr: 0.029296, loss: 1.6897
2022-03-03 17:45:07 - train: epoch 0130, iter [04300, 05004], lr: 0.029296, loss: 1.5474
2022-03-03 17:45:39 - train: epoch 0130, iter [04400, 05004], lr: 0.029296, loss: 1.6310
2022-03-03 17:46:13 - train: epoch 0130, iter [04500, 05004], lr: 0.029296, loss: 1.5908
2022-03-03 17:46:46 - train: epoch 0130, iter [04600, 05004], lr: 0.029296, loss: 1.4700
2022-03-03 17:47:19 - train: epoch 0130, iter [04700, 05004], lr: 0.029296, loss: 1.7493
2022-03-03 17:47:52 - train: epoch 0130, iter [04800, 05004], lr: 0.029296, loss: 1.6547
2022-03-03 17:48:25 - train: epoch 0130, iter [04900, 05004], lr: 0.029296, loss: 1.3617
2022-03-03 17:48:57 - train: epoch 0130, iter [05000, 05004], lr: 0.029296, loss: 1.7687
2022-03-03 17:48:58 - train: epoch 130, train_loss: 1.5657
2022-03-03 17:50:11 - eval: epoch: 130, acc1: 66.994%, acc5: 88.214%, test_loss: 1.3386, per_image_load_time: 0.752ms, per_image_inference_time: 0.534ms
2022-03-03 17:50:12 - until epoch: 130, best_acc1: 66.994%
2022-03-03 17:50:12 - epoch 131 lr: 0.028565371929847285
2022-03-03 17:50:50 - train: epoch 0131, iter [00100, 05004], lr: 0.028565, loss: 1.3899
2022-03-03 17:51:22 - train: epoch 0131, iter [00200, 05004], lr: 0.028565, loss: 1.5298
2022-03-03 17:51:55 - train: epoch 0131, iter [00300, 05004], lr: 0.028565, loss: 1.6408
2022-03-03 17:52:28 - train: epoch 0131, iter [00400, 05004], lr: 0.028565, loss: 1.6883
2022-03-03 17:53:01 - train: epoch 0131, iter [00500, 05004], lr: 0.028565, loss: 1.5313
2022-03-03 17:53:34 - train: epoch 0131, iter [00600, 05004], lr: 0.028565, loss: 1.6056
2022-03-03 17:54:08 - train: epoch 0131, iter [00700, 05004], lr: 0.028565, loss: 1.7145
2022-03-03 17:54:41 - train: epoch 0131, iter [00800, 05004], lr: 0.028565, loss: 1.4182
2022-03-03 17:55:14 - train: epoch 0131, iter [00900, 05004], lr: 0.028565, loss: 1.6088
2022-03-03 17:55:46 - train: epoch 0131, iter [01000, 05004], lr: 0.028565, loss: 1.3429
2022-03-03 17:56:21 - train: epoch 0131, iter [01100, 05004], lr: 0.028565, loss: 1.4384
2022-03-03 17:56:53 - train: epoch 0131, iter [01200, 05004], lr: 0.028565, loss: 1.7135
2022-03-03 17:57:27 - train: epoch 0131, iter [01300, 05004], lr: 0.028565, loss: 1.6468
2022-03-03 17:57:59 - train: epoch 0131, iter [01400, 05004], lr: 0.028565, loss: 1.5075
2022-03-03 17:58:33 - train: epoch 0131, iter [01500, 05004], lr: 0.028565, loss: 1.5645
2022-03-03 17:59:06 - train: epoch 0131, iter [01600, 05004], lr: 0.028565, loss: 1.7501
2022-03-03 17:59:39 - train: epoch 0131, iter [01700, 05004], lr: 0.028565, loss: 1.7077
2022-03-03 18:00:11 - train: epoch 0131, iter [01800, 05004], lr: 0.028565, loss: 1.7748
2022-03-03 18:00:44 - train: epoch 0131, iter [01900, 05004], lr: 0.028565, loss: 1.4780
2022-03-03 18:01:18 - train: epoch 0131, iter [02000, 05004], lr: 0.028565, loss: 1.5375
2022-03-03 18:01:50 - train: epoch 0131, iter [02100, 05004], lr: 0.028565, loss: 1.7580
2022-03-03 18:02:23 - train: epoch 0131, iter [02200, 05004], lr: 0.028565, loss: 1.8205
2022-03-03 18:02:56 - train: epoch 0131, iter [02300, 05004], lr: 0.028565, loss: 1.6963
2022-03-03 18:03:30 - train: epoch 0131, iter [02400, 05004], lr: 0.028565, loss: 1.3952
2022-03-03 18:04:02 - train: epoch 0131, iter [02500, 05004], lr: 0.028565, loss: 1.5873
2022-03-03 18:04:35 - train: epoch 0131, iter [02600, 05004], lr: 0.028565, loss: 1.4654
2022-03-03 18:05:08 - train: epoch 0131, iter [02700, 05004], lr: 0.028565, loss: 1.6617
2022-03-03 18:05:40 - train: epoch 0131, iter [02800, 05004], lr: 0.028565, loss: 1.3950
2022-03-03 18:06:14 - train: epoch 0131, iter [02900, 05004], lr: 0.028565, loss: 1.7129
2022-03-03 18:06:46 - train: epoch 0131, iter [03000, 05004], lr: 0.028565, loss: 1.3783
2022-03-03 18:07:20 - train: epoch 0131, iter [03100, 05004], lr: 0.028565, loss: 1.5320
2022-03-03 18:07:53 - train: epoch 0131, iter [03200, 05004], lr: 0.028565, loss: 1.4616
2022-03-03 18:08:27 - train: epoch 0131, iter [03300, 05004], lr: 0.028565, loss: 1.7384
2022-03-03 18:08:59 - train: epoch 0131, iter [03400, 05004], lr: 0.028565, loss: 1.6197
2022-03-03 18:09:33 - train: epoch 0131, iter [03500, 05004], lr: 0.028565, loss: 1.5003
2022-03-03 18:10:05 - train: epoch 0131, iter [03600, 05004], lr: 0.028565, loss: 1.8026
2022-03-03 18:10:38 - train: epoch 0131, iter [03700, 05004], lr: 0.028565, loss: 1.5416
2022-03-03 18:11:12 - train: epoch 0131, iter [03800, 05004], lr: 0.028565, loss: 1.6202
2022-03-03 18:11:45 - train: epoch 0131, iter [03900, 05004], lr: 0.028565, loss: 1.4656
2022-03-03 18:12:18 - train: epoch 0131, iter [04000, 05004], lr: 0.028565, loss: 1.7213
2022-03-03 18:12:51 - train: epoch 0131, iter [04100, 05004], lr: 0.028565, loss: 1.3903
2022-03-03 18:13:24 - train: epoch 0131, iter [04200, 05004], lr: 0.028565, loss: 1.7007
2022-03-03 18:13:58 - train: epoch 0131, iter [04300, 05004], lr: 0.028565, loss: 1.6034
2022-03-03 18:14:31 - train: epoch 0131, iter [04400, 05004], lr: 0.028565, loss: 1.7988
2022-03-03 18:15:03 - train: epoch 0131, iter [04500, 05004], lr: 0.028565, loss: 1.4338
2022-03-03 18:15:36 - train: epoch 0131, iter [04600, 05004], lr: 0.028565, loss: 1.5584
2022-03-03 18:16:09 - train: epoch 0131, iter [04700, 05004], lr: 0.028565, loss: 1.5124
2022-03-03 18:16:43 - train: epoch 0131, iter [04800, 05004], lr: 0.028565, loss: 1.6026
2022-03-03 18:17:15 - train: epoch 0131, iter [04900, 05004], lr: 0.028565, loss: 1.6752
2022-03-03 18:17:47 - train: epoch 0131, iter [05000, 05004], lr: 0.028565, loss: 1.4737
2022-03-03 18:17:48 - train: epoch 131, train_loss: 1.5587
2022-03-03 18:19:02 - eval: epoch: 131, acc1: 66.766%, acc5: 88.018%, test_loss: 1.3500, per_image_load_time: 2.302ms, per_image_inference_time: 0.560ms
2022-03-03 18:19:02 - until epoch: 131, best_acc1: 66.994%
2022-03-03 18:19:02 - epoch 132 lr: 0.02784042272003794
2022-03-03 18:19:41 - train: epoch 0132, iter [00100, 05004], lr: 0.027840, loss: 1.6171
2022-03-03 18:20:13 - train: epoch 0132, iter [00200, 05004], lr: 0.027840, loss: 1.3212
2022-03-03 18:20:46 - train: epoch 0132, iter [00300, 05004], lr: 0.027840, loss: 1.6995
2022-03-03 18:21:20 - train: epoch 0132, iter [00400, 05004], lr: 0.027840, loss: 1.3984
2022-03-03 18:21:53 - train: epoch 0132, iter [00500, 05004], lr: 0.027840, loss: 1.4536
2022-03-03 18:22:26 - train: epoch 0132, iter [00600, 05004], lr: 0.027840, loss: 1.6715
2022-03-03 18:23:00 - train: epoch 0132, iter [00700, 05004], lr: 0.027840, loss: 1.3205
2022-03-03 18:23:32 - train: epoch 0132, iter [00800, 05004], lr: 0.027840, loss: 1.4829
2022-03-03 18:24:06 - train: epoch 0132, iter [00900, 05004], lr: 0.027840, loss: 1.5839
2022-03-03 18:24:40 - train: epoch 0132, iter [01000, 05004], lr: 0.027840, loss: 1.4827
2022-03-03 18:25:13 - train: epoch 0132, iter [01100, 05004], lr: 0.027840, loss: 1.6151
2022-03-03 18:25:46 - train: epoch 0132, iter [01200, 05004], lr: 0.027840, loss: 1.5877
2022-03-03 18:26:19 - train: epoch 0132, iter [01300, 05004], lr: 0.027840, loss: 1.3493
2022-03-03 18:26:53 - train: epoch 0132, iter [01400, 05004], lr: 0.027840, loss: 1.4475
2022-03-03 18:27:26 - train: epoch 0132, iter [01500, 05004], lr: 0.027840, loss: 1.7107
2022-03-03 18:28:00 - train: epoch 0132, iter [01600, 05004], lr: 0.027840, loss: 1.8124
2022-03-03 18:28:33 - train: epoch 0132, iter [01700, 05004], lr: 0.027840, loss: 1.5345
2022-03-03 18:29:06 - train: epoch 0132, iter [01800, 05004], lr: 0.027840, loss: 1.7524
2022-03-03 18:29:40 - train: epoch 0132, iter [01900, 05004], lr: 0.027840, loss: 1.3147
2022-03-03 18:30:14 - train: epoch 0132, iter [02000, 05004], lr: 0.027840, loss: 1.4379
2022-03-03 18:30:46 - train: epoch 0132, iter [02100, 05004], lr: 0.027840, loss: 1.5868
2022-03-03 18:31:19 - train: epoch 0132, iter [02200, 05004], lr: 0.027840, loss: 1.4871
2022-03-03 18:31:52 - train: epoch 0132, iter [02300, 05004], lr: 0.027840, loss: 1.7813
2022-03-03 18:32:26 - train: epoch 0132, iter [02400, 05004], lr: 0.027840, loss: 1.3177
2022-03-03 18:32:59 - train: epoch 0132, iter [02500, 05004], lr: 0.027840, loss: 1.6196
2022-03-03 18:33:32 - train: epoch 0132, iter [02600, 05004], lr: 0.027840, loss: 1.6313
2022-03-03 18:34:05 - train: epoch 0132, iter [02700, 05004], lr: 0.027840, loss: 1.7151
2022-03-03 18:34:39 - train: epoch 0132, iter [02800, 05004], lr: 0.027840, loss: 1.5860
2022-03-03 18:35:12 - train: epoch 0132, iter [02900, 05004], lr: 0.027840, loss: 1.3928
2022-03-03 18:35:45 - train: epoch 0132, iter [03000, 05004], lr: 0.027840, loss: 1.5264
2022-03-03 18:36:19 - train: epoch 0132, iter [03100, 05004], lr: 0.027840, loss: 1.5816
2022-03-03 18:36:51 - train: epoch 0132, iter [03200, 05004], lr: 0.027840, loss: 1.6023
2022-03-03 18:37:24 - train: epoch 0132, iter [03300, 05004], lr: 0.027840, loss: 1.6397
2022-03-03 18:37:57 - train: epoch 0132, iter [03400, 05004], lr: 0.027840, loss: 1.5641
2022-03-03 18:38:30 - train: epoch 0132, iter [03500, 05004], lr: 0.027840, loss: 1.4697
2022-03-03 18:39:03 - train: epoch 0132, iter [03600, 05004], lr: 0.027840, loss: 1.6572
2022-03-03 18:39:36 - train: epoch 0132, iter [03700, 05004], lr: 0.027840, loss: 1.5736
2022-03-03 18:40:09 - train: epoch 0132, iter [03800, 05004], lr: 0.027840, loss: 1.5983
2022-03-03 18:40:42 - train: epoch 0132, iter [03900, 05004], lr: 0.027840, loss: 1.5388
2022-03-03 18:41:16 - train: epoch 0132, iter [04000, 05004], lr: 0.027840, loss: 1.6341
2022-03-03 18:41:49 - train: epoch 0132, iter [04100, 05004], lr: 0.027840, loss: 1.6507
2022-03-03 18:42:22 - train: epoch 0132, iter [04200, 05004], lr: 0.027840, loss: 1.7209
2022-03-03 18:42:55 - train: epoch 0132, iter [04300, 05004], lr: 0.027840, loss: 1.5382
2022-03-03 18:43:28 - train: epoch 0132, iter [04400, 05004], lr: 0.027840, loss: 1.5279
2022-03-03 18:44:01 - train: epoch 0132, iter [04500, 05004], lr: 0.027840, loss: 1.5122
2022-03-03 18:44:34 - train: epoch 0132, iter [04600, 05004], lr: 0.027840, loss: 1.7074
2022-03-03 18:45:07 - train: epoch 0132, iter [04700, 05004], lr: 0.027840, loss: 1.7232
2022-03-03 18:45:40 - train: epoch 0132, iter [04800, 05004], lr: 0.027840, loss: 1.5734
2022-03-03 18:46:13 - train: epoch 0132, iter [04900, 05004], lr: 0.027840, loss: 1.7480
2022-03-03 18:46:45 - train: epoch 0132, iter [05000, 05004], lr: 0.027840, loss: 1.8042
2022-03-03 18:46:46 - train: epoch 132, train_loss: 1.5479
2022-03-03 18:48:00 - eval: epoch: 132, acc1: 66.770%, acc5: 88.210%, test_loss: 1.3381, per_image_load_time: 1.614ms, per_image_inference_time: 0.535ms
2022-03-03 18:48:00 - until epoch: 132, best_acc1: 66.994%
2022-03-03 18:48:00 - epoch 133 lr: 0.02712122502978024
2022-03-03 18:48:38 - train: epoch 0133, iter [00100, 05004], lr: 0.027121, loss: 1.1887
2022-03-03 18:49:11 - train: epoch 0133, iter [00200, 05004], lr: 0.027121, loss: 1.7055
2022-03-03 18:49:43 - train: epoch 0133, iter [00300, 05004], lr: 0.027121, loss: 1.4978
2022-03-03 18:50:16 - train: epoch 0133, iter [00400, 05004], lr: 0.027121, loss: 1.6222
2022-03-03 18:50:50 - train: epoch 0133, iter [00500, 05004], lr: 0.027121, loss: 1.5340
2022-03-03 18:51:23 - train: epoch 0133, iter [00600, 05004], lr: 0.027121, loss: 1.3781
2022-03-03 18:51:56 - train: epoch 0133, iter [00700, 05004], lr: 0.027121, loss: 1.4648
2022-03-03 18:52:29 - train: epoch 0133, iter [00800, 05004], lr: 0.027121, loss: 1.4797
2022-03-03 18:53:02 - train: epoch 0133, iter [00900, 05004], lr: 0.027121, loss: 1.5652
2022-03-03 18:53:35 - train: epoch 0133, iter [01000, 05004], lr: 0.027121, loss: 1.6273
2022-03-03 18:54:08 - train: epoch 0133, iter [01100, 05004], lr: 0.027121, loss: 1.5328
2022-03-03 18:54:41 - train: epoch 0133, iter [01200, 05004], lr: 0.027121, loss: 1.5540
2022-03-03 18:55:14 - train: epoch 0133, iter [01300, 05004], lr: 0.027121, loss: 1.4049
2022-03-03 18:55:48 - train: epoch 0133, iter [01400, 05004], lr: 0.027121, loss: 1.6047
2022-03-03 18:56:21 - train: epoch 0133, iter [01500, 05004], lr: 0.027121, loss: 1.7082
2022-03-03 18:56:55 - train: epoch 0133, iter [01600, 05004], lr: 0.027121, loss: 1.5870
2022-03-03 18:57:28 - train: epoch 0133, iter [01700, 05004], lr: 0.027121, loss: 1.8288
2022-03-03 18:58:00 - train: epoch 0133, iter [01800, 05004], lr: 0.027121, loss: 1.5291
2022-03-03 18:58:34 - train: epoch 0133, iter [01900, 05004], lr: 0.027121, loss: 1.4087
2022-03-03 18:59:07 - train: epoch 0133, iter [02000, 05004], lr: 0.027121, loss: 1.3942
2022-03-03 18:59:40 - train: epoch 0133, iter [02100, 05004], lr: 0.027121, loss: 1.8579
2022-03-03 19:00:13 - train: epoch 0133, iter [02200, 05004], lr: 0.027121, loss: 1.6055
2022-03-03 19:00:46 - train: epoch 0133, iter [02300, 05004], lr: 0.027121, loss: 1.4624
2022-03-03 19:01:19 - train: epoch 0133, iter [02400, 05004], lr: 0.027121, loss: 1.4240
2022-03-03 19:01:52 - train: epoch 0133, iter [02500, 05004], lr: 0.027121, loss: 1.5682
2022-03-03 19:02:25 - train: epoch 0133, iter [02600, 05004], lr: 0.027121, loss: 1.4195
2022-03-03 19:02:58 - train: epoch 0133, iter [02700, 05004], lr: 0.027121, loss: 1.7018
2022-03-03 19:03:31 - train: epoch 0133, iter [02800, 05004], lr: 0.027121, loss: 1.6498
2022-03-03 19:04:05 - train: epoch 0133, iter [02900, 05004], lr: 0.027121, loss: 1.2651
2022-03-03 19:04:38 - train: epoch 0133, iter [03000, 05004], lr: 0.027121, loss: 1.4512
2022-03-03 19:05:11 - train: epoch 0133, iter [03100, 05004], lr: 0.027121, loss: 1.3995
2022-03-03 19:05:44 - train: epoch 0133, iter [03200, 05004], lr: 0.027121, loss: 1.3715
2022-03-03 19:06:17 - train: epoch 0133, iter [03300, 05004], lr: 0.027121, loss: 1.3846
2022-03-03 19:06:50 - train: epoch 0133, iter [03400, 05004], lr: 0.027121, loss: 1.5146
2022-03-03 19:07:23 - train: epoch 0133, iter [03500, 05004], lr: 0.027121, loss: 1.6133
2022-03-03 19:07:57 - train: epoch 0133, iter [03600, 05004], lr: 0.027121, loss: 1.5681
2022-03-03 19:08:30 - train: epoch 0133, iter [03700, 05004], lr: 0.027121, loss: 1.7234
2022-03-03 19:09:03 - train: epoch 0133, iter [03800, 05004], lr: 0.027121, loss: 1.6336
2022-03-03 19:09:36 - train: epoch 0133, iter [03900, 05004], lr: 0.027121, loss: 1.5029
2022-03-03 19:10:09 - train: epoch 0133, iter [04000, 05004], lr: 0.027121, loss: 1.3475
2022-03-03 19:10:42 - train: epoch 0133, iter [04100, 05004], lr: 0.027121, loss: 1.5335
2022-03-03 19:11:15 - train: epoch 0133, iter [04200, 05004], lr: 0.027121, loss: 1.5557
2022-03-03 19:11:48 - train: epoch 0133, iter [04300, 05004], lr: 0.027121, loss: 1.7545
2022-03-03 19:12:22 - train: epoch 0133, iter [04400, 05004], lr: 0.027121, loss: 1.4630
2022-03-03 19:12:55 - train: epoch 0133, iter [04500, 05004], lr: 0.027121, loss: 1.6426
2022-03-03 19:13:28 - train: epoch 0133, iter [04600, 05004], lr: 0.027121, loss: 1.3757
2022-03-03 19:14:01 - train: epoch 0133, iter [04700, 05004], lr: 0.027121, loss: 1.4992
2022-03-03 19:14:34 - train: epoch 0133, iter [04800, 05004], lr: 0.027121, loss: 1.6765
2022-03-03 19:15:07 - train: epoch 0133, iter [04900, 05004], lr: 0.027121, loss: 1.5709
2022-03-03 19:15:38 - train: epoch 0133, iter [05000, 05004], lr: 0.027121, loss: 1.4317
2022-03-03 19:15:40 - train: epoch 133, train_loss: 1.5354
2022-03-03 19:16:53 - eval: epoch: 133, acc1: 66.320%, acc5: 87.570%, test_loss: 1.3741, per_image_load_time: 2.148ms, per_image_inference_time: 0.509ms
2022-03-03 19:16:53 - until epoch: 133, best_acc1: 66.994%
2022-03-03 19:16:53 - epoch 134 lr: 0.0264079655268759
2022-03-03 19:17:31 - train: epoch 0134, iter [00100, 05004], lr: 0.026408, loss: 1.2954
2022-03-03 19:18:05 - train: epoch 0134, iter [00200, 05004], lr: 0.026408, loss: 1.6273
2022-03-03 19:18:38 - train: epoch 0134, iter [00300, 05004], lr: 0.026408, loss: 1.6066
2022-03-03 19:19:11 - train: epoch 0134, iter [00400, 05004], lr: 0.026408, loss: 1.3519
2022-03-03 19:19:44 - train: epoch 0134, iter [00500, 05004], lr: 0.026408, loss: 1.4824
2022-03-03 19:20:17 - train: epoch 0134, iter [00600, 05004], lr: 0.026408, loss: 1.5796
2022-03-03 19:20:50 - train: epoch 0134, iter [00700, 05004], lr: 0.026408, loss: 1.5762
2022-03-03 19:21:24 - train: epoch 0134, iter [00800, 05004], lr: 0.026408, loss: 1.3951
2022-03-03 19:21:57 - train: epoch 0134, iter [00900, 05004], lr: 0.026408, loss: 1.5512
2022-03-03 19:22:30 - train: epoch 0134, iter [01000, 05004], lr: 0.026408, loss: 1.4478
2022-03-03 19:23:03 - train: epoch 0134, iter [01100, 05004], lr: 0.026408, loss: 1.4414
2022-03-03 19:23:37 - train: epoch 0134, iter [01200, 05004], lr: 0.026408, loss: 1.4998
2022-03-03 19:24:10 - train: epoch 0134, iter [01300, 05004], lr: 0.026408, loss: 1.6360
2022-03-03 19:24:43 - train: epoch 0134, iter [01400, 05004], lr: 0.026408, loss: 1.2972
2022-03-03 19:25:16 - train: epoch 0134, iter [01500, 05004], lr: 0.026408, loss: 1.3649
2022-03-03 19:25:49 - train: epoch 0134, iter [01600, 05004], lr: 0.026408, loss: 1.5614
2022-03-03 19:26:21 - train: epoch 0134, iter [01700, 05004], lr: 0.026408, loss: 1.3980
2022-03-03 19:26:56 - train: epoch 0134, iter [01800, 05004], lr: 0.026408, loss: 1.6072
2022-03-03 19:27:28 - train: epoch 0134, iter [01900, 05004], lr: 0.026408, loss: 1.5320
2022-03-03 19:28:01 - train: epoch 0134, iter [02000, 05004], lr: 0.026408, loss: 1.4956
2022-03-03 19:28:34 - train: epoch 0134, iter [02100, 05004], lr: 0.026408, loss: 1.6052
2022-03-03 19:29:08 - train: epoch 0134, iter [02200, 05004], lr: 0.026408, loss: 1.4121
2022-03-03 19:29:41 - train: epoch 0134, iter [02300, 05004], lr: 0.026408, loss: 1.4099
2022-03-03 19:30:14 - train: epoch 0134, iter [02400, 05004], lr: 0.026408, loss: 1.4417
2022-03-03 19:30:47 - train: epoch 0134, iter [02500, 05004], lr: 0.026408, loss: 1.5618
2022-03-03 19:31:21 - train: epoch 0134, iter [02600, 05004], lr: 0.026408, loss: 1.3590
2022-03-03 19:31:54 - train: epoch 0134, iter [02700, 05004], lr: 0.026408, loss: 1.4042
2022-03-03 19:32:27 - train: epoch 0134, iter [02800, 05004], lr: 0.026408, loss: 1.5679
2022-03-03 19:33:00 - train: epoch 0134, iter [02900, 05004], lr: 0.026408, loss: 1.4576
2022-03-03 19:33:32 - train: epoch 0134, iter [03000, 05004], lr: 0.026408, loss: 1.3045
2022-03-03 19:34:07 - train: epoch 0134, iter [03100, 05004], lr: 0.026408, loss: 1.5707
2022-03-03 19:34:39 - train: epoch 0134, iter [03200, 05004], lr: 0.026408, loss: 1.4735
2022-03-03 19:35:12 - train: epoch 0134, iter [03300, 05004], lr: 0.026408, loss: 1.4815
2022-03-03 19:35:46 - train: epoch 0134, iter [03400, 05004], lr: 0.026408, loss: 1.4643
2022-03-03 19:36:18 - train: epoch 0134, iter [03500, 05004], lr: 0.026408, loss: 1.3687
2022-03-03 19:36:51 - train: epoch 0134, iter [03600, 05004], lr: 0.026408, loss: 1.4805
2022-03-03 19:37:25 - train: epoch 0134, iter [03700, 05004], lr: 0.026408, loss: 1.5642
2022-03-03 19:37:57 - train: epoch 0134, iter [03800, 05004], lr: 0.026408, loss: 1.3994
2022-03-03 19:38:30 - train: epoch 0134, iter [03900, 05004], lr: 0.026408, loss: 1.5382
2022-03-03 19:39:03 - train: epoch 0134, iter [04000, 05004], lr: 0.026408, loss: 1.6891
2022-03-03 19:39:36 - train: epoch 0134, iter [04100, 05004], lr: 0.026408, loss: 1.6990
2022-03-03 19:40:09 - train: epoch 0134, iter [04200, 05004], lr: 0.026408, loss: 1.4568
2022-03-03 19:40:42 - train: epoch 0134, iter [04300, 05004], lr: 0.026408, loss: 1.6619
2022-03-03 19:41:15 - train: epoch 0134, iter [04400, 05004], lr: 0.026408, loss: 1.6027
2022-03-03 19:41:49 - train: epoch 0134, iter [04500, 05004], lr: 0.026408, loss: 1.6966
2022-03-03 19:42:21 - train: epoch 0134, iter [04600, 05004], lr: 0.026408, loss: 1.3018
2022-03-03 19:42:55 - train: epoch 0134, iter [04700, 05004], lr: 0.026408, loss: 1.4297
2022-03-03 19:43:28 - train: epoch 0134, iter [04800, 05004], lr: 0.026408, loss: 1.5372
2022-03-03 19:44:01 - train: epoch 0134, iter [04900, 05004], lr: 0.026408, loss: 1.4106
2022-03-03 19:44:32 - train: epoch 0134, iter [05000, 05004], lr: 0.026408, loss: 1.7118
2022-03-03 19:44:33 - train: epoch 134, train_loss: 1.5260
2022-03-03 19:45:47 - eval: epoch: 134, acc1: 67.280%, acc5: 88.036%, test_loss: 1.3363, per_image_load_time: 2.316ms, per_image_inference_time: 0.543ms
2022-03-03 19:45:48 - until epoch: 134, best_acc1: 67.280%
2022-03-03 19:45:48 - epoch 135 lr: 0.0257008293378697
2022-03-03 19:46:26 - train: epoch 0135, iter [00100, 05004], lr: 0.025701, loss: 1.3987
2022-03-03 19:46:59 - train: epoch 0135, iter [00200, 05004], lr: 0.025701, loss: 1.6193
2022-03-03 19:47:32 - train: epoch 0135, iter [00300, 05004], lr: 0.025701, loss: 1.4665
2022-03-03 19:48:05 - train: epoch 0135, iter [00400, 05004], lr: 0.025701, loss: 1.2955
2022-03-03 19:48:38 - train: epoch 0135, iter [00500, 05004], lr: 0.025701, loss: 1.5656
2022-03-03 19:49:11 - train: epoch 0135, iter [00600, 05004], lr: 0.025701, loss: 1.3388
2022-03-03 19:49:44 - train: epoch 0135, iter [00700, 05004], lr: 0.025701, loss: 1.8284
2022-03-03 19:50:17 - train: epoch 0135, iter [00800, 05004], lr: 0.025701, loss: 1.5946
2022-03-03 19:50:50 - train: epoch 0135, iter [00900, 05004], lr: 0.025701, loss: 1.5461
2022-03-03 19:51:24 - train: epoch 0135, iter [01000, 05004], lr: 0.025701, loss: 1.5825
2022-03-03 19:51:57 - train: epoch 0135, iter [01100, 05004], lr: 0.025701, loss: 1.3728
2022-03-03 19:52:31 - train: epoch 0135, iter [01200, 05004], lr: 0.025701, loss: 1.3849
2022-03-03 19:53:04 - train: epoch 0135, iter [01300, 05004], lr: 0.025701, loss: 1.5440
2022-03-03 19:53:37 - train: epoch 0135, iter [01400, 05004], lr: 0.025701, loss: 1.5320
2022-03-03 19:54:10 - train: epoch 0135, iter [01500, 05004], lr: 0.025701, loss: 1.4879
2022-03-03 19:54:42 - train: epoch 0135, iter [01600, 05004], lr: 0.025701, loss: 1.4927
2022-03-03 19:55:17 - train: epoch 0135, iter [01700, 05004], lr: 0.025701, loss: 1.8221
2022-03-03 19:55:49 - train: epoch 0135, iter [01800, 05004], lr: 0.025701, loss: 1.5957
2022-03-03 19:56:22 - train: epoch 0135, iter [01900, 05004], lr: 0.025701, loss: 1.4415
2022-03-03 19:56:56 - train: epoch 0135, iter [02000, 05004], lr: 0.025701, loss: 1.6193
2022-03-03 19:57:28 - train: epoch 0135, iter [02100, 05004], lr: 0.025701, loss: 1.6767
2022-03-03 19:58:02 - train: epoch 0135, iter [02200, 05004], lr: 0.025701, loss: 1.5472
2022-03-03 19:58:35 - train: epoch 0135, iter [02300, 05004], lr: 0.025701, loss: 1.4986
2022-03-03 19:59:08 - train: epoch 0135, iter [02400, 05004], lr: 0.025701, loss: 1.4313
2022-03-03 19:59:41 - train: epoch 0135, iter [02500, 05004], lr: 0.025701, loss: 1.6208
2022-03-03 20:00:14 - train: epoch 0135, iter [02600, 05004], lr: 0.025701, loss: 1.4785
2022-03-03 20:00:47 - train: epoch 0135, iter [02700, 05004], lr: 0.025701, loss: 1.7160
2022-03-03 20:01:20 - train: epoch 0135, iter [02800, 05004], lr: 0.025701, loss: 1.4920
2022-03-03 20:01:53 - train: epoch 0135, iter [02900, 05004], lr: 0.025701, loss: 1.5727
2022-03-03 20:02:27 - train: epoch 0135, iter [03000, 05004], lr: 0.025701, loss: 1.4380
2022-03-03 20:03:00 - train: epoch 0135, iter [03100, 05004], lr: 0.025701, loss: 1.6435
2022-03-03 20:03:32 - train: epoch 0135, iter [03200, 05004], lr: 0.025701, loss: 1.5399
2022-03-03 20:04:05 - train: epoch 0135, iter [03300, 05004], lr: 0.025701, loss: 1.6821
2022-03-03 20:04:39 - train: epoch 0135, iter [03400, 05004], lr: 0.025701, loss: 1.7401
2022-03-03 20:05:12 - train: epoch 0135, iter [03500, 05004], lr: 0.025701, loss: 1.6794
2022-03-03 20:05:45 - train: epoch 0135, iter [03600, 05004], lr: 0.025701, loss: 1.4955
2022-03-03 20:06:18 - train: epoch 0135, iter [03700, 05004], lr: 0.025701, loss: 1.6065
2022-03-03 20:06:51 - train: epoch 0135, iter [03800, 05004], lr: 0.025701, loss: 1.4214
2022-03-03 20:07:24 - train: epoch 0135, iter [03900, 05004], lr: 0.025701, loss: 1.5812
2022-03-03 20:07:57 - train: epoch 0135, iter [04000, 05004], lr: 0.025701, loss: 1.6782
2022-03-03 20:08:30 - train: epoch 0135, iter [04100, 05004], lr: 0.025701, loss: 1.7400
2022-03-03 20:09:03 - train: epoch 0135, iter [04200, 05004], lr: 0.025701, loss: 1.7358
2022-03-03 20:09:36 - train: epoch 0135, iter [04300, 05004], lr: 0.025701, loss: 1.7478
2022-03-03 20:10:10 - train: epoch 0135, iter [04400, 05004], lr: 0.025701, loss: 1.5016
2022-03-03 20:10:43 - train: epoch 0135, iter [04500, 05004], lr: 0.025701, loss: 1.3636
2022-03-03 20:11:16 - train: epoch 0135, iter [04600, 05004], lr: 0.025701, loss: 1.6106
2022-03-03 20:11:49 - train: epoch 0135, iter [04700, 05004], lr: 0.025701, loss: 1.3244
2022-03-03 20:12:22 - train: epoch 0135, iter [04800, 05004], lr: 0.025701, loss: 1.4233
2022-03-03 20:12:56 - train: epoch 0135, iter [04900, 05004], lr: 0.025701, loss: 1.5263
2022-03-03 20:13:28 - train: epoch 0135, iter [05000, 05004], lr: 0.025701, loss: 1.3864
2022-03-03 20:13:29 - train: epoch 135, train_loss: 1.5165
2022-03-03 20:14:42 - eval: epoch: 135, acc1: 65.812%, acc5: 87.314%, test_loss: 1.3980, per_image_load_time: 2.245ms, per_image_inference_time: 0.533ms
2022-03-03 20:14:43 - until epoch: 135, best_acc1: 67.280%
2022-03-03 20:14:43 - epoch 136 lr: 0.025000000000000012
2022-03-03 20:15:21 - train: epoch 0136, iter [00100, 05004], lr: 0.025000, loss: 1.3562
2022-03-03 20:15:54 - train: epoch 0136, iter [00200, 05004], lr: 0.025000, loss: 1.5286
2022-03-03 20:16:27 - train: epoch 0136, iter [00300, 05004], lr: 0.025000, loss: 1.5813
2022-03-03 20:16:59 - train: epoch 0136, iter [00400, 05004], lr: 0.025000, loss: 1.2639
2022-03-03 20:17:33 - train: epoch 0136, iter [00500, 05004], lr: 0.025000, loss: 1.3934
2022-03-03 20:18:06 - train: epoch 0136, iter [00600, 05004], lr: 0.025000, loss: 1.2246
2022-03-03 20:18:39 - train: epoch 0136, iter [00700, 05004], lr: 0.025000, loss: 1.7261
2022-03-03 20:19:11 - train: epoch 0136, iter [00800, 05004], lr: 0.025000, loss: 1.6499
2022-03-03 20:19:44 - train: epoch 0136, iter [00900, 05004], lr: 0.025000, loss: 1.6358
2022-03-03 20:20:17 - train: epoch 0136, iter [01000, 05004], lr: 0.025000, loss: 1.4765
2022-03-03 20:20:50 - train: epoch 0136, iter [01100, 05004], lr: 0.025000, loss: 1.3174
2022-03-03 20:21:23 - train: epoch 0136, iter [01200, 05004], lr: 0.025000, loss: 1.5705
2022-03-03 20:21:57 - train: epoch 0136, iter [01300, 05004], lr: 0.025000, loss: 1.4948
2022-03-03 20:22:30 - train: epoch 0136, iter [01400, 05004], lr: 0.025000, loss: 1.5101
2022-03-03 20:23:03 - train: epoch 0136, iter [01500, 05004], lr: 0.025000, loss: 1.6411
2022-03-03 20:23:36 - train: epoch 0136, iter [01600, 05004], lr: 0.025000, loss: 1.4517
2022-03-03 20:24:09 - train: epoch 0136, iter [01700, 05004], lr: 0.025000, loss: 1.6164
2022-03-03 20:24:42 - train: epoch 0136, iter [01800, 05004], lr: 0.025000, loss: 1.4635
2022-03-03 20:25:15 - train: epoch 0136, iter [01900, 05004], lr: 0.025000, loss: 1.3510
2022-03-03 20:25:48 - train: epoch 0136, iter [02000, 05004], lr: 0.025000, loss: 1.6830
2022-03-03 20:26:21 - train: epoch 0136, iter [02100, 05004], lr: 0.025000, loss: 1.3335
2022-03-03 20:26:54 - train: epoch 0136, iter [02200, 05004], lr: 0.025000, loss: 1.6168
2022-03-03 20:27:26 - train: epoch 0136, iter [02300, 05004], lr: 0.025000, loss: 1.6467
2022-03-03 20:28:00 - train: epoch 0136, iter [02400, 05004], lr: 0.025000, loss: 1.2490
2022-03-03 20:28:33 - train: epoch 0136, iter [02500, 05004], lr: 0.025000, loss: 1.6477
2022-03-03 20:29:06 - train: epoch 0136, iter [02600, 05004], lr: 0.025000, loss: 1.3972
2022-03-03 20:29:40 - train: epoch 0136, iter [02700, 05004], lr: 0.025000, loss: 1.4875
2022-03-03 20:30:12 - train: epoch 0136, iter [02800, 05004], lr: 0.025000, loss: 1.6616
2022-03-03 20:30:46 - train: epoch 0136, iter [02900, 05004], lr: 0.025000, loss: 1.4254
2022-03-03 20:31:18 - train: epoch 0136, iter [03000, 05004], lr: 0.025000, loss: 1.6133
2022-03-03 20:31:51 - train: epoch 0136, iter [03100, 05004], lr: 0.025000, loss: 1.2804
2022-03-03 20:32:25 - train: epoch 0136, iter [03200, 05004], lr: 0.025000, loss: 1.6294
2022-03-03 20:32:58 - train: epoch 0136, iter [03300, 05004], lr: 0.025000, loss: 1.6803
2022-03-03 20:33:31 - train: epoch 0136, iter [03400, 05004], lr: 0.025000, loss: 1.6232
2022-03-03 20:34:04 - train: epoch 0136, iter [03500, 05004], lr: 0.025000, loss: 1.4470
2022-03-03 20:34:37 - train: epoch 0136, iter [03600, 05004], lr: 0.025000, loss: 1.6384
2022-03-03 20:35:11 - train: epoch 0136, iter [03700, 05004], lr: 0.025000, loss: 1.4898
2022-03-03 20:35:43 - train: epoch 0136, iter [03800, 05004], lr: 0.025000, loss: 1.5063
2022-03-03 20:36:17 - train: epoch 0136, iter [03900, 05004], lr: 0.025000, loss: 1.5059
2022-03-03 20:36:50 - train: epoch 0136, iter [04000, 05004], lr: 0.025000, loss: 1.5919
2022-03-03 20:37:22 - train: epoch 0136, iter [04100, 05004], lr: 0.025000, loss: 1.4807
2022-03-03 20:37:55 - train: epoch 0136, iter [04200, 05004], lr: 0.025000, loss: 1.4297
2022-03-03 20:38:28 - train: epoch 0136, iter [04300, 05004], lr: 0.025000, loss: 1.3907
2022-03-03 20:39:01 - train: epoch 0136, iter [04400, 05004], lr: 0.025000, loss: 1.2888
2022-03-03 20:39:35 - train: epoch 0136, iter [04500, 05004], lr: 0.025000, loss: 1.7519
2022-03-03 20:40:08 - train: epoch 0136, iter [04600, 05004], lr: 0.025000, loss: 1.6137
2022-03-03 20:40:41 - train: epoch 0136, iter [04700, 05004], lr: 0.025000, loss: 1.5039
2022-03-03 20:41:14 - train: epoch 0136, iter [04800, 05004], lr: 0.025000, loss: 1.5560
2022-03-03 20:41:47 - train: epoch 0136, iter [04900, 05004], lr: 0.025000, loss: 1.6331
2022-03-03 20:42:19 - train: epoch 0136, iter [05000, 05004], lr: 0.025000, loss: 1.6288
2022-03-03 20:42:20 - train: epoch 136, train_loss: 1.5086
2022-03-03 20:43:34 - eval: epoch: 136, acc1: 66.778%, acc5: 87.892%, test_loss: 1.3530, per_image_load_time: 2.034ms, per_image_inference_time: 0.563ms
2022-03-03 20:43:34 - until epoch: 136, best_acc1: 67.280%
2022-03-03 20:43:34 - epoch 137 lr: 0.02430565941356157
2022-03-03 20:44:13 - train: epoch 0137, iter [00100, 05004], lr: 0.024306, loss: 1.1961
2022-03-03 20:44:46 - train: epoch 0137, iter [00200, 05004], lr: 0.024306, loss: 1.6474
2022-03-03 20:45:18 - train: epoch 0137, iter [00300, 05004], lr: 0.024306, loss: 1.5071
2022-03-03 20:45:51 - train: epoch 0137, iter [00400, 05004], lr: 0.024306, loss: 1.3596
2022-03-03 20:46:24 - train: epoch 0137, iter [00500, 05004], lr: 0.024306, loss: 1.3988
2022-03-03 20:46:58 - train: epoch 0137, iter [00600, 05004], lr: 0.024306, loss: 1.6258
2022-03-03 20:47:31 - train: epoch 0137, iter [00700, 05004], lr: 0.024306, loss: 1.4636
2022-03-03 20:48:05 - train: epoch 0137, iter [00800, 05004], lr: 0.024306, loss: 1.3329
2022-03-03 20:48:38 - train: epoch 0137, iter [00900, 05004], lr: 0.024306, loss: 1.5337
2022-03-03 20:49:12 - train: epoch 0137, iter [01000, 05004], lr: 0.024306, loss: 1.3800
2022-03-03 20:49:45 - train: epoch 0137, iter [01100, 05004], lr: 0.024306, loss: 1.5973
2022-03-03 20:50:19 - train: epoch 0137, iter [01200, 05004], lr: 0.024306, loss: 1.3795
2022-03-03 20:50:52 - train: epoch 0137, iter [01300, 05004], lr: 0.024306, loss: 1.2030
2022-03-03 20:51:26 - train: epoch 0137, iter [01400, 05004], lr: 0.024306, loss: 1.6705
2022-03-03 20:51:59 - train: epoch 0137, iter [01500, 05004], lr: 0.024306, loss: 1.8263
2022-03-03 20:52:32 - train: epoch 0137, iter [01600, 05004], lr: 0.024306, loss: 1.4327
2022-03-03 20:53:06 - train: epoch 0137, iter [01700, 05004], lr: 0.024306, loss: 1.3908
2022-03-03 20:53:38 - train: epoch 0137, iter [01800, 05004], lr: 0.024306, loss: 1.5309
2022-03-03 20:54:12 - train: epoch 0137, iter [01900, 05004], lr: 0.024306, loss: 1.3291
2022-03-03 20:54:45 - train: epoch 0137, iter [02000, 05004], lr: 0.024306, loss: 1.3140
2022-03-03 20:55:19 - train: epoch 0137, iter [02100, 05004], lr: 0.024306, loss: 1.5237
2022-03-03 20:55:52 - train: epoch 0137, iter [02200, 05004], lr: 0.024306, loss: 1.5417
2022-03-03 20:56:26 - train: epoch 0137, iter [02300, 05004], lr: 0.024306, loss: 1.3012
2022-03-03 20:56:58 - train: epoch 0137, iter [02400, 05004], lr: 0.024306, loss: 1.3975
2022-03-03 20:57:32 - train: epoch 0137, iter [02500, 05004], lr: 0.024306, loss: 1.3830
2022-03-03 20:58:05 - train: epoch 0137, iter [02600, 05004], lr: 0.024306, loss: 1.4462
2022-03-03 20:58:38 - train: epoch 0137, iter [02700, 05004], lr: 0.024306, loss: 1.4181
2022-03-03 20:59:12 - train: epoch 0137, iter [02800, 05004], lr: 0.024306, loss: 1.4275
2022-03-03 20:59:45 - train: epoch 0137, iter [02900, 05004], lr: 0.024306, loss: 1.4272
2022-03-03 21:00:18 - train: epoch 0137, iter [03000, 05004], lr: 0.024306, loss: 1.4955
2022-03-03 21:00:52 - train: epoch 0137, iter [03100, 05004], lr: 0.024306, loss: 1.4896
2022-03-03 21:01:24 - train: epoch 0137, iter [03200, 05004], lr: 0.024306, loss: 1.6934
2022-03-03 21:01:58 - train: epoch 0137, iter [03300, 05004], lr: 0.024306, loss: 1.3648
2022-03-03 21:02:31 - train: epoch 0137, iter [03400, 05004], lr: 0.024306, loss: 1.6443
2022-03-03 21:03:05 - train: epoch 0137, iter [03500, 05004], lr: 0.024306, loss: 1.4918
2022-03-03 21:03:38 - train: epoch 0137, iter [03600, 05004], lr: 0.024306, loss: 1.3777
2022-03-03 21:04:11 - train: epoch 0137, iter [03700, 05004], lr: 0.024306, loss: 1.6550
2022-03-03 21:04:44 - train: epoch 0137, iter [03800, 05004], lr: 0.024306, loss: 1.4507
2022-03-03 21:05:17 - train: epoch 0137, iter [03900, 05004], lr: 0.024306, loss: 1.5876
2022-03-03 21:05:50 - train: epoch 0137, iter [04000, 05004], lr: 0.024306, loss: 1.5715
2022-03-03 21:06:23 - train: epoch 0137, iter [04100, 05004], lr: 0.024306, loss: 1.4811
2022-03-03 21:06:56 - train: epoch 0137, iter [04200, 05004], lr: 0.024306, loss: 1.7368
2022-03-03 21:07:29 - train: epoch 0137, iter [04300, 05004], lr: 0.024306, loss: 1.4952
2022-03-03 21:08:03 - train: epoch 0137, iter [04400, 05004], lr: 0.024306, loss: 1.4008
2022-03-03 21:08:36 - train: epoch 0137, iter [04500, 05004], lr: 0.024306, loss: 1.6166
2022-03-03 21:09:09 - train: epoch 0137, iter [04600, 05004], lr: 0.024306, loss: 1.3151
2022-03-03 21:09:42 - train: epoch 0137, iter [04700, 05004], lr: 0.024306, loss: 1.3595
2022-03-03 21:10:15 - train: epoch 0137, iter [04800, 05004], lr: 0.024306, loss: 1.5421
2022-03-03 21:10:49 - train: epoch 0137, iter [04900, 05004], lr: 0.024306, loss: 1.3795
2022-03-03 21:11:21 - train: epoch 0137, iter [05000, 05004], lr: 0.024306, loss: 1.3654
2022-03-03 21:11:22 - train: epoch 137, train_loss: 1.4944
2022-03-03 21:12:36 - eval: epoch: 137, acc1: 68.124%, acc5: 88.454%, test_loss: 1.3080, per_image_load_time: 2.343ms, per_image_inference_time: 0.537ms
2022-03-03 21:12:37 - until epoch: 137, best_acc1: 68.124%
2022-03-03 21:12:37 - epoch 138 lr: 0.02361798779469336
2022-03-03 21:13:15 - train: epoch 0138, iter [00100, 05004], lr: 0.023618, loss: 1.2881
2022-03-03 21:13:48 - train: epoch 0138, iter [00200, 05004], lr: 0.023618, loss: 1.4943
2022-03-03 21:14:21 - train: epoch 0138, iter [00300, 05004], lr: 0.023618, loss: 1.5208
2022-03-03 21:14:55 - train: epoch 0138, iter [00400, 05004], lr: 0.023618, loss: 1.5230
2022-03-03 21:15:29 - train: epoch 0138, iter [00500, 05004], lr: 0.023618, loss: 1.2527
2022-03-03 21:16:02 - train: epoch 0138, iter [00600, 05004], lr: 0.023618, loss: 1.6244
2022-03-03 21:16:35 - train: epoch 0138, iter [00700, 05004], lr: 0.023618, loss: 1.3671
2022-03-03 21:17:08 - train: epoch 0138, iter [00800, 05004], lr: 0.023618, loss: 1.2329
2022-03-03 21:17:41 - train: epoch 0138, iter [00900, 05004], lr: 0.023618, loss: 1.6316
2022-03-03 21:18:15 - train: epoch 0138, iter [01000, 05004], lr: 0.023618, loss: 1.3499
2022-03-03 21:18:49 - train: epoch 0138, iter [01100, 05004], lr: 0.023618, loss: 1.6219
2022-03-03 21:19:23 - train: epoch 0138, iter [01200, 05004], lr: 0.023618, loss: 1.5938
2022-03-03 21:19:56 - train: epoch 0138, iter [01300, 05004], lr: 0.023618, loss: 1.3754
2022-03-03 21:20:28 - train: epoch 0138, iter [01400, 05004], lr: 0.023618, loss: 1.5216
2022-03-03 21:21:02 - train: epoch 0138, iter [01500, 05004], lr: 0.023618, loss: 1.3720
2022-03-03 21:21:35 - train: epoch 0138, iter [01600, 05004], lr: 0.023618, loss: 1.5388
2022-03-03 21:22:09 - train: epoch 0138, iter [01700, 05004], lr: 0.023618, loss: 1.3981
2022-03-03 21:22:42 - train: epoch 0138, iter [01800, 05004], lr: 0.023618, loss: 1.4345
2022-03-03 21:23:15 - train: epoch 0138, iter [01900, 05004], lr: 0.023618, loss: 1.5525
2022-03-03 21:23:48 - train: epoch 0138, iter [02000, 05004], lr: 0.023618, loss: 1.6622
2022-03-03 21:24:22 - train: epoch 0138, iter [02100, 05004], lr: 0.023618, loss: 1.4998
2022-03-03 21:24:55 - train: epoch 0138, iter [02200, 05004], lr: 0.023618, loss: 1.1531
2022-03-03 21:25:28 - train: epoch 0138, iter [02300, 05004], lr: 0.023618, loss: 1.3031
2022-03-03 21:26:01 - train: epoch 0138, iter [02400, 05004], lr: 0.023618, loss: 1.3115
2022-03-03 21:26:35 - train: epoch 0138, iter [02500, 05004], lr: 0.023618, loss: 1.4417
2022-03-03 21:27:08 - train: epoch 0138, iter [02600, 05004], lr: 0.023618, loss: 1.6564
2022-03-03 21:27:42 - train: epoch 0138, iter [02700, 05004], lr: 0.023618, loss: 1.4993
2022-03-03 21:28:15 - train: epoch 0138, iter [02800, 05004], lr: 0.023618, loss: 1.5619
2022-03-03 21:28:48 - train: epoch 0138, iter [02900, 05004], lr: 0.023618, loss: 1.5939
2022-03-03 21:29:21 - train: epoch 0138, iter [03000, 05004], lr: 0.023618, loss: 1.4556
2022-03-03 21:29:54 - train: epoch 0138, iter [03100, 05004], lr: 0.023618, loss: 1.5934
2022-03-03 21:30:28 - train: epoch 0138, iter [03200, 05004], lr: 0.023618, loss: 1.5244
2022-03-03 21:31:01 - train: epoch 0138, iter [03300, 05004], lr: 0.023618, loss: 1.5690
2022-03-03 21:31:34 - train: epoch 0138, iter [03400, 05004], lr: 0.023618, loss: 1.3989
2022-03-03 21:32:07 - train: epoch 0138, iter [03500, 05004], lr: 0.023618, loss: 1.3881
2022-03-03 21:32:40 - train: epoch 0138, iter [03600, 05004], lr: 0.023618, loss: 1.6379
2022-03-03 21:33:14 - train: epoch 0138, iter [03700, 05004], lr: 0.023618, loss: 1.5517
2022-03-03 21:33:47 - train: epoch 0138, iter [03800, 05004], lr: 0.023618, loss: 1.6490
2022-03-03 21:34:20 - train: epoch 0138, iter [03900, 05004], lr: 0.023618, loss: 1.6128
2022-03-03 21:34:53 - train: epoch 0138, iter [04000, 05004], lr: 0.023618, loss: 1.5345
2022-03-03 21:35:27 - train: epoch 0138, iter [04100, 05004], lr: 0.023618, loss: 1.2828
2022-03-03 21:36:00 - train: epoch 0138, iter [04200, 05004], lr: 0.023618, loss: 1.4509
2022-03-03 21:36:34 - train: epoch 0138, iter [04300, 05004], lr: 0.023618, loss: 1.4629
2022-03-03 21:37:07 - train: epoch 0138, iter [04400, 05004], lr: 0.023618, loss: 1.2291
2022-03-03 21:37:40 - train: epoch 0138, iter [04500, 05004], lr: 0.023618, loss: 1.4608
2022-03-03 21:38:14 - train: epoch 0138, iter [04600, 05004], lr: 0.023618, loss: 1.5969
2022-03-03 21:38:47 - train: epoch 0138, iter [04700, 05004], lr: 0.023618, loss: 1.2650
2022-03-03 21:39:20 - train: epoch 0138, iter [04800, 05004], lr: 0.023618, loss: 1.6116
2022-03-03 21:39:53 - train: epoch 0138, iter [04900, 05004], lr: 0.023618, loss: 1.6592
2022-03-03 21:40:25 - train: epoch 0138, iter [05000, 05004], lr: 0.023618, loss: 1.6458
2022-03-03 21:40:26 - train: epoch 138, train_loss: 1.4869
2022-03-03 21:41:41 - eval: epoch: 138, acc1: 67.208%, acc5: 88.130%, test_loss: 1.3357, per_image_load_time: 2.380ms, per_image_inference_time: 0.509ms
2022-03-03 21:41:41 - until epoch: 138, best_acc1: 68.124%
2022-03-03 21:41:41 - epoch 139 lr: 0.022937163628603437
2022-03-03 21:42:19 - train: epoch 0139, iter [00100, 05004], lr: 0.022937, loss: 1.4204
2022-03-03 21:42:53 - train: epoch 0139, iter [00200, 05004], lr: 0.022937, loss: 1.4079
2022-03-03 21:43:25 - train: epoch 0139, iter [00300, 05004], lr: 0.022937, loss: 1.2371
2022-03-03 21:43:59 - train: epoch 0139, iter [00400, 05004], lr: 0.022937, loss: 1.3999
2022-03-03 21:44:33 - train: epoch 0139, iter [00500, 05004], lr: 0.022937, loss: 1.5564
2022-03-03 21:45:06 - train: epoch 0139, iter [00600, 05004], lr: 0.022937, loss: 1.4737
2022-03-03 21:45:38 - train: epoch 0139, iter [00700, 05004], lr: 0.022937, loss: 1.4197
2022-03-03 21:46:12 - train: epoch 0139, iter [00800, 05004], lr: 0.022937, loss: 1.6315
2022-03-03 21:46:45 - train: epoch 0139, iter [00900, 05004], lr: 0.022937, loss: 1.6905
2022-03-03 21:47:18 - train: epoch 0139, iter [01000, 05004], lr: 0.022937, loss: 1.5456
2022-03-03 21:47:52 - train: epoch 0139, iter [01100, 05004], lr: 0.022937, loss: 1.4545
2022-03-03 21:48:25 - train: epoch 0139, iter [01200, 05004], lr: 0.022937, loss: 1.4494
2022-03-03 21:48:58 - train: epoch 0139, iter [01300, 05004], lr: 0.022937, loss: 1.4465
2022-03-03 21:49:31 - train: epoch 0139, iter [01400, 05004], lr: 0.022937, loss: 1.5058
2022-03-03 21:50:05 - train: epoch 0139, iter [01500, 05004], lr: 0.022937, loss: 1.5917
2022-03-03 21:50:37 - train: epoch 0139, iter [01600, 05004], lr: 0.022937, loss: 1.4938
2022-03-03 21:51:11 - train: epoch 0139, iter [01700, 05004], lr: 0.022937, loss: 1.2808
2022-03-03 21:51:43 - train: epoch 0139, iter [01800, 05004], lr: 0.022937, loss: 1.4334
2022-03-03 21:52:17 - train: epoch 0139, iter [01900, 05004], lr: 0.022937, loss: 1.7618
2022-03-03 21:52:51 - train: epoch 0139, iter [02000, 05004], lr: 0.022937, loss: 1.6191
2022-03-03 21:53:23 - train: epoch 0139, iter [02100, 05004], lr: 0.022937, loss: 1.4128
2022-03-03 21:53:56 - train: epoch 0139, iter [02200, 05004], lr: 0.022937, loss: 1.6850
2022-03-03 21:54:29 - train: epoch 0139, iter [02300, 05004], lr: 0.022937, loss: 1.4876
2022-03-03 21:55:02 - train: epoch 0139, iter [02400, 05004], lr: 0.022937, loss: 1.4645
2022-03-03 21:55:35 - train: epoch 0139, iter [02500, 05004], lr: 0.022937, loss: 1.4396
2022-03-03 21:56:08 - train: epoch 0139, iter [02600, 05004], lr: 0.022937, loss: 1.5194
2022-03-03 21:56:41 - train: epoch 0139, iter [02700, 05004], lr: 0.022937, loss: 1.5476
2022-03-03 21:57:15 - train: epoch 0139, iter [02800, 05004], lr: 0.022937, loss: 1.4193
2022-03-03 21:57:48 - train: epoch 0139, iter [02900, 05004], lr: 0.022937, loss: 1.4256
2022-03-03 21:58:21 - train: epoch 0139, iter [03000, 05004], lr: 0.022937, loss: 1.3255
2022-03-03 21:58:54 - train: epoch 0139, iter [03100, 05004], lr: 0.022937, loss: 1.6668
2022-03-03 21:59:28 - train: epoch 0139, iter [03200, 05004], lr: 0.022937, loss: 1.3761
2022-03-03 22:00:01 - train: epoch 0139, iter [03300, 05004], lr: 0.022937, loss: 1.4289
2022-03-03 22:00:33 - train: epoch 0139, iter [03400, 05004], lr: 0.022937, loss: 1.2866
2022-03-03 22:01:07 - train: epoch 0139, iter [03500, 05004], lr: 0.022937, loss: 1.5191
2022-03-03 22:01:41 - train: epoch 0139, iter [03600, 05004], lr: 0.022937, loss: 1.4292
2022-03-03 22:02:14 - train: epoch 0139, iter [03700, 05004], lr: 0.022937, loss: 1.5576
2022-03-03 22:02:48 - train: epoch 0139, iter [03800, 05004], lr: 0.022937, loss: 1.5475
2022-03-03 22:03:21 - train: epoch 0139, iter [03900, 05004], lr: 0.022937, loss: 1.4439
2022-03-03 22:03:54 - train: epoch 0139, iter [04000, 05004], lr: 0.022937, loss: 1.4722
2022-03-03 22:04:28 - train: epoch 0139, iter [04100, 05004], lr: 0.022937, loss: 1.5767
2022-03-03 22:05:01 - train: epoch 0139, iter [04200, 05004], lr: 0.022937, loss: 1.7514
2022-03-03 22:05:34 - train: epoch 0139, iter [04300, 05004], lr: 0.022937, loss: 1.4219
2022-03-03 22:06:07 - train: epoch 0139, iter [04400, 05004], lr: 0.022937, loss: 1.7682
2022-03-03 22:06:41 - train: epoch 0139, iter [04500, 05004], lr: 0.022937, loss: 1.6645
2022-03-03 22:07:14 - train: epoch 0139, iter [04600, 05004], lr: 0.022937, loss: 1.5916
2022-03-03 22:07:48 - train: epoch 0139, iter [04700, 05004], lr: 0.022937, loss: 1.5197
2022-03-03 22:08:21 - train: epoch 0139, iter [04800, 05004], lr: 0.022937, loss: 1.4492
2022-03-03 22:08:54 - train: epoch 0139, iter [04900, 05004], lr: 0.022937, loss: 1.6704
2022-03-03 22:09:25 - train: epoch 0139, iter [05000, 05004], lr: 0.022937, loss: 1.4133
2022-03-03 22:09:26 - train: epoch 139, train_loss: 1.4757
2022-03-03 22:10:40 - eval: epoch: 139, acc1: 68.246%, acc5: 88.862%, test_loss: 1.2730, per_image_load_time: 1.128ms, per_image_inference_time: 0.510ms
2022-03-03 22:10:41 - until epoch: 139, best_acc1: 68.246%
2022-03-03 22:10:41 - epoch 140 lr: 0.022263363623243056
2022-03-03 22:11:19 - train: epoch 0140, iter [00100, 05004], lr: 0.022263, loss: 1.3186
2022-03-03 22:11:52 - train: epoch 0140, iter [00200, 05004], lr: 0.022263, loss: 1.6767
2022-03-03 22:12:25 - train: epoch 0140, iter [00300, 05004], lr: 0.022263, loss: 1.3051
2022-03-03 22:12:58 - train: epoch 0140, iter [00400, 05004], lr: 0.022263, loss: 1.5082
2022-03-03 22:13:31 - train: epoch 0140, iter [00500, 05004], lr: 0.022263, loss: 1.2925
2022-03-03 22:14:04 - train: epoch 0140, iter [00600, 05004], lr: 0.022263, loss: 1.5647
2022-03-03 22:14:37 - train: epoch 0140, iter [00700, 05004], lr: 0.022263, loss: 1.1912
2022-03-03 22:15:09 - train: epoch 0140, iter [00800, 05004], lr: 0.022263, loss: 1.6517
2022-03-03 22:15:43 - train: epoch 0140, iter [00900, 05004], lr: 0.022263, loss: 1.6060
2022-03-03 22:16:16 - train: epoch 0140, iter [01000, 05004], lr: 0.022263, loss: 1.4629
2022-03-03 22:16:49 - train: epoch 0140, iter [01100, 05004], lr: 0.022263, loss: 1.4736
2022-03-03 22:17:23 - train: epoch 0140, iter [01200, 05004], lr: 0.022263, loss: 1.5023
2022-03-03 22:17:55 - train: epoch 0140, iter [01300, 05004], lr: 0.022263, loss: 1.5569
2022-03-03 22:18:28 - train: epoch 0140, iter [01400, 05004], lr: 0.022263, loss: 1.3382
2022-03-03 22:19:02 - train: epoch 0140, iter [01500, 05004], lr: 0.022263, loss: 1.5407
2022-03-03 22:19:36 - train: epoch 0140, iter [01600, 05004], lr: 0.022263, loss: 1.4951
2022-03-03 22:20:09 - train: epoch 0140, iter [01700, 05004], lr: 0.022263, loss: 1.3595
2022-03-03 22:20:42 - train: epoch 0140, iter [01800, 05004], lr: 0.022263, loss: 1.4477
2022-03-03 22:21:15 - train: epoch 0140, iter [01900, 05004], lr: 0.022263, loss: 1.6036
2022-03-03 22:21:49 - train: epoch 0140, iter [02000, 05004], lr: 0.022263, loss: 1.4398
2022-03-03 22:22:23 - train: epoch 0140, iter [02100, 05004], lr: 0.022263, loss: 1.6093
2022-03-03 22:22:56 - train: epoch 0140, iter [02200, 05004], lr: 0.022263, loss: 1.5107
2022-03-03 22:23:29 - train: epoch 0140, iter [02300, 05004], lr: 0.022263, loss: 1.2050
2022-03-03 22:24:02 - train: epoch 0140, iter [02400, 05004], lr: 0.022263, loss: 1.4573
2022-03-03 22:24:36 - train: epoch 0140, iter [02500, 05004], lr: 0.022263, loss: 1.5198
2022-03-03 22:25:09 - train: epoch 0140, iter [02600, 05004], lr: 0.022263, loss: 1.4387
2022-03-03 22:25:43 - train: epoch 0140, iter [02700, 05004], lr: 0.022263, loss: 1.3557
2022-03-03 22:26:15 - train: epoch 0140, iter [02800, 05004], lr: 0.022263, loss: 1.5459
2022-03-03 22:26:49 - train: epoch 0140, iter [02900, 05004], lr: 0.022263, loss: 1.4326
2022-03-03 22:27:23 - train: epoch 0140, iter [03000, 05004], lr: 0.022263, loss: 1.4156
2022-03-03 22:27:56 - train: epoch 0140, iter [03100, 05004], lr: 0.022263, loss: 1.3852
2022-03-03 22:28:29 - train: epoch 0140, iter [03200, 05004], lr: 0.022263, loss: 1.4287
2022-03-03 22:29:03 - train: epoch 0140, iter [03300, 05004], lr: 0.022263, loss: 1.5203
2022-03-03 22:29:36 - train: epoch 0140, iter [03400, 05004], lr: 0.022263, loss: 1.6700
2022-03-03 22:30:09 - train: epoch 0140, iter [03500, 05004], lr: 0.022263, loss: 1.4881
2022-03-03 22:30:43 - train: epoch 0140, iter [03600, 05004], lr: 0.022263, loss: 1.4234
2022-03-03 22:31:16 - train: epoch 0140, iter [03700, 05004], lr: 0.022263, loss: 1.3085
2022-03-03 22:31:50 - train: epoch 0140, iter [03800, 05004], lr: 0.022263, loss: 1.5936
2022-03-03 22:32:22 - train: epoch 0140, iter [03900, 05004], lr: 0.022263, loss: 1.4852
2022-03-03 22:32:55 - train: epoch 0140, iter [04000, 05004], lr: 0.022263, loss: 1.5177
2022-03-03 22:33:29 - train: epoch 0140, iter [04100, 05004], lr: 0.022263, loss: 1.5492
2022-03-03 22:34:02 - train: epoch 0140, iter [04200, 05004], lr: 0.022263, loss: 1.3113
