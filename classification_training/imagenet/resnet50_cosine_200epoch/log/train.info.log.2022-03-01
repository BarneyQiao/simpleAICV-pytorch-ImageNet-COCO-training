2022-03-01 22:34:32 - train: epoch 0041, iter [01700, 05004], lr: 0.092260, loss: 2.1359
2022-03-01 22:35:05 - train: epoch 0041, iter [01800, 05004], lr: 0.092260, loss: 2.1211
2022-03-01 22:35:39 - train: epoch 0041, iter [01900, 05004], lr: 0.092260, loss: 2.0241
2022-03-01 22:36:11 - train: epoch 0041, iter [02000, 05004], lr: 0.092260, loss: 1.9694
2022-03-01 22:36:45 - train: epoch 0041, iter [02100, 05004], lr: 0.092260, loss: 1.9684
2022-03-01 22:37:17 - train: epoch 0041, iter [02200, 05004], lr: 0.092260, loss: 1.9503
2022-03-01 22:37:50 - train: epoch 0041, iter [02300, 05004], lr: 0.092260, loss: 1.7731
2022-03-01 22:38:23 - train: epoch 0041, iter [02400, 05004], lr: 0.092260, loss: 2.2632
2022-03-01 22:38:56 - train: epoch 0041, iter [02500, 05004], lr: 0.092260, loss: 1.8474
2022-03-01 22:39:29 - train: epoch 0041, iter [02600, 05004], lr: 0.092260, loss: 2.1807
2022-03-01 22:40:03 - train: epoch 0041, iter [02700, 05004], lr: 0.092260, loss: 2.3255
2022-03-01 22:40:35 - train: epoch 0041, iter [02800, 05004], lr: 0.092260, loss: 2.1091
2022-03-01 22:41:09 - train: epoch 0041, iter [02900, 05004], lr: 0.092260, loss: 2.2823
2022-03-01 22:41:41 - train: epoch 0041, iter [03000, 05004], lr: 0.092260, loss: 2.1780
2022-03-01 22:42:15 - train: epoch 0041, iter [03100, 05004], lr: 0.092260, loss: 2.2268
2022-03-01 22:42:48 - train: epoch 0041, iter [03200, 05004], lr: 0.092260, loss: 2.0254
2022-03-01 22:43:21 - train: epoch 0041, iter [03300, 05004], lr: 0.092260, loss: 1.9083
2022-03-01 22:43:53 - train: epoch 0041, iter [03400, 05004], lr: 0.092260, loss: 1.8112
2022-03-01 22:44:27 - train: epoch 0041, iter [03500, 05004], lr: 0.092260, loss: 2.1688
2022-03-01 22:44:59 - train: epoch 0041, iter [03600, 05004], lr: 0.092260, loss: 2.2256
2022-03-01 22:45:33 - train: epoch 0041, iter [03700, 05004], lr: 0.092260, loss: 2.1880
2022-03-01 22:46:05 - train: epoch 0041, iter [03800, 05004], lr: 0.092260, loss: 1.8854
2022-03-01 22:46:38 - train: epoch 0041, iter [03900, 05004], lr: 0.092260, loss: 2.0944
2022-03-01 22:47:12 - train: epoch 0041, iter [04000, 05004], lr: 0.092260, loss: 1.9840
2022-03-01 22:47:46 - train: epoch 0041, iter [04100, 05004], lr: 0.092260, loss: 2.0848
2022-03-01 22:48:18 - train: epoch 0041, iter [04200, 05004], lr: 0.092260, loss: 1.9411
2022-03-01 22:48:52 - train: epoch 0041, iter [04300, 05004], lr: 0.092260, loss: 2.1868
2022-03-01 22:49:24 - train: epoch 0041, iter [04400, 05004], lr: 0.092260, loss: 2.0222
2022-03-01 22:49:58 - train: epoch 0041, iter [04500, 05004], lr: 0.092260, loss: 2.1522
2022-03-01 22:50:30 - train: epoch 0041, iter [04600, 05004], lr: 0.092260, loss: 2.1440
2022-03-01 22:51:05 - train: epoch 0041, iter [04700, 05004], lr: 0.092260, loss: 2.3629
2022-03-01 22:51:37 - train: epoch 0041, iter [04800, 05004], lr: 0.092260, loss: 2.1906
2022-03-01 22:52:11 - train: epoch 0041, iter [04900, 05004], lr: 0.092260, loss: 2.1585
2022-03-01 22:52:43 - train: epoch 0041, iter [05000, 05004], lr: 0.092260, loss: 2.1445
2022-03-01 22:52:44 - train: epoch 041, train_loss: 2.0933
2022-03-01 22:53:58 - eval: epoch: 041, acc1: 54.650%, acc5: 79.800%, test_loss: 1.9309, per_image_load_time: 2.111ms, per_image_inference_time: 0.542ms
2022-03-01 22:53:59 - until epoch: 041, best_acc1: 56.262%
2022-03-01 22:53:59 - epoch 042 lr: 0.09182350690051133
2022-03-01 22:54:37 - train: epoch 0042, iter [00100, 05004], lr: 0.091824, loss: 1.8038
2022-03-01 22:55:10 - train: epoch 0042, iter [00200, 05004], lr: 0.091824, loss: 2.0623
2022-03-01 22:55:43 - train: epoch 0042, iter [00300, 05004], lr: 0.091824, loss: 2.1290
2022-03-01 22:56:16 - train: epoch 0042, iter [00400, 05004], lr: 0.091824, loss: 1.7914
2022-03-01 22:56:49 - train: epoch 0042, iter [00500, 05004], lr: 0.091824, loss: 2.1725
2022-03-01 22:57:22 - train: epoch 0042, iter [00600, 05004], lr: 0.091824, loss: 1.9269
2022-03-01 22:57:55 - train: epoch 0042, iter [00700, 05004], lr: 0.091824, loss: 2.0548
2022-03-01 22:58:29 - train: epoch 0042, iter [00800, 05004], lr: 0.091824, loss: 1.9255
2022-03-01 22:59:02 - train: epoch 0042, iter [00900, 05004], lr: 0.091824, loss: 2.2856
2022-03-01 22:59:35 - train: epoch 0042, iter [01000, 05004], lr: 0.091824, loss: 2.2437
2022-03-01 23:00:08 - train: epoch 0042, iter [01100, 05004], lr: 0.091824, loss: 1.9520
2022-03-01 23:00:41 - train: epoch 0042, iter [01200, 05004], lr: 0.091824, loss: 2.0112
2022-03-01 23:01:14 - train: epoch 0042, iter [01300, 05004], lr: 0.091824, loss: 2.1668
2022-03-01 23:01:47 - train: epoch 0042, iter [01400, 05004], lr: 0.091824, loss: 2.3705
2022-03-01 23:02:21 - train: epoch 0042, iter [01500, 05004], lr: 0.091824, loss: 2.0522
2022-03-01 23:02:54 - train: epoch 0042, iter [01600, 05004], lr: 0.091824, loss: 2.0912
2022-03-01 23:03:28 - train: epoch 0042, iter [01700, 05004], lr: 0.091824, loss: 2.0773
2022-03-01 23:04:00 - train: epoch 0042, iter [01800, 05004], lr: 0.091824, loss: 2.1541
2022-03-01 23:04:33 - train: epoch 0042, iter [01900, 05004], lr: 0.091824, loss: 2.2461
2022-03-01 23:05:06 - train: epoch 0042, iter [02000, 05004], lr: 0.091824, loss: 1.9383
2022-03-01 23:05:39 - train: epoch 0042, iter [02100, 05004], lr: 0.091824, loss: 1.8256
2022-03-01 23:06:13 - train: epoch 0042, iter [02200, 05004], lr: 0.091824, loss: 2.0040
2022-03-01 23:06:47 - train: epoch 0042, iter [02300, 05004], lr: 0.091824, loss: 1.9579
2022-03-01 23:07:20 - train: epoch 0042, iter [02400, 05004], lr: 0.091824, loss: 2.4093
2022-03-01 23:07:53 - train: epoch 0042, iter [02500, 05004], lr: 0.091824, loss: 2.2489
2022-03-01 23:08:26 - train: epoch 0042, iter [02600, 05004], lr: 0.091824, loss: 2.1947
2022-03-01 23:09:00 - train: epoch 0042, iter [02700, 05004], lr: 0.091824, loss: 1.8365
2022-03-01 23:09:33 - train: epoch 0042, iter [02800, 05004], lr: 0.091824, loss: 2.1224
2022-03-01 23:10:06 - train: epoch 0042, iter [02900, 05004], lr: 0.091824, loss: 2.2353
2022-03-01 23:10:40 - train: epoch 0042, iter [03000, 05004], lr: 0.091824, loss: 2.1743
2022-03-01 23:11:13 - train: epoch 0042, iter [03100, 05004], lr: 0.091824, loss: 2.1342
2022-03-01 23:11:45 - train: epoch 0042, iter [03200, 05004], lr: 0.091824, loss: 2.0906
2022-03-01 23:12:19 - train: epoch 0042, iter [03300, 05004], lr: 0.091824, loss: 2.3151
2022-03-01 23:12:52 - train: epoch 0042, iter [03400, 05004], lr: 0.091824, loss: 2.0222
2022-03-01 23:13:25 - train: epoch 0042, iter [03500, 05004], lr: 0.091824, loss: 2.0664
2022-03-01 23:13:58 - train: epoch 0042, iter [03600, 05004], lr: 0.091824, loss: 2.2561
2022-03-01 23:14:32 - train: epoch 0042, iter [03700, 05004], lr: 0.091824, loss: 2.1456
2022-03-01 23:15:05 - train: epoch 0042, iter [03800, 05004], lr: 0.091824, loss: 1.9866
2022-03-01 23:15:37 - train: epoch 0042, iter [03900, 05004], lr: 0.091824, loss: 1.9413
2022-03-01 23:16:11 - train: epoch 0042, iter [04000, 05004], lr: 0.091824, loss: 2.1225
2022-03-01 23:16:45 - train: epoch 0042, iter [04100, 05004], lr: 0.091824, loss: 2.3327
2022-03-01 23:17:18 - train: epoch 0042, iter [04200, 05004], lr: 0.091824, loss: 2.2402
2022-03-01 23:17:51 - train: epoch 0042, iter [04300, 05004], lr: 0.091824, loss: 1.9035
2022-03-01 23:18:24 - train: epoch 0042, iter [04400, 05004], lr: 0.091824, loss: 1.9093
2022-03-01 23:18:58 - train: epoch 0042, iter [04500, 05004], lr: 0.091824, loss: 2.0684
2022-03-01 23:19:30 - train: epoch 0042, iter [04600, 05004], lr: 0.091824, loss: 2.4140
2022-03-01 23:20:04 - train: epoch 0042, iter [04700, 05004], lr: 0.091824, loss: 1.9998
2022-03-01 23:20:37 - train: epoch 0042, iter [04800, 05004], lr: 0.091824, loss: 2.1801
2022-03-01 23:21:10 - train: epoch 0042, iter [04900, 05004], lr: 0.091824, loss: 2.1834
2022-03-01 23:21:42 - train: epoch 0042, iter [05000, 05004], lr: 0.091824, loss: 1.9463
2022-03-01 23:21:43 - train: epoch 042, train_loss: 2.0888
2022-03-01 23:22:57 - eval: epoch: 042, acc1: 54.944%, acc5: 79.782%, test_loss: 1.9178, per_image_load_time: 2.076ms, per_image_inference_time: 0.540ms
2022-03-01 23:22:57 - until epoch: 042, best_acc1: 56.262%
2022-03-01 23:22:57 - epoch 043 lr: 0.09137665423022251
2022-03-01 23:23:35 - train: epoch 0043, iter [00100, 05004], lr: 0.091377, loss: 2.1403
2022-03-01 23:24:08 - train: epoch 0043, iter [00200, 05004], lr: 0.091377, loss: 2.1271
2022-03-01 23:24:41 - train: epoch 0043, iter [00300, 05004], lr: 0.091377, loss: 1.8039
2022-03-01 23:25:14 - train: epoch 0043, iter [00400, 05004], lr: 0.091377, loss: 1.9432
2022-03-01 23:25:47 - train: epoch 0043, iter [00500, 05004], lr: 0.091377, loss: 1.9921
2022-03-01 23:26:20 - train: epoch 0043, iter [00600, 05004], lr: 0.091377, loss: 2.0167
2022-03-01 23:26:52 - train: epoch 0043, iter [00700, 05004], lr: 0.091377, loss: 2.2095
2022-03-01 23:27:26 - train: epoch 0043, iter [00800, 05004], lr: 0.091377, loss: 2.1801
2022-03-01 23:27:59 - train: epoch 0043, iter [00900, 05004], lr: 0.091377, loss: 2.0415
2022-03-01 23:28:32 - train: epoch 0043, iter [01000, 05004], lr: 0.091377, loss: 2.1293
2022-03-01 23:29:05 - train: epoch 0043, iter [01100, 05004], lr: 0.091377, loss: 2.1213
2022-03-01 23:29:39 - train: epoch 0043, iter [01200, 05004], lr: 0.091377, loss: 2.0212
2022-03-01 23:30:11 - train: epoch 0043, iter [01300, 05004], lr: 0.091377, loss: 2.1469
2022-03-01 23:30:45 - train: epoch 0043, iter [01400, 05004], lr: 0.091377, loss: 2.1278
2022-03-01 23:31:18 - train: epoch 0043, iter [01500, 05004], lr: 0.091377, loss: 1.9289
2022-03-01 23:31:52 - train: epoch 0043, iter [01600, 05004], lr: 0.091377, loss: 2.2424
2022-03-01 23:32:25 - train: epoch 0043, iter [01700, 05004], lr: 0.091377, loss: 2.0913
2022-03-01 23:32:58 - train: epoch 0043, iter [01800, 05004], lr: 0.091377, loss: 2.1037
2022-03-01 23:33:32 - train: epoch 0043, iter [01900, 05004], lr: 0.091377, loss: 2.0539
2022-03-01 23:34:05 - train: epoch 0043, iter [02000, 05004], lr: 0.091377, loss: 1.8864
2022-03-01 23:34:38 - train: epoch 0043, iter [02100, 05004], lr: 0.091377, loss: 2.0008
2022-03-01 23:35:11 - train: epoch 0043, iter [02200, 05004], lr: 0.091377, loss: 2.1495
2022-03-01 23:35:44 - train: epoch 0043, iter [02300, 05004], lr: 0.091377, loss: 2.2212
2022-03-01 23:36:17 - train: epoch 0043, iter [02400, 05004], lr: 0.091377, loss: 2.0285
2022-03-01 23:36:51 - train: epoch 0043, iter [02500, 05004], lr: 0.091377, loss: 2.2600
2022-03-01 23:37:24 - train: epoch 0043, iter [02600, 05004], lr: 0.091377, loss: 1.8251
2022-03-01 23:37:57 - train: epoch 0043, iter [02700, 05004], lr: 0.091377, loss: 2.1841
2022-03-01 23:38:30 - train: epoch 0043, iter [02800, 05004], lr: 0.091377, loss: 1.9127
2022-03-01 23:39:03 - train: epoch 0043, iter [02900, 05004], lr: 0.091377, loss: 2.1336
2022-03-01 23:39:36 - train: epoch 0043, iter [03000, 05004], lr: 0.091377, loss: 2.3258
2022-03-01 23:40:08 - train: epoch 0043, iter [03100, 05004], lr: 0.091377, loss: 2.2292
2022-03-01 23:40:41 - train: epoch 0043, iter [03200, 05004], lr: 0.091377, loss: 2.2138
2022-03-01 23:41:14 - train: epoch 0043, iter [03300, 05004], lr: 0.091377, loss: 2.1613
2022-03-01 23:41:47 - train: epoch 0043, iter [03400, 05004], lr: 0.091377, loss: 2.0838
2022-03-01 23:42:21 - train: epoch 0043, iter [03500, 05004], lr: 0.091377, loss: 2.0212
2022-03-01 23:42:54 - train: epoch 0043, iter [03600, 05004], lr: 0.091377, loss: 2.0020
2022-03-01 23:43:27 - train: epoch 0043, iter [03700, 05004], lr: 0.091377, loss: 1.9936
2022-03-01 23:44:01 - train: epoch 0043, iter [03800, 05004], lr: 0.091377, loss: 2.2633
2022-03-01 23:44:33 - train: epoch 0043, iter [03900, 05004], lr: 0.091377, loss: 2.2816
2022-03-01 23:45:07 - train: epoch 0043, iter [04000, 05004], lr: 0.091377, loss: 2.0325
2022-03-01 23:45:40 - train: epoch 0043, iter [04100, 05004], lr: 0.091377, loss: 2.3776
2022-03-01 23:46:12 - train: epoch 0043, iter [04200, 05004], lr: 0.091377, loss: 2.0432
2022-03-01 23:46:46 - train: epoch 0043, iter [04300, 05004], lr: 0.091377, loss: 2.1189
2022-03-01 23:47:18 - train: epoch 0043, iter [04400, 05004], lr: 0.091377, loss: 2.0019
2022-03-01 23:47:52 - train: epoch 0043, iter [04500, 05004], lr: 0.091377, loss: 2.0783
2022-03-01 23:48:25 - train: epoch 0043, iter [04600, 05004], lr: 0.091377, loss: 1.9651
2022-03-01 23:48:58 - train: epoch 0043, iter [04700, 05004], lr: 0.091377, loss: 1.9878
2022-03-01 23:49:31 - train: epoch 0043, iter [04800, 05004], lr: 0.091377, loss: 2.0674
2022-03-01 23:50:05 - train: epoch 0043, iter [04900, 05004], lr: 0.091377, loss: 2.0637
2022-03-01 23:50:37 - train: epoch 0043, iter [05000, 05004], lr: 0.091377, loss: 1.9946
2022-03-01 23:50:38 - train: epoch 043, train_loss: 2.0829
2022-03-01 23:51:52 - eval: epoch: 043, acc1: 56.198%, acc5: 80.974%, test_loss: 1.8436, per_image_load_time: 0.763ms, per_image_inference_time: 0.528ms
2022-03-01 23:51:52 - until epoch: 043, best_acc1: 56.262%
2022-03-01 23:51:52 - epoch 044 lr: 0.09091906224695935
2022-03-01 23:52:30 - train: epoch 0044, iter [00100, 05004], lr: 0.090919, loss: 2.1194
2022-03-01 23:53:03 - train: epoch 0044, iter [00200, 05004], lr: 0.090919, loss: 2.1201
2022-03-01 23:53:36 - train: epoch 0044, iter [00300, 05004], lr: 0.090919, loss: 2.0345
2022-03-01 23:54:09 - train: epoch 0044, iter [00400, 05004], lr: 0.090919, loss: 1.6949
2022-03-01 23:54:42 - train: epoch 0044, iter [00500, 05004], lr: 0.090919, loss: 1.9762
2022-03-01 23:55:14 - train: epoch 0044, iter [00600, 05004], lr: 0.090919, loss: 1.8204
2022-03-01 23:55:48 - train: epoch 0044, iter [00700, 05004], lr: 0.090919, loss: 1.8322
2022-03-01 23:56:21 - train: epoch 0044, iter [00800, 05004], lr: 0.090919, loss: 1.9794
2022-03-01 23:56:54 - train: epoch 0044, iter [00900, 05004], lr: 0.090919, loss: 2.0277
2022-03-01 23:57:28 - train: epoch 0044, iter [01000, 05004], lr: 0.090919, loss: 2.1523
2022-03-01 23:58:00 - train: epoch 0044, iter [01100, 05004], lr: 0.090919, loss: 1.9308
2022-03-01 23:58:35 - train: epoch 0044, iter [01200, 05004], lr: 0.090919, loss: 1.9374
2022-03-01 23:59:08 - train: epoch 0044, iter [01300, 05004], lr: 0.090919, loss: 2.0753
2022-03-01 23:59:41 - train: epoch 0044, iter [01400, 05004], lr: 0.090919, loss: 1.9096
2022-03-02 00:00:14 - train: epoch 0044, iter [01500, 05004], lr: 0.090919, loss: 2.0139
2022-03-02 00:00:47 - train: epoch 0044, iter [01600, 05004], lr: 0.090919, loss: 1.9228
2022-03-02 00:01:20 - train: epoch 0044, iter [01700, 05004], lr: 0.090919, loss: 2.0804
2022-03-02 00:01:53 - train: epoch 0044, iter [01800, 05004], lr: 0.090919, loss: 2.0332
2022-03-02 00:02:26 - train: epoch 0044, iter [01900, 05004], lr: 0.090919, loss: 2.1979
2022-03-02 00:02:59 - train: epoch 0044, iter [02000, 05004], lr: 0.090919, loss: 1.9980
2022-03-02 00:03:33 - train: epoch 0044, iter [02100, 05004], lr: 0.090919, loss: 2.1784
2022-03-02 00:04:06 - train: epoch 0044, iter [02200, 05004], lr: 0.090919, loss: 2.0695
2022-03-02 00:04:39 - train: epoch 0044, iter [02300, 05004], lr: 0.090919, loss: 2.1391
2022-03-02 00:05:12 - train: epoch 0044, iter [02400, 05004], lr: 0.090919, loss: 2.1312
2022-03-02 00:05:46 - train: epoch 0044, iter [02500, 05004], lr: 0.090919, loss: 2.2192
2022-03-02 00:06:19 - train: epoch 0044, iter [02600, 05004], lr: 0.090919, loss: 2.2292
2022-03-02 00:06:52 - train: epoch 0044, iter [02700, 05004], lr: 0.090919, loss: 2.1915
2022-03-02 00:07:26 - train: epoch 0044, iter [02800, 05004], lr: 0.090919, loss: 1.7698
2022-03-02 00:07:59 - train: epoch 0044, iter [02900, 05004], lr: 0.090919, loss: 1.9296
2022-03-02 00:08:31 - train: epoch 0044, iter [03000, 05004], lr: 0.090919, loss: 2.1465
2022-03-02 00:09:05 - train: epoch 0044, iter [03100, 05004], lr: 0.090919, loss: 2.1083
2022-03-02 00:09:37 - train: epoch 0044, iter [03200, 05004], lr: 0.090919, loss: 1.8579
2022-03-02 00:10:11 - train: epoch 0044, iter [03300, 05004], lr: 0.090919, loss: 1.9716
2022-03-02 00:10:44 - train: epoch 0044, iter [03400, 05004], lr: 0.090919, loss: 2.2582
2022-03-02 00:11:18 - train: epoch 0044, iter [03500, 05004], lr: 0.090919, loss: 1.9393
2022-03-02 00:11:50 - train: epoch 0044, iter [03600, 05004], lr: 0.090919, loss: 2.2025
2022-03-02 00:12:23 - train: epoch 0044, iter [03700, 05004], lr: 0.090919, loss: 2.1507
2022-03-02 00:12:56 - train: epoch 0044, iter [03800, 05004], lr: 0.090919, loss: 1.8098
2022-03-02 00:13:29 - train: epoch 0044, iter [03900, 05004], lr: 0.090919, loss: 2.1685
2022-03-02 00:14:03 - train: epoch 0044, iter [04000, 05004], lr: 0.090919, loss: 2.2587
2022-03-02 00:14:37 - train: epoch 0044, iter [04100, 05004], lr: 0.090919, loss: 1.9040
2022-03-02 00:15:09 - train: epoch 0044, iter [04200, 05004], lr: 0.090919, loss: 2.1271
2022-03-02 00:15:42 - train: epoch 0044, iter [04300, 05004], lr: 0.090919, loss: 2.3128
2022-03-02 00:16:15 - train: epoch 0044, iter [04400, 05004], lr: 0.090919, loss: 1.9338
2022-03-02 00:16:49 - train: epoch 0044, iter [04500, 05004], lr: 0.090919, loss: 2.2635
2022-03-02 00:17:22 - train: epoch 0044, iter [04600, 05004], lr: 0.090919, loss: 2.0500
2022-03-02 00:17:55 - train: epoch 0044, iter [04700, 05004], lr: 0.090919, loss: 2.1843
2022-03-02 00:18:28 - train: epoch 0044, iter [04800, 05004], lr: 0.090919, loss: 2.3160
2022-03-02 00:19:02 - train: epoch 0044, iter [04900, 05004], lr: 0.090919, loss: 1.9330
2022-03-02 00:19:33 - train: epoch 0044, iter [05000, 05004], lr: 0.090919, loss: 2.2255
2022-03-02 00:19:34 - train: epoch 044, train_loss: 2.0809
2022-03-02 00:20:48 - eval: epoch: 044, acc1: 55.484%, acc5: 80.552%, test_loss: 1.8932, per_image_load_time: 0.792ms, per_image_inference_time: 0.514ms
2022-03-02 00:20:48 - until epoch: 044, best_acc1: 56.262%
2022-03-02 00:20:48 - epoch 045 lr: 0.09045084971874738
2022-03-02 00:21:27 - train: epoch 0045, iter [00100, 05004], lr: 0.090451, loss: 1.9675
2022-03-02 00:22:00 - train: epoch 0045, iter [00200, 05004], lr: 0.090451, loss: 2.0952
2022-03-02 00:22:33 - train: epoch 0045, iter [00300, 05004], lr: 0.090451, loss: 2.1025
2022-03-02 00:23:05 - train: epoch 0045, iter [00400, 05004], lr: 0.090451, loss: 2.0305
2022-03-02 00:23:39 - train: epoch 0045, iter [00500, 05004], lr: 0.090451, loss: 2.2384
2022-03-02 00:24:12 - train: epoch 0045, iter [00600, 05004], lr: 0.090451, loss: 2.1083
2022-03-02 00:24:46 - train: epoch 0045, iter [00700, 05004], lr: 0.090451, loss: 1.7739
2022-03-02 00:25:18 - train: epoch 0045, iter [00800, 05004], lr: 0.090451, loss: 1.9950
2022-03-02 00:25:51 - train: epoch 0045, iter [00900, 05004], lr: 0.090451, loss: 2.1358
2022-03-02 00:26:24 - train: epoch 0045, iter [01000, 05004], lr: 0.090451, loss: 2.0153
2022-03-02 00:26:58 - train: epoch 0045, iter [01100, 05004], lr: 0.090451, loss: 2.2467
2022-03-02 00:27:31 - train: epoch 0045, iter [01200, 05004], lr: 0.090451, loss: 2.0889
2022-03-02 00:28:04 - train: epoch 0045, iter [01300, 05004], lr: 0.090451, loss: 2.2673
2022-03-02 00:28:37 - train: epoch 0045, iter [01400, 05004], lr: 0.090451, loss: 2.0824
2022-03-02 00:29:10 - train: epoch 0045, iter [01500, 05004], lr: 0.090451, loss: 2.1865
2022-03-02 00:29:43 - train: epoch 0045, iter [01600, 05004], lr: 0.090451, loss: 1.9132
2022-03-02 00:30:17 - train: epoch 0045, iter [01700, 05004], lr: 0.090451, loss: 1.9304
2022-03-02 00:30:49 - train: epoch 0045, iter [01800, 05004], lr: 0.090451, loss: 1.9274
2022-03-02 00:31:23 - train: epoch 0045, iter [01900, 05004], lr: 0.090451, loss: 2.1465
2022-03-02 00:31:56 - train: epoch 0045, iter [02000, 05004], lr: 0.090451, loss: 2.2642
2022-03-02 00:32:28 - train: epoch 0045, iter [02100, 05004], lr: 0.090451, loss: 2.3164
2022-03-02 00:33:02 - train: epoch 0045, iter [02200, 05004], lr: 0.090451, loss: 2.1437
2022-03-02 00:33:35 - train: epoch 0045, iter [02300, 05004], lr: 0.090451, loss: 1.8746
2022-03-02 00:34:08 - train: epoch 0045, iter [02400, 05004], lr: 0.090451, loss: 2.1055
2022-03-02 00:34:41 - train: epoch 0045, iter [02500, 05004], lr: 0.090451, loss: 2.0218
2022-03-02 00:35:14 - train: epoch 0045, iter [02600, 05004], lr: 0.090451, loss: 1.9484
2022-03-02 00:35:47 - train: epoch 0045, iter [02700, 05004], lr: 0.090451, loss: 1.8675
2022-03-02 00:36:20 - train: epoch 0045, iter [02800, 05004], lr: 0.090451, loss: 1.9726
2022-03-02 00:36:52 - train: epoch 0045, iter [02900, 05004], lr: 0.090451, loss: 2.2675
2022-03-02 00:37:26 - train: epoch 0045, iter [03000, 05004], lr: 0.090451, loss: 2.3265
2022-03-02 00:37:59 - train: epoch 0045, iter [03100, 05004], lr: 0.090451, loss: 2.0867
2022-03-02 00:38:33 - train: epoch 0045, iter [03200, 05004], lr: 0.090451, loss: 2.1453
2022-03-02 00:39:06 - train: epoch 0045, iter [03300, 05004], lr: 0.090451, loss: 2.0362
2022-03-02 00:39:38 - train: epoch 0045, iter [03400, 05004], lr: 0.090451, loss: 1.8082
2022-03-02 00:40:12 - train: epoch 0045, iter [03500, 05004], lr: 0.090451, loss: 2.2745
2022-03-02 00:40:45 - train: epoch 0045, iter [03600, 05004], lr: 0.090451, loss: 2.0997
2022-03-02 00:41:18 - train: epoch 0045, iter [03700, 05004], lr: 0.090451, loss: 2.0157
2022-03-02 00:41:50 - train: epoch 0045, iter [03800, 05004], lr: 0.090451, loss: 1.9771
2022-03-02 00:42:24 - train: epoch 0045, iter [03900, 05004], lr: 0.090451, loss: 2.1719
2022-03-02 00:42:57 - train: epoch 0045, iter [04000, 05004], lr: 0.090451, loss: 2.1683
2022-03-02 00:43:31 - train: epoch 0045, iter [04100, 05004], lr: 0.090451, loss: 2.0522
2022-03-02 00:44:04 - train: epoch 0045, iter [04200, 05004], lr: 0.090451, loss: 2.2925
2022-03-02 00:44:37 - train: epoch 0045, iter [04300, 05004], lr: 0.090451, loss: 2.2923
2022-03-02 00:45:10 - train: epoch 0045, iter [04400, 05004], lr: 0.090451, loss: 2.3163
2022-03-02 00:45:44 - train: epoch 0045, iter [04500, 05004], lr: 0.090451, loss: 1.9399
2022-03-02 00:46:17 - train: epoch 0045, iter [04600, 05004], lr: 0.090451, loss: 2.2611
2022-03-02 00:46:50 - train: epoch 0045, iter [04700, 05004], lr: 0.090451, loss: 2.0493
2022-03-02 00:47:23 - train: epoch 0045, iter [04800, 05004], lr: 0.090451, loss: 2.0618
2022-03-02 00:47:56 - train: epoch 0045, iter [04900, 05004], lr: 0.090451, loss: 2.0239
2022-03-02 00:48:27 - train: epoch 0045, iter [05000, 05004], lr: 0.090451, loss: 2.0273
2022-03-02 00:48:29 - train: epoch 045, train_loss: 2.0746
2022-03-02 00:49:42 - eval: epoch: 045, acc1: 54.370%, acc5: 79.518%, test_loss: 1.9416, per_image_load_time: 2.344ms, per_image_inference_time: 0.508ms
2022-03-02 00:49:42 - until epoch: 045, best_acc1: 56.262%
2022-03-02 00:49:42 - epoch 046 lr: 0.08997213817017508
2022-03-02 00:50:21 - train: epoch 0046, iter [00100, 05004], lr: 0.089972, loss: 1.6533
2022-03-02 00:50:54 - train: epoch 0046, iter [00200, 05004], lr: 0.089972, loss: 1.9736
2022-03-02 00:51:27 - train: epoch 0046, iter [00300, 05004], lr: 0.089972, loss: 2.0345
2022-03-02 00:52:01 - train: epoch 0046, iter [00400, 05004], lr: 0.089972, loss: 2.2398
2022-03-02 00:52:33 - train: epoch 0046, iter [00500, 05004], lr: 0.089972, loss: 1.8032
2022-03-02 00:53:07 - train: epoch 0046, iter [00600, 05004], lr: 0.089972, loss: 2.1465
2022-03-02 00:53:41 - train: epoch 0046, iter [00700, 05004], lr: 0.089972, loss: 1.9809
2022-03-02 00:54:14 - train: epoch 0046, iter [00800, 05004], lr: 0.089972, loss: 2.2284
2022-03-02 00:54:47 - train: epoch 0046, iter [00900, 05004], lr: 0.089972, loss: 2.2065
2022-03-02 00:55:20 - train: epoch 0046, iter [01000, 05004], lr: 0.089972, loss: 1.9068
2022-03-02 00:55:52 - train: epoch 0046, iter [01100, 05004], lr: 0.089972, loss: 1.9812
2022-03-02 00:56:26 - train: epoch 0046, iter [01200, 05004], lr: 0.089972, loss: 2.0724
2022-03-02 00:56:59 - train: epoch 0046, iter [01300, 05004], lr: 0.089972, loss: 2.0289
2022-03-02 00:57:32 - train: epoch 0046, iter [01400, 05004], lr: 0.089972, loss: 2.2453
2022-03-02 00:58:05 - train: epoch 0046, iter [01500, 05004], lr: 0.089972, loss: 1.9706
2022-03-02 00:58:39 - train: epoch 0046, iter [01600, 05004], lr: 0.089972, loss: 2.1080
2022-03-02 00:59:12 - train: epoch 0046, iter [01700, 05004], lr: 0.089972, loss: 1.9635
2022-03-02 00:59:46 - train: epoch 0046, iter [01800, 05004], lr: 0.089972, loss: 2.1159
2022-03-02 01:00:19 - train: epoch 0046, iter [01900, 05004], lr: 0.089972, loss: 1.8855
2022-03-02 01:00:52 - train: epoch 0046, iter [02000, 05004], lr: 0.089972, loss: 2.0230
2022-03-02 01:01:25 - train: epoch 0046, iter [02100, 05004], lr: 0.089972, loss: 2.1684
2022-03-02 01:01:59 - train: epoch 0046, iter [02200, 05004], lr: 0.089972, loss: 1.8841
2022-03-02 01:02:31 - train: epoch 0046, iter [02300, 05004], lr: 0.089972, loss: 2.2910
2022-03-02 01:03:04 - train: epoch 0046, iter [02400, 05004], lr: 0.089972, loss: 2.0788
2022-03-02 01:03:38 - train: epoch 0046, iter [02500, 05004], lr: 0.089972, loss: 2.0625
2022-03-02 01:04:11 - train: epoch 0046, iter [02600, 05004], lr: 0.089972, loss: 2.2190
2022-03-02 01:04:45 - train: epoch 0046, iter [02700, 05004], lr: 0.089972, loss: 1.9987
2022-03-02 01:05:18 - train: epoch 0046, iter [02800, 05004], lr: 0.089972, loss: 1.9642
2022-03-02 01:05:51 - train: epoch 0046, iter [02900, 05004], lr: 0.089972, loss: 2.0436
2022-03-02 01:06:23 - train: epoch 0046, iter [03000, 05004], lr: 0.089972, loss: 1.9342
2022-03-02 01:06:58 - train: epoch 0046, iter [03100, 05004], lr: 0.089972, loss: 1.8446
2022-03-02 01:07:31 - train: epoch 0046, iter [03200, 05004], lr: 0.089972, loss: 2.2604
2022-03-02 01:08:04 - train: epoch 0046, iter [03300, 05004], lr: 0.089972, loss: 2.0989
2022-03-02 01:08:37 - train: epoch 0046, iter [03400, 05004], lr: 0.089972, loss: 2.0331
2022-03-02 01:09:10 - train: epoch 0046, iter [03500, 05004], lr: 0.089972, loss: 2.0838
2022-03-02 01:09:43 - train: epoch 0046, iter [03600, 05004], lr: 0.089972, loss: 1.9665
2022-03-02 01:10:16 - train: epoch 0046, iter [03700, 05004], lr: 0.089972, loss: 1.9125
2022-03-02 01:10:49 - train: epoch 0046, iter [03800, 05004], lr: 0.089972, loss: 2.2946
2022-03-02 01:11:22 - train: epoch 0046, iter [03900, 05004], lr: 0.089972, loss: 2.1536
2022-03-02 01:11:56 - train: epoch 0046, iter [04000, 05004], lr: 0.089972, loss: 1.9405
2022-03-02 01:12:29 - train: epoch 0046, iter [04100, 05004], lr: 0.089972, loss: 2.2967
2022-03-02 01:13:02 - train: epoch 0046, iter [04200, 05004], lr: 0.089972, loss: 2.0291
2022-03-02 01:13:36 - train: epoch 0046, iter [04300, 05004], lr: 0.089972, loss: 2.2169
2022-03-02 01:14:09 - train: epoch 0046, iter [04400, 05004], lr: 0.089972, loss: 2.2081
2022-03-02 01:14:42 - train: epoch 0046, iter [04500, 05004], lr: 0.089972, loss: 1.9570
2022-03-02 01:15:16 - train: epoch 0046, iter [04600, 05004], lr: 0.089972, loss: 2.0784
2022-03-02 01:15:49 - train: epoch 0046, iter [04700, 05004], lr: 0.089972, loss: 2.0058
2022-03-02 01:16:21 - train: epoch 0046, iter [04800, 05004], lr: 0.089972, loss: 2.1384
2022-03-02 01:16:54 - train: epoch 0046, iter [04900, 05004], lr: 0.089972, loss: 2.1524
2022-03-02 01:17:26 - train: epoch 0046, iter [05000, 05004], lr: 0.089972, loss: 2.1369
2022-03-02 01:17:27 - train: epoch 046, train_loss: 2.0703
2022-03-02 01:18:42 - eval: epoch: 046, acc1: 55.066%, acc5: 80.452%, test_loss: 1.8942, per_image_load_time: 0.965ms, per_image_inference_time: 0.524ms
2022-03-02 01:18:43 - until epoch: 046, best_acc1: 56.262%
2022-03-02 01:18:43 - epoch 047 lr: 0.08948305185085226
2022-03-02 01:19:20 - train: epoch 0047, iter [00100, 05004], lr: 0.089483, loss: 1.9621
2022-03-02 01:19:53 - train: epoch 0047, iter [00200, 05004], lr: 0.089483, loss: 2.1926
2022-03-02 01:20:26 - train: epoch 0047, iter [00300, 05004], lr: 0.089483, loss: 1.9777
2022-03-02 01:21:00 - train: epoch 0047, iter [00400, 05004], lr: 0.089483, loss: 1.8611
2022-03-02 01:21:33 - train: epoch 0047, iter [00500, 05004], lr: 0.089483, loss: 2.0046
2022-03-02 01:22:06 - train: epoch 0047, iter [00600, 05004], lr: 0.089483, loss: 2.1717
2022-03-02 01:22:39 - train: epoch 0047, iter [00700, 05004], lr: 0.089483, loss: 2.2062
2022-03-02 01:23:12 - train: epoch 0047, iter [00800, 05004], lr: 0.089483, loss: 1.9472
2022-03-02 01:23:44 - train: epoch 0047, iter [00900, 05004], lr: 0.089483, loss: 2.1491
2022-03-02 01:24:18 - train: epoch 0047, iter [01000, 05004], lr: 0.089483, loss: 2.0940
2022-03-02 01:24:51 - train: epoch 0047, iter [01100, 05004], lr: 0.089483, loss: 2.1203
2022-03-02 01:25:23 - train: epoch 0047, iter [01200, 05004], lr: 0.089483, loss: 2.0683
2022-03-02 01:25:57 - train: epoch 0047, iter [01300, 05004], lr: 0.089483, loss: 2.1227
2022-03-02 01:26:30 - train: epoch 0047, iter [01400, 05004], lr: 0.089483, loss: 2.1045
2022-03-02 01:27:03 - train: epoch 0047, iter [01500, 05004], lr: 0.089483, loss: 1.8961
2022-03-02 01:27:36 - train: epoch 0047, iter [01600, 05004], lr: 0.089483, loss: 2.0737
2022-03-02 01:28:10 - train: epoch 0047, iter [01700, 05004], lr: 0.089483, loss: 1.9665
2022-03-02 01:28:43 - train: epoch 0047, iter [01800, 05004], lr: 0.089483, loss: 2.1182
2022-03-02 01:29:17 - train: epoch 0047, iter [01900, 05004], lr: 0.089483, loss: 1.9770
2022-03-02 01:29:49 - train: epoch 0047, iter [02000, 05004], lr: 0.089483, loss: 1.9947
2022-03-02 01:30:23 - train: epoch 0047, iter [02100, 05004], lr: 0.089483, loss: 2.2227
2022-03-02 01:30:56 - train: epoch 0047, iter [02200, 05004], lr: 0.089483, loss: 2.1869
2022-03-02 01:31:29 - train: epoch 0047, iter [02300, 05004], lr: 0.089483, loss: 2.1234
2022-03-02 01:32:03 - train: epoch 0047, iter [02400, 05004], lr: 0.089483, loss: 1.9627
2022-03-02 01:32:35 - train: epoch 0047, iter [02500, 05004], lr: 0.089483, loss: 2.1453
2022-03-02 01:33:08 - train: epoch 0047, iter [02600, 05004], lr: 0.089483, loss: 2.3690
2022-03-02 01:33:43 - train: epoch 0047, iter [02700, 05004], lr: 0.089483, loss: 2.1143
2022-03-02 01:34:16 - train: epoch 0047, iter [02800, 05004], lr: 0.089483, loss: 2.3127
2022-03-02 01:34:49 - train: epoch 0047, iter [02900, 05004], lr: 0.089483, loss: 1.9825
2022-03-02 01:35:22 - train: epoch 0047, iter [03000, 05004], lr: 0.089483, loss: 2.0977
2022-03-02 01:35:56 - train: epoch 0047, iter [03100, 05004], lr: 0.089483, loss: 1.9477
2022-03-02 01:36:29 - train: epoch 0047, iter [03200, 05004], lr: 0.089483, loss: 2.3353
2022-03-02 01:37:02 - train: epoch 0047, iter [03300, 05004], lr: 0.089483, loss: 1.9105
2022-03-02 01:37:35 - train: epoch 0047, iter [03400, 05004], lr: 0.089483, loss: 1.9620
2022-03-02 01:38:08 - train: epoch 0047, iter [03500, 05004], lr: 0.089483, loss: 2.2384
2022-03-02 01:38:41 - train: epoch 0047, iter [03600, 05004], lr: 0.089483, loss: 2.0924
2022-03-02 01:39:14 - train: epoch 0047, iter [03700, 05004], lr: 0.089483, loss: 2.2114
2022-03-02 01:39:48 - train: epoch 0047, iter [03800, 05004], lr: 0.089483, loss: 2.0458
2022-03-02 01:40:20 - train: epoch 0047, iter [03900, 05004], lr: 0.089483, loss: 1.8837
2022-03-02 01:40:54 - train: epoch 0047, iter [04000, 05004], lr: 0.089483, loss: 2.1797
2022-03-02 01:41:27 - train: epoch 0047, iter [04100, 05004], lr: 0.089483, loss: 2.1199
2022-03-02 01:42:00 - train: epoch 0047, iter [04200, 05004], lr: 0.089483, loss: 2.0420
2022-03-02 01:42:34 - train: epoch 0047, iter [04300, 05004], lr: 0.089483, loss: 1.9404
2022-03-02 01:43:07 - train: epoch 0047, iter [04400, 05004], lr: 0.089483, loss: 2.0607
2022-03-02 01:43:40 - train: epoch 0047, iter [04500, 05004], lr: 0.089483, loss: 2.1164
2022-03-02 01:44:12 - train: epoch 0047, iter [04600, 05004], lr: 0.089483, loss: 2.1163
2022-03-02 01:44:46 - train: epoch 0047, iter [04700, 05004], lr: 0.089483, loss: 2.1599
2022-03-02 01:45:19 - train: epoch 0047, iter [04800, 05004], lr: 0.089483, loss: 2.0310
2022-03-02 01:45:52 - train: epoch 0047, iter [04900, 05004], lr: 0.089483, loss: 2.2471
2022-03-02 01:46:24 - train: epoch 0047, iter [05000, 05004], lr: 0.089483, loss: 1.9159
2022-03-02 01:46:25 - train: epoch 047, train_loss: 2.0691
2022-03-02 01:47:38 - eval: epoch: 047, acc1: 52.640%, acc5: 78.184%, test_loss: 2.0412, per_image_load_time: 0.887ms, per_image_inference_time: 0.510ms
2022-03-02 01:47:39 - until epoch: 047, best_acc1: 56.262%
2022-03-02 01:47:39 - epoch 048 lr: 0.08898371770316112
2022-03-02 01:48:17 - train: epoch 0048, iter [00100, 05004], lr: 0.088984, loss: 2.2139
2022-03-02 01:48:49 - train: epoch 0048, iter [00200, 05004], lr: 0.088984, loss: 2.3884
2022-03-02 01:49:22 - train: epoch 0048, iter [00300, 05004], lr: 0.088984, loss: 2.0470
2022-03-02 01:49:56 - train: epoch 0048, iter [00400, 05004], lr: 0.088984, loss: 1.9726
2022-03-02 01:50:29 - train: epoch 0048, iter [00500, 05004], lr: 0.088984, loss: 1.9314
2022-03-02 01:51:02 - train: epoch 0048, iter [00600, 05004], lr: 0.088984, loss: 2.0329
2022-03-02 01:51:36 - train: epoch 0048, iter [00700, 05004], lr: 0.088984, loss: 2.0226
2022-03-02 01:52:09 - train: epoch 0048, iter [00800, 05004], lr: 0.088984, loss: 2.1044
2022-03-02 01:52:42 - train: epoch 0048, iter [00900, 05004], lr: 0.088984, loss: 2.2194
2022-03-02 01:53:15 - train: epoch 0048, iter [01000, 05004], lr: 0.088984, loss: 2.1440
2022-03-02 01:53:48 - train: epoch 0048, iter [01100, 05004], lr: 0.088984, loss: 2.0927
2022-03-02 01:54:21 - train: epoch 0048, iter [01200, 05004], lr: 0.088984, loss: 2.1475
2022-03-02 01:54:54 - train: epoch 0048, iter [01300, 05004], lr: 0.088984, loss: 1.8654
2022-03-02 01:55:27 - train: epoch 0048, iter [01400, 05004], lr: 0.088984, loss: 2.1069
2022-03-02 01:56:00 - train: epoch 0048, iter [01500, 05004], lr: 0.088984, loss: 2.0898
2022-03-02 01:56:34 - train: epoch 0048, iter [01600, 05004], lr: 0.088984, loss: 1.9890
2022-03-02 01:57:06 - train: epoch 0048, iter [01700, 05004], lr: 0.088984, loss: 2.1890
2022-03-02 01:57:40 - train: epoch 0048, iter [01800, 05004], lr: 0.088984, loss: 2.1089
2022-03-02 01:58:13 - train: epoch 0048, iter [01900, 05004], lr: 0.088984, loss: 2.1281
2022-03-02 01:58:46 - train: epoch 0048, iter [02000, 05004], lr: 0.088984, loss: 2.2810
2022-03-02 01:59:19 - train: epoch 0048, iter [02100, 05004], lr: 0.088984, loss: 2.0091
2022-03-02 01:59:52 - train: epoch 0048, iter [02200, 05004], lr: 0.088984, loss: 2.3639
2022-03-02 02:00:25 - train: epoch 0048, iter [02300, 05004], lr: 0.088984, loss: 1.9287
2022-03-02 02:00:58 - train: epoch 0048, iter [02400, 05004], lr: 0.088984, loss: 2.1755
2022-03-02 02:01:30 - train: epoch 0048, iter [02500, 05004], lr: 0.088984, loss: 2.0540
2022-03-02 02:02:04 - train: epoch 0048, iter [02600, 05004], lr: 0.088984, loss: 2.3003
2022-03-02 02:02:37 - train: epoch 0048, iter [02700, 05004], lr: 0.088984, loss: 2.2691
2022-03-02 02:03:10 - train: epoch 0048, iter [02800, 05004], lr: 0.088984, loss: 2.0444
2022-03-02 02:03:43 - train: epoch 0048, iter [02900, 05004], lr: 0.088984, loss: 2.1914
2022-03-02 02:04:16 - train: epoch 0048, iter [03000, 05004], lr: 0.088984, loss: 2.0234
2022-03-02 02:04:49 - train: epoch 0048, iter [03100, 05004], lr: 0.088984, loss: 2.0525
2022-03-02 02:05:22 - train: epoch 0048, iter [03200, 05004], lr: 0.088984, loss: 1.9400
2022-03-02 02:05:55 - train: epoch 0048, iter [03300, 05004], lr: 0.088984, loss: 2.1867
2022-03-02 02:06:28 - train: epoch 0048, iter [03400, 05004], lr: 0.088984, loss: 2.1374
2022-03-02 02:07:01 - train: epoch 0048, iter [03500, 05004], lr: 0.088984, loss: 2.2937
2022-03-02 02:07:35 - train: epoch 0048, iter [03600, 05004], lr: 0.088984, loss: 2.0323
2022-03-02 02:08:08 - train: epoch 0048, iter [03700, 05004], lr: 0.088984, loss: 2.1729
2022-03-02 02:08:41 - train: epoch 0048, iter [03800, 05004], lr: 0.088984, loss: 2.0597
2022-03-02 02:09:14 - train: epoch 0048, iter [03900, 05004], lr: 0.088984, loss: 2.1324
2022-03-02 02:09:47 - train: epoch 0048, iter [04000, 05004], lr: 0.088984, loss: 1.9403
2022-03-02 02:10:20 - train: epoch 0048, iter [04100, 05004], lr: 0.088984, loss: 2.3534
2022-03-02 02:10:53 - train: epoch 0048, iter [04200, 05004], lr: 0.088984, loss: 1.9123
2022-03-02 02:11:26 - train: epoch 0048, iter [04300, 05004], lr: 0.088984, loss: 2.1000
2022-03-02 02:12:00 - train: epoch 0048, iter [04400, 05004], lr: 0.088984, loss: 1.9084
2022-03-02 02:12:33 - train: epoch 0048, iter [04500, 05004], lr: 0.088984, loss: 2.2376
2022-03-02 02:13:06 - train: epoch 0048, iter [04600, 05004], lr: 0.088984, loss: 2.1416
2022-03-02 02:13:39 - train: epoch 0048, iter [04700, 05004], lr: 0.088984, loss: 2.1230
2022-03-02 02:14:12 - train: epoch 0048, iter [04800, 05004], lr: 0.088984, loss: 2.3249
2022-03-02 02:14:45 - train: epoch 0048, iter [04900, 05004], lr: 0.088984, loss: 2.1730
2022-03-02 02:15:17 - train: epoch 0048, iter [05000, 05004], lr: 0.088984, loss: 1.9929
2022-03-02 02:15:18 - train: epoch 048, train_loss: 2.0679
2022-03-02 02:16:31 - eval: epoch: 048, acc1: 56.984%, acc5: 81.692%, test_loss: 1.8109, per_image_load_time: 2.342ms, per_image_inference_time: 0.536ms
2022-03-02 02:16:32 - until epoch: 048, best_acc1: 56.984%
2022-03-02 02:16:32 - epoch 049 lr: 0.0884742653293083
2022-03-02 02:17:11 - train: epoch 0049, iter [00100, 05004], lr: 0.088474, loss: 2.2356
2022-03-02 02:17:43 - train: epoch 0049, iter [00200, 05004], lr: 0.088474, loss: 1.9551
2022-03-02 02:18:17 - train: epoch 0049, iter [00300, 05004], lr: 0.088474, loss: 2.1208
2022-03-02 02:18:50 - train: epoch 0049, iter [00400, 05004], lr: 0.088474, loss: 2.0877
2022-03-02 02:19:23 - train: epoch 0049, iter [00500, 05004], lr: 0.088474, loss: 2.0556
2022-03-02 02:19:56 - train: epoch 0049, iter [00600, 05004], lr: 0.088474, loss: 2.0388
2022-03-02 02:20:29 - train: epoch 0049, iter [00700, 05004], lr: 0.088474, loss: 2.0265
2022-03-02 02:21:02 - train: epoch 0049, iter [00800, 05004], lr: 0.088474, loss: 2.4403
2022-03-02 02:21:36 - train: epoch 0049, iter [00900, 05004], lr: 0.088474, loss: 1.9189
2022-03-02 02:22:09 - train: epoch 0049, iter [01000, 05004], lr: 0.088474, loss: 2.1230
2022-03-02 02:22:43 - train: epoch 0049, iter [01100, 05004], lr: 0.088474, loss: 1.8321
2022-03-02 02:23:16 - train: epoch 0049, iter [01200, 05004], lr: 0.088474, loss: 1.8400
2022-03-02 02:23:49 - train: epoch 0049, iter [01300, 05004], lr: 0.088474, loss: 2.0993
2022-03-02 02:24:22 - train: epoch 0049, iter [01400, 05004], lr: 0.088474, loss: 2.1747
2022-03-02 02:24:55 - train: epoch 0049, iter [01500, 05004], lr: 0.088474, loss: 2.0824
2022-03-02 02:25:28 - train: epoch 0049, iter [01600, 05004], lr: 0.088474, loss: 2.2664
2022-03-02 02:26:02 - train: epoch 0049, iter [01700, 05004], lr: 0.088474, loss: 2.1638
2022-03-02 02:26:35 - train: epoch 0049, iter [01800, 05004], lr: 0.088474, loss: 1.9192
2022-03-02 02:27:08 - train: epoch 0049, iter [01900, 05004], lr: 0.088474, loss: 2.0417
2022-03-02 02:27:41 - train: epoch 0049, iter [02000, 05004], lr: 0.088474, loss: 1.9026
2022-03-02 02:28:14 - train: epoch 0049, iter [02100, 05004], lr: 0.088474, loss: 1.9157
2022-03-02 02:28:48 - train: epoch 0049, iter [02200, 05004], lr: 0.088474, loss: 1.9961
2022-03-02 02:29:20 - train: epoch 0049, iter [02300, 05004], lr: 0.088474, loss: 1.9039
2022-03-02 02:29:53 - train: epoch 0049, iter [02400, 05004], lr: 0.088474, loss: 2.1509
2022-03-02 02:30:26 - train: epoch 0049, iter [02500, 05004], lr: 0.088474, loss: 2.1577
2022-03-02 02:30:59 - train: epoch 0049, iter [02600, 05004], lr: 0.088474, loss: 2.0096
2022-03-02 02:31:33 - train: epoch 0049, iter [02700, 05004], lr: 0.088474, loss: 1.8706
2022-03-02 02:32:05 - train: epoch 0049, iter [02800, 05004], lr: 0.088474, loss: 2.0825
2022-03-02 02:32:38 - train: epoch 0049, iter [02900, 05004], lr: 0.088474, loss: 2.1648
2022-03-02 02:33:11 - train: epoch 0049, iter [03000, 05004], lr: 0.088474, loss: 2.0556
2022-03-02 02:33:45 - train: epoch 0049, iter [03100, 05004], lr: 0.088474, loss: 2.1394
2022-03-02 02:34:17 - train: epoch 0049, iter [03200, 05004], lr: 0.088474, loss: 2.1242
2022-03-02 02:34:50 - train: epoch 0049, iter [03300, 05004], lr: 0.088474, loss: 1.9816
2022-03-02 02:35:23 - train: epoch 0049, iter [03400, 05004], lr: 0.088474, loss: 2.1827
2022-03-02 02:35:56 - train: epoch 0049, iter [03500, 05004], lr: 0.088474, loss: 2.1392
2022-03-02 02:36:29 - train: epoch 0049, iter [03600, 05004], lr: 0.088474, loss: 2.2807
2022-03-02 02:37:03 - train: epoch 0049, iter [03700, 05004], lr: 0.088474, loss: 2.0583
2022-03-02 02:37:35 - train: epoch 0049, iter [03800, 05004], lr: 0.088474, loss: 2.2503
2022-03-02 02:38:09 - train: epoch 0049, iter [03900, 05004], lr: 0.088474, loss: 2.3076
2022-03-02 02:38:42 - train: epoch 0049, iter [04000, 05004], lr: 0.088474, loss: 1.9924
2022-03-02 02:39:14 - train: epoch 0049, iter [04100, 05004], lr: 0.088474, loss: 1.8907
2022-03-02 02:39:47 - train: epoch 0049, iter [04200, 05004], lr: 0.088474, loss: 2.1337
2022-03-02 02:40:20 - train: epoch 0049, iter [04300, 05004], lr: 0.088474, loss: 2.3847
2022-03-02 02:40:53 - train: epoch 0049, iter [04400, 05004], lr: 0.088474, loss: 2.1103
2022-03-02 02:41:26 - train: epoch 0049, iter [04500, 05004], lr: 0.088474, loss: 2.0145
2022-03-02 02:41:58 - train: epoch 0049, iter [04600, 05004], lr: 0.088474, loss: 2.1917
2022-03-02 02:42:32 - train: epoch 0049, iter [04700, 05004], lr: 0.088474, loss: 2.1471
2022-03-02 02:43:06 - train: epoch 0049, iter [04800, 05004], lr: 0.088474, loss: 1.8974
2022-03-02 02:43:38 - train: epoch 0049, iter [04900, 05004], lr: 0.088474, loss: 1.9701
2022-03-02 02:44:10 - train: epoch 0049, iter [05000, 05004], lr: 0.088474, loss: 1.8683
2022-03-02 02:44:11 - train: epoch 049, train_loss: 2.0598
2022-03-02 02:45:25 - eval: epoch: 049, acc1: 55.878%, acc5: 80.812%, test_loss: 1.8594, per_image_load_time: 0.622ms, per_image_inference_time: 0.499ms
2022-03-02 02:45:25 - until epoch: 049, best_acc1: 56.984%
2022-03-02 02:45:25 - epoch 050 lr: 0.08795482695768658
2022-03-02 02:46:04 - train: epoch 0050, iter [00100, 05004], lr: 0.087955, loss: 2.3029
2022-03-02 02:46:36 - train: epoch 0050, iter [00200, 05004], lr: 0.087955, loss: 2.0081
2022-03-02 02:47:09 - train: epoch 0050, iter [00300, 05004], lr: 0.087955, loss: 2.1469
2022-03-02 02:47:42 - train: epoch 0050, iter [00400, 05004], lr: 0.087955, loss: 1.9325
2022-03-02 02:48:16 - train: epoch 0050, iter [00500, 05004], lr: 0.087955, loss: 2.1052
2022-03-02 02:48:49 - train: epoch 0050, iter [00600, 05004], lr: 0.087955, loss: 2.1784
2022-03-02 02:49:22 - train: epoch 0050, iter [00700, 05004], lr: 0.087955, loss: 1.9086
2022-03-02 02:49:56 - train: epoch 0050, iter [00800, 05004], lr: 0.087955, loss: 1.7854
2022-03-02 02:50:28 - train: epoch 0050, iter [00900, 05004], lr: 0.087955, loss: 1.8619
2022-03-02 02:51:02 - train: epoch 0050, iter [01000, 05004], lr: 0.087955, loss: 2.2838
2022-03-02 02:51:36 - train: epoch 0050, iter [01100, 05004], lr: 0.087955, loss: 2.2396
2022-03-02 02:52:08 - train: epoch 0050, iter [01200, 05004], lr: 0.087955, loss: 2.0544
2022-03-02 02:52:43 - train: epoch 0050, iter [01300, 05004], lr: 0.087955, loss: 1.9077
2022-03-02 02:53:16 - train: epoch 0050, iter [01400, 05004], lr: 0.087955, loss: 2.1044
2022-03-02 02:53:49 - train: epoch 0050, iter [01500, 05004], lr: 0.087955, loss: 2.0549
2022-03-02 02:54:22 - train: epoch 0050, iter [01600, 05004], lr: 0.087955, loss: 2.0590
2022-03-02 02:54:55 - train: epoch 0050, iter [01700, 05004], lr: 0.087955, loss: 2.1941
2022-03-02 02:55:28 - train: epoch 0050, iter [01800, 05004], lr: 0.087955, loss: 2.1060
2022-03-02 02:56:02 - train: epoch 0050, iter [01900, 05004], lr: 0.087955, loss: 2.0619
2022-03-02 02:56:35 - train: epoch 0050, iter [02000, 05004], lr: 0.087955, loss: 2.0711
2022-03-02 02:57:09 - train: epoch 0050, iter [02100, 05004], lr: 0.087955, loss: 1.6605
2022-03-02 02:57:41 - train: epoch 0050, iter [02200, 05004], lr: 0.087955, loss: 2.0670
2022-03-02 02:58:15 - train: epoch 0050, iter [02300, 05004], lr: 0.087955, loss: 1.9911
2022-03-02 02:58:48 - train: epoch 0050, iter [02400, 05004], lr: 0.087955, loss: 2.0142
2022-03-02 02:59:21 - train: epoch 0050, iter [02500, 05004], lr: 0.087955, loss: 2.1647
2022-03-02 02:59:53 - train: epoch 0050, iter [02600, 05004], lr: 0.087955, loss: 1.9598
2022-03-02 03:00:27 - train: epoch 0050, iter [02700, 05004], lr: 0.087955, loss: 2.0708
2022-03-02 03:01:00 - train: epoch 0050, iter [02800, 05004], lr: 0.087955, loss: 2.1869
2022-03-02 03:01:33 - train: epoch 0050, iter [02900, 05004], lr: 0.087955, loss: 2.3556
2022-03-02 03:02:06 - train: epoch 0050, iter [03000, 05004], lr: 0.087955, loss: 2.2383
2022-03-02 03:02:39 - train: epoch 0050, iter [03100, 05004], lr: 0.087955, loss: 2.0007
2022-03-02 03:03:13 - train: epoch 0050, iter [03200, 05004], lr: 0.087955, loss: 1.9757
2022-03-02 03:03:46 - train: epoch 0050, iter [03300, 05004], lr: 0.087955, loss: 1.9435
2022-03-02 03:04:19 - train: epoch 0050, iter [03400, 05004], lr: 0.087955, loss: 1.9001
2022-03-02 03:04:53 - train: epoch 0050, iter [03500, 05004], lr: 0.087955, loss: 1.9710
2022-03-02 03:05:26 - train: epoch 0050, iter [03600, 05004], lr: 0.087955, loss: 1.9819
2022-03-02 03:05:58 - train: epoch 0050, iter [03700, 05004], lr: 0.087955, loss: 2.1684
2022-03-02 03:06:31 - train: epoch 0050, iter [03800, 05004], lr: 0.087955, loss: 1.9155
2022-03-02 03:07:05 - train: epoch 0050, iter [03900, 05004], lr: 0.087955, loss: 1.8924
2022-03-02 03:07:37 - train: epoch 0050, iter [04000, 05004], lr: 0.087955, loss: 2.2096
2022-03-02 03:08:11 - train: epoch 0050, iter [04100, 05004], lr: 0.087955, loss: 2.0555
2022-03-02 03:08:43 - train: epoch 0050, iter [04200, 05004], lr: 0.087955, loss: 2.0623
2022-03-02 03:09:16 - train: epoch 0050, iter [04300, 05004], lr: 0.087955, loss: 2.0593
2022-03-02 03:09:50 - train: epoch 0050, iter [04400, 05004], lr: 0.087955, loss: 1.9905
2022-03-02 03:10:22 - train: epoch 0050, iter [04500, 05004], lr: 0.087955, loss: 2.0255
2022-03-02 03:10:55 - train: epoch 0050, iter [04600, 05004], lr: 0.087955, loss: 2.0525
2022-03-02 03:11:28 - train: epoch 0050, iter [04700, 05004], lr: 0.087955, loss: 2.0878
2022-03-02 03:12:01 - train: epoch 0050, iter [04800, 05004], lr: 0.087955, loss: 1.6953
2022-03-02 03:12:33 - train: epoch 0050, iter [04900, 05004], lr: 0.087955, loss: 1.8493
2022-03-02 03:13:06 - train: epoch 0050, iter [05000, 05004], lr: 0.087955, loss: 2.0254
2022-03-02 03:13:07 - train: epoch 050, train_loss: 2.0551
2022-03-02 03:14:20 - eval: epoch: 050, acc1: 57.046%, acc5: 81.702%, test_loss: 1.7980, per_image_load_time: 2.265ms, per_image_inference_time: 0.505ms
2022-03-02 03:14:21 - until epoch: 050, best_acc1: 57.046%
2022-03-02 03:14:21 - epoch 051 lr: 0.08742553740855506
2022-03-02 03:15:00 - train: epoch 0051, iter [00100, 05004], lr: 0.087426, loss: 2.3381
2022-03-02 03:15:32 - train: epoch 0051, iter [00200, 05004], lr: 0.087426, loss: 2.4266
2022-03-02 03:16:05 - train: epoch 0051, iter [00300, 05004], lr: 0.087426, loss: 2.0801
2022-03-02 03:16:38 - train: epoch 0051, iter [00400, 05004], lr: 0.087426, loss: 1.8144
2022-03-02 03:17:11 - train: epoch 0051, iter [00500, 05004], lr: 0.087426, loss: 2.0990
2022-03-02 03:17:44 - train: epoch 0051, iter [00600, 05004], lr: 0.087426, loss: 2.0034
2022-03-02 03:18:17 - train: epoch 0051, iter [00700, 05004], lr: 0.087426, loss: 2.0469
2022-03-02 03:18:50 - train: epoch 0051, iter [00800, 05004], lr: 0.087426, loss: 2.3548
2022-03-02 03:19:23 - train: epoch 0051, iter [00900, 05004], lr: 0.087426, loss: 1.8829
2022-03-02 03:19:56 - train: epoch 0051, iter [01000, 05004], lr: 0.087426, loss: 2.3869
2022-03-02 03:20:30 - train: epoch 0051, iter [01100, 05004], lr: 0.087426, loss: 2.2034
2022-03-02 03:21:03 - train: epoch 0051, iter [01200, 05004], lr: 0.087426, loss: 2.0313
2022-03-02 03:21:36 - train: epoch 0051, iter [01300, 05004], lr: 0.087426, loss: 1.7914
2022-03-02 03:22:09 - train: epoch 0051, iter [01400, 05004], lr: 0.087426, loss: 1.9888
2022-03-02 03:22:42 - train: epoch 0051, iter [01500, 05004], lr: 0.087426, loss: 1.9956
2022-03-02 03:23:16 - train: epoch 0051, iter [01600, 05004], lr: 0.087426, loss: 1.8301
2022-03-02 03:23:49 - train: epoch 0051, iter [01700, 05004], lr: 0.087426, loss: 2.1754
2022-03-02 03:24:22 - train: epoch 0051, iter [01800, 05004], lr: 0.087426, loss: 1.9098
2022-03-02 03:24:55 - train: epoch 0051, iter [01900, 05004], lr: 0.087426, loss: 1.7875
2022-03-02 03:25:28 - train: epoch 0051, iter [02000, 05004], lr: 0.087426, loss: 2.0799
2022-03-02 03:26:01 - train: epoch 0051, iter [02100, 05004], lr: 0.087426, loss: 2.1046
2022-03-02 03:26:34 - train: epoch 0051, iter [02200, 05004], lr: 0.087426, loss: 2.1267
2022-03-02 03:27:07 - train: epoch 0051, iter [02300, 05004], lr: 0.087426, loss: 2.2640
2022-03-02 03:27:40 - train: epoch 0051, iter [02400, 05004], lr: 0.087426, loss: 2.1400
2022-03-02 03:28:14 - train: epoch 0051, iter [02500, 05004], lr: 0.087426, loss: 1.9987
2022-03-02 03:28:47 - train: epoch 0051, iter [02600, 05004], lr: 0.087426, loss: 1.8622
2022-03-02 03:29:20 - train: epoch 0051, iter [02700, 05004], lr: 0.087426, loss: 2.0826
2022-03-02 03:29:53 - train: epoch 0051, iter [02800, 05004], lr: 0.087426, loss: 1.9251
2022-03-02 03:30:27 - train: epoch 0051, iter [02900, 05004], lr: 0.087426, loss: 2.0066
2022-03-02 03:31:00 - train: epoch 0051, iter [03000, 05004], lr: 0.087426, loss: 2.0250
2022-03-02 03:31:32 - train: epoch 0051, iter [03100, 05004], lr: 0.087426, loss: 1.9576
2022-03-02 03:32:05 - train: epoch 0051, iter [03200, 05004], lr: 0.087426, loss: 2.0900
2022-03-02 03:32:39 - train: epoch 0051, iter [03300, 05004], lr: 0.087426, loss: 2.1397
2022-03-02 03:33:11 - train: epoch 0051, iter [03400, 05004], lr: 0.087426, loss: 2.0731
2022-03-02 03:33:45 - train: epoch 0051, iter [03500, 05004], lr: 0.087426, loss: 1.9100
2022-03-02 03:34:18 - train: epoch 0051, iter [03600, 05004], lr: 0.087426, loss: 2.0012
2022-03-02 03:34:51 - train: epoch 0051, iter [03700, 05004], lr: 0.087426, loss: 2.1944
2022-03-02 03:35:24 - train: epoch 0051, iter [03800, 05004], lr: 0.087426, loss: 2.0827
2022-03-02 03:35:56 - train: epoch 0051, iter [03900, 05004], lr: 0.087426, loss: 2.1825
2022-03-02 03:36:29 - train: epoch 0051, iter [04000, 05004], lr: 0.087426, loss: 2.1433
2022-03-02 03:37:03 - train: epoch 0051, iter [04100, 05004], lr: 0.087426, loss: 2.1795
2022-03-02 03:37:35 - train: epoch 0051, iter [04200, 05004], lr: 0.087426, loss: 2.2062
2022-03-02 03:38:08 - train: epoch 0051, iter [04300, 05004], lr: 0.087426, loss: 1.9340
2022-03-02 03:38:42 - train: epoch 0051, iter [04400, 05004], lr: 0.087426, loss: 2.1305
2022-03-02 03:39:15 - train: epoch 0051, iter [04500, 05004], lr: 0.087426, loss: 1.8263
2022-03-02 03:39:48 - train: epoch 0051, iter [04600, 05004], lr: 0.087426, loss: 2.0482
2022-03-02 03:40:22 - train: epoch 0051, iter [04700, 05004], lr: 0.087426, loss: 2.2127
2022-03-02 03:40:56 - train: epoch 0051, iter [04800, 05004], lr: 0.087426, loss: 2.0130
2022-03-02 03:41:29 - train: epoch 0051, iter [04900, 05004], lr: 0.087426, loss: 2.1901
2022-03-02 03:42:00 - train: epoch 0051, iter [05000, 05004], lr: 0.087426, loss: 1.9585
2022-03-02 03:42:01 - train: epoch 051, train_loss: 2.0526
2022-03-02 03:43:15 - eval: epoch: 051, acc1: 57.110%, acc5: 81.780%, test_loss: 1.7891, per_image_load_time: 2.383ms, per_image_inference_time: 0.497ms
2022-03-02 03:43:16 - until epoch: 051, best_acc1: 57.110%
2022-03-02 03:43:16 - epoch 052 lr: 0.08688653405904652
2022-03-02 03:43:54 - train: epoch 0052, iter [00100, 05004], lr: 0.086887, loss: 1.8877
2022-03-02 03:44:26 - train: epoch 0052, iter [00200, 05004], lr: 0.086887, loss: 2.0363
2022-03-02 03:45:00 - train: epoch 0052, iter [00300, 05004], lr: 0.086887, loss: 2.0738
2022-03-02 03:45:33 - train: epoch 0052, iter [00400, 05004], lr: 0.086887, loss: 2.1612
2022-03-02 03:46:07 - train: epoch 0052, iter [00500, 05004], lr: 0.086887, loss: 2.1147
2022-03-02 03:46:40 - train: epoch 0052, iter [00600, 05004], lr: 0.086887, loss: 2.0283
2022-03-02 03:47:13 - train: epoch 0052, iter [00700, 05004], lr: 0.086887, loss: 2.1355
2022-03-02 03:47:46 - train: epoch 0052, iter [00800, 05004], lr: 0.086887, loss: 1.9574
2022-03-02 03:48:20 - train: epoch 0052, iter [00900, 05004], lr: 0.086887, loss: 2.0661
2022-03-02 03:48:52 - train: epoch 0052, iter [01000, 05004], lr: 0.086887, loss: 2.3141
2022-03-02 03:49:26 - train: epoch 0052, iter [01100, 05004], lr: 0.086887, loss: 2.0135
2022-03-02 03:50:00 - train: epoch 0052, iter [01200, 05004], lr: 0.086887, loss: 1.9191
2022-03-02 03:50:33 - train: epoch 0052, iter [01300, 05004], lr: 0.086887, loss: 1.9782
2022-03-02 03:51:06 - train: epoch 0052, iter [01400, 05004], lr: 0.086887, loss: 2.3295
2022-03-02 03:51:39 - train: epoch 0052, iter [01500, 05004], lr: 0.086887, loss: 1.8164
2022-03-02 03:52:12 - train: epoch 0052, iter [01600, 05004], lr: 0.086887, loss: 1.9002
2022-03-02 03:52:45 - train: epoch 0052, iter [01700, 05004], lr: 0.086887, loss: 1.8419
2022-03-02 03:53:18 - train: epoch 0052, iter [01800, 05004], lr: 0.086887, loss: 1.8404
2022-03-02 03:53:52 - train: epoch 0052, iter [01900, 05004], lr: 0.086887, loss: 1.9505
2022-03-02 03:54:25 - train: epoch 0052, iter [02000, 05004], lr: 0.086887, loss: 2.3035
2022-03-02 03:54:57 - train: epoch 0052, iter [02100, 05004], lr: 0.086887, loss: 1.9973
2022-03-02 03:55:31 - train: epoch 0052, iter [02200, 05004], lr: 0.086887, loss: 2.1639
2022-03-02 03:56:04 - train: epoch 0052, iter [02300, 05004], lr: 0.086887, loss: 1.8296
2022-03-02 03:56:37 - train: epoch 0052, iter [02400, 05004], lr: 0.086887, loss: 1.9035
2022-03-02 03:57:10 - train: epoch 0052, iter [02500, 05004], lr: 0.086887, loss: 1.8733
2022-03-02 03:57:44 - train: epoch 0052, iter [02600, 05004], lr: 0.086887, loss: 1.8541
2022-03-02 03:58:16 - train: epoch 0052, iter [02700, 05004], lr: 0.086887, loss: 1.9561
2022-03-02 03:58:50 - train: epoch 0052, iter [02800, 05004], lr: 0.086887, loss: 2.0641
2022-03-02 03:59:24 - train: epoch 0052, iter [02900, 05004], lr: 0.086887, loss: 1.8779
2022-03-02 03:59:57 - train: epoch 0052, iter [03000, 05004], lr: 0.086887, loss: 2.0513
2022-03-02 04:00:29 - train: epoch 0052, iter [03100, 05004], lr: 0.086887, loss: 2.2184
2022-03-02 04:01:03 - train: epoch 0052, iter [03200, 05004], lr: 0.086887, loss: 2.1471
2022-03-02 04:01:36 - train: epoch 0052, iter [03300, 05004], lr: 0.086887, loss: 1.9734
2022-03-02 04:02:09 - train: epoch 0052, iter [03400, 05004], lr: 0.086887, loss: 2.1504
2022-03-02 04:02:42 - train: epoch 0052, iter [03500, 05004], lr: 0.086887, loss: 2.1189
2022-03-02 04:03:15 - train: epoch 0052, iter [03600, 05004], lr: 0.086887, loss: 2.1630
2022-03-02 04:03:48 - train: epoch 0052, iter [03700, 05004], lr: 0.086887, loss: 2.2567
2022-03-02 04:04:22 - train: epoch 0052, iter [03800, 05004], lr: 0.086887, loss: 1.9605
2022-03-02 04:04:54 - train: epoch 0052, iter [03900, 05004], lr: 0.086887, loss: 2.0526
2022-03-02 04:05:27 - train: epoch 0052, iter [04000, 05004], lr: 0.086887, loss: 2.2461
2022-03-02 04:06:01 - train: epoch 0052, iter [04100, 05004], lr: 0.086887, loss: 1.8296
2022-03-02 04:06:34 - train: epoch 0052, iter [04200, 05004], lr: 0.086887, loss: 1.9468
2022-03-02 04:07:07 - train: epoch 0052, iter [04300, 05004], lr: 0.086887, loss: 2.1302
2022-03-02 04:07:40 - train: epoch 0052, iter [04400, 05004], lr: 0.086887, loss: 2.1834
2022-03-02 04:08:13 - train: epoch 0052, iter [04500, 05004], lr: 0.086887, loss: 1.8885
2022-03-02 04:08:47 - train: epoch 0052, iter [04600, 05004], lr: 0.086887, loss: 2.0938
2022-03-02 04:09:20 - train: epoch 0052, iter [04700, 05004], lr: 0.086887, loss: 2.2360
2022-03-02 04:09:53 - train: epoch 0052, iter [04800, 05004], lr: 0.086887, loss: 1.9394
2022-03-02 04:10:26 - train: epoch 0052, iter [04900, 05004], lr: 0.086887, loss: 1.9496
2022-03-02 04:10:58 - train: epoch 0052, iter [05000, 05004], lr: 0.086887, loss: 2.0257
2022-03-02 04:10:59 - train: epoch 052, train_loss: 2.0489
2022-03-02 04:12:13 - eval: epoch: 052, acc1: 55.448%, acc5: 80.456%, test_loss: 1.8787, per_image_load_time: 2.324ms, per_image_inference_time: 0.516ms
2022-03-02 04:12:13 - until epoch: 052, best_acc1: 57.110%
2022-03-02 04:12:13 - epoch 053 lr: 0.08633795680751116
2022-03-02 04:12:52 - train: epoch 0053, iter [00100, 05004], lr: 0.086338, loss: 2.0788
2022-03-02 04:13:24 - train: epoch 0053, iter [00200, 05004], lr: 0.086338, loss: 2.2716
2022-03-02 04:13:57 - train: epoch 0053, iter [00300, 05004], lr: 0.086338, loss: 2.0456
2022-03-02 04:14:30 - train: epoch 0053, iter [00400, 05004], lr: 0.086338, loss: 2.2370
2022-03-02 04:15:03 - train: epoch 0053, iter [00500, 05004], lr: 0.086338, loss: 2.2741
2022-03-02 04:15:36 - train: epoch 0053, iter [00600, 05004], lr: 0.086338, loss: 2.0486
2022-03-02 04:16:09 - train: epoch 0053, iter [00700, 05004], lr: 0.086338, loss: 1.8977
2022-03-02 04:16:43 - train: epoch 0053, iter [00800, 05004], lr: 0.086338, loss: 2.1183
2022-03-02 04:17:16 - train: epoch 0053, iter [00900, 05004], lr: 0.086338, loss: 1.9470
2022-03-02 04:17:49 - train: epoch 0053, iter [01000, 05004], lr: 0.086338, loss: 2.0347
2022-03-02 04:18:22 - train: epoch 0053, iter [01100, 05004], lr: 0.086338, loss: 2.0444
2022-03-02 04:18:55 - train: epoch 0053, iter [01200, 05004], lr: 0.086338, loss: 2.0404
2022-03-02 04:19:29 - train: epoch 0053, iter [01300, 05004], lr: 0.086338, loss: 2.0628
2022-03-02 04:20:02 - train: epoch 0053, iter [01400, 05004], lr: 0.086338, loss: 2.3892
2022-03-02 04:20:35 - train: epoch 0053, iter [01500, 05004], lr: 0.086338, loss: 1.9656
2022-03-02 04:21:08 - train: epoch 0053, iter [01600, 05004], lr: 0.086338, loss: 2.3635
2022-03-02 04:21:41 - train: epoch 0053, iter [01700, 05004], lr: 0.086338, loss: 2.1789
2022-03-02 04:22:14 - train: epoch 0053, iter [01800, 05004], lr: 0.086338, loss: 2.1269
2022-03-02 04:22:47 - train: epoch 0053, iter [01900, 05004], lr: 0.086338, loss: 1.9420
2022-03-02 04:23:20 - train: epoch 0053, iter [02000, 05004], lr: 0.086338, loss: 2.1304
2022-03-02 04:23:53 - train: epoch 0053, iter [02100, 05004], lr: 0.086338, loss: 2.1215
2022-03-02 04:24:26 - train: epoch 0053, iter [02200, 05004], lr: 0.086338, loss: 2.1062
2022-03-02 04:24:59 - train: epoch 0053, iter [02300, 05004], lr: 0.086338, loss: 2.0168
2022-03-02 04:25:32 - train: epoch 0053, iter [02400, 05004], lr: 0.086338, loss: 2.0873
2022-03-02 04:26:05 - train: epoch 0053, iter [02500, 05004], lr: 0.086338, loss: 2.3288
2022-03-02 04:26:38 - train: epoch 0053, iter [02600, 05004], lr: 0.086338, loss: 2.1078
2022-03-02 04:27:12 - train: epoch 0053, iter [02700, 05004], lr: 0.086338, loss: 2.3165
2022-03-02 04:27:45 - train: epoch 0053, iter [02800, 05004], lr: 0.086338, loss: 2.2423
2022-03-02 04:28:18 - train: epoch 0053, iter [02900, 05004], lr: 0.086338, loss: 1.8351
2022-03-02 04:28:51 - train: epoch 0053, iter [03000, 05004], lr: 0.086338, loss: 1.8739
2022-03-02 04:29:24 - train: epoch 0053, iter [03100, 05004], lr: 0.086338, loss: 2.3821
2022-03-02 04:29:58 - train: epoch 0053, iter [03200, 05004], lr: 0.086338, loss: 2.3546
2022-03-02 04:30:30 - train: epoch 0053, iter [03300, 05004], lr: 0.086338, loss: 1.9884
2022-03-02 04:31:03 - train: epoch 0053, iter [03400, 05004], lr: 0.086338, loss: 2.1882
2022-03-02 04:31:36 - train: epoch 0053, iter [03500, 05004], lr: 0.086338, loss: 2.0061
2022-03-02 04:32:09 - train: epoch 0053, iter [03600, 05004], lr: 0.086338, loss: 2.1185
2022-03-02 04:32:42 - train: epoch 0053, iter [03700, 05004], lr: 0.086338, loss: 2.2794
2022-03-02 04:33:14 - train: epoch 0053, iter [03800, 05004], lr: 0.086338, loss: 1.9421
2022-03-02 04:33:48 - train: epoch 0053, iter [03900, 05004], lr: 0.086338, loss: 2.2737
2022-03-02 04:34:21 - train: epoch 0053, iter [04000, 05004], lr: 0.086338, loss: 2.0164
2022-03-02 04:34:54 - train: epoch 0053, iter [04100, 05004], lr: 0.086338, loss: 2.0889
2022-03-02 04:35:27 - train: epoch 0053, iter [04200, 05004], lr: 0.086338, loss: 1.9744
2022-03-02 04:36:00 - train: epoch 0053, iter [04300, 05004], lr: 0.086338, loss: 2.2494
2022-03-02 04:36:33 - train: epoch 0053, iter [04400, 05004], lr: 0.086338, loss: 1.9614
2022-03-02 04:37:06 - train: epoch 0053, iter [04500, 05004], lr: 0.086338, loss: 2.2775
2022-03-02 04:37:39 - train: epoch 0053, iter [04600, 05004], lr: 0.086338, loss: 1.9136
2022-03-02 04:38:12 - train: epoch 0053, iter [04700, 05004], lr: 0.086338, loss: 2.3718
2022-03-02 04:38:45 - train: epoch 0053, iter [04800, 05004], lr: 0.086338, loss: 2.1690
2022-03-02 04:39:18 - train: epoch 0053, iter [04900, 05004], lr: 0.086338, loss: 2.0078
2022-03-02 04:39:50 - train: epoch 0053, iter [05000, 05004], lr: 0.086338, loss: 1.9986
2022-03-02 04:39:51 - train: epoch 053, train_loss: 2.0445
2022-03-02 04:41:05 - eval: epoch: 053, acc1: 55.392%, acc5: 80.244%, test_loss: 1.8960, per_image_load_time: 2.322ms, per_image_inference_time: 0.526ms
2022-03-02 04:41:05 - until epoch: 053, best_acc1: 57.110%
2022-03-02 04:41:05 - epoch 054 lr: 0.08577994803720607
2022-03-02 04:41:44 - train: epoch 0054, iter [00100, 05004], lr: 0.085780, loss: 1.8562
2022-03-02 04:42:17 - train: epoch 0054, iter [00200, 05004], lr: 0.085780, loss: 2.3553
2022-03-02 04:42:49 - train: epoch 0054, iter [00300, 05004], lr: 0.085780, loss: 2.1009
2022-03-02 04:43:22 - train: epoch 0054, iter [00400, 05004], lr: 0.085780, loss: 1.8621
2022-03-02 04:43:55 - train: epoch 0054, iter [00500, 05004], lr: 0.085780, loss: 1.9518
2022-03-02 04:44:30 - train: epoch 0054, iter [00600, 05004], lr: 0.085780, loss: 2.0024
2022-03-02 04:45:02 - train: epoch 0054, iter [00700, 05004], lr: 0.085780, loss: 2.1012
2022-03-02 04:45:35 - train: epoch 0054, iter [00800, 05004], lr: 0.085780, loss: 2.1704
2022-03-02 04:46:08 - train: epoch 0054, iter [00900, 05004], lr: 0.085780, loss: 1.7911
2022-03-02 04:46:41 - train: epoch 0054, iter [01000, 05004], lr: 0.085780, loss: 1.7945
2022-03-02 04:47:14 - train: epoch 0054, iter [01100, 05004], lr: 0.085780, loss: 2.0384
2022-03-02 04:47:47 - train: epoch 0054, iter [01200, 05004], lr: 0.085780, loss: 2.2263
2022-03-02 04:48:21 - train: epoch 0054, iter [01300, 05004], lr: 0.085780, loss: 2.0306
2022-03-02 04:48:54 - train: epoch 0054, iter [01400, 05004], lr: 0.085780, loss: 2.1375
2022-03-02 04:49:27 - train: epoch 0054, iter [01500, 05004], lr: 0.085780, loss: 2.0751
2022-03-02 04:50:00 - train: epoch 0054, iter [01600, 05004], lr: 0.085780, loss: 1.7483
2022-03-02 04:50:33 - train: epoch 0054, iter [01700, 05004], lr: 0.085780, loss: 2.1044
2022-03-02 04:51:05 - train: epoch 0054, iter [01800, 05004], lr: 0.085780, loss: 1.9984
2022-03-02 04:51:39 - train: epoch 0054, iter [01900, 05004], lr: 0.085780, loss: 2.4472
2022-03-02 04:52:12 - train: epoch 0054, iter [02000, 05004], lr: 0.085780, loss: 2.2261
2022-03-02 04:52:45 - train: epoch 0054, iter [02100, 05004], lr: 0.085780, loss: 1.7607
2022-03-02 04:53:18 - train: epoch 0054, iter [02200, 05004], lr: 0.085780, loss: 2.0550
2022-03-02 04:53:51 - train: epoch 0054, iter [02300, 05004], lr: 0.085780, loss: 1.9631
2022-03-02 04:54:24 - train: epoch 0054, iter [02400, 05004], lr: 0.085780, loss: 1.9518
2022-03-02 04:54:57 - train: epoch 0054, iter [02500, 05004], lr: 0.085780, loss: 2.2372
2022-03-02 04:55:30 - train: epoch 0054, iter [02600, 05004], lr: 0.085780, loss: 1.9251
2022-03-02 04:56:03 - train: epoch 0054, iter [02700, 05004], lr: 0.085780, loss: 2.1809
2022-03-02 04:56:37 - train: epoch 0054, iter [02800, 05004], lr: 0.085780, loss: 2.3998
2022-03-02 04:57:09 - train: epoch 0054, iter [02900, 05004], lr: 0.085780, loss: 1.8313
2022-03-02 04:57:42 - train: epoch 0054, iter [03000, 05004], lr: 0.085780, loss: 2.1736
2022-03-02 04:58:15 - train: epoch 0054, iter [03100, 05004], lr: 0.085780, loss: 2.0795
2022-03-02 04:58:49 - train: epoch 0054, iter [03200, 05004], lr: 0.085780, loss: 2.1432
2022-03-02 04:59:22 - train: epoch 0054, iter [03300, 05004], lr: 0.085780, loss: 2.0297
2022-03-02 04:59:55 - train: epoch 0054, iter [03400, 05004], lr: 0.085780, loss: 1.9848
2022-03-02 05:00:28 - train: epoch 0054, iter [03500, 05004], lr: 0.085780, loss: 2.1707
2022-03-02 05:01:02 - train: epoch 0054, iter [03600, 05004], lr: 0.085780, loss: 1.9096
2022-03-02 05:01:34 - train: epoch 0054, iter [03700, 05004], lr: 0.085780, loss: 1.8761
2022-03-02 05:02:08 - train: epoch 0054, iter [03800, 05004], lr: 0.085780, loss: 2.2041
2022-03-02 05:02:40 - train: epoch 0054, iter [03900, 05004], lr: 0.085780, loss: 1.9548
2022-03-02 05:03:13 - train: epoch 0054, iter [04000, 05004], lr: 0.085780, loss: 1.8830
2022-03-02 05:03:46 - train: epoch 0054, iter [04100, 05004], lr: 0.085780, loss: 2.1883
2022-03-02 05:04:20 - train: epoch 0054, iter [04200, 05004], lr: 0.085780, loss: 1.9829
2022-03-02 05:04:53 - train: epoch 0054, iter [04300, 05004], lr: 0.085780, loss: 2.0743
2022-03-02 05:05:26 - train: epoch 0054, iter [04400, 05004], lr: 0.085780, loss: 1.7359
2022-03-02 05:06:00 - train: epoch 0054, iter [04500, 05004], lr: 0.085780, loss: 1.9819
2022-03-02 05:06:33 - train: epoch 0054, iter [04600, 05004], lr: 0.085780, loss: 2.1745
2022-03-02 05:07:06 - train: epoch 0054, iter [04700, 05004], lr: 0.085780, loss: 2.3912
2022-03-02 05:07:39 - train: epoch 0054, iter [04800, 05004], lr: 0.085780, loss: 2.2658
2022-03-02 05:08:12 - train: epoch 0054, iter [04900, 05004], lr: 0.085780, loss: 1.9125
2022-03-02 05:08:44 - train: epoch 0054, iter [05000, 05004], lr: 0.085780, loss: 2.1673
2022-03-02 05:08:45 - train: epoch 054, train_loss: 2.0439
2022-03-02 05:09:58 - eval: epoch: 054, acc1: 55.638%, acc5: 81.142%, test_loss: 1.8495, per_image_load_time: 2.056ms, per_image_inference_time: 0.542ms
2022-03-02 05:09:59 - until epoch: 054, best_acc1: 57.110%
2022-03-02 05:09:59 - epoch 055 lr: 0.08521265257933948
2022-03-02 05:10:37 - train: epoch 0055, iter [00100, 05004], lr: 0.085213, loss: 2.0221
2022-03-02 05:11:10 - train: epoch 0055, iter [00200, 05004], lr: 0.085213, loss: 1.9481
2022-03-02 05:11:44 - train: epoch 0055, iter [00300, 05004], lr: 0.085213, loss: 1.8619
2022-03-02 05:12:16 - train: epoch 0055, iter [00400, 05004], lr: 0.085213, loss: 1.9682
2022-03-02 05:12:49 - train: epoch 0055, iter [00500, 05004], lr: 0.085213, loss: 1.7903
2022-03-02 05:13:22 - train: epoch 0055, iter [00600, 05004], lr: 0.085213, loss: 1.8414
2022-03-02 05:13:56 - train: epoch 0055, iter [00700, 05004], lr: 0.085213, loss: 2.3092
2022-03-02 05:14:29 - train: epoch 0055, iter [00800, 05004], lr: 0.085213, loss: 2.0232
2022-03-02 05:15:02 - train: epoch 0055, iter [00900, 05004], lr: 0.085213, loss: 2.1972
2022-03-02 05:15:35 - train: epoch 0055, iter [01000, 05004], lr: 0.085213, loss: 2.0112
2022-03-02 05:16:09 - train: epoch 0055, iter [01100, 05004], lr: 0.085213, loss: 2.0101
2022-03-02 05:16:42 - train: epoch 0055, iter [01200, 05004], lr: 0.085213, loss: 2.1200
2022-03-02 05:17:16 - train: epoch 0055, iter [01300, 05004], lr: 0.085213, loss: 2.1323
2022-03-02 05:17:49 - train: epoch 0055, iter [01400, 05004], lr: 0.085213, loss: 2.1200
2022-03-02 05:18:23 - train: epoch 0055, iter [01500, 05004], lr: 0.085213, loss: 2.0558
2022-03-02 05:18:55 - train: epoch 0055, iter [01600, 05004], lr: 0.085213, loss: 2.1355
2022-03-02 05:19:29 - train: epoch 0055, iter [01700, 05004], lr: 0.085213, loss: 2.1448
2022-03-02 05:20:02 - train: epoch 0055, iter [01800, 05004], lr: 0.085213, loss: 2.2752
2022-03-02 05:20:35 - train: epoch 0055, iter [01900, 05004], lr: 0.085213, loss: 2.0938
2022-03-02 05:21:08 - train: epoch 0055, iter [02000, 05004], lr: 0.085213, loss: 2.0419
2022-03-02 05:21:41 - train: epoch 0055, iter [02100, 05004], lr: 0.085213, loss: 1.7831
2022-03-02 05:22:14 - train: epoch 0055, iter [02200, 05004], lr: 0.085213, loss: 2.2299
2022-03-02 05:22:47 - train: epoch 0055, iter [02300, 05004], lr: 0.085213, loss: 1.9269
2022-03-02 05:23:20 - train: epoch 0055, iter [02400, 05004], lr: 0.085213, loss: 1.9506
2022-03-02 05:23:53 - train: epoch 0055, iter [02500, 05004], lr: 0.085213, loss: 2.0786
2022-03-02 05:24:26 - train: epoch 0055, iter [02600, 05004], lr: 0.085213, loss: 1.9095
2022-03-02 05:25:00 - train: epoch 0055, iter [02700, 05004], lr: 0.085213, loss: 1.8315
2022-03-02 05:25:32 - train: epoch 0055, iter [02800, 05004], lr: 0.085213, loss: 1.9650
2022-03-02 05:26:05 - train: epoch 0055, iter [02900, 05004], lr: 0.085213, loss: 2.1475
2022-03-02 05:26:38 - train: epoch 0055, iter [03000, 05004], lr: 0.085213, loss: 2.0301
2022-03-02 05:27:12 - train: epoch 0055, iter [03100, 05004], lr: 0.085213, loss: 2.1290
2022-03-02 05:27:44 - train: epoch 0055, iter [03200, 05004], lr: 0.085213, loss: 1.9045
2022-03-02 05:28:18 - train: epoch 0055, iter [03300, 05004], lr: 0.085213, loss: 1.8940
2022-03-02 05:28:51 - train: epoch 0055, iter [03400, 05004], lr: 0.085213, loss: 1.9334
2022-03-02 05:29:24 - train: epoch 0055, iter [03500, 05004], lr: 0.085213, loss: 1.8118
2022-03-02 05:29:58 - train: epoch 0055, iter [03600, 05004], lr: 0.085213, loss: 2.0448
2022-03-02 05:30:30 - train: epoch 0055, iter [03700, 05004], lr: 0.085213, loss: 2.1194
2022-03-02 05:31:04 - train: epoch 0055, iter [03800, 05004], lr: 0.085213, loss: 2.0679
2022-03-02 05:31:37 - train: epoch 0055, iter [03900, 05004], lr: 0.085213, loss: 2.4811
2022-03-02 05:32:10 - train: epoch 0055, iter [04000, 05004], lr: 0.085213, loss: 2.0010
2022-03-02 05:32:43 - train: epoch 0055, iter [04100, 05004], lr: 0.085213, loss: 2.0583
2022-03-02 05:33:16 - train: epoch 0055, iter [04200, 05004], lr: 0.085213, loss: 2.0803
2022-03-02 05:33:49 - train: epoch 0055, iter [04300, 05004], lr: 0.085213, loss: 2.1646
2022-03-02 05:34:23 - train: epoch 0055, iter [04400, 05004], lr: 0.085213, loss: 2.2482
2022-03-02 05:34:56 - train: epoch 0055, iter [04500, 05004], lr: 0.085213, loss: 1.9735
2022-03-02 05:35:29 - train: epoch 0055, iter [04600, 05004], lr: 0.085213, loss: 2.1658
2022-03-02 05:36:03 - train: epoch 0055, iter [04700, 05004], lr: 0.085213, loss: 1.8717
2022-03-02 05:36:35 - train: epoch 0055, iter [04800, 05004], lr: 0.085213, loss: 2.1129
2022-03-02 05:37:09 - train: epoch 0055, iter [04900, 05004], lr: 0.085213, loss: 2.0177
2022-03-02 05:37:40 - train: epoch 0055, iter [05000, 05004], lr: 0.085213, loss: 2.0982
2022-03-02 05:37:42 - train: epoch 055, train_loss: 2.0371
2022-03-02 05:38:55 - eval: epoch: 055, acc1: 57.318%, acc5: 82.070%, test_loss: 1.7713, per_image_load_time: 2.295ms, per_image_inference_time: 0.544ms
2022-03-02 05:38:56 - until epoch: 055, best_acc1: 57.318%
2022-03-02 05:38:56 - epoch 056 lr: 0.08463621767547998
2022-03-02 05:39:34 - train: epoch 0056, iter [00100, 05004], lr: 0.084636, loss: 2.1200
2022-03-02 05:40:06 - train: epoch 0056, iter [00200, 05004], lr: 0.084636, loss: 2.2241
2022-03-02 05:40:40 - train: epoch 0056, iter [00300, 05004], lr: 0.084636, loss: 1.8847
2022-03-02 05:41:12 - train: epoch 0056, iter [00400, 05004], lr: 0.084636, loss: 1.9581
2022-03-02 05:41:46 - train: epoch 0056, iter [00500, 05004], lr: 0.084636, loss: 1.8151
2022-03-02 05:42:19 - train: epoch 0056, iter [00600, 05004], lr: 0.084636, loss: 1.8873
2022-03-02 05:42:52 - train: epoch 0056, iter [00700, 05004], lr: 0.084636, loss: 2.0467
2022-03-02 05:43:25 - train: epoch 0056, iter [00800, 05004], lr: 0.084636, loss: 2.1718
2022-03-02 05:43:59 - train: epoch 0056, iter [00900, 05004], lr: 0.084636, loss: 2.1034
2022-03-02 05:44:32 - train: epoch 0056, iter [01000, 05004], lr: 0.084636, loss: 2.0627
2022-03-02 05:45:05 - train: epoch 0056, iter [01100, 05004], lr: 0.084636, loss: 1.8902
2022-03-02 05:45:38 - train: epoch 0056, iter [01200, 05004], lr: 0.084636, loss: 1.9236
2022-03-02 05:46:12 - train: epoch 0056, iter [01300, 05004], lr: 0.084636, loss: 2.1555
2022-03-02 05:46:46 - train: epoch 0056, iter [01400, 05004], lr: 0.084636, loss: 1.9658
2022-03-02 05:47:19 - train: epoch 0056, iter [01500, 05004], lr: 0.084636, loss: 2.2839
2022-03-02 05:47:52 - train: epoch 0056, iter [01600, 05004], lr: 0.084636, loss: 1.9190
2022-03-02 05:48:25 - train: epoch 0056, iter [01700, 05004], lr: 0.084636, loss: 2.1771
2022-03-02 05:48:59 - train: epoch 0056, iter [01800, 05004], lr: 0.084636, loss: 2.3572
2022-03-02 05:49:32 - train: epoch 0056, iter [01900, 05004], lr: 0.084636, loss: 2.0407
2022-03-02 05:50:06 - train: epoch 0056, iter [02000, 05004], lr: 0.084636, loss: 1.9872
2022-03-02 05:50:39 - train: epoch 0056, iter [02100, 05004], lr: 0.084636, loss: 2.0810
2022-03-02 05:51:12 - train: epoch 0056, iter [02200, 05004], lr: 0.084636, loss: 2.1532
2022-03-02 05:51:45 - train: epoch 0056, iter [02300, 05004], lr: 0.084636, loss: 2.1095
2022-03-02 05:52:18 - train: epoch 0056, iter [02400, 05004], lr: 0.084636, loss: 2.0672
2022-03-02 05:52:52 - train: epoch 0056, iter [02500, 05004], lr: 0.084636, loss: 2.1593
2022-03-02 05:53:26 - train: epoch 0056, iter [02600, 05004], lr: 0.084636, loss: 1.9363
2022-03-02 05:53:59 - train: epoch 0056, iter [02700, 05004], lr: 0.084636, loss: 2.0464
2022-03-02 05:54:31 - train: epoch 0056, iter [02800, 05004], lr: 0.084636, loss: 1.9263
2022-03-02 05:55:05 - train: epoch 0056, iter [02900, 05004], lr: 0.084636, loss: 2.1491
2022-03-02 05:55:38 - train: epoch 0056, iter [03000, 05004], lr: 0.084636, loss: 2.1841
2022-03-02 05:56:12 - train: epoch 0056, iter [03100, 05004], lr: 0.084636, loss: 1.9602
2022-03-02 05:56:45 - train: epoch 0056, iter [03200, 05004], lr: 0.084636, loss: 1.8735
2022-03-02 05:57:18 - train: epoch 0056, iter [03300, 05004], lr: 0.084636, loss: 2.3499
2022-03-02 05:57:51 - train: epoch 0056, iter [03400, 05004], lr: 0.084636, loss: 1.9199
2022-03-02 05:58:25 - train: epoch 0056, iter [03500, 05004], lr: 0.084636, loss: 2.0177
2022-03-02 05:58:57 - train: epoch 0056, iter [03600, 05004], lr: 0.084636, loss: 1.6491
2022-03-02 05:59:31 - train: epoch 0056, iter [03700, 05004], lr: 0.084636, loss: 1.8754
2022-03-02 06:00:04 - train: epoch 0056, iter [03800, 05004], lr: 0.084636, loss: 1.8762
2022-03-02 06:00:37 - train: epoch 0056, iter [03900, 05004], lr: 0.084636, loss: 2.3517
2022-03-02 06:01:10 - train: epoch 0056, iter [04000, 05004], lr: 0.084636, loss: 2.1045
2022-03-02 06:01:43 - train: epoch 0056, iter [04100, 05004], lr: 0.084636, loss: 2.1603
2022-03-02 06:02:16 - train: epoch 0056, iter [04200, 05004], lr: 0.084636, loss: 2.0782
2022-03-02 06:02:49 - train: epoch 0056, iter [04300, 05004], lr: 0.084636, loss: 1.8855
2022-03-02 06:03:22 - train: epoch 0056, iter [04400, 05004], lr: 0.084636, loss: 2.1148
2022-03-02 06:03:55 - train: epoch 0056, iter [04500, 05004], lr: 0.084636, loss: 1.9371
2022-03-02 06:04:28 - train: epoch 0056, iter [04600, 05004], lr: 0.084636, loss: 2.1639
2022-03-02 06:05:01 - train: epoch 0056, iter [04700, 05004], lr: 0.084636, loss: 2.1505
2022-03-02 06:05:34 - train: epoch 0056, iter [04800, 05004], lr: 0.084636, loss: 2.1449
2022-03-02 06:06:08 - train: epoch 0056, iter [04900, 05004], lr: 0.084636, loss: 2.0969
2022-03-02 06:06:39 - train: epoch 0056, iter [05000, 05004], lr: 0.084636, loss: 2.0183
2022-03-02 06:06:40 - train: epoch 056, train_loss: 2.0320
2022-03-02 06:07:53 - eval: epoch: 056, acc1: 56.732%, acc5: 81.280%, test_loss: 1.8311, per_image_load_time: 1.573ms, per_image_inference_time: 0.537ms
2022-03-02 06:07:54 - until epoch: 056, best_acc1: 57.318%
2022-03-02 06:07:54 - epoch 057 lr: 0.08405079293933987
2022-03-02 06:08:32 - train: epoch 0057, iter [00100, 05004], lr: 0.084051, loss: 2.1779
2022-03-02 06:09:05 - train: epoch 0057, iter [00200, 05004], lr: 0.084051, loss: 1.9389
2022-03-02 06:09:39 - train: epoch 0057, iter [00300, 05004], lr: 0.084051, loss: 1.8525
2022-03-02 06:10:11 - train: epoch 0057, iter [00400, 05004], lr: 0.084051, loss: 2.0862
2022-03-02 06:10:44 - train: epoch 0057, iter [00500, 05004], lr: 0.084051, loss: 1.8380
2022-03-02 06:11:18 - train: epoch 0057, iter [00600, 05004], lr: 0.084051, loss: 2.1387
2022-03-02 06:11:50 - train: epoch 0057, iter [00700, 05004], lr: 0.084051, loss: 1.6632
2022-03-02 06:12:23 - train: epoch 0057, iter [00800, 05004], lr: 0.084051, loss: 1.9576
2022-03-02 06:12:56 - train: epoch 0057, iter [00900, 05004], lr: 0.084051, loss: 2.0853
2022-03-02 06:13:30 - train: epoch 0057, iter [01000, 05004], lr: 0.084051, loss: 1.9636
2022-03-02 06:14:03 - train: epoch 0057, iter [01100, 05004], lr: 0.084051, loss: 1.9686
2022-03-02 06:14:37 - train: epoch 0057, iter [01200, 05004], lr: 0.084051, loss: 1.8827
2022-03-02 06:15:10 - train: epoch 0057, iter [01300, 05004], lr: 0.084051, loss: 2.0007
2022-03-02 06:15:44 - train: epoch 0057, iter [01400, 05004], lr: 0.084051, loss: 2.1141
2022-03-02 06:16:17 - train: epoch 0057, iter [01500, 05004], lr: 0.084051, loss: 2.1525
2022-03-02 06:16:50 - train: epoch 0057, iter [01600, 05004], lr: 0.084051, loss: 2.1949
2022-03-02 06:17:24 - train: epoch 0057, iter [01700, 05004], lr: 0.084051, loss: 2.1956
2022-03-02 06:17:57 - train: epoch 0057, iter [01800, 05004], lr: 0.084051, loss: 2.2287
2022-03-02 06:18:30 - train: epoch 0057, iter [01900, 05004], lr: 0.084051, loss: 1.9077
2022-03-02 06:19:03 - train: epoch 0057, iter [02000, 05004], lr: 0.084051, loss: 2.0110
2022-03-02 06:19:37 - train: epoch 0057, iter [02100, 05004], lr: 0.084051, loss: 2.1136
2022-03-02 06:20:09 - train: epoch 0057, iter [02200, 05004], lr: 0.084051, loss: 2.1417
2022-03-02 06:20:42 - train: epoch 0057, iter [02300, 05004], lr: 0.084051, loss: 1.9078
2022-03-02 06:21:15 - train: epoch 0057, iter [02400, 05004], lr: 0.084051, loss: 1.8880
2022-03-02 06:21:48 - train: epoch 0057, iter [02500, 05004], lr: 0.084051, loss: 2.0894
2022-03-02 06:22:21 - train: epoch 0057, iter [02600, 05004], lr: 0.084051, loss: 1.9047
2022-03-02 06:22:55 - train: epoch 0057, iter [02700, 05004], lr: 0.084051, loss: 1.7977
2022-03-02 06:23:27 - train: epoch 0057, iter [02800, 05004], lr: 0.084051, loss: 1.6183
2022-03-02 06:24:01 - train: epoch 0057, iter [02900, 05004], lr: 0.084051, loss: 2.1970
2022-03-02 06:24:33 - train: epoch 0057, iter [03000, 05004], lr: 0.084051, loss: 2.0894
2022-03-02 06:25:06 - train: epoch 0057, iter [03100, 05004], lr: 0.084051, loss: 2.3394
2022-03-02 06:25:39 - train: epoch 0057, iter [03200, 05004], lr: 0.084051, loss: 2.2150
2022-03-02 06:26:12 - train: epoch 0057, iter [03300, 05004], lr: 0.084051, loss: 2.1333
2022-03-02 06:26:44 - train: epoch 0057, iter [03400, 05004], lr: 0.084051, loss: 2.0678
2022-03-02 06:27:18 - train: epoch 0057, iter [03500, 05004], lr: 0.084051, loss: 2.0818
2022-03-02 06:27:51 - train: epoch 0057, iter [03600, 05004], lr: 0.084051, loss: 2.0431
2022-03-02 06:28:23 - train: epoch 0057, iter [03700, 05004], lr: 0.084051, loss: 1.8699
2022-03-02 06:28:57 - train: epoch 0057, iter [03800, 05004], lr: 0.084051, loss: 2.0266
2022-03-02 06:29:29 - train: epoch 0057, iter [03900, 05004], lr: 0.084051, loss: 2.0941
2022-03-02 06:30:02 - train: epoch 0057, iter [04000, 05004], lr: 0.084051, loss: 1.9608
2022-03-02 06:30:35 - train: epoch 0057, iter [04100, 05004], lr: 0.084051, loss: 2.0891
2022-03-02 06:31:08 - train: epoch 0057, iter [04200, 05004], lr: 0.084051, loss: 2.0606
2022-03-02 06:31:42 - train: epoch 0057, iter [04300, 05004], lr: 0.084051, loss: 1.9030
2022-03-02 06:32:15 - train: epoch 0057, iter [04400, 05004], lr: 0.084051, loss: 2.1223
2022-03-02 06:32:49 - train: epoch 0057, iter [04500, 05004], lr: 0.084051, loss: 2.2238
2022-03-02 06:33:21 - train: epoch 0057, iter [04600, 05004], lr: 0.084051, loss: 2.2155
2022-03-02 06:33:55 - train: epoch 0057, iter [04700, 05004], lr: 0.084051, loss: 2.0035
2022-03-02 06:34:28 - train: epoch 0057, iter [04800, 05004], lr: 0.084051, loss: 2.1283
2022-03-02 06:35:01 - train: epoch 0057, iter [04900, 05004], lr: 0.084051, loss: 2.2422
2022-03-02 06:35:32 - train: epoch 0057, iter [05000, 05004], lr: 0.084051, loss: 2.2835
2022-03-02 06:35:33 - train: epoch 057, train_loss: 2.0297
2022-03-02 06:36:47 - eval: epoch: 057, acc1: 57.094%, acc5: 81.910%, test_loss: 1.7909, per_image_load_time: 0.554ms, per_image_inference_time: 0.524ms
2022-03-02 06:36:47 - until epoch: 057, best_acc1: 57.318%
2022-03-02 06:36:47 - epoch 058 lr: 0.08345653031794292
2022-03-02 06:37:25 - train: epoch 0058, iter [00100, 05004], lr: 0.083457, loss: 2.0634
2022-03-02 06:37:58 - train: epoch 0058, iter [00200, 05004], lr: 0.083457, loss: 1.8412
2022-03-02 06:38:31 - train: epoch 0058, iter [00300, 05004], lr: 0.083457, loss: 1.9976
2022-03-02 06:39:05 - train: epoch 0058, iter [00400, 05004], lr: 0.083457, loss: 2.0448
2022-03-02 06:39:38 - train: epoch 0058, iter [00500, 05004], lr: 0.083457, loss: 1.7694
2022-03-02 06:40:12 - train: epoch 0058, iter [00600, 05004], lr: 0.083457, loss: 2.2613
2022-03-02 06:40:45 - train: epoch 0058, iter [00700, 05004], lr: 0.083457, loss: 2.0599
2022-03-02 06:41:18 - train: epoch 0058, iter [00800, 05004], lr: 0.083457, loss: 1.8215
2022-03-02 06:41:51 - train: epoch 0058, iter [00900, 05004], lr: 0.083457, loss: 1.8394
2022-03-02 06:42:24 - train: epoch 0058, iter [01000, 05004], lr: 0.083457, loss: 2.0508
2022-03-02 06:42:57 - train: epoch 0058, iter [01100, 05004], lr: 0.083457, loss: 1.8152
2022-03-02 06:43:31 - train: epoch 0058, iter [01200, 05004], lr: 0.083457, loss: 1.9113
2022-03-02 06:44:05 - train: epoch 0058, iter [01300, 05004], lr: 0.083457, loss: 2.1991
2022-03-02 06:44:38 - train: epoch 0058, iter [01400, 05004], lr: 0.083457, loss: 2.1123
2022-03-02 06:45:11 - train: epoch 0058, iter [01500, 05004], lr: 0.083457, loss: 1.8642
2022-03-02 06:45:45 - train: epoch 0058, iter [01600, 05004], lr: 0.083457, loss: 1.9956
2022-03-02 06:46:19 - train: epoch 0058, iter [01700, 05004], lr: 0.083457, loss: 2.1897
2022-03-02 06:46:52 - train: epoch 0058, iter [01800, 05004], lr: 0.083457, loss: 2.1389
2022-03-02 06:47:24 - train: epoch 0058, iter [01900, 05004], lr: 0.083457, loss: 2.1464
2022-03-02 06:47:58 - train: epoch 0058, iter [02000, 05004], lr: 0.083457, loss: 2.1949
2022-03-02 06:48:31 - train: epoch 0058, iter [02100, 05004], lr: 0.083457, loss: 2.0135
2022-03-02 06:49:05 - train: epoch 0058, iter [02200, 05004], lr: 0.083457, loss: 1.8672
2022-03-02 06:49:38 - train: epoch 0058, iter [02300, 05004], lr: 0.083457, loss: 1.8585
2022-03-02 06:50:11 - train: epoch 0058, iter [02400, 05004], lr: 0.083457, loss: 2.0306
2022-03-02 06:50:44 - train: epoch 0058, iter [02500, 05004], lr: 0.083457, loss: 2.0835
2022-03-02 06:51:18 - train: epoch 0058, iter [02600, 05004], lr: 0.083457, loss: 1.8339
2022-03-02 06:51:50 - train: epoch 0058, iter [02700, 05004], lr: 0.083457, loss: 2.2259
2022-03-02 06:52:24 - train: epoch 0058, iter [02800, 05004], lr: 0.083457, loss: 1.7567
2022-03-02 06:52:57 - train: epoch 0058, iter [02900, 05004], lr: 0.083457, loss: 1.9270
2022-03-02 06:53:30 - train: epoch 0058, iter [03000, 05004], lr: 0.083457, loss: 2.2188
2022-03-02 06:54:03 - train: epoch 0058, iter [03100, 05004], lr: 0.083457, loss: 1.8983
2022-03-02 06:54:36 - train: epoch 0058, iter [03200, 05004], lr: 0.083457, loss: 1.8548
2022-03-02 06:55:09 - train: epoch 0058, iter [03300, 05004], lr: 0.083457, loss: 1.9396
2022-03-02 06:55:42 - train: epoch 0058, iter [03400, 05004], lr: 0.083457, loss: 1.8674
2022-03-02 06:56:16 - train: epoch 0058, iter [03500, 05004], lr: 0.083457, loss: 1.9697
2022-03-02 06:56:49 - train: epoch 0058, iter [03600, 05004], lr: 0.083457, loss: 1.8553
2022-03-02 06:57:22 - train: epoch 0058, iter [03700, 05004], lr: 0.083457, loss: 1.9998
2022-03-02 06:57:55 - train: epoch 0058, iter [03800, 05004], lr: 0.083457, loss: 2.1082
2022-03-02 06:58:28 - train: epoch 0058, iter [03900, 05004], lr: 0.083457, loss: 2.0558
2022-03-02 06:59:02 - train: epoch 0058, iter [04000, 05004], lr: 0.083457, loss: 2.1101
2022-03-02 06:59:35 - train: epoch 0058, iter [04100, 05004], lr: 0.083457, loss: 2.1739
2022-03-02 07:00:09 - train: epoch 0058, iter [04200, 05004], lr: 0.083457, loss: 1.7051
2022-03-02 07:00:42 - train: epoch 0058, iter [04300, 05004], lr: 0.083457, loss: 2.3378
2022-03-02 07:01:15 - train: epoch 0058, iter [04400, 05004], lr: 0.083457, loss: 1.8484
2022-03-02 07:01:48 - train: epoch 0058, iter [04500, 05004], lr: 0.083457, loss: 2.0760
2022-03-02 07:02:22 - train: epoch 0058, iter [04600, 05004], lr: 0.083457, loss: 1.8855
2022-03-02 07:02:55 - train: epoch 0058, iter [04700, 05004], lr: 0.083457, loss: 2.0849
2022-03-02 07:03:28 - train: epoch 0058, iter [04800, 05004], lr: 0.083457, loss: 2.1000
2022-03-02 07:04:01 - train: epoch 0058, iter [04900, 05004], lr: 0.083457, loss: 1.9061
2022-03-02 07:04:33 - train: epoch 0058, iter [05000, 05004], lr: 0.083457, loss: 1.9808
2022-03-02 07:04:34 - train: epoch 058, train_loss: 2.0225
2022-03-02 07:05:48 - eval: epoch: 058, acc1: 57.654%, acc5: 82.160%, test_loss: 1.7766, per_image_load_time: 1.546ms, per_image_inference_time: 0.535ms
2022-03-02 07:05:49 - until epoch: 058, best_acc1: 57.654%
2022-03-02 07:05:49 - epoch 059 lr: 0.08285358405218655
2022-03-02 07:06:27 - train: epoch 0059, iter [00100, 05004], lr: 0.082854, loss: 2.1292
2022-03-02 07:06:59 - train: epoch 0059, iter [00200, 05004], lr: 0.082854, loss: 1.8195
2022-03-02 07:07:32 - train: epoch 0059, iter [00300, 05004], lr: 0.082854, loss: 1.9166
2022-03-02 07:08:04 - train: epoch 0059, iter [00400, 05004], lr: 0.082854, loss: 2.0514
2022-03-02 07:08:38 - train: epoch 0059, iter [00500, 05004], lr: 0.082854, loss: 2.2822
2022-03-02 07:09:11 - train: epoch 0059, iter [00600, 05004], lr: 0.082854, loss: 1.9412
2022-03-02 07:09:45 - train: epoch 0059, iter [00700, 05004], lr: 0.082854, loss: 1.9240
2022-03-02 07:10:18 - train: epoch 0059, iter [00800, 05004], lr: 0.082854, loss: 1.9588
2022-03-02 07:10:51 - train: epoch 0059, iter [00900, 05004], lr: 0.082854, loss: 1.9582
2022-03-02 07:11:24 - train: epoch 0059, iter [01000, 05004], lr: 0.082854, loss: 2.0833
2022-03-02 07:11:56 - train: epoch 0059, iter [01100, 05004], lr: 0.082854, loss: 2.1838
2022-03-02 07:12:30 - train: epoch 0059, iter [01200, 05004], lr: 0.082854, loss: 1.6831
2022-03-02 07:13:03 - train: epoch 0059, iter [01300, 05004], lr: 0.082854, loss: 2.2339
2022-03-02 07:13:36 - train: epoch 0059, iter [01400, 05004], lr: 0.082854, loss: 2.3060
2022-03-02 07:14:09 - train: epoch 0059, iter [01500, 05004], lr: 0.082854, loss: 2.0449
2022-03-02 07:14:43 - train: epoch 0059, iter [01600, 05004], lr: 0.082854, loss: 1.9852
2022-03-02 07:15:16 - train: epoch 0059, iter [01700, 05004], lr: 0.082854, loss: 2.1084
2022-03-02 07:15:49 - train: epoch 0059, iter [01800, 05004], lr: 0.082854, loss: 1.9292
2022-03-02 07:16:22 - train: epoch 0059, iter [01900, 05004], lr: 0.082854, loss: 2.0371
2022-03-02 07:16:54 - train: epoch 0059, iter [02000, 05004], lr: 0.082854, loss: 1.8079
2022-03-02 07:17:28 - train: epoch 0059, iter [02100, 05004], lr: 0.082854, loss: 2.2718
2022-03-02 07:18:01 - train: epoch 0059, iter [02200, 05004], lr: 0.082854, loss: 2.1625
2022-03-02 07:18:33 - train: epoch 0059, iter [02300, 05004], lr: 0.082854, loss: 1.9313
2022-03-02 07:19:07 - train: epoch 0059, iter [02400, 05004], lr: 0.082854, loss: 2.1755
2022-03-02 07:19:40 - train: epoch 0059, iter [02500, 05004], lr: 0.082854, loss: 2.0438
2022-03-02 07:20:13 - train: epoch 0059, iter [02600, 05004], lr: 0.082854, loss: 1.9465
2022-03-02 07:20:46 - train: epoch 0059, iter [02700, 05004], lr: 0.082854, loss: 2.0771
2022-03-02 07:21:19 - train: epoch 0059, iter [02800, 05004], lr: 0.082854, loss: 2.2955
2022-03-02 07:21:52 - train: epoch 0059, iter [02900, 05004], lr: 0.082854, loss: 1.9638
2022-03-02 07:22:24 - train: epoch 0059, iter [03000, 05004], lr: 0.082854, loss: 2.3743
2022-03-02 07:22:57 - train: epoch 0059, iter [03100, 05004], lr: 0.082854, loss: 1.8752
2022-03-02 07:23:30 - train: epoch 0059, iter [03200, 05004], lr: 0.082854, loss: 2.0093
2022-03-02 07:24:03 - train: epoch 0059, iter [03300, 05004], lr: 0.082854, loss: 2.1075
2022-03-02 07:24:37 - train: epoch 0059, iter [03400, 05004], lr: 0.082854, loss: 2.3218
2022-03-02 07:25:09 - train: epoch 0059, iter [03500, 05004], lr: 0.082854, loss: 1.7477
2022-03-02 07:25:42 - train: epoch 0059, iter [03600, 05004], lr: 0.082854, loss: 1.9505
2022-03-02 07:26:16 - train: epoch 0059, iter [03700, 05004], lr: 0.082854, loss: 1.8348
2022-03-02 07:26:49 - train: epoch 0059, iter [03800, 05004], lr: 0.082854, loss: 1.9948
2022-03-02 07:27:22 - train: epoch 0059, iter [03900, 05004], lr: 0.082854, loss: 1.9621
2022-03-02 07:27:55 - train: epoch 0059, iter [04000, 05004], lr: 0.082854, loss: 2.3069
2022-03-02 07:28:28 - train: epoch 0059, iter [04100, 05004], lr: 0.082854, loss: 2.0069
2022-03-02 07:29:01 - train: epoch 0059, iter [04200, 05004], lr: 0.082854, loss: 2.1797
2022-03-02 07:29:35 - train: epoch 0059, iter [04300, 05004], lr: 0.082854, loss: 2.3233
2022-03-02 07:30:08 - train: epoch 0059, iter [04400, 05004], lr: 0.082854, loss: 2.2181
2022-03-02 07:30:41 - train: epoch 0059, iter [04500, 05004], lr: 0.082854, loss: 2.1458
2022-03-02 07:31:14 - train: epoch 0059, iter [04600, 05004], lr: 0.082854, loss: 2.0995
2022-03-02 07:31:48 - train: epoch 0059, iter [04700, 05004], lr: 0.082854, loss: 1.9414
2022-03-02 07:32:20 - train: epoch 0059, iter [04800, 05004], lr: 0.082854, loss: 1.8100
2022-03-02 07:32:54 - train: epoch 0059, iter [04900, 05004], lr: 0.082854, loss: 2.2137
2022-03-02 07:33:26 - train: epoch 0059, iter [05000, 05004], lr: 0.082854, loss: 2.2467
2022-03-02 07:33:27 - train: epoch 059, train_loss: 2.0206
2022-03-02 07:34:41 - eval: epoch: 059, acc1: 57.486%, acc5: 82.030%, test_loss: 1.7749, per_image_load_time: 2.180ms, per_image_inference_time: 0.521ms
2022-03-02 07:34:41 - until epoch: 059, best_acc1: 57.654%
2022-03-02 07:34:41 - epoch 060 lr: 0.08224211063680853
2022-03-02 07:35:20 - train: epoch 0060, iter [00100, 05004], lr: 0.082242, loss: 1.8939
2022-03-02 07:35:52 - train: epoch 0060, iter [00200, 05004], lr: 0.082242, loss: 2.1032
2022-03-02 07:36:25 - train: epoch 0060, iter [00300, 05004], lr: 0.082242, loss: 1.9963
2022-03-02 07:36:58 - train: epoch 0060, iter [00400, 05004], lr: 0.082242, loss: 2.0886
2022-03-02 07:37:31 - train: epoch 0060, iter [00500, 05004], lr: 0.082242, loss: 2.2451
2022-03-02 07:38:05 - train: epoch 0060, iter [00600, 05004], lr: 0.082242, loss: 2.0888
2022-03-02 07:38:38 - train: epoch 0060, iter [00700, 05004], lr: 0.082242, loss: 2.0554
2022-03-02 07:39:11 - train: epoch 0060, iter [00800, 05004], lr: 0.082242, loss: 2.2335
2022-03-02 07:39:44 - train: epoch 0060, iter [00900, 05004], lr: 0.082242, loss: 1.7181
2022-03-02 07:40:18 - train: epoch 0060, iter [01000, 05004], lr: 0.082242, loss: 1.7223
2022-03-02 07:40:50 - train: epoch 0060, iter [01100, 05004], lr: 0.082242, loss: 1.7777
2022-03-02 07:41:24 - train: epoch 0060, iter [01200, 05004], lr: 0.082242, loss: 1.9691
2022-03-02 07:41:57 - train: epoch 0060, iter [01300, 05004], lr: 0.082242, loss: 1.9398
2022-03-02 07:42:30 - train: epoch 0060, iter [01400, 05004], lr: 0.082242, loss: 1.9832
2022-03-02 07:43:03 - train: epoch 0060, iter [01500, 05004], lr: 0.082242, loss: 2.0898
2022-03-02 07:43:37 - train: epoch 0060, iter [01600, 05004], lr: 0.082242, loss: 1.9742
2022-03-02 07:44:10 - train: epoch 0060, iter [01700, 05004], lr: 0.082242, loss: 2.0440
2022-03-02 07:44:44 - train: epoch 0060, iter [01800, 05004], lr: 0.082242, loss: 2.0999
2022-03-02 07:45:17 - train: epoch 0060, iter [01900, 05004], lr: 0.082242, loss: 2.1802
2022-03-02 07:45:50 - train: epoch 0060, iter [02000, 05004], lr: 0.082242, loss: 1.9151
2022-03-02 07:46:23 - train: epoch 0060, iter [02100, 05004], lr: 0.082242, loss: 2.0298
2022-03-02 07:46:56 - train: epoch 0060, iter [02200, 05004], lr: 0.082242, loss: 2.2027
2022-03-02 07:47:29 - train: epoch 0060, iter [02300, 05004], lr: 0.082242, loss: 1.7201
2022-03-02 07:48:02 - train: epoch 0060, iter [02400, 05004], lr: 0.082242, loss: 1.9552
2022-03-02 07:48:35 - train: epoch 0060, iter [02500, 05004], lr: 0.082242, loss: 2.1463
2022-03-02 07:49:09 - train: epoch 0060, iter [02600, 05004], lr: 0.082242, loss: 2.1416
2022-03-02 07:49:42 - train: epoch 0060, iter [02700, 05004], lr: 0.082242, loss: 2.0109
2022-03-02 07:50:15 - train: epoch 0060, iter [02800, 05004], lr: 0.082242, loss: 1.8768
2022-03-02 07:50:49 - train: epoch 0060, iter [02900, 05004], lr: 0.082242, loss: 2.1675
2022-03-02 07:51:21 - train: epoch 0060, iter [03000, 05004], lr: 0.082242, loss: 2.2891
2022-03-02 07:51:54 - train: epoch 0060, iter [03100, 05004], lr: 0.082242, loss: 2.1009
2022-03-02 07:52:28 - train: epoch 0060, iter [03200, 05004], lr: 0.082242, loss: 1.9570
2022-03-02 07:53:01 - train: epoch 0060, iter [03300, 05004], lr: 0.082242, loss: 1.8377
2022-03-02 07:53:33 - train: epoch 0060, iter [03400, 05004], lr: 0.082242, loss: 2.0314
2022-03-02 07:54:07 - train: epoch 0060, iter [03500, 05004], lr: 0.082242, loss: 2.1621
2022-03-02 07:54:40 - train: epoch 0060, iter [03600, 05004], lr: 0.082242, loss: 2.0987
2022-03-02 07:55:13 - train: epoch 0060, iter [03700, 05004], lr: 0.082242, loss: 1.9803
2022-03-02 07:55:46 - train: epoch 0060, iter [03800, 05004], lr: 0.082242, loss: 2.0627
2022-03-02 07:56:19 - train: epoch 0060, iter [03900, 05004], lr: 0.082242, loss: 2.3627
2022-03-02 07:56:52 - train: epoch 0060, iter [04000, 05004], lr: 0.082242, loss: 2.1046
2022-03-02 07:57:24 - train: epoch 0060, iter [04100, 05004], lr: 0.082242, loss: 2.1392
2022-03-02 07:57:58 - train: epoch 0060, iter [04200, 05004], lr: 0.082242, loss: 2.0939
2022-03-02 07:58:31 - train: epoch 0060, iter [04300, 05004], lr: 0.082242, loss: 2.0158
2022-03-02 07:59:04 - train: epoch 0060, iter [04400, 05004], lr: 0.082242, loss: 2.1691
2022-03-02 07:59:37 - train: epoch 0060, iter [04500, 05004], lr: 0.082242, loss: 1.9331
2022-03-02 08:00:10 - train: epoch 0060, iter [04600, 05004], lr: 0.082242, loss: 1.9151
2022-03-02 08:00:44 - train: epoch 0060, iter [04700, 05004], lr: 0.082242, loss: 2.0559
2022-03-02 08:01:16 - train: epoch 0060, iter [04800, 05004], lr: 0.082242, loss: 1.7777
2022-03-02 08:01:49 - train: epoch 0060, iter [04900, 05004], lr: 0.082242, loss: 1.9757
2022-03-02 08:02:22 - train: epoch 0060, iter [05000, 05004], lr: 0.082242, loss: 2.0091
2022-03-02 08:02:23 - train: epoch 060, train_loss: 2.0157
2022-03-02 08:03:36 - eval: epoch: 060, acc1: 56.702%, acc5: 81.370%, test_loss: 1.8203, per_image_load_time: 1.760ms, per_image_inference_time: 0.544ms
2022-03-02 08:03:37 - until epoch: 060, best_acc1: 57.654%
2022-03-02 08:03:37 - epoch 061 lr: 0.08162226877976886
2022-03-02 08:04:15 - train: epoch 0061, iter [00100, 05004], lr: 0.081622, loss: 1.8846
2022-03-02 08:04:48 - train: epoch 0061, iter [00200, 05004], lr: 0.081622, loss: 2.0921
2022-03-02 08:05:21 - train: epoch 0061, iter [00300, 05004], lr: 0.081622, loss: 1.8468
2022-03-02 08:05:55 - train: epoch 0061, iter [00400, 05004], lr: 0.081622, loss: 2.1800
2022-03-02 08:06:27 - train: epoch 0061, iter [00500, 05004], lr: 0.081622, loss: 1.9384
2022-03-02 08:07:01 - train: epoch 0061, iter [00600, 05004], lr: 0.081622, loss: 2.0421
2022-03-02 08:07:34 - train: epoch 0061, iter [00700, 05004], lr: 0.081622, loss: 1.7052
2022-03-02 08:08:08 - train: epoch 0061, iter [00800, 05004], lr: 0.081622, loss: 1.9971
2022-03-02 08:08:41 - train: epoch 0061, iter [00900, 05004], lr: 0.081622, loss: 1.8746
2022-03-02 08:09:14 - train: epoch 0061, iter [01000, 05004], lr: 0.081622, loss: 1.8852
2022-03-02 08:09:48 - train: epoch 0061, iter [01100, 05004], lr: 0.081622, loss: 1.7797
2022-03-02 08:10:21 - train: epoch 0061, iter [01200, 05004], lr: 0.081622, loss: 1.9312
2022-03-02 08:10:54 - train: epoch 0061, iter [01300, 05004], lr: 0.081622, loss: 1.8470
2022-03-02 08:11:28 - train: epoch 0061, iter [01400, 05004], lr: 0.081622, loss: 1.9293
2022-03-02 08:12:01 - train: epoch 0061, iter [01500, 05004], lr: 0.081622, loss: 2.0262
2022-03-02 08:12:35 - train: epoch 0061, iter [01600, 05004], lr: 0.081622, loss: 1.8332
2022-03-02 08:13:07 - train: epoch 0061, iter [01700, 05004], lr: 0.081622, loss: 1.9372
2022-03-02 08:13:40 - train: epoch 0061, iter [01800, 05004], lr: 0.081622, loss: 2.0639
2022-03-02 08:14:13 - train: epoch 0061, iter [01900, 05004], lr: 0.081622, loss: 2.1446
2022-03-02 08:14:47 - train: epoch 0061, iter [02000, 05004], lr: 0.081622, loss: 1.9384
2022-03-02 08:15:20 - train: epoch 0061, iter [02100, 05004], lr: 0.081622, loss: 2.4239
2022-03-02 08:15:54 - train: epoch 0061, iter [02200, 05004], lr: 0.081622, loss: 1.8297
2022-03-02 08:16:27 - train: epoch 0061, iter [02300, 05004], lr: 0.081622, loss: 1.7994
2022-03-02 08:17:01 - train: epoch 0061, iter [02400, 05004], lr: 0.081622, loss: 1.7917
2022-03-02 08:17:34 - train: epoch 0061, iter [02500, 05004], lr: 0.081622, loss: 1.8854
2022-03-02 08:18:07 - train: epoch 0061, iter [02600, 05004], lr: 0.081622, loss: 2.0937
2022-03-02 08:18:41 - train: epoch 0061, iter [02700, 05004], lr: 0.081622, loss: 2.0863
2022-03-02 08:19:15 - train: epoch 0061, iter [02800, 05004], lr: 0.081622, loss: 2.0214
2022-03-02 08:19:48 - train: epoch 0061, iter [02900, 05004], lr: 0.081622, loss: 2.0478
2022-03-02 08:20:21 - train: epoch 0061, iter [03000, 05004], lr: 0.081622, loss: 1.9879
2022-03-02 08:20:55 - train: epoch 0061, iter [03100, 05004], lr: 0.081622, loss: 2.1288
2022-03-02 08:21:29 - train: epoch 0061, iter [03200, 05004], lr: 0.081622, loss: 2.0763
2022-03-02 08:22:02 - train: epoch 0061, iter [03300, 05004], lr: 0.081622, loss: 1.9528
2022-03-02 08:22:35 - train: epoch 0061, iter [03400, 05004], lr: 0.081622, loss: 1.8159
2022-03-02 08:23:08 - train: epoch 0061, iter [03500, 05004], lr: 0.081622, loss: 2.0651
2022-03-02 08:23:41 - train: epoch 0061, iter [03600, 05004], lr: 0.081622, loss: 2.0698
2022-03-02 08:24:15 - train: epoch 0061, iter [03700, 05004], lr: 0.081622, loss: 2.0537
2022-03-02 08:24:48 - train: epoch 0061, iter [03800, 05004], lr: 0.081622, loss: 1.9893
2022-03-02 08:25:21 - train: epoch 0061, iter [03900, 05004], lr: 0.081622, loss: 1.9493
2022-03-02 08:25:54 - train: epoch 0061, iter [04000, 05004], lr: 0.081622, loss: 1.9972
2022-03-02 08:26:28 - train: epoch 0061, iter [04100, 05004], lr: 0.081622, loss: 2.2912
2022-03-02 08:27:01 - train: epoch 0061, iter [04200, 05004], lr: 0.081622, loss: 1.8986
2022-03-02 08:27:33 - train: epoch 0061, iter [04300, 05004], lr: 0.081622, loss: 2.0265
2022-03-02 08:28:07 - train: epoch 0061, iter [04400, 05004], lr: 0.081622, loss: 2.1370
2022-03-02 08:28:41 - train: epoch 0061, iter [04500, 05004], lr: 0.081622, loss: 2.0932
2022-03-02 08:29:15 - train: epoch 0061, iter [04600, 05004], lr: 0.081622, loss: 2.1909
2022-03-02 08:29:47 - train: epoch 0061, iter [04700, 05004], lr: 0.081622, loss: 2.0936
2022-03-02 08:30:20 - train: epoch 0061, iter [04800, 05004], lr: 0.081622, loss: 2.0544
2022-03-02 08:30:54 - train: epoch 0061, iter [04900, 05004], lr: 0.081622, loss: 2.0194
2022-03-02 08:31:25 - train: epoch 0061, iter [05000, 05004], lr: 0.081622, loss: 2.2091
2022-03-02 08:31:26 - train: epoch 061, train_loss: 2.0109
2022-03-02 08:32:41 - eval: epoch: 061, acc1: 58.018%, acc5: 82.314%, test_loss: 1.7572, per_image_load_time: 2.186ms, per_image_inference_time: 0.533ms
2022-03-02 08:32:42 - until epoch: 061, best_acc1: 58.018%
2022-03-02 08:32:42 - epoch 062 lr: 0.08099421936105702
2022-03-02 08:33:20 - train: epoch 0062, iter [00100, 05004], lr: 0.080994, loss: 2.0597
2022-03-02 08:33:52 - train: epoch 0062, iter [00200, 05004], lr: 0.080994, loss: 2.1437
2022-03-02 08:34:26 - train: epoch 0062, iter [00300, 05004], lr: 0.080994, loss: 1.9243
2022-03-02 08:34:58 - train: epoch 0062, iter [00400, 05004], lr: 0.080994, loss: 1.9413
2022-03-02 08:35:31 - train: epoch 0062, iter [00500, 05004], lr: 0.080994, loss: 1.9302
2022-03-02 08:36:05 - train: epoch 0062, iter [00600, 05004], lr: 0.080994, loss: 1.8567
2022-03-02 08:36:38 - train: epoch 0062, iter [00700, 05004], lr: 0.080994, loss: 2.0411
2022-03-02 08:37:11 - train: epoch 0062, iter [00800, 05004], lr: 0.080994, loss: 1.9235
2022-03-02 08:37:44 - train: epoch 0062, iter [00900, 05004], lr: 0.080994, loss: 1.9905
2022-03-02 08:38:17 - train: epoch 0062, iter [01000, 05004], lr: 0.080994, loss: 2.1259
2022-03-02 08:38:51 - train: epoch 0062, iter [01100, 05004], lr: 0.080994, loss: 1.9177
2022-03-02 08:39:24 - train: epoch 0062, iter [01200, 05004], lr: 0.080994, loss: 2.2469
2022-03-02 08:39:57 - train: epoch 0062, iter [01300, 05004], lr: 0.080994, loss: 2.1081
2022-03-02 08:40:30 - train: epoch 0062, iter [01400, 05004], lr: 0.080994, loss: 1.9556
2022-03-02 08:41:03 - train: epoch 0062, iter [01500, 05004], lr: 0.080994, loss: 2.0953
2022-03-02 08:41:36 - train: epoch 0062, iter [01600, 05004], lr: 0.080994, loss: 2.1321
2022-03-02 08:42:10 - train: epoch 0062, iter [01700, 05004], lr: 0.080994, loss: 2.0293
2022-03-02 08:42:43 - train: epoch 0062, iter [01800, 05004], lr: 0.080994, loss: 1.9379
2022-03-02 08:43:16 - train: epoch 0062, iter [01900, 05004], lr: 0.080994, loss: 2.0190
2022-03-02 08:43:50 - train: epoch 0062, iter [02000, 05004], lr: 0.080994, loss: 1.9141
2022-03-02 08:44:23 - train: epoch 0062, iter [02100, 05004], lr: 0.080994, loss: 2.1268
2022-03-02 08:44:57 - train: epoch 0062, iter [02200, 05004], lr: 0.080994, loss: 1.7732
2022-03-02 08:45:30 - train: epoch 0062, iter [02300, 05004], lr: 0.080994, loss: 1.9397
2022-03-02 08:46:03 - train: epoch 0062, iter [02400, 05004], lr: 0.080994, loss: 2.1058
2022-03-02 08:46:37 - train: epoch 0062, iter [02500, 05004], lr: 0.080994, loss: 2.1686
2022-03-02 08:47:09 - train: epoch 0062, iter [02600, 05004], lr: 0.080994, loss: 1.8284
2022-03-02 08:47:43 - train: epoch 0062, iter [02700, 05004], lr: 0.080994, loss: 2.0734
2022-03-02 08:48:16 - train: epoch 0062, iter [02800, 05004], lr: 0.080994, loss: 2.1857
2022-03-02 08:48:49 - train: epoch 0062, iter [02900, 05004], lr: 0.080994, loss: 2.0545
2022-03-02 08:49:22 - train: epoch 0062, iter [03000, 05004], lr: 0.080994, loss: 2.1778
2022-03-02 08:49:55 - train: epoch 0062, iter [03100, 05004], lr: 0.080994, loss: 2.0691
2022-03-02 08:50:28 - train: epoch 0062, iter [03200, 05004], lr: 0.080994, loss: 1.8623
2022-03-02 08:51:01 - train: epoch 0062, iter [03300, 05004], lr: 0.080994, loss: 2.1423
2022-03-02 08:51:35 - train: epoch 0062, iter [03400, 05004], lr: 0.080994, loss: 1.9792
2022-03-02 08:52:08 - train: epoch 0062, iter [03500, 05004], lr: 0.080994, loss: 2.0818
2022-03-02 08:52:41 - train: epoch 0062, iter [03600, 05004], lr: 0.080994, loss: 2.1218
2022-03-02 08:53:14 - train: epoch 0062, iter [03700, 05004], lr: 0.080994, loss: 2.0534
2022-03-02 08:53:47 - train: epoch 0062, iter [03800, 05004], lr: 0.080994, loss: 2.1149
2022-03-02 08:54:21 - train: epoch 0062, iter [03900, 05004], lr: 0.080994, loss: 1.9391
2022-03-02 08:54:55 - train: epoch 0062, iter [04000, 05004], lr: 0.080994, loss: 1.8163
2022-03-02 08:55:27 - train: epoch 0062, iter [04100, 05004], lr: 0.080994, loss: 2.0446
2022-03-02 08:56:00 - train: epoch 0062, iter [04200, 05004], lr: 0.080994, loss: 1.8682
2022-03-02 08:56:34 - train: epoch 0062, iter [04300, 05004], lr: 0.080994, loss: 2.0752
2022-03-02 08:57:07 - train: epoch 0062, iter [04400, 05004], lr: 0.080994, loss: 1.9710
2022-03-02 08:57:40 - train: epoch 0062, iter [04500, 05004], lr: 0.080994, loss: 1.7636
2022-03-02 08:58:13 - train: epoch 0062, iter [04600, 05004], lr: 0.080994, loss: 1.7503
2022-03-02 08:58:46 - train: epoch 0062, iter [04700, 05004], lr: 0.080994, loss: 1.9316
2022-03-02 08:59:19 - train: epoch 0062, iter [04800, 05004], lr: 0.080994, loss: 2.0169
2022-03-02 08:59:52 - train: epoch 0062, iter [04900, 05004], lr: 0.080994, loss: 2.0065
2022-03-02 09:00:24 - train: epoch 0062, iter [05000, 05004], lr: 0.080994, loss: 2.1316
2022-03-02 09:00:25 - train: epoch 062, train_loss: 2.0070
2022-03-02 09:01:39 - eval: epoch: 062, acc1: 57.082%, acc5: 82.036%, test_loss: 1.7801, per_image_load_time: 1.307ms, per_image_inference_time: 0.531ms
2022-03-02 09:01:39 - until epoch: 062, best_acc1: 58.018%
2022-03-02 09:01:39 - epoch 063 lr: 0.08035812539093556
2022-03-02 09:02:16 - train: epoch 0063, iter [00100, 05004], lr: 0.080358, loss: 1.9280
2022-03-02 09:02:50 - train: epoch 0063, iter [00200, 05004], lr: 0.080358, loss: 1.8815
2022-03-02 09:03:23 - train: epoch 0063, iter [00300, 05004], lr: 0.080358, loss: 2.1144
2022-03-02 09:03:55 - train: epoch 0063, iter [00400, 05004], lr: 0.080358, loss: 2.0093
2022-03-02 09:04:29 - train: epoch 0063, iter [00500, 05004], lr: 0.080358, loss: 1.7217
2022-03-02 09:05:01 - train: epoch 0063, iter [00600, 05004], lr: 0.080358, loss: 2.1643
2022-03-02 09:05:34 - train: epoch 0063, iter [00700, 05004], lr: 0.080358, loss: 2.0535
2022-03-02 09:06:07 - train: epoch 0063, iter [00800, 05004], lr: 0.080358, loss: 2.0669
2022-03-02 09:06:41 - train: epoch 0063, iter [00900, 05004], lr: 0.080358, loss: 2.0195
2022-03-02 09:07:14 - train: epoch 0063, iter [01000, 05004], lr: 0.080358, loss: 2.1389
2022-03-02 09:07:48 - train: epoch 0063, iter [01100, 05004], lr: 0.080358, loss: 1.8720
2022-03-02 09:08:21 - train: epoch 0063, iter [01200, 05004], lr: 0.080358, loss: 2.0019
2022-03-02 09:08:54 - train: epoch 0063, iter [01300, 05004], lr: 0.080358, loss: 1.9956
2022-03-02 09:09:27 - train: epoch 0063, iter [01400, 05004], lr: 0.080358, loss: 2.4126
2022-03-02 09:10:01 - train: epoch 0063, iter [01500, 05004], lr: 0.080358, loss: 2.0303
2022-03-02 09:10:34 - train: epoch 0063, iter [01600, 05004], lr: 0.080358, loss: 1.9105
2022-03-02 09:11:08 - train: epoch 0063, iter [01700, 05004], lr: 0.080358, loss: 1.7998
2022-03-02 09:11:41 - train: epoch 0063, iter [01800, 05004], lr: 0.080358, loss: 2.2234
2022-03-02 09:12:14 - train: epoch 0063, iter [01900, 05004], lr: 0.080358, loss: 2.1959
2022-03-02 09:12:48 - train: epoch 0063, iter [02000, 05004], lr: 0.080358, loss: 1.9033
2022-03-02 09:13:20 - train: epoch 0063, iter [02100, 05004], lr: 0.080358, loss: 1.8806
2022-03-02 09:13:54 - train: epoch 0063, iter [02200, 05004], lr: 0.080358, loss: 2.3976
2022-03-02 09:14:28 - train: epoch 0063, iter [02300, 05004], lr: 0.080358, loss: 2.0905
2022-03-02 09:15:00 - train: epoch 0063, iter [02400, 05004], lr: 0.080358, loss: 2.2578
2022-03-02 09:15:34 - train: epoch 0063, iter [02500, 05004], lr: 0.080358, loss: 1.9897
2022-03-02 09:16:08 - train: epoch 0063, iter [02600, 05004], lr: 0.080358, loss: 1.8645
2022-03-02 09:16:40 - train: epoch 0063, iter [02700, 05004], lr: 0.080358, loss: 2.1894
2022-03-02 09:17:14 - train: epoch 0063, iter [02800, 05004], lr: 0.080358, loss: 2.1079
2022-03-02 09:17:47 - train: epoch 0063, iter [02900, 05004], lr: 0.080358, loss: 2.0357
2022-03-02 09:18:21 - train: epoch 0063, iter [03000, 05004], lr: 0.080358, loss: 2.0763
2022-03-02 09:18:54 - train: epoch 0063, iter [03100, 05004], lr: 0.080358, loss: 2.4199
2022-03-02 09:19:27 - train: epoch 0063, iter [03200, 05004], lr: 0.080358, loss: 2.0680
2022-03-02 09:20:00 - train: epoch 0063, iter [03300, 05004], lr: 0.080358, loss: 2.0388
2022-03-02 09:20:33 - train: epoch 0063, iter [03400, 05004], lr: 0.080358, loss: 1.7487
2022-03-02 09:21:06 - train: epoch 0063, iter [03500, 05004], lr: 0.080358, loss: 2.1470
2022-03-02 09:21:39 - train: epoch 0063, iter [03600, 05004], lr: 0.080358, loss: 1.8036
2022-03-02 09:22:13 - train: epoch 0063, iter [03700, 05004], lr: 0.080358, loss: 1.9853
2022-03-02 09:22:46 - train: epoch 0063, iter [03800, 05004], lr: 0.080358, loss: 2.1629
2022-03-02 09:23:19 - train: epoch 0063, iter [03900, 05004], lr: 0.080358, loss: 1.7156
2022-03-02 09:23:52 - train: epoch 0063, iter [04000, 05004], lr: 0.080358, loss: 1.7652
2022-03-02 09:24:26 - train: epoch 0063, iter [04100, 05004], lr: 0.080358, loss: 2.0740
2022-03-02 09:24:58 - train: epoch 0063, iter [04200, 05004], lr: 0.080358, loss: 1.9421
2022-03-02 09:25:32 - train: epoch 0063, iter [04300, 05004], lr: 0.080358, loss: 2.1109
2022-03-02 09:26:05 - train: epoch 0063, iter [04400, 05004], lr: 0.080358, loss: 2.0035
2022-03-02 09:26:38 - train: epoch 0063, iter [04500, 05004], lr: 0.080358, loss: 1.9409
2022-03-02 09:27:11 - train: epoch 0063, iter [04600, 05004], lr: 0.080358, loss: 1.9499
2022-03-02 09:27:45 - train: epoch 0063, iter [04700, 05004], lr: 0.080358, loss: 1.9240
2022-03-02 09:28:18 - train: epoch 0063, iter [04800, 05004], lr: 0.080358, loss: 1.9711
2022-03-02 09:28:51 - train: epoch 0063, iter [04900, 05004], lr: 0.080358, loss: 1.9486
2022-03-02 09:29:23 - train: epoch 0063, iter [05000, 05004], lr: 0.080358, loss: 2.0399
2022-03-02 09:29:24 - train: epoch 063, train_loss: 2.0037
2022-03-02 09:30:37 - eval: epoch: 063, acc1: 57.552%, acc5: 81.932%, test_loss: 1.7781, per_image_load_time: 2.299ms, per_image_inference_time: 0.541ms
2022-03-02 09:30:38 - until epoch: 063, best_acc1: 58.018%
2022-03-02 09:30:38 - epoch 064 lr: 0.07971415196763088
2022-03-02 09:31:17 - train: epoch 0064, iter [00100, 05004], lr: 0.079714, loss: 1.9132
2022-03-02 09:31:49 - train: epoch 0064, iter [00200, 05004], lr: 0.079714, loss: 2.0482
2022-03-02 09:32:22 - train: epoch 0064, iter [00300, 05004], lr: 0.079714, loss: 1.8348
2022-03-02 09:32:56 - train: epoch 0064, iter [00400, 05004], lr: 0.079714, loss: 1.9998
2022-03-02 09:33:28 - train: epoch 0064, iter [00500, 05004], lr: 0.079714, loss: 1.8882
2022-03-02 09:34:02 - train: epoch 0064, iter [00600, 05004], lr: 0.079714, loss: 2.0597
2022-03-02 09:34:35 - train: epoch 0064, iter [00700, 05004], lr: 0.079714, loss: 1.9920
2022-03-02 09:35:08 - train: epoch 0064, iter [00800, 05004], lr: 0.079714, loss: 1.8819
2022-03-02 09:35:41 - train: epoch 0064, iter [00900, 05004], lr: 0.079714, loss: 1.9953
2022-03-02 09:36:14 - train: epoch 0064, iter [01000, 05004], lr: 0.079714, loss: 1.8965
2022-03-02 09:36:47 - train: epoch 0064, iter [01100, 05004], lr: 0.079714, loss: 1.9039
2022-03-02 09:37:21 - train: epoch 0064, iter [01200, 05004], lr: 0.079714, loss: 1.8755
2022-03-02 09:37:55 - train: epoch 0064, iter [01300, 05004], lr: 0.079714, loss: 2.0416
2022-03-02 09:38:27 - train: epoch 0064, iter [01400, 05004], lr: 0.079714, loss: 2.2617
2022-03-02 09:39:00 - train: epoch 0064, iter [01500, 05004], lr: 0.079714, loss: 1.9850
2022-03-02 09:39:34 - train: epoch 0064, iter [01600, 05004], lr: 0.079714, loss: 1.9183
2022-03-02 09:40:06 - train: epoch 0064, iter [01700, 05004], lr: 0.079714, loss: 1.7795
2022-03-02 09:40:40 - train: epoch 0064, iter [01800, 05004], lr: 0.079714, loss: 1.9779
2022-03-02 09:41:13 - train: epoch 0064, iter [01900, 05004], lr: 0.079714, loss: 1.9534
2022-03-02 09:41:46 - train: epoch 0064, iter [02000, 05004], lr: 0.079714, loss: 1.9199
2022-03-02 09:42:20 - train: epoch 0064, iter [02100, 05004], lr: 0.079714, loss: 1.9745
2022-03-02 09:42:53 - train: epoch 0064, iter [02200, 05004], lr: 0.079714, loss: 2.1700
2022-03-02 09:43:26 - train: epoch 0064, iter [02300, 05004], lr: 0.079714, loss: 2.0641
2022-03-02 09:43:59 - train: epoch 0064, iter [02400, 05004], lr: 0.079714, loss: 2.0217
2022-03-02 09:44:32 - train: epoch 0064, iter [02500, 05004], lr: 0.079714, loss: 1.9251
2022-03-02 09:45:05 - train: epoch 0064, iter [02600, 05004], lr: 0.079714, loss: 2.0006
2022-03-02 09:45:39 - train: epoch 0064, iter [02700, 05004], lr: 0.079714, loss: 1.9857
2022-03-02 09:46:12 - train: epoch 0064, iter [02800, 05004], lr: 0.079714, loss: 1.7713
2022-03-02 09:46:45 - train: epoch 0064, iter [02900, 05004], lr: 0.079714, loss: 2.2447
2022-03-02 09:47:18 - train: epoch 0064, iter [03000, 05004], lr: 0.079714, loss: 2.1202
2022-03-02 09:47:51 - train: epoch 0064, iter [03100, 05004], lr: 0.079714, loss: 1.9860
2022-03-02 09:48:24 - train: epoch 0064, iter [03200, 05004], lr: 0.079714, loss: 1.9560
2022-03-02 09:48:58 - train: epoch 0064, iter [03300, 05004], lr: 0.079714, loss: 1.9537
2022-03-02 09:49:31 - train: epoch 0064, iter [03400, 05004], lr: 0.079714, loss: 2.1065
2022-03-02 09:50:05 - train: epoch 0064, iter [03500, 05004], lr: 0.079714, loss: 1.8912
2022-03-02 09:50:38 - train: epoch 0064, iter [03600, 05004], lr: 0.079714, loss: 1.9547
2022-03-02 09:51:11 - train: epoch 0064, iter [03700, 05004], lr: 0.079714, loss: 1.5937
2022-03-02 09:51:45 - train: epoch 0064, iter [03800, 05004], lr: 0.079714, loss: 2.1421
2022-03-02 09:52:17 - train: epoch 0064, iter [03900, 05004], lr: 0.079714, loss: 1.9167
2022-03-02 09:52:51 - train: epoch 0064, iter [04000, 05004], lr: 0.079714, loss: 1.9220
2022-03-02 09:53:23 - train: epoch 0064, iter [04100, 05004], lr: 0.079714, loss: 2.0423
2022-03-02 09:53:57 - train: epoch 0064, iter [04200, 05004], lr: 0.079714, loss: 1.9481
2022-03-02 09:54:29 - train: epoch 0064, iter [04300, 05004], lr: 0.079714, loss: 2.1239
2022-03-02 09:55:03 - train: epoch 0064, iter [04400, 05004], lr: 0.079714, loss: 1.9346
2022-03-02 09:55:37 - train: epoch 0064, iter [04500, 05004], lr: 0.079714, loss: 1.9933
2022-03-02 09:56:10 - train: epoch 0064, iter [04600, 05004], lr: 0.079714, loss: 2.2499
2022-03-02 09:56:43 - train: epoch 0064, iter [04700, 05004], lr: 0.079714, loss: 2.4160
2022-03-02 09:57:16 - train: epoch 0064, iter [04800, 05004], lr: 0.079714, loss: 2.0023
2022-03-02 09:57:50 - train: epoch 0064, iter [04900, 05004], lr: 0.079714, loss: 2.1981
2022-03-02 09:58:21 - train: epoch 0064, iter [05000, 05004], lr: 0.079714, loss: 1.9040
2022-03-02 09:58:22 - train: epoch 064, train_loss: 1.9993
2022-03-02 09:59:36 - eval: epoch: 064, acc1: 56.600%, acc5: 81.298%, test_loss: 1.8227, per_image_load_time: 1.856ms, per_image_inference_time: 0.511ms
2022-03-02 09:59:37 - until epoch: 064, best_acc1: 58.018%
2022-03-02 09:59:37 - epoch 065 lr: 0.07906246623448183
2022-03-02 10:00:15 - train: epoch 0065, iter [00100, 05004], lr: 0.079062, loss: 2.1092
2022-03-02 10:00:49 - train: epoch 0065, iter [00200, 05004], lr: 0.079062, loss: 2.1121
2022-03-02 10:01:22 - train: epoch 0065, iter [00300, 05004], lr: 0.079062, loss: 1.9603
2022-03-02 10:01:56 - train: epoch 0065, iter [00400, 05004], lr: 0.079062, loss: 2.0892
2022-03-02 10:02:29 - train: epoch 0065, iter [00500, 05004], lr: 0.079062, loss: 1.8295
2022-03-02 10:03:02 - train: epoch 0065, iter [00600, 05004], lr: 0.079062, loss: 2.1190
2022-03-02 10:03:34 - train: epoch 0065, iter [00700, 05004], lr: 0.079062, loss: 1.9910
2022-03-02 10:04:07 - train: epoch 0065, iter [00800, 05004], lr: 0.079062, loss: 1.8993
2022-03-02 10:04:40 - train: epoch 0065, iter [00900, 05004], lr: 0.079062, loss: 1.8668
2022-03-02 10:05:13 - train: epoch 0065, iter [01000, 05004], lr: 0.079062, loss: 1.8531
2022-03-02 10:05:47 - train: epoch 0065, iter [01100, 05004], lr: 0.079062, loss: 1.9543
2022-03-02 10:06:20 - train: epoch 0065, iter [01200, 05004], lr: 0.079062, loss: 2.1540
2022-03-02 10:06:53 - train: epoch 0065, iter [01300, 05004], lr: 0.079062, loss: 1.9469
2022-03-02 10:07:26 - train: epoch 0065, iter [01400, 05004], lr: 0.079062, loss: 1.8181
2022-03-02 10:07:59 - train: epoch 0065, iter [01500, 05004], lr: 0.079062, loss: 2.0240
2022-03-02 10:08:32 - train: epoch 0065, iter [01600, 05004], lr: 0.079062, loss: 2.2605
2022-03-02 10:09:05 - train: epoch 0065, iter [01700, 05004], lr: 0.079062, loss: 2.0679
2022-03-02 10:09:38 - train: epoch 0065, iter [01800, 05004], lr: 0.079062, loss: 1.8713
2022-03-02 10:10:12 - train: epoch 0065, iter [01900, 05004], lr: 0.079062, loss: 1.7723
2022-03-02 10:10:45 - train: epoch 0065, iter [02000, 05004], lr: 0.079062, loss: 1.9536
2022-03-02 10:11:18 - train: epoch 0065, iter [02100, 05004], lr: 0.079062, loss: 1.9546
2022-03-02 10:11:52 - train: epoch 0065, iter [02200, 05004], lr: 0.079062, loss: 2.0403
2022-03-02 10:12:25 - train: epoch 0065, iter [02300, 05004], lr: 0.079062, loss: 1.8971
2022-03-02 10:12:59 - train: epoch 0065, iter [02400, 05004], lr: 0.079062, loss: 1.8788
2022-03-02 10:13:32 - train: epoch 0065, iter [02500, 05004], lr: 0.079062, loss: 2.0364
2022-03-02 10:14:05 - train: epoch 0065, iter [02600, 05004], lr: 0.079062, loss: 2.2334
2022-03-02 10:14:37 - train: epoch 0065, iter [02700, 05004], lr: 0.079062, loss: 2.1257
2022-03-02 10:15:11 - train: epoch 0065, iter [02800, 05004], lr: 0.079062, loss: 1.9541
2022-03-02 10:15:44 - train: epoch 0065, iter [02900, 05004], lr: 0.079062, loss: 1.9389
2022-03-02 10:16:18 - train: epoch 0065, iter [03000, 05004], lr: 0.079062, loss: 1.9100
2022-03-02 10:16:51 - train: epoch 0065, iter [03100, 05004], lr: 0.079062, loss: 1.9874
2022-03-02 10:17:23 - train: epoch 0065, iter [03200, 05004], lr: 0.079062, loss: 2.1554
2022-03-02 10:17:57 - train: epoch 0065, iter [03300, 05004], lr: 0.079062, loss: 2.0346
2022-03-02 10:18:30 - train: epoch 0065, iter [03400, 05004], lr: 0.079062, loss: 1.9163
2022-03-02 10:19:03 - train: epoch 0065, iter [03500, 05004], lr: 0.079062, loss: 2.1684
2022-03-02 10:19:36 - train: epoch 0065, iter [03600, 05004], lr: 0.079062, loss: 2.2277
2022-03-02 10:20:08 - train: epoch 0065, iter [03700, 05004], lr: 0.079062, loss: 1.8214
2022-03-02 10:20:41 - train: epoch 0065, iter [03800, 05004], lr: 0.079062, loss: 1.9406
2022-03-02 10:21:15 - train: epoch 0065, iter [03900, 05004], lr: 0.079062, loss: 2.1538
2022-03-02 10:21:48 - train: epoch 0065, iter [04000, 05004], lr: 0.079062, loss: 2.0259
2022-03-02 10:22:20 - train: epoch 0065, iter [04100, 05004], lr: 0.079062, loss: 1.7413
2022-03-02 10:22:53 - train: epoch 0065, iter [04200, 05004], lr: 0.079062, loss: 1.9744
2022-03-02 10:23:27 - train: epoch 0065, iter [04300, 05004], lr: 0.079062, loss: 1.9425
2022-03-02 10:24:00 - train: epoch 0065, iter [04400, 05004], lr: 0.079062, loss: 2.0418
2022-03-02 10:24:34 - train: epoch 0065, iter [04500, 05004], lr: 0.079062, loss: 2.0415
2022-03-02 10:25:07 - train: epoch 0065, iter [04600, 05004], lr: 0.079062, loss: 2.1022
2022-03-02 10:25:40 - train: epoch 0065, iter [04700, 05004], lr: 0.079062, loss: 1.9540
2022-03-02 10:26:14 - train: epoch 0065, iter [04800, 05004], lr: 0.079062, loss: 1.7287
2022-03-02 10:26:47 - train: epoch 0065, iter [04900, 05004], lr: 0.079062, loss: 2.0410
2022-03-02 10:27:18 - train: epoch 0065, iter [05000, 05004], lr: 0.079062, loss: 1.9753
2022-03-02 10:27:19 - train: epoch 065, train_loss: 1.9940
2022-03-02 10:28:33 - eval: epoch: 065, acc1: 58.868%, acc5: 82.640%, test_loss: 1.7307, per_image_load_time: 2.270ms, per_image_inference_time: 0.554ms
2022-03-02 10:28:34 - until epoch: 065, best_acc1: 58.868%
2022-03-02 10:28:34 - epoch 066 lr: 0.0784032373365578
2022-03-02 10:29:12 - train: epoch 0066, iter [00100, 05004], lr: 0.078403, loss: 1.8481
2022-03-02 10:29:45 - train: epoch 0066, iter [00200, 05004], lr: 0.078403, loss: 2.1696
2022-03-02 10:30:18 - train: epoch 0066, iter [00300, 05004], lr: 0.078403, loss: 1.8643
2022-03-02 10:30:51 - train: epoch 0066, iter [00400, 05004], lr: 0.078403, loss: 1.7098
2022-03-02 10:31:24 - train: epoch 0066, iter [00500, 05004], lr: 0.078403, loss: 2.0808
2022-03-02 10:31:57 - train: epoch 0066, iter [00600, 05004], lr: 0.078403, loss: 2.0269
2022-03-02 10:32:30 - train: epoch 0066, iter [00700, 05004], lr: 0.078403, loss: 2.0103
2022-03-02 10:33:03 - train: epoch 0066, iter [00800, 05004], lr: 0.078403, loss: 2.3571
2022-03-02 10:33:36 - train: epoch 0066, iter [00900, 05004], lr: 0.078403, loss: 2.0265
2022-03-02 10:34:10 - train: epoch 0066, iter [01000, 05004], lr: 0.078403, loss: 2.0372
2022-03-02 10:34:42 - train: epoch 0066, iter [01100, 05004], lr: 0.078403, loss: 1.9870
2022-03-02 10:35:15 - train: epoch 0066, iter [01200, 05004], lr: 0.078403, loss: 2.1182
2022-03-02 10:35:49 - train: epoch 0066, iter [01300, 05004], lr: 0.078403, loss: 2.1212
2022-03-02 10:36:22 - train: epoch 0066, iter [01400, 05004], lr: 0.078403, loss: 1.8764
2022-03-02 10:36:55 - train: epoch 0066, iter [01500, 05004], lr: 0.078403, loss: 2.0135
2022-03-02 10:37:28 - train: epoch 0066, iter [01600, 05004], lr: 0.078403, loss: 1.9941
2022-03-02 10:38:01 - train: epoch 0066, iter [01700, 05004], lr: 0.078403, loss: 1.9583
2022-03-02 10:38:35 - train: epoch 0066, iter [01800, 05004], lr: 0.078403, loss: 1.8508
2022-03-02 10:39:08 - train: epoch 0066, iter [01900, 05004], lr: 0.078403, loss: 2.2335
2022-03-02 10:39:41 - train: epoch 0066, iter [02000, 05004], lr: 0.078403, loss: 2.0192
2022-03-02 10:40:15 - train: epoch 0066, iter [02100, 05004], lr: 0.078403, loss: 2.1690
2022-03-02 10:40:46 - train: epoch 0066, iter [02200, 05004], lr: 0.078403, loss: 1.8883
2022-03-02 10:41:20 - train: epoch 0066, iter [02300, 05004], lr: 0.078403, loss: 2.1871
2022-03-02 10:41:53 - train: epoch 0066, iter [02400, 05004], lr: 0.078403, loss: 1.8621
2022-03-02 10:42:26 - train: epoch 0066, iter [02500, 05004], lr: 0.078403, loss: 1.9093
2022-03-02 10:42:59 - train: epoch 0066, iter [02600, 05004], lr: 0.078403, loss: 1.9871
2022-03-02 10:43:33 - train: epoch 0066, iter [02700, 05004], lr: 0.078403, loss: 2.1406
2022-03-02 10:44:06 - train: epoch 0066, iter [02800, 05004], lr: 0.078403, loss: 1.9811
2022-03-02 10:44:39 - train: epoch 0066, iter [02900, 05004], lr: 0.078403, loss: 2.0681
2022-03-02 10:45:12 - train: epoch 0066, iter [03000, 05004], lr: 0.078403, loss: 1.8049
2022-03-02 10:45:45 - train: epoch 0066, iter [03100, 05004], lr: 0.078403, loss: 2.0641
2022-03-02 10:46:19 - train: epoch 0066, iter [03200, 05004], lr: 0.078403, loss: 1.9082
2022-03-02 10:46:51 - train: epoch 0066, iter [03300, 05004], lr: 0.078403, loss: 1.9742
2022-03-02 10:47:24 - train: epoch 0066, iter [03400, 05004], lr: 0.078403, loss: 2.2814
2022-03-02 10:47:58 - train: epoch 0066, iter [03500, 05004], lr: 0.078403, loss: 1.9860
2022-03-02 10:48:31 - train: epoch 0066, iter [03600, 05004], lr: 0.078403, loss: 2.0343
2022-03-02 10:49:04 - train: epoch 0066, iter [03700, 05004], lr: 0.078403, loss: 2.1171
2022-03-02 10:49:37 - train: epoch 0066, iter [03800, 05004], lr: 0.078403, loss: 1.8648
2022-03-02 10:50:10 - train: epoch 0066, iter [03900, 05004], lr: 0.078403, loss: 1.7569
2022-03-02 10:50:43 - train: epoch 0066, iter [04000, 05004], lr: 0.078403, loss: 2.0437
2022-03-02 10:51:16 - train: epoch 0066, iter [04100, 05004], lr: 0.078403, loss: 1.8402
2022-03-02 10:51:49 - train: epoch 0066, iter [04200, 05004], lr: 0.078403, loss: 1.8111
2022-03-02 10:52:22 - train: epoch 0066, iter [04300, 05004], lr: 0.078403, loss: 1.8902
2022-03-02 10:52:55 - train: epoch 0066, iter [04400, 05004], lr: 0.078403, loss: 1.9622
2022-03-02 10:53:28 - train: epoch 0066, iter [04500, 05004], lr: 0.078403, loss: 2.1580
2022-03-02 10:54:02 - train: epoch 0066, iter [04600, 05004], lr: 0.078403, loss: 2.2326
2022-03-02 10:54:35 - train: epoch 0066, iter [04700, 05004], lr: 0.078403, loss: 1.9748
2022-03-02 10:55:08 - train: epoch 0066, iter [04800, 05004], lr: 0.078403, loss: 1.9656
2022-03-02 10:55:41 - train: epoch 0066, iter [04900, 05004], lr: 0.078403, loss: 2.0327
2022-03-02 10:56:12 - train: epoch 0066, iter [05000, 05004], lr: 0.078403, loss: 1.8682
2022-03-02 10:56:13 - train: epoch 066, train_loss: 1.9901
2022-03-02 10:57:27 - eval: epoch: 066, acc1: 58.242%, acc5: 82.610%, test_loss: 1.7472, per_image_load_time: 2.322ms, per_image_inference_time: 0.531ms
2022-03-02 10:57:27 - until epoch: 066, best_acc1: 58.868%
2022-03-02 10:57:27 - epoch 067 lr: 0.07773663637675694
2022-03-02 10:58:05 - train: epoch 0067, iter [00100, 05004], lr: 0.077737, loss: 1.8451
2022-03-02 10:58:38 - train: epoch 0067, iter [00200, 05004], lr: 0.077737, loss: 1.8808
2022-03-02 10:59:12 - train: epoch 0067, iter [00300, 05004], lr: 0.077737, loss: 2.1471
2022-03-02 10:59:44 - train: epoch 0067, iter [00400, 05004], lr: 0.077737, loss: 1.9821
2022-03-02 11:00:18 - train: epoch 0067, iter [00500, 05004], lr: 0.077737, loss: 1.8773
2022-03-02 11:00:51 - train: epoch 0067, iter [00600, 05004], lr: 0.077737, loss: 1.8614
2022-03-02 11:01:25 - train: epoch 0067, iter [00700, 05004], lr: 0.077737, loss: 2.0281
2022-03-02 11:01:56 - train: epoch 0067, iter [00800, 05004], lr: 0.077737, loss: 2.0651
2022-03-02 11:02:31 - train: epoch 0067, iter [00900, 05004], lr: 0.077737, loss: 2.2584
2022-03-02 11:03:03 - train: epoch 0067, iter [01000, 05004], lr: 0.077737, loss: 1.8928
2022-03-02 11:03:37 - train: epoch 0067, iter [01100, 05004], lr: 0.077737, loss: 2.1027
2022-03-02 11:04:09 - train: epoch 0067, iter [01200, 05004], lr: 0.077737, loss: 1.8612
2022-03-02 11:04:43 - train: epoch 0067, iter [01300, 05004], lr: 0.077737, loss: 2.1276
2022-03-02 11:05:16 - train: epoch 0067, iter [01400, 05004], lr: 0.077737, loss: 1.8917
2022-03-02 11:05:49 - train: epoch 0067, iter [01500, 05004], lr: 0.077737, loss: 1.9708
2022-03-02 11:06:21 - train: epoch 0067, iter [01600, 05004], lr: 0.077737, loss: 2.0297
2022-03-02 11:06:55 - train: epoch 0067, iter [01700, 05004], lr: 0.077737, loss: 1.8794
2022-03-02 11:07:28 - train: epoch 0067, iter [01800, 05004], lr: 0.077737, loss: 2.2899
2022-03-02 11:08:02 - train: epoch 0067, iter [01900, 05004], lr: 0.077737, loss: 1.9104
2022-03-02 11:08:34 - train: epoch 0067, iter [02000, 05004], lr: 0.077737, loss: 2.1124
2022-03-02 11:09:08 - train: epoch 0067, iter [02100, 05004], lr: 0.077737, loss: 1.8552
2022-03-02 11:09:41 - train: epoch 0067, iter [02200, 05004], lr: 0.077737, loss: 1.8459
2022-03-02 11:10:14 - train: epoch 0067, iter [02300, 05004], lr: 0.077737, loss: 1.9490
2022-03-02 11:10:47 - train: epoch 0067, iter [02400, 05004], lr: 0.077737, loss: 1.9053
2022-03-02 11:11:19 - train: epoch 0067, iter [02500, 05004], lr: 0.077737, loss: 1.8783
2022-03-02 11:11:53 - train: epoch 0067, iter [02600, 05004], lr: 0.077737, loss: 2.0024
2022-03-02 11:12:26 - train: epoch 0067, iter [02700, 05004], lr: 0.077737, loss: 1.9093
2022-03-02 11:13:00 - train: epoch 0067, iter [02800, 05004], lr: 0.077737, loss: 2.0166
2022-03-02 11:13:33 - train: epoch 0067, iter [02900, 05004], lr: 0.077737, loss: 2.0217
2022-03-02 11:14:06 - train: epoch 0067, iter [03000, 05004], lr: 0.077737, loss: 1.8139
2022-03-02 11:14:39 - train: epoch 0067, iter [03100, 05004], lr: 0.077737, loss: 1.7370
2022-03-02 11:15:12 - train: epoch 0067, iter [03200, 05004], lr: 0.077737, loss: 2.2600
2022-03-02 11:15:45 - train: epoch 0067, iter [03300, 05004], lr: 0.077737, loss: 1.8593
2022-03-02 11:16:18 - train: epoch 0067, iter [03400, 05004], lr: 0.077737, loss: 2.0379
2022-03-02 11:16:51 - train: epoch 0067, iter [03500, 05004], lr: 0.077737, loss: 1.8627
2022-03-02 11:17:24 - train: epoch 0067, iter [03600, 05004], lr: 0.077737, loss: 2.0035
2022-03-02 11:17:57 - train: epoch 0067, iter [03700, 05004], lr: 0.077737, loss: 2.0955
2022-03-02 11:18:29 - train: epoch 0067, iter [03800, 05004], lr: 0.077737, loss: 1.9290
2022-03-02 11:19:02 - train: epoch 0067, iter [03900, 05004], lr: 0.077737, loss: 2.1810
2022-03-02 11:19:35 - train: epoch 0067, iter [04000, 05004], lr: 0.077737, loss: 2.0419
2022-03-02 11:20:08 - train: epoch 0067, iter [04100, 05004], lr: 0.077737, loss: 2.0461
2022-03-02 11:20:41 - train: epoch 0067, iter [04200, 05004], lr: 0.077737, loss: 2.0901
2022-03-02 11:21:14 - train: epoch 0067, iter [04300, 05004], lr: 0.077737, loss: 1.8399
2022-03-02 11:21:47 - train: epoch 0067, iter [04400, 05004], lr: 0.077737, loss: 2.0038
2022-03-02 11:22:20 - train: epoch 0067, iter [04500, 05004], lr: 0.077737, loss: 1.9150
2022-03-02 11:22:53 - train: epoch 0067, iter [04600, 05004], lr: 0.077737, loss: 1.6652
2022-03-02 11:23:26 - train: epoch 0067, iter [04700, 05004], lr: 0.077737, loss: 1.8963
2022-03-02 11:23:59 - train: epoch 0067, iter [04800, 05004], lr: 0.077737, loss: 1.6817
2022-03-02 11:24:33 - train: epoch 0067, iter [04900, 05004], lr: 0.077737, loss: 1.9643
2022-03-02 11:25:05 - train: epoch 0067, iter [05000, 05004], lr: 0.077737, loss: 2.0751
2022-03-02 11:25:06 - train: epoch 067, train_loss: 1.9837
2022-03-02 11:26:20 - eval: epoch: 067, acc1: 58.874%, acc5: 82.802%, test_loss: 1.7236, per_image_load_time: 1.787ms, per_image_inference_time: 0.527ms
2022-03-02 11:26:21 - until epoch: 067, best_acc1: 58.874%
2022-03-02 11:26:21 - epoch 068 lr: 0.07706283637139658
2022-03-02 11:26:58 - train: epoch 0068, iter [00100, 05004], lr: 0.077063, loss: 1.9671
2022-03-02 11:27:32 - train: epoch 0068, iter [00200, 05004], lr: 0.077063, loss: 1.9370
2022-03-02 11:28:05 - train: epoch 0068, iter [00300, 05004], lr: 0.077063, loss: 2.1528
2022-03-02 11:28:37 - train: epoch 0068, iter [00400, 05004], lr: 0.077063, loss: 1.9070
2022-03-02 11:29:10 - train: epoch 0068, iter [00500, 05004], lr: 0.077063, loss: 1.8222
2022-03-02 11:29:43 - train: epoch 0068, iter [00600, 05004], lr: 0.077063, loss: 1.9294
2022-03-02 11:30:16 - train: epoch 0068, iter [00700, 05004], lr: 0.077063, loss: 2.3144
2022-03-02 11:30:49 - train: epoch 0068, iter [00800, 05004], lr: 0.077063, loss: 2.0427
2022-03-02 11:31:22 - train: epoch 0068, iter [00900, 05004], lr: 0.077063, loss: 1.9172
2022-03-02 11:31:55 - train: epoch 0068, iter [01000, 05004], lr: 0.077063, loss: 1.9960
2022-03-02 11:32:28 - train: epoch 0068, iter [01100, 05004], lr: 0.077063, loss: 2.2671
2022-03-02 11:33:02 - train: epoch 0068, iter [01200, 05004], lr: 0.077063, loss: 1.8333
2022-03-02 11:33:35 - train: epoch 0068, iter [01300, 05004], lr: 0.077063, loss: 1.9873
2022-03-02 11:34:08 - train: epoch 0068, iter [01400, 05004], lr: 0.077063, loss: 1.8531
2022-03-02 11:34:42 - train: epoch 0068, iter [01500, 05004], lr: 0.077063, loss: 2.0768
2022-03-02 11:35:15 - train: epoch 0068, iter [01600, 05004], lr: 0.077063, loss: 2.0605
2022-03-02 11:35:48 - train: epoch 0068, iter [01700, 05004], lr: 0.077063, loss: 2.1648
2022-03-02 11:36:22 - train: epoch 0068, iter [01800, 05004], lr: 0.077063, loss: 1.9863
2022-03-02 11:36:55 - train: epoch 0068, iter [01900, 05004], lr: 0.077063, loss: 2.1226
2022-03-02 11:37:29 - train: epoch 0068, iter [02000, 05004], lr: 0.077063, loss: 2.1925
2022-03-02 11:38:02 - train: epoch 0068, iter [02100, 05004], lr: 0.077063, loss: 2.0248
2022-03-02 11:38:35 - train: epoch 0068, iter [02200, 05004], lr: 0.077063, loss: 1.9819
2022-03-02 11:39:08 - train: epoch 0068, iter [02300, 05004], lr: 0.077063, loss: 1.9184
2022-03-02 11:39:42 - train: epoch 0068, iter [02400, 05004], lr: 0.077063, loss: 2.0460
2022-03-02 11:40:14 - train: epoch 0068, iter [02500, 05004], lr: 0.077063, loss: 2.0127
2022-03-02 11:40:48 - train: epoch 0068, iter [02600, 05004], lr: 0.077063, loss: 1.9600
2022-03-02 11:41:20 - train: epoch 0068, iter [02700, 05004], lr: 0.077063, loss: 2.0171
2022-03-02 11:41:54 - train: epoch 0068, iter [02800, 05004], lr: 0.077063, loss: 2.1696
2022-03-02 11:42:27 - train: epoch 0068, iter [02900, 05004], lr: 0.077063, loss: 2.0440
2022-03-02 11:43:00 - train: epoch 0068, iter [03000, 05004], lr: 0.077063, loss: 2.2026
2022-03-02 11:43:33 - train: epoch 0068, iter [03100, 05004], lr: 0.077063, loss: 1.7216
2022-03-02 11:44:07 - train: epoch 0068, iter [03200, 05004], lr: 0.077063, loss: 2.1911
2022-03-02 11:44:40 - train: epoch 0068, iter [03300, 05004], lr: 0.077063, loss: 2.0417
2022-03-02 11:45:13 - train: epoch 0068, iter [03400, 05004], lr: 0.077063, loss: 1.8340
2022-03-02 11:45:45 - train: epoch 0068, iter [03500, 05004], lr: 0.077063, loss: 2.1019
2022-03-02 11:46:19 - train: epoch 0068, iter [03600, 05004], lr: 0.077063, loss: 1.8969
2022-03-02 11:46:52 - train: epoch 0068, iter [03700, 05004], lr: 0.077063, loss: 2.0805
2022-03-02 11:47:25 - train: epoch 0068, iter [03800, 05004], lr: 0.077063, loss: 2.1067
2022-03-02 11:47:58 - train: epoch 0068, iter [03900, 05004], lr: 0.077063, loss: 2.0528
2022-03-02 11:48:31 - train: epoch 0068, iter [04000, 05004], lr: 0.077063, loss: 1.9886
2022-03-02 11:49:04 - train: epoch 0068, iter [04100, 05004], lr: 0.077063, loss: 1.6808
2022-03-02 11:49:37 - train: epoch 0068, iter [04200, 05004], lr: 0.077063, loss: 2.1228
2022-03-02 11:50:11 - train: epoch 0068, iter [04300, 05004], lr: 0.077063, loss: 1.9571
2022-03-02 11:50:43 - train: epoch 0068, iter [04400, 05004], lr: 0.077063, loss: 2.0032
2022-03-02 11:51:16 - train: epoch 0068, iter [04500, 05004], lr: 0.077063, loss: 2.0053
2022-03-02 11:51:50 - train: epoch 0068, iter [04600, 05004], lr: 0.077063, loss: 2.0846
2022-03-02 11:52:22 - train: epoch 0068, iter [04700, 05004], lr: 0.077063, loss: 2.1646
2022-03-02 11:52:55 - train: epoch 0068, iter [04800, 05004], lr: 0.077063, loss: 2.1618
2022-03-02 11:53:29 - train: epoch 0068, iter [04900, 05004], lr: 0.077063, loss: 2.1593
2022-03-02 11:54:01 - train: epoch 0068, iter [05000, 05004], lr: 0.077063, loss: 1.8977
2022-03-02 11:54:02 - train: epoch 068, train_loss: 1.9821
2022-03-02 11:55:15 - eval: epoch: 068, acc1: 56.724%, acc5: 81.424%, test_loss: 1.8157, per_image_load_time: 1.077ms, per_image_inference_time: 0.503ms
2022-03-02 11:55:16 - until epoch: 068, best_acc1: 58.874%
2022-03-02 11:55:16 - epoch 069 lr: 0.07638201220530665
2022-03-02 11:55:53 - train: epoch 0069, iter [00100, 05004], lr: 0.076382, loss: 2.2923
2022-03-02 11:56:27 - train: epoch 0069, iter [00200, 05004], lr: 0.076382, loss: 2.1563
2022-03-02 11:57:00 - train: epoch 0069, iter [00300, 05004], lr: 0.076382, loss: 1.9762
2022-03-02 11:57:33 - train: epoch 0069, iter [00400, 05004], lr: 0.076382, loss: 1.7888
2022-03-02 11:58:06 - train: epoch 0069, iter [00500, 05004], lr: 0.076382, loss: 1.7833
2022-03-02 11:58:39 - train: epoch 0069, iter [00600, 05004], lr: 0.076382, loss: 1.8371
2022-03-02 11:59:11 - train: epoch 0069, iter [00700, 05004], lr: 0.076382, loss: 1.9212
2022-03-02 11:59:45 - train: epoch 0069, iter [00800, 05004], lr: 0.076382, loss: 2.0465
2022-03-02 12:00:17 - train: epoch 0069, iter [00900, 05004], lr: 0.076382, loss: 1.9882
2022-03-02 12:00:51 - train: epoch 0069, iter [01000, 05004], lr: 0.076382, loss: 1.8156
2022-03-02 12:01:24 - train: epoch 0069, iter [01100, 05004], lr: 0.076382, loss: 1.9278
2022-03-02 12:01:58 - train: epoch 0069, iter [01200, 05004], lr: 0.076382, loss: 1.9358
2022-03-02 12:02:30 - train: epoch 0069, iter [01300, 05004], lr: 0.076382, loss: 2.2193
2022-03-02 12:03:04 - train: epoch 0069, iter [01400, 05004], lr: 0.076382, loss: 2.0174
2022-03-02 12:03:36 - train: epoch 0069, iter [01500, 05004], lr: 0.076382, loss: 2.0214
2022-03-02 12:04:10 - train: epoch 0069, iter [01600, 05004], lr: 0.076382, loss: 2.0144
2022-03-02 12:04:44 - train: epoch 0069, iter [01700, 05004], lr: 0.076382, loss: 1.9079
2022-03-02 12:05:17 - train: epoch 0069, iter [01800, 05004], lr: 0.076382, loss: 1.6781
2022-03-02 12:05:50 - train: epoch 0069, iter [01900, 05004], lr: 0.076382, loss: 2.0871
2022-03-02 12:06:23 - train: epoch 0069, iter [02000, 05004], lr: 0.076382, loss: 1.8397
2022-03-02 12:06:56 - train: epoch 0069, iter [02100, 05004], lr: 0.076382, loss: 1.9373
2022-03-02 12:07:29 - train: epoch 0069, iter [02200, 05004], lr: 0.076382, loss: 2.0161
2022-03-02 12:08:01 - train: epoch 0069, iter [02300, 05004], lr: 0.076382, loss: 1.9342
2022-03-02 12:08:34 - train: epoch 0069, iter [02400, 05004], lr: 0.076382, loss: 2.0885
2022-03-02 12:09:08 - train: epoch 0069, iter [02500, 05004], lr: 0.076382, loss: 2.0079
2022-03-02 12:09:41 - train: epoch 0069, iter [02600, 05004], lr: 0.076382, loss: 2.0266
2022-03-02 12:10:14 - train: epoch 0069, iter [02700, 05004], lr: 0.076382, loss: 2.1864
2022-03-02 12:10:47 - train: epoch 0069, iter [02800, 05004], lr: 0.076382, loss: 2.0024
2022-03-02 12:11:20 - train: epoch 0069, iter [02900, 05004], lr: 0.076382, loss: 1.8211
2022-03-02 12:11:54 - train: epoch 0069, iter [03000, 05004], lr: 0.076382, loss: 1.9874
2022-03-02 12:12:27 - train: epoch 0069, iter [03100, 05004], lr: 0.076382, loss: 1.9374
2022-03-02 12:13:01 - train: epoch 0069, iter [03200, 05004], lr: 0.076382, loss: 1.8533
2022-03-02 12:13:33 - train: epoch 0069, iter [03300, 05004], lr: 0.076382, loss: 1.7714
2022-03-02 12:14:06 - train: epoch 0069, iter [03400, 05004], lr: 0.076382, loss: 1.8554
2022-03-02 12:14:40 - train: epoch 0069, iter [03500, 05004], lr: 0.076382, loss: 1.8890
2022-03-02 12:15:13 - train: epoch 0069, iter [03600, 05004], lr: 0.076382, loss: 1.7661
2022-03-02 12:15:46 - train: epoch 0069, iter [03700, 05004], lr: 0.076382, loss: 2.1040
2022-03-02 12:16:19 - train: epoch 0069, iter [03800, 05004], lr: 0.076382, loss: 2.0357
2022-03-02 12:16:52 - train: epoch 0069, iter [03900, 05004], lr: 0.076382, loss: 1.9627
2022-03-02 12:17:26 - train: epoch 0069, iter [04000, 05004], lr: 0.076382, loss: 2.1542
2022-03-02 12:17:58 - train: epoch 0069, iter [04100, 05004], lr: 0.076382, loss: 2.0940
2022-03-02 12:18:32 - train: epoch 0069, iter [04200, 05004], lr: 0.076382, loss: 1.8047
2022-03-02 12:19:05 - train: epoch 0069, iter [04300, 05004], lr: 0.076382, loss: 2.0626
2022-03-02 12:19:38 - train: epoch 0069, iter [04400, 05004], lr: 0.076382, loss: 2.0592
2022-03-02 12:20:11 - train: epoch 0069, iter [04500, 05004], lr: 0.076382, loss: 1.9835
2022-03-02 12:20:45 - train: epoch 0069, iter [04600, 05004], lr: 0.076382, loss: 2.2403
2022-03-02 12:21:18 - train: epoch 0069, iter [04700, 05004], lr: 0.076382, loss: 1.9936
2022-03-02 12:21:50 - train: epoch 0069, iter [04800, 05004], lr: 0.076382, loss: 1.9460
2022-03-02 12:22:24 - train: epoch 0069, iter [04900, 05004], lr: 0.076382, loss: 1.8810
2022-03-02 12:22:55 - train: epoch 0069, iter [05000, 05004], lr: 0.076382, loss: 2.0003
2022-03-02 12:22:56 - train: epoch 069, train_loss: 1.9752
2022-03-02 12:24:10 - eval: epoch: 069, acc1: 57.962%, acc5: 82.424%, test_loss: 1.7536, per_image_load_time: 2.330ms, per_image_inference_time: 0.505ms
2022-03-02 12:24:10 - until epoch: 069, best_acc1: 58.874%
2022-03-02 12:24:10 - epoch 070 lr: 0.07569434058643844
2022-03-02 12:24:48 - train: epoch 0070, iter [00100, 05004], lr: 0.075694, loss: 1.9506
2022-03-02 12:25:21 - train: epoch 0070, iter [00200, 05004], lr: 0.075694, loss: 1.8810
2022-03-02 12:25:56 - train: epoch 0070, iter [00300, 05004], lr: 0.075694, loss: 2.0393
2022-03-02 12:26:27 - train: epoch 0070, iter [00400, 05004], lr: 0.075694, loss: 1.9381
2022-03-02 12:27:00 - train: epoch 0070, iter [00500, 05004], lr: 0.075694, loss: 2.0722
2022-03-02 12:27:34 - train: epoch 0070, iter [00600, 05004], lr: 0.075694, loss: 1.8654
2022-03-02 12:28:08 - train: epoch 0070, iter [00700, 05004], lr: 0.075694, loss: 1.8290
2022-03-02 12:28:41 - train: epoch 0070, iter [00800, 05004], lr: 0.075694, loss: 1.9154
2022-03-02 12:29:14 - train: epoch 0070, iter [00900, 05004], lr: 0.075694, loss: 2.0591
2022-03-02 12:29:49 - train: epoch 0070, iter [01000, 05004], lr: 0.075694, loss: 1.9940
2022-03-02 12:30:22 - train: epoch 0070, iter [01100, 05004], lr: 0.075694, loss: 2.3518
2022-03-02 12:30:55 - train: epoch 0070, iter [01200, 05004], lr: 0.075694, loss: 1.7717
2022-03-02 12:31:28 - train: epoch 0070, iter [01300, 05004], lr: 0.075694, loss: 1.9072
2022-03-02 12:32:02 - train: epoch 0070, iter [01400, 05004], lr: 0.075694, loss: 1.9716
2022-03-02 12:32:35 - train: epoch 0070, iter [01500, 05004], lr: 0.075694, loss: 1.8684
2022-03-02 12:33:08 - train: epoch 0070, iter [01600, 05004], lr: 0.075694, loss: 2.0724
2022-03-02 12:33:42 - train: epoch 0070, iter [01700, 05004], lr: 0.075694, loss: 1.9862
2022-03-02 12:34:15 - train: epoch 0070, iter [01800, 05004], lr: 0.075694, loss: 1.7767
2022-03-02 12:34:48 - train: epoch 0070, iter [01900, 05004], lr: 0.075694, loss: 1.7812
2022-03-02 12:35:22 - train: epoch 0070, iter [02000, 05004], lr: 0.075694, loss: 1.9850
2022-03-02 12:35:55 - train: epoch 0070, iter [02100, 05004], lr: 0.075694, loss: 2.1018
2022-03-02 12:36:28 - train: epoch 0070, iter [02200, 05004], lr: 0.075694, loss: 2.0261
2022-03-02 12:37:00 - train: epoch 0070, iter [02300, 05004], lr: 0.075694, loss: 1.8625
2022-03-02 12:37:35 - train: epoch 0070, iter [02400, 05004], lr: 0.075694, loss: 2.0847
2022-03-02 12:38:08 - train: epoch 0070, iter [02500, 05004], lr: 0.075694, loss: 1.9823
2022-03-02 12:38:42 - train: epoch 0070, iter [02600, 05004], lr: 0.075694, loss: 1.9598
2022-03-02 12:39:15 - train: epoch 0070, iter [02700, 05004], lr: 0.075694, loss: 1.9965
2022-03-02 12:39:48 - train: epoch 0070, iter [02800, 05004], lr: 0.075694, loss: 2.1502
2022-03-02 12:40:21 - train: epoch 0070, iter [02900, 05004], lr: 0.075694, loss: 2.1615
2022-03-02 12:40:54 - train: epoch 0070, iter [03000, 05004], lr: 0.075694, loss: 2.0623
2022-03-02 12:41:28 - train: epoch 0070, iter [03100, 05004], lr: 0.075694, loss: 2.0274
2022-03-02 12:42:01 - train: epoch 0070, iter [03200, 05004], lr: 0.075694, loss: 2.1438
2022-03-02 12:42:34 - train: epoch 0070, iter [03300, 05004], lr: 0.075694, loss: 2.0168
2022-03-02 12:43:07 - train: epoch 0070, iter [03400, 05004], lr: 0.075694, loss: 1.9064
2022-03-02 12:43:41 - train: epoch 0070, iter [03500, 05004], lr: 0.075694, loss: 1.9337
2022-03-02 12:44:14 - train: epoch 0070, iter [03600, 05004], lr: 0.075694, loss: 2.0490
2022-03-02 12:44:47 - train: epoch 0070, iter [03700, 05004], lr: 0.075694, loss: 2.0704
2022-03-02 12:45:20 - train: epoch 0070, iter [03800, 05004], lr: 0.075694, loss: 1.9054
2022-03-02 12:45:53 - train: epoch 0070, iter [03900, 05004], lr: 0.075694, loss: 1.7735
2022-03-02 12:46:27 - train: epoch 0070, iter [04000, 05004], lr: 0.075694, loss: 1.9302
2022-03-02 12:46:59 - train: epoch 0070, iter [04100, 05004], lr: 0.075694, loss: 1.9365
2022-03-02 12:47:33 - train: epoch 0070, iter [04200, 05004], lr: 0.075694, loss: 1.9870
2022-03-02 12:48:06 - train: epoch 0070, iter [04300, 05004], lr: 0.075694, loss: 2.0785
2022-03-02 12:48:40 - train: epoch 0070, iter [04400, 05004], lr: 0.075694, loss: 2.0681
2022-03-02 12:49:13 - train: epoch 0070, iter [04500, 05004], lr: 0.075694, loss: 2.0551
2022-03-02 12:49:46 - train: epoch 0070, iter [04600, 05004], lr: 0.075694, loss: 2.1610
2022-03-02 12:50:20 - train: epoch 0070, iter [04700, 05004], lr: 0.075694, loss: 2.0165
2022-03-02 12:50:52 - train: epoch 0070, iter [04800, 05004], lr: 0.075694, loss: 2.0205
2022-03-02 12:51:26 - train: epoch 0070, iter [04900, 05004], lr: 0.075694, loss: 2.1885
2022-03-02 12:51:58 - train: epoch 0070, iter [05000, 05004], lr: 0.075694, loss: 2.0529
2022-03-02 12:51:59 - train: epoch 070, train_loss: 1.9717
2022-03-02 12:53:13 - eval: epoch: 070, acc1: 56.808%, acc5: 81.604%, test_loss: 1.8090, per_image_load_time: 2.343ms, per_image_inference_time: 0.526ms
2022-03-02 12:53:14 - until epoch: 070, best_acc1: 58.874%
2022-03-02 12:53:14 - epoch 071 lr: 0.07500000000000001
2022-03-02 12:53:52 - train: epoch 0071, iter [00100, 05004], lr: 0.075000, loss: 1.6294
2022-03-02 12:54:25 - train: epoch 0071, iter [00200, 05004], lr: 0.075000, loss: 1.8755
2022-03-02 12:54:57 - train: epoch 0071, iter [00300, 05004], lr: 0.075000, loss: 1.9341
2022-03-02 12:55:30 - train: epoch 0071, iter [00400, 05004], lr: 0.075000, loss: 2.0805
2022-03-02 12:56:03 - train: epoch 0071, iter [00500, 05004], lr: 0.075000, loss: 1.9589
2022-03-02 12:56:36 - train: epoch 0071, iter [00600, 05004], lr: 0.075000, loss: 2.0419
2022-03-02 12:57:09 - train: epoch 0071, iter [00700, 05004], lr: 0.075000, loss: 2.1524
2022-03-02 12:57:42 - train: epoch 0071, iter [00800, 05004], lr: 0.075000, loss: 1.8434
2022-03-02 12:58:16 - train: epoch 0071, iter [00900, 05004], lr: 0.075000, loss: 2.0143
2022-03-02 12:58:49 - train: epoch 0071, iter [01000, 05004], lr: 0.075000, loss: 2.0423
2022-03-02 12:59:22 - train: epoch 0071, iter [01100, 05004], lr: 0.075000, loss: 2.1296
2022-03-02 12:59:56 - train: epoch 0071, iter [01200, 05004], lr: 0.075000, loss: 1.8903
2022-03-02 13:00:29 - train: epoch 0071, iter [01300, 05004], lr: 0.075000, loss: 2.0012
2022-03-02 13:01:02 - train: epoch 0071, iter [01400, 05004], lr: 0.075000, loss: 2.0643
2022-03-02 13:01:35 - train: epoch 0071, iter [01500, 05004], lr: 0.075000, loss: 1.7556
2022-03-02 13:02:09 - train: epoch 0071, iter [01600, 05004], lr: 0.075000, loss: 1.8077
2022-03-02 13:02:42 - train: epoch 0071, iter [01700, 05004], lr: 0.075000, loss: 1.9447
2022-03-02 13:03:15 - train: epoch 0071, iter [01800, 05004], lr: 0.075000, loss: 1.9971
2022-03-02 13:03:49 - train: epoch 0071, iter [01900, 05004], lr: 0.075000, loss: 2.0034
2022-03-02 13:04:21 - train: epoch 0071, iter [02000, 05004], lr: 0.075000, loss: 2.0539
2022-03-02 13:04:54 - train: epoch 0071, iter [02100, 05004], lr: 0.075000, loss: 1.8408
2022-03-02 13:05:28 - train: epoch 0071, iter [02200, 05004], lr: 0.075000, loss: 1.8505
2022-03-02 13:06:00 - train: epoch 0071, iter [02300, 05004], lr: 0.075000, loss: 1.9822
2022-03-02 13:06:34 - train: epoch 0071, iter [02400, 05004], lr: 0.075000, loss: 1.8213
2022-03-02 13:07:07 - train: epoch 0071, iter [02500, 05004], lr: 0.075000, loss: 2.0861
2022-03-02 13:07:39 - train: epoch 0071, iter [02600, 05004], lr: 0.075000, loss: 1.7200
2022-03-02 13:08:13 - train: epoch 0071, iter [02700, 05004], lr: 0.075000, loss: 2.1028
2022-03-02 13:08:45 - train: epoch 0071, iter [02800, 05004], lr: 0.075000, loss: 2.0597
2022-03-02 13:09:18 - train: epoch 0071, iter [02900, 05004], lr: 0.075000, loss: 1.8916
2022-03-02 13:09:52 - train: epoch 0071, iter [03000, 05004], lr: 0.075000, loss: 2.0872
2022-03-02 13:10:24 - train: epoch 0071, iter [03100, 05004], lr: 0.075000, loss: 1.8806
2022-03-02 13:10:57 - train: epoch 0071, iter [03200, 05004], lr: 0.075000, loss: 1.7148
2022-03-02 13:11:30 - train: epoch 0071, iter [03300, 05004], lr: 0.075000, loss: 1.8269
2022-03-02 13:12:02 - train: epoch 0071, iter [03400, 05004], lr: 0.075000, loss: 1.8631
2022-03-02 13:12:34 - train: epoch 0071, iter [03500, 05004], lr: 0.075000, loss: 2.0713
2022-03-02 13:13:08 - train: epoch 0071, iter [03600, 05004], lr: 0.075000, loss: 2.1825
2022-03-02 13:13:41 - train: epoch 0071, iter [03700, 05004], lr: 0.075000, loss: 2.0247
2022-03-02 13:14:15 - train: epoch 0071, iter [03800, 05004], lr: 0.075000, loss: 1.8722
2022-03-02 13:14:47 - train: epoch 0071, iter [03900, 05004], lr: 0.075000, loss: 2.0156
2022-03-02 13:15:21 - train: epoch 0071, iter [04000, 05004], lr: 0.075000, loss: 2.1146
2022-03-02 13:15:53 - train: epoch 0071, iter [04100, 05004], lr: 0.075000, loss: 1.8482
2022-03-02 13:16:26 - train: epoch 0071, iter [04200, 05004], lr: 0.075000, loss: 2.2176
2022-03-02 13:16:59 - train: epoch 0071, iter [04300, 05004], lr: 0.075000, loss: 2.0874
2022-03-02 13:17:32 - train: epoch 0071, iter [04400, 05004], lr: 0.075000, loss: 2.1008
2022-03-02 13:18:05 - train: epoch 0071, iter [04500, 05004], lr: 0.075000, loss: 2.0093
2022-03-02 13:18:38 - train: epoch 0071, iter [04600, 05004], lr: 0.075000, loss: 2.0214
2022-03-02 13:19:11 - train: epoch 0071, iter [04700, 05004], lr: 0.075000, loss: 1.8894
2022-03-02 13:19:44 - train: epoch 0071, iter [04800, 05004], lr: 0.075000, loss: 1.8043
2022-03-02 13:20:17 - train: epoch 0071, iter [04900, 05004], lr: 0.075000, loss: 1.8016
2022-03-02 13:20:49 - train: epoch 0071, iter [05000, 05004], lr: 0.075000, loss: 1.7770
2022-03-02 13:20:50 - train: epoch 071, train_loss: 1.9681
2022-03-02 13:22:04 - eval: epoch: 071, acc1: 54.886%, acc5: 79.710%, test_loss: 1.9318, per_image_load_time: 1.083ms, per_image_inference_time: 0.520ms
2022-03-02 13:22:05 - until epoch: 071, best_acc1: 58.874%
2022-03-02 13:22:05 - epoch 072 lr: 0.0742991706621303
2022-03-02 13:22:43 - train: epoch 0072, iter [00100, 05004], lr: 0.074299, loss: 2.1481
2022-03-02 13:23:16 - train: epoch 0072, iter [00200, 05004], lr: 0.074299, loss: 1.6687
2022-03-02 13:23:49 - train: epoch 0072, iter [00300, 05004], lr: 0.074299, loss: 1.9072
2022-03-02 13:24:22 - train: epoch 0072, iter [00400, 05004], lr: 0.074299, loss: 1.9319
2022-03-02 13:24:56 - train: epoch 0072, iter [00500, 05004], lr: 0.074299, loss: 1.7887
2022-03-02 13:25:29 - train: epoch 0072, iter [00600, 05004], lr: 0.074299, loss: 1.8177
2022-03-02 13:26:02 - train: epoch 0072, iter [00700, 05004], lr: 0.074299, loss: 1.9250
2022-03-02 13:26:35 - train: epoch 0072, iter [00800, 05004], lr: 0.074299, loss: 2.1681
2022-03-02 13:27:08 - train: epoch 0072, iter [00900, 05004], lr: 0.074299, loss: 1.8547
2022-03-02 13:27:42 - train: epoch 0072, iter [01000, 05004], lr: 0.074299, loss: 1.8478
2022-03-02 13:28:16 - train: epoch 0072, iter [01100, 05004], lr: 0.074299, loss: 2.1538
2022-03-02 13:28:49 - train: epoch 0072, iter [01200, 05004], lr: 0.074299, loss: 1.8197
2022-03-02 13:29:22 - train: epoch 0072, iter [01300, 05004], lr: 0.074299, loss: 1.8824
2022-03-02 13:29:56 - train: epoch 0072, iter [01400, 05004], lr: 0.074299, loss: 1.9927
2022-03-02 13:30:29 - train: epoch 0072, iter [01500, 05004], lr: 0.074299, loss: 1.9991
2022-03-02 13:31:03 - train: epoch 0072, iter [01600, 05004], lr: 0.074299, loss: 1.9842
2022-03-02 13:31:35 - train: epoch 0072, iter [01700, 05004], lr: 0.074299, loss: 1.7793
2022-03-02 13:32:08 - train: epoch 0072, iter [01800, 05004], lr: 0.074299, loss: 1.8121
2022-03-02 13:32:42 - train: epoch 0072, iter [01900, 05004], lr: 0.074299, loss: 1.8481
2022-03-02 13:33:15 - train: epoch 0072, iter [02000, 05004], lr: 0.074299, loss: 2.0385
2022-03-02 13:33:48 - train: epoch 0072, iter [02100, 05004], lr: 0.074299, loss: 2.0993
2022-03-02 13:34:21 - train: epoch 0072, iter [02200, 05004], lr: 0.074299, loss: 2.1186
2022-03-02 13:34:54 - train: epoch 0072, iter [02300, 05004], lr: 0.074299, loss: 2.1004
2022-03-02 13:35:27 - train: epoch 0072, iter [02400, 05004], lr: 0.074299, loss: 1.8136
2022-03-02 13:36:00 - train: epoch 0072, iter [02500, 05004], lr: 0.074299, loss: 1.8005
2022-03-02 13:36:32 - train: epoch 0072, iter [02600, 05004], lr: 0.074299, loss: 1.8437
2022-03-02 13:37:06 - train: epoch 0072, iter [02700, 05004], lr: 0.074299, loss: 1.8829
2022-03-02 13:37:39 - train: epoch 0072, iter [02800, 05004], lr: 0.074299, loss: 1.9706
2022-03-02 13:38:12 - train: epoch 0072, iter [02900, 05004], lr: 0.074299, loss: 1.8917
2022-03-02 13:38:45 - train: epoch 0072, iter [03000, 05004], lr: 0.074299, loss: 2.0708
2022-03-02 13:39:18 - train: epoch 0072, iter [03100, 05004], lr: 0.074299, loss: 2.1119
2022-03-02 13:39:51 - train: epoch 0072, iter [03200, 05004], lr: 0.074299, loss: 1.8374
2022-03-02 13:40:24 - train: epoch 0072, iter [03300, 05004], lr: 0.074299, loss: 1.9257
2022-03-02 13:40:58 - train: epoch 0072, iter [03400, 05004], lr: 0.074299, loss: 2.1500
2022-03-02 13:41:31 - train: epoch 0072, iter [03500, 05004], lr: 0.074299, loss: 1.9597
2022-03-02 13:42:04 - train: epoch 0072, iter [03600, 05004], lr: 0.074299, loss: 2.0584
2022-03-02 13:42:36 - train: epoch 0072, iter [03700, 05004], lr: 0.074299, loss: 2.0747
2022-03-02 13:43:10 - train: epoch 0072, iter [03800, 05004], lr: 0.074299, loss: 2.0333
2022-03-02 13:43:43 - train: epoch 0072, iter [03900, 05004], lr: 0.074299, loss: 2.1601
2022-03-02 13:44:16 - train: epoch 0072, iter [04000, 05004], lr: 0.074299, loss: 1.9275
2022-03-02 13:44:49 - train: epoch 0072, iter [04100, 05004], lr: 0.074299, loss: 2.1459
2022-03-02 13:45:22 - train: epoch 0072, iter [04200, 05004], lr: 0.074299, loss: 1.8994
2022-03-02 13:45:55 - train: epoch 0072, iter [04300, 05004], lr: 0.074299, loss: 2.0032
2022-03-02 13:46:29 - train: epoch 0072, iter [04400, 05004], lr: 0.074299, loss: 1.8042
2022-03-02 13:47:01 - train: epoch 0072, iter [04500, 05004], lr: 0.074299, loss: 2.1842
2022-03-02 13:47:34 - train: epoch 0072, iter [04600, 05004], lr: 0.074299, loss: 1.8369
2022-03-02 13:48:07 - train: epoch 0072, iter [04700, 05004], lr: 0.074299, loss: 2.1200
2022-03-02 13:48:41 - train: epoch 0072, iter [04800, 05004], lr: 0.074299, loss: 2.1359
2022-03-02 13:49:14 - train: epoch 0072, iter [04900, 05004], lr: 0.074299, loss: 1.9724
2022-03-02 13:49:45 - train: epoch 0072, iter [05000, 05004], lr: 0.074299, loss: 1.9323
2022-03-02 13:49:46 - train: epoch 072, train_loss: 1.9651
2022-03-02 13:51:00 - eval: epoch: 072, acc1: 57.874%, acc5: 82.494%, test_loss: 1.7497, per_image_load_time: 2.368ms, per_image_inference_time: 0.514ms
2022-03-02 13:51:01 - until epoch: 072, best_acc1: 58.874%
2022-03-02 13:51:01 - epoch 073 lr: 0.07359203447312411
2022-03-02 13:51:39 - train: epoch 0073, iter [00100, 05004], lr: 0.073592, loss: 2.2445
2022-03-02 13:52:12 - train: epoch 0073, iter [00200, 05004], lr: 0.073592, loss: 1.9349
2022-03-02 13:52:45 - train: epoch 0073, iter [00300, 05004], lr: 0.073592, loss: 1.9544
2022-03-02 13:53:19 - train: epoch 0073, iter [00400, 05004], lr: 0.073592, loss: 1.7092
2022-03-02 13:53:51 - train: epoch 0073, iter [00500, 05004], lr: 0.073592, loss: 1.7857
2022-03-02 13:54:25 - train: epoch 0073, iter [00600, 05004], lr: 0.073592, loss: 1.9473
2022-03-02 13:54:58 - train: epoch 0073, iter [00700, 05004], lr: 0.073592, loss: 2.0646
2022-03-02 13:55:32 - train: epoch 0073, iter [00800, 05004], lr: 0.073592, loss: 1.9117
2022-03-02 13:56:04 - train: epoch 0073, iter [00900, 05004], lr: 0.073592, loss: 1.7478
2022-03-02 13:56:38 - train: epoch 0073, iter [01000, 05004], lr: 0.073592, loss: 1.9304
2022-03-02 13:57:12 - train: epoch 0073, iter [01100, 05004], lr: 0.073592, loss: 2.0192
2022-03-02 13:57:45 - train: epoch 0073, iter [01200, 05004], lr: 0.073592, loss: 1.8991
2022-03-02 13:58:18 - train: epoch 0073, iter [01300, 05004], lr: 0.073592, loss: 1.9708
2022-03-02 13:58:51 - train: epoch 0073, iter [01400, 05004], lr: 0.073592, loss: 1.8744
2022-03-02 13:59:24 - train: epoch 0073, iter [01500, 05004], lr: 0.073592, loss: 1.9302
2022-03-02 13:59:57 - train: epoch 0073, iter [01600, 05004], lr: 0.073592, loss: 1.9282
2022-03-02 14:00:30 - train: epoch 0073, iter [01700, 05004], lr: 0.073592, loss: 2.3274
2022-03-02 14:01:03 - train: epoch 0073, iter [01800, 05004], lr: 0.073592, loss: 1.7597
2022-03-02 14:01:37 - train: epoch 0073, iter [01900, 05004], lr: 0.073592, loss: 2.0120
2022-03-02 14:02:10 - train: epoch 0073, iter [02000, 05004], lr: 0.073592, loss: 1.7100
2022-03-02 14:02:44 - train: epoch 0073, iter [02100, 05004], lr: 0.073592, loss: 2.0433
2022-03-02 14:03:17 - train: epoch 0073, iter [02200, 05004], lr: 0.073592, loss: 2.1088
2022-03-02 14:03:50 - train: epoch 0073, iter [02300, 05004], lr: 0.073592, loss: 2.1179
2022-03-02 14:04:23 - train: epoch 0073, iter [02400, 05004], lr: 0.073592, loss: 1.8961
2022-03-02 14:04:56 - train: epoch 0073, iter [02500, 05004], lr: 0.073592, loss: 2.1866
2022-03-02 14:05:29 - train: epoch 0073, iter [02600, 05004], lr: 0.073592, loss: 2.0679
2022-03-02 14:06:02 - train: epoch 0073, iter [02700, 05004], lr: 0.073592, loss: 1.9612
2022-03-02 14:06:36 - train: epoch 0073, iter [02800, 05004], lr: 0.073592, loss: 1.9561
2022-03-02 14:07:08 - train: epoch 0073, iter [02900, 05004], lr: 0.073592, loss: 2.1229
2022-03-02 14:07:42 - train: epoch 0073, iter [03000, 05004], lr: 0.073592, loss: 1.8233
2022-03-02 14:08:15 - train: epoch 0073, iter [03100, 05004], lr: 0.073592, loss: 1.9395
2022-03-02 14:08:48 - train: epoch 0073, iter [03200, 05004], lr: 0.073592, loss: 1.8224
2022-03-02 14:09:22 - train: epoch 0073, iter [03300, 05004], lr: 0.073592, loss: 2.0979
2022-03-02 14:09:55 - train: epoch 0073, iter [03400, 05004], lr: 0.073592, loss: 1.9899
2022-03-02 14:10:27 - train: epoch 0073, iter [03500, 05004], lr: 0.073592, loss: 1.9886
2022-03-02 14:11:01 - train: epoch 0073, iter [03600, 05004], lr: 0.073592, loss: 1.8104
2022-03-02 14:11:35 - train: epoch 0073, iter [03700, 05004], lr: 0.073592, loss: 1.9991
2022-03-02 14:12:08 - train: epoch 0073, iter [03800, 05004], lr: 0.073592, loss: 2.1800
2022-03-02 14:12:41 - train: epoch 0073, iter [03900, 05004], lr: 0.073592, loss: 1.9631
2022-03-02 14:13:14 - train: epoch 0073, iter [04000, 05004], lr: 0.073592, loss: 2.2314
2022-03-02 14:13:47 - train: epoch 0073, iter [04100, 05004], lr: 0.073592, loss: 1.8689
2022-03-02 14:14:20 - train: epoch 0073, iter [04200, 05004], lr: 0.073592, loss: 2.2226
2022-03-02 14:14:53 - train: epoch 0073, iter [04300, 05004], lr: 0.073592, loss: 1.8916
2022-03-02 14:15:27 - train: epoch 0073, iter [04400, 05004], lr: 0.073592, loss: 2.0272
2022-03-02 14:16:00 - train: epoch 0073, iter [04500, 05004], lr: 0.073592, loss: 1.8444
2022-03-02 14:16:33 - train: epoch 0073, iter [04600, 05004], lr: 0.073592, loss: 2.1206
2022-03-02 14:17:06 - train: epoch 0073, iter [04700, 05004], lr: 0.073592, loss: 1.7470
2022-03-02 14:17:39 - train: epoch 0073, iter [04800, 05004], lr: 0.073592, loss: 2.1288
2022-03-02 14:18:12 - train: epoch 0073, iter [04900, 05004], lr: 0.073592, loss: 1.8505
2022-03-02 14:18:44 - train: epoch 0073, iter [05000, 05004], lr: 0.073592, loss: 2.0870
2022-03-02 14:18:45 - train: epoch 073, train_loss: 1.9577
2022-03-02 14:19:58 - eval: epoch: 073, acc1: 57.914%, acc5: 82.210%, test_loss: 1.7688, per_image_load_time: 2.316ms, per_image_inference_time: 0.531ms
2022-03-02 14:19:58 - until epoch: 073, best_acc1: 58.874%
2022-03-02 14:19:58 - epoch 074 lr: 0.07287877497021977
2022-03-02 14:20:37 - train: epoch 0074, iter [00100, 05004], lr: 0.072879, loss: 1.8314
2022-03-02 14:21:09 - train: epoch 0074, iter [00200, 05004], lr: 0.072879, loss: 1.9722
2022-03-02 14:21:42 - train: epoch 0074, iter [00300, 05004], lr: 0.072879, loss: 1.9681
2022-03-02 14:22:16 - train: epoch 0074, iter [00400, 05004], lr: 0.072879, loss: 2.2754
2022-03-02 14:22:49 - train: epoch 0074, iter [00500, 05004], lr: 0.072879, loss: 1.7845
2022-03-02 14:23:22 - train: epoch 0074, iter [00600, 05004], lr: 0.072879, loss: 2.0986
2022-03-02 14:23:55 - train: epoch 0074, iter [00700, 05004], lr: 0.072879, loss: 1.9736
2022-03-02 14:24:28 - train: epoch 0074, iter [00800, 05004], lr: 0.072879, loss: 2.0408
2022-03-02 14:25:01 - train: epoch 0074, iter [00900, 05004], lr: 0.072879, loss: 1.7407
2022-03-02 14:25:34 - train: epoch 0074, iter [01000, 05004], lr: 0.072879, loss: 2.1022
2022-03-02 14:26:08 - train: epoch 0074, iter [01100, 05004], lr: 0.072879, loss: 2.1614
2022-03-02 14:26:41 - train: epoch 0074, iter [01200, 05004], lr: 0.072879, loss: 1.8732
2022-03-02 14:27:14 - train: epoch 0074, iter [01300, 05004], lr: 0.072879, loss: 2.1272
2022-03-02 14:27:47 - train: epoch 0074, iter [01400, 05004], lr: 0.072879, loss: 1.8647
2022-03-02 14:28:20 - train: epoch 0074, iter [01500, 05004], lr: 0.072879, loss: 1.9906
2022-03-02 14:28:53 - train: epoch 0074, iter [01600, 05004], lr: 0.072879, loss: 1.7581
2022-03-02 14:29:26 - train: epoch 0074, iter [01700, 05004], lr: 0.072879, loss: 2.0393
2022-03-02 14:29:59 - train: epoch 0074, iter [01800, 05004], lr: 0.072879, loss: 2.3110
2022-03-02 14:30:33 - train: epoch 0074, iter [01900, 05004], lr: 0.072879, loss: 1.9832
2022-03-02 14:31:06 - train: epoch 0074, iter [02000, 05004], lr: 0.072879, loss: 1.6544
2022-03-02 14:31:38 - train: epoch 0074, iter [02100, 05004], lr: 0.072879, loss: 1.9644
2022-03-02 14:32:12 - train: epoch 0074, iter [02200, 05004], lr: 0.072879, loss: 2.3316
2022-03-02 14:32:45 - train: epoch 0074, iter [02300, 05004], lr: 0.072879, loss: 2.1226
2022-03-02 14:33:18 - train: epoch 0074, iter [02400, 05004], lr: 0.072879, loss: 1.9731
2022-03-02 14:33:51 - train: epoch 0074, iter [02500, 05004], lr: 0.072879, loss: 1.7108
2022-03-02 14:34:24 - train: epoch 0074, iter [02600, 05004], lr: 0.072879, loss: 1.6631
2022-03-02 14:34:58 - train: epoch 0074, iter [02700, 05004], lr: 0.072879, loss: 1.8382
2022-03-02 14:35:30 - train: epoch 0074, iter [02800, 05004], lr: 0.072879, loss: 2.1546
2022-03-02 14:36:05 - train: epoch 0074, iter [02900, 05004], lr: 0.072879, loss: 2.0585
2022-03-02 14:36:38 - train: epoch 0074, iter [03000, 05004], lr: 0.072879, loss: 1.9130
2022-03-02 14:37:10 - train: epoch 0074, iter [03100, 05004], lr: 0.072879, loss: 1.9117
2022-03-02 14:37:44 - train: epoch 0074, iter [03200, 05004], lr: 0.072879, loss: 1.7524
2022-03-02 14:38:17 - train: epoch 0074, iter [03300, 05004], lr: 0.072879, loss: 2.0634
2022-03-02 14:38:49 - train: epoch 0074, iter [03400, 05004], lr: 0.072879, loss: 2.0028
2022-03-02 14:39:23 - train: epoch 0074, iter [03500, 05004], lr: 0.072879, loss: 1.8702
2022-03-02 14:39:56 - train: epoch 0074, iter [03600, 05004], lr: 0.072879, loss: 2.0102
2022-03-02 14:40:29 - train: epoch 0074, iter [03700, 05004], lr: 0.072879, loss: 2.0932
2022-03-02 14:41:02 - train: epoch 0074, iter [03800, 05004], lr: 0.072879, loss: 1.8589
2022-03-02 14:41:36 - train: epoch 0074, iter [03900, 05004], lr: 0.072879, loss: 1.7584
2022-03-02 14:42:09 - train: epoch 0074, iter [04000, 05004], lr: 0.072879, loss: 1.8398
2022-03-02 14:42:42 - train: epoch 0074, iter [04100, 05004], lr: 0.072879, loss: 2.0393
2022-03-02 14:43:15 - train: epoch 0074, iter [04200, 05004], lr: 0.072879, loss: 2.0138
2022-03-02 14:43:49 - train: epoch 0074, iter [04300, 05004], lr: 0.072879, loss: 2.0129
2022-03-02 14:44:22 - train: epoch 0074, iter [04400, 05004], lr: 0.072879, loss: 1.8224
2022-03-02 14:44:55 - train: epoch 0074, iter [04500, 05004], lr: 0.072879, loss: 1.7968
2022-03-02 14:45:29 - train: epoch 0074, iter [04600, 05004], lr: 0.072879, loss: 2.1288
2022-03-02 14:46:02 - train: epoch 0074, iter [04700, 05004], lr: 0.072879, loss: 1.9897
2022-03-02 14:46:35 - train: epoch 0074, iter [04800, 05004], lr: 0.072879, loss: 2.0410
2022-03-02 14:47:08 - train: epoch 0074, iter [04900, 05004], lr: 0.072879, loss: 2.2167
2022-03-02 14:47:40 - train: epoch 0074, iter [05000, 05004], lr: 0.072879, loss: 1.8737
2022-03-02 14:47:41 - train: epoch 074, train_loss: 1.9520
2022-03-02 14:48:55 - eval: epoch: 074, acc1: 58.602%, acc5: 82.958%, test_loss: 1.7193, per_image_load_time: 2.301ms, per_image_inference_time: 0.536ms
2022-03-02 14:48:56 - until epoch: 074, best_acc1: 58.874%
2022-03-02 14:48:56 - epoch 075 lr: 0.07215957727996207
2022-03-02 14:49:34 - train: epoch 0075, iter [00100, 05004], lr: 0.072160, loss: 1.9536
2022-03-02 14:50:07 - train: epoch 0075, iter [00200, 05004], lr: 0.072160, loss: 1.8063
2022-03-02 14:50:39 - train: epoch 0075, iter [00300, 05004], lr: 0.072160, loss: 1.8788
2022-03-02 14:51:13 - train: epoch 0075, iter [00400, 05004], lr: 0.072160, loss: 1.9596
2022-03-02 14:51:46 - train: epoch 0075, iter [00500, 05004], lr: 0.072160, loss: 1.7767
2022-03-02 14:52:18 - train: epoch 0075, iter [00600, 05004], lr: 0.072160, loss: 1.8914
2022-03-02 14:52:52 - train: epoch 0075, iter [00700, 05004], lr: 0.072160, loss: 2.1622
2022-03-02 14:53:26 - train: epoch 0075, iter [00800, 05004], lr: 0.072160, loss: 2.1112
2022-03-02 14:53:59 - train: epoch 0075, iter [00900, 05004], lr: 0.072160, loss: 2.0646
2022-03-02 14:54:32 - train: epoch 0075, iter [01000, 05004], lr: 0.072160, loss: 1.7418
2022-03-02 14:55:05 - train: epoch 0075, iter [01100, 05004], lr: 0.072160, loss: 2.0075
2022-03-02 14:55:38 - train: epoch 0075, iter [01200, 05004], lr: 0.072160, loss: 1.7481
2022-03-02 14:56:11 - train: epoch 0075, iter [01300, 05004], lr: 0.072160, loss: 1.7568
2022-03-02 14:56:44 - train: epoch 0075, iter [01400, 05004], lr: 0.072160, loss: 1.8031
2022-03-02 14:57:17 - train: epoch 0075, iter [01500, 05004], lr: 0.072160, loss: 2.1874
2022-03-02 14:57:50 - train: epoch 0075, iter [01600, 05004], lr: 0.072160, loss: 1.5498
2022-03-02 14:58:23 - train: epoch 0075, iter [01700, 05004], lr: 0.072160, loss: 2.0232
2022-03-02 14:58:56 - train: epoch 0075, iter [01800, 05004], lr: 0.072160, loss: 1.9655
2022-03-02 14:59:28 - train: epoch 0075, iter [01900, 05004], lr: 0.072160, loss: 1.7836
2022-03-02 15:00:01 - train: epoch 0075, iter [02000, 05004], lr: 0.072160, loss: 1.9132
2022-03-02 15:00:34 - train: epoch 0075, iter [02100, 05004], lr: 0.072160, loss: 1.7207
2022-03-02 15:01:08 - train: epoch 0075, iter [02200, 05004], lr: 0.072160, loss: 1.9756
2022-03-02 15:01:40 - train: epoch 0075, iter [02300, 05004], lr: 0.072160, loss: 1.8700
2022-03-02 15:02:13 - train: epoch 0075, iter [02400, 05004], lr: 0.072160, loss: 1.9543
2022-03-02 15:02:46 - train: epoch 0075, iter [02500, 05004], lr: 0.072160, loss: 2.0349
2022-03-02 15:03:19 - train: epoch 0075, iter [02600, 05004], lr: 0.072160, loss: 2.1497
2022-03-02 15:03:52 - train: epoch 0075, iter [02700, 05004], lr: 0.072160, loss: 1.7424
2022-03-02 15:04:25 - train: epoch 0075, iter [02800, 05004], lr: 0.072160, loss: 2.0144
2022-03-02 15:04:58 - train: epoch 0075, iter [02900, 05004], lr: 0.072160, loss: 2.0859
2022-03-02 15:05:32 - train: epoch 0075, iter [03000, 05004], lr: 0.072160, loss: 2.1425
2022-03-02 15:06:05 - train: epoch 0075, iter [03100, 05004], lr: 0.072160, loss: 2.2332
2022-03-02 15:06:38 - train: epoch 0075, iter [03200, 05004], lr: 0.072160, loss: 2.0853
2022-03-02 15:07:11 - train: epoch 0075, iter [03300, 05004], lr: 0.072160, loss: 2.0316
2022-03-02 15:07:43 - train: epoch 0075, iter [03400, 05004], lr: 0.072160, loss: 1.8587
2022-03-02 15:08:17 - train: epoch 0075, iter [03500, 05004], lr: 0.072160, loss: 1.8681
2022-03-02 15:08:50 - train: epoch 0075, iter [03600, 05004], lr: 0.072160, loss: 2.0301
2022-03-02 15:09:23 - train: epoch 0075, iter [03700, 05004], lr: 0.072160, loss: 2.0324
2022-03-02 15:09:57 - train: epoch 0075, iter [03800, 05004], lr: 0.072160, loss: 1.9211
2022-03-02 15:10:29 - train: epoch 0075, iter [03900, 05004], lr: 0.072160, loss: 2.1199
2022-03-02 15:11:01 - train: epoch 0075, iter [04000, 05004], lr: 0.072160, loss: 1.8874
2022-03-02 15:11:35 - train: epoch 0075, iter [04100, 05004], lr: 0.072160, loss: 1.6753
2022-03-02 15:12:07 - train: epoch 0075, iter [04200, 05004], lr: 0.072160, loss: 2.0203
2022-03-02 15:12:40 - train: epoch 0075, iter [04300, 05004], lr: 0.072160, loss: 2.1997
2022-03-02 15:13:13 - train: epoch 0075, iter [04400, 05004], lr: 0.072160, loss: 1.6868
2022-03-02 15:13:47 - train: epoch 0075, iter [04500, 05004], lr: 0.072160, loss: 1.8776
2022-03-02 15:14:19 - train: epoch 0075, iter [04600, 05004], lr: 0.072160, loss: 1.7534
2022-03-02 15:14:53 - train: epoch 0075, iter [04700, 05004], lr: 0.072160, loss: 2.2038
2022-03-02 15:15:26 - train: epoch 0075, iter [04800, 05004], lr: 0.072160, loss: 1.9748
2022-03-02 15:15:59 - train: epoch 0075, iter [04900, 05004], lr: 0.072160, loss: 1.7247
2022-03-02 15:16:30 - train: epoch 0075, iter [05000, 05004], lr: 0.072160, loss: 2.0237
2022-03-02 15:16:32 - train: epoch 075, train_loss: 1.9486
2022-03-02 15:17:46 - eval: epoch: 075, acc1: 59.334%, acc5: 83.092%, test_loss: 1.7076, per_image_load_time: 2.381ms, per_image_inference_time: 0.512ms
2022-03-02 15:17:47 - until epoch: 075, best_acc1: 59.334%
2022-03-02 15:17:47 - epoch 076 lr: 0.0714346280701527
2022-03-02 15:18:25 - train: epoch 0076, iter [00100, 05004], lr: 0.071435, loss: 1.7565
2022-03-02 15:18:56 - train: epoch 0076, iter [00200, 05004], lr: 0.071435, loss: 1.8756
2022-03-02 15:19:30 - train: epoch 0076, iter [00300, 05004], lr: 0.071435, loss: 1.9264
2022-03-02 15:20:03 - train: epoch 0076, iter [00400, 05004], lr: 0.071435, loss: 1.8807
2022-03-02 15:20:36 - train: epoch 0076, iter [00500, 05004], lr: 0.071435, loss: 1.9454
2022-03-02 15:21:09 - train: epoch 0076, iter [00600, 05004], lr: 0.071435, loss: 1.9518
2022-03-02 15:21:43 - train: epoch 0076, iter [00700, 05004], lr: 0.071435, loss: 1.9977
2022-03-02 15:22:16 - train: epoch 0076, iter [00800, 05004], lr: 0.071435, loss: 1.7927
2022-03-02 15:22:49 - train: epoch 0076, iter [00900, 05004], lr: 0.071435, loss: 1.8593
2022-03-02 15:23:22 - train: epoch 0076, iter [01000, 05004], lr: 0.071435, loss: 1.8677
2022-03-02 15:23:55 - train: epoch 0076, iter [01100, 05004], lr: 0.071435, loss: 1.7084
2022-03-02 15:24:29 - train: epoch 0076, iter [01200, 05004], lr: 0.071435, loss: 1.9659
2022-03-02 15:25:02 - train: epoch 0076, iter [01300, 05004], lr: 0.071435, loss: 1.8225
2022-03-02 15:25:36 - train: epoch 0076, iter [01400, 05004], lr: 0.071435, loss: 1.8336
2022-03-02 15:26:09 - train: epoch 0076, iter [01500, 05004], lr: 0.071435, loss: 1.8603
2022-03-02 15:26:43 - train: epoch 0076, iter [01600, 05004], lr: 0.071435, loss: 1.9898
2022-03-02 15:27:15 - train: epoch 0076, iter [01700, 05004], lr: 0.071435, loss: 1.9381
2022-03-02 15:27:49 - train: epoch 0076, iter [01800, 05004], lr: 0.071435, loss: 1.7120
2022-03-02 15:28:23 - train: epoch 0076, iter [01900, 05004], lr: 0.071435, loss: 2.0212
2022-03-02 15:28:56 - train: epoch 0076, iter [02000, 05004], lr: 0.071435, loss: 1.9291
2022-03-02 15:29:29 - train: epoch 0076, iter [02100, 05004], lr: 0.071435, loss: 2.0822
2022-03-02 15:30:01 - train: epoch 0076, iter [02200, 05004], lr: 0.071435, loss: 2.0523
2022-03-02 15:30:34 - train: epoch 0076, iter [02300, 05004], lr: 0.071435, loss: 1.8085
2022-03-02 15:31:07 - train: epoch 0076, iter [02400, 05004], lr: 0.071435, loss: 2.1763
2022-03-02 15:31:41 - train: epoch 0076, iter [02500, 05004], lr: 0.071435, loss: 1.8441
2022-03-02 15:32:14 - train: epoch 0076, iter [02600, 05004], lr: 0.071435, loss: 2.1508
2022-03-02 15:32:47 - train: epoch 0076, iter [02700, 05004], lr: 0.071435, loss: 1.9759
2022-03-02 15:33:21 - train: epoch 0076, iter [02800, 05004], lr: 0.071435, loss: 1.8252
2022-03-02 15:33:54 - train: epoch 0076, iter [02900, 05004], lr: 0.071435, loss: 2.0488
2022-03-02 15:34:27 - train: epoch 0076, iter [03000, 05004], lr: 0.071435, loss: 1.7682
2022-03-02 15:35:00 - train: epoch 0076, iter [03100, 05004], lr: 0.071435, loss: 2.1594
2022-03-02 15:35:33 - train: epoch 0076, iter [03200, 05004], lr: 0.071435, loss: 1.8686
2022-03-02 15:36:06 - train: epoch 0076, iter [03300, 05004], lr: 0.071435, loss: 1.8144
2022-03-02 15:36:39 - train: epoch 0076, iter [03400, 05004], lr: 0.071435, loss: 1.9965
2022-03-02 15:37:12 - train: epoch 0076, iter [03500, 05004], lr: 0.071435, loss: 1.8243
2022-03-02 15:37:46 - train: epoch 0076, iter [03600, 05004], lr: 0.071435, loss: 1.8025
2022-03-02 15:38:19 - train: epoch 0076, iter [03700, 05004], lr: 0.071435, loss: 1.8186
2022-03-02 15:38:53 - train: epoch 0076, iter [03800, 05004], lr: 0.071435, loss: 1.8619
2022-03-02 15:39:26 - train: epoch 0076, iter [03900, 05004], lr: 0.071435, loss: 1.8704
2022-03-02 15:39:59 - train: epoch 0076, iter [04000, 05004], lr: 0.071435, loss: 1.8943
2022-03-02 15:40:32 - train: epoch 0076, iter [04100, 05004], lr: 0.071435, loss: 1.9701
2022-03-02 15:41:05 - train: epoch 0076, iter [04200, 05004], lr: 0.071435, loss: 2.1594
2022-03-02 15:41:39 - train: epoch 0076, iter [04300, 05004], lr: 0.071435, loss: 2.1294
2022-03-02 15:42:11 - train: epoch 0076, iter [04400, 05004], lr: 0.071435, loss: 1.7378
2022-03-02 15:42:45 - train: epoch 0076, iter [04500, 05004], lr: 0.071435, loss: 1.8521
2022-03-02 15:43:18 - train: epoch 0076, iter [04600, 05004], lr: 0.071435, loss: 1.9304
2022-03-02 15:43:51 - train: epoch 0076, iter [04700, 05004], lr: 0.071435, loss: 1.9954
2022-03-02 15:44:23 - train: epoch 0076, iter [04800, 05004], lr: 0.071435, loss: 1.8280
2022-03-02 15:44:57 - train: epoch 0076, iter [04900, 05004], lr: 0.071435, loss: 1.8989
2022-03-02 15:45:29 - train: epoch 0076, iter [05000, 05004], lr: 0.071435, loss: 1.9996
2022-03-02 15:45:30 - train: epoch 076, train_loss: 1.9446
2022-03-02 15:46:44 - eval: epoch: 076, acc1: 59.308%, acc5: 83.266%, test_loss: 1.6997, per_image_load_time: 2.364ms, per_image_inference_time: 0.516ms
2022-03-02 15:46:45 - until epoch: 076, best_acc1: 59.334%
2022-03-02 15:46:45 - epoch 077 lr: 0.0707041155014006
2022-03-02 15:47:23 - train: epoch 0077, iter [00100, 05004], lr: 0.070704, loss: 2.0619
2022-03-02 15:47:56 - train: epoch 0077, iter [00200, 05004], lr: 0.070704, loss: 2.0028
2022-03-02 15:48:29 - train: epoch 0077, iter [00300, 05004], lr: 0.070704, loss: 1.6716
2022-03-02 15:49:01 - train: epoch 0077, iter [00400, 05004], lr: 0.070704, loss: 2.0251
2022-03-02 15:49:34 - train: epoch 0077, iter [00500, 05004], lr: 0.070704, loss: 1.7207
2022-03-02 15:50:08 - train: epoch 0077, iter [00600, 05004], lr: 0.070704, loss: 1.6399
2022-03-02 15:50:40 - train: epoch 0077, iter [00700, 05004], lr: 0.070704, loss: 1.9517
2022-03-02 15:51:14 - train: epoch 0077, iter [00800, 05004], lr: 0.070704, loss: 2.0497
2022-03-02 15:51:47 - train: epoch 0077, iter [00900, 05004], lr: 0.070704, loss: 2.1314
2022-03-02 15:52:20 - train: epoch 0077, iter [01000, 05004], lr: 0.070704, loss: 1.6913
2022-03-02 15:52:54 - train: epoch 0077, iter [01100, 05004], lr: 0.070704, loss: 2.0774
2022-03-02 15:53:26 - train: epoch 0077, iter [01200, 05004], lr: 0.070704, loss: 2.0743
2022-03-02 15:54:00 - train: epoch 0077, iter [01300, 05004], lr: 0.070704, loss: 1.6370
2022-03-02 15:54:33 - train: epoch 0077, iter [01400, 05004], lr: 0.070704, loss: 2.0243
2022-03-02 15:55:06 - train: epoch 0077, iter [01500, 05004], lr: 0.070704, loss: 1.7139
2022-03-02 15:55:40 - train: epoch 0077, iter [01600, 05004], lr: 0.070704, loss: 2.1161
2022-03-02 15:56:13 - train: epoch 0077, iter [01700, 05004], lr: 0.070704, loss: 2.2480
2022-03-02 15:56:46 - train: epoch 0077, iter [01800, 05004], lr: 0.070704, loss: 1.7551
2022-03-02 15:57:20 - train: epoch 0077, iter [01900, 05004], lr: 0.070704, loss: 1.8717
2022-03-02 15:57:53 - train: epoch 0077, iter [02000, 05004], lr: 0.070704, loss: 1.7101
2022-03-02 15:58:27 - train: epoch 0077, iter [02100, 05004], lr: 0.070704, loss: 2.0685
2022-03-02 15:58:59 - train: epoch 0077, iter [02200, 05004], lr: 0.070704, loss: 1.9382
2022-03-02 15:59:33 - train: epoch 0077, iter [02300, 05004], lr: 0.070704, loss: 2.1511
2022-03-02 16:00:07 - train: epoch 0077, iter [02400, 05004], lr: 0.070704, loss: 1.8516
2022-03-02 16:00:39 - train: epoch 0077, iter [02500, 05004], lr: 0.070704, loss: 2.1694
2022-03-02 16:01:12 - train: epoch 0077, iter [02600, 05004], lr: 0.070704, loss: 1.8072
2022-03-02 16:01:46 - train: epoch 0077, iter [02700, 05004], lr: 0.070704, loss: 2.0295
2022-03-02 16:02:19 - train: epoch 0077, iter [02800, 05004], lr: 0.070704, loss: 2.1331
2022-03-02 16:02:52 - train: epoch 0077, iter [02900, 05004], lr: 0.070704, loss: 2.3474
2022-03-02 16:03:26 - train: epoch 0077, iter [03000, 05004], lr: 0.070704, loss: 1.7525
2022-03-02 16:03:58 - train: epoch 0077, iter [03100, 05004], lr: 0.070704, loss: 1.9265
2022-03-02 16:04:32 - train: epoch 0077, iter [03200, 05004], lr: 0.070704, loss: 2.0205
2022-03-02 16:05:05 - train: epoch 0077, iter [03300, 05004], lr: 0.070704, loss: 1.7773
2022-03-02 16:05:38 - train: epoch 0077, iter [03400, 05004], lr: 0.070704, loss: 2.2104
2022-03-02 16:06:11 - train: epoch 0077, iter [03500, 05004], lr: 0.070704, loss: 1.7443
2022-03-02 16:06:45 - train: epoch 0077, iter [03600, 05004], lr: 0.070704, loss: 1.9483
2022-03-02 16:07:18 - train: epoch 0077, iter [03700, 05004], lr: 0.070704, loss: 1.9982
2022-03-02 16:07:51 - train: epoch 0077, iter [03800, 05004], lr: 0.070704, loss: 1.8872
2022-03-02 16:08:24 - train: epoch 0077, iter [03900, 05004], lr: 0.070704, loss: 1.9554
2022-03-02 16:08:57 - train: epoch 0077, iter [04000, 05004], lr: 0.070704, loss: 2.0350
2022-03-02 16:09:31 - train: epoch 0077, iter [04100, 05004], lr: 0.070704, loss: 2.0486
2022-03-02 16:10:04 - train: epoch 0077, iter [04200, 05004], lr: 0.070704, loss: 2.0794
2022-03-02 16:10:38 - train: epoch 0077, iter [04300, 05004], lr: 0.070704, loss: 1.8559
2022-03-02 16:11:11 - train: epoch 0077, iter [04400, 05004], lr: 0.070704, loss: 1.8687
2022-03-02 16:11:44 - train: epoch 0077, iter [04500, 05004], lr: 0.070704, loss: 2.0776
2022-03-02 16:12:17 - train: epoch 0077, iter [04600, 05004], lr: 0.070704, loss: 1.9034
2022-03-02 16:12:50 - train: epoch 0077, iter [04700, 05004], lr: 0.070704, loss: 1.8631
2022-03-02 16:13:23 - train: epoch 0077, iter [04800, 05004], lr: 0.070704, loss: 2.0014
2022-03-02 16:13:56 - train: epoch 0077, iter [04900, 05004], lr: 0.070704, loss: 2.1931
2022-03-02 16:14:28 - train: epoch 0077, iter [05000, 05004], lr: 0.070704, loss: 2.1032
2022-03-02 16:14:30 - train: epoch 077, train_loss: 1.9376
2022-03-02 16:15:43 - eval: epoch: 077, acc1: 58.536%, acc5: 82.806%, test_loss: 1.7408, per_image_load_time: 2.364ms, per_image_inference_time: 0.498ms
2022-03-02 16:15:44 - until epoch: 077, best_acc1: 59.334%
2022-03-02 16:15:44 - epoch 078 lr: 0.06996822917828477
2022-03-02 16:16:22 - train: epoch 0078, iter [00100, 05004], lr: 0.069968, loss: 1.9161
2022-03-02 16:16:54 - train: epoch 0078, iter [00200, 05004], lr: 0.069968, loss: 2.0377
2022-03-02 16:17:27 - train: epoch 0078, iter [00300, 05004], lr: 0.069968, loss: 1.8414
2022-03-02 16:18:00 - train: epoch 0078, iter [00400, 05004], lr: 0.069968, loss: 1.8925
2022-03-02 16:18:33 - train: epoch 0078, iter [00500, 05004], lr: 0.069968, loss: 1.8280
2022-03-02 16:19:07 - train: epoch 0078, iter [00600, 05004], lr: 0.069968, loss: 1.9157
2022-03-02 16:19:39 - train: epoch 0078, iter [00700, 05004], lr: 0.069968, loss: 2.0053
2022-03-02 16:20:12 - train: epoch 0078, iter [00800, 05004], lr: 0.069968, loss: 2.1351
2022-03-02 16:20:46 - train: epoch 0078, iter [00900, 05004], lr: 0.069968, loss: 2.1108
2022-03-02 16:21:18 - train: epoch 0078, iter [01000, 05004], lr: 0.069968, loss: 1.9659
2022-03-02 16:21:51 - train: epoch 0078, iter [01100, 05004], lr: 0.069968, loss: 1.7288
2022-03-02 16:22:25 - train: epoch 0078, iter [01200, 05004], lr: 0.069968, loss: 1.7572
2022-03-02 16:22:57 - train: epoch 0078, iter [01300, 05004], lr: 0.069968, loss: 2.0345
2022-03-02 16:23:30 - train: epoch 0078, iter [01400, 05004], lr: 0.069968, loss: 1.7568
2022-03-02 16:24:04 - train: epoch 0078, iter [01500, 05004], lr: 0.069968, loss: 1.9472
2022-03-02 16:24:38 - train: epoch 0078, iter [01600, 05004], lr: 0.069968, loss: 2.0935
2022-03-02 16:25:12 - train: epoch 0078, iter [01700, 05004], lr: 0.069968, loss: 1.9619
2022-03-02 16:25:44 - train: epoch 0078, iter [01800, 05004], lr: 0.069968, loss: 1.9774
2022-03-02 16:26:18 - train: epoch 0078, iter [01900, 05004], lr: 0.069968, loss: 1.8342
2022-03-02 16:26:51 - train: epoch 0078, iter [02000, 05004], lr: 0.069968, loss: 1.8991
2022-03-02 16:27:25 - train: epoch 0078, iter [02100, 05004], lr: 0.069968, loss: 2.3034
2022-03-02 16:27:58 - train: epoch 0078, iter [02200, 05004], lr: 0.069968, loss: 1.9771
2022-03-02 16:28:31 - train: epoch 0078, iter [02300, 05004], lr: 0.069968, loss: 2.1957
2022-03-02 16:29:05 - train: epoch 0078, iter [02400, 05004], lr: 0.069968, loss: 1.8527
2022-03-02 16:29:38 - train: epoch 0078, iter [02500, 05004], lr: 0.069968, loss: 1.7575
2022-03-02 16:30:12 - train: epoch 0078, iter [02600, 05004], lr: 0.069968, loss: 1.9525
2022-03-02 16:30:45 - train: epoch 0078, iter [02700, 05004], lr: 0.069968, loss: 1.8942
2022-03-02 16:31:18 - train: epoch 0078, iter [02800, 05004], lr: 0.069968, loss: 1.9166
2022-03-02 16:31:51 - train: epoch 0078, iter [02900, 05004], lr: 0.069968, loss: 1.8967
2022-03-02 16:32:24 - train: epoch 0078, iter [03000, 05004], lr: 0.069968, loss: 1.9103
2022-03-02 16:32:57 - train: epoch 0078, iter [03100, 05004], lr: 0.069968, loss: 2.0209
2022-03-02 16:33:30 - train: epoch 0078, iter [03200, 05004], lr: 0.069968, loss: 1.9965
2022-03-02 16:34:03 - train: epoch 0078, iter [03300, 05004], lr: 0.069968, loss: 2.1504
2022-03-02 16:34:36 - train: epoch 0078, iter [03400, 05004], lr: 0.069968, loss: 1.9486
2022-03-02 16:35:09 - train: epoch 0078, iter [03500, 05004], lr: 0.069968, loss: 1.8255
2022-03-02 16:35:43 - train: epoch 0078, iter [03600, 05004], lr: 0.069968, loss: 1.9632
2022-03-02 16:36:16 - train: epoch 0078, iter [03700, 05004], lr: 0.069968, loss: 1.9170
2022-03-02 16:36:49 - train: epoch 0078, iter [03800, 05004], lr: 0.069968, loss: 1.9121
2022-03-02 16:37:22 - train: epoch 0078, iter [03900, 05004], lr: 0.069968, loss: 1.9106
2022-03-02 16:37:55 - train: epoch 0078, iter [04000, 05004], lr: 0.069968, loss: 2.0276
2022-03-02 16:38:28 - train: epoch 0078, iter [04100, 05004], lr: 0.069968, loss: 2.1365
2022-03-02 16:39:00 - train: epoch 0078, iter [04200, 05004], lr: 0.069968, loss: 2.1725
2022-03-02 16:39:33 - train: epoch 0078, iter [04300, 05004], lr: 0.069968, loss: 2.0037
2022-03-02 16:40:07 - train: epoch 0078, iter [04400, 05004], lr: 0.069968, loss: 2.0395
2022-03-02 16:40:39 - train: epoch 0078, iter [04500, 05004], lr: 0.069968, loss: 1.9103
2022-03-02 16:41:12 - train: epoch 0078, iter [04600, 05004], lr: 0.069968, loss: 1.6063
2022-03-02 16:41:46 - train: epoch 0078, iter [04700, 05004], lr: 0.069968, loss: 2.0704
2022-03-02 16:42:18 - train: epoch 0078, iter [04800, 05004], lr: 0.069968, loss: 2.1369
2022-03-02 16:42:52 - train: epoch 0078, iter [04900, 05004], lr: 0.069968, loss: 2.0202
2022-03-02 16:43:24 - train: epoch 0078, iter [05000, 05004], lr: 0.069968, loss: 1.9454
2022-03-02 16:43:25 - train: epoch 078, train_loss: 1.9296
2022-03-02 16:44:39 - eval: epoch: 078, acc1: 58.566%, acc5: 82.844%, test_loss: 1.7235, per_image_load_time: 2.305ms, per_image_inference_time: 0.543ms
2022-03-02 16:44:40 - until epoch: 078, best_acc1: 59.334%
2022-03-02 16:44:40 - epoch 079 lr: 0.06922716010014256
2022-03-02 16:45:18 - train: epoch 0079, iter [00100, 05004], lr: 0.069227, loss: 1.8901
2022-03-02 16:45:50 - train: epoch 0079, iter [00200, 05004], lr: 0.069227, loss: 1.8394
2022-03-02 16:46:23 - train: epoch 0079, iter [00300, 05004], lr: 0.069227, loss: 2.0278
2022-03-02 16:46:57 - train: epoch 0079, iter [00400, 05004], lr: 0.069227, loss: 2.0390
2022-03-02 16:47:30 - train: epoch 0079, iter [00500, 05004], lr: 0.069227, loss: 1.9445
2022-03-02 16:48:03 - train: epoch 0079, iter [00600, 05004], lr: 0.069227, loss: 1.8689
2022-03-02 16:48:37 - train: epoch 0079, iter [00700, 05004], lr: 0.069227, loss: 1.8215
2022-03-02 16:49:10 - train: epoch 0079, iter [00800, 05004], lr: 0.069227, loss: 1.9571
2022-03-02 16:49:43 - train: epoch 0079, iter [00900, 05004], lr: 0.069227, loss: 1.8189
2022-03-02 16:50:16 - train: epoch 0079, iter [01000, 05004], lr: 0.069227, loss: 1.7861
2022-03-02 16:50:50 - train: epoch 0079, iter [01100, 05004], lr: 0.069227, loss: 2.0854
2022-03-02 16:51:23 - train: epoch 0079, iter [01200, 05004], lr: 0.069227, loss: 2.1482
2022-03-02 16:51:57 - train: epoch 0079, iter [01300, 05004], lr: 0.069227, loss: 1.7331
2022-03-02 16:52:29 - train: epoch 0079, iter [01400, 05004], lr: 0.069227, loss: 1.8801
2022-03-02 16:53:03 - train: epoch 0079, iter [01500, 05004], lr: 0.069227, loss: 1.8506
2022-03-02 16:53:36 - train: epoch 0079, iter [01600, 05004], lr: 0.069227, loss: 1.7256
2022-03-02 16:54:09 - train: epoch 0079, iter [01700, 05004], lr: 0.069227, loss: 1.9729
2022-03-02 16:54:42 - train: epoch 0079, iter [01800, 05004], lr: 0.069227, loss: 1.9012
2022-03-02 16:55:16 - train: epoch 0079, iter [01900, 05004], lr: 0.069227, loss: 1.8763
2022-03-02 16:55:48 - train: epoch 0079, iter [02000, 05004], lr: 0.069227, loss: 2.1544
2022-03-02 16:56:22 - train: epoch 0079, iter [02100, 05004], lr: 0.069227, loss: 1.7188
2022-03-02 16:56:55 - train: epoch 0079, iter [02200, 05004], lr: 0.069227, loss: 1.9713
2022-03-02 16:57:28 - train: epoch 0079, iter [02300, 05004], lr: 0.069227, loss: 1.9909
2022-03-02 16:58:01 - train: epoch 0079, iter [02400, 05004], lr: 0.069227, loss: 1.9292
2022-03-02 16:58:33 - train: epoch 0079, iter [02500, 05004], lr: 0.069227, loss: 1.8045
2022-03-02 16:59:06 - train: epoch 0079, iter [02600, 05004], lr: 0.069227, loss: 1.8516
2022-03-02 16:59:39 - train: epoch 0079, iter [02700, 05004], lr: 0.069227, loss: 1.7087
2022-03-02 17:00:13 - train: epoch 0079, iter [02800, 05004], lr: 0.069227, loss: 1.7595
2022-03-02 17:00:46 - train: epoch 0079, iter [02900, 05004], lr: 0.069227, loss: 1.7507
2022-03-02 17:01:20 - train: epoch 0079, iter [03000, 05004], lr: 0.069227, loss: 1.9055
2022-03-02 17:01:53 - train: epoch 0079, iter [03100, 05004], lr: 0.069227, loss: 1.8303
2022-03-02 17:02:26 - train: epoch 0079, iter [03200, 05004], lr: 0.069227, loss: 2.2615
2022-03-02 17:02:59 - train: epoch 0079, iter [03300, 05004], lr: 0.069227, loss: 1.8589
2022-03-02 17:03:33 - train: epoch 0079, iter [03400, 05004], lr: 0.069227, loss: 1.8215
2022-03-02 17:04:05 - train: epoch 0079, iter [03500, 05004], lr: 0.069227, loss: 2.0800
2022-03-02 17:04:38 - train: epoch 0079, iter [03600, 05004], lr: 0.069227, loss: 1.9275
2022-03-02 17:05:12 - train: epoch 0079, iter [03700, 05004], lr: 0.069227, loss: 1.7615
2022-03-02 17:05:45 - train: epoch 0079, iter [03800, 05004], lr: 0.069227, loss: 1.9534
2022-03-02 17:06:18 - train: epoch 0079, iter [03900, 05004], lr: 0.069227, loss: 1.7675
2022-03-02 17:06:50 - train: epoch 0079, iter [04000, 05004], lr: 0.069227, loss: 1.7869
2022-03-02 17:07:23 - train: epoch 0079, iter [04100, 05004], lr: 0.069227, loss: 2.0552
2022-03-02 17:07:57 - train: epoch 0079, iter [04200, 05004], lr: 0.069227, loss: 1.7520
2022-03-02 17:08:30 - train: epoch 0079, iter [04300, 05004], lr: 0.069227, loss: 1.6265
2022-03-02 17:09:03 - train: epoch 0079, iter [04400, 05004], lr: 0.069227, loss: 2.1353
2022-03-02 17:09:36 - train: epoch 0079, iter [04500, 05004], lr: 0.069227, loss: 2.2925
2022-03-02 17:10:09 - train: epoch 0079, iter [04600, 05004], lr: 0.069227, loss: 2.1972
2022-03-02 17:10:43 - train: epoch 0079, iter [04700, 05004], lr: 0.069227, loss: 1.9325
2022-03-02 17:11:16 - train: epoch 0079, iter [04800, 05004], lr: 0.069227, loss: 2.0761
2022-03-02 17:11:49 - train: epoch 0079, iter [04900, 05004], lr: 0.069227, loss: 2.0600
2022-03-02 17:12:21 - train: epoch 0079, iter [05000, 05004], lr: 0.069227, loss: 1.7669
2022-03-02 17:12:22 - train: epoch 079, train_loss: 1.9293
2022-03-02 17:13:35 - eval: epoch: 079, acc1: 58.060%, acc5: 82.542%, test_loss: 1.7559, per_image_load_time: 0.763ms, per_image_inference_time: 0.495ms
2022-03-02 17:13:36 - until epoch: 079, best_acc1: 59.334%
2022-03-02 17:13:36 - epoch 080 lr: 0.06848110061149555
2022-03-02 17:14:14 - train: epoch 0080, iter [00100, 05004], lr: 0.068481, loss: 1.8260
2022-03-02 17:14:47 - train: epoch 0080, iter [00200, 05004], lr: 0.068481, loss: 1.9240
2022-03-02 17:15:20 - train: epoch 0080, iter [00300, 05004], lr: 0.068481, loss: 1.9496
2022-03-02 17:15:53 - train: epoch 0080, iter [00400, 05004], lr: 0.068481, loss: 1.6972
2022-03-02 17:16:26 - train: epoch 0080, iter [00500, 05004], lr: 0.068481, loss: 1.9081
2022-03-02 17:17:00 - train: epoch 0080, iter [00600, 05004], lr: 0.068481, loss: 1.7601
2022-03-02 17:17:33 - train: epoch 0080, iter [00700, 05004], lr: 0.068481, loss: 1.7337
2022-03-02 17:18:06 - train: epoch 0080, iter [00800, 05004], lr: 0.068481, loss: 1.8133
2022-03-02 17:18:38 - train: epoch 0080, iter [00900, 05004], lr: 0.068481, loss: 1.9166
2022-03-02 17:19:13 - train: epoch 0080, iter [01000, 05004], lr: 0.068481, loss: 1.7787
2022-03-02 17:19:45 - train: epoch 0080, iter [01100, 05004], lr: 0.068481, loss: 1.8276
2022-03-02 17:20:18 - train: epoch 0080, iter [01200, 05004], lr: 0.068481, loss: 1.8855
2022-03-02 17:20:52 - train: epoch 0080, iter [01300, 05004], lr: 0.068481, loss: 1.8724
2022-03-02 17:21:25 - train: epoch 0080, iter [01400, 05004], lr: 0.068481, loss: 1.9055
2022-03-02 17:21:59 - train: epoch 0080, iter [01500, 05004], lr: 0.068481, loss: 1.8341
2022-03-02 17:22:31 - train: epoch 0080, iter [01600, 05004], lr: 0.068481, loss: 1.8363
2022-03-02 17:23:05 - train: epoch 0080, iter [01700, 05004], lr: 0.068481, loss: 1.9747
2022-03-02 17:23:38 - train: epoch 0080, iter [01800, 05004], lr: 0.068481, loss: 2.1589
2022-03-02 17:24:11 - train: epoch 0080, iter [01900, 05004], lr: 0.068481, loss: 2.0062
2022-03-02 17:24:44 - train: epoch 0080, iter [02000, 05004], lr: 0.068481, loss: 2.0448
2022-03-02 17:25:18 - train: epoch 0080, iter [02100, 05004], lr: 0.068481, loss: 2.0822
2022-03-02 17:25:51 - train: epoch 0080, iter [02200, 05004], lr: 0.068481, loss: 2.0148
2022-03-02 17:26:24 - train: epoch 0080, iter [02300, 05004], lr: 0.068481, loss: 1.8743
2022-03-02 17:26:56 - train: epoch 0080, iter [02400, 05004], lr: 0.068481, loss: 1.9095
2022-03-02 17:27:30 - train: epoch 0080, iter [02500, 05004], lr: 0.068481, loss: 1.7553
2022-03-02 17:28:03 - train: epoch 0080, iter [02600, 05004], lr: 0.068481, loss: 1.9492
2022-03-02 17:28:37 - train: epoch 0080, iter [02700, 05004], lr: 0.068481, loss: 2.1121
2022-03-02 17:29:10 - train: epoch 0080, iter [02800, 05004], lr: 0.068481, loss: 1.9891
2022-03-02 17:29:43 - train: epoch 0080, iter [02900, 05004], lr: 0.068481, loss: 1.8315
2022-03-02 17:30:15 - train: epoch 0080, iter [03000, 05004], lr: 0.068481, loss: 1.9005
2022-03-02 17:30:49 - train: epoch 0080, iter [03100, 05004], lr: 0.068481, loss: 2.0334
2022-03-02 17:31:22 - train: epoch 0080, iter [03200, 05004], lr: 0.068481, loss: 1.5202
2022-03-02 17:31:55 - train: epoch 0080, iter [03300, 05004], lr: 0.068481, loss: 1.8384
2022-03-02 17:32:28 - train: epoch 0080, iter [03400, 05004], lr: 0.068481, loss: 1.7134
2022-03-02 17:33:01 - train: epoch 0080, iter [03500, 05004], lr: 0.068481, loss: 1.7610
2022-03-02 17:33:34 - train: epoch 0080, iter [03600, 05004], lr: 0.068481, loss: 1.9465
2022-03-02 17:34:08 - train: epoch 0080, iter [03700, 05004], lr: 0.068481, loss: 1.9307
2022-03-02 17:34:41 - train: epoch 0080, iter [03800, 05004], lr: 0.068481, loss: 2.0569
2022-03-02 17:35:14 - train: epoch 0080, iter [03900, 05004], lr: 0.068481, loss: 1.8835
2022-03-02 17:35:46 - train: epoch 0080, iter [04000, 05004], lr: 0.068481, loss: 2.0497
2022-03-02 17:36:19 - train: epoch 0080, iter [04100, 05004], lr: 0.068481, loss: 2.0693
2022-03-02 17:36:52 - train: epoch 0080, iter [04200, 05004], lr: 0.068481, loss: 1.9427
2022-03-02 17:37:25 - train: epoch 0080, iter [04300, 05004], lr: 0.068481, loss: 2.1211
2022-03-02 17:37:59 - train: epoch 0080, iter [04400, 05004], lr: 0.068481, loss: 1.9665
2022-03-02 17:38:32 - train: epoch 0080, iter [04500, 05004], lr: 0.068481, loss: 1.8226
2022-03-02 17:39:05 - train: epoch 0080, iter [04600, 05004], lr: 0.068481, loss: 2.0884
2022-03-02 17:39:38 - train: epoch 0080, iter [04700, 05004], lr: 0.068481, loss: 1.9967
2022-03-02 17:40:12 - train: epoch 0080, iter [04800, 05004], lr: 0.068481, loss: 1.9167
2022-03-02 17:40:44 - train: epoch 0080, iter [04900, 05004], lr: 0.068481, loss: 2.1478
2022-03-02 17:41:17 - train: epoch 0080, iter [05000, 05004], lr: 0.068481, loss: 1.8473
2022-03-02 17:41:18 - train: epoch 080, train_loss: 1.9221
2022-03-02 17:42:31 - eval: epoch: 080, acc1: 59.162%, acc5: 83.032%, test_loss: 1.7073, per_image_load_time: 2.316ms, per_image_inference_time: 0.555ms
2022-03-02 17:42:32 - until epoch: 080, best_acc1: 59.334%
2022-03-02 17:42:32 - epoch 081 lr: 0.06773024435212678
2022-03-02 17:43:10 - train: epoch 0081, iter [00100, 05004], lr: 0.067730, loss: 1.7063
2022-03-02 17:43:43 - train: epoch 0081, iter [00200, 05004], lr: 0.067730, loss: 1.9676
2022-03-02 17:44:16 - train: epoch 0081, iter [00300, 05004], lr: 0.067730, loss: 1.9338
2022-03-02 17:44:49 - train: epoch 0081, iter [00400, 05004], lr: 0.067730, loss: 2.1598
2022-03-02 17:45:22 - train: epoch 0081, iter [00500, 05004], lr: 0.067730, loss: 2.1100
2022-03-02 17:45:55 - train: epoch 0081, iter [00600, 05004], lr: 0.067730, loss: 1.9525
2022-03-02 17:46:28 - train: epoch 0081, iter [00700, 05004], lr: 0.067730, loss: 1.8661
2022-03-02 17:47:01 - train: epoch 0081, iter [00800, 05004], lr: 0.067730, loss: 2.0865
2022-03-02 17:47:35 - train: epoch 0081, iter [00900, 05004], lr: 0.067730, loss: 1.8479
2022-03-02 17:48:08 - train: epoch 0081, iter [01000, 05004], lr: 0.067730, loss: 1.9212
2022-03-02 17:48:41 - train: epoch 0081, iter [01100, 05004], lr: 0.067730, loss: 1.8305
2022-03-02 17:49:15 - train: epoch 0081, iter [01200, 05004], lr: 0.067730, loss: 1.9251
2022-03-02 17:49:48 - train: epoch 0081, iter [01300, 05004], lr: 0.067730, loss: 1.6722
2022-03-02 17:50:21 - train: epoch 0081, iter [01400, 05004], lr: 0.067730, loss: 1.5482
2022-03-02 17:50:54 - train: epoch 0081, iter [01500, 05004], lr: 0.067730, loss: 1.9372
2022-03-02 17:51:27 - train: epoch 0081, iter [01600, 05004], lr: 0.067730, loss: 1.8729
2022-03-02 17:52:01 - train: epoch 0081, iter [01700, 05004], lr: 0.067730, loss: 2.0180
2022-03-02 17:52:35 - train: epoch 0081, iter [01800, 05004], lr: 0.067730, loss: 1.8664
2022-03-02 17:53:07 - train: epoch 0081, iter [01900, 05004], lr: 0.067730, loss: 1.8916
2022-03-02 17:53:40 - train: epoch 0081, iter [02000, 05004], lr: 0.067730, loss: 1.9539
2022-03-02 17:54:14 - train: epoch 0081, iter [02100, 05004], lr: 0.067730, loss: 1.9749
2022-03-02 17:54:47 - train: epoch 0081, iter [02200, 05004], lr: 0.067730, loss: 1.9028
2022-03-02 17:55:19 - train: epoch 0081, iter [02300, 05004], lr: 0.067730, loss: 1.7718
2022-03-02 17:55:53 - train: epoch 0081, iter [02400, 05004], lr: 0.067730, loss: 1.9833
2022-03-02 17:56:25 - train: epoch 0081, iter [02500, 05004], lr: 0.067730, loss: 2.0903
2022-03-02 17:56:58 - train: epoch 0081, iter [02600, 05004], lr: 0.067730, loss: 2.2010
2022-03-02 17:57:32 - train: epoch 0081, iter [02700, 05004], lr: 0.067730, loss: 1.9375
2022-03-02 17:58:05 - train: epoch 0081, iter [02800, 05004], lr: 0.067730, loss: 1.8189
2022-03-02 17:58:38 - train: epoch 0081, iter [02900, 05004], lr: 0.067730, loss: 1.8851
2022-03-02 17:59:11 - train: epoch 0081, iter [03000, 05004], lr: 0.067730, loss: 2.0123
2022-03-02 17:59:45 - train: epoch 0081, iter [03100, 05004], lr: 0.067730, loss: 1.8139
2022-03-02 18:00:18 - train: epoch 0081, iter [03200, 05004], lr: 0.067730, loss: 1.9492
2022-03-02 18:00:51 - train: epoch 0081, iter [03300, 05004], lr: 0.067730, loss: 1.8468
2022-03-02 18:01:24 - train: epoch 0081, iter [03400, 05004], lr: 0.067730, loss: 2.2460
2022-03-02 18:01:58 - train: epoch 0081, iter [03500, 05004], lr: 0.067730, loss: 1.8904
2022-03-02 18:02:31 - train: epoch 0081, iter [03600, 05004], lr: 0.067730, loss: 1.7550
2022-03-02 18:03:04 - train: epoch 0081, iter [03700, 05004], lr: 0.067730, loss: 2.0691
2022-03-02 18:03:37 - train: epoch 0081, iter [03800, 05004], lr: 0.067730, loss: 1.8755
2022-03-02 18:04:10 - train: epoch 0081, iter [03900, 05004], lr: 0.067730, loss: 2.0800
2022-03-02 18:04:44 - train: epoch 0081, iter [04000, 05004], lr: 0.067730, loss: 1.7212
2022-03-02 18:05:17 - train: epoch 0081, iter [04100, 05004], lr: 0.067730, loss: 1.9366
2022-03-02 18:05:50 - train: epoch 0081, iter [04200, 05004], lr: 0.067730, loss: 2.0126
2022-03-02 18:06:22 - train: epoch 0081, iter [04300, 05004], lr: 0.067730, loss: 1.9387
2022-03-02 18:06:56 - train: epoch 0081, iter [04400, 05004], lr: 0.067730, loss: 2.1366
2022-03-02 18:07:29 - train: epoch 0081, iter [04500, 05004], lr: 0.067730, loss: 2.0032
2022-03-02 18:08:02 - train: epoch 0081, iter [04600, 05004], lr: 0.067730, loss: 1.8809
2022-03-02 18:08:35 - train: epoch 0081, iter [04700, 05004], lr: 0.067730, loss: 1.9777
2022-03-02 18:09:08 - train: epoch 0081, iter [04800, 05004], lr: 0.067730, loss: 2.0321
2022-03-02 18:09:41 - train: epoch 0081, iter [04900, 05004], lr: 0.067730, loss: 1.8479
2022-03-02 18:10:13 - train: epoch 0081, iter [05000, 05004], lr: 0.067730, loss: 1.6480
2022-03-02 18:10:15 - train: epoch 081, train_loss: 1.9169
2022-03-02 18:11:28 - eval: epoch: 081, acc1: 59.144%, acc5: 82.814%, test_loss: 1.7223, per_image_load_time: 1.163ms, per_image_inference_time: 0.534ms
2022-03-02 18:11:29 - until epoch: 081, best_acc1: 59.334%
2022-03-02 18:11:29 - epoch 082 lr: 0.06697478620682136
2022-03-02 18:12:06 - train: epoch 0082, iter [00100, 05004], lr: 0.066975, loss: 1.6049
2022-03-02 18:12:40 - train: epoch 0082, iter [00200, 05004], lr: 0.066975, loss: 1.7411
2022-03-02 18:13:13 - train: epoch 0082, iter [00300, 05004], lr: 0.066975, loss: 1.8450
2022-03-02 18:13:46 - train: epoch 0082, iter [00400, 05004], lr: 0.066975, loss: 1.9202
2022-03-02 18:14:20 - train: epoch 0082, iter [00500, 05004], lr: 0.066975, loss: 1.9787
2022-03-02 18:14:53 - train: epoch 0082, iter [00600, 05004], lr: 0.066975, loss: 1.7985
2022-03-02 18:15:26 - train: epoch 0082, iter [00700, 05004], lr: 0.066975, loss: 2.0133
2022-03-02 18:15:59 - train: epoch 0082, iter [00800, 05004], lr: 0.066975, loss: 1.8168
2022-03-02 18:16:32 - train: epoch 0082, iter [00900, 05004], lr: 0.066975, loss: 2.0450
2022-03-02 18:17:05 - train: epoch 0082, iter [01000, 05004], lr: 0.066975, loss: 2.0959
2022-03-02 18:17:39 - train: epoch 0082, iter [01100, 05004], lr: 0.066975, loss: 1.9644
2022-03-02 18:18:12 - train: epoch 0082, iter [01200, 05004], lr: 0.066975, loss: 1.8681
2022-03-02 18:18:46 - train: epoch 0082, iter [01300, 05004], lr: 0.066975, loss: 2.1571
2022-03-02 18:19:19 - train: epoch 0082, iter [01400, 05004], lr: 0.066975, loss: 1.6525
2022-03-02 18:19:52 - train: epoch 0082, iter [01500, 05004], lr: 0.066975, loss: 1.8748
2022-03-02 18:20:25 - train: epoch 0082, iter [01600, 05004], lr: 0.066975, loss: 1.8299
2022-03-02 18:20:58 - train: epoch 0082, iter [01700, 05004], lr: 0.066975, loss: 1.8261
2022-03-02 18:21:31 - train: epoch 0082, iter [01800, 05004], lr: 0.066975, loss: 1.7145
2022-03-02 18:22:05 - train: epoch 0082, iter [01900, 05004], lr: 0.066975, loss: 1.8324
2022-03-02 18:22:38 - train: epoch 0082, iter [02000, 05004], lr: 0.066975, loss: 1.7415
2022-03-02 18:23:11 - train: epoch 0082, iter [02100, 05004], lr: 0.066975, loss: 1.8185
2022-03-02 18:23:44 - train: epoch 0082, iter [02200, 05004], lr: 0.066975, loss: 1.8675
2022-03-02 18:24:19 - train: epoch 0082, iter [02300, 05004], lr: 0.066975, loss: 1.8396
2022-03-02 18:24:51 - train: epoch 0082, iter [02400, 05004], lr: 0.066975, loss: 2.0364
2022-03-02 18:25:25 - train: epoch 0082, iter [02500, 05004], lr: 0.066975, loss: 1.8438
2022-03-02 18:25:59 - train: epoch 0082, iter [02600, 05004], lr: 0.066975, loss: 1.9079
2022-03-02 18:26:31 - train: epoch 0082, iter [02700, 05004], lr: 0.066975, loss: 1.8146
2022-03-02 18:27:05 - train: epoch 0082, iter [02800, 05004], lr: 0.066975, loss: 1.8018
2022-03-02 18:27:38 - train: epoch 0082, iter [02900, 05004], lr: 0.066975, loss: 1.7478
2022-03-02 18:28:11 - train: epoch 0082, iter [03000, 05004], lr: 0.066975, loss: 1.8604
2022-03-02 18:28:45 - train: epoch 0082, iter [03100, 05004], lr: 0.066975, loss: 1.9113
2022-03-02 18:29:18 - train: epoch 0082, iter [03200, 05004], lr: 0.066975, loss: 2.0954
2022-03-02 18:29:51 - train: epoch 0082, iter [03300, 05004], lr: 0.066975, loss: 1.8894
2022-03-02 18:30:25 - train: epoch 0082, iter [03400, 05004], lr: 0.066975, loss: 1.8756
2022-03-02 18:30:58 - train: epoch 0082, iter [03500, 05004], lr: 0.066975, loss: 1.8376
2022-03-02 18:31:31 - train: epoch 0082, iter [03600, 05004], lr: 0.066975, loss: 1.9276
2022-03-02 18:32:04 - train: epoch 0082, iter [03700, 05004], lr: 0.066975, loss: 1.6705
2022-03-02 18:32:37 - train: epoch 0082, iter [03800, 05004], lr: 0.066975, loss: 2.2433
2022-03-02 18:33:10 - train: epoch 0082, iter [03900, 05004], lr: 0.066975, loss: 2.1103
2022-03-02 18:33:44 - train: epoch 0082, iter [04000, 05004], lr: 0.066975, loss: 1.9241
2022-03-02 18:34:17 - train: epoch 0082, iter [04100, 05004], lr: 0.066975, loss: 1.9675
2022-03-02 18:34:50 - train: epoch 0082, iter [04200, 05004], lr: 0.066975, loss: 2.0666
2022-03-02 18:35:23 - train: epoch 0082, iter [04300, 05004], lr: 0.066975, loss: 1.8333
2022-03-02 18:35:56 - train: epoch 0082, iter [04400, 05004], lr: 0.066975, loss: 1.7091
2022-03-02 18:36:29 - train: epoch 0082, iter [04500, 05004], lr: 0.066975, loss: 2.0507
2022-03-02 18:37:03 - train: epoch 0082, iter [04600, 05004], lr: 0.066975, loss: 2.0101
2022-03-02 18:37:35 - train: epoch 0082, iter [04700, 05004], lr: 0.066975, loss: 1.7522
2022-03-02 18:38:09 - train: epoch 0082, iter [04800, 05004], lr: 0.066975, loss: 1.8725
2022-03-02 18:38:42 - train: epoch 0082, iter [04900, 05004], lr: 0.066975, loss: 1.8837
2022-03-02 18:39:14 - train: epoch 0082, iter [05000, 05004], lr: 0.066975, loss: 1.8716
2022-03-02 18:39:15 - train: epoch 082, train_loss: 1.9105
2022-03-02 18:40:28 - eval: epoch: 082, acc1: 58.684%, acc5: 82.872%, test_loss: 1.7248, per_image_load_time: 2.317ms, per_image_inference_time: 0.519ms
2022-03-02 18:40:29 - until epoch: 082, best_acc1: 59.334%
2022-03-02 18:40:29 - epoch 083 lr: 0.06621492225478413
2022-03-02 18:41:07 - train: epoch 0083, iter [00100, 05004], lr: 0.066215, loss: 1.8236
2022-03-02 18:41:40 - train: epoch 0083, iter [00200, 05004], lr: 0.066215, loss: 1.7645
2022-03-02 18:42:13 - train: epoch 0083, iter [00300, 05004], lr: 0.066215, loss: 1.9976
2022-03-02 18:42:46 - train: epoch 0083, iter [00400, 05004], lr: 0.066215, loss: 1.9761
2022-03-02 18:43:19 - train: epoch 0083, iter [00500, 05004], lr: 0.066215, loss: 1.8958
2022-03-02 18:43:52 - train: epoch 0083, iter [00600, 05004], lr: 0.066215, loss: 1.8508
2022-03-02 18:44:25 - train: epoch 0083, iter [00700, 05004], lr: 0.066215, loss: 1.8598
2022-03-02 18:44:58 - train: epoch 0083, iter [00800, 05004], lr: 0.066215, loss: 1.9054
2022-03-02 18:45:32 - train: epoch 0083, iter [00900, 05004], lr: 0.066215, loss: 2.0310
2022-03-02 18:46:05 - train: epoch 0083, iter [01000, 05004], lr: 0.066215, loss: 1.9784
2022-03-02 18:46:38 - train: epoch 0083, iter [01100, 05004], lr: 0.066215, loss: 1.8584
2022-03-02 18:47:11 - train: epoch 0083, iter [01200, 05004], lr: 0.066215, loss: 1.9695
2022-03-02 18:47:44 - train: epoch 0083, iter [01300, 05004], lr: 0.066215, loss: 1.8712
2022-03-02 18:48:17 - train: epoch 0083, iter [01400, 05004], lr: 0.066215, loss: 2.0139
2022-03-02 18:48:50 - train: epoch 0083, iter [01500, 05004], lr: 0.066215, loss: 1.7387
2022-03-02 18:49:23 - train: epoch 0083, iter [01600, 05004], lr: 0.066215, loss: 1.8527
2022-03-02 18:49:56 - train: epoch 0083, iter [01700, 05004], lr: 0.066215, loss: 2.0529
2022-03-02 18:50:30 - train: epoch 0083, iter [01800, 05004], lr: 0.066215, loss: 2.0429
2022-03-02 18:51:02 - train: epoch 0083, iter [01900, 05004], lr: 0.066215, loss: 1.7451
2022-03-02 18:51:36 - train: epoch 0083, iter [02000, 05004], lr: 0.066215, loss: 1.6036
2022-03-02 18:52:09 - train: epoch 0083, iter [02100, 05004], lr: 0.066215, loss: 1.7248
2022-03-02 18:52:42 - train: epoch 0083, iter [02200, 05004], lr: 0.066215, loss: 1.8665
2022-03-02 18:53:16 - train: epoch 0083, iter [02300, 05004], lr: 0.066215, loss: 1.9228
2022-03-02 18:53:48 - train: epoch 0083, iter [02400, 05004], lr: 0.066215, loss: 1.7930
2022-03-02 18:54:21 - train: epoch 0083, iter [02500, 05004], lr: 0.066215, loss: 1.9343
2022-03-02 18:54:54 - train: epoch 0083, iter [02600, 05004], lr: 0.066215, loss: 1.7922
2022-03-02 18:55:27 - train: epoch 0083, iter [02700, 05004], lr: 0.066215, loss: 1.8108
2022-03-02 18:56:00 - train: epoch 0083, iter [02800, 05004], lr: 0.066215, loss: 1.7773
2022-03-02 18:56:33 - train: epoch 0083, iter [02900, 05004], lr: 0.066215, loss: 1.9905
2022-03-02 18:57:06 - train: epoch 0083, iter [03000, 05004], lr: 0.066215, loss: 2.0338
2022-03-02 18:57:39 - train: epoch 0083, iter [03100, 05004], lr: 0.066215, loss: 1.8081
2022-03-02 18:58:13 - train: epoch 0083, iter [03200, 05004], lr: 0.066215, loss: 1.8143
2022-03-02 18:58:46 - train: epoch 0083, iter [03300, 05004], lr: 0.066215, loss: 2.1438
2022-03-02 18:59:19 - train: epoch 0083, iter [03400, 05004], lr: 0.066215, loss: 2.1042
2022-03-02 18:59:52 - train: epoch 0083, iter [03500, 05004], lr: 0.066215, loss: 2.0080
2022-03-02 19:00:25 - train: epoch 0083, iter [03600, 05004], lr: 0.066215, loss: 1.9793
2022-03-02 19:00:58 - train: epoch 0083, iter [03700, 05004], lr: 0.066215, loss: 1.8431
2022-03-02 19:01:32 - train: epoch 0083, iter [03800, 05004], lr: 0.066215, loss: 1.7213
2022-03-02 19:02:05 - train: epoch 0083, iter [03900, 05004], lr: 0.066215, loss: 1.9758
2022-03-02 19:02:38 - train: epoch 0083, iter [04000, 05004], lr: 0.066215, loss: 2.2140
2022-03-02 19:03:11 - train: epoch 0083, iter [04100, 05004], lr: 0.066215, loss: 1.8391
2022-03-02 19:03:44 - train: epoch 0083, iter [04200, 05004], lr: 0.066215, loss: 2.3203
2022-03-02 19:04:16 - train: epoch 0083, iter [04300, 05004], lr: 0.066215, loss: 1.9692
2022-03-02 19:04:50 - train: epoch 0083, iter [04400, 05004], lr: 0.066215, loss: 1.8551
2022-03-02 19:05:23 - train: epoch 0083, iter [04500, 05004], lr: 0.066215, loss: 1.9521
2022-03-02 19:05:56 - train: epoch 0083, iter [04600, 05004], lr: 0.066215, loss: 2.1688
2022-03-02 19:06:29 - train: epoch 0083, iter [04700, 05004], lr: 0.066215, loss: 2.0668
2022-03-02 19:07:02 - train: epoch 0083, iter [04800, 05004], lr: 0.066215, loss: 2.1460
2022-03-02 19:07:35 - train: epoch 0083, iter [04900, 05004], lr: 0.066215, loss: 2.1362
2022-03-02 19:08:07 - train: epoch 0083, iter [05000, 05004], lr: 0.066215, loss: 1.8468
2022-03-02 19:08:08 - train: epoch 083, train_loss: 1.9061
2022-03-02 19:09:22 - eval: epoch: 083, acc1: 56.924%, acc5: 81.372%, test_loss: 1.8254, per_image_load_time: 2.259ms, per_image_inference_time: 0.541ms
2022-03-02 19:09:23 - until epoch: 083, best_acc1: 59.334%
2022-03-02 19:09:23 - epoch 084 lr: 0.06545084971874737
2022-03-02 19:10:01 - train: epoch 0084, iter [00100, 05004], lr: 0.065451, loss: 1.8302
2022-03-02 19:10:34 - train: epoch 0084, iter [00200, 05004], lr: 0.065451, loss: 1.9935
2022-03-02 19:11:08 - train: epoch 0084, iter [00300, 05004], lr: 0.065451, loss: 1.6861
2022-03-02 19:11:41 - train: epoch 0084, iter [00400, 05004], lr: 0.065451, loss: 1.8364
2022-03-02 19:12:15 - train: epoch 0084, iter [00500, 05004], lr: 0.065451, loss: 1.8946
2022-03-02 19:12:48 - train: epoch 0084, iter [00600, 05004], lr: 0.065451, loss: 2.1899
2022-03-02 19:13:21 - train: epoch 0084, iter [00700, 05004], lr: 0.065451, loss: 1.9847
2022-03-02 19:13:53 - train: epoch 0084, iter [00800, 05004], lr: 0.065451, loss: 1.9754
2022-03-02 19:14:27 - train: epoch 0084, iter [00900, 05004], lr: 0.065451, loss: 1.7233
2022-03-02 19:15:00 - train: epoch 0084, iter [01000, 05004], lr: 0.065451, loss: 1.9182
2022-03-02 19:15:33 - train: epoch 0084, iter [01100, 05004], lr: 0.065451, loss: 1.9934
2022-03-02 19:16:07 - train: epoch 0084, iter [01200, 05004], lr: 0.065451, loss: 2.1811
2022-03-02 19:16:41 - train: epoch 0084, iter [01300, 05004], lr: 0.065451, loss: 2.0228
2022-03-02 19:17:14 - train: epoch 0084, iter [01400, 05004], lr: 0.065451, loss: 2.0565
2022-03-02 19:17:47 - train: epoch 0084, iter [01500, 05004], lr: 0.065451, loss: 2.0053
2022-03-02 19:18:21 - train: epoch 0084, iter [01600, 05004], lr: 0.065451, loss: 1.7923
2022-03-02 19:18:54 - train: epoch 0084, iter [01700, 05004], lr: 0.065451, loss: 1.9959
2022-03-02 19:19:28 - train: epoch 0084, iter [01800, 05004], lr: 0.065451, loss: 1.8973
2022-03-02 19:20:02 - train: epoch 0084, iter [01900, 05004], lr: 0.065451, loss: 1.8344
2022-03-02 19:20:34 - train: epoch 0084, iter [02000, 05004], lr: 0.065451, loss: 1.9364
2022-03-02 19:21:08 - train: epoch 0084, iter [02100, 05004], lr: 0.065451, loss: 1.7967
2022-03-02 19:21:42 - train: epoch 0084, iter [02200, 05004], lr: 0.065451, loss: 1.6369
2022-03-02 19:22:15 - train: epoch 0084, iter [02300, 05004], lr: 0.065451, loss: 1.9296
2022-03-02 19:22:48 - train: epoch 0084, iter [02400, 05004], lr: 0.065451, loss: 1.8077
2022-03-02 19:23:21 - train: epoch 0084, iter [02500, 05004], lr: 0.065451, loss: 1.9435
2022-03-02 19:23:55 - train: epoch 0084, iter [02600, 05004], lr: 0.065451, loss: 2.0006
2022-03-02 19:24:28 - train: epoch 0084, iter [02700, 05004], lr: 0.065451, loss: 1.8734
2022-03-02 19:25:01 - train: epoch 0084, iter [02800, 05004], lr: 0.065451, loss: 2.0732
2022-03-02 19:25:34 - train: epoch 0084, iter [02900, 05004], lr: 0.065451, loss: 1.8275
2022-03-02 19:26:07 - train: epoch 0084, iter [03000, 05004], lr: 0.065451, loss: 1.9156
2022-03-02 19:26:41 - train: epoch 0084, iter [03100, 05004], lr: 0.065451, loss: 1.8865
2022-03-02 19:27:14 - train: epoch 0084, iter [03200, 05004], lr: 0.065451, loss: 1.8582
2022-03-02 19:27:48 - train: epoch 0084, iter [03300, 05004], lr: 0.065451, loss: 1.9751
2022-03-02 19:28:21 - train: epoch 0084, iter [03400, 05004], lr: 0.065451, loss: 1.7618
2022-03-02 19:28:54 - train: epoch 0084, iter [03500, 05004], lr: 0.065451, loss: 1.9041
2022-03-02 19:29:27 - train: epoch 0084, iter [03600, 05004], lr: 0.065451, loss: 1.9845
2022-03-02 19:30:00 - train: epoch 0084, iter [03700, 05004], lr: 0.065451, loss: 2.0532
2022-03-02 19:30:34 - train: epoch 0084, iter [03800, 05004], lr: 0.065451, loss: 2.2701
2022-03-02 19:31:07 - train: epoch 0084, iter [03900, 05004], lr: 0.065451, loss: 1.9579
2022-03-02 19:31:40 - train: epoch 0084, iter [04000, 05004], lr: 0.065451, loss: 1.9403
2022-03-02 19:32:14 - train: epoch 0084, iter [04100, 05004], lr: 0.065451, loss: 1.9304
2022-03-02 19:32:47 - train: epoch 0084, iter [04200, 05004], lr: 0.065451, loss: 1.9185
2022-03-02 19:33:20 - train: epoch 0084, iter [04300, 05004], lr: 0.065451, loss: 2.0174
2022-03-02 19:33:53 - train: epoch 0084, iter [04400, 05004], lr: 0.065451, loss: 2.0874
2022-03-02 19:34:26 - train: epoch 0084, iter [04500, 05004], lr: 0.065451, loss: 1.6993
2022-03-02 19:34:59 - train: epoch 0084, iter [04600, 05004], lr: 0.065451, loss: 1.7955
2022-03-02 19:35:32 - train: epoch 0084, iter [04700, 05004], lr: 0.065451, loss: 2.2224
2022-03-02 19:36:05 - train: epoch 0084, iter [04800, 05004], lr: 0.065451, loss: 1.7558
2022-03-02 19:36:39 - train: epoch 0084, iter [04900, 05004], lr: 0.065451, loss: 1.7173
2022-03-02 19:37:11 - train: epoch 0084, iter [05000, 05004], lr: 0.065451, loss: 1.8778
2022-03-02 19:37:12 - train: epoch 084, train_loss: 1.9009
2022-03-02 19:38:26 - eval: epoch: 084, acc1: 60.450%, acc5: 83.994%, test_loss: 1.6427, per_image_load_time: 2.263ms, per_image_inference_time: 0.554ms
2022-03-02 19:38:27 - until epoch: 084, best_acc1: 60.450%
2022-03-02 19:38:27 - epoch 085 lr: 0.06468276691378154
2022-03-02 19:39:05 - train: epoch 0085, iter [00100, 05004], lr: 0.064683, loss: 1.8018
2022-03-02 19:39:39 - train: epoch 0085, iter [00200, 05004], lr: 0.064683, loss: 1.7709
2022-03-02 19:40:11 - train: epoch 0085, iter [00300, 05004], lr: 0.064683, loss: 2.0057
2022-03-02 19:40:44 - train: epoch 0085, iter [00400, 05004], lr: 0.064683, loss: 1.7790
2022-03-02 19:41:17 - train: epoch 0085, iter [00500, 05004], lr: 0.064683, loss: 2.0661
2022-03-02 19:41:50 - train: epoch 0085, iter [00600, 05004], lr: 0.064683, loss: 1.6106
2022-03-02 19:42:23 - train: epoch 0085, iter [00700, 05004], lr: 0.064683, loss: 1.7574
2022-03-02 19:42:57 - train: epoch 0085, iter [00800, 05004], lr: 0.064683, loss: 1.7595
2022-03-02 19:43:30 - train: epoch 0085, iter [00900, 05004], lr: 0.064683, loss: 1.7546
2022-03-02 19:44:02 - train: epoch 0085, iter [01000, 05004], lr: 0.064683, loss: 2.0573
2022-03-02 19:44:35 - train: epoch 0085, iter [01100, 05004], lr: 0.064683, loss: 2.0620
2022-03-02 19:45:09 - train: epoch 0085, iter [01200, 05004], lr: 0.064683, loss: 1.7109
2022-03-02 19:45:43 - train: epoch 0085, iter [01300, 05004], lr: 0.064683, loss: 1.8558
2022-03-02 19:46:16 - train: epoch 0085, iter [01400, 05004], lr: 0.064683, loss: 1.9564
2022-03-02 19:46:49 - train: epoch 0085, iter [01500, 05004], lr: 0.064683, loss: 1.8144
2022-03-02 19:47:22 - train: epoch 0085, iter [01600, 05004], lr: 0.064683, loss: 1.9378
2022-03-02 19:47:55 - train: epoch 0085, iter [01700, 05004], lr: 0.064683, loss: 2.2077
2022-03-02 19:48:28 - train: epoch 0085, iter [01800, 05004], lr: 0.064683, loss: 1.6959
2022-03-02 19:49:02 - train: epoch 0085, iter [01900, 05004], lr: 0.064683, loss: 1.7483
2022-03-02 19:49:36 - train: epoch 0085, iter [02000, 05004], lr: 0.064683, loss: 1.9131
2022-03-02 19:50:09 - train: epoch 0085, iter [02100, 05004], lr: 0.064683, loss: 1.5119
2022-03-02 19:50:42 - train: epoch 0085, iter [02200, 05004], lr: 0.064683, loss: 2.0026
2022-03-02 19:51:15 - train: epoch 0085, iter [02300, 05004], lr: 0.064683, loss: 1.7063
2022-03-02 19:51:48 - train: epoch 0085, iter [02400, 05004], lr: 0.064683, loss: 1.9785
2022-03-02 19:52:22 - train: epoch 0085, iter [02500, 05004], lr: 0.064683, loss: 2.1484
2022-03-02 19:52:56 - train: epoch 0085, iter [02600, 05004], lr: 0.064683, loss: 1.9705
2022-03-02 19:53:29 - train: epoch 0085, iter [02700, 05004], lr: 0.064683, loss: 1.7101
2022-03-02 19:54:03 - train: epoch 0085, iter [02800, 05004], lr: 0.064683, loss: 2.0815
2022-03-02 19:54:36 - train: epoch 0085, iter [02900, 05004], lr: 0.064683, loss: 2.0517
2022-03-02 19:55:09 - train: epoch 0085, iter [03000, 05004], lr: 0.064683, loss: 2.1269
2022-03-02 19:55:42 - train: epoch 0085, iter [03100, 05004], lr: 0.064683, loss: 1.9372
2022-03-02 19:56:16 - train: epoch 0085, iter [03200, 05004], lr: 0.064683, loss: 1.9317
2022-03-02 19:56:49 - train: epoch 0085, iter [03300, 05004], lr: 0.064683, loss: 1.9406
2022-03-02 19:57:22 - train: epoch 0085, iter [03400, 05004], lr: 0.064683, loss: 1.9790
2022-03-02 19:57:57 - train: epoch 0085, iter [03500, 05004], lr: 0.064683, loss: 1.9234
2022-03-02 19:58:31 - train: epoch 0085, iter [03600, 05004], lr: 0.064683, loss: 1.9579
2022-03-02 19:59:04 - train: epoch 0085, iter [03700, 05004], lr: 0.064683, loss: 1.9377
2022-03-02 19:59:37 - train: epoch 0085, iter [03800, 05004], lr: 0.064683, loss: 1.8413
2022-03-02 20:00:10 - train: epoch 0085, iter [03900, 05004], lr: 0.064683, loss: 1.8703
2022-03-02 20:00:44 - train: epoch 0085, iter [04000, 05004], lr: 0.064683, loss: 2.2046
2022-03-02 20:01:16 - train: epoch 0085, iter [04100, 05004], lr: 0.064683, loss: 1.7826
2022-03-02 20:01:50 - train: epoch 0085, iter [04200, 05004], lr: 0.064683, loss: 2.0222
2022-03-02 20:02:24 - train: epoch 0085, iter [04300, 05004], lr: 0.064683, loss: 1.8347
2022-03-02 20:02:57 - train: epoch 0085, iter [04400, 05004], lr: 0.064683, loss: 1.8566
2022-03-02 20:03:29 - train: epoch 0085, iter [04500, 05004], lr: 0.064683, loss: 2.0470
2022-03-02 20:04:03 - train: epoch 0085, iter [04600, 05004], lr: 0.064683, loss: 2.0101
2022-03-02 20:04:36 - train: epoch 0085, iter [04700, 05004], lr: 0.064683, loss: 2.1818
2022-03-02 20:05:10 - train: epoch 0085, iter [04800, 05004], lr: 0.064683, loss: 1.6328
2022-03-02 20:05:43 - train: epoch 0085, iter [04900, 05004], lr: 0.064683, loss: 2.0469
2022-03-02 20:06:16 - train: epoch 0085, iter [05000, 05004], lr: 0.064683, loss: 1.8387
2022-03-02 20:06:17 - train: epoch 085, train_loss: 1.8966
2022-03-02 20:07:31 - eval: epoch: 085, acc1: 58.182%, acc5: 82.086%, test_loss: 1.7733, per_image_load_time: 2.256ms, per_image_inference_time: 0.547ms
2022-03-02 20:07:31 - until epoch: 085, best_acc1: 60.450%
2022-03-02 20:07:31 - epoch 086 lr: 0.06391087319582264
2022-03-02 20:08:10 - train: epoch 0086, iter [00100, 05004], lr: 0.063911, loss: 1.6625
2022-03-02 20:08:42 - train: epoch 0086, iter [00200, 05004], lr: 0.063911, loss: 1.7361
2022-03-02 20:09:15 - train: epoch 0086, iter [00300, 05004], lr: 0.063911, loss: 2.2340
2022-03-02 20:09:49 - train: epoch 0086, iter [00400, 05004], lr: 0.063911, loss: 1.9207
2022-03-02 20:10:22 - train: epoch 0086, iter [00500, 05004], lr: 0.063911, loss: 1.9358
2022-03-02 20:10:55 - train: epoch 0086, iter [00600, 05004], lr: 0.063911, loss: 1.8007
2022-03-02 20:11:28 - train: epoch 0086, iter [00700, 05004], lr: 0.063911, loss: 1.7679
2022-03-02 20:12:01 - train: epoch 0086, iter [00800, 05004], lr: 0.063911, loss: 2.1157
2022-03-02 20:12:35 - train: epoch 0086, iter [00900, 05004], lr: 0.063911, loss: 1.9933
2022-03-02 20:13:08 - train: epoch 0086, iter [01000, 05004], lr: 0.063911, loss: 1.9852
2022-03-02 20:13:41 - train: epoch 0086, iter [01100, 05004], lr: 0.063911, loss: 1.9463
2022-03-02 20:14:14 - train: epoch 0086, iter [01200, 05004], lr: 0.063911, loss: 1.7407
2022-03-02 20:14:48 - train: epoch 0086, iter [01300, 05004], lr: 0.063911, loss: 1.8948
2022-03-02 20:15:21 - train: epoch 0086, iter [01400, 05004], lr: 0.063911, loss: 1.8548
2022-03-02 20:15:54 - train: epoch 0086, iter [01500, 05004], lr: 0.063911, loss: 1.7558
2022-03-02 20:16:26 - train: epoch 0086, iter [01600, 05004], lr: 0.063911, loss: 1.8500
2022-03-02 20:17:00 - train: epoch 0086, iter [01700, 05004], lr: 0.063911, loss: 1.8132
2022-03-02 20:17:34 - train: epoch 0086, iter [01800, 05004], lr: 0.063911, loss: 2.0639
2022-03-02 20:18:06 - train: epoch 0086, iter [01900, 05004], lr: 0.063911, loss: 2.0358
2022-03-02 20:18:39 - train: epoch 0086, iter [02000, 05004], lr: 0.063911, loss: 1.8445
2022-03-02 20:19:13 - train: epoch 0086, iter [02100, 05004], lr: 0.063911, loss: 1.7720
2022-03-02 20:19:46 - train: epoch 0086, iter [02200, 05004], lr: 0.063911, loss: 1.9950
2022-03-02 20:20:20 - train: epoch 0086, iter [02300, 05004], lr: 0.063911, loss: 1.8334
2022-03-02 20:20:53 - train: epoch 0086, iter [02400, 05004], lr: 0.063911, loss: 1.5765
2022-03-02 20:21:26 - train: epoch 0086, iter [02500, 05004], lr: 0.063911, loss: 1.7850
2022-03-02 20:21:59 - train: epoch 0086, iter [02600, 05004], lr: 0.063911, loss: 1.9408
2022-03-02 20:22:32 - train: epoch 0086, iter [02700, 05004], lr: 0.063911, loss: 1.7719
2022-03-02 20:23:05 - train: epoch 0086, iter [02800, 05004], lr: 0.063911, loss: 1.9621
2022-03-02 20:23:40 - train: epoch 0086, iter [02900, 05004], lr: 0.063911, loss: 1.6896
2022-03-02 20:24:12 - train: epoch 0086, iter [03000, 05004], lr: 0.063911, loss: 1.9878
2022-03-02 20:24:46 - train: epoch 0086, iter [03100, 05004], lr: 0.063911, loss: 1.8944
2022-03-02 20:25:19 - train: epoch 0086, iter [03200, 05004], lr: 0.063911, loss: 2.0237
2022-03-02 20:25:52 - train: epoch 0086, iter [03300, 05004], lr: 0.063911, loss: 2.0570
2022-03-02 20:26:25 - train: epoch 0086, iter [03400, 05004], lr: 0.063911, loss: 2.0279
2022-03-02 20:26:59 - train: epoch 0086, iter [03500, 05004], lr: 0.063911, loss: 2.1546
2022-03-02 20:27:32 - train: epoch 0086, iter [03600, 05004], lr: 0.063911, loss: 1.9333
2022-03-02 20:28:05 - train: epoch 0086, iter [03700, 05004], lr: 0.063911, loss: 1.9542
2022-03-02 20:28:38 - train: epoch 0086, iter [03800, 05004], lr: 0.063911, loss: 1.8703
2022-03-02 20:29:12 - train: epoch 0086, iter [03900, 05004], lr: 0.063911, loss: 1.8885
2022-03-02 20:29:44 - train: epoch 0086, iter [04000, 05004], lr: 0.063911, loss: 2.0753
2022-03-02 20:30:18 - train: epoch 0086, iter [04100, 05004], lr: 0.063911, loss: 1.7682
2022-03-02 20:30:51 - train: epoch 0086, iter [04200, 05004], lr: 0.063911, loss: 2.0447
2022-03-02 20:31:25 - train: epoch 0086, iter [04300, 05004], lr: 0.063911, loss: 2.0320
2022-03-02 20:31:58 - train: epoch 0086, iter [04400, 05004], lr: 0.063911, loss: 1.8371
2022-03-02 20:32:31 - train: epoch 0086, iter [04500, 05004], lr: 0.063911, loss: 1.7358
2022-03-02 20:33:04 - train: epoch 0086, iter [04600, 05004], lr: 0.063911, loss: 1.8752
2022-03-02 20:33:38 - train: epoch 0086, iter [04700, 05004], lr: 0.063911, loss: 2.0111
2022-03-02 20:34:11 - train: epoch 0086, iter [04800, 05004], lr: 0.063911, loss: 1.9681
2022-03-02 20:34:45 - train: epoch 0086, iter [04900, 05004], lr: 0.063911, loss: 1.9698
2022-03-02 20:35:16 - train: epoch 0086, iter [05000, 05004], lr: 0.063911, loss: 1.8347
2022-03-02 20:35:17 - train: epoch 086, train_loss: 1.8896
2022-03-02 20:36:31 - eval: epoch: 086, acc1: 59.486%, acc5: 83.202%, test_loss: 1.6978, per_image_load_time: 1.420ms, per_image_inference_time: 0.545ms
2022-03-02 20:36:31 - until epoch: 086, best_acc1: 60.450%
2022-03-02 20:36:31 - epoch 087 lr: 0.06313536890992935
2022-03-02 20:37:10 - train: epoch 0087, iter [00100, 05004], lr: 0.063135, loss: 1.8761
2022-03-02 20:37:43 - train: epoch 0087, iter [00200, 05004], lr: 0.063135, loss: 1.6284
2022-03-02 20:38:15 - train: epoch 0087, iter [00300, 05004], lr: 0.063135, loss: 1.9219
2022-03-02 20:38:48 - train: epoch 0087, iter [00400, 05004], lr: 0.063135, loss: 2.1283
2022-03-02 20:39:21 - train: epoch 0087, iter [00500, 05004], lr: 0.063135, loss: 1.6097
2022-03-02 20:39:54 - train: epoch 0087, iter [00600, 05004], lr: 0.063135, loss: 1.9110
2022-03-02 20:40:28 - train: epoch 0087, iter [00700, 05004], lr: 0.063135, loss: 1.7278
2022-03-02 20:41:00 - train: epoch 0087, iter [00800, 05004], lr: 0.063135, loss: 1.9289
2022-03-02 20:41:33 - train: epoch 0087, iter [00900, 05004], lr: 0.063135, loss: 2.0073
2022-03-02 20:42:07 - train: epoch 0087, iter [01000, 05004], lr: 0.063135, loss: 1.8711
2022-03-02 20:42:40 - train: epoch 0087, iter [01100, 05004], lr: 0.063135, loss: 1.9586
2022-03-02 20:43:12 - train: epoch 0087, iter [01200, 05004], lr: 0.063135, loss: 1.9283
2022-03-02 20:43:46 - train: epoch 0087, iter [01300, 05004], lr: 0.063135, loss: 1.9852
2022-03-02 20:44:19 - train: epoch 0087, iter [01400, 05004], lr: 0.063135, loss: 1.8282
2022-03-02 20:44:52 - train: epoch 0087, iter [01500, 05004], lr: 0.063135, loss: 1.7272
2022-03-02 20:45:25 - train: epoch 0087, iter [01600, 05004], lr: 0.063135, loss: 1.7451
2022-03-02 20:45:58 - train: epoch 0087, iter [01700, 05004], lr: 0.063135, loss: 2.0965
2022-03-02 20:46:31 - train: epoch 0087, iter [01800, 05004], lr: 0.063135, loss: 1.9135
2022-03-02 20:47:05 - train: epoch 0087, iter [01900, 05004], lr: 0.063135, loss: 1.9227
2022-03-02 20:47:37 - train: epoch 0087, iter [02000, 05004], lr: 0.063135, loss: 1.8722
2022-03-02 20:48:11 - train: epoch 0087, iter [02100, 05004], lr: 0.063135, loss: 1.9519
2022-03-02 20:48:45 - train: epoch 0087, iter [02200, 05004], lr: 0.063135, loss: 1.9329
2022-03-02 20:49:17 - train: epoch 0087, iter [02300, 05004], lr: 0.063135, loss: 1.9830
2022-03-02 20:49:50 - train: epoch 0087, iter [02400, 05004], lr: 0.063135, loss: 1.8179
2022-03-02 20:50:23 - train: epoch 0087, iter [02500, 05004], lr: 0.063135, loss: 1.8797
2022-03-02 20:50:56 - train: epoch 0087, iter [02600, 05004], lr: 0.063135, loss: 1.8264
2022-03-02 20:51:29 - train: epoch 0087, iter [02700, 05004], lr: 0.063135, loss: 1.8931
2022-03-02 20:52:03 - train: epoch 0087, iter [02800, 05004], lr: 0.063135, loss: 1.7001
2022-03-02 20:52:35 - train: epoch 0087, iter [02900, 05004], lr: 0.063135, loss: 1.8659
2022-03-02 20:53:08 - train: epoch 0087, iter [03000, 05004], lr: 0.063135, loss: 1.9312
2022-03-02 20:53:41 - train: epoch 0087, iter [03100, 05004], lr: 0.063135, loss: 1.9394
2022-03-02 20:54:15 - train: epoch 0087, iter [03200, 05004], lr: 0.063135, loss: 1.8594
2022-03-02 20:54:47 - train: epoch 0087, iter [03300, 05004], lr: 0.063135, loss: 1.9134
2022-03-02 20:55:20 - train: epoch 0087, iter [03400, 05004], lr: 0.063135, loss: 1.9367
2022-03-02 20:55:54 - train: epoch 0087, iter [03500, 05004], lr: 0.063135, loss: 1.8950
2022-03-02 20:56:26 - train: epoch 0087, iter [03600, 05004], lr: 0.063135, loss: 1.7717
2022-03-02 20:57:00 - train: epoch 0087, iter [03700, 05004], lr: 0.063135, loss: 1.8057
2022-03-02 20:57:33 - train: epoch 0087, iter [03800, 05004], lr: 0.063135, loss: 1.8529
2022-03-02 20:58:06 - train: epoch 0087, iter [03900, 05004], lr: 0.063135, loss: 1.8398
2022-03-02 20:58:39 - train: epoch 0087, iter [04000, 05004], lr: 0.063135, loss: 2.0239
2022-03-02 20:59:13 - train: epoch 0087, iter [04100, 05004], lr: 0.063135, loss: 1.9489
2022-03-02 20:59:46 - train: epoch 0087, iter [04200, 05004], lr: 0.063135, loss: 1.9797
2022-03-02 21:00:19 - train: epoch 0087, iter [04300, 05004], lr: 0.063135, loss: 1.7734
2022-03-02 21:00:51 - train: epoch 0087, iter [04400, 05004], lr: 0.063135, loss: 1.9400
2022-03-02 21:01:25 - train: epoch 0087, iter [04500, 05004], lr: 0.063135, loss: 2.0591
2022-03-02 21:01:58 - train: epoch 0087, iter [04600, 05004], lr: 0.063135, loss: 1.9495
2022-03-02 21:02:31 - train: epoch 0087, iter [04700, 05004], lr: 0.063135, loss: 2.0314
2022-03-02 21:03:05 - train: epoch 0087, iter [04800, 05004], lr: 0.063135, loss: 1.8879
2022-03-02 21:03:38 - train: epoch 0087, iter [04900, 05004], lr: 0.063135, loss: 1.9338
2022-03-02 21:04:10 - train: epoch 0087, iter [05000, 05004], lr: 0.063135, loss: 1.9253
2022-03-02 21:04:11 - train: epoch 087, train_loss: 1.8857
2022-03-02 21:05:25 - eval: epoch: 087, acc1: 60.232%, acc5: 83.682%, test_loss: 1.6590, per_image_load_time: 0.543ms, per_image_inference_time: 0.492ms
2022-03-02 21:05:25 - until epoch: 087, best_acc1: 60.450%
2022-03-02 21:05:25 - epoch 088 lr: 0.06235645533828348
2022-03-02 21:06:03 - train: epoch 0088, iter [00100, 05004], lr: 0.062356, loss: 1.7716
2022-03-02 21:06:36 - train: epoch 0088, iter [00200, 05004], lr: 0.062356, loss: 1.8182
2022-03-02 21:07:09 - train: epoch 0088, iter [00300, 05004], lr: 0.062356, loss: 1.8535
2022-03-02 21:07:42 - train: epoch 0088, iter [00400, 05004], lr: 0.062356, loss: 1.8975
2022-03-02 21:08:16 - train: epoch 0088, iter [00500, 05004], lr: 0.062356, loss: 1.8568
2022-03-02 21:08:49 - train: epoch 0088, iter [00600, 05004], lr: 0.062356, loss: 1.7700
2022-03-02 21:09:23 - train: epoch 0088, iter [00700, 05004], lr: 0.062356, loss: 1.7710
2022-03-02 21:09:55 - train: epoch 0088, iter [00800, 05004], lr: 0.062356, loss: 1.7493
2022-03-02 21:10:29 - train: epoch 0088, iter [00900, 05004], lr: 0.062356, loss: 2.0501
2022-03-02 21:11:03 - train: epoch 0088, iter [01000, 05004], lr: 0.062356, loss: 1.7763
2022-03-02 21:11:34 - train: epoch 0088, iter [01100, 05004], lr: 0.062356, loss: 1.9665
2022-03-02 21:12:09 - train: epoch 0088, iter [01200, 05004], lr: 0.062356, loss: 1.8365
2022-03-02 21:12:42 - train: epoch 0088, iter [01300, 05004], lr: 0.062356, loss: 2.0683
2022-03-02 21:13:15 - train: epoch 0088, iter [01400, 05004], lr: 0.062356, loss: 1.7465
2022-03-02 21:13:49 - train: epoch 0088, iter [01500, 05004], lr: 0.062356, loss: 1.9431
2022-03-02 21:14:22 - train: epoch 0088, iter [01600, 05004], lr: 0.062356, loss: 1.7801
2022-03-02 21:14:56 - train: epoch 0088, iter [01700, 05004], lr: 0.062356, loss: 1.9575
2022-03-02 21:15:29 - train: epoch 0088, iter [01800, 05004], lr: 0.062356, loss: 1.8008
2022-03-02 21:16:02 - train: epoch 0088, iter [01900, 05004], lr: 0.062356, loss: 1.9561
2022-03-02 21:16:35 - train: epoch 0088, iter [02000, 05004], lr: 0.062356, loss: 1.7987
2022-03-02 21:17:09 - train: epoch 0088, iter [02100, 05004], lr: 0.062356, loss: 1.8448
2022-03-02 21:17:42 - train: epoch 0088, iter [02200, 05004], lr: 0.062356, loss: 1.9458
2022-03-02 21:18:15 - train: epoch 0088, iter [02300, 05004], lr: 0.062356, loss: 1.7056
2022-03-02 21:18:48 - train: epoch 0088, iter [02400, 05004], lr: 0.062356, loss: 2.0760
2022-03-02 21:19:21 - train: epoch 0088, iter [02500, 05004], lr: 0.062356, loss: 1.9678
2022-03-02 21:19:54 - train: epoch 0088, iter [02600, 05004], lr: 0.062356, loss: 1.7988
2022-03-02 21:20:28 - train: epoch 0088, iter [02700, 05004], lr: 0.062356, loss: 1.8591
2022-03-02 21:21:01 - train: epoch 0088, iter [02800, 05004], lr: 0.062356, loss: 1.9425
2022-03-02 21:21:35 - train: epoch 0088, iter [02900, 05004], lr: 0.062356, loss: 1.8817
2022-03-02 21:22:08 - train: epoch 0088, iter [03000, 05004], lr: 0.062356, loss: 2.1143
2022-03-02 21:22:41 - train: epoch 0088, iter [03100, 05004], lr: 0.062356, loss: 1.7303
2022-03-02 21:23:14 - train: epoch 0088, iter [03200, 05004], lr: 0.062356, loss: 1.7286
2022-03-02 21:23:48 - train: epoch 0088, iter [03300, 05004], lr: 0.062356, loss: 1.7322
2022-03-02 21:24:21 - train: epoch 0088, iter [03400, 05004], lr: 0.062356, loss: 1.6719
2022-03-02 21:24:54 - train: epoch 0088, iter [03500, 05004], lr: 0.062356, loss: 1.7279
2022-03-02 21:25:27 - train: epoch 0088, iter [03600, 05004], lr: 0.062356, loss: 1.8711
2022-03-02 21:25:59 - train: epoch 0088, iter [03700, 05004], lr: 0.062356, loss: 1.9821
2022-03-02 21:26:33 - train: epoch 0088, iter [03800, 05004], lr: 0.062356, loss: 1.9781
2022-03-02 21:27:07 - train: epoch 0088, iter [03900, 05004], lr: 0.062356, loss: 2.0615
2022-03-02 21:27:40 - train: epoch 0088, iter [04000, 05004], lr: 0.062356, loss: 1.7808
2022-03-02 21:28:14 - train: epoch 0088, iter [04100, 05004], lr: 0.062356, loss: 1.9681
2022-03-02 21:28:47 - train: epoch 0088, iter [04200, 05004], lr: 0.062356, loss: 1.8834
2022-03-02 21:29:20 - train: epoch 0088, iter [04300, 05004], lr: 0.062356, loss: 1.9065
2022-03-02 21:29:54 - train: epoch 0088, iter [04400, 05004], lr: 0.062356, loss: 1.8192
2022-03-02 21:30:27 - train: epoch 0088, iter [04500, 05004], lr: 0.062356, loss: 1.8611
2022-03-02 21:31:00 - train: epoch 0088, iter [04600, 05004], lr: 0.062356, loss: 2.0471
2022-03-02 21:31:33 - train: epoch 0088, iter [04700, 05004], lr: 0.062356, loss: 2.0143
2022-03-02 21:32:05 - train: epoch 0088, iter [04800, 05004], lr: 0.062356, loss: 1.9067
2022-03-02 21:32:39 - train: epoch 0088, iter [04900, 05004], lr: 0.062356, loss: 1.6270
2022-03-02 21:33:11 - train: epoch 0088, iter [05000, 05004], lr: 0.062356, loss: 1.9278
2022-03-02 21:33:12 - train: epoch 088, train_loss: 1.8783
2022-03-02 21:34:26 - eval: epoch: 088, acc1: 60.566%, acc5: 84.210%, test_loss: 1.6384, per_image_load_time: 1.604ms, per_image_inference_time: 0.526ms
2022-03-02 21:34:27 - until epoch: 088, best_acc1: 60.566%
2022-03-02 21:34:27 - epoch 089 lr: 0.06157433464794716
2022-03-02 21:35:05 - train: epoch 0089, iter [00100, 05004], lr: 0.061574, loss: 2.0925
2022-03-02 21:35:38 - train: epoch 0089, iter [00200, 05004], lr: 0.061574, loss: 1.6610
2022-03-02 21:36:10 - train: epoch 0089, iter [00300, 05004], lr: 0.061574, loss: 1.8528
2022-03-02 21:36:44 - train: epoch 0089, iter [00400, 05004], lr: 0.061574, loss: 1.7250
2022-03-02 21:37:17 - train: epoch 0089, iter [00500, 05004], lr: 0.061574, loss: 1.8791
2022-03-02 21:37:50 - train: epoch 0089, iter [00600, 05004], lr: 0.061574, loss: 1.7851
2022-03-02 21:38:22 - train: epoch 0089, iter [00700, 05004], lr: 0.061574, loss: 1.8848
2022-03-02 21:38:56 - train: epoch 0089, iter [00800, 05004], lr: 0.061574, loss: 2.1514
2022-03-02 21:39:29 - train: epoch 0089, iter [00900, 05004], lr: 0.061574, loss: 1.9377
2022-03-02 21:40:02 - train: epoch 0089, iter [01000, 05004], lr: 0.061574, loss: 1.9746
2022-03-02 21:40:35 - train: epoch 0089, iter [01100, 05004], lr: 0.061574, loss: 1.8910
2022-03-02 21:41:08 - train: epoch 0089, iter [01200, 05004], lr: 0.061574, loss: 1.9871
2022-03-02 21:41:41 - train: epoch 0089, iter [01300, 05004], lr: 0.061574, loss: 1.8296
2022-03-02 21:42:15 - train: epoch 0089, iter [01400, 05004], lr: 0.061574, loss: 1.8554
2022-03-02 21:42:48 - train: epoch 0089, iter [01500, 05004], lr: 0.061574, loss: 1.9345
2022-03-02 21:43:21 - train: epoch 0089, iter [01600, 05004], lr: 0.061574, loss: 1.7248
2022-03-02 21:43:55 - train: epoch 0089, iter [01700, 05004], lr: 0.061574, loss: 1.8529
2022-03-02 21:44:28 - train: epoch 0089, iter [01800, 05004], lr: 0.061574, loss: 1.8463
2022-03-02 21:45:01 - train: epoch 0089, iter [01900, 05004], lr: 0.061574, loss: 1.8241
2022-03-02 21:45:34 - train: epoch 0089, iter [02000, 05004], lr: 0.061574, loss: 1.7791
2022-03-02 21:46:08 - train: epoch 0089, iter [02100, 05004], lr: 0.061574, loss: 1.9725
2022-03-02 21:46:40 - train: epoch 0089, iter [02200, 05004], lr: 0.061574, loss: 1.9589
2022-03-02 21:47:14 - train: epoch 0089, iter [02300, 05004], lr: 0.061574, loss: 1.8477
2022-03-02 21:47:47 - train: epoch 0089, iter [02400, 05004], lr: 0.061574, loss: 2.0247
2022-03-02 21:48:20 - train: epoch 0089, iter [02500, 05004], lr: 0.061574, loss: 1.9148
2022-03-02 21:48:54 - train: epoch 0089, iter [02600, 05004], lr: 0.061574, loss: 1.8453
2022-03-02 21:49:27 - train: epoch 0089, iter [02700, 05004], lr: 0.061574, loss: 1.7824
2022-03-02 21:50:01 - train: epoch 0089, iter [02800, 05004], lr: 0.061574, loss: 1.9523
2022-03-02 21:50:34 - train: epoch 0089, iter [02900, 05004], lr: 0.061574, loss: 1.8781
2022-03-02 21:51:06 - train: epoch 0089, iter [03000, 05004], lr: 0.061574, loss: 1.9240
2022-03-02 21:51:40 - train: epoch 0089, iter [03100, 05004], lr: 0.061574, loss: 2.0255
2022-03-02 21:52:13 - train: epoch 0089, iter [03200, 05004], lr: 0.061574, loss: 2.0428
2022-03-02 21:52:46 - train: epoch 0089, iter [03300, 05004], lr: 0.061574, loss: 2.1455
2022-03-02 21:53:19 - train: epoch 0089, iter [03400, 05004], lr: 0.061574, loss: 1.8378
2022-03-02 21:53:52 - train: epoch 0089, iter [03500, 05004], lr: 0.061574, loss: 1.6201
2022-03-02 21:54:26 - train: epoch 0089, iter [03600, 05004], lr: 0.061574, loss: 1.7951
2022-03-02 21:55:00 - train: epoch 0089, iter [03700, 05004], lr: 0.061574, loss: 2.0831
2022-03-02 21:55:33 - train: epoch 0089, iter [03800, 05004], lr: 0.061574, loss: 1.8769
2022-03-02 21:56:06 - train: epoch 0089, iter [03900, 05004], lr: 0.061574, loss: 2.0868
2022-03-02 21:56:38 - train: epoch 0089, iter [04000, 05004], lr: 0.061574, loss: 1.7575
2022-03-02 21:57:12 - train: epoch 0089, iter [04100, 05004], lr: 0.061574, loss: 2.1894
2022-03-02 21:57:45 - train: epoch 0089, iter [04200, 05004], lr: 0.061574, loss: 1.8307
2022-03-02 21:58:18 - train: epoch 0089, iter [04300, 05004], lr: 0.061574, loss: 1.8712
2022-03-02 21:58:51 - train: epoch 0089, iter [04400, 05004], lr: 0.061574, loss: 1.8937
2022-03-02 21:59:25 - train: epoch 0089, iter [04500, 05004], lr: 0.061574, loss: 1.7368
2022-03-02 21:59:59 - train: epoch 0089, iter [04600, 05004], lr: 0.061574, loss: 1.7969
2022-03-02 22:00:32 - train: epoch 0089, iter [04700, 05004], lr: 0.061574, loss: 1.8452
2022-03-02 22:01:05 - train: epoch 0089, iter [04800, 05004], lr: 0.061574, loss: 1.9371
2022-03-02 22:01:39 - train: epoch 0089, iter [04900, 05004], lr: 0.061574, loss: 1.8664
2022-03-02 22:02:11 - train: epoch 0089, iter [05000, 05004], lr: 0.061574, loss: 1.6434
2022-03-02 22:02:12 - train: epoch 089, train_loss: 1.8735
2022-03-02 22:03:26 - eval: epoch: 089, acc1: 58.084%, acc5: 82.240%, test_loss: 1.7588, per_image_load_time: 2.320ms, per_image_inference_time: 0.555ms
2022-03-02 22:03:27 - until epoch: 089, best_acc1: 60.566%
2022-03-02 22:03:27 - epoch 090 lr: 0.06078920983839031
2022-03-02 22:04:04 - train: epoch 0090, iter [00100, 05004], lr: 0.060789, loss: 1.8764
2022-03-02 22:04:38 - train: epoch 0090, iter [00200, 05004], lr: 0.060789, loss: 1.8307
2022-03-02 22:05:10 - train: epoch 0090, iter [00300, 05004], lr: 0.060789, loss: 1.7837
2022-03-02 22:05:43 - train: epoch 0090, iter [00400, 05004], lr: 0.060789, loss: 1.7193
2022-03-02 22:06:17 - train: epoch 0090, iter [00500, 05004], lr: 0.060789, loss: 1.8651
2022-03-02 22:06:50 - train: epoch 0090, iter [00600, 05004], lr: 0.060789, loss: 1.9678
2022-03-02 22:07:23 - train: epoch 0090, iter [00700, 05004], lr: 0.060789, loss: 1.8866
2022-03-02 22:07:56 - train: epoch 0090, iter [00800, 05004], lr: 0.060789, loss: 1.9305
2022-03-02 22:08:29 - train: epoch 0090, iter [00900, 05004], lr: 0.060789, loss: 1.7067
2022-03-02 22:09:02 - train: epoch 0090, iter [01000, 05004], lr: 0.060789, loss: 1.7024
2022-03-02 22:09:35 - train: epoch 0090, iter [01100, 05004], lr: 0.060789, loss: 1.9211
2022-03-02 22:10:09 - train: epoch 0090, iter [01200, 05004], lr: 0.060789, loss: 1.8193
2022-03-02 22:10:41 - train: epoch 0090, iter [01300, 05004], lr: 0.060789, loss: 2.0212
2022-03-02 22:11:15 - train: epoch 0090, iter [01400, 05004], lr: 0.060789, loss: 1.6492
2022-03-02 22:11:48 - train: epoch 0090, iter [01500, 05004], lr: 0.060789, loss: 2.1031
2022-03-02 22:12:21 - train: epoch 0090, iter [01600, 05004], lr: 0.060789, loss: 1.8868
2022-03-02 22:12:54 - train: epoch 0090, iter [01700, 05004], lr: 0.060789, loss: 1.6343
2022-03-02 22:13:27 - train: epoch 0090, iter [01800, 05004], lr: 0.060789, loss: 1.7268
2022-03-02 22:14:01 - train: epoch 0090, iter [01900, 05004], lr: 0.060789, loss: 1.6716
2022-03-02 22:14:34 - train: epoch 0090, iter [02000, 05004], lr: 0.060789, loss: 2.0268
2022-03-02 22:15:07 - train: epoch 0090, iter [02100, 05004], lr: 0.060789, loss: 2.0867
2022-03-02 22:15:40 - train: epoch 0090, iter [02200, 05004], lr: 0.060789, loss: 1.9253
2022-03-02 22:16:13 - train: epoch 0090, iter [02300, 05004], lr: 0.060789, loss: 2.1275
2022-03-02 22:16:46 - train: epoch 0090, iter [02400, 05004], lr: 0.060789, loss: 1.9567
2022-03-02 22:17:20 - train: epoch 0090, iter [02500, 05004], lr: 0.060789, loss: 1.9442
2022-03-02 22:17:52 - train: epoch 0090, iter [02600, 05004], lr: 0.060789, loss: 1.9540
2022-03-02 22:18:25 - train: epoch 0090, iter [02700, 05004], lr: 0.060789, loss: 1.7611
2022-03-02 22:18:59 - train: epoch 0090, iter [02800, 05004], lr: 0.060789, loss: 1.8897
2022-03-02 22:19:31 - train: epoch 0090, iter [02900, 05004], lr: 0.060789, loss: 1.8387
2022-03-02 22:20:04 - train: epoch 0090, iter [03000, 05004], lr: 0.060789, loss: 1.8035
2022-03-02 22:20:37 - train: epoch 0090, iter [03100, 05004], lr: 0.060789, loss: 1.6811
2022-03-02 22:21:10 - train: epoch 0090, iter [03200, 05004], lr: 0.060789, loss: 1.7451
2022-03-02 22:21:43 - train: epoch 0090, iter [03300, 05004], lr: 0.060789, loss: 2.0761
2022-03-02 22:22:15 - train: epoch 0090, iter [03400, 05004], lr: 0.060789, loss: 1.6988
2022-03-02 22:22:48 - train: epoch 0090, iter [03500, 05004], lr: 0.060789, loss: 1.8299
2022-03-02 22:23:21 - train: epoch 0090, iter [03600, 05004], lr: 0.060789, loss: 1.6976
2022-03-02 22:23:55 - train: epoch 0090, iter [03700, 05004], lr: 0.060789, loss: 1.9315
2022-03-02 22:24:27 - train: epoch 0090, iter [03800, 05004], lr: 0.060789, loss: 1.9057
2022-03-02 22:25:01 - train: epoch 0090, iter [03900, 05004], lr: 0.060789, loss: 1.5257
2022-03-02 22:25:34 - train: epoch 0090, iter [04000, 05004], lr: 0.060789, loss: 1.8791
2022-03-02 22:26:07 - train: epoch 0090, iter [04100, 05004], lr: 0.060789, loss: 2.2333
2022-03-02 22:26:40 - train: epoch 0090, iter [04200, 05004], lr: 0.060789, loss: 2.0132
2022-03-02 22:27:13 - train: epoch 0090, iter [04300, 05004], lr: 0.060789, loss: 1.9229
2022-03-02 22:27:46 - train: epoch 0090, iter [04400, 05004], lr: 0.060789, loss: 1.7666
2022-03-02 22:28:19 - train: epoch 0090, iter [04500, 05004], lr: 0.060789, loss: 1.8534
2022-03-02 22:28:52 - train: epoch 0090, iter [04600, 05004], lr: 0.060789, loss: 1.9647
2022-03-02 22:29:24 - train: epoch 0090, iter [04700, 05004], lr: 0.060789, loss: 1.7795
2022-03-02 22:29:58 - train: epoch 0090, iter [04800, 05004], lr: 0.060789, loss: 1.9627
2022-03-02 22:30:32 - train: epoch 0090, iter [04900, 05004], lr: 0.060789, loss: 1.8852
2022-03-02 22:31:03 - train: epoch 0090, iter [05000, 05004], lr: 0.060789, loss: 1.7931
2022-03-02 22:31:04 - train: epoch 090, train_loss: 1.8663
2022-03-02 22:32:17 - eval: epoch: 090, acc1: 60.090%, acc5: 83.634%, test_loss: 1.6583, per_image_load_time: 0.584ms, per_image_inference_time: 0.505ms
2022-03-02 22:32:18 - until epoch: 090, best_acc1: 60.566%
2022-03-02 22:32:18 - epoch 091 lr: 0.060001284688802226
2022-03-02 22:32:55 - train: epoch 0091, iter [00100, 05004], lr: 0.060001, loss: 1.7542
2022-03-02 22:33:28 - train: epoch 0091, iter [00200, 05004], lr: 0.060001, loss: 1.8441
2022-03-02 22:34:01 - train: epoch 0091, iter [00300, 05004], lr: 0.060001, loss: 1.8669
